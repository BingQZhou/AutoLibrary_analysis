,title,abstract
0,Revisiting the Issues On Netflow Sample and Export Performance,"  The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail.
"
1,Optimal Synthesis of Multiple Algorithms,"  In this paper we give a definition of ""algorithm,"" ""finite algorithm,""
""equivalent algorithms,"" and what it means for a single algorithm to dominate a
set of algorithms. We define a derived algorithm which may have a smaller mean
execution time than any of its component algorithms. We give an explicit
expression for the mean execution time (when it exists) of the derived
algorithm. We give several illustrative examples of derived algorithms with two
component algorithms. We include mean execution time solutions for
two-algorithm processors whose joint density of execution times are of several
general forms. For the case in which the joint density for a two-algorithm
processor is a step function, we give a maximum-likelihood estimation scheme
with which to analyze empirical processing time data.
"
2,"Availability assessment of SunOS/Solaris Unix Systems based on Syslogd
  and wtmpx logfiles : a case study","  This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality.
"
3,"Empirical analysis and statistical modeling of attack processes based on
  honeypots","  Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives.
"
4,An architecture-based dependability modeling framework using AADL,"  For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains.
"
5,"A Hierarchical Approach for Dependability Analysis of a Commercial
  Cache-Based RAID Storage Architecture","  We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks.
"
6,"Differential Diversity Reception of MDPSK over Independent Rayleigh
  Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum","  This paper is concerned with optimum diversity receiver structure and its
performance analysis of differential phase shift keying (DPSK) with
differential detection over nonselective, independent, nonidentically
distributed, Rayleigh fading channels. The fading process in each branch is
assumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth,
but to have distinct, asymmetric fading power spectral density characteristic.
Using 8-DPSK as an example, the average bit error probability (BEP) of the
optimum diversity receiver is obtained by calculating the BEP for each of the
three individual bits. The BEP results derived are given in exact, explicit,
closed-form expressions which show clearly the behavior of the performance as a
function of various system parameters.
"
7,A Technical Report On Grid Benchmarking using ATLAS V.O,"  Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.
"
8,Towards Informative Statistical Flow Inversion,"  A problem which has recently attracted research attention is that of
estimating the distribution of flow sizes in internet traffic. On high traffic
links it is sometimes impossible to record every packet. Researchers have
approached the problem of estimating flow lengths from sampled packet data in
two separate ways. Firstly, different sampling methodologies can be tried to
more accurately measure the desired system parameters. One such method is the
sample-and-hold method where, if a packet is sampled, all subsequent packets in
that flow are sampled. Secondly, statistical methods can be used to ``invert''
the sampled data and produce an estimate of flow lengths from a sample.
  In this paper we propose, implement and test two variants on the
sample-and-hold method. In addition we show how the sample-and-hold method can
be inverted to get an estimation of the genuine distribution of flow sizes.
Experiments are carried out on real network traces to compare standard packet
sampling with three variants of sample-and-hold. The methods are compared for
their ability to reconstruct the genuine distribution of flow sizes in the
traffic.
"
9,Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems,"  The churn rate of a peer-to-peer system places direct limitations on the rate
at which messages can be effectively communicated to a group of peers. These
limitations are independent of the topology and message transmission latency.
In this paper we consider a peer-to-peer network, based on the Engset model,
where peers arrive and depart independently at random. We show how the arrival
and departure rates directly limit the capacity for message streams to be
broadcast to all other peers, by deriving mean field models that accurately
describe the system behavior. Our models cover the unit and more general k
buffer cases, i.e. where a peer can buffer at most k messages at any one time,
and we give results for both single and multi-source message streams. We define
coverage rate as peer-messages per unit time, i.e. the rate at which a number
of peers receive messages, and show that the coverage rate is limited by the
churn rate and buffer size. Our theory introduces an Instantaneous Message
Exchange (IME) model and provides a template for further analysis of more
complicated systems. Using the IME model, and assuming random processes, we
have obtained very accurate equations of the system dynamics in a variety of
interesting cases, that allow us to tune a peer-to-peer system. It remains to
be seen if we can maintain this accuracy for general processes and when
applying a non-instantaneous model.
"
10,An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF,"  Admission control as a mechanism for providing QoS requires an accurate
description of the requested flow as well as already admitted flows. Since
802.11 WLAN capacity is shared between flows belonging to all stations,
admission control requires knowledge of all flows in the WLAN. Further,
estimation of the load-dependent WLAN capacity through analytical model
requires inputs about channel data rate, payload size and the number of
stations. These factors combined point to a centralized admission control
whereas for 802.11 DCF it is ideally performed in a distributed manner. The use
of measurements from the channel avoids explicit inputs about the state of the
channel described above. BUFFET, a model based measurement-assisted distributed
admission control scheme for DCF proposed in this paper relies on measurements
to derive model inputs and predict WLAN saturation, thereby maintaining average
delay within acceptable limits. Being measurement based, it adapts to a
combination of data rates and payload sizes, making it completely autonomous
and distributed. Performance analysis using OPNET simulations suggests that
BUFFET is able to ensure average delay under 7ms at a near-optimal throughput.
"
11,"An Extensible Timing Infrastructure for Adaptive Large-scale
  Applications","  Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.
"
12,Cache Analysis of Non-uniform Distribution Sorting Algorithms,"  We analyse the average-case cache performance of distribution sorting
algorithms in the case when keys are independently but not necessarily
uniformly distributed. The analysis is for both `in-place' and `out-of-place'
distribution sorting algorithms and is more accurate than the analysis
presented in \cite{RRESA00}. In particular, this new analysis yields tighter
upper and lower bounds when the keys are drawn from a uniform distribution.
  We use this analysis to tune the performance of the integer sorting algorithm
MSB radix sort when it is used to sort independent uniform floating-point
numbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a
cache-tuned implementations of bucketsort \cite{RR99} and Quicksort when
sorting uniform floats from $[0, 1)$.
"
13,A Comparison of Push and Pull Techniques for Ajax,"  Ajax applications are designed to have high user interactivity and low
user-perceived latency. Real-time dynamic web data such as news headlines,
stock tickers, and auction updates need to be propagated to the users as soon
as possible. However, Ajax still suffers from the limitations of the Web's
request/response architecture which prevents servers from pushing real-time
dynamic web data. Such applications usually use a pull style to obtain the
latest updates, where the client actively requests the changes based on a
predefined interval. It is possible to overcome this limitation by adopting a
push style of interaction where the server broadcasts data when a change occurs
on the server side. Both these options have their own trade-offs. This paper
explores the fundamental limits of browser-based applications and analyzes push
solutions for Ajax technology. It also shows the results of an empirical study
comparing push and pull.
"
14,Fast and Simple Relational Processing of Uncertain Data,"  This paper introduces U-relations, a succinct and purely relational
representation system for uncertain databases. U-relations support
attribute-level uncertainty using vertical partitioning. If we consider
positive relational algebra extended by an operation for computing possible
answers, a query on the logical level can be translated into, and evaluated as,
a single relational algebra query on the U-relation representation. The
translation scheme essentially preserves the size of the query in terms of
number of operations and, in particular, number of joins. Standard techniques
employed in off-the-shelf relational database management systems are effective
for optimizing and processing queries on U-relations. In our experiments we
show that query evaluation on U-relations scales to large amounts of data with
high degrees of uncertainty.
"
15,Formally Verified Argument Reduction with a Fused-Multiply-Add,"  Cody & Waite argument reduction technique works perfectly for reasonably
large arguments but as the input grows there are no bit left to approximate the
constant with enough accuracy. Under mild assumptions, we show that the result
computed with a fused-multiply-add provides a fully accurate result for many
possible values of the input with a constant almost accurate to the full
working precision. We also present an algorithm for a fully accurate second
reduction step to reach double full accuracy (all the significand bits of two
numbers are significant) even in the worst cases of argument reduction. Our
work recalls the common algorithms and presents proofs of correctness. All the
proofs are formally verified using the Coq automatic proof checker.
"
16,"Computational performance of a parallelized high-order spectral and
  mortar element toolbox","  In this paper, a comprehensive performance review of a MPI-based high-order
spectral and mortar element method C++ toolbox is presented. The focus is put
on the performance evaluation of several aspects with a particular emphasis on
the parallel efficiency. The performance evaluation is analyzed and compared to
predictions given by a heuristic model, the so-called Gamma model. A
tailor-made CFD computation benchmark case is introduced and used to carry out
this review, stressing the particular interest for commodity clusters.
Conclusions are drawn from this extensive series of analyses and modeling
leading to specific recommendations concerning such toolbox development and
parallel implementation.
"
17,Wireless Local Area Networks with Multiple-Packet Reception Capability,"  Thanks to its simplicity and cost efficiency, wireless local area network
(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless
services in hot spots and indoor environments. Traditional WLAN
medium-access-control (MAC) protocols assume that only one station can transmit
at a time: simultaneous transmissions of more than one station causes the
destruction of all packets involved. By exploiting recent advances in PHY-layer
multiuser detection (MUD) techniques, it is possible for a receiver to receive
multiple packets simultaneously. This paper argues that such multipacket
reception (MPR) capability can greatly enhance the capacity of future WLANs. In
addition, it provides the MAC-layer and PHY-layer designs needed to achieve the
improved capacity. First, to demonstrate MUD/MPR as a powerful
capacity-enhancement technique, we prove a ""super-linearity"" result, which
states that the system throughput per unit cost increases as the MPR capability
increases. Second, we show that the commonly deployed binary exponential
backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR
system, and that the optimal backoff factor increases with the MPR capability:
the number of packets that can be received simultaneously. Third, based on the
above insights, we design a joint MAC-PHY layer protocol for an IEEE
802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD
techniques to implement MPR
"
18,Superrecursive Features of Interactive Computation,"  Functioning and interaction of distributed devices and concurrent algorithms
are analyzed in the context of the theory of algorithms. Our main concern here
is how and under what conditions algorithmic interactive devices can be more
powerful than the recursive models of computation, such as Turing machines.
Realization of such a higher computing power makes these systems
superrecursive. We find here five sources for superrecursiveness in
interaction. In addition, we prove that when all of these sources are excluded,
the algorithmic interactive system in question is able to perform only
recursive computations. These results provide computer scientists with
necessary and sufficient conditions for achieving superrecursiveness by
algorithmic interactive devices.
"
19,"Implementation, Compilation, Optimization of Object-Oriented Languages,
  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06","  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended
to bring researchers and practitioners both from academia and industry
together, with a spirit of openness, to try and identify and begin to address
the numerous and very varied issues of optimization. This succeeded, as can be
seen from the papers, the attendance and the liveliness of the discussions that
took place during and after the workshop, not to mention a few new cooperations
or postdoctoral contracts. The 22 talented people from different groups who
participated were unanimous to appreciate this first edition and recommend that
ICOOOLPS be continued next year. A community is thus beginning to form, and
should be reinforced by a second edition next year, with all the improvements
this first edition made emerge.
"
20,"Effects of Non-Identical Rayleigh Fading on Differential Unitary
  Space-Time Modulation","  This paper has been withdrawn by the author.
"
21,"Optimized Design of Survivable MPLS over Optical Transport Networks.
  Optical Switching and Networking","  In this paper we study different options for the survivability implementation
in MPLS over Optical Transport Networks in terms of network resource usage and
configuration cost. We investigate two approaches to the survivability
deployment: single layer and multilayer survivability and present various
methods for spare capacity allocation (SCA) to reroute disrupted traffic. The
comparative analysis shows the influence of the traffic granularity on the
survivability cost: for high bandwidth LSPs, close to the optical channel
capacity, the multilayer survivability outperforms the single layer one,
whereas for low bandwidth LSPs the single layer survivability is more
cost-efficient. For the multilayer survivability we demonstrate that by mapping
efficiently the spare capacity of the MPLS layer onto the resources of the
optical layer one can achieve up to 22% savings in the total configuration cost
and up to 37% in the optical layer cost. Further savings (up to 9 %) in the
wavelength use can be obtained with the integrated approach to network
configuration over the sequential one, however, at the increase in the
optimization problem complexity. These results are based on a cost model with
actual technology pricing and were obtained for networks targeted to a
nationwide coverage.
"
22,"On the Behavior of the Distributed Coordination Function of IEEE 802.11
  with Multirate Capability under General Transmission Conditions","  The aim of this paper is threefold. First, it presents a multi-dimensional
Markovian state transition model characterizing the behavior of the IEEE 802.11
protocol at the Medium Access Control layer which accounts for packet
transmission failures due to channel errors modeling both saturated and
non-saturated traffic conditions. Second, it provides a throughput analysis of
the IEEE 802.11 protocol at the data link layer in both saturated and
non-saturated traffic conditions taking into account the impact of both the
physical propagation channel and multirate transmission in Rayleigh fading
environment. The general traffic model assumed is M/M/1/K. Finally, it shows
that the behavior of the throughput in non-saturated traffic conditions is a
linear combination of two system parameters; the payload size and the packet
rates, $\lambda^{(s)}$, of each contending station. The validity interval of
the proposed model is also derived.
  Simulation results closely match the theoretical derivations, confirming the
effectiveness of the proposed models.
"
23,"Survivable MPLS Over Optical Transport Networks: Cost and Resource Usage
  Analysis","  In this paper we study different options for the survivability implementation
in MPLS over Optical Transport Networks (OTN) in terms of network resource
usage and configuration cost. We investigate two approaches to the
survivability deployment: single layer and multilayer survivability and present
various methods for spare capacity allocation (SCA) to reroute disrupted
traffic. The comparative analysis shows the influence of the offered traffic
granularity and the physical network structure on the survivability cost: for
high bandwidth LSPs, close to the optical channel capacity, the multilayer
survivability outperforms the single layer one, whereas for low bandwidth LSPs
the single layer survivability is more cost-efficient. On the other hand,
sparse networks of low connectivity parameter use more wavelengths for optical
path routing and increase the configuration cost, as compared with dense
networks. We demonstrate that by mapping efficiently the spare capacity of the
MPLS layer onto the resources of the optical layer one can achieve up to 22%
savings in the total configuration cost and up to 37% in the optical layer
cost. Further savings (up to 9 %) in the wavelength use can be obtained with
the integrated approach to network configuration over the sequential one,
however, at the increase in the optimization problem complexity. These results
are based on a cost model with different cost variations, and were obtained for
networks targeted to a nationwide coverage.
"
24,Non-linear estimation is easy,"  Non-linear state estimation and some related topics, like parametric
estimation, fault diagnosis, and perturbation attenuation, are tackled here via
a new methodology in numerical differentiation. The corresponding basic system
theoretic definitions and properties are presented within the framework of
differential algebra, which permits to handle system variables and their
derivatives of any order. Several academic examples and their computer
simulations, with on-line estimations, are illustrating our viewpoint.
"
25,"Nano-Sim: A Step Wise Equivalent Conductance based Statistical Simulator
  for Nanotechnology Circuit Design","  New nanotechnology based devices are replacing CMOS devices to overcome CMOS
technology's scaling limitations. However, many such devices exhibit
non-monotonic I-V characteristics and uncertain properties which lead to the
negative differential resistance (NDR) problem and the chaotic performance.
This paper proposes a new circuit simulation approach that can effectively
simulate nanotechnology devices with uncertain input sources and negative
differential resistance (NDR) problem. The experimental results show a 20-30
times speedup comparing with existing simulators.
"
26,"Generic Pipelined Processor Modeling and High Performance Cycle-Accurate
  Simulator Generation","  Detailed modeling of processors and high performance cycle-accurate
simulators are essential for today's hardware and software design. These
problems are challenging enough by themselves and have seen many previous
research efforts. Addressing both simultaneously is even more challenging, with
many existing approaches focusing on one over another. In this paper, we
propose the Reduced Colored Petri Net (RCPN) model that has two advantages:
first, it offers a very simple and intuitive way of modeling pipelined
processors; second, it can generate high performance cycle-accurate simulators.
RCPN benefits from all the useful features of Colored Petri Nets without
suffering from their exponential growth in complexity. RCPN processor models
are very intuitive since they are a mirror image of the processor pipeline
block diagram. Furthermore, in our experiments on the generated cycle-accurate
simulators for XScale and StrongArm processor models, we achieved an order of
magnitude (~15 times) speedup over the popular SimpleScalar ARM simulator.
"
27,"A Prediction Packetizing Scheme for Reducing Channel Traffic in
  Transaction-Level Hardware/Software Co-Emulation","  This paper presents a scheme for efficient channel usage between simulator
and accelerator where the accelerator models some RTL sub-blocks in the
accelerator-based hardware/software co-simulation while the simulator runs
transaction-level model of the remaining part of the whole chip being verified.
With conventional simulation accelerator, evaluations of simulator and
accelerator alternate at every valid simulation time, which results in poor
simulation performance due to startup overhead of simulator-accelerator channel
access. The startup overhead can be reduced by merging multiple transactions on
the channel into a single burst traffic. We propose a predictive packetizing
scheme for reducing channel traffic by merging as many transactions into a
burst traffic as possible based on 'prediction and rollback.' Under ideal
condition with 100% prediction accuracy, the proposed method shows a
performance gain of 1500% compared to the conventional one.
"
28,"Simulation Methodology for Analysis of Substrate Noise Impact on Analog
  / RF Circuits Including Interconnect Resistance","  This paper reports a novel simulation methodology for analysis and prediction
of substrate noise impact on analog / RF circuits taking into account the role
of the parasitic resistance of the on-chip interconnect in the impact
mechanism. This methodology allows investigation of the role of the separate
devices (also parasitic devices) in the analog / RF circuit in the overall
impact. This way is revealed which devices have to be taken care of (shielding,
topology change) to protect the circuit against substrate noise. The developed
methodology is used to analyze impact of substrate noise on a 3 GHz LC-tank
Voltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\mu$m 1PM6
CMOS technology. For this VCO (in the investigated frequency range from DC to
15 MHz) impact is mainly caused by resistive coupling of noise from the
substrate to the non-ideal on-chip ground interconnect, resulting in analog
ground bounce and frequency modulation. Hence, the presented test-case reveals
the important role of the on-chip interconnect in the phenomenon of substrate
noise impact.
"
29,The Fast Fibonacci Decompression Algorithm,"  Data compression has been widely applied in many data processing areas.
Compression methods use variable-size codes with the shorter codes assigned to
symbols or groups of symbols that appear in the data frequently. Fibonacci
coding, as a representative of these codes, is used for compressing small
numbers. Time consumption of a decompression algorithm is not usually as
important as the time of a compression algorithm. However, efficiency of the
decompression may be a critical issue in some cases. For example, a real-time
compression of tree data structures follows this issue. Tree's pages are
decompressed during every reading from a secondary storage into the main
memory. In this case, the efficiency of a decompression algorithm is extremely
important. We have developed a Fast Fibonacci decompression for this purpose.
Our approach is up to $3.5\times$ faster than the original implementation.
"
30,"Back-of-the-Envelope Computation of Throughput Distributions in CSMA
  Wireless Networks","  This work started out with our accidental discovery of a pattern of
throughput distributions among links in IEEE 802.11 networks from experimental
results. This pattern gives rise to an easy computation method, which we term
back-of-the-envelop (BoE) computation, because for many network configurations,
very accurate results can be obtained within minutes, if not seconds, by simple
hand computation. BoE beats prior methods in terms of both speed and accuracy.
While the computation procedure of BoE is simple, explaining why it works is by
no means trivial. Indeed the majority of our investigative efforts have been
devoted to the construction of a theory to explain BoE. This paper models an
ideal CSMA network as a set of interacting on-off telegraph processes. In
developing the theory, we discovered a number of analytical techniques and
observations that have eluded prior research, such as that the carrier-sensing
interactions among links in an ideal CSMA network result in a system state
evolution that is time-reversible; and that the probability distribution of the
system state is insensitive to the distributions of the ""on"" and ""off""
durations given their means, and is a Markov random field. We believe these
theoretical frameworks are useful not just for explaining BoE, but could also
be a foundation for a fundamental understanding of how links in CSMA networks
interact. Last but not least, because of their basic nature, we surmise that
some of the techniques and results developed in this paper may be applicable to
not just CSMA networks, but also to other physical and engineering systems
consisting of entities interacting with each other in time and space.
"
31,"Data access optimizations for highly threaded multi-core CPUs with
  multiple memory controllers","  Processor and system architectures that feature multiple memory controllers
are prone to show bottlenecks and erratic performance numbers on codes with
regular access patterns. Although such effects are well known in the form of
cache thrashing and aliasing conflicts, they become more severe when memory
access is involved. Using the new Sun UltraSPARC T2 processor as a prototypical
multi-core design, we analyze performance patterns in low-level and application
benchmarks and show ways to circumvent bottlenecks by careful data layout and
padding.
"
32,"Middleware-based Database Replication: The Gaps between Theory and
  Practice","  The need for high availability and performance in data management systems has
been fueling a long running interest in database replication from both academia
and industry. However, academic groups often attack replication problems in
isolation, overlooking the need for completeness in their solutions, while
commercial teams take a holistic approach that often misses opportunities for
fundamental innovation. This has created over time a gap between academic
research and industrial practice.
  This paper aims to characterize the gap along three axes: performance,
availability, and administration. We build on our own experience developing and
deploying replication systems in commercial and academic settings, as well as
on a large body of prior related work. We sift through representative examples
from the last decade of open-source, academic, and commercial database
replication systems and combine this material with case studies from real
systems deployed at Fortune 500 customers. We propose two agendas, one for
academic research and one for industrial R&D, which we believe can bridge the
gap within 5-10 years. This way, we hope to both motivate and help researchers
in making the theory and practice of middleware-based database replication more
relevant to each other.
"
33,"RZBENCH: Performance evaluation of current HPC architectures using
  low-level and application benchmarks","  RZBENCH is a benchmark suite that was specifically developed to reflect the
requirements of scientific supercomputer users at the University of
Erlangen-Nuremberg (FAU). It comprises a number of application and low-level
codes under a common build infrastructure that fosters maintainability and
expandability. This paper reviews the structure of the suite and briefly
introduces the most relevant benchmarks. In addition, some widely known
standard benchmark codes are reviewed in order to emphasize the need for a
critical review of often-cited performance results. Benchmark data is presented
for the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well
as two uncommon system architectures: A bandwidth-optimized InfiniBand cluster
based on single socket nodes (""Port Townsend"") and an early version of Sun's
highly threaded T2 architecture (""Niagara 2"").
"
34,Substitute Valuations: Generation and Structure,"  Substitute valuations (in some contexts called gross substitute valuations)
are prominent in combinatorial auction theory. An algorithm is given in this
paper for generating a substitute valuation through Monte Carlo simulation. In
addition, the geometry of the set of all substitute valuations for a fixed
number of goods K is investigated. The set consists of a union of polyhedrons,
and the maximal polyhedrons are identified for K=4. It is shown that the
maximum dimension of the maximal polyhedrons increases with K nearly as fast as
two to the power K. Consequently, under broad conditions, if a combinatorial
algorithm can present an arbitrary substitute valuation given a list of input
numbers, the list must grow nearly as fast as two to the power K.
"
35,A System Theoretic Approach to Bandwidth Estimation,"  It is shown that bandwidth estimation in packet networks can be viewed in
terms of min-plus linear system theory. The available bandwidth of a link or
complete path is expressed in terms of a {\em service curve}, which is a
function that appears in the network calculus to express the service available
to a traffic flow. The service curve is estimated based on measurements of a
sequence of probing packets or passive measurements of a sample path of
arrivals. It is shown that existing bandwidth estimation methods can be derived
in the min-plus algebra of the network calculus, thus providing further
mathematical justification for these methods. Principal difficulties of
estimating available bandwidth from measurement of network probes are related
to potential non-linearities of the underlying network. When networks are
viewed as systems that operate either in a linear or in a non-linear regime, it
is argued that probing schemes extract the most information at a point when the
network crosses from a linear to a non-linear regime. Experiments on the Emulab
testbed at the University of Utah evaluate the robustness of the system
theoretic interpretation of networks in practice. Multi-node experiments
evaluate how well the convolution operation of the min-plus algebra provides
estimates for the available bandwidth of a path from estimates of individual
links.
"
36,"Understanding the Paradoxical Effects of Power Control on the Capacity
  of Wireless Networks","  Recent works show conflicting results: network capacity may increase or
decrease with higher transmission power under different scenarios. In this
work, we want to understand this paradox. Specifically, we address the
following questions: (1)Theoretically, should we increase or decrease
transmission power to maximize network capacity? (2) Theoretically, how much
network capacity gain can we achieve by power control? (3) Under realistic
situations, how do power control, link scheduling and routing interact with
each other? Under which scenarios can we expect a large capacity gain by using
higher transmission power? To answer these questions, firstly, we prove that
the optimal network capacity is a non-decreasing function of transmission
power. Secondly, we prove that the optimal network capacity can be increased
unlimitedly by higher transmission power in some network configurations.
However, when nodes are distributed uniformly, the gain of optimal network
capacity by higher transmission power is upper-bounded by a positive constant.
Thirdly, we discuss why network capacity in practice may increase or decrease
with higher transmission power under different scenarios using carrier sensing
and the minimum hop-count routing. Extensive simulations are carried out to
verify our analysis.
"
37,Snap-Stabilization in Message-Passing Systems,"  In this paper, we tackle the open problem of snap-stabilization in
message-passing systems. Snap-stabilization is a nice approach to design
protocols that withstand transient faults. Compared to the well-known
self-stabilizing approach, snap-stabilization guarantees that the effect of
faults is contained immediately after faults cease to occur. Our contribution
is twofold: we show that (1) snap-stabilization is impossible for a wide class
of problems if we consider networks with finite yet unbounded channel capacity;
(2) snap-stabilization becomes possible in the same setting if we assume
bounded-capacity channels. We propose three snap-stabilizing protocols working
in fully-connected networks. Our work opens exciting new research perspectives,
as it enables the snap-stabilizing paradigm to be implemented in actual
networks.
"
38,Note sur les temps de service r\'esiduels dans les syst\`emes type M/G/c,"  Approximations for the mean performance indices for the M/G/c queue rely on
the approximate computation of the probability that an arriving request has to
wait for service and of the minimum of residual service times if all servers
are found busy. Using numerical examples, we investigate properties of these
two quantities. In particular, we show that the minimum of residual service
times depends on higher order properties, beyond the first two moments, of the
service time distribution. Improved knowledge of the properties of the two
quantities studied in this paper provides insight into avenues for improving
the accuracy of approximations for the M/G/c queue.
"
39,Self-* overload control for distributed web systems,"  Unexpected increases in demand and most of all flash crowds are considered
the bane of every web application as they may cause intolerable delays or even
service unavailability. Proper quality of service policies must guarantee rapid
reactivity and responsiveness even in such critical situations. Previous
solutions fail to meet common performance requirements when the system has to
face sudden and unpredictable surges of traffic. Indeed they often rely on a
proper setting of key parameters which requires laborious manual tuning,
preventing a fast adaptation of the control policies. We contribute an original
Self-* Overload Control (SOC) policy. This allows the system to self-configure
a dynamic constraint on the rate of admitted sessions in order to respect
service level agreements and maximize the resource utilization at the same
time. Our policy does not require any prior information on the incoming traffic
or manual configuration of key parameters. We ran extensive simulations under a
wide range of operating conditions, showing that SOC rapidly adapts to time
varying traffic and self-optimizes the resource utilization. It admits as many
new sessions as possible in observance of the agreements, even under intense
workload variations. We compared our algorithm to previously proposed
approaches highlighting a more stable behavior and a better performance.
"
40,Sketch-Based Estimation of Subpopulation-Weight,"  Summaries of massive data sets support approximate query processing over the
original data. A basic aggregate over a set of records is the weight of
subpopulations specified as a predicate over records' attributes. Bottom-k
sketches are a powerful summarization format of weighted items that includes
priority sampling and the classic weighted sampling without replacement. They
can be computed efficiently for many representations of the data including
distributed databases and data streams.
  We derive novel unbiased estimators and efficient confidence bounds for
subpopulation weight. Our estimators and bounds are tailored by distinguishing
between applications (such as data streams) where the total weight of the
sketched set can be computed by the summarization algorithm without a
significant use of additional resources, and applications (such as sketches of
network neighborhoods) where this is not the case.
  Our rigorous derivations are based on clever applications of the
Horvitz-Thompson estimator, and are complemented by efficient computational
methods. We demonstrate their benefit on a wide range of Pareto distributions.
"
41,Data Traffic Dynamics and Saturation on a Single Link,"  The dynamics of User Datagram Protocol (UDP) traffic over Ethernet between
two computers are analyzed using nonlinear dynamics which shows that there are
two clear regimes in the data flow: free flow and saturated. The two most
important variables affecting this are the packet size and packet flow rate.
However, this transition is due to a transcritical bifurcation rather than
phase transition in models such as in vehicle traffic or theorized large-scale
computer network congestion. It is hoped this model will help lay the
groundwork for further research on the dynamics of networks, especially
computer networks.
"
42,Quiescence of Self-stabilizing Gossiping among Mobile Agents in Graphs,"  This paper considers gossiping among mobile agents in graphs: agents move on
the graph and have to disseminate their initial information to every other
agent. We focus on self-stabilizing solutions for the gossip problem, where
agents may start from arbitrary locations in arbitrary states.
Self-stabilization requires (some of the) participating agents to keep moving
forever, hinting at maximizing the number of agents that could be allowed to
stop moving eventually. This paper formalizes the self-stabilizing agent gossip
problem, introduces the quiescence number (i.e., the maximum number of
eventually stopping agents) of self-stabilizing solutions and investigates the
quiescence number with respect to several assumptions related to agent
anonymity, synchrony, link duplex capacity, and whiteboard capacity.
"
43,"Estimation of available bandwidth and measurement infrastructure for
  Russian segment of Internet","  In paper the method for estimation of available bandwidth is supposed which
does not demand the advanced utilities. Our method is based on the measurement
of network delay $D$ for packets of different sizes $W$. The simple expression
for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is substantiated. For the
experimental testing the measurement infrastructure for Russian segment of
Internet was installed in framework of RFBR grant 06-07-89074.
"
44,SAFIUS - A secure and accountable filesystem over untrusted storage,"  We describe SAFIUS, a secure accountable file system that resides over an
untrusted storage. SAFIUS provides strong security guarantees like
confidentiality, integrity, prevention from rollback attacks, and
accountability. SAFIUS also enables read/write sharing of data and provides the
standard UNIX-like interface for applications. To achieve accountability with
good performance, it uses asynchronous signatures; to reduce the space required
for storing these signatures, a novel signature pruning mechanism is used.
SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.
Preliminary performance studies show that SAFIUS has a tolerable overhead for
providing secure storage: while it has an overhead of about 50% of OpenGFS in
data intensive workloads (due to the overhead of performing
encryption/decryption in software), it is comparable (or better in some cases)
to OpenGFS in metadata intensive workloads.
"
45,Performance Evaluation of Multiple TCP connections in iSCSI,"  Scaling data storage is a significant concern in enterprise systems and
Storage Area Networks (SANs) are deployed as a means to scale enterprise
storage. SANs based on Fibre Channel have been used extensively in the last
decade while iSCSI is fast becoming a serious contender due to its reduced
costs and unified infrastructure. This work examines the performance of iSCSI
with multiple TCP connections. Multiple TCP connections are often used to
realize higher bandwidth but there may be no fairness in how bandwidth is
distributed. We propose a mechanism to share congestion information across
multiple flows in ``Fair-TCP'' for improved performance. Our results show that
Fair-TCP significantly improves the performance for I/O intensive workloads.
"
46,Link Enhancer for Vehicular Wireless ATM Communications,"  Majority of the applications used in defense are voice, video and data
oriented and has strict QoS requirements. One of the technologies that enabled
this is Asynchronous Transfer Mode (ATM) networking. Traditional ATM networks
are wired networks. But Tactical networks are meant to be mobile and this
necessitates the use of radio relays for Vehicle-to-Infrastructure (V2I) and
Vehicle-to-Vehicle (V2V) communications. ATM networks assume a physical link
layer BER of 10^-9 or better because of the availability of reliable media like
optical fiber links. But this assumption is no longer valid when ATM switches
are connected through radio relay where error rates are in the rage of 10^-3.
This paper presents the architecture of a Link Enhancer meant to improve the
Bit Error Rate of the Wireless links used for V2I and V2V communications from 1
in 10^4 to 1 in 10^8
"
47,"Energy and Time Efficient Scheduling of Tasks with Dependencies on
  Asymmetric Multiprocessors","  In this work we study the problem of scheduling tasks with dependencies in
multiprocessor architectures where processors have different speeds. We present
the preemptive algorithm ""Save-Energy"" that given a schedule of tasks it post
processes it to improve the energy efficiency without any deterioration of the
makespan. In terms of time efficiency, we show that preemptive scheduling in an
asymmetric system can achieve the same or better optimal makespan than in a
symmetric system. Motivited by real multiprocessor systems, we investigate
architectures that exhibit limited asymmetry: there are two essentially
different speeds. Interestingly, this special case has not been studied in the
field of parallel computing and scheduling theory; only the general case was
studied where processors have $K$ essentially different speeds. We present the
non-preemptive algorithm ``Remnants'' that achieves almost optimal makespan. We
provide a refined analysis of a recent scheduling method. Based on this
analysis, we specialize the scheduling policy and provide an algorithm of $(3 +
o(1))$ expected approximation factor. Note that this improves the previous best
factor (6 for two speeds). We believe that our work will convince researchers
to revisit this well studied scheduling problem for these simple, yet
realistic, asymmetric multiprocessor architectures.
"
48,Heavy-Tailed Limits for Medium Size Jobs and Comparison Scheduling,"  We study the conditional sojourn time distributions of processor sharing
(PS), foreground background processor sharing (FBPS) and shortest remaining
processing time first (SRPT) scheduling disciplines on an event where the job
size of a customer arriving in stationarity is smaller than exactly k>=0 out of
the preceding m>=k arrivals. Then, conditioning on the preceding event, the
sojourn time distribution of this newly arriving customer behaves
asymptotically the same as if the customer were served in isolation with a
server of rate (1-\rho)/(k+1) for PS/FBPS, and (1-\rho) for SRPT, respectively,
where \rho is the traffic intensity. Hence, the introduced notion of
conditional limits allows us to distinguish the asymptotic performance of the
studied schedulers by showing that SRPT exhibits considerably better asymptotic
behavior for relatively smaller jobs than PS/FBPS.
  Inspired by the preceding results, we propose an approximation to the SRPT
discipline based on a novel adaptive job grouping mechanism that uses relative
size comparison of a newly arriving job to the preceding m arrivals.
Specifically, if the newly arriving job is smaller than k and larger than m-k
of the previous m jobs, it is routed into class k. Then, the classes of smaller
jobs are served with higher priorities using the static priority scheduling.
The good performance of this mechanism, even for a small number of classes m+1,
is demonstrated using the asymptotic queueing analysis under the heavy-tailed
job requirements. We also discuss refinements of the comparison grouping
mechanism that improve the accuracy of job classification at the expense of a
small additional complexity.
"
49,Performability Aspects of the Atlas Vo; Using Lmbench Suite,"  The ATLAS Virtual Organization is grid's largest Virtual Organization which
is currently in full production stage. Hereby a case is being made that a user
working within that VO is going to face a wide spectrum of different systems,
whose heterogeneity is enough to count as ""orders of magnitude"" according to a
number of metrics; including integer/float operations, memory throughput
(STREAM) and communication latencies. Furthermore, the spread of performance
does not appear to follow any known distribution pattern, which is demonstrated
in graphs produced during May 2007 measurements. It is implied that the current
practice where either ""all-WNs-are-equal"" or, the alternative of SPEC-based
rating used by LCG/EGEE is an oversimplification which is inappropriate and
expensive from an operational point of view, therefore new techniques are
needed for optimal grid resources allocation.
"
50,"SPARK00: A Benchmark Package for the Compiler Evaluation of
  Irregular/Sparse Codes","  We propose a set of benchmarks that specifically targets a major cause of
performance degradation in high performance computing platforms: irregular
access patterns. These benchmarks are meant to be used to asses the performance
of optimizing compilers on codes with a varying degree of irregular access. The
irregularity caused by the use of pointers and indirection arrays are a major
challenge for optimizing compilers. Codes containing such patterns are
notoriously hard to optimize but they have a huge impact on the performance of
modern architectures, which are under-utilized when encountering irregular
memory accesses. In this paper, a set of benchmarks is described that
explicitly measures the performance of kernels containing a variety of
different access patterns found in real world applications. By offering a
varying degree of complexity, we provide a platform for measuring the
effectiveness of transformations. The difference in complexity stems from a
difference in traversal patterns, the use of multiple indirections and control
flow statements. The kernels used cover a variety of different access patterns,
namely pointer traversals, indirection arrays, dynamic loop bounds and run-time
dependent if-conditions. The kernels are small enough to be fully understood
which makes this benchmark set very suitable for the evaluation of
restructuring transformations.
"
51,Significant Diagnostic Counterexamples in Probabilistic Model Checking,"  This paper presents a novel technique for counterexample generation in
probabilistic model checking of Markov Chains and Markov Decision Processes.
(Finite) paths in counterexamples are grouped together in witnesses that are
likely to provide similar debugging information to the user. We list five
properties that witnesses should satisfy in order to be useful as debugging
aid: similarity, accuracy, originality, significance, and finiteness. Our
witnesses contain paths that behave similar outside strongly connected
components.
  This papers shows how to compute these witnesses by reducing the problem of
generating counterexamples for general properties over Markov Decision
Processes, in several steps, to the easy problem of generating counterexamples
for reachability properties over acyclic Markov Chains.
"
52,SP2Bench: A SPARQL Performance Benchmark,"  Recently, the SPARQL query language for RDF has reached the W3C
recommendation status. In response to this emerging standard, the database
community is currently exploring efficient storage techniques for RDF data and
evaluation strategies for SPARQL queries. A meaningful analysis and comparison
of these approaches necessitates a comprehensive and universal benchmark
platform. To this end, we have developed SP^2Bench, a publicly available,
language-specific SPARQL performance benchmark. SP^2Bench is settled in the
DBLP scenario and comprises both a data generator for creating arbitrarily
large DBLP-like documents and a set of carefully designed benchmark queries.
The generated documents mirror key characteristics and social-world
distributions encountered in the original DBLP data set, while the queries
implement meaningful requests on top of this data, covering a variety of SPARQL
operator constellations and RDF access patterns. As a proof of concept, we
apply SP^2Bench to existing engines and discuss their strengths and weaknesses
that follow immediately from the benchmark results.
"
53,"Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive
  Networks","  This paper deals with asymptotic expressions of the Mean Time To Failure
(MTTF) and higher moments for large, recursive, and non-repairable systems in
the context of two-terminal reliability. Our aim is to extend the well-known
results of the series and parallel cases. We first consider several exactly
solvable configurations of identical components with exponential failure-time
distribution functions to illustrate different (logarithmic or power-law)
behaviors as the size of the system, indexed by an integer n, increases. The
general case is then addressed: it provides a simple interpretation of the
origin of the power-law exponent and an efficient asymptotic expression for the
total reliability of large, recursive systems. Finally, we assess the influence
of the non-exponential character of the component reliability on the
n-dependence of the MTTF.
"
54,Exact two-terminal reliability of some directed networks,"  The calculation of network reliability in a probabilistic context has long
been an issue of practical and academic importance. Conventional approaches
(determination of bounds, sums of disjoint products algorithms, Monte Carlo
evaluations, studies of the reliability polynomials, etc.) only provide
approximations when the network's size increases, even when nodes do not fail
and all edges have the same reliability p. We consider here a directed, generic
graph of arbitrary size mimicking real-life long-haul communication networks,
and give the exact, analytical solution for the two-terminal reliability. This
solution involves a product of transfer matrices, in which individual
reliabilities of edges and nodes are taken into account. The special case of
identical edge and node reliabilities (p and rho, respectively) is addressed.
We consider a case study based on a commonly-used configuration, and assess the
influence of the edges being directed (or not) on various measures of network
performance. While the two-terminal reliability, the failure frequency and the
failure rate of the connection are quite similar, the locations of complex
zeros of the two-terminal reliability polynomials exhibit strong differences,
and various structure transitions at specific values of rho. The present work
could be extended to provide a catalog of exactly solvable networks in terms of
reliability, which could be useful as building blocks for new and improved
bounds, as well as benchmarks, in the general case.
"
55,WCET analysis of multi-level set-associative instruction caches,"  With the advent of increasingly complex hardware in real-time embedded
systems (processors with performance enhancing features such as pipelines,
cache hierarchy, multiple cores), many processors now have a set-associative L2
cache. Thus, there is a need for considering cache hierarchies when validating
the temporal behavior of real-time systems, in particular when estimating
tasks' worst-case execution times (WCETs). To the best of our knowledge, there
is only one approach for WCET estimation for systems with cache hierarchies
[Mueller, 1997], which turns out to be unsafe for set-associative caches. In
this paper, we highlight the conditions under which the approach described in
[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is
then presented. Contrary to [Mueller, 1997] our method supports set-associative
and fully associative caches. The proposed method is experimented on
medium-size and large programs. We show that the method is most of the time
tight. We further show that in all cases WCET estimations are much tighter when
considering the cache hierarchy than when considering only the L1 cache. An
evaluation of the analysis time is conducted, demonstrating that analysing the
cache hierarchy has a reasonable computation time.
"
56,"Session Initiation Protocol (SIP) Server Overload Control: Design and
  Evaluation","  A Session Initiation Protocol (SIP) server may be overloaded by
emergency-induced call volume, ``American Idol'' style flash crowd effects or
denial of service attacks. The SIP server overload problem is interesting
especially because the costs of serving or rejecting a SIP session can be
similar. For this reason, the built-in SIP overload control mechanism based on
generating rejection messages cannot prevent the server from entering
congestion collapse under heavy load. The SIP overload problem calls for a
pushback control solution in which the potentially overloaded receiving server
may notify its upstream sending servers to have them send only the amount of
load within the receiving server's processing capacity. The pushback framework
can be achieved by either a rate-based feedback or a window-based feedback. The
centerpiece of the feedback mechanism is the algorithm used to generate load
regulation information. We propose three new window-based feedback algorithms
and evaluate them together with two existing rate-based feedback algorithms. We
compare the different algorithms in terms of the number of tuning parameters
and performance under both steady and variable load. Furthermore, we identify
two categories of fairness requirements for SIP overload control, namely,
user-centric and provider-centric fairness. With the introduction of a new
double-feed SIP overload control architecture, we show how the algorithms can
meet those fairness criteria.
"
57,Measurement and Evaluation of ENUM Server Performance,"  ENUM is a DNS-based protocol standard for mapping E.164 telephone numbers to
Internet Uniform Resource Identifiers (URIs). It places unique requirements on
the existing DNS infrastructure, such as data scalability, query throughput,
response time, and database update rates. This paper measures and evaluates the
performance of existing name server implementation as ENUM servers. We compared
PowerDNS (PDNS), BIND and Navitas. Results show that BIND is not suitable for
ENUM due to its poor scaling property. Both PDNS and Navitas can serve ENUM.
However, Navitas turns out to be highly optimized and clearly outperforms PDNS
in all aspects we have tested. We also instrumented the PDNS server to identify
its performance bottleneck and investigated ways to improve it.
"
58,"Restricted Mobility Improves Delay-Throughput Trade-offs in Mobile
  Ad-Hoc Networks","  In this paper, we analyze asymptotic delay-throughput trade-offs in mobile
ad-hoc networks comprising heterogeneous nodes with restricted mobility. We
show that node spatial heterogeneity has the ability to drastically improve
upon existing scaling laws established under the assumption that nodes are
identical and uniformly visit the entire network area. In particular, we
consider the situation in which each node moves around its own home-point
according to a restricted mobility process which results into a spatial
stationary distribution that decays as a power law of exponent delta with the
distance from the home-point. For such restricted mobility model, we propose a
novel class of scheduling and routing schemes, which significantly outperforms
all delay-throughput results previously obtained in the case of identical
nodes. In particular, for delta = 2 it is possible to achieve almost constant
delay and almost constant per-node throughput (except for a poly-logarithmic
factor) as the number of nodes increases, even without resorting to
sophisticated coding or signal processing techniques.
"
59,"Optimization of Location Management for PCS Networks with CTRW Mobility
  Model","  This paper considers the design of the optimal locationupdate area (LA) of
the distance-based scheme for personal communication service (PCS) networks. We
focus on the optimization of two design parameters associated with the LA: 1)
initial position upon LA update; 2) distance threshold for triggering of LA
update. Based on the popular continuous-time random walk (CTRW) mobility model,
we propose a novel analytical framework that uses a diffusion equation to
minimize the location management cost. In this framework, a number of
measurable physical parameters, such as length of road section, angle between
road sections, and road section crossing time, can be integrated into the
system design. This framework allows us to easily evaluate the total cost under
general call arrival distributions and LA of different shapes. For the
particular case of circular LA and small Poisson call-arrival rate, we prove
the following: (1) When the drift is weak, the optimal initial position
approaches the center of the LA; when the drift is strong, it approaches the
boundary of the LA. (2) Comparing the optimal initial-position and
center-initial-position solutions (which is assumed in most prior work), when
the drift is weak, the optimal distance threshold and the minimum total cost
are roughly equal; when the drift is strong, the optimal distance threshold in
the later is about 1.260 times that in the former, and the minimum total cost
in the later is about 1.587 times that in the former. That is, optimizing on
initial position, which previous work did not consider, has the potential of
reducing the cost measure by 37%.
"
60,"A General Theory of Computational Scalability Based on Rational
  Functions","  The universal scalability law of computational capacity is a rational
function C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree
polynomial in the number of physical processors p, that has been long used for
statistical modeling and prediction of computer system performance. We prove
that C_p is equivalent to the synchronous throughput bound for a
machine-repairman with state-dependent service rate. Simpler rational
functions, such as Amdahl's law and Gustafson speedup, are corollaries of this
queue-theoretic bound. C_p is further shown to be both necessary and sufficient
for modeling all practical characteristics of computational scalability.
"
61,Optimizing Compiler for Engineering Problems,"  New information technologies provide a lot of prospects for performance
improvement. One of them is ""Dynamic Source Code Generation and Compilation"".
This article shows how this way provides high performance for engineering
problems.
"
62,Forward Correction and Fountain codes in Delay Tolerant Networks,"  Delay tolerant Ad-hoc Networks make use of mobility of relay nodes to
compensate for lack of permanent connectivity and thus enable communication
between nodes that are out of range of each other. To decrease delivery delay,
the information that needs to be delivered is replicated in the network. Our
objective in this paper is to study replication mechanisms that include coding
in order to improve the probability of successful delivery within a given time
limit. We propose an analytical approach that allows to quantify tradeoffs
between resources and performance measures (energy and delay). We study the
effect of coding on the performance of the network while optimizing parameters
that govern routing. Our results, based on fluid approximations, are compared
to simulations which validate the model
"
63,"Understanding Fairness and its Impact on Quality of Service in IEEE
  802.11","  The Distributed Coordination Function (DCF) aims at fair and efficient medium
access in IEEE 802.11. In face of its success, it is remarkable that there is
little consensus on the actual degree of fairness achieved, particularly
bearing its impact on quality of service in mind. In this paper we provide an
accurate model for the fairness of the DCF. Given M greedy stations we assume
fairness if a tagged station contributes a share of 1/M to the overall number
of packets transmitted. We derive the probability distribution of fairness
deviations and support our analytical results by an extensive set of
measurements. We find a closed-form expression for the improvement of long-term
over short-term fairness. Regarding the random countdown values we quantify the
significance of their distribution whereas we discover that fairness is largely
insensitive to the distribution parameters. Based on our findings we view the
DCF as emulating an ideal fair queuing system to quantify the deviations from a
fair rate allocation. We deduce a stochastic service curve model for the DCF to
predict packet delays in IEEE 802.11. We show how a station can estimate its
fair bandwidth share from passive measurements of its traffic arrivals and
departures.
"
64,Amdahl's and Gustafson-Barsis laws revisited,"  The paper presents a simple derivation of the Gustafson-Barsis law from the
Amdahl's law. In the computer literature these two laws describing the speedup
limits of parallel applications are derived separately. It is shown, that
treating the time of the execution of the sequential part of the application as
a constant, in few lines the Gustafson-Barsis law can be obtained from the
Amdahl's law and that the popular claim, that Gustafson-Barsis law overthrows
Amdahl's law is a mistake.
"
65,Multidimensional Visualization of Oracle Performance Using Barry007,"  Most generic performance tools display only system-level performance data
using 2-dimensional plots or diagrams and this limits the informational detail
that can be displayed. Moreover, a modern relational database system, like
Oracle, can concurrently serve thousands of client processes with different
workload characteristics, so that generic performance-data displays inevitably
hide important information. Drawing on our previous work, this paper
demonstrates the application of Barry007 multidimensional visualization to the
analysis of Oracle end-user, session-level, performance data, showing both
collective trends and individual performance anomalies.
"
66,Getting in the Zone for Successful Scalability,"  The universal scalability law (USL) is an analytic model used to quantify
application scaling. It is universal because it subsumes Amdahl's law and
Gustafson linearized scaling as special cases. Using simulation, we show: (i)
that the USL is equivalent to synchronous queueing in a load-dependent machine
repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be
regarded as boundaries defining three scalability zones. Typical throughput
measurements lie across all three zones. Simulation scenarios provide deeper
insight into queueing effects and thus provide a clearer indication of which
application features should be tuned to get into the optimal performance zone.
"
67,"Occupancy distributions of homogeneous queueing systems under
  opportunistic scheduling","  We analyze opportunistic schemes for transmission scheduling from one of $n$
homogeneous queues whose channel states fluctuate independently. Considered
schemes consist of the LCQ policy, which transmits from a longest connected
queue in the entire system, and its low-complexity variants that transmit from
a longest queue within a randomly chosen subset of connected queues. A
Markovian model is studied where mean packet transmission time is $n^{-1}$ and
packet arrival rate is $\lambda<1$ per queue. Transient and equilibrium
distributions of queue occupancies are obtained in the limit as the system size
$n$ tends to infinity.
"
68,Mobility Management Framework,"  This paper investigates mobility management strategies from the point of view
of their need of signalling and processing resources on the backbone network
and load on the air interface. A method is proposed to model the serving
network and mobile node mobility in order to be able to compare the different
types of mobility management algorithms. To obtain a good description of the
network we calculate descriptive parameters from given topologies. Most
mobility approaches derived from existing protocols are analyzed and their
performances are numerically compared in various network and mobility
scenarios. We developed a mobility management framework that is able to give
general designing guidelines for the next generation mobility managements on
given network, technology and mobility properties. With our model an operator
can design the network and tune the parameters to obtain the optimal
implementation of course revising existing systems is also possible. We present
a vertical handover decision method as a special application of our model
framework.
"
69,"An Analytical Model of Information Dissemination for a Gossip-based
  Protocol","  We develop an analytical model of information dissemination for a gossiping
protocol that combines both pull and push approaches. With this model we
analyse how fast an item is replicated through a network, and how fast the item
spreads in the network, and how fast the item covers the network. We also
determine the optimal size of the exchange buffer, to obtain fast replication.
Our results are confirmed by large-scale simulation experiments.
"
70,A Call-Graph Profiler for GNU Octave,"  We report the design and implementation of a call-graph profiler for GNU
Octave, a numerical computing platform. GNU Octave simplifies matrix
computation for use in modeling or simulation. Our work provides a call-graph
profiler, which is an improvement on the flat profiler. We elaborate design
constraints of building a profiler for numerical computation, and benchmark the
profiler by comparing it to the rudimentary timer start-stop (tic-toc)
measurements, for a similar set of programs. The profiler code provides clean
interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU
Octave.
"
71,A Model for Probabilistic Reasoning on Assume/Guarantee Contracts,"  In this paper, we present a probabilistic adaptation of an Assume/Guarantee
contract formalism. For the sake of generality, we assume that the extended
state machines used in the contracts and implementations define sets of runs on
a given set of variables, that compose by intersection over the common
variables. In order to enable probabilistic reasoning, we consider that the
contracts dictate how certain input variables will behave, being either
non-deterministic, or probabilistic; the introduction of probabilistic
variables leading us to tune the notions of implementation, refinement and
composition. As shown in the report, this probabilistic adaptation of the
Assume/Guarantee contract theory preserves compositionality and therefore
allows modular reliability analysis, either with a top-down or a bottom-up
approach.
"
72,"An Enhanced Mathematical Model for Performance Evaluation of Optical
  Burst Switched Networks","  This paper has been withdrawn by the authors.
"
73,Characterizing the Robustness of Complex Networks,"  With increasingly ambitious initiatives such as GENI and FIND that seek to
design the future Internet, it becomes imperative to define the characteristics
of robust topologies, and build future networks optimized for robustness. This
paper investigates the characteristics of network topologies that maintain a
high level of throughput in spite of multiple attacks. To this end, we select
network topologies belonging to the main network models and some real world
networks. We consider three types of attacks: removal of random nodes, high
degree nodes, and high betweenness nodes. We use elasticity as our robustness
measure and, through our analysis, illustrate that different topologies can
have different degrees of robustness. In particular, elasticity can fall as low
as 0.8% of the upper bound based on the attack employed. This result
substantiates the need for optimized network topology design. Furthermore, we
implement a tradeoff function that combines elasticity under the three attack
strategies and considers the cost of the network. Our extensive simulations
show that, for a given network density, regular and semi-regular topologies can
have higher degrees of robustness than heterogeneous topologies, and that link
redundancy is a sufficient but not necessary condition for robustness.
"
74,"Broadcasting in Prefix Space: P2P Data Dissemination with Predictable
  Performance","  A broadcast mode may augment peer-to-peer overlay networks with an efficient,
scalable data replication function, but may also give rise to a virtual link
layer in VPN-type solutions. We introduce a simple broadcasting mechanism that
operates in the prefix space of distributed hash tables without signaling. This
paper concentrates on the performance analysis of the prefix flooding scheme.
Starting from simple models of recursive $k$-ary trees, we analytically derive
distributions of hop counts and the replication load. Extensive simulation
results are presented further on, based on an implementation within the OverSim
framework. Comparisons are drawn to Scribe, taken as a general reference model
for group communication according to the shared, rendezvous-point-centered
distribution paradigm. The prefix flooding scheme thereby confirmed its widely
predictable performance and consistently outperformed Scribe in all metrics.
Reverse path selection in overlays is identified as a major cause of
performance degradation.
"
75,Performance Modeling and Evaluation for Information-Driven Networks,"  Information-driven networks include a large category of networking systems,
where network nodes are aware of information delivered and thus can not only
forward data packets but may also perform information processing. In many
situations, the quality of service (QoS) in information-driven networks is
provisioned with the redundancy in information. Traditional performance models
generally adopt evaluation measures suitable for packet-oriented service
guarantee, such as packet delay, throughput, and packet loss rate. These
performance measures, however, do not align well with the actual need of
information-driven networks. New performance measures and models for
information-driven networks, despite their importance, have been mainly blank,
largely because information processing is clearly application dependent and
cannot be easily captured within a generic framework. To fill the vacancy, we
present a new performance evaluation framework particularly tailored for
information-driven networks, based on the recent development of stochastic
network calculus. We analyze the QoS with respect to information delivery and
study the scheduling problem with the new performance metrics. Our analytical
framework can be used to calculate the network capacity in information delivery
and in the meantime to help transmission scheduling for a large body of systems
where QoS is stochastically guaranteed with the redundancy in information.
"
76,"A Simple Performance Analysis of a Core Node in an Optical Burst
  Switched Network","  This paper has been withdrawn
"
77,"An Approximation of the Outage Probability for Multi-hop AF Fixed Gain
  Relay","  In this letter, we present a closed-form approximation of the outage
probability for the multi-hop amplify-and-forward (AF) relaying systems with
fixed gain in Rayleigh fading channel. The approximation is derived from the
outage event for each hop. The simulation results show the tightness of the
proposed approximation in low and high signal-to-noise ratio (SNR) region.
"
78,"Using constraint programming to resolve the multi-source/multi-site data
  movement paradigm on the Grid","  In order to achieve both fast and coordinated data transfer to collaborative
sites as well as to create a distribution of data over multiple sites,
efficient data movement is one of the most essential aspects in distributed
environment. With such capabilities at hand, truly distributed task scheduling
with minimal latencies would be reachable by internationally distributed
collaborations (such as ones in HENP) seeking for scavenging or maximizing on
geographically spread computational resources. But it is often not all clear
(a) how to move data when available from multiple sources or (b) how to move
data to multiple compute resources to achieve an optimal usage of available
resources. We present a method of creating a Constraint Programming (CP) model
consisting of sites, links and their attributes such as bandwidth for grid
network data transfer also considering user tasks as part of the objective
function for an optimal solution. We will explore and explain trade-off between
schedule generation time and divergence from the optimal solution and show how
to improve and render viable the solution's finding time by using search tree
time limit, approximations, restrictions such as symmetry breaking or grouping
similar tasks together, or generating sequence of optimal schedules by
splitting the input problem. Results of data transfer simulation for each case
will also include a well known Peer-2-Peer model, and time taken to generate a
schedule as well as time needed for a schedule execution will be compared to a
CP optimal solution. We will additionally present a possible implementation
aimed to bring a distributed datasets (multiple sources) to a given site in a
minimal time.
"
79,A Holistic Approach to Information Distribution in Ad Hoc Networks,"  We investigate the problem of spreading information contents in a wireless ad
hoc network with mechanisms embracing the peer-to-peer paradigm. In our vision,
information dissemination should satisfy the following requirements: (i) it
conforms to a predefined distribution and (ii) it is evenly and fairly carried
by all nodes in their turn. In this paper, we observe the dissemination effects
when the information moves across nodes according to two well-known mobility
models, namely random walk and random direction. Our approach is fully
distributed and comes at a very low cost in terms of protocol overhead; in
addition, simulation results show that the proposed solution can achieve the
aforementioned goals under different network scenarios, provided that a
sufficient number of information replicas are injected into the network. This
observation calls for a further step: in the realistic case where the user
content demand varies over time, we need a content replication/drop strategy to
adapt the number of information replicas to the changes in the information
query rate. We therefore devise a distributed, lightweight scheme that performs
efficiently in a variety of scenarios.
"
80,Analysis of bandwidth measurement methodologies over WLAN systems,"  WLAN devices have become a fundamental component of nowadays network
deployments. However, even though traditional networking applications run
mostly unchanged over wireless links, the actual interaction between these
applications and the dynamics of wireless transmissions is not yet fully
understood. An important example of such applications are bandwidth estimation
tools. This area has become a mature research topic with well-developed
results. Unfortunately recent studies have shown that the application of these
results to WLAN links is not straightforward. The main reasons for this is that
the assumptions taken to develop bandwidth measurements tools do not hold any
longer in the presence of wireless links (e.g. non-FIFO scheduling). This paper
builds from these observations and its main goal is to analyze the interaction
between probe packets and WLAN transmissions in bandwidth estimation processes.
The paper proposes an analytical model that better accounts for the
particularities of WLAN links. The model is validated through extensive
experimentation and simulation and reveals that (1) the distribution of the
delay to transmit probing packets is not the same for the whole probing
sequence, this biases the measurements process and (2) existing tools and
techniques point at the achievable throughput rather than the available
bandwidth or the capacity, as previously assumed.
"
81,"A Multiobjective Optimization Framework for Routing in Wireless Ad Hoc
  Networks","  Wireless ad hoc networks are seldom characterized by one single performance
metric, yet the current literature lacks a flexible framework to assist in
characterizing the design tradeoffs in such networks. In this work, we address
this problem by proposing a new modeling framework for routing in ad hoc
networks, which used in conjunction with metaheuristic multiobjective search
algorithms, will result in a better understanding of network behavior and
performance when multiple criteria are relevant. Our approach is to take a
holistic view of the network that captures the cross-interactions among
interference management techniques implemented at various layers of the
protocol stack. The resulting framework is a complex multiobjective
optimization problem that can be efficiently solved through existing
multiobjective search techniques. In this contribution, we present the Pareto
optimal sets for an example sensor network when delay, robustness and energy
are considered. The aim of this paper is to present the framework and hence for
conciseness purposes, the multiobjective optimization search is not developed
herein.
"
82,"Towards a Statistical Methodology to Evaluate Program Speedups and their
  Optimisation Techniques","  The community of program optimisation and analysis, code performance
evaluation, parallelisation and optimising compilation has published since many
decades hundreds of research and engineering articles in major conferences and
journals. These articles study efficient algorithms, strategies and techniques
to accelerate programs execution times, or optimise other performance metrics
(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but
nobody is able to reproduce them exactly. The non-reproducibility of our
research results is a dark point of the art, and we cannot be qualified as {\it
computer scientists} if we do not provide rigorous experimental methodology.
This article provides a first effort towards a correct statistical protocol for
analysing and measuring speedups. As we will see, some common mistakes are done
by the community inside published articles, explaining part of the
non-reproducibility of the results. Our current article is not sufficient by
its own to deliver a complete experimental methodology, further efforts must be
done by the community to decide about a common protocol for our future
experiences. Anyway, our community should take care about the aspect of
reproducibility of the results in the future.
"
83,Node Weighted Scheduling,"  This paper proposes a new class of online policies for scheduling in
input-buffered crossbar switches. Our policies are throughput optimal for a
large class of arrival processes which satisfy strong-law of large numbers.
Given an initial configuration and no further arrivals, our policies drain all
packets in the system in the minimal amount of time (providing an online
alternative to the batch approach based on Birkhoff-VonNeumann decompositions).
We show that it is possible for policies in our class to be throughput optimal
even if they are not constrained to be maximal in every time slot.
  Most algorithms for switch scheduling take an edge based approach; in
contrast, we focus on scheduling (a large enough set of) the most congested
ports. This alternate approach allows for lower-complexity algorithms, and also
requires a non-standard technique to prove throughput-optimality. One algorithm
in our class, Maximum Vertex-weighted Matching (MVM) has worst-case complexity
similar to Max-size Matching, and in simulations shows slightly better delay
performance than Max-(edge)weighted-Matching (MWM).
"
84,"Fundamental delay bounds in peer-to-peer chunk-based real-time streaming
  systems","  This paper addresses the following foundational question: what is the maximum
theoretical delay performance achievable by an overlay peer-to-peer streaming
system where the streamed content is subdivided into chunks? As shown in this
paper, when posed for chunk-based systems, and as a consequence of the
store-and-forward way in which chunks are delivered across the network, this
question has a fundamentally different answer with respect to the case of
systems where the streamed content is distributed through one or more flows
(sub-streams). To circumvent the complexity emerging when directly dealing with
delay, we express performance in term of a convenient metric, called ""stream
diffusion metric"". We show that it is directly related to the end-to-end
minimum delay achievable in a P2P streaming network. In a homogeneous scenario,
we derive a performance bound for such metric, and we show how this bound
relates to two fundamental parameters: the upload bandwidth available at each
node, and the number of neighbors a node may deliver chunks to. In this bound,
k-step Fibonacci sequences do emerge, and appear to set the fundamental laws
that characterize the optimal operation of chunk-based systems.
"
85,A Proof of Concept for Optimizing Task Parallelism by Locality Queues,"  Task parallelism as employed by the OpenMP task construct, although ideal for
tackling irregular problems or typical producer/consumer schemes, bears some
potential for performance bottlenecks if locality of data access is important,
which is typically the case for memory-bound code on ccNUMA systems. We present
a programming technique which ameliorates adverse effects of dynamic task
distribution by sorting tasks into locality queues, each of which is preferably
processed by threads that belong to the same locality domain. Dynamic
scheduling is fully preserved inside each domain, and is preferred over
possible load imbalance even if non-local access is required. The effectiveness
of the approach is demonstrated using a blocked six-point stencil solver as a
toy model.
"
86,Random Fruits on the Zielonka Tree,"  Stochastic games are a natural model for the synthesis of controllers
confronted to adversarial and/or random actions. In particular,
$\omega$-regular games of infinite length can represent reactive systems which
are not expected to reach a correct state, but rather to handle a continuous
stream of events. One critical resource in such applications is the memory used
by the controller. In this paper, we study the amount of memory that can be
saved through the use of randomisation in strategies, and present matching
upper and lower bounds for stochastic Muller games.
"
87,The Multi-Branched Method of Moments for Queueing Networks,"  We propose a new exact solution algorithm for closed multiclass product-form
queueing networks that is several orders of magnitude faster and less memory
consuming than established methods for multiclass models, such as the Mean
Value Analysis (MVA) algorithm. The technique is an important generalization of
the recently proposed Method of Moments (MoM) which, differently from MVA,
recursively computes higher-order moments of queue-lengths instead of mean
values.
  The main contribution of this paper is to prove that the information used in
the MoM recursion can be increased by considering multiple recursive branches
that evaluate models with different number of queues. This reformulation allows
to formulate a simpler matrix difference equation which leads to large
computational savings with respect to the original MoM recursion. Computational
analysis shows several cases where the proposed algorithm is between 1,000 and
10,000 times faster and less memory consuming than the original MoM, thus
extending the range of multiclass models where exact solutions are feasible.
"
88,Stability of Finite Population ALOHA with Variable Packets,"  ALOHA is one of the most basic Medium Access Control (MAC) protocols and
represents a foundation for other more sophisticated distributed and
asynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the
traditional work that focused on mean value analysis, we study the
distributional properties of packet transmission delays over an ALOHA channel.
We discover a new phenomenon showing that a basic finite population ALOHA model
with variable size (exponential) packets is characterized by power law
transmission delays, possibly even resulting in zero throughput. These results
are in contrast to the classical work that shows exponential delays and
positive throughput for finite population ALOHA with fixed packets.
Furthermore, we characterize a new stability condition that is entirely derived
from the tail behavior of the packet and backoff distributions that may not be
determined by mean values. The power law effects and the possible instability
might be diminished, or perhaps eliminated, by reducing the variability of
packets. However, we show that even a slotted (synchronized) ALOHA with packets
of constant size can exhibit power law delays when the number of active users
is random. From an engineering perspective, our results imply that the
variability of packet sizes and number of active users need to be taken into
consideration when designing robust MAC protocols, especially for ad-hoc/sensor
networks where other factors, such as link failures and mobility, might further
compound the problem.
"
89,EXtensible Animator for Mobile Simulations: EXAMS,"  One of the most widely used simulation environments for mobile wireless
networks is the Network Simulator 2 (NS-2). However NS-2 stores its outcome in
a text file, so there is a need for a visualization tool to animate the
simulation of the wireless network. The purpose of this tool is to help the
researcher examine in detail how the wireless protocol works both on a network
and a node basis. It is clear that much of this information is protocol
dependent and cannot be depicted properly by a general purpose animation
process. Existing animation tools do not provide this level of information
neither permit the specific protocol to control the animation at all. EXAMS is
an NS-2 visualization tool for mobile simulations which makes possible the
portrayal of NS-2 internal information like transmission properties and node
data structures. This is mainly possible due to EXAMS extensible architecture
which separates the animation process into a general and a protocol specific
part. The latter can be developed independently by the protocol designer and
loaded on demand. These and other useful characteristics of the EXAMS tool can
be an invaluable help for a researcher in order to investigate and debug a
mobile networking protocol.
"
90,"Lightweight Task Analysis for Cache-Aware Scheduling on Heterogeneous
  Clusters","  We present a novel characterization of how a program stresses cache. This
characterization permits fast performance prediction in order to simulate and
assist task scheduling on heterogeneous clusters. It is based on the estimation
of stack distance probability distributions. The analysis requires the
observation of a very small subset of memory accesses, and yields a reasonable
to very accurate prediction in constant time.
"
91,Measuring Independence of Datasets,"  A data stream model represents setting where approximating pairwise, or
$k$-wise, independence with sublinear memory is of considerable importance. In
the streaming model the joint distribution is given by a stream of $k$-tuples,
with the goal of testing correlations among the components measured over the
entire stream. In the streaming model, Indyk and McGregor (SODA 08) recently
gave exciting new results for measuring pairwise independence. The Indyk and
McGregor methods provide $\log{n}$-approximation under statistical distance
between the joint and product distributions in the streaming model. Indyk and
McGregor leave, as their main open question, the problem of improving their
$\log n$-approximation for the statistical distance metric.
  In this paper we solve the main open problem posed by of Indyk and McGregor
for the statistical distance for pairwise independence and extend this result
to any constant $k$. In particular, we present an algorithm that computes an
$(\epsilon, \delta)$-approximation of the statistical distance between the
joint and product distributions defined by a stream of $k$-tuples. Our
algorithm requires $O(({1\over \epsilon}\log({nm\over \delta}))^{(30+k)^k})$
memory and a single pass over the data stream.
"
92,"ScALPEL: A Scalable Adaptive Lightweight Performance Evaluation Library
  for application performance monitoring","  As supercomputers continue to grow in scale and capabilities, it is becoming
increasingly difficult to isolate processor and system level causes of
performance degradation. Over the last several years, a significant number of
performance analysis and monitoring tools have been built/proposed. However,
these tools suffer from several important shortcomings, particularly in
distributed environments. In this paper we present ScALPEL, a Scalable Adaptive
Lightweight Performance Evaluation Library for application performance
monitoring at the functional level. Our approach provides several distinct
advantages. First, ScALPEL is portable across a wide variety of architectures,
and its ability to selectively monitor functions presents low run-time
overhead, enabling its use for large-scale production applications. Second, it
is run-time configurable, enabling both dynamic selection of functions to
profile as well as events of interest on a per function basis. Third, our
approach is transparent in that it requires no source code modifications.
Finally, ScALPEL is implemented as a pluggable unit by reusing existing
performance monitoring frameworks such as Perfmon and PAPI and extending them
to support both sequential and MPI applications.
"
93,"Modeling Multi-Cell IEEE 802.11 WLANs with Application to Channel
  Assignment","  We provide a simple and accurate analytical model for multi-cell
infrastructure IEEE 802.11 WLANs. Our model applies if the cell radius, $R$, is
much smaller than the carrier sensing range, $R_{cs}$. We argue that, the
condition $R_{cs} >> R$ is likely to hold in a dense deployment of Access
Points (APs) where, for every client or station (STA), there is an AP very
close to the STA such that the STA can associate with the AP at a high physical
rate. We develop a scalable cell level model for such WLANs with saturated AP
and STA queues as well as for TCP-controlled long file downloads. The accuracy
of our model is demonstrated by comparison with ns-2 simulations. We also
demonstrate how our analytical model could be applied in conjunction with a
Learning Automata (LA) algorithm for optimal channel assignment. Based on the
insights provided by our analytical model, we propose a simple decentralized
algorithm which provides static channel assignments that are Nash equilibria in
pure strategies for the objective of maximizing normalized network throughput.
Our channel assignment algorithm requires neither any explicit knowledge of the
topology nor any message passing, and provides assignments in only as many
steps as there are channels. In contrast to prior work, our approach to channel
assignment is based on the throughput metric.
"
94,MANETS: High mobility can make up for low transmission power,"  We consider a Mobile Ad-hoc NETworks (MANET) formed by ""n"" nodes that move
independently at random over a finite square region of the plane. Nodes
exchange data if they are at distance at most ""r"" within each other, where r>0
is the node transmission radius. The ""flooding time"" is the number of time
steps required to broadcast a message from a source node to every node of the
network. Flooding time is an important measure of the speed of information
spreading in dynamic networks.
  We derive a nearly-tight upper bound on the flooding time which is a
decreasing function of the maximal ""velocity"" of the nodes. It turns out that,
when the node velocity is sufficiently high, even if the node transmission
radius ""r"" is far below the ""connectivity threshold"", the flooding time does
not asymptotically depend on ""r"". This implies that flooding can be very fast
even though every ""snapshot"" (i.e. the static random geometric graph at any
fixed time) of the MANET is fully disconnected. Data reach all nodes quickly
despite these ones use very low transmission power.
  Our result is the first analytical evidence of the fact that high, random
node mobility strongly speed-up information spreading and, at the same time,
let nodes save energy.
"
95,"Adaptive Mesh Approach for Predicting Algorithm Behavior with
  Application to Visibility Culling in Computer Graphics","  We propose a concise approximate description, and a method for efficiently
obtaining this description, via adaptive random sampling of the performance
(running time, memory consumption, or any other profileable numerical quantity)
of a given algorithm on some low-dimensional rectangular grid of inputs. The
formal correctness is proven under reasonable assumptions on the algorithm
under consideration; and the approach's practical benefit is demonstrated by
predicting for which observer positions and viewing directions an occlusion
culling algorithm yields a net performance benefit or loss compared to a simple
brute force renderer.
"
96,"A Mean Field Approach for Optimization in Particles Systems and
  Applications","  This paper investigates the limit behavior of Markov Decision Processes
(MDPs) made of independent particles evolving in a common environment, when the
number of particles goes to infinity. In the finite horizon case or with a
discounted cost and an infinite horizon, we show that when the number of
particles becomes large, the optimal cost of the system converges almost surely
to the optimal cost of a discrete deterministic system (the ``optimal mean
field''). Convergence also holds for optimal policies. We further provide
insights on the speed of convergence by proving several central limits theorems
for the cost and the state of the Markov decision process with explicit
formulas for the variance of the limit Gaussian laws. Then, our framework is
applied to a brokering problem in grid computing. The optimal policy for the
limit deterministic system is computed explicitly. Several simulations with
growing numbers of processors are reported. They compare the performance of the
optimal policy of the limit system used in the finite case with classical
policies (such as Join the Shortest Queue) by measuring its asymptotic gain as
well as the threshold above which it starts outperforming classical policies.
"
97,Stabilizing Maximal Independent Set in Unidirectional Networks is Hard,"  A distributed algorithm is self-stabilizing if after faults and attacks hit
the system and place it in some arbitrary global state, the system recovers
from this catastrophic situation without external intervention in finite time.
In this paper, we consider the problem of constructing self-stabilizingly a
\emph{maximal independent set} in uniform unidirectional networks of arbitrary
shape. On the negative side, we present evidence that in uniform networks,
\emph{deterministic} self-stabilization of this problem is \emph{impossible}.
Also, the \emph{silence} property (\emph{i.e.} having communication fixed from
some point in every execution) is impossible to guarantee, either for
deterministic or for probabilistic variants of protocols. On the positive side,
we present a deterministic protocol for networks with arbitrary unidirectional
networks with unique identifiers that exhibits polynomial space and time
complexity in asynchronous scheduling. We complement the study with
probabilistic protocols for the uniform case: the first probabilistic protocol
requires infinite memory but copes with asynchronous scheduling, while the
second probabilistic protocol has polynomial space complexity but can only
handle synchronous scheduling. Both probabilistic solutions have expected
polynomial time complexity.
"
98,"Optimized Implementation of Elliptic Curve Based Additive Homomorphic
  Encryption for Wireless Sensor Networks","  When deploying wireless sensor networks (WSNs) in public environments it may
become necessary to secure their data storage and transmission against possible
attacks such as node-compromise and eavesdropping. The nodes feature only small
computational and energy resources, thus requiring efficient algorithms. As a
solution for this problem the TinyPEDS approach was proposed in [7], which
utilizes the Elliptic Curve ElGamal (EC-ElGamal) cryptosystem for additive
homomorphic encryption allowing concealed data aggregation. This work presents
an optimized implementation of EC-ElGamal on a MicaZ mote, which is a typical
sensor node platform with 8-bit processor for WSNs. Compared to the best
previous result, our implementation is at least 44% faster for fixed-point
multiplication. Because most parts of the algorithm are similar to standard
Elliptic Curve algorithms, the results may be reused in other realizations on
constrained devices as well.
"
99,"Asymptotic Optimality of the Static Frequency Caching in the Presence of
  Correlated Requests","  It is well known that the static caching algorithm that keeps the most
frequently requested documents in the cache is optimal in case when documents
are of the same size and requests are independent and equally distributed.
However, it is hard to develop explicit and provably optimal caching algorithms
when requests are statistically correlated. In this paper, we show that keeping
the most frequently requested documents in the cache is still optimal for large
cache sizes even if the requests are strongly correlated.
"
100,"Effect of cell residence time variance on the performance of an advanced
  paging algorithm","  The use of advanced sequential paging algorithms has been suggested as a
means to reduce the signaling cost in future mobile cellular networks. In a
proposed algorithm (Koukoutsidis and Theologou, 2003), the system can use the
additional information of the last interaction cell combined with a mobility
model to predict the short-term location probabilities at the time of an
incoming call arrival. The short-term location probabilities reduce the
uncertainty in mobile user position and thus greatly improve the search. In
this paper, an analytical model is derived that allows for a general
distribution of cell residence times. By considering a Gamma distribution, we
study the effect of the variance of cell residence times and derive useful
results on the performance of the algorithm.
"
101,Stochastic Service Guarantee Analysis Based on Time-Domain Models,"  Stochastic network calculus is a theory for stochastic service guarantee
analysis of computer communication networks. In the current stochastic network
calculus literature, its traffic and server models are typically based on the
cumulative amount of traffic and cumulative amount of service respectively.
However, there are network scenarios where the applicability of such models is
limited, and hence new ways of modeling traffic and service are needed to
address this limitation. This paper presents time-domain models and results for
stochastic network calculus. Particularly, we define traffic models, which are
based on probabilistic lower-bounds on cumulative packet inter-arrival time,
and server models, which are based on probabilistic upper-bounds on cumulative
packet service time. In addition, examples demonstrating the use of the
proposed time-domain models are provided. On the basis of the proposed models,
the five basic properties of stochastic network calculus are also proved, which
implies broad applicability of the proposed time-domain approach.
"
102,"Fundamentals of the Backoff Process in 802.11: Dichotomy of the
  Aggregation","  This paper discovers fundamental principles of the backoff process that
governs the performance of IEEE 802.11. A simplistic principle founded upon
regular variation theory is that the backoff time has a truncated Pareto-type
tail distribution with an exponent of $(\log \gamma)/\log m$ ($m$ is the
multiplicative factor and $\gamma$ is the collision probability). This reveals
that the per-node backoff process is heavy-tailed in the strict sense for
$\gamma>1/m^2$, and paves the way for the following unifying result.
  The state-of-the-art theory on the superposition of the heavy-tailed
processes is applied to establish a dichotomy exhibited by the aggregate
backoff process, putting emphasis on the importance of time-scale on which we
view the backoff processes. While the aggregation on normal time-scales leads
to a Poisson process, it is approximated by a new limiting process possessing
long-range dependence (LRD) on coarse time-scales. This dichotomy turns out to
be instrumental in formulating short-term fairness, extending existing formulas
to arbitrary population, and to elucidate the absence of LRD in practical
situations. A refined wavelet analysis is conducted to strengthen this
argument.
"
103,Bounds on series-parallel slowdown,"  We use activity networks (task graphs) to model parallel programs and
consider series-parallel extensions of these networks. Our motivation is
two-fold: the benefits of series-parallel activity networks and the modelling
of programming constructs, such as those imposed by current parallel computing
environments. Series-parallelisation adds precedence constraints to an activity
network, usually increasing its makespan (execution time). The slowdown ratio
describes how additional constraints affect the makespan. We disprove an
existing conjecture positing a bound of two on the slowdown when workload is
not considered. Where workload is known, we conjecture that 4/3 slowdown is
always achievable, and prove our conjecture for small networks using max-plus
algebra. We analyse a polynomial-time algorithm showing that achieving 4/3
slowdown is in exp-APX. Finally, we discuss the implications of our results.
"
104,Introducing a Performance Model for Bandwidth-Limited Loop Kernels,"  We present a performance model for bandwidth limited loop kernels which is
founded on the analysis of modern cache based microarchitectures. This model
allows an accurate performance prediction and evaluation for existing
instruction codes. It provides an in-depth understanding of how performance for
different memory hierarchy levels is made up. The performance of raw memory
load, store and copy operations and a stream vector triad are analyzed and
benchmarked on three modern x86-type quad-core architectures in order to
demonstrate the capabilities of the model.
"
105,Information Ranking and Power Laws on Trees,"  We study the situations when the solution to a weighted stochastic recursion
has a power law tail. To this end, we develop two complementary approaches, the
first one extends Goldie's (1991) implicit renewal theorem to cover recursions
on trees; and the second one is based on a direct sample path large deviations
analysis of weighted recursive random sums. We believe that these methods may
be of independent interest in the analysis of more general weighted branching
processes as well as in the analysis of algorithms.
"
106,An Axiomatic Theory of Fairness in Network Resource Allocation,"  We present a set of five axioms for fairness measures in resource allocation.
A family of fairness measures satisfying the axioms is constructed. Well-known
notions such as alpha-fairness, Jain's index, and entropy are shown to be
special cases. Properties of fairness measures satisfying the axioms are
proven, including Schur-concavity. Among the engineering implications is a
generalized Jain's index that tunes the resolution of the fairness measure, a
new understanding of alpha-fair utility functions, and an interpretation of
""larger alpha is more fair"". We also construct an alternative set of four
axioms to capture efficiency objectives and feasibility constraints.
"
107,"Enabling and Optimizing Pilot Jobs using Xen based Virtual Machines for
  the HPC Grid Applications","  The primary motivation for uptake of virtualization have been resource
isolation, capacity management and resource customization: isolation and
capacity management allow providers to isolate users from the site and control
their resources usage while customization allows end-users to easily project
the required environment onto a variety of sites. Various approaches have been
taken to integrate virtualization with Grid technologies. In this paper, we
propose an approach that combines virtualization on the existing software
infrastructure such as Pilot Jobs with minimum change on the part of resource
providers.
"
108,"Similarity Analysis in Automatic Performance Debugging of SPMD Parallel
  Programs","  Different from sequential programs, parallel programs possess their own
characteristics which are difficult to analyze in the multi-process or
multi-thread environment. This paper presents an innovative method to
automatically analyze the SPMD programs. Firstly, with the help of clustering
method focusing on similarity analysis, an algorithm is designed to locate
performance problems in parallel programs automatically. Secondly a Rough Set
method is used to uncover the performance problem and provide the insight into
the micro-level causes. Lastly, we have analyzed a production parallel
application to verify the effectiveness of our method and system.
"
109,"Methodology for assessing system performance loss within a proactive
  maintenance framework","  Maintenance plays now a critical role in manufacturing for achieving
important cost savings and competitive advantage while preserving product
conditions. It suggests moving from conventional maintenance practices to
predictive strategy. Indeed the maintenance action has to be done at the right
time based on the system performance and component Remaining Useful Life (RUL)
assessed by a prognostic process. In that way, this paper proposes a
methodology in order to evaluate the performance loss of the system according
to the degradation of component and the deviations of system input flows. This
methodology is supported by the neuro-fuzzy tool ANFIS (Adaptive Neuro-Fuzzy
Inference Systems) that allows to integrate knowledge from two different
sources: expertise and real data. The feasibility and added value of such
methodology is then highlighted through an application case extracted from the
TELMA platform used for education and research.
"
110,A note on uniform power connectivity in the SINR model,"  In this paper we study the connectivity problem for wireless networks under
the Signal to Interference plus Noise Ratio (SINR) model. Given a set of radio
transmitters distributed in some area, we seek to build a directed strongly
connected communication graph, and compute an edge coloring of this graph such
that the transmitter-receiver pairs in each color class can communicate
simultaneously. Depending on the interference model, more or less colors,
corresponding to the number of frequencies or time slots, are necessary. We
consider the SINR model that compares the received power of a signal at a
receiver to the sum of the strength of other signals plus ambient noise . The
strength of a signal is assumed to fade polynomially with the distance from the
sender, depending on the so-called path-loss exponent $\alpha$.
  We show that, when all transmitters use the same power, the number of colors
needed is constant in one-dimensional grids if $\alpha>1$ as well as in
two-dimensional grids if $\alpha>2$. For smaller path-loss exponents and
two-dimensional grids we prove upper and lower bounds in the order of
$\mathcal{O}(\log n)$ and $\Omega(\log n/\log\log n)$ for $\alpha=2$ and
$\Theta(n^{2/\alpha-1})$ for $\alpha<2$ respectively. If nodes are distributed
uniformly at random on the interval $[0,1]$, a \emph{regular} coloring of
$\mathcal{O}(\log n)$ colors guarantees connectivity, while $\Omega(\log \log
n)$ colors are required for any coloring.
"
111,"AIS for Misbehavior Detection in Wireless Sensor Networks: Performance
  and Design Principles","  A sensor network is a collection of wireless devices that are able to monitor
physical or environmental conditions. These devices (nodes) are expected to
operate autonomously, be battery powered and have very limited computational
capabilities. This makes the task of protecting a sensor network against
misbehavior or possible malfunction a challenging problem. In this document we
discuss performance of Artificial immune systems (AIS) when used as the
mechanism for detecting misbehavior.
  We show that (i) mechanism of the AIS have to be carefully applied in order
to avoid security weaknesses, (ii) the choice of genes and their interaction
have a profound influence on the performance of the AIS, (iii) randomly created
detectors do not comply with limitations imposed by communications protocols
and (iv) the data traffic pattern seems not to impact significantly the overall
performance.
  We identified a specific MAC layer based gene that showed to be especially
useful for detection; genes measure a network's performance from a node's
viewpoint. Furthermore, we identified an interesting complementarity property
of genes; this property exploits the local nature of sensor networks and moves
the burden of excessive communication from normally behaving nodes to
misbehaving nodes. These results have a direct impact on the design of AIS for
sensor networks and on engineering of sensor networks.
"
112,"Application of non-uniform laxity to EDF for aperiodic tasks to improve
  task utilisation on multicore platforms","  This paper proposes a new scheduler applying the concept of non-uniform
laxity to Earliest deadline first (EDF) approach for aperiodic tasks. This
scheduler improves task utilisation (Execution time / deadline) and also
increases the number of tasks that are being scheduled. Laxity is a measure of
the spare time permitted for the task before it misses its deadline, and is
computed using the expression (deadline - (current time + execution time)).
Weight decides the priority of the task and is defined by the expression
(quantum slice time / allocated time)*total core time for the task. Quantum
slice time is the time actually used, allocated time is the time allocated by
the scheduler, and total core time is the time actually reserved by the core
for execution of one quantum of the task. Non-uniform laxity enables scheduling
of tasks that have higher priority before the normal execution of other tasks
and is computed by multiplying the weight of the task with its laxity. The
algorithm presented in the paper has been simulated on Cheddar, a real time
scheduling tool and also on SESC, an architectural simulator for multicore
platforms, for upto 5000 random task sets, and upto 5000 cores. This scheduler
improves task utilisation by 35% and the number of tasks being scheduled by
36%, compared to conventional EDF.
"
113,A Performance Analysis of HICCUPS - a Steganographic System for WLAN,"  The paper presents an analysis of performance features of the HICCUPS (HIdden
Communication system for CorrUPted networkS) including the efficiency and the
cost of the system in WLANs (Wireless Local Area Networks). The analysis relies
on the original CSMA/CA (Carrier Sense Multiple Access with Collision
Avoidance) 802.11 Markov chain-based model.
"
114,"Global Stability Analysis for an Internet Congestion Control Model with
  a Time-Varying Link Capacity","  In this paper, a global stability analysis is given for a rate-based
congestion control system modeled by a nonlinear delayed differential equation.
The model determines the dynamics of a single-source single-link network, with
a time-varying capacity of link and a fixed communication delay. We obtain a
sufficient delay-independent conditions on system parameters under which global
asymptotic stability of the system is guarantied. The proof is based on an
extension of Lyapunov-Krasovskii theorem for a class of nonlinear time-delay
systems. The numerical simulations for a typical scenario justify the
theoretical results.
"
115,A New Approach to Manage QoS in Distributed Multimedia Systems,"  Dealing with network congestion is a criterion used to enhance quality of
service (QoS) in distributed multimedia systems. The existing solutions for the
problem of network congestion ignore scalability considerations because they
maintain a separate classification for each video stream. In this paper, we
propose a new method allowing to control QoS provided to clients according to
the network congestion, by discarding some frames when needed. The technique
proposed, called (m,k)-frame, is scalable with little degradation in
application performances. (m,k)-frame method is issued from the notion of
(m,k)-firm realtime constraints which means that among k invocations of a task,
m invocations must meet their deadline. Our simulation studies show the
usefulness of (m,k)-frame method to adapt the QoS to the real conditions in a
multimedia application, according to the current system load. Notably, the
system must adjust the QoS provided to active clients1 when their number
varies, i.e. dynamic arrival of clients.
"
116,"Adaptive Point-to-Multipoint Transmission for Multimedia Broadcast
  Multicast Services in LTE","  This paper investigates point-to-multipoint (PTM) transmission supporting
adaptive modulation and coding (AMC) as well as retransmissions based on
incremental redundancy. In contrast to the classical PTM transmission which was
introduced by the Multimedia Broadcast Multicast Service (MBMS), the
adaptiveness requires user individual feedback channels that allow the
receivers to report their radio conditions and send positive or negative
acknowledgments (ACK/NACK) for a Layer 1 transport block to the eNodeB. In this
work, an adaptive PTM scheme based on feedback from multiple users is presented
and evaluated. Furthermore, a simple NACK-oriented feedback mechanism is
introduced to relieve the feedback channel that is used in the uplink. Finally,
the performance of different single-cell MBMS transmission modes is evaluated
by dynamic radio network simulations. It is shown that adaptive PTM
transmission outperforms the conventional MBMS configurations in terms of radio
resource consumption and user satisfaction rate.
"
117,Gossip-based Search in Multipeer Communication Networks,"  We study a gossip-based algorithm for searching data objects in a multipeer
communication network. All of the nodes in the network are able to communicate
with each other. There exists an initiator node that starts a round of searches
by randomly querying one or more of its neighbors for a desired object. The
queried nodes can also be activated and look for the object. We examine several
behavioural patterns of nodes with respect to their willingness to cooperate in
the search. We derive mathematical models for the search process based on the
balls and bins model, as well as known approximations for the rumour-spreading
problem. All models are validated with simulations. We also evaluate the
performance of the algorithm and examine the impact of search parameters.
"
118,Performance of Network and Service Monitoring Frameworks,"  The efficiency and the performance of anagement systems is becoming a hot
research topic within the networks and services management community. This
concern is due to the new challenges of large scale managed systems, where the
management plane is integrated within the functional plane and where management
activities have to carry accurate and up-to-date information. We defined a set
of primary and secondary metrics to measure the performance of a management
approach. Secondary metrics are derived from the primary ones and quantifies
mainly the efficiency, the scalability and the impact of management activities.
To validate our proposals, we have designed and developed a benchmarking
platform dedicated to the measurement of the performance of a JMX manager-agent
based management system. The second part of our work deals with the collection
of measurement data sets from our JMX benchmarking platform. We mainly studied
the effect of both load and the number of agents on the scalability, the impact
of management activities on the user perceived performance of a managed server
and the delays of JMX operations when carrying variables values. Our findings
show that most of these delays follow a Weibull statistical distribution. We
used this statistical model to study the behavior of a monitoring algorithm
proposed in the literature, under heavy tail delays distribution. In this case,
the view of the managed system on the manager side becomes noisy and out of
date.
"
119,Throughput metrics and packet delay in TCP/IP networks,"  In the paper the method for estimation of throughput metrics like available
bandwidth and end-t-end capacity is supposed. This method is based on
measurement of network delay $D_i$ for packets of different sizes $W_i$. The
simple expression for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is
substantiated. The number of experiments on matching of the results received
new and traditional methods is spent. The received results testify to
possibility of application of new model.
"
120,"Approximate mechanism for measuring stability of Internet link in
  aggregated Internet pipe","  In this article we propose a method for measuring internet connection
stability which is fast and has negligible overhead for the process of its
complexity. This method finds a relative value for representing the stability
of internet connections and can also be extended for aggregated internet
connections. The method is documented with help of a real time implementation
and results are shared. This proposed measurement scheme uses HTTP GET method
for each connections. The normalized responses to identified sites like
gateways of ISPs, google.com etc are used for calculating current link
stability. The novelty of the approach is that historic values are used to
calculate overall link stability. In this discussion, we also document a method
to use the calculated values as a dynamic threshold metric. This is used in
routing decisions and for load-balancing each of the connections in an
aggregated bandwidth pipe. This scheme is a very popular practice in aggregated
internet connections.
"
121,"Dynamic Bandwidth Management in Distributed VoD based on the User Class
  Using Agents","  This paper proposes a dynamic bandwidth management algorithm in which more
bandwidth is allocated for higher class users and also higher priority is given
to the videos with higher popularity within a class using agent technology. The
popularity and weight profile of the videos which is used for efficiently
allocating bandwidth is periodically updated by a mobile agent. The proposed
approach allocates more bandwidth for higher class users and gives higher
priority for higher weight videos [popular videos] so that they can be served
with high QoS, reduces the load on the central multimedia server and maximizes
the channel utilization between the neighboring proxy servers and the central
multimedia server and lower video rejection ratio. The simulation results prove
the reduction of load on central multimedia server by load sharing among the
neighboring proxy servers, maximum bandwidth utilization, and more bandwidth
allocation for higher class users.
"
122,"Applicability of a Novel Integer Programming Model for Wireless Sensor
  Networks","  This paper presents an applicability analysis over a novel integer
programming model devoted to optimize power consumption efficiency in
heterogeneous wireless sensor networks. This model is based upon a schedule of
sensor allocation plans in multiple time intervals subject to coverage and
connectivity constraints. By turning off a specific set of redundant sensors in
each time interval, it is possible to reduce the total energy consumption in
the network and, at the same time, avoid partitioning the whole network by
losing some strategic sensors too prematurely. Since the network is
heterogeneous, sensors can sense different phenomena from different demand
points, with different sample rates. As the problem instances grows the time
spent to the execution turns impracticable.
"
123,"Performance Evaluation of Mesh based Multicast Reactive Routing Protocol
  under Black Hole Attack","  A mobile ad-hoc network is an autonomous system of mobile nodes connected by
wireless links in which nodes cooperate by forwarding packets for each other
thereby enabling communication beyond direct wireless transmission range. The
wireless and dynamic nature of ad-hoc networks makes them vulnerable to attacks
especially in routing protocols. Providing security in mobile ad-hoc networks
has been a major issue over the recent years. One of the prominent mesh base
reactive multicast routing protocols used in ad-hoc networks is On Demand
Multicast Routing protocol (ODMRP). The security of ODMRP is compromised by a
primary routing attack called black hole attack. In this attack a malicious
node advertises itself as having the shortest path to the node whose packets it
wants to intercept. This paper discusses the impact of black hole attack on
ODMRP under various scenarios. The performance is evaluated using metrics such
as packet delivery ratio and end to end delay for various numbers of senders
and receivers via simulation. Simulations are carried out using network
simulator ns-2. The results enable us to propose solutions to counter the
effect of black hole attack.
"
124,"Nuzzer: A Large-Scale Device-Free Passive Localization System for
  Wireless Environments","  The widespread usage of wireless local area networks and mobile devices has
fostered the interest in localization systems for wireless environments. The
majority of research in the context of wireless-based localization systems has
focused on device-based active localization, in which a device is attached to
tracked entities. Recently, device-free passive localization (DfP) has been
proposed where the tracked entity is neither required to carry devices nor
participate actively in the localization process. DfP systems are based on the
fact that RF signals are affected by the presence of people and objects in the
environment. The DfP concept enables a wide range of applications including
intrusion detection and tracking, border protection, and smart buildings
automation. Previous studies have focused on small areas with direct line of
sight and/or controlled environments. In this paper, we present the design,
implementation and analysis of Nuzzer, a large-scale device-free passive
localization system for real environments.
  Without any additional hardware, it makes use of the already installed
wireless data networks to monitor and process changes in the received signal
strength (RSS) transmitted from access points at one or more monitoring points.
We present probabilistic techniques for DfP localization and evaluate their
performance in a typical office building, rich in multipath, with an area of
1500 square meters. Our results show that the Nuzzer system gives device-free
location estimates with less than 2 meters median distance error using only two
monitoring laptops and three access points. This indicates the suitability of
Nuzzer to a large number of application domains.
"
125,"Transmission Performance Analysis of Digital Wire and Wireless Optical
  Links in Local and Wide Areas Optical Networks","  In the present paper, the transmission performance analysis of digital wire
and wireless optical links in local and wide areas optical networks have been
modeled and parametrically investigated over wide range of the affecting
parameters. Moreover, we have analyzed the basic equations of the comparative
study of the performance of digital fiber optic links with wire and wireless
optical links. The development of optical wireless communication systems is
accelerating as a high cost effective to wire fiber optic links. The optical
wireless technology is used mostly in wide bandwidth data transmission
applications. Finally, we have investigated the maximum transmission distance
and data transmission bit rates that can be achieved within digital wire and
wireless optical links for local and wide areas optical network applications.
"
126,Enhanced Algorithm for Link to System level Interface Mapping,"  The current SINR mechanism does not provide the base station (BS) with any
knowledge on the frequency selectivity of channel from mobile service
station(MSS). This knowledge is important since, contrary to the AWGN channel,
in a frequency selective channel there is no longer a 1 to 1 relation between
amount of increase in power and amount of improvement in effective SINR 1.
Furthermore, the relation is dependent on MCS level. This lack of knowledge in
the BS side results in larger fade margins, which translates directly to
reduction in capacity. In this paper we propose a enhanced algorithm on the
EESM model with weighted beta (\beta) that provides the BS with sufficient
knowledge on the channel-dependent relationship between power increase, MCS
change and improvement in effective SINR.
"
127,"Generalized Analysis of a Distributed Energy Efficient Algorithm for
  Change Detection","  An energy efficient distributed Change Detection scheme based on Page's CUSUM
algorithm was presented in \cite{icassp}. In this paper we consider a
nonparametric version of this algorithm. In the algorithm in \cite{icassp},
each sensor runs CUSUM and transmits only when the CUSUM is above some
threshold. The transmissions from the sensors are fused at the physical layer.
The channel is modeled as a Multiple Access Channel (MAC) corrupted with noise.
The fusion center performs another CUSUM to detect the change. In this paper,
we generalize the algorithm to also include nonparametric CUSUM and provide a
unified analysis.
"
128,"Effects of Diversity and Procrastination in Priority Queuing Theory: the
  Different Power Law Regimes","  Empirical analysis show that, after the update of a browser, the publication
of the vulnerability of a software, or the discovery of a cyber worm, the
fraction of computers still using the older version, or being not yet patched,
or exhibiting worm activity decays as power laws $\sim 1/t^{\alpha}$ with $0 <
\alpha \leq 1$ over time scales of years. We present a simple model for this
persistence phenomenon framed within the standard priority queuing theory, of a
target task which has the lowest priority compared with all other tasks that
flow on the computer of an individual. We identify a ""time deficit"" control
parameter $\beta$ and a bifurcation to a regime where there is a non-zero
probability for the target task to never be completed. The distribution of
waiting time ${\cal T}$ till the completion of the target task has the power
law tail $\sim 1/t^{1/2}$, resulting from a first-passage solution of an
equivalent Wiener process. Taking into account a diversity of time deficit
parameters in a population of individuals, the power law tail is changed into
$1/t^\alpha$ with $\alpha\in(0.5,\infty)$, including the well-known case $1/t$.
We also study the effect of ""procrastination"", defined as the situation in
which the target task may be postponed or delayed even after the individual has
solved all other pending tasks. This new regime provides an explanation for
even slower apparent decay and longer persistence.
"
129,In-packet Bloom filters: Design and networking applications,"  The Bloom filter (BF) is a well-known space-efficient data structure that
answers set membership queries with some probability of false positives. In an
attempt to solve many of the limitations of current inter-networking
architectures, some recent proposals rely on including small BFs in packet
headers for routing, security, accountability or other purposes that move
application states into the packets themselves. In this paper, we consider the
design of such in-packet Bloom filters (iBF). Our main contributions are
exploring the design space and the evaluation of a series of extensions (1) to
increase the practicality and performance of iBFs, (2) to enable
false-negative-free element deletion, and (3) to provide security enhancements.
In addition to the theoretical estimates, extensive simulations of the multiple
design parameters and implementation alternatives validate the usefulness of
the extensions, providing for enhanced and novel iBF networking applications.
"
130,Integrating Post-Newtonian Equations on Graphics Processing Units,"  We report on early results of a numerical and statistical study of binary
black hole inspirals. The two black holes are evolved using post-Newtonian
approximations starting with initially randomly distributed spin vectors. We
characterize certain aspects of the distribution shortly before merger. In
particular we note the uniform distribution of black hole spin vector dot
products shortly before merger and a high correlation between the initial and
final black hole spin vector dot products in the equal-mass, maximally spinning
case. These simulations were performed on Graphics Processing Units, and we
demonstrate a speed-up of a factor 50 over a more conventional CPU
implementation.
"
131,"Experimental Performances Analysis of Load Balancing Algorithms in IEEE
  802.11","  In IEEE 802.11, load balancing algorithms (LBA) consider only the associated
stations to balance the load of the available access points (APs). However,
although the APs are balanced, it causes a bad situation if the AP has a lower
signal length (SNR) less than the neighbor APs. So, balance the load and
associate one mobile station to an access point without care about the signal
to noise ratio (SNR) of the AP cause possibly an unforeseen QoS, such as the
bit rate, the end to end delay, the packet loss. In this way, we study an
improvement load balancing algorithm with SNR integration at the selection
policy.
"
132,"Statistical End-to-end Performance Bounds for Networks under Long Memory
  FBM Cross Traffic","  Fractional Brownian motion (fBm) emerged as a useful model for self-similar
and long-range dependent Internet traffic. Approximate performance measures are
known from large deviations theory for single queuing systems with fBm through
traffic. In this paper we derive end-to-end performance bounds for a through
flow in a network of tandem queues under fBm cross traffic. To this end, we
prove a rigorous sample path envelope for fBm that complements previous
approximate results. We find that both approaches agree in their outcome that
overflow probabilities for fBm traffic have a Weibullian tail. We employ the
sample path envelope and the concept of leftover service curves to model the
remaining service after scheduling fBm cross traffic at a system. Using
composition results for tandem systems from the stochastic network calculus we
derive end-to-end statistical performance bounds for individual flows in
networks under fBm cross traffic. We discover that these bounds grow in O(n
(log n)^(1/(2-2H))) for n systems in series where H is the Hurst parameter of
the fBm cross traffic. We show numerical results on the impact of the
variability and the correlation of fBm traffic on network performance.
"
133,Teaching an Old Elephant New Tricks,"  In recent years, column stores (or C-stores for short) have emerged as a
novel approach to deal with read-mostly data warehousing applications.
Experimental evidence suggests that, for certain types of queries, the new
features of C-stores result in orders of magnitude improvement over traditional
relational engines. At the same time, some C-store proponents argue that
C-stores are fundamentally different from traditional engines, and therefore
their benefits cannot be incorporated into a relational engine short of a
complete rewrite. In this paper we challenge this claim and show that many of
the benefits of C-stores can indeed be simulated in traditional engines with no
changes whatsoever. We then identify some limitations of our ?pure-simulation?
approach for the case of more complex queries. Finally, we predict that
traditional relational engines will eventually leverage most of the benefits of
C-stores natively, as is currently happening in other domains such as XML data.
"
134,Visualizing the robustness of query execution,"  In database query processing, actual run-time conditions (e.g., actual
selectivities and actual available memory) very often differ from compile-time
expectations of run-time conditions (e.g., estimated predicate selectivities
and anticipated memory availability). Robustness of query processing can be
defined as the ability to handle unexpected conditions. Robustness of query
execution, specifically, can be defined as the ability to process a specific
plan efficiently in an unexpected condition. We focus on query execution
(run-time), ignoring query optimization (compile-time), in order to complement
existing research and to explore untapped potential for improved robustness in
database query processing.
  One of our initial steps has been to devise diagrams or maps that show how
well plans perform in the face of varying run-time conditions and how
gracefully a system's query architecture, operators, and their implementation
degrade in the face of adverse conditions. In this paper, we show several kinds
of diagrams with data from three real systems and report on what we have
learned both about these visualization techniques and about the three database
systems
"
135,uFLIP: Understanding Flash IO Patterns,"  Does the advent of flash devices constitute a radical change for secondary
storage? How should database systems adapt to this new form of secondary
storage? Before we can answer these questions, we need to fully understand the
performance characteristics of flash devices. More specifically, we want to
establish what kind of IOs should be favored (or avoided) when designing
algorithms and architectures for flash-based systems. In this paper, we focus
on flash IO patterns, that capture relevant distribution of IOs in time and
space, and our goal is to quantify their performance. We define uFLIP, a
benchmark for measuring the response time of flash IO patterns. We also present
a benchmarking methodology which takes into account the particular
characteristics of flash devices. Finally, we present the results obtained by
measuring eleven flash devices, and derive a set of design hints that should
drive the development of flash-based systems on current devices.
"
136,"Energy Efficiency: The New Holy Grail of Data Management Systems
  Research","  Energy costs are quickly rising in large-scale data centers and are soon
projected to overtake the cost of hardware. As a result, data center operators
have recently started turning into using more energy-friendly hardware. Despite
the growing body of research in power management techniques, there has been
little work to date on energy efficiency from a data management software
perspective.
  In this paper, we argue that hardware-only approaches are only part of the
solution, and that data management software will be key in optimizing for
energy efficiency. We discuss the problems arising from growing energy use in
data centers and the trends that point to an increasing set of opportunities
for software-level optimizations. Using two simple experiments, we illustrate
the potential of such optimizations, and, motivated by these examples, we
discuss general approaches for reducing energy waste. Lastly, we point out
existing places within database systems that are promising for
energy-efficiency optimizations and urge the data management systems community
to shift focus from performance-oriented research to energy-efficient
computing.
"
137,"A Lightweight Distributed Solution to Content Replication in Mobile
  Networks","  Performance and reliability of content access in mobile networks is
conditioned by the number and location of content replicas deployed at the
network nodes. Facility location theory has been the traditional, centralized
approach to study content replication: computing the number and placement of
replicas in a network can be cast as an uncapacitated facility location
problem. The endeavour of this work is to design a distributed, lightweight
solution to the above joint optimization problem, while taking into account the
network dynamics. In particular, we devise a mechanism that lets nodes share
the burden of storing and providing content, so as to achieve load balancing,
and decide whether to replicate or drop the information so as to adapt to a
dynamic content demand and time-varying topology. We evaluate our mechanism
through simulation, by exploring a wide range of settings and studying
realistic content access mechanisms that go beyond the traditional
assumptionmatching demand points to their closest content replica. Results show
that our mechanism, which uses local measurements only, is: (i) extremely
precise in approximating an optimal solution to content placement and
replication; (ii) robust against network mobility; (iii) flexible in
accommodating various content access patterns, including variation in time and
space of the content demand.
"
138,"Simulation of Resource Usage in Parallel Evolutionary Peptide
  Optimization using JavaSpaces Technology","  Peptide Optimization is a highly complex problem and it takes very long time
of computation. This optimization process uses many software applications in a
cluster running GNU/Linux Operating System that perform special tasks. The
application to organize the whole optimization process had been already
developed, namely SEPP (System for Evolutionary Pareto Optimization of
Peptides/Polymers). A single peptide optimization takes a lot of computation
time to produce a certain number of individuals. However, it can be accelerated
by increasing the degree of parallelism as well as the number of nodes
(processors) in the cluster. In this master thesis, I build a model simulating
the interplay of the programs so that the usage of each resource (processor)
can be determined and also the approximated time needed for the overall
optimization process. There are two Evolutionary Algorithms that could be used
in the optimization, namely Generation-based and Steady-state Evolutionary
Algorithm. The results of each Evolutionary Algorithm are shown based on the
simulations. Moreover, the results are also compared by using different
parameters (the degree of parallelism and the number of processors) in the
simulation to give an overview of the advantages and the disadvantages of the
algorithms in terms of computation time and resource usage. The model is built
up using JavaSpaces Technology.
"
139,Capacity of Large-scale CSMA Wireless Networks,"  In the literature, asymptotic studies of multi-hop wireless network capacity
often consider only centralized and deterministic TDMA (time-division
multi-access) coordination schemes. There have been fewer studies of the
asymptotic capacity of large-scale wireless networks based on CSMA
(carrier-sensing multi-access), which schedules transmissions in a distributed
and random manner. With the rapid and widespread adoption of CSMA technology, a
critical question is that whether CSMA networks can be as scalable as TDMA
networks. To answer this question and explore the capacity of CSMA networks, we
first formulate the models of CSMA protocols to take into account the unique
CSMA characteristics not captured by existing interference models in the
literature. These CSMA models determine the feasible states, and consequently
the capacity of CSMA networks. We then study the throughput efficiency of CSMA
scheduling as compared to TDMA. Finally, we tune the CSMA parameters so as to
maximize the throughput to the optimal order. As a result, we show that CSMA
can achieve throughput as $\Omega(\frac{1}{\sqrt{n}})$, the same order as
optimal centralized TDMA, on uniform random networks. Our CSMA scheme makes use
of an efficient backbone-peripheral routing scheme and a careful design of dual
carrier-sensing and dual channel scheme. We also address the implementation
issues of our CSMA scheme.
"
140,"Analytical Models for Energy Consumption in Infrastructure WLAN STAs
  Carrying TCP Traffic","  We develop analytical models for estimating the energy spent by stations
(STAs) in infrastructure WLANs when performing TCP controlled file downloads.
We focus on the energy spent in radio communication when the STAs are in the
Continuously Active Mode (CAM), or in the static Power Save Mode (PSM). Our
approach is to develop accurate models for obtaining the fraction of times the
STA radios spend in idling, receiving and transmitting. We discuss two traffic
models for each mode of operation: (i) each STA performs one large file
download, and (ii) the STAs perform short file transfers. We evaluate the rate
of STA energy expenditure with long file downloads, and show that static PSM is
worse than just using CAM. For short file downloads we compute the number of
file downloads that can be completed with given battery capacity, and show that
PSM performs better than CAM for this case. We provide a validation of our
analytical models using the NS-2 simulator. In contrast to earlier work on
analytical modeling of PSM, our models that capture the details of the
interactions between the 802.11 MAC in PSM and certain aspects of TCP.
"
141,"Characteristics of multithreading models for high-performance IO driven
  network applications","  In a technological landscape that is quickly moving toward dense multi-CPU
and multi-core computer systems, where using multithreading is an increasingly
popular application design decision, it is important to choose a proper model
for distributing tasks across multiple threads that will result in the best
efficiency for the application and the system as a whole. The work described in
this paper creates, implements and evaluates various models of distributing
tasks to CPU threads and investigates their characteristics for use in modern
high-performance network servers. The results presented here comprise a roadmap
of models for building multithreaded server applications for modern server
hardware and Unix-like operating systems.
"
142,Criticisms of modelling packet traffic using long-range dependence,"  This paper criticises the notion that long-range dependence is an important
contributor to the queuing behaviour of real Internet traffic. The idea is
questioned in two different ways. Firstly, a class of models used to simulate
Internet traffic is shown to have important theoretical flaws. It is shown that
this behaviour is inconsistent with the behaviour of real traffic traces.
Secondly, the notion that long-range correlations significantly affects the
queuing performance of traffic is investigated by destroying those correlations
in real traffic traces (by reordering). It is shown that the longer ranges of
correlations are not important except in one case with an extremely high load.
"
143,"An Analysis of Energy Consumption on ACK plus Rate Packet in Rate Based
  Transport Protocol","  Rate based transport protocol determines the rate of data transmission
between the sender and receiver and then sends the data according to that rate.
To notify the rate to the sender, the receiver sends ACKplusRate packet based
on epoch timer expiry. In this paper, through detailed arguments and simulation
it is shown that the transmission of ACKplusRate packet based on epoch timer
expiry consumes more energy in network with low mobility. To overcome this
problem, a new technique called Dynamic Rate Feedback (DRF) is proposed. DRF
sends ACKplusRate whenever there is a change in rate of (plus or minus) 25
percent than the previous rate. Based on ns2 simulation DRF is compared with a
reliable transport protocol for ad hoc network (ATP)
"
144,"Performance Evaluation of Wimax Physical Layer under Adaptive Modulation
  Techniques and Communication Channels","  Wimax (Worldwide Interoperability for Microwave Access) is a promising
technology which can offer high speed voice, video and data service up to the
customer end. The aim of this paper is the performance evaluation of an Wimax
system under different combinations of digital modulation (BPSK, QPSK, 4 QAM
and 16 QAM) and different communication channels AWGN and fading channels
(Rayleigh and Rician). And the Wimax system incorporates Reed Solomon (RS)
encoder with Convolutional encoder with half and two third rated codes in FEC
channel coding. The simulation results of estimated Bit Error Rate (BER)
displays that the implementation of interleaved RS code (255, 239, 8) with two
third rated Convolutional code under BPSK modulation technique is highly
effective to combat in the Wimax communication system. To complete this
performance analysis in Wimax based systems, a segment of audio signal is used
for analysis. The transmitted audio message is found to have retrieved
effectively under noisy situation.
"
145,On Metric Skyline Processing by PM-tree,"  The task of similarity search in multimedia databases is usually accomplished
by range or k nearest neighbor queries. However, the expressing power of these
""single-example"" queries fails when the user's delicate query intent is not
available as a single example. Recently, the well-known skyline operator was
reused in metric similarity search as a ""multi-example"" query type. When
applied on a multi-dimensional database (i.e., on a multi-attribute table), the
traditional skyline operator selects all database objects that are not
dominated by other objects. The metric skyline query adopts the skyline
operator such that the multiple attributes are represented by distances
(similarities) to multiple query examples. Hence, we can view the metric
skyline as a set of representative database objects which are as similar to all
the examples as possible and, simultaneously, are semantically distinct. In
this paper we propose a technique of processing the metric skyline query by use
of PM-tree, while we show that our technique significantly outperforms the
original M-tree based implementation in both time and space costs. In
experiments we also evaluate the partial metric skyline processing, where only
a controlled number of skyline objects is retrieved.
"
146,High availability using virtualization,"  High availability has always been one of the main problems for a data center.
Till now high availability was achieved by host per host redundancy, a highly
expensive method in terms of hardware and human costs. A new approach to the
problem can be offered by virtualization. Using virtualization, it is possible
to achieve a redundancy system for all the services running on a data center.
This new approach to high availability allows to share the running virtual
machines over the servers up and running, by exploiting the features of the
virtualization layer: start, stop and move virtual machines between physical
hosts. The system (3RC) is based on a finite state machine with hysteresis,
providing the possibility to restart each virtual machine over any physical
host, or reinstall it from scratch. A complete infrastructure has been
developed to install operating system and middleware in a few minutes. To
virtualize the main servers of a data center, a new procedure has been
developed to migrate physical to virtual hosts. The whole Grid data center
SNS-PISA is running at the moment in virtual environment under the high
availability system. As extension of the 3RC architecture, several storage
solutions have been tested to store and centralize all the virtual disks, from
NAS to SAN, to grant data safety and access from everywhere. Exploiting
virtualization and ability to automatically reinstall a host, we provide a sort
of host on-demand, where the action on a virtual machine is performed only when
a disaster occurs.
"
147,Scalable Distributed-Memory External Sorting,"  We engineer algorithms for sorting huge data sets on massively parallel
machines. The algorithms are based on the multiway merging paradigm. We first
outline an algorithm whose I/O requirement is close to a lower bound. Thus, in
contrast to naive implementations of multiway merging and all other approaches
known to us, the algorithm works with just two passes over the data even for
the largest conceivable inputs. A second algorithm reduces communication
overhead and uses more conventional specifications of the result at the cost of
slightly increased I/O requirements. An implementation wins the well known
sorting benchmark in several categories and by a large margin over its
competitors.
"
148,"Performance limitations for sparse matrix-vector multiplications on
  current multicore environments","  The increasing importance of multicore processors calls for a reevaluation of
established numerical algorithms in view of their ability to profit from this
new hardware concept. In order to optimize the existent algorithms, a detailed
knowledge of the different performance-limiting factors is mandatory. In this
contribution we investigate sparse matrix-vector multiplication, which is the
dominant operation in many sparse eigenvalue solvers. Two conceptually
different storage schemes and computational kernels have been conceived in the
past to target cache-based and vector architectures, respectively. Starting
from a series of microbenchmarks we apply the gained insight on optimized
sparse MVM implementations, whose serial and OpenMP-parallel performance we
review on state-of-the-art multicore systems.
"
149,"Multi-core architectures: Complexities of performance prediction and the
  impact of cache topology","  The balance metric is a simple approach to estimate the performance of
bandwidth-limited loop kernels. However, applying the method to in-cache
situations and modern multi-core architectures yields unsatisfactory results.
This paper analyzes the in uence of cache hierarchy design on performance
predictions for bandwidth-limited loop kernels on current mainstream
processors. We present a diagnostic model with improved predictive power,
correcting the limitations of the simple balance metric. The importance of code
execution overhead even in bandwidth-bound situations is emphasized. Finally we
analyze the impact of synchronization overhead on multi-threaded performance
with a special emphasis on the in uence of cache topology.
"
150,Undecidability of performance equivalence of Petri nets,"  We investigate bisimulation equivalence on Petri nets under durational
semantics. Our motivation was to verify the conjecture that in durational
setting, the bisimulation equivalence checking problem becomes more tractable
than in ordinary setting (which is the case, e.g., over communication-free
nets). We disprove this conjecture in three of four proposed variants of
durational semantics. The fourth variant remains an intriguing open problem.
"
151,"Routing Technique Based on Clustering for Data Duplication Prevention in
  Wireless Sensor Network","  Wireless Sensor Networks is important to nodes energy consumption for long
activity of sensor nodes because nodes that compose sensor network are small
size, and battery capacity is limited. For energy consumption decrease of
sensor nodes, sensor networks routing technique is divided by flat routing and
hierarchical routing technique. Specially, hierarchical routing technique is
energy efficient routing protocol to pare down energy consumption of whole
sensor nodes and to scatter energy consumption of sensor nodes by forming
cluster and communicating with cluster head. but though hierarchical routing
technique based on clustering is advantage more than flat routing technique,
this is not used for reason that is not realistic. The reason that is not
realistic is because hierarchical routing technique does not consider data
transmission radius of sensor node in actually. so this paper propose realistic
routing technique base on clustering.
"
152,AES Implementation and Performance Evaluation on 8-bit Microcontrollers,"  The sensor network is a network technique for the implementation of
Ubiquitous computing environment. It is wireless network environment that
consists of the many sensors of lightweight and low power. Though sensor
network provides various capabilities, it is unable to ensure the secure
authentication between nodes. Eventually it causes the losing reliability of
the entire network and many secure problems. Therefore, encryption algorithm
for the implementation of reliable sensor network environments is required to
the applicable sensor network. In this paper, we proposed the solution of
reliable sensor network to analyze the communication efficiency through
measuring performance of AES encryption algorithm by plaintext size, and cost
of operation per hop according to the network scale.
"
153,GoS Proposal to Improve Trust and Delay of MPLS Flows for MCN Services,"  In this article, Guarantee of Service (GoS) is defined as a proposal to
improve the integration of Mission Critical Networking (MCN) services in the
Internet, analyzing the congestion impact on those privileged flows with high
requirements of trust and delay. Multiprotocol Label Switching (MPLS) is a
technology that offers flow differentiation and QoS in the Internet. Therefore,
in order to improve network performance in case of congested domains, GoS is
proposed as a technique that allows the local recovering of lost packets of
MPLS privileged flows. To fulfill the GoS requirements for integration of MCN
in MPLS, a minimum set of extensions to RSVPTE has been proposed to provide GoS
capable routes. Moreover, we have carried out an analytical study of GoS
scalability and a performance improvement analysis by means of simulations.
"
154,"SOAP Serialization Performance Enhancement, Design And Implementation Of
  A Middleware","  The most straightforward way to improve performance of any system is to
define the bottlenecks and think of ways to remove them. Web services are the
inseparable part of any web application, as a result enhancing performance of
web services will have a great effect on the overall performance of the system.
The most widely used communication protocol in the web services model, SOAP, is
a simple protocol for the exchange of messages. The serialization of large SOAP
responses is a major performance bottleneck in a SOAP message exchange.
Clearly, some web servers can expect to receive many similar messages for a
particular web service as they share the same signature. The idea behind this
paper is to avoid the redundant serialization stage of SOAP responses for
request which have the same call parameters. The technique exploits the
similarities between call parameters to improve web service Response Time by
avoiding redundant serialization of the same response with the help of a
middleware running on top of web server. The middleware will maintain a trie of
incoming parameters for every set of current requests. This way request
processing and serialization of the response of same requests will be done only
once. In a nutshell, to serialize only the different responses is the simplest
way to avoid extra work done by a serializer. It might worth noting that
although our approach is to utilize the exact repeating portion parameters, the
middleware can be configured to apply changes made to the result set of
response to the serialized response being maintained in a trie to generate
valid results.
"
155,Throughput Limits of IEEE 802.11 and IEEE 802.15.3,"  IEEE 802.11 and IEEE 802.15.3 are wireless standards originally designed for
wireless local area network (WLAN) and wireless personal area network (WPAN).
This paper studies MAC throughput analysis of both standards. We present a
comparative analysis of both standards in terms of MAC throughput and bandwidth
efficiency. Numerical results show that the performance of IEEE 802.15.3
transcends IEEE 802.11 in all cases.
"
156,"Vertical partitioning of relational OLTP databases using integer
  programming","  A way to optimize performance of relational row store databases is to reduce
the row widths by vertically partitioning tables into table fractions in order
to minimize the number of irrelevant columns/attributes read by each
transaction. This paper considers vertical partitioning algorithms for
relational row-store OLTP databases with an H-store-like architecture, meaning
that we would like to maximize the number of single-sited transactions. We
present a model for the vertical partitioning problem that, given a schema
together with a vertical partitioning and a workload, estimates the costs
(bytes read/written by storage layer access methods and bytes transferred
between sites) of evaluating the workload on the given partitioning. The cost
model allows for arbitrarily prioritizing load balancing of sites vs. total
cost minimization. We show that finding a minimum-cost vertical partitioning in
this model is NP-hard and present two algorithms returning solutions in which
single-sitedness of read queries is preserved while allowing column replication
(which may allow a drastically reduced cost compared to disjoint partitioning).
The first algorithm is a quadratic integer program that finds optimal
minimum-cost solutions with respect to the model, and the second algorithm is a
more scalable heuristic based on simulated annealing. Experiments show that the
algorithms can reduce the cost of the model objective by 37% when applied to
the TPC-C benchmark and the heuristic is shown to obtain solutions with cost
close to the ones found using the quadratic program.
"
157,Global communications in multiprocessor simulations of flames,"  In this paper we investigate performance of global communications in a
particular parallel code. The code simulates dynamics of expansion of premixed
spherical flames using an asymptotic model of Sivashinsky type and a spectral
numerical algorithm. As a result, the code heavily relies on global all-to-all
interprocessor communications implementing transposition of the distributed
data array in which numerical solution to the problem is stored. This global
data interdependence makes interprocessor connectivity of the HPC system as
important as the floating-point power of the processors of which the system is
built. Our experiments show that efficient numerical simulation of this
particular model, with global data interdependence, on modern HPC systems is
possible. Prospects of performance of more sophisticated models of flame
dynamics are analysed as well.
"
158,Delay Bounds for Networks with Heavy-Tailed and Self-Similar Traffic,"  We provide upper bounds on the end-to-end backlog and delay in a network with
heavy-tailed and self-similar traffic. The analysis follows a network calculus
approach where traffic is characterized by envelope functions and service is
described by service curves. A key contribution of this paper is the derivation
of a probabilistic sample path bound for heavy-tailed self-similar arrival
processes, which is enabled by a suitable envelope characterization, referred
to as `htss envelope'. We derive a heavy-tailed service curve for an entire
network path when the service at each node on the path is characterized by
heavy-tailed service curves. We obtain backlog and delay bounds for traffic
that is characterized by an htss envelope and receives service given by a
heavy-tailed service curve. The derived performance bounds are non-asymptotic
in that they do not assume a steady-state, large buffer, or many sources
regime. We also explore the scale of growth of delays as a function of the
length of the path. The appendix contains an analysis for self-similar traffic
with a Gaussian tail distribution.
"
159,"Extending Firewall Session Table to Accelerate NAT, QoS Classification
  and Routing","  security and QoS are the two most precious objectives for network systems to
be attained. Unfortunately, they are in conflict, while QoS tries to minimize
processing delay, strong security protection requires more processing time and
cause packet delay. This article is a step towards resolving this conflict by
extending the firewall session table to accelerate NAT, QoS classification, and
routing processing time while providing the same level of security protection.
Index Terms ? stateful packet filtering; firewall; session/state table; QoS;
NAT; Routing.
"
160,Q-ESP: a QoS-compliant Security Protocol to enrich IPSec Framework,"  IPSec is a protocol that allows to make secure connections between branch
offices and allows secure VPN accesses. However, the efforts to improve IPSec
are still under way; one aspect of this improvement is to take Quality of
Service (QoS) requirements into account. QoS is the ability of the network to
provide a service at an assured service level while optimizing the global usage
of network resources. The QoS level that a flow receives depends on a six-bit
identifier in the IP header; the so-called Differentiated Services code point
(DSCP). Basically, Multi-Field classifiers classify a packet by inspecting
IP/TCP headers, to decide how the packet should be processed. The current IPSec
standard does hardly offer any guidance to do this, because the existing IPSec
ESP security protocol hides much of this information in its encrypted payloads,
preventing network control devices such as routers and switches from utilizing
this information in performing classification appropriately. To solve this
problem, we propose a QoS-friendly Encapsulated Security Payload (Q-ESP) as a
new IPSec security protocol that provides both security and QoS supports. We
also present our NetBSD kernel-based implementation as well as our evaluation
results of Q-ESP.
"
161,Efficient Local Unfolding with Ancestor Stacks,"  The most successful unfolding rules used nowadays in the partial evaluation
of logic programs are based on well quasi orders (wqo) applied over (covering)
ancestors, i.e., a subsequence of the atoms selected during a derivation.
Ancestor (sub)sequences are used to increase the specialization power of
unfolding while still guaranteeing termination and also to reduce the number of
atoms for which the wqo has to be checked. Unfortunately, maintaining the
structure of the ancestor relation during unfolding introduces significant
overhead. We propose an efficient, practical local unfolding rule based on the
notion of covering ancestors which can be used in combination with a wqo and
allows a stack-based implementation without losing any opportunities for
specialization. Using our technique, certain non-leftmost unfoldings are
allowed as long as local unfolding is performed, i.e., we cover depth-first
strategies.
"
162,Adaptive Scheduling of Data Paths using Uppaal Tiga,"  We apply Uppaal Tiga to automatically compute adaptive scheduling strategies
for an industrial case study dealing with a state-of-the-art image processing
pipeline of a printer. As far as we know, this is the first application of
timed automata technology to an industrial scheduling problem with uncertainty
in job arrivals.
"
163,Markovian Testing Equivalence and Exponentially Timed Internal Actions,"  In the theory of testing for Markovian processes developed so far,
exponentially timed internal actions are not admitted within processes. When
present, these actions cannot be abstracted away, because their execution takes
a nonzero amount of time and hence can be observed. On the other hand, they
must be carefully taken into account, in order not to equate processes that are
distinguishable from a timing viewpoint. In this paper, we recast the
definition of Markovian testing equivalence in the framework of a Markovian
process calculus including exponentially timed internal actions. Then, we show
that the resulting behavioral equivalence is a congruence, has a sound and
complete axiomatization, has a modal logic characterization, and can be decided
in polynomial time.
"
164,"Strong, Weak and Branching Bisimulation for Transition Systems and
  Markov Reward Chains: A Unifying Matrix Approach","  We first study labeled transition systems with explicit successful
termination. We establish the notions of strong, weak, and branching
bisimulation in terms of boolean matrix theory, introducing thus a novel and
powerful algebraic apparatus. Next we consider Markov reward chains which are
standardly presented in real matrix theory. By interpreting the obtained matrix
conditions for bisimulations in this setting, we automatically obtain the
definitions of strong, weak, and branching bisimulation for Markov reward
chains. The obtained strong and weak bisimulations are shown to coincide with
some existing notions, while the obtained branching bisimulation is new, but
its usefulness is questionable.
"
165,"Proceedings First Workshop on Quantitative Formal Methods: Theory and
  Applications","  This volume contains the papers presented at the 1st workshop on Quantitative
Formal Methods: Theory and Applications, which was held in Eindhoven on 3
November 2009 as part of the International Symposium on Formal Methods 2009.
This volume contains the final versions of all contributions accepted for
presentation at the workshop.
"
166,Sharp utilization thresholds for some real-time scheduling problems,"  Scheduling policies for real-time systems exhibit threshold behavior that is
related to the utilization of the task set they schedule, and in some cases
this threshold is sharp. For the rate monotonic scheduling policy, we show that
periodic workload with utilization less than a threshold $U_{RM}^{*}$ can be
scheduled almost surely and that all workload with utilization greater than
$U_{RM}^{*}$ is almost surely not schedulable. We study such sharp threshold
behavior in the context of processor scheduling using static task priorities,
not only for periodic real-time tasks but for aperiodic real-time tasks as
well. The notion of a utilization threshold provides a simple schedulability
test for most real-time applications. These results improve our understanding
of scheduling policies and provide an interesting characterization of the
typical behavior of policies. The threshold is sharp (small deviations around
the threshold cause schedulability, as a property, to appear or disappear) for
most policies; this is a happy consequence that can be used to address the
limitations of existing utilization-based tests for schedulability. We
demonstrate the use of such an approach for balancing power consumption with
the need to meet deadlines in web servers.
"
167,"Multicore-aware parallel temporal blocking of stencil codes for shared
  and distributed memory","  New algorithms and optimization techniques are needed to balance the
accelerating trend towards bandwidth-starved multicore chips. It is well known
that the performance of stencil codes can be improved by temporal blocking,
lessening the pressure on the memory interface. We introduce a new pipelined
approach that makes explicit use of shared caches in multicore environments and
minimizes synchronization and boundary overhead. For clusters of shared-memory
nodes we demonstrate how temporal blocking can be employed successfully in a
hybrid shared/distributed-memory environment.
"
168,Tiling for Performance Tuning on Different Models of GPUs,"  The strategy of using CUDA-compatible GPUs as a parallel computation solution
to improve the performance of programs has been more and more widely approved
during the last two years since the CUDA platform was released. Its benefit
extends from the graphic domain to many other computationally intensive
domains. Tiling, as the most general and important technique, is widely used
for optimization in CUDA programs. New models of GPUs with better compute
capabilities have, however, been released, new versions of CUDA SDKs were also
released. These updated compute capabilities must to be considered when
optimizing using the tiling technique. In this paper, we implement image
interpolation algorithms as a test case to discuss how different tiling
strategies affect the program's performance. We especially focus on how the
different models of GPUs affect the tiling's effectiveness by executing the
same program on two different models of GPUs equipped testing platforms. The
results demonstrate that an optimized tiling strategy on one GPU model is not
always a good solution when execute on other GPU models, especially when some
external conditions were changed.
"
169,"OMI4papps: Optimisation, Modelling and Implementation for Highly
  Parallel Applications","  This article reports on first results of the KONWIHR-II project OMI4papps at
the Leibniz Supercomputing Centre (LRZ). The first part describes Apex-MAP, a
tunable synthetic benchmark designed to simulate the performance of typical
scientific applications. Apex-MAP mimics common memory access patterns and
different computational intensity of scientific codes. An approach for
modelling LRZ's application mix is given whichh makes use of performance
counter measurements of real applications running on ""HLRB II"", an SGI Altix
system based on 9728 Intel Montecito dual-cores.
  The second part will show how the Apex-MAP benchmark could be used to
simulate the performance of two mathematical kernels frequently used in
scientific applications: a dense matrix-matrix multiplication and a sparse
matrix-vector multiplication. The performance of both kernels has been
intensively studied on x86 cores and hardware accelerators. We will compare the
predicted performance with measured data to validate our Apex-MAP approach.
"
170,RapidMind: Portability across Architectures and its Limitations,"  Recently, hybrid architectures using accelerators like GPGPUs or the Cell
processor have gained much interest in the HPC community. The RapidMind
Multi-Core Development Platform is a programming environment that allows
generating code which is able to seamlessly run on hardware accelerators like
GPUs or the Cell processor and multicore CPUs both from AMD and Intel. This
paper describes the ports of three mathematical kernels to RapidMind which are
chosen as synthetic benchmarks and representatives of scientific codes.
Performance of these kernels has been measured on various RapidMind backends
(cuda, cell and x86) and compared to other hardware-specific implementations
(using CUDA, Cell SDK and Intel MKL). The results give an insight in the degree
of portability of RapidMind code and code performance across different
architectures.
"
171,Classifying Application Phases in Asymmetric Chip Multiprocessors,"  In present study, in order to improve the performance and reduce the amount
of power which is dissipated in heterogeneous multicore processors, the ability
of detecting the program execution phases is investigated. The programs
execution intervals have been classified in different phases based on their
throughput and the utilization of the cores. The results of implementing the
phase detection technique are investigated on a single core processor and also
on a multicore processor. To minimize the profiling overhead, an algorithm for
the dynamic adjustment of the profiling intervals is presented. It is based on
the behavior of the program and reduces the profiling overhead more than three
fold. The results are obtained from executing multiprocessor benchmarks on a
given processor. In order to show the program phases clearly, throughput and
utilization of execution intervals are presented on a scatter plot. The results
are presented for both fixed and variable intervals.
"
172,On the Model Transform in Stochastic Network Calculus,"  Stochastic network calculus requires special care in the search of proper
stochastic traffic arrival models and stochastic service models. Tradeoff must
be considered between the feasibility for the analysis of performance bounds,
the usefulness of performance bounds, and the ease of their numerical
calculation. In theory, transform between different traffic arrival models and
transform between different service models are possible. Nevertheless, the
impact of the model transform on performance bounds has not been thoroughly
investigated. This paper is to investigate the effect of the model transform
and to provide practical guidance in the model selection in stochastic network
calculus.
"
173,Towards Transactional Load over XtreemFS,"  We propose using trace-based assessment of the performance of distributed
file systems (DFS) under transactional IO load. The assessment includes
simulations and experiments using the IO traces. Our experiments suggest that
DFS, and specifically XtreemFS have a good potential to support transactional
IO load in distributed environments: they demonstrate good performance, high
availability and scalability, while at the same time opening the way to TCO
reduction.
"
174,"Bayesian inference for queueing networks and modeling of internet
  services","  Modern Internet services, such as those at Google, Yahoo!, and Amazon, handle
billions of requests per day on clusters of thousands of computers. Because
these services operate under strict performance requirements, a statistical
understanding of their performance is of great practical interest. Such
services are modeled by networks of queues, where each queue models one of the
computers in the system. A key challenge is that the data are incomplete,
because recording detailed information about every request to a heavily used
system can require unacceptable overhead. In this paper we develop a Bayesian
perspective on queueing models in which the arrival and departure times that
are not observed are treated as latent variables. Underlying this viewpoint is
the observation that a queueing model defines a deterministic transformation
between the data and a set of independent variables called the service times.
With this viewpoint in hand, we sample from the posterior distribution over
missing data and model parameters using Markov chain Monte Carlo. We evaluate
our framework on data from a benchmark Web application. We also present a
simple technique for selection among nested queueing models. We are unaware of
any previous work that considers inference in networks of queues in the
presence of missing data.
"
175,Fault Tolerant Real Time Systems,"  Real time systems are systems in which there is a commitment for timely
response by the computer to external stimuli. Real time applications have to
function correctly even in presence of faults. Fault tolerance can be achieved
by either hardware or software or time redundancy. Safety-critical applications
have strict time and cost constraints, which means that not only faults have to
be tolerated but also the constraints should be satisfied. Deadline scheduling
means that the taskwith the earliest required response time is processed. The
most common scheduling algorithms are :Rate Monotonic(RM) and Earliest deadline
first(EDF).This paper deals with the interaction between the fault tolerant
strategy and the EDF real time scheduling strategy.
"
176,A Multi-Stage CUDA Kernel for Floyd-Warshall,"  We present a new implementation of the Floyd-Warshall All-Pairs Shortest
Paths algorithm on CUDA. Our algorithm runs approximately 5 times faster than
the previously best reported algorithm. In order to achieve this speedup, we
applied a new technique to reduce usage of on-chip shared memory and allow the
CUDA scheduler to more effectively hide instruction latency.
"
177,Constraint solvers: An empirical evaluation of design decisions,"  This paper presents an evaluation of the design decisions made in four
state-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To
assess the impact of design decisions, instances of the five problem classes
n-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete
Block Design are modelled and solved with each solver. The results of the
experiments are not meant to give an indication of the performance of a solver,
but rather investigate what influence the choice of algorithms and data
structures has.
  The analysis of the impact of the design decisions focuses on the different
ways of memory management, behaviour with increasing problem size, and
specialised algorithms for specific types of variables. It also briefly
considers other, less significant decisions.
"
178,"The ""Hot Potato"" Case: Challenges in Multiplayer Pervasive Games Based
  on Ad hoc Mobile Sensor Networks and the Experimental Evaluation of a
  Prototype Game","  In this work, we discuss multiplayer pervasive games that rely on the use of
ad hoc mobile sensor networks. The unique feature in such games is that players
interact with each other and their surrounding environment by using movement
and presence as a means of performing game-related actions, utilizing sensor
devices. We discuss the fundamental issues and challenges related to these type
of games and the scenarios associated with them. We also present and evaluate
an example of such a game, called the ""Hot Potato"", developed using the Sun
SPOT hardware platform. We provide a set of experimental results, so as to both
evaluate our implementation and also to identify issues that arise in pervasive
games which utilize sensor network nodes, which show that there is great
potential in this type of games.
"
179,A Performance Study of GA and LSH in Multiprocessor Job Scheduling,"  Multiprocessor task scheduling is an important and computationally difficult
problem. This paper proposes a comparison study of genetic algorithm and list
scheduling algorithm. Both algorithms are naturally parallelizable but have
heavy data dependencies. Based on experimental results, this paper presents a
detailed analysis of the scalability, advantages and disadvantages of each
algorithm. Multiprocessors have emerged as a powerful computing means for
running real-time applications, especially where a uni-processor system would
not be sufficient enough to execute all the tasks. The high performance and
reliability of multiprocessors have made them a powerful computing resource.
Such computing environment requires an efficient algorithm to determine when
and on which processor a given task should execute. In multiprocessor systems,
an efficient scheduling of a parallel program onto the processors that
minimizes the entire execution time is vital for achieving a high performance.
This scheduling problem is known to be NP- Hard. In multiprocessor scheduling
problem, a given program is to be scheduled in a given multiprocessor system
such that the program's execution time is minimized. The last job must be
completed as early as possible. Genetic algorithm (GA) is one of the widely
used techniques for constrained optimization.
"
180,Performance Analysis of Software to Hardware Task Migration in Codesign,"  The complexity of multimedia applications in terms of intensity of
computation and heterogeneity of treated data led the designers to embark them
on multiprocessor systems on chip. The complexity of these systems on one hand
and the expectations of the consumers on the other hand complicate the
designers job to conceive and supply strong and successful systems in the
shortest deadlines. They have to explore the different solutions of the design
space and estimate their performances in order to deduce the solution that
respects their design constraints. In this context, we propose the modeling of
one of the design space possible solutions: the software to hardware task
migration. This modeling exploits the synchronous dataflow graphs to take into
account the different migration impacts and estimate their performances in
terms of throughput.
"
181,"Performance Evaluation of Unicast and Broadcast Mobile Ad hoc Network
  Routing Protocols","  Efficient routing mechanism is a challenging issue for group oriented
computing in Mobile Ad Hoc Networks (MANETs). The ability of MANETs to support
adequate Quality of Service (QoS) for group communication is limited by the
ability of the underlying ad-hoc routing protocols to provide consistent
behavior despite the dynamic properties of mobile computing devices. In MANET
QoS requirements can be quantified in terms of Packet Delivery Ratio (PDR),
Data Latency, Packet Loss Probability, Routing Overhead, Medium Access Control
(MAC) Overhead and Data Throughput etc. This paper presents an in depth study
of one to many and many to many communications in MANETs and provides a
comparative performance evaluation of unicast and broadcast routing protocols.
Dynamic Source Routing protocol (DSR) is used as unicast protocol and BCAST is
used to represent broadcast protocol. The performance differentials are
analyzed using ns2 network simulator varying multicast group size (number of
data senders and data receivers). Both protocols are simulated with identical
traffic loads and mobility models. Simulation result shows that BCAST performs
better than DSR in most cases.
"
182,Ahb Compatible DDR Sdram Controller Ip Core for Arm Based Soc,"  DDR SDRAM is similar in function to the regular SDRAM but doubles the
bandwidth of the memory by transferring data on both edges of the clock cycles.
DDR SDRAM most commonly used in various embedded application like networking,
image or video processing, Laptops ete. Now a days many applications needs more
and more cheap and fast memory. Especially in the field of signal processing,
requires significant amount of memory. The most used type of dynamic memory for
that purpose is DDR SDRAM. For FPGA design the IC manufacturers are providing
commercial memory controller IP cores working only on their products. Main
disadvantage is the lack of memory access optimization for random memory access
patterns. The data path part of those controllers can be used free of charge.
This work propose an architecture of a DDR SDRAM controller, which takes
advantage of those available and well tested data paths and can be used for any
FPGA device or ASIC design.(5). In most of the SOC design, DDR SDRAM is
commonly used. ARM processor is widely used in SOCs; so that we focused to
implement AHB compatible DDR SDRAM controller suitable for ARM based SOC
design.
"
183,"Modeling the Probability of Failure on LDAP Binding Operations in
  Iplanet Web Proxy 3.6 Server","  This paper is devoted to the theoretical analysis of a problem derived from
interaction between two Iplanet products: Web Proxy Server and the Directory
Server. In particular, a probabilistic and stochastic-approximation model is
proposed to minimize the occurrence of LDAP connection failures in Iplanet Web
Proxy 3.6 Server. The proposed model serves not only to provide a
parameterization of the aforementioned phenomena, but also to provide
meaningful insights illustrating and supporting these theoretical results. In
addition, we shall also address practical considerations when estimating the
parameters of the proposed model from experimental data. Finally, we shall
provide some interesting results from real-world data collected from our
customers.
"
184,The Missing Piece Syndrome in Peer-to-Peer Communication,"  Typical protocols for peer-to-peer file sharing over the Internet divide
files to be shared into pieces. New peers strive to obtain a complete
collection of pieces from other peers and from a seed. In this paper we
investigate a problem that can occur if the seeding rate is not large enough.
The problem is that, even if the statistics of the system are symmetric in the
pieces, there can be symmetry breaking, with one piece becoming very rare. If
peers depart after obtaining a complete collection, they can tend to leave
before helping other peers receive the rare piece. Assuming that peers arrive
with no pieces, there is a single seed, random peer contacts are made, random
useful pieces are downloaded, and peers depart upon receiving the complete
file, the system is stable if the seeding rate (in pieces per time unit) is
greater than the arrival rate, and is unstable if the seeding rate is less than
the arrival rate. The result persists for any piece selection policy that
selects from among useful pieces, such as rarest first, and it persists with
the use of network coding.
"
185,Window-Based Greedy Contention Management for Transactional Memory,"  We consider greedy contention managers for transactional memory for M x N
execution windows of transactions with M threads and N transactions per thread.
Assuming that each transaction conflicts with at most C other transactions
inside the window, a trivial greedy contention manager can schedule them within
CN time. In this paper, we show that there are much better schedules. We
present and analyze two new randomized greedy contention management algorithms.
The first algorithm Offline-Greedy produces a schedule of length O(C + N
log(MN)) with high probability, and gives competitive ratio O(log(MN)) for C <=
N log(MN). The offline algorithm depends on knowing the conflict graph. The
second algorithm Online-Greedy produces a schedule of length O(C log(MN) + N
log^2(MN)) with high probability which is only a O(log(NM)) factor worse, but
does not require knowledge of the conflict graph. We also give an adaptive
version which achieves similar worst-case performance and C is determined on
the fly under execution. Our algorithms provide new tradeoffs for greedy
transaction scheduling that parameterize window sizes and transaction conflicts
within the window.
"
186,Automatic Performance Debugging of SPMD Parallel Programs,"  Automatic performance debugging of parallel applications usually involves two
steps: automatic detection of performance bottlenecks and uncovering their root
causes for performance optimization. Previous work fails to resolve this
challenging issue in several ways: first, several previous efforts automate
analysis processes, but present the results in a confined way that only
identifies performance problems with apriori knowledge; second, several tools
take exploratory or confirmatory data analysis to automatically discover
relevant performance data relationships. However, these efforts do not focus on
locating performance bottlenecks or uncovering their root causes. In this
paper, we design and implement an innovative system, AutoAnalyzer, to
automatically debug the performance problems of single program multi-data
(SPMD) parallel programs. Our system is unique in terms of two dimensions:
first, without any apriori knowledge, we automatically locate bottlenecks and
uncover their root causes for performance optimization; second, our method is
lightweight in terms of size of collected and analyzed performance data. Our
contribution is three-fold. First, we propose a set of simple performance
metrics to represent behavior of different processes of parallel programs, and
present two effective clustering and searching algorithms to locate
bottlenecks. Second, we propose to use the rough set algorithm to automatically
uncover the root causes of bottlenecks. Third, we design and implement the
AutoAnalyzer system, and use two production applications to verify the
effectiveness and correctness of our methods. According to the analysis results
of AutoAnalyzer, we optimize two parallel programs with performance
improvements by minimally 20% and maximally 170%.
"
187,"Precise Request Tracing and Performance Debugging for Multi-tier
  Services of Black Boxes","  As more and more multi-tier services are developed from commercial components
or heterogeneous middleware without the source code available, both developers
and administrators need a precise request tracing tool to help understand and
debug performance problems of large concurrent services of black boxes.
Previous work fails to resolve this issue in several ways: they either accept
the imprecision of probabilistic correlation methods, or rely on knowledge of
protocols to isolate requests in pursuit of tracing accuracy. This paper
introduces a tool named PreciseTracer to help debug performance problems of
multi-tier services of black boxes. Our contributions are two-fold: first, we
propose a precise request tracing algorithm for multi-tier services of black
boxes, which only uses application-independent knowledge; secondly, we present
a component activity graph abstraction to represent causal paths of requests
and facilitate end-to-end performance debugging. The low overhead and tolerance
of noise make PreciseTracer a promising tracing tool for using on production
systems.
"
188,PhoenixCloud: Provisioning Resources for Heterogeneous Cloud Workloads,"  As more and more service providers choose Cloud platforms, a resource
provider needs to provision resources and supporting runtime environments (REs)
for heterogeneous workloads in different scenarios. Previous work fails to
resolve this issue in several ways: (1) it fails to pay attention to diverse RE
requirements, and does not enable creating coordinated REs on demand; (2) few
work investigates coordinated resource provisioning for heterogeneous
workloads. In this paper, our contributions are three-fold: (1) we present an
RE agreement that expresses diverse RE requirements, and build an innovative
system PhoenixCloud that enables a resource provider to create REs on demand
according to RE agreements; (2) we propose two coordinated resource
provisioning solutions for heterogeneous workloads in two typical Cloud
scenarios: first, a large organization operates a private Cloud for two
heterogeneous workloads; second, a large organization or two service providers
running heterogeneous workloads revert to a public Cloud; and (3) A
comprehensive evaluation has been performed in experiments. For typical
workload traces of parallel batch jobs and Web services, our experiments show
that: a) In the first Cloud scenario, when the throughput is almost same like
that of a dedicated cluster system, our solution decreases the configuration
size of cluster by about 40%; b) in the second scenario, our solution decreases
not only the total resource consumption, but also the peak resource consumption
maximally to 31% with respect to that of EC2 + RightScale solution.
"
189,Decreasing log data of multi-tier services for effective request tracing,"  Previous work shows request tracing systems help understand and debug the
performance problems of multi-tier services. However, for large-scale data
centers, more than hundreds of thousands of service instances provide online
service at the same time. Previous work such as white-box or black box tracing
systems will produce large amount of log data, which would be correlated into
large quantities of causal paths for performance debugging. In this paper, we
propose an innovative algorithm to eliminate valueless logs of multitiers
services. Our experiment shows our method filters 84% valueless causal paths
and is promising to be used in large-scale data centers.
"
190,"In Cloud, Do MTC or HTC Service Providers Benefit from the Economies of
  Scale?","  In this paper, we intend to answer one key question to the success of cloud
computing: in cloud, do many task computing (MTC) or high throughput computing
(HTC) service providers, which offer the corresponding computing service to end
users, benefit from the economies of scale? Our research contributions are
three-fold: first, we propose an innovative usage model, called dynamic service
provision (DSP) model, for MTC or HTC service providers. In the DSP model, the
resource provider provides the service of creating and managing runtime
environments for MTC or HTC service providers, and consolidates heterogeneous
MTC or HTC workloads on the cloud platform; second, according to the DSP model,
we design and implement DawningCloud, which provides automatic management for
heterogeneous workloads; third, a comprehensive evaluation of DawningCloud has
been performed in an emulatation experiment. We found that for typical
workloads, in comparison with the previous two cloud solutions, DawningCloud
saves the resource consumption maximally by 46.4% (HTC) and 74.9% (MTC) for the
service providers, and saves the total resource consumption maximally by 29.7%
for the resource provider. At the same time, comparing with the traditional
solution that provides MTC or HTC services with dedicated systems, DawningCloud
is more cost-effective. To this end, we conclude that for typical MTC and HTC
workloads, on the cloud platform, MTC and HTC service providers and the
resource provider can benefit from the economies of scale.
"
191,Asynchronous Bounded Expected Delay Networks,"  The commonly used asynchronous bounded delay (ABD) network models assume a
fixed bound on message delay. We propose a probabilistic network model, called
asynchronous bounded expected delay (ABE) model. Instead of a strict bound, the
ABE model requires only a bound on the expected message delay. While the
conditions of ABD networks restrict the set of possible executions, in ABE
networks all asynchronous executions are possible, but executions with
extremely long delays are less probable. In contrast to ABD networks, ABE
networks cannot be synchronised efficiently. At the example of an election
algorithm, we show that the minimal assumptions of ABE networks are sufficient
for the development of efficient algorithms. For anonymous, unidirectional ABE
rings of known size N we devise a probabilistic leader election algorithm
having average message and time complexity O(N).
"
192,"A Rank Based Replacement Policy for Multimedia Server Cache Using
  Zipf-Like Law","  The cache replacement algorithm plays an important role in the overall
performance of Proxy-Server system. In this paper we have proposed VoD cache
memory replacement algorithm for a multimedia server system. We propose a Rank
based cache replacement policy to manage the cache space in individual proxy
server cache. Proposed replacement strategy incorporates in a simple way the
most important characteristics of the video and its accesses such as its size,
access frequency, recentness of the last access and the cost incurred while
transferring the requested video from the server to the proxy. We compare our
algorithm with some popular cache replacement algorithm using simulation. The
video objects are ranked based on the access trend by considering the factors
such as size, frequency and cost. Many studies have demonstrated that
Zipf's-like law can govern many features of the VoD and is used to describe the
popularity of the video. In this paper, we have designed a model, which ranks
the video on the basis of its popularity using the Zipf-like law. The video
with higher ranking is named ""hot"", while the video with lower ranking is named
""cold"". The result show that the proposed rank based algorithm improves cache
hit ratio, cache byte ratio and average request latencies compared to other
algorithms. Our experimental results indicate that Rank based cache replacement
algorithm outperforms LRU, LFU and Greedy Dual.
"
193,Measuring Bandwidth for Super Computer Workloads,"  Parallel computing plays a major role in almost all the fields from research
to major concern problem solving purposes. Many researches are till now
focusing towards the area of parallel processing. Nowadays it extends its usage
towards the end user application such as GPU as well as multi-core processor
development. The bandwidth measurement is essential for resource management and
for studying the various performance factors of the existing super computer
systems which will be helpful for better system utilization since super
computers are very few and their resources should be properly utilized. In this
paper the real workload trace of one of the super computers LANL is taken and
shown how the bandwidth is estimated with the given parameters.
"
194,On the stability of flow-aware CSMA,"  We consider a wireless network where each flow (instead of each link) runs
its own CSMA (Carrier Sense Multiple Access) algorithm. Specifically, each flow
attempts to access the radio channel after some random time and transmits a
packet if the channel is sensed idle. We prove that, unlike the standard CSMA
algorithm, this simple distributed access scheme is optimal in the sense that
the network is stable for all traffic intensities in the capacity region of the
network.
"
195,"High-Performance Physics Simulations Using Multi-Core CPUs and GPGPUs in
  a Volunteer Computing Context","  This paper presents two conceptually simple methods for parallelizing a
Parallel Tempering Monte Carlo simulation in a distributed volunteer computing
context, where computers belonging to the general public are used. The first
method uses conventional multi-threading. The second method uses CUDA, a
graphics card computing system. Parallel Tempering is described, and challenges
such as parallel random number generation and mapping of Monte Carlo chains to
different threads are explained. While conventional multi-threading on CPUs is
well-established, GPGPU programming techniques and technologies are still
developing and present several challenges, such as the effective use of a
relatively large number of threads. Having multiple chains in Parallel
Tempering allows parallelization in a manner that is similar to the serial
algorithm. Volunteer computing introduces important constraints to high
performance computing, and we show that both versions of the application are
able to adapt themselves to the varying and unpredictable computing resources
of volunteers' computers, while leaving the machines responsive enough to use.
We present experiments to show the scalable performance of these two
approaches, and indicate that the efficiency of the methods increases with
bigger problem sizes.
"
196,"Importance of Explicit Vectorization for CPU and GPU Software
  Performance","  Much of the current focus in high-performance computing is on
multi-threading, multi-computing, and graphics processing unit (GPU) computing.
However, vectorization and non-parallel optimization techniques, which can
often be employed additionally, are less frequently discussed. In this paper,
we present an analysis of several optimizations done on both central processing
unit (CPU) and GPU implementations of a particular computationally intensive
Metropolis Monte Carlo algorithm. Explicit vectorization on the CPU and the
equivalent, explicit memory coalescing, on the GPU are found to be critical to
achieving good performance of this algorithm in both environments. The
fully-optimized CPU version achieves a 9x to 12x speedup over the original CPU
version, in addition to speedup from multi-threading. This is 2x faster than
the fully-optimized GPU version.
"
197,On Memory Accelerated Signal Processing within Software Defined Radios,"  Since J. Mitola's work in 1992, Software Defined Radios (SDRs) have been
quite a hot topic in wireless systems research. Though many notable
achievements were reported in the field, the scarcity of computational power on
general purpose CPUs has always constrained their wide adoption in production
environments. If conveniently applied within an SDR context, classical concepts
known in computer science as space/time tradeoffs can be extremely helpful when
trying to mitigate this problem. Inspired by and building on those concepts,
this paper presents a novel SDR implementation technique which we call Memory
Acceleration (MA) that makes extensive use of the memory resources available on
a general purpose computing system, in order to accelerate signal computation.
MA can provide substantial acceleration factors when applied to conventional
SDRs without reducing their peculiar flexibility. As a practical proof of this,
an example of MA applied in the real world to the ETSI DVB-T Viterbi decoder is
provided. Actually MA is shown able to provide, when applied to such Viterbi
decoder, an acceleration factor of 10.4x, with no impact on error correction
performances of the decoder and by making no use of any other typical
performance enhancement techniques such as low level (Assembler) programming or
parallel computation, which though remain compatible with MA. Opportunity for
extending the MA approach to the entire radio system, thus implementing what we
call a Memory-Based Software Defined Radio (MB-SDR) is finally considered and
discussed.
"
198,Estimating Self-Sustainability in Peer-to-Peer Swarming Systems,"  Peer-to-peer swarming is one of the \emph{de facto} solutions for distributed
content dissemination in today's Internet. By leveraging resources provided by
clients, swarming systems reduce the load on and costs to publishers. However,
there is a limit to how much cost savings can be gained from swarming; for
example, for unpopular content peers will always depend on the publisher in
order to complete their downloads. In this paper, we investigate this
dependence. For this purpose, we propose a new metric, namely \emph{swarm
self-sustainability}. A swarm is referred to as self-sustaining if all its
blocks are collectively held by peers; the self-sustainability of a swarm is
the fraction of time in which the swarm is self-sustaining. We pose the
following question: how does the self-sustainability of a swarm vary as a
function of content popularity, the service capacity of the users, and the size
of the file? We present a model to answer the posed question. We then propose
efficient solution methods to compute self-sustainability. The accuracy of our
estimates is validated against simulation. Finally, we also provide closed-form
expressions for the fraction of time that a given number of blocks is
collectively held by peers.
"
199,"Impact of Connection Admission Process on the Direct Retry Load
  Balancing Algorithm in Cellular Network","  We present an analytical framework for modeling a priority-based load
balancing scheme in cellular networks based on a new algorithm called direct
retry with truncated offloading channel resource pool (DR$_{K}$). The model,
developed for a baseline case of two cell network, differs in many respects
from previous works on load balancing. Foremost, it incorporates the call
admission process, through random access. In specific, the proposed model
implements the Physical Random Access Channel used in 3GPP network standards.
Furthermore, the proposed model allows the differentiation of users based on
their priorities. The quantitative results illustrate that, for example,
cellular network operators can control the manner in which traffic is offloaded
between neighboring cells by simply adjusting the length of the random access
phase. Our analysis also allows for the quantitative determination of the
blocking probability individual users will experience given a specific length
of random access phase. Furthermore, we observe that the improvement in
blocking probability per shared channel for load balanced users using DR$_{K}$
is maximized at an intermediate number of shared channels, as opposed to the
maximum number of these shared resources. This occurs because a balance is
achieved between the number of users requesting connections and those that are
already admitted to the network. We also present an extension of our analytical
model to a multi-cell network (by means of an approximation) and an application
of the proposed load balancing scheme in the context of opportunistic spectrum
access.
"
200,"Magnetohydrodynamics on Heterogeneous architectures: a performance
  comparison","  We present magneto-hydrodynamic simulation results for heterogeneous systems.
Heterogeneous architectures combine high floating point performance many-core
units hosted in conventional server nodes. Examples include Graphics Processing
Units (GPU's) and Cell. They have potentially large gains in performance, at
modest power and monetary cost. We implemented a magneto-hydrodynamic (MHD)
simulation code on a variety of heterogeneous and multi-core architectures ---
multi-core x86, Cell, Nvidia and ATI GPU --- in different languages, FORTRAN,
C, Cell, CUDA and OpenCL. We present initial performance results for these
systems. To our knowledge, this is the widest comparison of heterogeneous
systems for MHD simulations. We review the different challenges faced in each
architecture, and potential bottlenecks. We conclude that substantial gains in
performance over traditional systems are possible, and in particular that is
possible to extract a greater percentage of peak theoretical performance from
some systems when compared to x86 architectures.
"
201,"Efficient multicore-aware parallelization strategies for iterative
  stencil computations","  Stencil computations consume a major part of runtime in many scientific
simulation codes. As prototypes for this class of algorithms we consider the
iterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient
parallel implementations for cache-based multicore architectures. Temporal
cache blocking is a known advanced optimization technique, which can reduce the
pressure on the memory bus significantly. We apply and refine this optimization
for a recently presented temporal blocking strategy designed to explicitly
utilize multicore characteristics. Especially for the case of Gauss-Seidel
smoothers we show that simultaneous multi-threading (SMT) can yield substantial
performance improvements for our optimized algorithm.
"
202,"Mean field for Markov Decision Processes: from Discrete to Continuous
  Optimization","  We study the convergence of Markov Decision Processes made of a large number
of objects to optimization problems on ordinary differential equations (ODE).
We show that the optimal reward of such a Markov Decision Process, satisfying a
Bellman equation, converges to the solution of a continuous
Hamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of
the Markov Decision Process. We give bounds on the difference of the rewards,
and a constructive algorithm for deriving an approximating solution to the
Markov Decision Process from a solution of the HJB equations. We illustrate the
method on three examples pertaining respectively to investment strategies,
population dynamics control and scheduling in queues are developed. They are
used to illustrate and justify the construction of the controlled ODE and to
show the gain obtained by solving a continuous HJB equation rather than a large
discrete Bellman equation.
"
203,"Performance Evaluation of Components Using a Granularity-based Interface
  Between Real-Time Calculus and Timed Automata","  To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time.
"
204,"Applying Stochastic Network Calculus to 802.11 Backlog and Delay
  Analysis","  Stochastic network calculus provides an elegant way to characterize traffic
and service processes. However, little effort has been made on applying it to
multi-access communication systems such as 802.11. In this paper, we take the
first step to apply it to the backlog and delay analysis of an 802.11 wireless
local network. In particular, we address the following questions: In applying
stochastic network calculus, under what situations can we derive stable backlog
and delay bounds? How to derive the backlog and delay bounds of an 802.11
wireless node? And how tight are these bounds when compared with simulations?
To answer these questions, we first derive the general stability condition of a
wireless node (not restricted to 802.11). From this, we give the specific
stability condition of an 802.11 wireless node. Then we derive the backlog and
delay bounds of an 802.11 node based on an existing model of 802.11. We observe
that the derived bounds are loose when compared with ns-2 simulations,
indicating that improvements are needed in the current version of stochastic
network calculus.
"
205,"Automatic Mapping Tasks to Cores - Evaluating AMTHA Algorithm in
  Multicore Architectures","  The AMTHA (Automatic Mapping Task on Heterogeneous Architectures) algorithm
for task-to-processors assignment and the MPAHA (Model of Parallel Algorithms
on Heterogeneous Architectures) model are presented. The use of AMTHA is
analyzed for multicore processor-based architectures, considering the
communication model among processes in use. The results obtained in the tests
carried out are presented, comparing the real execution times on multicores of
a set of synthetic applications with the predictions obtained with AMTHA.
Finally current lines of research are presented, focusing on clusters of
multicores and hybrid programming paradigms.
"
206,"Comparison of the Performance of Two Service Disciplines for a Shared
  Bus Multiprocessor with Private Caches","  In this paper, we compare two analytical models for evaluation of cache
coherence overhead of a shared bus multiprocessor with private caches. The
models are based on a closed queuing network with different service
disciplines. We find that the priority discipline can be used as a lower-level
bound. Some numerical results are shown graphically.
"
207,Space-efficient scheduling of stochastically generated tasks,"  We study the problem of scheduling tasks for execution by a processor when
the tasks can stochastically generate new tasks. Tasks can be of different
types, and each type has a fixed, known probability of generating other tasks.
We present results on the random variable S^sigma modeling the maximal space
needed by the processor to store the currently active tasks when acting under
the scheduler sigma. We obtain tail bounds for the distribution of S^sigma for
both offline and online schedulers, and investigate the expected value of
S^sigma.
"
208,"LIKWID: A lightweight performance-oriented tool suite for x86 multicore
  environments","  Exploiting the performance of today's processors requires intimate knowledge
of the microarchitecture as well as an awareness of the ever-growing complexity
in thread and cache topology. LIKWID is a set of command-line utilities that
addresses four key problems: Probing the thread and cache topology of a
shared-memory node, enforcing thread-core affinity on a program, measuring
performance counter metrics, and toggling hardware prefetchers. An API for
using the performance counting features from user code is also included. We
clearly state the differences to the widely used PAPI interface. To demonstrate
the capabilities of the tool set we show the influence of thread pinning on
performance using the well-known OpenMP STREAM triad benchmark, and use the
affinity and hardware counter tools to study the performance of a stencil code
specifically optimized to utilize shared caches on multicore chips.
"
209,Optimal Content Placement for Peer-to-Peer Video-on-Demand Systems,"  In this paper, we address the problem of content placement in peer-to-peer
systems, with the objective of maximizing the utilization of peers' uplink
bandwidth resources. We consider system performance under a many-user
asymptotic. We distinguish two scenarios, namely ""Distributed Server Networks""
(DSN) for which requests are exogenous to the system, and ""Pure P2P Networks""
(PP2PN) for which requests emanate from the peers themselves. For both
scenarios, we consider a loss network model of performance, and determine
asymptotically optimal content placement strategies in the case of a limited
content catalogue. We then turn to an alternative ""large catalogue"" scaling
where the catalogue size scales with the peer population. Under this scaling,
we establish that storage space per peer must necessarily grow unboundedly if
bandwidth utilization is to be maximized. Relating the system performance to
properties of a specific random graph model, we then identify a content
placement strategy and a request acceptance policy which jointly maximize
bandwidth utilization, provided storage space per peer grows unboundedly,
although arbitrarily slowly, with system size.
"
210,"Analysis of Non-Persistent CSMA Protocols with Exponential Backoff
  Scheduling","  This paper studies the performance of Non-persistent CSMA/CA protocols with
K-Exponential Backoff scheduling algorithms. A multi-queue single-server system
is proposed to model multiple access networks. The input buffer of each access
node is modeled as a Geo/G/1 queue, and the service time distribution of
head-of-line packets is derived from the Markov chain of underlying scheduling
algorithm. The main results include the complete analysis of the throughput and
delay distribution, from which we obtained stable regions with respect to the
throughput and bounded mean delay of the Geometric Retransmission and
Exponential Backoff schemes. We show that the throughput stable region of
Geometric Retransmission will vanish as the number of nodes n \rightarrow
\infty; thus, it is inherently unstable for large n. In contrast to Geometric
Retransmission, the throughput stable region of Exponential Backoff can be
obtained for an infinite population. We found that the bounded mean delay
region of Geometric Retransmission remains the same as its throughput stable
region. Besides, the variance of service time of Exponential Backoff can be
unbounded due to the capture effect; thus, its bounded delay region is only a
sub-set of its throughput stable region. Analytical results presented in this
paper are all verified by simulation.
"
211,A New Benchmark For Evaluation Of Graph-Theoretic Algorithms,"  We propose a new graph-theoretic benchmark in this paper. The benchmark is
developed to address shortcomings of an existing widely-used graph benchmark.
We thoroughly studied a large number of traditional and contemporary graph
algorithms reported in the literature to have clear understanding of their
algorithmic and run-time characteristics. Based on this study, we designed a
suite of kernels, each of which represents a specific class of graph
algorithms. The kernels are designed to capture the typical run-time behavior
of target algorithms accurately, while limiting computational and spatial
overhead to ensure its computation finishes in reasonable time. We expect that
the developed benchmark will serve as a much needed tool for evaluating
different architectures and programming models to run graph algorithms.
"
212,Analyzing the Performance of Active Queue Management Algorithms,"  Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph.
"
213,A Performance Comparison of CUDA and OpenCL,"  CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is
an open standard that can be used to program CPUs, GPUs, and other devices from
different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL
promises a portable language for GPU programming, its generality may entail a
performance penalty. In this paper, we use complex, near-identical kernels from
a Quantum Monte Carlo application to compare the performance of CUDA and
OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel
to an OpenCL kernel involves minimal modifications. Making such a kernel
compile with ATI's build tools involves more modifications. Our performance
tests measure and compare data transfer times to and from the GPU, kernel
execution times, and end-to-end application execution times for both CUDA and
OpenCL.
"
214,"Saturation Throughput - Delay Analysis of IEEE 802.11 DCF in Fading
  Channel","  In this paper, we analytically analyzed the impact of an error-prone channel
over all performance measures in a trafficsaturated IEEE 802.11 WLAN. We
calculated station's transmission probability by using the modified Markov
chain model of the backoff window size that considers the frame-error rates and
maximal allowable number of retransmission attempts. The frame error rate has a
significant impact over theoretical throughput, mean frame delay, and discard
probability. The peak throughput of a WLAN is insensitive of the maximal number
of retransmissions. Discard probabilities are insensitive to the station access
method, Basic or RTS/CTS.
"
215,Simulation de traces r\'eelles d'E/S disque de PC,"  Under Windows operating system, existing I/O benchmarking tools does not
allow a developer to efficiently define a file access strategy according to the
applications' constraints. This is essentially due to the fact that the
existing tools do allow only a restricted set of I/O workloads that does not
generally correspond to the target applications. To cope with this problem, we
designed and implemented a precise I/O simulator allowing to simulate whatever
real I/O trace on a given defined architecture, and in which most of file and
disk cache strategies, their interactions and the detailed storage system
architecture are implemented. Simulation results on different workloads and
architectures show a very high degree of precision. In fact, the mean error
rate as compared to real measures is of about 6% with a maximum of 10% on
global throughput.
"
216,"Seeing Through Black Boxes : Tracking Transactions through Queues under
  Monitoring Resource Constraints","  The problem of optimal allocation of monitoring resources for tracking
transactions progressing through a distributed system, modeled as a queueing
network, is considered. Two forms of monitoring information are considered,
viz., locally unique transaction identifiers, and arrival and departure
timestamps of transactions at each processing queue. The timestamps are assumed
available at all the queues but in the absence of identifiers, only enable
imprecise tracking since parallel processing can result in out-of-order
departures. On the other hand, identifiers enable precise tracking but are not
available without proper instrumentation. Given an instrumentation budget, only
a subset of queues can be selected for production of identifiers, while the
remaining queues have to resort to imprecise tracking using timestamps. The
goal is then to optimally allocate the instrumentation budget to maximize the
overall tracking accuracy. The challenge is that the optimal allocation
strategy depends on accuracies of timestamp-based tracking at different queues,
which has complex dependencies on the arrival and service processes, and the
queueing discipline. We propose two simple heuristics for allocation by
predicting the order of timestamp-based tracking accuracies of different
queues. We derive sufficient conditions for these heuristics to achieve
optimality through the notion of stochastic comparison of queues. Simulations
show that our heuristics are close to optimality, even when the parameters
deviate from these conditions.
"
217,Highly Parallel Sparse Matrix-Matrix Multiplication,"  Generalized sparse matrix-matrix multiplication is a key primitive for many
high performance graph algorithms as well as some linear solvers such as
multigrid. We present the first parallel algorithms that achieve increasing
speedups for an unbounded number of processors. Our algorithms are based on
two-dimensional block distribution of sparse matrices where serial sections use
a novel hypersparse kernel for scalability. We give a state-of-the-art MPI
implementation of one of our algorithms. Our experiments show scaling up to
thousands of processors on a variety of test scenarios.
"
218,"On the flow-level stability of data networks without congestion control:
  the case of linear networks and upstream trees","  In this paper, flow models of networks without congestion control are
considered. Users generate data transfers according to some Poisson processes
and transmit corresponding packet at a fixed rate equal to their access rate
until the entire document is received at the destination; some erasure codes
are used to make the transmission robust to packet losses. We study the
stability of the stochastic process representing the number of active flows in
two particular cases: linear networks and upstream trees. For the case of
linear networks, we notably use fluid limits and an interesting phenomenon of
""time scale separation"" occurs. Bounds on the stability region of linear
networks are given. For the case of upstream trees, underlying monotonic
properties are used. Finally, the asymptotic stability of those processes is
analyzed when the access rate of the users decreases to 0. An appropriate
scaling is introduced and used to prove that the stability region of those
networks is asymptotically maximized.
"
219,"Leveraging shared caches for parallel temporal blocking of stencil codes
  on multicore processors and clusters","  Bandwidth-starved multicore chips have become ubiquitous. It is well known
that the performance of stencil codes can be improved by temporal blocking,
lessening the pressure on the memory interface. We introduce a new pipelined
approach that makes explicit use of shared caches in multicore environments and
minimizes synchronization and boundary overhead. Benchmark results are
presented for three current x86-based microprocessors, showing clearly that our
optimization works best on designs with high-speed shared caches and low memory
bandwidth per core. We furthermore demonstrate that simple bandwidth-based
performance models are inaccurate for this kind of algorithm and employ a more
elaborate, synthetic modeling procedure. Finally we show that temporal blocking
can be employed successfully in a hybrid shared/distributed-memory environment,
albeit with limited benefit at strong scaling.
"
220,Implicit Renewal Theory and Power Tails on Trees,"  We extend Goldie's (1991) Implicit Renewal Theorem to enable the analysis of
recursions on weighted branching trees. We illustrate the developed method by
deriving the power tail asymptotics of the distributions of the solutions R to:
R =_D sum_{i=1}^N C_i R_i + Q, R =_D max(max_{i=1}^N C_i R_i, Q), and similar
recursions, where (Q, N, C_1,..., C_N) is a nonnegative random vector with N in
{0, 1, 2, 3, ..., infinity}, and {R_i}_{i >= 1} are iid copies of R,
independent of (Q, N, C_1,..., C_N); =_D denotes the equality in distribution.
"
221,"Sustainable Throughput of Wireless LANs with Multi-Packet Reception
  Capability under Bounded Delay-Moment Requirements","  With the rapid proliferation of broadband wireless services, it is of
paramount importance to understand how fast data can be sent through a wireless
local area network (WLAN). Thanks to a large body of research following the
seminal work of Bianchi, WLAN throughput under saturated traffic condition has
been well understood. By contrast, prior investigations on throughput
performance under unsaturated traffic condition was largely based on
phenomenological observations, which lead to a common misconception that WLAN
can support a traffic load as high as saturation throughput, if not higher,
under non-saturation condition. In this paper, we show through rigorous
analysis that this misconception may result in unacceptable quality of service:
mean packet delay and delay jitter may approach infinity even when the traffic
load is far below the saturation throughput. Hence, saturation throughput is
not a sound measure of WLAN capacity under non-saturation condition. To bridge
the gap, we define safe-bounded-mean-delay (SBMD) throughput and
safe-bounded-delay-jitter (SBDJ) throughput that reflect the actual network
capacity users can enjoy when they require finite mean delay and delay jitter,
respectively.
  Our earlier work proved that in a WLAN with multi-packet reception (MPR)
capability, saturation throughput scales super-linearly with the MPR capability
of the network. This paper extends the investigation to the non-saturation case
and shows that super-linear scaling also holds for SBMD and SBDJ throughputs.
Our results here complete the demonstration of MPR as a powerful
capacity-enhancement technique for WLAN under both saturation and
non-saturation conditions.
"
222,"Decentralized Fair Scheduling in Two-Hop Relay-Assisted Cognitive OFDMA
  Systems","  In this paper, we consider a two-hop relay-assisted cognitive downlink OFDMA
system (named as secondary system) dynamically accessing a spectrum licensed to
a primary network, thereby improving the efficiency of spectrum usage. A
cluster-based relay-assisted architecture is proposed for the secondary system,
where relay stations are employed for minimizing the interference to the users
in the primary network and achieving fairness for cell-edge users. Based on
this architecture, an asymptotically optimal solution is derived for jointly
controlling data rates, transmission power, and subchannel allocation to
optimize the average weighted sum goodput where the proportional fair
scheduling (PFS) is included as a special case. This solution supports
decentralized implementation, requires small communication overhead, and is
robust against imperfect channel state information at the transmitter (CSIT)
and sensing measurement. The proposed solution achieves significant throughput
gains and better user-fairness compared with the existing designs. Finally, we
derived a simple and asymptotically optimal scheduling solution as well as the
associated closed-form performance under the proportional fair scheduling for a
large number of users. The system throughput is shown to be
$\mathcal{O}\left(N(1-q_p)(1-q_p^N)\ln\ln K_c\right)$, where $K_c$ is the
number of users in one cluster, $N$ is the number of subchannels and $q_p$ is
the active probability of primary users.
"
223,"A Virtual Queue Approach for Online Estimation of Loss Probability Based
  on MVA Theory","  In network quality of service provisioning, premium services generally
require to keep a very small loss probability, which is infeasible to measure
directly. The proposed virtual queue scheme estimates the small packet loss
probability of a real queueing system by measuring queue statistics in a set of
separate virtual queues. A novel scaling property between the real queue and
the virtual queues is deduced on the basis of the maximum variance asymptotic
(MVA) theory. The new scheme retains the high accuracy and wide applicability
of the MVA method for aggregated traffic while avoiding the high computational
complexity in a direct application of the original MVA analysis in real time.
This makes it suitable for online measurement applications such as network
performance monitoring and measurement-based admission control.
"
224,"Performance Evaluation of Components Using a Granularity-based Interface
  Between Real-Time Calculus and Timed Automata","  To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time.
"
225,"A new tool for the performance analysis of massively parallel computer
  systems","  We present a new tool, GPA, that can generate key performance measures for
very large systems. Based on solving systems of ordinary differential equations
(ODEs), this method of performance analysis is far more scalable than
stochastic simulation. The GPA tool is the first to produce higher moment
analysis from differential equation approximation, which is essential, in many
cases, to obtain an accurate performance prediction. We identify so-called
switch points as the source of error in the ODE approximation. We investigate
the switch point behaviour in several large models and observe that as the
scale of the model is increased, in general the ODE performance prediction
improves in accuracy. In the case of the variance measure, we are able to
justify theoretically that in the limit of model scale, the ODE approximation
can be expected to tend to the actual variance of the model.
"
226,"Proceedings Eighth Workshop on Quantitative Aspects of Programming
  Languages","  This volume contains the proceedings of the Eighth Workshop on Quantitative
Aspects of Programming Languages (QAPL 2010), held in Paphos, Cyprus, on March
27-28, 2010. QAPL 2010 is a satellite event of the European Joint Conferences
on Theory and Practice of Software (ETAPS 2010).
  The workshop theme is on quantitative aspects of computation. These aspects
are related to the use of physical quantities (storage space, time, bandwidth,
etc.) as well as mathematical quantities (e.g. probability and measures for
reliability, security and trust), and play an important (sometimes essential)
role in characterising the behavior and determining the properties of systems.
Such quantities are central to the definition of both the model of systems
(architecture, language design, semantics) and the methodologies and tools for
the analysis and verification of the systems properties.
  The aim of this workshop is to discuss the explicit use of quantitative
information such as time and probabilities either directly in the model or as a
tool for the analysis of systems.
"
227,"A Fluid Limit for an Overloaded X Model Via a Stochastic Averaging
  Principle","  We prove a many-server heavy-traffic fluid limit for an overloaded Markovian
queueing system having two customer classes and two service pools, known in the
call-center literature as the X model. The system uses the
fixed-queue-ratio-with-thresholds (FQR-T) control, which we proposed in a
recent paper as a way for one service system to help another in face of an
unexpected overload. Under FQR-T, customers are served by their own service
pool until a threshold is exceeded. Then, one-way sharing is activated with
customers from one class allowed to be served in both pools. After the control
is activated, it aims to keep the two queues at a pre-specified fixed ratio.
For large systems that fixed ratio is achieved approximately. For the fluid
limit, or FWLLN, we consider a sequence of properly scaled X models in overload
operating under FQR-T. Our proof of the FWLLN follows the compactness approach,
i.e., we show that the sequence of scaled processes is tight, and then show
that all converging subsequences have the specified limit. The characterization
step is complicated because the queue-difference processes, which determine the
customer-server assignments, remain stochastically bounded, and need to be
considered without spatial scaling. Asymptotically, these queue-difference
processes operate in a faster time scale than the fluid-scaled processes. In
the limit, due to a separation of time scales, the driving processes converge
to a time-dependent steady state (or local average) of a time-varying
fast-time-scale process (FTSP). This averaging principle (AP) allows us to
replace the driving processes with the long-run average behavior of the FTSP.
"
228,"Limits of responsiveness concerning human-readable knowledge bases: an
  operational analysis","  Introduction. The purpose of this work is the evaluation of responsiveness
when remote users communicate with a human-readable knowledge base (KB).
Responsiveness [R(s)] is considered here as a measure of service quality.
Method. The preferred method is operational analysis, a variation of classical
stochastic theory, which allows for the study of user-system interaction with
minimal computational effort. Analysis. The analysis is based on well-known
performance metrics, such as service ability, elapsed time, and throughput:
from these metrics estimates of R(s) are derived analytically. Results.
Critical points indicating congestion are obtained: these are limits on the
number of admissible requests and the number of connected users. Also obtained
is a sufficient condition for achieving flow balance between the KB host and
the request-relaying servers. Conclusions. When R(s) is within normal limits,
users should appreciate the benefits from using the services offered by their
KB host. When bottlenecks are formed, R(s) declines, and the whole
communication system heads for saturation. Flow balancing procedures are
necessary for the elimination of bottlenecks, which leads to a better resource
management.
"
229,"A Flexible Patch-Based Lattice Boltzmann Parallelization Approach for
  Heterogeneous GPU-CPU Clusters","  Sustaining a large fraction of single GPU performance in parallel
computations is considered to be the major problem of GPU-based clusters. In
this article, this topic is addressed in the context of a lattice Boltzmann
flow solver that is integrated in the WaLBerla software framework. We propose a
multi-GPU implementation using a block-structured MPI parallelization, suitable
for load balancing and heterogeneous computations on CPUs and GPUs. The
overhead required for multi-GPU simulations is discussed in detail and it is
demonstrated that the kernel performance can be sustained to a large extent.
With our GPU implementation, we achieve nearly perfect weak scalability on
InfiniBand clusters. However, in strong scaling scenarios multi-GPUs make less
efficient use of the hardware than IBM BG/P and x86 clusters. Hence, a cost
analysis must determine the best course of action for a particular simulation
task. Additionally, weak scaling results of heterogeneous simulations conducted
on CPUs and GPUs simultaneously are presented using clusters equipped with
varying node configurations.
"
230,"Performance bounds in wormhole routing, a network calculus approach","  We present a model of performance bound calculus on feedforward networks
where data packets are routed under wormhole routing discipline. We are
interested in determining maximum end-to-end delays and backlogs of messages or
packets going from a source node to a destination node, through a given virtual
path in the network. Our objective here is to give a network calculus approach
for calculating the performance bounds. First we propose a new concept of
curves that we call packet curves. The curves permit to model constraints on
packet lengths of a given data flow, when the lengths are allowed to be
different. Second, we use this new concept to propose an approach for
calculating residual services for data flows served under non preemptive
service disciplines. Third, we model a binary switch (with two input ports and
two output ports), where data is served under wormhole discipline. We present
our approach for computing the residual services and deduce the worst case
bounds for flows passing through a wormhole binary switch. Finally, we
illustrate this approach in numerical examples, and show how to extend it to
feedforward networks.
"
231,"Analysis Framework for Opportunistic Spectrum OFDMA and its Application
  to the IEEE 802.22 Standard","  We present an analytical model that enables throughput evaluation of
Opportunistic Spectrum Orthogonal Frequency Division Multiple Access (OS-OFDMA)
networks. The core feature of the model, based on a discrete time Markov chain,
is the consideration of different channel and subchannel allocation strategies
under different Primary and Secondary user types, traffic and priority levels.
The analytical model also assesses the impact of different spectrum sensing
strategies on the throughput of OS-OFDMA network. The analysis applies to the
IEEE 802.22 standard, to evaluate the impact of two-stage spectrum sensing
strategy and varying temporal activity of wireless microphones on the IEEE
802.22 throughput. Our study suggests that OS-OFDMA with subchannel notching
and channel bonding could provide almost ten times higher throughput compared
with the design without those options, when the activity and density of
wireless microphones is very high. Furthermore, we confirm that OS-OFDMA
implementation without subchannel notching, used in the IEEE 802.22, is able to
support real-time and non-real-time quality of service classes, provided that
wireless microphones temporal activity is moderate (with approximately one
wireless microphone per 3,000 inhabitants with light urban population density
and short duty cycles). Finally, two-stage spectrum sensing option improves
OS-OFDMA throughput, provided that the length of spectrum sensing at every
stage is optimized using our model.
"
232,A Compositional Semantics for Stochastic Reo Connectors,"  In this paper we present a compositional semantics for the channel-based
coordination language Reo which enables the analysis of quality of service
(QoS) properties of service compositions. For this purpose, we annotate Reo
channels with stochastic delay rates and explicitly model data-arrival rates at
the boundary of a connector, to capture its interaction with the services that
comprise its environment. We propose Stochastic Reo automata as an extension of
Reo automata, in order to compositionally derive a QoS-aware semantics for Reo.
We further present a translation of Stochastic Reo automata to Continuous-Time
Markov Chains (CTMCs). This translation enables us to use third-party CTMC
verification tools to do an end-to-end performance analysis of service
compositions.
"
233,"A Foundation for Stochastic Bandwidth Estimation of Networks with Random
  Service","  We develop a stochastic foundation for bandwidth estimation of networks with
random service, where bandwidth availability is expressed in terms of bounding
functions with a defined violation probability. Exploiting properties of a
stochastic max-plus algebra and system theory, the task of bandwidth estimation
is formulated as inferring an unknown bounding function from measurements of
probing traffic. We derive an estimation methodology that is based on iterative
constant rate probes. Our solution provides evidence for the utility of packet
trains for bandwidth estimation in the presence of variable cross traffic.
Taking advantage of statistical methods, we show how our estimation method can
be realized in practice, with adaptive train lengths of probe packets, probing
rates, and replicated measurements required to achieve both high accuracy and
confidence levels. We evaluate our method in a controlled testbed network,
where we show the impact of cross traffic variability on the time-scales of
service availability, and provide a comparison with existing bandwidth
estimation tools.
"
234,Fast Mixing of Parallel Glauber Dynamics and Low-Delay CSMA Scheduling,"  Glauber dynamics is a powerful tool to generate randomized, approximate
solutions to combinatorially difficult problems. It has been used to analyze
and design distributed CSMA (Carrier Sense Multiple Access) scheduling
algorithms for multi-hop wireless networks. In this paper we derive bounds on
the mixing time of a generalization of Glauber dynamics where multiple links
are allowed to update their states in parallel and the fugacity of each link
can be different. The results can be used to prove that the average queue
length (and hence, the delay) under the parallel Glauber dynamics based CSMA
grows polynomially in the number of links for wireless networks with
bounded-degree interference graphs when the arrival rate lies in a fraction of
the capacity region. We also show that in specific network topologies, the
low-delay capacity region can be further improved.
"
235,"An ODE for an Overloaded X Model Involving a Stochastic Averaging
  Principle","  We study an ordinary differential equation (ODE) arising as the many-server
heavy-traffic fluid limit of a sequence of overloaded Markovian queueing models
with two customer classes and two service pools. The system, known as the X
model in the call-center literature, operates under the
fixed-queue-ratio-with-thresholds (FQR-T) control, which we proposed in a
recent paper as a way for one service system to help another in face of an
unanticipated overload. Each pool serves only its own class until a threshold
is exceeded; then one-way sharing is activated with all customer-server
assignments then driving the two queues toward a fixed ratio. For large
systems, that fixed ratio is achieved approximately. The ODE describes system
performance during an overload. The control is driven by a queue-difference
stochastic process, which operates in a faster time scale than the queueing
processes themselves, thus achieving a time-dependent steady state
instantaneously in the limit. As a result, for the ODE, the driving process is
replaced by its long-run average behavior at each instant of time; i.e., the
ODE involves a heavy-traffic averaging principle (AP).
"
236,Scaling Turbo Boost to a 1000 cores,"  The Intel Core i7 processor code named Nehalem provides a feature named Turbo
Boost which opportunistically varies the frequencies of the processor's cores.
The frequency of a core is determined by core temperature, the number of active
cores, the estimated power consumption, the estimated current consumption, and
operating system frequency scaling requests. For a chip multi-processor(CMP)
that has a small number of physical cores and a small set of performance
states, deciding the Turbo Boost frequency to use on a given core might not be
difficult. However, we do not know the complexity of this decision making
process in the context of a large number of cores, scaling to the 100s, as
predicted by researchers in the field.
"
237,"Performance Analysis of Markov Modulated 1-Persistent CSMA/CA Protocols
  with Exponential Backoff Scheduling","  This paper proposes a Markovian model of 1-persistent CSMA/CA protocols with
K-Exponential Backoff scheduling algorithms. The input buffer of each access
node is modeled as a Geo/G/1 queue, and the service time distribution of each
individual head-of-line packet is derived from the Markov chain of the
underlying scheduling algorithm. From the queuing model, we derive the
characteristic equation of network throughput and obtain the stable throughput
and bounded delay regions with respect to the retransmission factor. Our
results show that the stable throughput region of the exponential backoff
scheme exists even for an infinite population. Moreover, we find that the
bounded delay region of exponential backoff is only a sub-set of its stable
throughput region due to the large variance of the service time of input
packets caused by the capture effect. All analytical results presented in this
paper are verified by simulations.
"
238,Faster Radix Sort via Virtual Memory and Write-Combining,"  Sorting algorithms are the deciding factor for the performance of common
operations such as removal of duplicates or database sort-merge joins. This
work focuses on 32-bit integer keys, optionally paired with a 32-bit value. We
present a fast radix sorting algorithm that builds upon a
microarchitecture-aware variant of counting sort. Taking advantage of virtual
memory and making use of write-combining yields a per-pass throughput
corresponding to at least 88 % of the system's peak memory bandwidth. Our
implementation outperforms Intel's recently published radix sort by a factor of
1.5. It also compares favorably to the reported performance of an algorithm for
Fermi GPUs when data-transfer overhead is included. These results indicate that
scalar, bandwidth-sensitive sorting algorithms remain competitive on current
architectures. Various other memory-intensive applications can benefit from the
techniques described herein.
"
239,EVM as generic QoS trigger for heterogeneous wieless overlay network,"  Fourth Generation (4G) Wireless System will integrate heterogeneous wireless
overlay systems i.e. interworking of WLAN/ GSM/ CDMA/ WiMAX/ LTE/ etc with
guaranteed Quality of Service (QoS) and Experience (QoE).QoS(E) vary from
network to network and is application sensitive. User needs an optimal mobility
solution while roaming in Overlaid wireless environment i.e. user could
seamlessly transfer his session/ call to a best available network bearing
guaranteed Quality of Experience. And If this Seamless transfer of session is
executed between two networks having different access standards then it is
called Vertical Handover (VHO). Contemporary VHO decision algorithms are based
on generic QoS metrics viz. SNR, bandwidth, jitter, BER and delay. In this
paper, Error Vector Magnitude (EVM) is proposed to be a generic QoS trigger for
VHO execution. EVM is defined as the deviation of inphase/ quadrature (I/Q)
values from ideal signal states and thus provides a measure of signal quality.
In 4G Interoperable environment, OFDM is the leading Modulation scheme (more
prone to multi-path fading). EVM (modulation error) properly characterises the
wireless link/ channel for accurate VHO decision. EVM depends on the inherent
transmission impairments viz. frequency offset, phase noise,
non-linear-impairment, skewness etc. for a given wireless link. Paper provides
an insight to the analytical aspect of EVM & measures EVM (%) for key
management subframes like association/re-association/disassociation/ probe
request/response frames. EVM relation is explored for different possible
NAV-Network Allocation Vectors (frame duration). Finally EVM is compared with
SNR, BER and investigation concludes EVM as a promising QoS trigger for OFDM
based emerging wireless standards.
"
240,"Doubly Exponential Solution for Randomized Load Balancing Models with
  General Service Times","  In this paper, we provide a novel and simple approach to study the
supermarket model with general service times. This approach is based on the
supplementary variable method used in analyzing stochastic models extensively.
We organize an infinite-size system of integral-differential equations by means
of the density dependent jump Markov process, and obtain a close-form solution:
doubly exponential structure, for the fixed point satisfying the system of
nonlinear equations, which is always a key in the study of supermarket models.
The fixed point is decomposited into two groups of information under a product
form: the arrival information and the service information. based on this, we
indicate two important observations: the fixed point for the supermarket model
is different from the tail of stationary queue length distribution for the
ordinary M/G/1 queue, and the doubly exponential solution to the fixed point
can extensively exist even if the service time distribution is heavy-tailed.
Furthermore, we analyze the exponential convergence of the current location of
the supermarket model to its fixed point, and study the Lipschitz condition in
the Kurtz Theorem under general service times. Based on these analysis, one can
gain a new understanding how workload probing can help in load balancing jobs
with general service times such as heavy-tailed service.
"
241,"Multiple Timescale Dispatch and Scheduling for Stochastic Reliability in
  Smart Grids with Wind Generation Integration","  Integrating volatile renewable energy resources into the bulk power grid is
challenging, due to the reliability requirement that at each instant the load
and generation in the system remain balanced. In this study, we tackle this
challenge for smart grid with integrated wind generation, by leveraging
multi-timescale dispatch and scheduling. Specifically, we consider smart grids
with two classes of energy users - traditional energy users and opportunistic
energy users (e.g., smart meters or smart appliances), and investigate pricing
and dispatch at two timescales, via day-ahead scheduling and realtime
scheduling. In day-ahead scheduling, with the statistical information on wind
generation and energy demands, we characterize the optimal procurement of the
energy supply and the day-ahead retail price for the traditional energy users;
in realtime scheduling, with the realization of wind generation and the load of
traditional energy users, we optimize real-time prices to manage the
opportunistic energy users so as to achieve systemwide reliability. More
specifically, when the opportunistic users are non-persistent, i.e., a subset
of them leave the power market when the real-time price is not acceptable, we
obtain closedform solutions to the two-level scheduling problem. For the
persistent case, we treat the scheduling problem as a multitimescale Markov
decision process. We show that it can be recast, explicitly, as a classic
Markov decision process with continuous state and action spaces, the solution
to which can be found via standard techniques. We conclude that the proposed
multi-scale dispatch and scheduling with real-time pricing can effectively
address the volatility and uncertainty of wind generation and energy demand,
and has the potential to improve the penetration of renewable energy into smart
grids.
"
242,"An analytical model for evaluating outage and handover probability of
  cellular wireless networks","  We consider stochastic cellular networks where base stations locations form a
homogenous Poisson point process and each mobile is attached to the base
station that provides the best mean signal power. The mobile is in outage if
the SINR falls below some threshold. The handover decision has to be made if
the mobile is in outage for some time slots. The outage probability and the
handover probability is evaluated in taking into account the effect of path
loss, shadowing, Rayleigh fast fading, frequency factor reuse and conventional
beamforming. The main assumption is that the Rayleigh fast fading changes each
time slot while other network components remain static during the period of
study.
"
243,Analyzing the performance of probabilistic algorithm in noisy manets,"  Probabilistic broadcast has been widely used as a flooding optimization
mechanism to alleviate the effect of broadcast storm problem (BSP) in mobile ad
hoc networks (MANETs). Many research studies have been carried-out to develop
and evaluate the performance of this mechanism in an error-free (noiseless)
environment. In reality, wireless communication channels in MANETs are an
error-prone and suffer from high packet-loss due to presence of noise, i.e.,
noisy environment. In this paper, we propose a simulation model that can be
used to evaluate the performance of probabilistic broadcast for flooding in
noisy environment. In the proposed model, the noise-level is represented by a
generic name, probability of reception (pc) (0<=pc<=1), where pc=1 for
noiseless and <1 for noisy environment. The effect of noise is determined
randomly by generating a random number \zeta (0<=\zeta<1); if \zeta<=pc means
the packet is successfully delivered to the receiving node, otherwise,
unsuccessful delivery occurs. The proposed model is implemented on a MANET
simulator, namely, MANSim. The effect of noise on the performance of
probabilistic algorithm was investigated in four scenarios. The main
conclusions of these scenarios are: the performance of probabilistic algorithm
suffers in presence of noise. However, this suffering is less in high density
networks, or if the nodes characterized by high retransmission probability or
large radio transmission range. The nodes' speed has no or insignificant effect
on the performance.
"
244,"Performance Evaluation of an OMPR Algorithm for Route Discovery in Noisy
  MANETs","  It has been revealed in the literature that pure multipoint relaying (MPR)
algorithms demonstrate both simplicity and outstanding performance, as compared
to other flooding algorithms in wireless networks. One drawback of pure MPR
algorithms is that the selected forwarding set may not represent the optimum
selection. In addition, little efforts have been carried-out to investigate the
performance of such algorithms in noisy mobile ad hoc networks (MANETs)
suffering from high packet-loss and node mobility. In this paper, we develop
and evaluate the performance of an optimal MPR (OMPR) algorithm for route
discovery in noisy MANETs. The main feature of this new algorithm is that it
calculates all possible sets of multipoint relays (MPRs) and then selects the
set with minimum number of nodes. The algorithm demonstrates an excellent
performance when it is compared with other route discovery algorithms as it
achieves the highest cost-effective reachability.
"
245,"Online Advertisement, Optimization and Stochastic Networks","  In this paper, we propose a stochastic model to describe how search service
providers charge client companies based on users' queries for the keywords
related to these companies' ads by using certain advertisement assignment
strategies. We formulate an optimization problem to maximize the long-term
average revenue for the service provider under each client's long-term average
budget constraint, and design an online algorithm which captures the stochastic
properties of users' queries and click-through behaviors. We solve the
optimization problem by making connections to scheduling problems in wireless
networks, queueing theory and stochastic networks. Unlike prior models, we do
not assume that the number of query arrivals is known. Due to the stochastic
nature of the arrival process considered here, either temporary ""free"" service,
i.e., service above the specified budget or under-utilization of the budget is
unavoidable. We prove that our online algorithm can achieve a revenue that is
within $O(\epsilon)$ of the optimal revenue while ensuring that the overdraft
or underdraft is $O(1/\epsilon)$, where $\epsilon$ can be arbitrarily small.
With a view towards practice, we can show that one can always operate strictly
under the budget. In addition, we extend our results to a click-through rate
maximization model, and also show how our algorithm can be modified to handle
non-stationary query arrival processes and clients with short-term contracts.
  Our algorithm allows us to quantify the effect of errors in click-through
rate estimation on the achieved revenue. We also show that in the long run, an
expected overdraft level of $\Omega(\log(1/\epsilon))$ is unavoidable (a
universal lower bound) under any stationary ad assignment algorithm which
achieves a long-term average revenue within $O(\epsilon)$ of the offline
optimum.
"
246,"On the Performance Evaluation and Analysis of the Hybridised Bittorrent
  Protocol with Partial Mobility Characteristics","  Engaging mobility with file sharing is considered very promising in today's
run Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing
protocol can be rarely combined with the mobility scenario framework since
resources are not available due to the dynamically changing topology network.
As a result, mobility in P2P-oriented file sharing platforms, degrades the
end-to-end efficiency and the system's performance. This work proposes a new
hybridized model, which takes into account the mobility characteristics of the
combined Bittorrent protocol in a centralized manner enabling partial mobility
characteristics, where the clients of the network use a distinct technique to
differentiate between mobile and static nodes. Many parameters were taken into
consideration like the round trip delays, the diffusion process, and the
seeding techniques, targeting the maximization of the average throughput in the
clustered swarms containing mobile peers. Partial mobility characteristics are
set in a peer-tracker and peer-peer communication enhancement schema with
partial mobility, allowing an optimistic approach to attain high availability
and throughput response as simulation results show.
"
247,General-purpose molecular dynamics simulations on GPU-based clusters,"  We present a GPU implementation of LAMMPS, a widely-used parallel molecular
dynamics (MD) software package, and show 5x to 13x single node speedups versus
the CPU-only version of LAMMPS. This new CUDA package for LAMMPS also enables
multi-GPU simulation on hybrid heterogeneous clusters, using MPI for inter-node
communication, CUDA kernels on the GPU for all methods working with particle
data, and standard LAMMPS C++ code for CPU execution. Cell and neighbor list
approaches are compared for best performance on GPUs, with thread-per-atom and
block-per-atom neighbor list variants showing best performance at low and high
neighbor counts, respectively. Computational performance results of GPU-enabled
LAMMPS are presented for a variety of materials classes (e.g. biomolecules,
polymers, metals, semiconductors), along with a speed comparison versus other
available GPU-enabled MD software. Finally, we show strong and weak scaling
performance on a CPU/GPU cluster using up to 128 dual GPU nodes.
"
248,On Polynomial Multiplication in Chebyshev Basis,"  In a recent paper Lima, Panario and Wang have provided a new method to
multiply polynomials in Chebyshev basis which aims at reducing the total number
of multiplication when polynomials have small degree. Their idea is to use
Karatsuba's multiplication scheme to improve upon the naive method but without
being able to get rid of its quadratic complexity. In this paper, we extend
their result by providing a reduction scheme which allows to multiply
polynomial in Chebyshev basis by using algorithms from the monomial basis case
and therefore get the same asymptotic complexity estimate. Our reduction allows
to use any of these algorithms without converting polynomials input to monomial
basis which therefore provide a more direct reduction scheme then the one using
conversions. We also demonstrate that our reduction is efficient in practice,
and even outperform the performance of the best known algorithm for Chebyshev
basis when polynomials have large degree. Finally, we demonstrate a linear time
equivalence between the polynomial multiplication problem under monomial basis
and under Chebyshev basis.
"
249,Forever Young: Aging Control For Smartphones In Hybrid Networks,"  The demand for Internet services that require frequent updates through small
messages, such as microblogging, has tremendously grown in the past few years.
Although the use of such applications by domestic users is usually free, their
access from mobile devices is subject to fees and consumes energy from limited
batteries. If a user activates his mobile device and is in range of a service
provider, a content update is received at the expense of monetary and energy
costs. Thus, users face a tradeoff between such costs and their messages aging.
The goal of this paper is to show how to cope with such a tradeoff, by devising
\emph{aging control policies}. An aging control policy consists of deciding,
based on the current utility of the last message received, whether to activate
the mobile device, and if so, which technology to use (WiFi or 3G). We present
a model that yields the optimal aging control policy. Our model is based on a
Markov Decision Process in which states correspond to message ages. Using our
model, we show the existence of an optimal strategy in the class of threshold
strategies, wherein users activate their mobile devices if the age of their
messages surpasses a given threshold and remain inactive otherwise. We then
consider strategic content providers (publishers) that offer \emph{bonus
packages} to users, so as to incent them to download updates of advertisement
campaigns. We provide simple algorithms for publishers to determine optimal
bonus levels, leveraging the fact that users adopt their optimal aging control
strategies. The accuracy of our model is validated against traces from the
UMass DieselNet bus network.
"
250,"Dynamic scheduling of virtual machines running hpc workloads in
  scientific grids","  The primary motivation for uptake of virtualization has been resource
isolation, capacity management and resource customization allowing resource
providers to consolidate their resources in virtual machines. Various
approaches have been taken to integrate virtualization in to scientific Grids
especially in the arena of High Performance Computing (HPC) to run grid jobs in
virtual machines, thus enabling better provisioning of the underlying resources
and customization of the execution environment on runtime. Despite the gains,
virtualization layer also incur a performance penalty and its not very well
understood that how such an overhead will impact the performance of systems
where jobs are scheduled with tight deadlines. Since this overhead varies the
types of workload whether they are memory intensive, CPU intensive or network
I/O bound, and could lead to unpredictable deadline estimation for the running
jobs in the system. In our study, we have attempted to tackle this problem by
developing an intelligent scheduling technique for virtual machines which
monitors the workload types and deadlines, and calculate the system over head
in real time to maximize number of jobs finishing within their agreed
deadlines.
"
251,"Deadline aware virtual machine scheduler for scientific grids and cloud
  computing","  Virtualization technology has enabled applications to be decoupled from the
underlying hardware providing the benefits of portability, better control over
execution environment and isolation. It has been widely adopted in scientific
grids and commercial clouds. Since virtualization, despite its benefits incurs
a performance penalty, which could be significant for systems dealing with
uncertainty such as High Performance Computing (HPC) applications where jobs
have tight deadlines and have dependencies on other jobs before they could run.
The major obstacle lies in bridging the gap between performance requirements of
a job and performance offered by the virtualization technology if the jobs were
to be executed in virtual machines. In this paper, we present a novel approach
to optimize job deadlines when run in virtual machines by developing a
deadline-aware algorithm that responds to job execution delays in real time,
and dynamically optimizes jobs to meet their deadline obligations. Our
approaches borrowed concepts both from signal processing and statistical
techniques, and their comparative performance results are presented later in
the paper including the impact on utilization rate of the hardware resources.
"
252,Performance analysis of Xen virtual machines in real-world scenarios,"  This paper presents results of the performance benchmarks of the Open Source
hypervisor Xen. The study focuses on the network related performance as well as
on the application related performance of multiple virtual machines that were
running on the same Xen hypervisor. The comparison was carried out using a
self-developed benchmark suite that consists of easily available Open Source
tools. The goal is to measure the performance of the hypervisor in typical
real-world application scenarios when used for ""mass virtual hosting"", such as
hosting solutions of so called virtual private servers for small-to-medium
sized businesses environments. The results of the benchmarks show, that the
tested Xen setup offers good performance with respect to network traffic stress
tests, but only 75% of the performance of the non-virtualized reference
environment. This application performance score decreases as more virtual
machines are running simultaneously.
"
253,Low Power Reversible Parallel Binary Adder/Subtractor,"  In recent years, Reversible Logic is becoming more and more prominent
technology having its applications in Low Power CMOS, Quantum Computing,
Nanotechnology, and Optical Computing. Reversibility plays an important role
when energy efficient computations are considered. In this paper, Reversible
eight-bit Parallel Binary Adder/Subtractor with Design I, Design II and Design
III are proposed. In all the three design approaches, the full Adder and
Subtractors are realized in a single unit as compared to only full Subtractor
in the existing design. The performance analysis is verified using number
reversible gates, Garbage input/outputs and Quantum Cost. It is observed that
Reversible eight-bit Parallel Binary Adder/Subtractor with Design III is
efficient compared to Design I, Design II and existing design.
"
254,"Mantis: Predicting System Performance through Program Analysis and
  Modeling","  We present Mantis, a new framework that automatically predicts program
performance with high accuracy. Mantis integrates techniques from programming
language and machine learning for performance modeling, and is a radical
departure from traditional approaches. Mantis extracts program features, which
are information about program execution runs, through program instrumentation.
It uses machine learning techniques to select features relevant to performance
and creates prediction models as a function of the selected features. Through
program analysis, it then generates compact code slices that compute these
feature values for prediction. Our evaluation shows that Mantis can achieve
more than 93% accuracy with less than 10% training data set, which is a
significant improvement over models that are oblivious to program features. The
system generates code slices that are cheap to compute feature values.
"
255,"Throughput and Collision Analysis of Multi-Channel Multi-Stage Spectrum
  Sensing Algorithms","  Multi-stage sensing is a novel concept that refers to a general class of
spectrum sensing algorithms that divide the sensing process into a number of
sequential stages. The number of sensing stages and the sensing technique per
stage can be used to optimize performance with respect to secondary user
throughput and the collision probability between primary and secondary users.
So far, the impact of multi-stage sensing on network throughput and collision
probability for a realistic network model is relatively unexplored. Therefore,
we present the first analytical framework which enables performance evaluation
of different multi-channel multi-stage spectrum sensing algorithms for
Opportunistic Spectrum Access networks. The contribution of our work lies in
studying the effect of the following parameters on performance: number of
sensing stages, physical layer sensing techniques and durations per each stage,
single and parallel channel sensing and access, number of available channels,
primary and secondary user traffic, buffering of incoming secondary user
traffic, as well as MAC layer sensing algorithms. Analyzed performance metrics
include the average secondary user throughput and the average collision
probability between primary and secondary users. Our results show that when the
probability of primary user mis-detection is constrained, the performance of
multi-stage sensing is, in most cases, superior to the single stage sensing
counterpart. Besides, prolonged channel observation at the first stage of
sensing decreases the collision probability considerably, while keeping the
throughput at an acceptable level. Finally, in realistic primary user traffic
scenarios, using two stages of sensing provides a good balance between
secondary users throughput and collision probability while meeting successful
detection constraints subjected by Opportunistic Spectrum Access communication.
"
256,"Performance of wireless network coding: motivating small encoding
  numbers","  This paper focuses on a particular transmission scheme called local network
coding, which has been reported to provide significant performance gains in
practical wireless networks. The performance of this scheme strongly depends on
the network topology and thus on the locations of the wireless nodes. Also, it
has been shown previously that finding the encoding strategy, which achieves
maximum performance, requires complex calculations to be undertaken by the
wireless node in real-time.
  Both deterministic and random point pattern are explored and using the
Boolean connectivity model we provide upper bounds for the maximum coding
number, i.e., the number of packets that can be combined such that the
corresponding receivers are able to decode. For the models studied, this upper
bound is of order of $\sqrt{N}$, where $N$ denotes the (mean) number of
neighbors. Moreover, achievable coding numbers are provided for grid-like
networks. We also calculate the multiplicative constants that determine the
gain in case of a small network. Building on the above results, we provide an
analytic expression for the upper bound of the efficiency of local network
coding. The conveyed message is that it is favorable to reduce computational
complexity by relying only on small encoding numbers since the resulting
expected throughput loss is negligible.
"
257,"State Dependent Attempt Rate Modeling of Single Cell IEEE~802.11 WLANs
  with Homogeneous Nodes and Poisson Packet Arrivals","  Analytical models for IEEE 802.11-based WLANs are invariably based on
approximations, such as the well-known \textit{decoupling approximation}
proposed by Bianchi for modeling single cell WLANs consisting of saturated
nodes. In this paper, we provide a new approach to model the situation when the
nodes are not saturated. We study a State Dependent Attempt Rate (SDAR)
approximation to model $M$ queues (one queue per node) served by the CSMA/CA
protocol as standardized in the IEEE 802.11 DCF MAC protocol. The approximation
is that, when $n$ of the $M$ queues are non-empty, the transmission attempt
probability of the $n$ non-empty nodes is given by the long-term transmission
attempt probability of $n$ ""saturated"" nodes as provided by Bianchi's model.
The SDAR approximation reduces a single cell WLAN with non-saturated nodes to a
""coupled queue system"". When packets arrive to the $M$ queues according to
independent Poisson processes, we provide a Markov model for the coupled queue
system with SDAR service. \textit{The main contribution of this paper is to
provide an analysis of the coupled queue process by studying a lower
dimensional process, and by introducing a certain conditional independence
approximation}. We show that the SDAR model of contention provides an accurate
model for the DCF MAC protocol in single cells, and report the simulation
speed-ups thus obtained by our \textit{model-based simulation}.
"
258,"Jointly Optimal Channel Pairing and Power Allocation for Multichannel
  Multihop Relaying","  We study the problem of channel pairing and power allocation in a
multichannel multihop relay network to enhance the end-to-end data rate. Both
amplify-and-forward (AF) and decode-and-forward (DF) relaying strategies are
considered. Given fixed power allocation to the channels, we show that channel
pairing over multiple hops can be decomposed into independent pairing problems
at each relay, and a sorted-SNR channel pairing strategy is sum-rate optimal,
where each relay pairs its incoming and outgoing channels by their SNR order.
For the joint optimization of channel pairing and power allocation under both
total and individual power constraints, we show that the problem can be
decoupled into two subproblems solved separately. This separation principle is
established by observing the equivalence between sorting SNRs and sorting
channel gains in the jointly optimal solution. It significantly reduces the
computational complexity in finding the jointly optimal solution. It follows
that the channel pairing problem in joint optimization can be again decomposed
into independent pairing problems at each relay based on sorted channel gains.
The solution for optimizing power allocation for DF relaying is also provided,
as well as an asymptotically optimal solution for AF relaying. Numerical
results are provided to demonstrate substantial performance gain of the jointly
optimal solution over some suboptimal alternatives. It is also observed that
more gain is obtained from optimal channel pairing than optimal power
allocation through judiciously exploiting the variation among multiple
channels. Impact of the variation of channel gain, the number of channels, and
the number of hops on the performance gain is also studied through numerical
examples.
"
259,A Generalized Coupon Collector Problem,"  This paper provides analysis to a generalized version of the coupon collector
problem, in which the collector gets $d$ distinct coupons each run and she
chooses the one that she has the least so far. On the asymptotic case when the
number of coupons $n$ goes to infinity, we show that on average $\frac{n\log
n}{d} + \frac{n}{d}(m-1)\log\log{n}+O(mn)$ runs are needed to collect $m$ sets
of coupons. An efficient exact algorithm is also developed for any finite case
to compute the average needed runs exactly. Numerical examples are provided to
verify our theoretical predictions.
"
260,Fast Histograms using Adaptive CUDA Streams,"  Histograms are widely used in medical imaging, network intrusion detection,
packet analysis and other stream-based high throughput applications. However,
while porting such software stacks to the GPU, the computation of the histogram
is a typical bottleneck primarily due to the large impact on kernel speed by
atomic operations. In this work, we propose a stream-based model implemented in
CUDA, using a new adaptive kernel that can be optimized based on latency hidden
CPU compute. We also explore the tradeoffs of using the new kernel vis-\`a-vis
the stock NVIDIA SDK kernel, and discuss an intelligent kernel switching method
for the stream based on a degeneracy criterion that is adaptively computed from
the input stream.
"
261,"Weighted Centroid Algorithm for Estimating Primary User Location:
  Theoretical Analysis and Distributed Implementation","  Information about primary transmitter location is crucial in enabling several
key capabilities in cognitive radio networks, including improved
spatio-temporal sensing, intelligent location-aware routing, as well as aiding
spectrum policy enforcement. Compared to other proposed non-interactive
localization algorithms, the weighted centroid localization (WCL) scheme uses
only the received signal strength information, which makes it simple to
implement and robust to variations in the propagation environment. In this
paper we present the first theoretical framework for WCL performance analysis
in terms of its localization error distribution parameterized by node density,
node placement, shadowing variance, correlation distance and inaccuracy of
sensor node positioning. Using this analysis, we quantify the robustness of WCL
to various physical conditions and provide design guidelines, such as node
placement and spacing, for the practical deployment of WCL. We also propose a
power-efficient method for implementing WCL through a distributed cluster-based
algorithm, that achieves comparable accuracy with its centralized counterpart.
"
262,Fast GPGPU Data Rearrangement Kernels using CUDA,"  Many high performance-computing algorithms are bandwidth limited, hence the
need for optimal data rearrangement kernels as well as their easy integration
into the rest of the application. In this work, we have built a CUDA library of
fast kernels for a set of data rearrangement operations. In particular, we have
built generic kernels for rearranging m dimensional data into n dimensions,
including Permute, Reorder, Interlace/De-interlace, etc. We have also built
kernels for generic Stencil computations on a two-dimensional data using
templates and functors that allow application developers to rapidly build
customized high performance kernels. All the kernels built achieve or surpass
best-known performance in terms of bandwidth utilization.
"
263,Optimizing real-time RDF data streams,"  The Resource Description Framework (RDF) provides a common data model for the
integration of ""real-time"" social and sensor data streams with the Web and with
each other. While there exist numerous protocols and data formats for
exchanging dynamic RDF data, or RDF updates, these options should be examined
carefully in order to enable a Semantic Web equivalent of the high-throughput,
low-latency streams of typical Web 2.0, multimedia, and gaming applications.
This paper contains a brief survey of RDF update formats and a high-level
discussion of both TCP and UDP-based transport protocols for updates. Its main
contribution is the experimental evaluation of a UDP-based architecture which
serves as a real-world example of a high-performance RDF streaming application
in an Internet-scale distributed environment.
"
264,Performance of CSMA in Multi-Channel Wireless Networks,"  We analyze the performance of CSMA in multi-channel wireless networks,
accounting for the random nature of traffic. Specifically, we assess the
ability of CSMA to fully utilize the radio resources and in turn to stabilize
the network in a dynamic setting with flow arrivals and departures. We prove
that CSMA is optimal in ad-hoc mode but not in infrastructure mode, when all
data flows originate from or are destined to some access points, due to the
inherent bias of CSMA against downlink traffic. We propose a slight
modification of CSMA, that we refer to as flow-aware CSMA, which corrects this
bias and makes the algorithm optimal in all cases. The analysis is based on
some time-scale separation assumption which is proved valid in the limit of
large flow sizes.
"
265,Delay-Based Back-Pressure Scheduling in Multihop Wireless Networks,"  Scheduling is a critical and challenging resource allocation mechanism for
multihop wireless networks. It is well known that scheduling schemes that favor
links with larger queue length can achieve high throughput performance.
However, these queue-length-based schemes could potentially suffer from large
(even infinite) packet delays due to the well-known last packet problem,
whereby packets belonging to some flows may be excessively delayed due to lack
of subsequent packet arrivals. Delay-based schemes have the potential to
resolve this last packet problem by scheduling the link based on the delay the
packet has encountered. However, characterizing throughput-optimality of these
delay-based schemes has largely been an open problem in multihop wireless
networks (except in limited cases where the traffic is single-hop.) In this
paper, we investigate delay-based scheduling schemes for multihop traffic
scenarios with fixed routes. We develop a scheduling scheme based on a new
delay metric, and show that the proposed scheme achieves optimal throughput
performance. Further, we conduct simulations to support our analytical results,
and show that the delay-based scheduler successfully removes excessive packet
delays, while it achieves the same throughput region as the queue-length-based
scheme.
"
266,"A framework to experiment optimizations for real-time and embedded
  software","  Typical constraints on embedded systems include code size limits, upper
bounds on energy consumption and hard or soft deadlines. To meet these
requirements, it may be necessary to improve the software by applying various
kinds of transformations like compiler optimizations, specific mapping of code
and data in the available memories, code compression, etc. However, a
transformation that aims at improving the software with respect to a given
criterion might engender side effects on other criteria and these effects must
be carefully analyzed. For this purpose, we have developed a common framework
that makes it possible to experiment various code transfor-mations and to
evaluate their impact of various criteria. This work has been carried out
within the French ANR MORE project.
"
267,Just-In-Time compilation of OCaml byte-code,"  This paper presents various improvements that were applied to OCamlJIT2, a
Just-In-Time compiler for the OCaml byte-code virtual machine. OCamlJIT2
currently runs on various Unix-like systems with x86 or x86-64 processors. The
improvements, including the new x86 port, are described in detail, and
performance measures are given, including a direct comparison of OCamlJIT2 to
OCamlJIT.
"
268,Enhancing neural-network performance via assortativity,"  The performance of attractor neural networks has been shown to depend
crucially on the heterogeneity of the underlying topology. We take this
analysis a step further by examining the effect of degree-degree correlations
-- or assortativity -- on neural-network behavior. We make use of a method
recently put forward for studying correlated networks and dynamics thereon,
both analytically and computationally, which is independent of how the topology
may have evolved. We show how the robustness to noise is greatly enhanced in
assortative (positively correlated) neural networks, especially if it is the
hub neurons that store the information.
"
269,"Computationally Efficient Modulation Level Classification Based on
  Probability Distribution Distance Functions","  We present a novel modulation level classification (MLC) method based on
probability distribution distance functions. The proposed method uses modified
Kuiper and Kolmogorov-Smirnov distances to achieve low computational complexity
and outperforms the state of the art methods based on cumulants and
goodness-of-fit tests. We derive the theoretical performance of the proposed
MLC method and verify it via simulations. The best classification accuracy,
under AWGN with SNR mismatch and phase jitter, is achieved with the proposed
MLC method using Kuiper distances.
"
270,"Parallel sparse matrix-vector multiplication as a test case for hybrid
  MPI+OpenMP programming","  We evaluate optimized parallel sparse matrix-vector operations for two
representative application areas on widespread multicore-based cluster
configurations. First the single-socket baseline performance is analyzed and
modeled with respect to basic architectural properties of standard multicore
chips. Going beyond the single node, parallel sparse matrix-vector operations
often suffer from an unfavorable communication to computation ratio. Starting
from the observation that nonblocking MPI is not able to hide communication
cost using standard MPI implementations, we demonstrate that explicit overlap
of communication and computation can be achieved by using a dedicated
communication thread, which may run on a virtual core. We compare our approach
to pure MPI and the widely used ""vector-like"" hybrid programming strategy.
"
271,Statistical Analysis of Link Scheduling on Long Paths,"  We study how the choice of packet scheduling algorithms influences end-to-end
performance on long network paths. Taking a network calculus approach, we
consider both deterministic and statistical performance metrics. A key enabling
contribution for our analysis is a significantly sharpened method for computing
a statistical bound for the service given to a flow by the network as a whole.
For a suitably parsimonious traffic model we develop closed-form expressions
for end-to-end delays, backlog, and output burstiness. The deterministic
versions of our bounds yield optimal bounds on end-to-end backlog and output
burstiness for some schedulers, and are highly accurate for end-to-end delay
bounds.
"
272,"Throughput-optimal Scheduling in Multi-hop Wireless Networks without
  Per-flow Information","  In this paper, we consider the problem of link scheduling in multi-hop
wireless networks under general interference constraints. Our goal is to design
scheduling schemes that do not use per-flow or per-destination information,
maintain a single data queue for each link, and exploit only local information,
while guaranteeing throughput optimality. Although the celebrated back-pressure
algorithm maximizes throughput, it requires per-flow or per-destination
information. It is usually difficult to obtain and maintain this type of
information, especially in large networks, where there are numerous flows.
Also, the back-pressure algorithm maintains a complex data structure at each
node, keeps exchanging queue length information among neighboring nodes, and
commonly results in poor delay performance. In this paper, we propose
scheduling schemes that can circumvent these drawbacks and guarantee throughput
optimality. These schemes use either the readily available hop-count
information or only the local information for each link. We rigorously analyze
the performance of the proposed schemes using fluid limit techniques via an
inductive argument and show that they are throughput-optimal. We also conduct
simulations to validate our theoretical results in various settings, and show
that the proposed schemes can substantially improve the delay performance in
most scenarios.
"
273,Bandwidth sharing networks with priority scaling,"  In multi-class communication networks, traffic surges due to one class of
users can significantly degrade the performance for other classes. During these
transient periods, it is thus of crucial importance to implement priority
mechanisms that conserve the quality of service experienced by the affected
classes, while ensuring that the temporarily unstable class is not entirely
neglected. In this paper, we examine the complex interaction occurring between
several classes of traffic when classes obtain bandwidth proportionally to
their incoming traffic.
  We characterize the evolution of the network from the moment the initial
surge takes place until the system reaches its equilibrium. Using an
appropriate scaling, we show that the trajectories of the temporarily unstable
class can be described by a differential equation, while those of the stable
classes retain their stochastic nature. A stochastic averaging phenomenon
occurs and the dynamics of the temporarily unstable and the stable classes
continue to influence one another. We further proceed to characterize the
obtained differential equations and the stability region under this scaling for
monotone networks. We illustrate these result on several toy examples and we
finally build a penalization rule using these results for a network integrating
streaming and elastic traffic.
"
274,An Algebra of Synchronous Scheduling Interfaces,"  In this paper we propose an algebra of synchronous scheduling interfaces
which combines the expressiveness of Boolean algebra for logical and functional
behaviour with the min-max-plus arithmetic for quantifying the non-functional
aspects of synchronous interfaces. The interface theory arises from a
realisability interpretation of intuitionistic modal logic (also known as
Curry-Howard-Isomorphism or propositions-as-types principle). The resulting
algebra of interface types aims to provide a general setting for specifying
type-directed and compositional analyses of worst-case scheduling bounds. It
covers synchronous control flow under concurrent, multi-processing or
multi-threading execution and permits precise statements about exactness and
coverage of the analyses supporting a variety of abstractions. The paper
illustrates the expressiveness of the algebra by way of some examples taken
from network flow problems, shortest-path, task scheduling and worst-case
reaction times in synchronous programming.
"
275,"A Novel Unified Expression for the Capacity and Bit Error Probability of
  Wireless Communication Systems over Generalized Fading Channels","  Analysis of the average binary error probabilities (ABEP) and average
capacity (AC) of wireless communications systems over generalized fading
channels have been considered separately in the past. This paper introduces a
novel moment generating function (MGF)-based \emph{unified expression} for the
ABEP and AC of single and multiple link communication with maximal ratio
combining. In addition, this paper proposes the hyper-Fox's H fading model as a
unified fading distribution of a majority of the well-known generalized fading
models. As such, we offer a generic unified performance expression that can be
easily calculated and that is applicable to a wide variety of fading scenarios.
The mathematical formalism is illustrated with some selected numerical examples
that validate the correctness of our newly derived results.
"
276,"Accurate Performance Analysis of Opportunistic Decode-and-Forward
  Relaying","  In this paper, we investigate an opportunistic relaying scheme where the
selected relay assists the source-destination (direct) communication. In our
study, we consider a regenerative opportunistic relaying scheme in which the
direct path can be considered unusable, and takes into account the effect of
the possible erroneously detected and transmitted data at the best relay. We
first derive statistics based on exact probability density function (PDF) of
each hop. Then, the PDFs are used to determine accurate closed form expressions
for end-to-end bit-error rate (BER) of binary phase-shift keying (BPSK)
modulation. Furthermore, we evaluate the asymptotical performance analysis and
the diversity order is deduced. Finally, we validate our analysis by showing
that performance simulation results coincide with our analytical results over
different network architectures.
"
277,Scheduling in a random environment: stability and asymptotic optimality,"  We investigate the scheduling of a common resource between several concurrent
users when the feasible transmission rate of each user varies randomly over
time. Time is slotted and users arrive and depart upon service completion. This
may model for example the flow-level behavior of end-users in a narrowband HDR
wireless channel (CDMA 1xEV-DO). As performance criteria we consider the
stability of the system and the mean delay experienced by the users. Given the
complexity of the problem we investigate the fluid-scaled system, which allows
to obtain important results and insights for the original system: (1) We
characterize for a large class of scheduling policies the stability conditions
and identify a set of maximum stable policies, giving in each time slot
preference to users being in their best possible channel condition. We find in
particular that many opportunistic scheduling policies like Score-Based,
Proportionally Best or Potential Improvement are stable under the maximum
stability conditions, whereas the opportunistic scheduler Relative-Best or the
cmu-rule are not. (2) We show that choosing the right tie-breaking rule is
crucial for the performance (e.g. average delay) as perceived by a user. We
prove that a policy is asymptotically optimal if it is maximum stable and the
tie-breaking rule gives priority to the user with the highest departure
probability. We will refer to such tie-breaking rule as myopic. (3) We derive
the growth rates of the number of users in the system in overload settings
under various policies, which give additional insights on the performance. (4)
We conclude that simple priority-index policies with the myopic tie-breaking
rule, are stable and asymptotically optimal. All our findings are validated
with extensive numerical experiments.
"
278,Maximizing Cloud Providers Revenues via Energy Aware Allocation Policies,"  Cloud providers, like Amazon, offer their data centers' computational and
storage capacities for lease to paying customers. High electricity consumption,
associated with running a data center, not only reflects on its carbon
footprint, but also increases the costs of running the data center itself. This
paper addresses the problem of maximizing the revenues of Cloud providers by
trimming down their electricity costs. As a solution allocation policies which
are based on the dynamic powering servers on and off are introduced and
evaluated. The policies aim at satisfying the conflicting goals of maximizing
the users' experience while minimizing the amount of consumed electricity. The
results of numerical experiments and simulations are described, showing that
the proposed scheme performs well under different traffic conditions.
"
279,Profit-Aware Server Allocation for Green Internet Services,"  A server farm is examined, where a number of servers are used to offer a
service to impatient customers. Every completed request generates a certain
amount of profit, running servers consume electricity for power and cooling,
while waiting customers might leave the system before receiving service if they
experience excessive delays. A dynamic allocation policy aiming at satisfying
the conflicting goals of maximizing the quality of users' experience while
minimizing the cost for the provider is introduced and evaluated. The results
of several experiments are described, showing that the proposed scheme performs
well under different traffic conditions.
"
280,"Modelling on the Guaranteed QoS for Wireless Sensor Networks: A Network
  Calculus Approach","  Wireless sensor networks (WSNs) became one of the high technology domains
during the last ten years. Real-time applications for them make it necessary to
provide the guaranteed Quality of Service (QoS). The main contributions of this
paper are a system skeleton and a guaranteed QoS model that are suitable for
the WSNs. To do it, we develop a sensor node model based on virtual buffer
sharing and present a two-layer scheduling model using the network calculus.
With the system skeleton, we develop a guaranteed QoS model, such as the upper
bounds on buffer queue length/delay/effective bandwidth, and single-hop/
multi-hops delay/jitter/effective bandwidth. Numerical results show the system
skeleton and the guaranteed QoS model are scalable for different types of
flows, including the self-similar traffic flows, and the parameters of flow
regulators and service curves of sensor nodes affect them. Our proposal leads
to buffer dimensioning, guaranteed QoS support and control in the WSNs.
"
281,On Allocation Policies for Power and Performance,"  With the increasing popularity of Internet-based services and applications,
power efficiency is becoming a major concern for data center operators, as high
electricity consumption not only increases greenhouse gas emissions, but also
increases the cost of running the server farm itself. In this paper we address
the problem of maximizing the revenue of a service provider by means of dynamic
allocation policies that run the minimum amount of servers necessary to meet
user's requirements in terms of performance. The results of several experiments
executed using Wikipedia traces are described, showing that the proposed
schemes work well, even if the workload is non-stationary. Since any resource
allocation policy requires the use of forecasting mechanisms, various schemes
allowing compensating errors in the load forecasts are presented and evaluated.
"
282,Towards Autonomic Service Provisioning Systems,"  This paper discusses our experience in building SPIRE, an autonomic system
for service provision. The architecture consists of a set of hosted Web
Services subject to QoS constraints, and a certain number of servers used to
run session-based traffic. Customers pay for having their jobs run, but require
in turn certain quality guarantees: there are different SLAs specifying charges
for running jobs and penalties for failing to meet promised performance
metrics. The system is driven by an utility function, aiming at optimizing the
average earned revenue per unit time. Demand and performance statistics are
collected, while traffic parameters are estimated in order to make dynamic
decisions concerning server allocation and admission control. Different utility
functions are introduced and a number of experiments aiming at testing their
performance are discussed. Results show that revenues can be dramatically
improved by imposing suitable conditions for accepting incoming traffic; the
proposed system performs well under different traffic settings, and it
successfully adapts to changes in the operating environment.
"
283,Allocation and Admission Policies for Service Streams,"  A service provisioning system is examined, where a number of servers are used
to offer different types of services to paying customers. A customer is charged
for the execution of a stream of jobs; the number of jobs in the stream and the
rate of their submission is specified. On the other hand, the provider promises
a certain quality of service (QoS), measured by the average waiting time of the
jobs in the stream. A penalty is paid if the agreed QoS requirement is not met.
The objective is to maximize the total average revenue per unit time. Dynamic
policies for making server allocation and stream admission decisions are
introduced and evaluated. The results of several simulations are described.
"
284,"Jointly Optimal Channel and Power Assignment for Dual-Hop Multi-channel
  Multi-user Relaying","  We consider the problem of jointly optimizing channel pairing, channel-user
assignment, and power allocation, to maximize the weighted sum-rate, in a
single-relay cooperative system with multiple channels and multiple users.
Common relaying strategies are considered, and transmission power constraints
are imposed on both individual transmitters and the aggregate over all
transmitters. The joint optimization problem naturally leads to a mixed-integer
program. Despite the general expectation that such problems are intractable, we
construct an efficient algorithm to find an optimal solution, which incurs
computational complexity that is polynomial in the number of channels and the
number of users. We further demonstrate through numerical experiments that the
jointly optimal solution can significantly improve system performance over its
suboptimal alternatives.
"
285,"Fitting Square Pegs Through Round Pipes: Unordered Delivery
  Wire-Compatible with TCP and TLS","  Internet applications increasingly employ TCP not as a stream abstraction,
but as a substrate for application-level transports, a use that converts TCP's
in-order semantics from a convenience blessing to a performance curse. As
Internet evolution makes TCP's use as a substrate likely to grow, we offer
Minion, an architecture for backward-compatible out-of-order delivery atop TCP
and TLS. Small OS API extensions allow applications to manage TCP's send buffer
and to receive TCP segments out-of-order. Atop these extensions, Minion builds
application-level protocols offering true unordered datagram delivery, within
streams preserving strict wire-compatibility with unsecured or TLS-secured TCP
connections. Minion's protocols can run on unmodified TCP stacks, but benefit
incrementally when either endpoint is upgraded, for a backward-compatible
deployment path. Experiments suggest that Minion can noticeably improve
performance of applications such as conferencing, virtual private networking,
and web browsing, while incurring minimal CPU or bandwidth costs.
"
286,"Generic Approach for Hierarchical Modulation Performance Analysis:
  Application to DVB-SH","  Broadcasting systems have to deal with channel diversity in order to offer
the best rate to the users. Hierarchical modulation is a practical solution to
provide several rates in function of the channel quality. Unfortunately the
performance evaluation of such modulations requires time consuming simulations.
We propose in this paper a novel approach based on the channel capacity to
avoid these simulations. The method allows to study the performance in terms of
spectrum efficiency of hierarchical and also classical modulations combined
with error correcting codes. Our method will be applied to the DVB-SH standard
which considers hierarchical modulation as an optional feature.
"
287,"Generic Approach for Hierarchical Modulation Performance Analysis:
  Application to DVB-SH and DVB-S2","  Broadcasting systems have to deal with channel variability in order to offer
the best rate to the users. Hierarchical modulation is a practical solution to
provide different rates to the receivers in function of the channel quality.
Unfortunately, the performance evaluation of such modulations requires time
consuming simulations. We propose in this paper a novel approach based on the
channel capacity to avoid these simulations. The method allows to study the
performance of hierarchical and also classical modulations combined with error
correcting codes. We will also compare hierarchical modulation with time
sharing strategy in terms of achievable rates and indisponibility. Our work
will be applied to the DVB-SH and DVB-S2 standards, which both consider
hierarchical modulation as an optional feature.
"
288,Optimal Power Cost Management Using Stored Energy in Data Centers,"  Since the electricity bill of a data center constitutes a significant portion
of its overall operational costs, reducing this has become important. We
investigate cost reduction opportunities that arise by the use of uninterrupted
power supply (UPS) units as energy storage devices. This represents a deviation
from the usual use of these devices as mere transitional fail-over mechanisms
between utility and captive sources such as diesel generators. We consider the
problem of opportunistically using these devices to reduce the time average
electric utility bill in a data center. Using the technique of Lyapunov
optimization, we develop an online control algorithm that can optimally exploit
these devices to minimize the time average cost. This algorithm operates
without any knowledge of the statistics of the workload or electricity cost
processes, making it attractive in the presence of workload and pricing
uncertainties. An interesting feature of our algorithm is that its deviation
from optimality reduces as the storage capacity is increased. Our work opens up
a new area in data center power management.
"
289,Measuring NUMA effects with the STREAM benchmark,"  Modern high-end machines feature multiple processor packages, each of which
contains multiple independent cores and integrated memory controllers connected
directly to dedicated physical RAM. These packages are connected via a shared
bus, creating a system with a heterogeneous memory hierarchy. Since this shared
bus has less bandwidth than the sum of the links to memory, aggregate memory
bandwidth is higher when parallel threads all access memory local to their
processor package than when they access memory attached to a remote package.
  But, the impact of this heterogeneous memory architecture is not easily
understood from vendor benchmarks. Even where these measurements are available,
they provide only best-case memory throughput. This work presents a series of
modifications to the well-known STREAM benchmark to measure the effects of NUMA
on both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine.
"
290,"A Comparative Study of Relaying Schemes with Decode-and-Forward over
  Nakagami-m Fading Channels","  Utilizing relaying techniques to improve performance of wireless systems is a
promising avenue. However, it is crucial to understand what type of relaying
schemes should be used for achieving different performance objectives under
realistic fading conditions. In this paper, we present a general framework for
modelling and evaluating the performance of relaying schemes based on the
decode-and-forward (DF) protocol over independent and not necessarily
identically distributed (INID) Nakagami-m fading channels. In particular, we
present closed-form expressions for the statistics of the instantaneous output
signal-to-noise ratio of four significant relaying schemes with DF; two based
on repetitive transmission and the other two based on relay selection (RS).
These expressions are then used to obtain closed-form expressions for the
outage probability and the average symbol error probability for several
modulations of all considered relaying schemes over INID Nakagami-m fading.
Importantly, it is shown that when the channel state information for RS is
perfect, RS-based transmission schemes always outperform repetitive ones.
Furthermore, when the direct link between the source and the destination nodes
is sufficiently strong, relaying may not result in any gains and in this case
it should be switched-off.
"
291,"Expression Templates Revisited: A Performance Analysis of the Current ET
  Methodology","  In the last decade, Expression Templates (ET) have gained a reputation as an
efficient performance optimization tool for C++ codes. This reputation builds
on several ET-based linear algebra frameworks focused on combining both elegant
and high-performance C++ code. However, on closer examination the assumption
that ETs are a performance optimization technique cannot be maintained. In this
paper we demonstrate and explain the inability of current ET-based frameworks
to deliver high performance for dense and sparse linear algebra operations, and
introduce a new ""smart"" ET implementation that truly allows the combination of
high performance code with the elegance and maintainability of a
domain-specific language.
"
292,"Structural Analysis of Network Traffic Matrix via Relaxed Principal
  Component Pursuit","  The network traffic matrix is widely used in network operation and
management. It is therefore of crucial importance to analyze the components and
the structure of the network traffic matrix, for which several mathematical
approaches such as Principal Component Analysis (PCA) were proposed. In this
paper, we first argue that PCA performs poorly for analyzing traffic matrix
that is polluted by large volume anomalies, and then propose a new
decomposition model for the network traffic matrix. According to this model, we
carry out the structural analysis by decomposing the network traffic matrix
into three sub-matrices, namely, the deterministic traffic, the anomaly traffic
and the noise traffic matrix, which is similar to the Robust Principal
Component Analysis (RPCA) problem previously studied in [13]. Based on the
Relaxed Principal Component Pursuit (Relaxed PCP) method and the Accelerated
Proximal Gradient (APG) algorithm, we present an iterative approach for
decomposing a traffic matrix, and demonstrate its efficiency and flexibility by
experimental results. Finally, we further discuss several features of the
deterministic and noise traffic. Our study develops a novel method for the
problem of structural analysis of the traffic matrix, which is robust against
pollution of large volume anomalies.
"
293,OpenCL/OpenGL approach for studying active Brownian motion,"  This work presents a methodology for studying active Brownian dynamics on
ratchet potentials using interoperating OpenCL and OpenGL frameworks.
Programing details along with optimization issues are discussed, followed by a
com- parison of performance on different devices. Time of visualization using
OpenGL sharing buffer with OpenCL has been tested against another technique
which, while using OpenGL, does not share memory buffer with OpenCL. Both
methods have been compared with visualizing data to an external software -
gnuplot. OpenCL/OpenGL interoperating method has been found the most
appropriate to visualize any large set of data for which calculation itself is
not very long.
"
294,A Note on Parallel Algorithmic Speedup Bounds,"  A parallel program can be represented as a directed acyclic graph. An
important performance bound is the time to execute the critical path through
the graph. We show how this performance metric is related to Amdahl speedup and
the degree of average parallelism. These bounds formally exclude superlinear
performance.
"
295,Parallel Breadth-First Search on Distributed Memory Systems,"  Data-intensive, graph-based computations are pervasive in several scientific
applications, and are known to to be quite challenging to implement on
distributed memory systems. In this work, we explore the design space of
parallel algorithms for Breadth-First Search (BFS), a key subroutine in several
graph algorithms. We present two highly-tuned parallel approaches for BFS on
large parallel systems: a level-synchronous strategy that relies on a simple
vertex-based partitioning of the graph, and a two-dimensional sparse
matrix-partitioning-based approach that mitigates parallel communication
overhead. For both approaches, we also present hybrid versions with intra-node
multithreading. Our novel hybrid two-dimensional algorithm reduces
communication times by up to a factor of 3.5, relative to a common vertex based
approach. Our experimental study identifies execution regimes in which these
approaches will be competitive, and we demonstrate extremely high performance
on leading distributed-memory parallel systems. For instance, for a 40,000-core
parallel execution on Hopper, an AMD Magny-Cours based system, we achieve a BFS
performance rate of 17.8 billion edge visits per second on an undirected graph
of 4.3 billion vertices and 68.7 billion edges with skewed degree distribution.
"
296,"Secured Message Transmission in Mobile AD HOC Networks through
  Identification and Removal of Byzantine Failures","  The emerging need for mobile ad hoc networks and secured data transmission
phase is of crucial importance depending upon the environments like military.
In this paper, a new way to improve the reliability of message transmission is
presented. In the open collaborative MANET environment, any node can
maliciously or selfishly disrupt and deny communication of other nodes. Dynamic
changing topology makes it hard to determine the adversary nodes that affect
the communication in MANET. An SMT protocol provides a way to secure message
transmission by dispersing the message among several paths with minimal
redundancy. The multiple routes selected are known as APS -Active Path Set.
This paper describes a technique for fault discovery process to identify
Byzantine failures which include nodes that drop, modify, or mis-route packets
in an attempt to disrupt the routing service. An adaptive probing technique
detects a malicious link through binary search and according to the nodes
behavior, these links are avoided in the active path by multiplicatively
increasing their weights. The proposed scheme provides secure communication
even with increased number of adversaries.
"
297,LIKWID: Lightweight Performance Tools,"  Exploiting the performance of today's microprocessors requires intimate
knowledge of the microarchitecture as well as an awareness of the ever-growing
complexity in thread and cache topology. LIKWID is a set of command line
utilities that addresses four key problems: Probing the thread and cache
topology of a shared-memory node, enforcing thread-core affinity on a program,
measuring performance counter metrics, and microbenchmarking for reliable upper
performance bounds. Moreover, it includes a mpirun wrapper allowing for
portable thread-core affinity in MPI and hybrid MPI/threaded applications. To
demonstrate the capabilities of the tool set we show the influence of thread
affinity on performance using the well-known OpenMP STREAM triad benchmark, use
hardware counter tools to study the performance of a stencil code, and finally
show how to detect bandwidth problems on ccNUMA-based compute nodes.
"
298,"Serial Concatenation of RS Codes with Kite Codes: Performance Analysis,
  Iterative Decoding and Design","  In this paper, we propose a new ensemble of rateless forward error correction
(FEC) codes. The proposed codes are serially concatenated codes with
Reed-Solomon (RS) codes as outer codes and Kite codes as inner codes. The inner
Kite codes are a special class of prefix rateless low-density parity-check
(PRLDPC) codes, which can generate potentially infinite (or as many as
required) random-like parity-check bits. The employment of RS codes as outer
codes not only lowers down error-floors but also ensures (with high
probability) the correctness of successfully decoded codewords. In addition to
the conventional two-stage decoding, iterative decoding between the inner code
and the outer code are also implemented to improve the performance further. The
performance of the Kite codes under maximum likelihood (ML) decoding is
analyzed by applying a refined Divsalar bound to the ensemble weight
enumerating functions (WEF). We propose a simulation-based optimization method
as well as density evolution (DE) using Gaussian approximations (GA) to design
the Kite codes. Numerical results along with semi-analytic bounds show that the
proposed codes can approach Shannon limits with extremely low error-floors. It
is also shown by simulation that the proposed codes performs well within a wide
range of signal-to-noise-ratios (SNRs).
"
299,"Pushing the limits for medical image reconstruction on recent standard
  multicore processors","  Volume reconstruction by backprojection is the computational bottleneck in
many interventional clinical computed tomography (CT) applications. Today
vendors in this field replace special purpose hardware accelerators by standard
hardware like multicore chips and GPGPUs. Medical imaging algorithms are on the
verge of employing High Performance Computing (HPC) technology, and are
therefore an interesting new candidate for optimization. This paper presents
low-level optimizations for the backprojection algorithm, guided by a thorough
performance analysis on four generations of Intel multicore processors
(Harpertown, Westmere, Westmere EX, and Sandy Bridge).
  We choose the RabbitCT benchmark, a standardized testcase well supported in
industry, to ensure transparent and comparable results. Our aim is to provide
not only the fastest possible implementation but also compare to performance
models and hardware counter data in order to fully understand the results. We
separate the influence of algorithmic optimizations, parallelization, SIMD
vectorization, and microarchitectural issues and pinpoint problems with current
SIMD instruction set extensions on standard CPUs (SSE, AVX). The use of
assembly language is mandatory for best performance. Finally we compare our
results to the best GPGPU implementations available for this open competition
benchmark.
"
300,A Framework for QoS-aware Execution of Workflows over the Cloud,"  The Cloud Computing paradigm is providing system architects with a new
powerful tool for building scalable applications. Clouds allow allocation of
resources on a ""pay-as-you-go"" model, so that additional resources can be
requested during peak loads and released after that. However, this flexibility
asks for appropriate dynamic reconfiguration strategies. In this paper we
describe SAVER (qoS-Aware workflows oVER the Cloud), a QoS-aware algorithm for
executing workflows involving Web Services hosted in a Cloud environment. SAVER
allows execution of arbitrary workflows subject to response time constraints.
SAVER uses a passive monitor to identify workload fluctuations based on the
observed system response time. The information collected by the monitor is used
by a planner component to identify the minimum number of instances of each Web
Service which should be allocated in order to satisfy the response time
constraint. SAVER uses a simple Queueing Network (QN) model to identify the
optimal resource allocation. Specifically, the QN model is used to identify
bottlenecks, and predict the system performance as Cloud resources are
allocated or released. The parameters used to evaluate the model are those
collected by the monitor, which means that SAVER does not require any
particular knowledge of the Web Services and workflows being executed. Our
approach has been validated through numerical simulations, whose results are
reported in this paper.
"
301,User Mode Memory Page Allocation: A Silver Bullet For Memory Allocation?,"  This paper proposes a novel solution: the elimination of paged virtual memory
and partial outsourcing of memory page allocation and manipulation from the
operating system kernel into the individual process' user space - a user mode
page allocator - which allows an application to have direct, bare metal access
to the page mappings used by the hardware Memory Management Unit (MMU) for its
part of the overall address space. A user mode page allocator based emulation
of the mmap() abstraction layer of dlmalloc is then benchmarked against the
traditional kernel mode implemented mmap() in a series of synthetic Monte-Carlo
and real world application settings. Given the superb synthetic and positive
real world results from the profiling conducted, this paper proposes that with
proper operating system and API support one could gain a further order higher
performance again while keeping allocator performance invariant to the amount
of memory being allocated or freed i.e. a 100x performance improvement or more
in some common use cases. It is rare that through a simple and easy to
implement API and operating system structure change one can gain a Silver
Bullet with the potential for a second one.
"
302,"User Mode Memory Page Management: An old idea applied anew to the memory
  wall problem","  It is often said that one of the biggest limitations on computer performance
is memory bandwidth (i.e.""the memory wall problem""). In this position paper, I
argue that if historical trends in computing evolution (where growth in
available capacity is exponential and reduction in its access latencies is
linear) continue as they have, then this view is wrong - in fact we ought to be
concentrating on reducing whole system memory access latencies wherever
possible, and by ""whole system"" I mean that we ought to look at how software
can be unnecessarily wasteful with memory bandwidth due to legacy design
decisions. To this end I conduct a feasibility study to determine whether we
ought to virtualise the MMU for each application process such that it has
direct access to its own MMU page tables and the memory allocated to a process
is managed exclusively by the process and not the kernel. I find under typical
conditions that nearly scale invariant performance to memory allocation size is
possible such that hundreds of megabytes of memory can be allocated, relocated,
swapped and deallocated in almost the same time as kilobytes (e.g. allocating
8Mb is 10x quicker under this experimental allocator than a conventional
allocator, and resizing a 128Kb block to 256Kb block is 4.5x faster). I find
that first time page access latencies are improved tenfold; moreover, because
the kernel page fault handler is never called, the lack of cache pollution
improves whole application memory access latencies increasing performance by up
to 2x. Finally, I try binary patching existing applications to use the
experimental allocation technique, finding almost universal performance
improvements without having to recompile these applications to make better use
of the new facilities.
"
303,Distributed Semantic Web Data Management in HBase and MySQL Cluster,"  Various computing and data resources on the Web are being enhanced with
machine-interpretable semantic descriptions to facilitate better search,
discovery and integration. This interconnected metadata constitutes the
Semantic Web, whose volume can potentially grow the scale of the Web. Efficient
management of Semantic Web data, expressed using the W3C's Resource Description
Framework (RDF), is crucial for supporting new data-intensive,
semantics-enabled applications. In this work, we study and compare two
approaches to distributed RDF data management based on emerging cloud computing
technologies and traditional relational database clustering technologies. In
particular, we design distributed RDF data storage and querying schemes for
HBase and MySQL Cluster and conduct an empirical comparison of these approaches
on a cluster of commodity machines using datasets and queries from the Third
Provenance Challenge and Lehigh University Benchmark. Our study reveals
interesting patterns in query evaluation, shows that our algorithms are
promising, and suggests that cloud computing has a great potential for scalable
Semantic Web data management.
"
304,"A Methodology for Optimizing Multithreaded System Scalability on
  Multi-cores","  We show how to quantify scalability with the Universal Scalability Law (USL)
by applying it to performance measurements of memcached, J2EE, and Weblogic on
multi-core platforms. Since commercial multicores are essentially black-boxes,
the accessible performance gains are primarily available at the application
level. We also demonstrate how our methodology can identify the most
significant performance tuning opportunities to optimize application
scalability, as well as providing an easy means for exploring other aspects of
the multi-core system design space.
"
305,The Chaos of Propagation in a Retrial Supermarket Model,"  When decomposing the total orbit into $N$ sub-orbits (or simply orbits)
related to each of $N$ servers and through comparing the numbers of customers
in these orbits, we introduce a retrial supermarket model of $N$ identical
servers, where two probing-server choice numbers are respectively designed for
dynamically allocating each primary arrival and each retrial arrival into these
orbits when the chosen servers are all busy. Note that the designed purpose of
the two choice numbers can effectively improve performance measures of this
retrial supermarket model.
  This paper analyzes a simple and basic retrial supermarket model of N
identical servers, that is, Poisson arrivals, exponential service and retrial
times. To this end, we first provide a detailed probability computation to set
up an infinite-dimensional system of differential equations (or mean-field
equations) satisfied by the expected fraction vector. Then, as N goes to
infinity, we apply the operator semigroup to obtaining the mean-field limit (or
chaos of propagation) for the sequence of Markov processes which express the
state of this retrial supermarket model. Specifically, some simple and basic
conditions for the mean-field limit as well as for the Lipschitz condition are
established through the first two moments of the queue length in any orbit.
Finally, we show that the fixed point satisfies a system of nonlinear equations
which is an interesting networking generalization of the tail equations given
in the M/M/1 retrial queue, and also use the fixed point to give performance
analysis of this retrial supermarket model through numerical computation.
"
306,"Reserved or On-Demand Instances? A Revenue Maximization Model for Cloud
  Providers","  We examine the problem of managing a server farm in a way that attempts to
maximize the net revenue earned by a cloud provider by renting servers to
customers according to a typical Platform-as-a-Service model. The Cloud
provider offers its resources to two classes of customers: `premium' and
`basic'. Premium customers pay upfront fees to reserve servers for a specified
period of time (e.g. a year). Premium customers can submit jobs for their
reserved servers at any time and pay a fee for the server-hours they use. The
provider is liable to pay a penalty every time a `premium' job can not be
executed due to lack of resources. On the other hand, `basic' customers are
served on a best-effort basis, and pay a server-hour fee that may be higher
than the one paid by premium customers. The provider incurs energy costs when
running servers. Hence, it has an incentive to turn off idle servers. The
question of how to choose the number of servers to allocate to each pool (basic
and premium) is answered by analyzing a suitable queuing model and maximizing a
revenue function. Experimental results show that the proposed scheme adapts to
different traffic conditions, penalty levels, energy costs and usage fees.
"
307,"Performance Acceleration of Kernel Polynomial Method Applying Graphics
  Processing Units","  The Kernel Polynomial Method (KPM) is one of the fast diagonalization methods
used for simulations of quantum systems in research fields of condensed matter
physics and chemistry. The algorithm has a difficulty to be parallelized on a
cluster computer or a supercomputer due to the fine-gain recursive
calculations. This paper proposes an implementation of the KPM on the recent
graphics processing units (GPU) where the recursive calculations are able to be
parallelized in the massively parallel environment. This paper also illustrates
performance evaluations regarding the cases when the actual simulation
parameters are applied, the one for increased intensive calculations and the
one for increased amount of memory usage. Finally, it concludes that the
performance on GPU promises very high performance compared to the one on CPU
and reduces the overall simulation time.
"
308,"On the random access performance of Cell Broadband Engine with graph
  analysis application","  The Cell Broad Engine (BE) Processor has unique memory access architecture
besides its powerful computing engines. Many computing-intensive applications
have been ported to Cell/BE successfully. But memory-intensive applications are
rarely investigated except for several micro benchmarks. Since Cell/BE has
powerful software visible DMA engine, this paper studies on whether Cell/BE is
suit for applica- tions with large amount of random memory accesses. Two
benchmarks, GUPS and SSCA#2, are used. The latter is a rather complex one that
in representative of real world graph analysis applications. We find both
benchmarks have good performance on Cell/BE based IBM QS20/22. Com- pared with
2 conventional multi-processor systems with the same core/thread number, GUPS
is about 40-80% fast and SSCA#2 about 17-30% fast. The dynamic load balanc- ing
and software pipeline for optimizing SSCA#2 are intro- duced. Based on the
experiment, the potential of Cell/BE for random access is analyzed in detail as
well as its limita- tions of memory controller, atomic engine and TLB manage-
ment.Our research shows although more programming effort are needed, Cell/BE
has the potencial for irregular memory access applications.
"
309,A Modeling Framework for Gossip-based Information Spread,"  We present an analytical framework for gossip protocols based on the pairwise
information exchange between interacting nodes. This framework allows for
studying the impact of protocol parameters on the performance of the protocol.
Previously, gossip-based information dissemination protocols have been analyzed
under the assumption of perfect, lossless communication channels. We extend our
framework for the analysis of networks with lossy channels. We show how the
presence of message loss, coupled with specific topology configurations,impacts
the expected behavior of the protocol. We validate the obtained models against
simulations for two protocols.
"
310,"Super-Exponential Solution in Markovian Supermarket Models: Framework
  and Challenge","  Marcel F. Neuts opened a key door in numerical computation of stochastic
models by means of phase-type (PH) distributions and Markovian arrival
processes (MAPs). To celebrate his 75th birthday, this paper reports a more
general framework of Markovian supermarket models, including a system of
differential equations for the fraction measure and a system of nonlinear
equations for the fixed point. To understand this framework heuristically, this
paper gives a detailed analysis for three important supermarket examples: M/G/1
type, GI/M/1 type and multiple choices, explains how to derive the system of
differential equations by means of density-dependent jump Markov processes, and
shows that the fixed point may be simply super-exponential through solving the
system of nonlinear equations. Note that supermarket models are a class of
complicated queueing systems and their analysis can not apply popular queueing
theory, it is necessary in the study of supermarket models to summarize such a
more general framework which enables us to focus on important research issues.
On this line, this paper develops matrix-analytical methods of Markovian
supermarket models. We hope this will be able to open a new avenue in
performance evaluation of supermarket models by means of matrix-analytical
methods.
"
311,"Performance Analysis of Sequential Method for HandOver in Cognitive
  Radio Networks","  This paper has been withdrawn by the author due to a crucial problem in Lemma
3. This equation must be changed.
"
312,Fixed-delay Events in Generalized Semi-Markov Processes Revisited,"  We study long run average behavior of generalized semi-Markov processes with
both fixed-delay events as well as variable-delay events. We show that allowing
two fixed-delay events and one variable-delay event may cause an unstable
behavior of a GSMP. In particular, we show that a frequency of a given state
may not be defined for almost all runs (or more generally, an invariant measure
may not exist). We use this observation to disprove several results from
literature. Next we study GSMP with at most one fixed-delay event combined with
an arbitrary number of variable-delay events. We prove that such a GSMP always
possesses an invariant measure which means that the frequencies of states are
always well defined and we provide algorithms for approximation of these
frequencies. Additionally, we show that the positive results remain valid even
if we allow an arbitrary number of reasonably restricted fixed-delay events.
"
313,"Mathematical Model for the Optimal Utilization Percentile in M/M/1
  Systems: A Contribution about Knees in Performance Curves","  Performance curves of queuing systems can be analyzed by separating them into
three regions: the flat region, the knee region, and the exponential region.
Practical considerations, usually locate the knee region between 70-90% of the
theoretical maximum utilization. However, there is not a clear agreement about
where the boundaries between regions are, and where exactly the utilization
knee is located. An open debate about knees in performance curves was
undertaken at least 20 years ago. This historical debate is mainly divided
between those who claim that a knee in the curve is not a well-defined term in
mathematics, or it is a subjective and not really meaningful concept, and those
who define knees mathematically and consider their relevance and application.
In this paper, we present a mathematical model and analysis for identifying the
three mentioned regions on performance curves for M/M/1 systems; specifically,
we found the knees, or optimal utilization percentiles, at the vertices of the
hyperbolas that relate response time as a function of utilization. Using these
results, we argue that an adaptive and optimal queuing system could be deployed
by keeping load and throughput within the knee region.
"
314,"HMTT: A Hybrid Hardware/Software Tracing System for Bridging Memory
  Trace's Semantic Gap","  Memory trace analysis is an important technology for architecture research,
system software (i.e., OS, compiler) optimization, and application performance
improvements. Hardware-snooping is an effective and efficient approach to
monitor and collect memory traces. Compared with software-based approaches,
memory traces collected by hardware-based approaches are usually lack of
semantic information, such as process/function/loop identifiers, virtual
address and I/O access. In this paper we propose a hybrid hardware/software
mechanism which is able to collect memory reference trace as well as semantic
information. Based on this mechanism, we designed and implemented a prototype
system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping
mechanism to snoop on memory bus and a software-controlled tracing mechanism to
inject semantic information into normal memory trace. To the best of our
knowledge, the HMTT system is the first hardware tracing system capable of
correlating memory trace with high-level events. Comprehensive validations and
evaluations show that the HMTT system has both hardware's (e.g., no distortion
or pollution) and software's advantages (e.g., flexibility and more
information).
"
315,A Characterization of the SPARC T3-4 System,"  This technical report covers a set of experiments on the 64-core SPARC T3-4
system, comparing it to two similar AMD and Intel systems. Key characteristics
as maximum integer and floating point arithmetic throughput are measured as
well as memory throughput, showing the scalability of the SPARC T3-4 system.
The performance of POSIX threads primitives is characterized and compared in
detail, such as thread creation and mutex synchronization. Scalability tests
with a fine grained multithreaded runtime are performed, showing problems with
atomic CAS operations on such physically highly parallel systems.
"
316,Decay of tails at equilibrium for FIFO join the shortest queue networks,"  In join the shortest queue networks, incoming jobs are assigned to the
shortest queue from among a randomly chosen subset of $D$ queues, in a system
of $N$ queues; after completion of service at its queue, a job leaves the
network. We also assume that jobs arrive into the system according to a
rate-$\alpha N$ Poisson process, $\alpha<1$, with rate-1 service at each queue.
When the service at queues is exponentially distributed, it was shown in
Vvedenskaya et al. [Probl. Inf. Transm. 32 (1996) 15-29] that the tail of the
equilibrium queue size decays doubly exponentially in the limit as
$N\rightarrow\infty$. This is a substantial improvement over the case D=1,
where the queue size decays exponentially. The reasoning in [Probl. Inf.
Transm. 32 (1996) 15-29] does not easily generalize to jobs with nonexponential
service time distributions. A modularized program for treating general service
time distributions was introduced in Bramson et al. [In Proc. ACM SIGMETRICS
(2010) 275-286]. The program relies on an ansatz that asserts, in equilibrium,
any fixed number of queues become independent of one another as
$N\rightarrow\infty$. This ansatz was demonstrated in several settings in
Bramson et al. [Queueing Syst. 71 (2012) 247-292], including for networks where
the service discipline is FIFO and the service time distribution has a
decreasing hazard rate. In this article, we investigate the limiting behavior,
as $N\rightarrow \infty$, of the equilibrium at a queue when the service
discipline is FIFO and the service time distribution has a power law with a
given exponent $-\beta$, for $\beta>1$. We show under the above ansatz that, as
$N\rightarrow\infty$, the tail of the equilibrium queue size exhibits a wide
range of behavior depending on the relationship between $\beta$ and $D$. In
particular, if $\beta>D/(D-1)$, the tail is doubly exponential and, if
$\beta<D/(D-1)$, the tail has a power law. When $\beta=D/(D-1)$, the tail is
exponentially distributed.
"
317,Efficient implementation of the overlap operator on multi-GPUs,"  Lattice QCD calculations were one of the first applications to show the
potential of GPUs in the area of high performance computing. Our interest is to
find ways to effectively use GPUs for lattice calculations using the overlap
operator. The large memory footprint of these codes requires the use of
multiple GPUs in parallel. In this paper we show the methods we used to
implement this operator efficiently. We run our codes both on a GPU cluster and
a CPU cluster with similar interconnects. We find that to match performance the
CPU cluster requires 20-30 times more CPU cores than GPUs.
"
318,"GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under
  Real-time Constraints","  In this paper we modify a fast heuristic solver for the Linear Sum Assignment
Problem (LSAP) for use on Graphical Processing Units (GPUs). The motivating
scenario is an industrial application for P2P live streaming that is moderated
by a central node which is periodically solving LSAP instances for assigning
peers to one another. The central node needs to handle LSAP instances involving
thousands of peers in as near to real-time as possible. Our findings are
generic enough to be applied in other contexts. Our main result is a parallel
version of a heuristic algorithm called Deep Greedy Switching (DGS) on GPUs
using the CUDA programming language. DGS sacrifices absolute optimality in
favor of low computation time and was designed as an alternative to classical
LSAP solvers such as the Hungarian and auctioning methods. The contribution of
the paper is threefold: First, we present the process of trial and error we
went through, in the hope that our experience will be beneficial to adopters of
GPU programming for similar problems. Second, we show the modifications needed
to parallelize the DGS algorithm. Third, we show the performance gains of our
approach compared to both a sequential CPU-based implementation of DGS and a
parallel GPU-based implementation of the auctioning algorithm.
"
319,"On the Asymptotic Validity of the Decoupling Assumption for Analyzing
  802.11 MAC Protocol","  Performance evaluation of the 802.11 MAC protocol is classically based on the
decoupling assumption, which hypothesizes that the backoff processes at
different nodes are independent. This decoupling assumption results from mean
field convergence and is generally true in transient regime in the asymptotic
sense (when the number of wireless nodes tends to infinity), but, contrary to
widespread belief, may not necessarily hold in stationary regime. The issue is
often related with the existence and uniqueness of a solution to a fixed point
equation; however, it was also recently shown that this condition is not
sufficient; in contrast, a sufficient condition is a global stability property
of the associated ordinary differential equation. In this paper, we give a
simple condition that establishes the asymptotic validity of the decoupling
assumption for the homogeneous case. We also discuss the heterogeneous and the
differentiated service cases and formulate a new ordinary differential
equation. We show that the uniqueness of a solution to the associated fixed
point equation is not sufficient; we exhibit one case where the fixed point
equation has a unique solution but the decoupling assumption is not valid in
the asymptotic sense in stationary regime.
"
320,"Proceedings Ninth Workshop on Quantitative Aspects of Programming
  Languages","  This volume contains the proceedings of the Ninth Workshop on Quantitative
Aspects of Programming Languages (QAPL 2011), held in Saarbrucken, Germany,
April 1--3, 2011. QAPL 2011 is a satellite event of the European Joint
Conferences on Theory and Practice of Software (ETAPS 2011).
  The workshop theme is on quantitative aspects of computation. These aspects
are related to the use of physical quantities (storage space, time, bandwidth,
etc.) as well as mathematical quantities (e.g. probability and measures for
reliability, security and trust), and play an important (sometimes essential)
role in characterising the behavior and determining the properties of systems.
Such quantities are central to the definition of both the model of systems
(architecture, language design, semantics) and the methodologies and tools for
the analysis and verification of the systems properties. The aim of this
workshop is to discuss the explicit use of quantitative information such as
time and probabilities either directly in the model or as a tool for the
analysis of systems.
"
321,A Stochastic Broadcast Pi-Calculus,"  In this paper we propose a stochastic broadcast PI-calculus which can be used
to model server-client based systems where synchronization is always governed
by only one participant. Therefore, there is no need to determine the joint
synchronization rates. We also take immediate transitions into account which is
useful to model behaviors with no impact on the temporal properties of a
system. Since immediate transitions may introduce non-determinism, we will show
how these non-determinism can be resolved, and as result a valid CTMC will be
obtained finally. Also some practical examples are given to show the
application of this calculus.
"
322,Accelerating Lossless Data Compression with GPUs,"  Huffman compression is a statistical, lossless, data compression algorithm
that compresses data by assigning variable length codes to symbols, with the
more frequently appearing symbols given shorter codes than the less. This work
is a modification of the Huffman algorithm which permits uncompressed data to
be decomposed into indepen- dently compressible and decompressible blocks,
allowing for concurrent compression and decompression on multiple processors.
We create implementations of this modified algorithm on a current NVIDIA GPU
using the CUDA API as well as on a current Intel chip and the performance
results are compared, showing favorable GPU performance for nearly all tests.
Lastly, we discuss the necessity for high performance data compression in
today's supercomputing ecosystem.
"
323,The M/M/Infinity Service System with Ranked Servers in Heavy Traffic,"  We consider an M/M/Infinity service system in which an arriving customer is
served by the first idle server in an infinite sequence S_1, S_2, ... of
servers. We determine the first two terms in the asymptotic expansions of the
moments of L as lambda tends to infinity, where L is the index of the server
S_L serving a newly arriving customer in equilibrium, and lambda is the ratio
of the arrival rate to the service rate. The leading terms of the moments show
that L/lambda tends to a uniform distribution on [0,1].
"
324,An EM Algorithm for Continuous-time Bivariate Markov Chains,"  We study properties and parameter estimation of finite-state homogeneous
continuous-time bivariate Markov chains. Only one of the two processes of the
bivariate Markov chain is observable. The general form of the bivariate Markov
chain studied here makes no assumptions on the structure of the generator of
the chain, and hence, neither the underlying process nor the observable process
is necessarily Markov. The bivariate Markov chain allows for simultaneous jumps
of the underlying and observable processes. Furthermore, the inter-arrival time
of observed events is phase-type. The bivariate Markov chain generalizes the
batch Markovian arrival process as well as the Markov modulated Markov process.
We develop an expectation-maximization (EM) procedure for estimating the
generator of a bivariate Markov chain, and we demonstrate its performance. The
procedure does not rely on any numerical integration or sampling scheme of the
continuous-time bivariate Markov chain. The proposed EM algorithm is equally
applicable to multivariate Markov chains.
"
325,"Non-equilibrium Information Envelopes and the
  Capacity-Delay-Error-Tradeoff of Source Coding","  This paper develops an envelope-based approach to establish a link between
information and queueing theory. Unlike classical, equilibrium information
theory, information envelopes focus on the dynamics of sources and coders,
using functions of time that bound the number of bits generated. In the limit
the information envelopes converge to the average behavior and recover the
entropy of a source, respectively, the average codeword length of a coder. In
contrast, on short time scales and for sources with memory it is shown that
large deviations from known equilibrium results occur with non-negligible
probability. These can cause significant network delays. Compared to well-known
traffic models from queueing theory, information envelopes consider the
functioning of information sources and coders, avoiding a priori assumptions,
such as exponential traffic, or empirical, trace-based traffic models. Using
results from the stochastic network calculus, the envelopes yield a
characterization of the operating points of source coders by the triplet of
capacity, delay, and error. In the limit, assuming an optimal coder the
required capacity approaches the entropy with arbitrarily small probability of
error if infinitely large delays are permitted. We derive a corresponding
characterization of channels and prove that the model has the desirable
property of additivity, that allows analyzing coders and channels separately.
"
326,AWRP: Adaptive Weight Ranking Policy for Improving Cache Performance,"  Due to the huge difference in performance between the computer memory and
processor, the virtual memory management plays a vital role in system
performance. A Cache memory is the fast memory which is used to compensate the
speed difference between the memory and processor. This paper gives an adaptive
replacement policy over the traditional policy which has low overhead, better
performance and is easy to implement. Simulations show that our algorithm
performs better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and
Clock with Adaptive Replacement (CAR).
"
327,"On the Performance of Space Shift Keying (SSK) Modulation with Imperfect
  Channel Knowledge","  In this paper, we study the sensitivity and robustness of Space Shift Keying
(SSK) modulation to imperfect channel knowledge at the receiver. Unlike the
common widespread belief, we show that SSK modulation is more robust to
imperfect channel knowledge than other state-of-the-art transmission
technologies, and only few training pilots are needed to get reliable enough
channel estimates for data detection. More precisely, we focus our attention on
the so-called Time-Orthogonal-Signal-Design (TOSD-) SSK modulation scheme,
which is an improved version of SSK modulation offering transmit-diversity
gains, and provide the following contributions: i) we develop a closed-form
analytical framework to compute the Average Bit Error Probability (ABEP) of a
mismatched detector for TOSD-SSK modulation, which can be used for arbitrary
transmit-antenna, receive-antenna, channel fading, and training pilots; ii) we
perform a comparative study of the performance of TOSD-SSK modulation and the
Alamouti code under the same imperfect channel knowledge, and show that
TOSD-SSK modulation is more robust to channel estimation errors; iii) we point
out that only few pilot pulses are required to get performance very close to
the perfect channel knowledge lower-bound; and iv) we verify that transmit- and
receive-diversity gains of TOSD-SSK modulation are preserved even for a
mismatched receiver.
"
328,"Analysis of Buffer Starvation with Application to Objective QoE
  Optimization of Streaming Services","  Our purpose in this paper is to characterize buffer starvations for streaming
services. The buffer is modeled as an M/M/1 queue, plus the consideration of
bursty arrivals. When the buffer is empty, the service restarts after a certain
amount of packets are \emph{prefetched}. With this goal, we propose two
approaches to obtain the \emph{exact distribution} of the number of buffer
starvations, one of which is based on \emph{Ballot theorem}, and the other uses
recursive equations. The Ballot theorem approach gives an explicit result. We
extend this approach to the scenario with a constant playback rate using
T\`{a}kacs Ballot theorem. The recursive approach, though not offering an
explicit result, can obtain the distribution of starvations with
non-independent and identically distributed (i.i.d.) arrival process in which
an ON/OFF bursty arrival process is considered in this work. We further compute
the starvation probability as a function of the amount of prefetched packets
for a large number of files via a fluid analysis. Among many potential
applications of starvation analysis, we show how to apply it to optimize the
objective quality of experience (QoE) of media streaming, by exploiting the
tradeoff between startup/rebuffering delay and starvations.
"
329,"An Efficient Architecture for Information Retrieval in P2P Context Using
  Hypergraph","  Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
Internet traffic. P2P systems have emerged as an accepted way to share enormous
volumes of data. Needs for widely distributed information systems supporting
virtual organizations have given rise to a new category of P2P systems called
schema-based. In such systems each peer is a database management system in
itself, ex-posing its own schema. In such a setting, the main objective is the
efficient search across peer databases by processing each incoming query
without overly consuming bandwidth. The usability of these systems depends on
successful techniques to find and retrieve data; however, efficient and
effective routing of content-based queries is an emerging problem in P2P
networks. This work was attended as an attempt to motivate the use of mining
algorithms in the P2P context may improve the significantly the efficiency of
such methods. Our proposed method based respectively on combination of
clustering with hypergraphs. We use ECCLAT to build approximate clustering and
discovering meaningful clusters with slight overlapping. We use an algorithm
MTMINER to extract all minimal transversals of a hypergraph (clusters) for
query routing. The set of clusters improves the robustness in queries routing
mechanism and scalability in P2P Network. We compare the performance of our
method with the baseline one considering the queries routing problem. Our
experimental results prove that our proposed methods generate impressive levels
of performance and scalability with with respect to important criteria such as
response time, precision and recall.
"
330,A Stochastic Calculus for Network Systems with Renewable Energy Sources,"  We consider the performance modeling and evaluation of network systems
powered with renewable energy sources such as solar and wind energy. Such
energy sources largely depend on environmental conditions, which are hard to
predict accurately. As such, it may only make sense to require the network
systems to support a soft quality of service (QoS) guarantee, i.e., to
guarantee a service requirement with a certain high probability. In this paper,
we intend to build a solid mathematical foundation to help better understand
the stochastic energy constraint and the inherent correlation between QoS and
the uncertain energy supply. We utilize a calculus approach to model the
cumulative amount of charged energy and the cumulative amount of consumed
energy. We derive upper and lower bounds on the remaining energy level based on
a stochastic energy charging rate and a stochastic energy discharging rate. By
building the bridge between energy consumption and task execution (i.e.,
service), we study the QoS guarantee under the constraint of uncertain energy
sources. We further show how performance bounds can be improved if some strong
assumptions can be made.
"
331,Read Operators and their Expressiveness in Process Algebras,"  We study two different ways to enhance PAFAS, a process algebra for modelling
asynchronous timed concurrent systems, with non-blocking reading actions. We
first add reading in the form of a read-action prefix operator. This operator
is very flexible, but its somewhat complex semantics requires two types of
transition relations. We also present a read-set prefix operator with a simpler
semantics, but with syntactic restrictions. We discuss the expressiveness of
read prefixes; in particular, we compare them to read-arcs in Petri nets and
justify the simple semantics of the second variant by showing that its
processes can be translated into processes of the first with timed-bisimilar
behaviour. It is still an open problem whether the first algebra is more
expressive than the second; we give a number of laws that are interesting in
their own right, and can help to find a backward translation.
"
332,"Analysis of an M/M/1 Queue Using Fixed Order of Search for Arrivals and
  Service","  We analyze an M/M/1 queue with a service discipline in which customers, upon
arriving when the server is busy, search a sequence of stations for a vacant
station at which to wait, and in which the server, upon becoming free when one
or more customers are waiting, searches the stations in the same order for a
station occupied by a customer to serve. We show how to find complete
asymptotic expansions for all the moments of the waiting time in the heavy
traffic limit. We show in particular that the variance of the waiting time for
this discipline is more similar to that of last-come-first-served (which has a
pole of order three as the arrival rate approaches the service rate) than that
of first-come-first-served (which has pole of order two).
"
333,"An Empirical Study on variants of TCP over AODV routing protocol in
  MANET","  The cardinal concept of TCP development was to carry data within the network
where network congestion plays a vital role to cause packet loss. On the other
hand, there are several other reasons to lose packets in Mobile Ad Hoc Networks
due to fading, interfaces, multi-path routing, malicious node, and black hole.
Along with throughput, fairness of TCP protocols is important to establish a
good communication. In this paper, an empirical study has been done by
simulation and analysis of TCP variations under AODV routing protocol. In our
simulation, we studied multiple variations of TCP, such as Reno, New-Reno,
Vegas, and Tahoe. The simulation work has been done in NS2 environment. Based
on the analysis simulation result of we carried out our observations with
respect to the behavior of AODV routing protocol for different TCP packets
under several QoS metrics such as drop, throughput, delay, and jitter.
"
334,Profiling parallel Mercury programs with ThreadScope,"  The behavior of parallel programs is even harder to understand than the
behavior of sequential programs. Parallel programs may suffer from any of the
performance problems affecting sequential programs, as well as from several
problems unique to parallel systems. Many of these problems are quite hard (or
even practically impossible) to diagnose without help from specialized tools.
We present a proposal for a tool for profiling the parallel execution of
Mercury programs, a proposal whose implementation we have already started. This
tool is an adaptation and extension of the ThreadScope profiler that was first
built to help programmers visualize the execution of parallel Haskell programs.
"
335,"Parallel Sparse Matrix-Matrix Multiplication and Indexing:
  Implementation and Experiments","  Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key
primitive for many high performance graph algorithms as well as for some linear
solvers, such as algebraic multigrid. Here we show that SpGEMM also yields
efficient algorithms for general sparse-matrix indexing in distributed memory,
provided that the underlying SpGEMM implementation is sufficiently flexible and
scalable. We demonstrate that our parallel SpGEMM methods, which use
two-dimensional block data distributions with serial hypersparse kernels, are
indeed highly flexible, scalable, and memory-efficient in the general case.
This algorithm is the first to yield increasing speedup on an unbounded number
of processors; our experiments show scaling up to thousands of processors in a
variety of test scenarios.
"
336,"Cost of Virtual Machine Live Migration in Clouds: A Performance
  Evaluation","  Virtualization has become commonplace in modern data centers, often referred
as ""computing clouds"". The capability of virtual machine live migration brings
benefits such as improved performance, manageability and fault tolerance, while
allowing workload movement with a short service downtime. However, service
levels of applications are likely to be negatively affected during a live
migration. For this reason, a better understanding of its effects on system
performance is desirable. In this paper, we evaluate the effects of live
migration of virtual machines on the performance of applications running inside
Xen VMs. Results show that, in most cases, migration overhead is acceptable but
cannot be disregarded, especially in systems where availability and
responsiveness are governed by strict Service Level Agreements. Despite that,
there is a high potential for live migration applicability in data centers
serving modernInternet applications. Our results are based on a workload
covering the domain of multi-tier Web 2.0 applications.
"
337,Queries mining for efficient routing in P2P communities,"  Peer-to-peer (P2P) computing is currently attracting enormous attention. In
P2P systems a very large number of autonomous computing nodes (the peers) pool
together their resources and rely on each other for data and services.
Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
Internet traffic. Examples include P2P systems for network storage, web
caching, searching and indexing of relevant documents and distributed
network-threat analysis. Requirements for widely distributed information
systems supporting virtual organizations have given rise to a new category of
P2P systems called schema-based. In such systems each peer exposes its own
schema and the main objective is the efficient search across the P2P network by
processing each incoming query without overly consuming bandwidth. The
usability of these systems depends on effective techniques to find and retrieve
data; however, efficient and effective routing of content-based queries is a
challenging problem in P2P networks. This work was attended as an attempt to
motivate the use of mining algorithms and hypergraphs context to develop two
different methods that improve significantly the efficiency of P2P
communications. The proposed query routing methods direct the query to a set of
relevant peers in such way as to avoid network traffic and bandwidth
consumption. We compare the performance of the two proposed methods with the
baseline one and our experimental results prove that our proposed methods
generate impressive levels of performance and scalability.
"
338,"An Empirical Study of UDP (CBR) Packet Performance over AODV Single &
  Multi-Channel Parallel Transmission in MANET","  Mobile Ad-hoc Network is a temporary network which is the cooperative
engagement of a collection of standalone mobile nodes that are not connected to
any external network. It is a decentralized network where mobile nodes can be
easily deployed in almost any environment without sophisticated infrastructure
support. An empirical study has been done for AODV routing protocol under
single channel and multi channel environment using the tool NS2. To compare the
performance of AODV in the two environments, the simulation results have been
analyzed by graphical manner and trace file based on QoS metrics such as
throughput, packet drop, delay and jitter. The simulation result analysis
verifies the AODV routing protocol performances for single channel and multi
channel. After the analysis of the simulation scenario we suggest that use of
Parallel MAC (P-MAC) may enhance the performance for multi channel.
"
339,Towards a native toplevel for the OCaml language,"  This paper presents the current state of our work on an interactive toplevel
for the OCaml language based on the optimizing native code compiler and
runtime. Our native toplevel is up to 100 times faster than the default OCaml
toplevel, which is based on the byte code compiler and interpreter. It uses
Just-In-Time techniques to compile toplevel phrases to native code at runtime,
and currently works with various Unix-like systems running on x86 or x86-64
processors.
"
340,"Performance improvement of the software development project using the
  Value Management approach","  Improving performance and delivering value for customers have become a
central theme in business. The software industry has become an increasingly
important sector for the economy growth in Tunisia. This study aims to show how
using Value Management in the Tunisian software industry for project analysis
gives new insight about true project value and performance. This new approach
is considered as an appropriate tool for guiding the process of making
decisions. It offers tools in order to analyze the service value from the
customer and organization perspectives. The results showed that the VM allows
to have better performance in the software development project by linking
customer satisfaction and cost analysis. The present case shows to service
managers how they can benchmark project function to reduce their costs and
improve resource allocation taking into consideration what customers consider
important during their overall service experience. It can identify best
professional practices, orient decisions to improve service value
"
341,Stability of a Peer-to-Peer Communication System,"  This paper focuses on the stationary portion of file download in an
unstructured peer-to-peer network, which typically follows for many hours after
a flash crowd initiation. The model includes the case that peers can have some
pieces at the time of arrival. The contribution of the paper is to identify how
much help is needed from the seeds, either fixed seeds or peer seeds (which are
peers remaining in the system after obtaining a complete collection) to
stabilize the system. The dominant cause for instability is the missing piece
syndrome, whereby one piece becomes very rare in the network. It is shown that
stability can be achieved with only a small amount of help from peer
seeds--even with very little help from a fixed seed, peers need dwell as peer
seeds on average only long enough to upload one additional piece. The region of
stability is insensitive to the piece selection policy. Network coding can
substantially increase the region of stability in case a portion of the new
peers arrive with randomly coded pieces.
"
342,"Optimization strategies for parallel CPU and GPU implementations of a
  meshfree particle method","  Much of the current focus in high performance computing (HPC) for
computational fluid dynamics (CFD) deals with grid based methods. However,
parallel implementations for new meshfree particle methods such as Smoothed
Particle Hydrodynamics (SPH) are less studied. In this work, we present
optimizations for both central processing unit (CPU) and graphics processing
unit (GPU) of a SPH method. These optimization strategies can be further
applied to many other meshfree methods. The obtained performance for each
architecture and a comparison between the most efficient implementations for
CPU and GPU are shown.
"
343,An Improved Analytical Expression for Write Amplification in NAND Flash,"  Agarwal et al. gave an closed-form expression for write amplification in NAND
flash memory by finding the probability of a page being valid over the whole
flash memory. This paper gives an improved analytic expression for write
amplification in NAND flash memory by finding the probability of a page being
invalid over the block selected for garbage collection. The improved expression
uses Lambert W function. Through asymptotic analysis, write amplification is
shown to depend on overprovisioning factor only, consistent with the previous
work. Comparison with numerical simulations shows that the improved expression
achieves a more accurate prediction of write amplification. For example, when
the overprovisioning factor is 0.3, the expression proposed by this paper gives
a write amplification of 2.36 whereas that of the previous work gives 2.17,
when the actual value is 2.35.
"
344,"A Survey on Delay-Aware Resource Control for Wireless Systems --- Large
  Deviation Theory, Stochastic Lyapunov Drift and Distributed Stochastic
  Learning","  In this tutorial paper, a comprehensive survey is given on several major
systematic approaches in dealing with delay-aware control problems, namely the
equivalent rate constraint approach, the Lyapunov stability drift approach and
the approximate Markov Decision Process (MDP) approach using stochastic
learning. These approaches essentially embrace most of the existing literature
regarding delay-aware resource control in wireless systems. They have their
relative pros and cons in terms of performance, complexity and implementation
issues. For each of the approaches, the problem setup, the general solution and
the design methodology are discussed. Applications of these approaches to
delay-aware resource allocation are illustrated with examples in single-hop
wireless networks. Furthermore, recent results regarding delay-aware multi-hop
routing designs in general multi-hop networks are elaborated. Finally, the
delay performance of the various approaches are compared through simulations
using an example of the uplink OFDMA systems.
"
345,Understanding BitTorrent Through Real Measurements,"  In this paper the results of the BitTorrent measurement study are presented.
Two sources of BitTorrent data were utilized: meta-data files that describe the
content of resources shared by BitTorrent users and the logs of one of the
currently most popular BitTorrent clients - {\mu}Torrent. {\mu}Torrent is
founded upon a rather newly released UDP-based {\mu}TP protocol that is claimed
to be more efficient than TCP-based clients. Experimental data have been
collected for fifteen days from the popular torrent-discovery site
thepiratebay.org (more than 30,000 torrents were captured and analyzed). During
this period the activity and logs of an unmodified version of {\mu}Torrent
client downloading sessions have been also captured. The obtained experimental
results are swarm-oriented (not tracker-oriented as has been previously
researched), which has allowed us to look at BitTorrent and its users from an
exchanged resources perspective. Moreover, comparative analysis of the clients'
connections with and without {\mu}TP protocol is carried out to verify to what
extent {\mu}TP improves BitTorrent transmissions. To the authors' best
knowledge, none of the previous studies have addressed these issues.
"
346,"A Generalized Loss Network Model with Overflow for Capacity Planning of
  a Perinatal Network","  We develop a generalized loss network framework for capacity planning of a
perinatal network in the UK. Decomposing the network by hospitals, each unit is
analyzed with a GI/G/c/0 overflow loss network model. A two-moment
approximation is performed to obtain the steady state solution of the GI/G/c/0
loss systems, and expressions for rejection probability and overflow
probability have been derived. Using the model framework, the number of
required cots can be estimated based on the rejection probability at each level
of care of the neonatal units in a network. The generalization ensures that the
model can be applied to any perinatal network for renewal arrival and discharge
processes.
"
347,Exploring Oracle RDBMS latches using Solaris DTrace,"  Rise of hundreds cores technologies bring again to the first plan the problem
of interprocess synchronization in database engines. Spinlocks are widely used
in contemporary DBMS to synchronize processes at microsecond timescale. Latches
are Oracle RDBMS specific spinlocks. The latch contention is common to observe
in contemporary high concurrency OLTP environments.
  In contrast to system spinlocks used in operating systems kernels, latches
work in user context. Such user level spinlocks are influenced by context
preemption and multitasking. Until recently there were no direct methods to
measure effectiveness of user spinlocks. This became possible with the
emergence of Solaris 10 Dynamic Tracing framework. DTrace allows tracing and
profiling both OS and user applications.
  This work investigates the possibilities to diagnose and tune Oracle latches.
It explores the contemporary latch realization and spinning-blocking
strategies, analyses corresponding statistic counters.
  A mathematical model developed to estimate analytically the effect of tuning
_SPIN_COUNT value.
"
348,Updatable Queue Protocol Based On TCP For Virtual Reality Environment,"  The variance in number and types of tasks required to be implemented within
Distributed Virtual Environments (DVE) highlights the needs for communication
protocols can achieve consistency. In addition, these applications have to
handle an increasing number of participants and deal with the difficult problem
of scalability. Moreover, the real-time requirements of these applications make
the scalability problem more difficult to solve. In this paper, we have
implemented Updatable Queue Abstraction protocol (UQA) on TCP (TCP-UQA) and
compared it with original TCP, UDP, and Updatable Queue Abstraction based on
UDP (UDP-UQA) protocols. Results showed that TCP-UQA was the best in queue
management.
"
349,"Performance Analysis of Sequential Method for Handover in Cognitive
  Radio Systems","  Powerful spectrum handover schemes enable cognitive radios (CRs) to use
transmission opportunities in primary users' channels appropriately. In this
paper, we consider the cognitive access of primary channels by a secondary
user. We evaluate the average detection time and the maximum achievable average
throughput of the secondary user when the sequential method for hand-over
(SMHO) is used. We assume that a prior knowledge of the primary users' presence
and absence probabilities are available. When investigating the maximum
achievable throughput of the secondary user, we end into an optimization
problem, in which the optimum value of sensing time must be selected. In our
optimization problem, we take into account the spectrum hand over due to false
detection of the primary user. We also propose a weighted based hand-over
(WBHO) scheme in which the impacts of channels conditions and primary users'
presence probability are considered. This Spectrum handover scheme provides
higher average throughput for the SU than the SMHO method. The tradeoff between
the maximum achievable throughput and consumed energy is discussed, and finally
an energy efficient optimization formulation for finding a proper sensing time
is provided.
"
350,Performance Evaluation of Different Scheduling Algorithms in WiMAX,"  Worldwide Interoperability for Microwave Access (WiMAX) networks were
expected to be the main Broadband Wireless Access (BWA) technology that
provided several services such as data, voice, and video services including
different classes of Quality of Services (QoS), which in turn were defined by
IEEE 802.16 standard. Scheduling in WiMAX became one of the most challenging
issues, since it was responsible for distributing available resources of the
network among all users; this leaded to the demand of constructing and
designing high efficient scheduling algorithms in order to improve the network
utilization, to increase the network throughput, and to minimize the end-to-end
delay. In this study, we presenedt a simulation study to measure the
performance of several scheduling algorithms in WiMAX, which were Strict
Priority algorithm, Round-Robin (RR), Weighted Round Robin (WRR), Weighted Fair
Queuing (WFQ), Self-Clocked Fair (SCF), and Diff-Serv Algorithm.
"
351,"Model Checking Probabilistic Real-Time Properties for Service-Oriented
  Systems with Service Level Agreements","  The assurance of quality of service properties is an important aspect of
service-oriented software engineering. Notations for so-called service level
agreements (SLAs), such as the Web Service Level Agreement (WSLA) language,
provide a formal syntax to specify such assurances in terms of (legally
binding) contracts between a service provider and a customer. On the other
hand, formal methods for verification of probabilistic real-time behavior have
reached a level of expressiveness and efficiency which allows to apply them in
real-world scenarios. In this paper, we suggest to employ the recently
introduced model of Interval Probabilistic Timed Automata (IPTA) for formal
verification of QoS properties of service-oriented systems. Specifically, we
show that IPTA in contrast to Probabilistic Timed Automata (PTA) are able to
capture the guarantees specified in SLAs directly. A particular challenge in
the analysis of IPTA is the fact that their naive semantics usually yields an
infinite set of states and infinitely-branching transitions. However, using
symbolic representations, IPTA can be analyzed rather efficiently. We have
developed the first implementation of an IPTA model checker by extending the
PRISM tool and show that model checking IPTA is only slightly more expensive
than model checking comparable PTA.
"
352,Parametric Estimation of the Ultimate Size of Hypercomputers,"  The performance of the emerging petaflops-scale supercomputers of the nearest
future (hypercomputers) will be governed not only by the clock frequency of the
processing nodes or by the width of the system bus, but also by such factors as
the overall power consumption and the geometric size. In this paper, we study
the influence of such parameters on one of the most important characteristics
of a general purpose computer - on the degree of multithreading that must be
present in an application to make the use of the hypercomputer justifiable. Our
major finding is that for the class of applications with purely random memory
access patterns ""super-fast computing"" and ""high-performance computing"" are
essentially synonyms for ""massively-parallel computing.""
"
353,Greening File Distribution: Centralized or Distributed?,"  Despite file-distribution applications are responsible for a major portion of
the current Internet traffic, so far little effort has been dedicated to study
file distribution from the point of view of energy efficiency. In this paper,
we present a first approach at the problem of energy efficiency for file
distribution. Specifically, we first demonstrate that the general problem of
minimizing energy consumption in file distribution in heterogeneous settings is
NP-hard. For homogeneous settings, we derive tight lower bounds on energy
consumption, and we design a family of algorithms that achieve these bounds.
Our results prove that collaborative p2p schemes achieve up to 50% energy
savings with respect to the best available centralized file distribution
scheme. Through simulation, we demonstrate that in more realistic cases (e.g.,
considering network congestion, and link variability across hosts) we validate
this observation, since our collaborative algorithms always achieve significant
energy savings with respect to the power consumption of centralized file
distribution systems.
"
354,"Sensing Matrix Setting Schemes for Cognitive Networks and Their
  Performance Analysis","  Powerful spectrum decision schemes enable cognitive radios (CRs) to find
transmission opportunities in spectral resources allocated exclusively to the
primary users. One of the key effecting factor on the CR network throughput is
the spectrum sensing sequence used by each secondary user. In this paper,
secondary users' throughput maximization through finding an appropriate sensing
matrix (SM) is investigated. To this end, first the average throughput of the
CR network is evaluated for a given SM. Then, an optimization problem based on
the maximization of the network throughput is formulated in order to find the
optimal SM. As the optimum solution is very complicated, to avoid its major
challenges, three novel sub optimum solutions for finding an appropriate SM are
proposed for various cases including perfect and non-perfect sensing. Despite
of having less computational complexities as well as lower consumed energies,
the proposed solutions perform quite well compared to the optimum solution (the
optimum SM). The structure and performance of the proposed SM setting schemes
are discussed in detail and a set of illustrative simulation results is
presented to validate their efficiencies.
"
355,On Mean Field Convergence and Stationary Regime,"  Assume that a family of stochastic processes on some Polish space $E$
converges to a deterministic process; the convergence is in distribution (hence
in probability) at every fixed point in time. This assumption holds for a large
family of processes, among which many mean field interaction models and is
weaker than previously assumed. We show that any limit point of an invariant
probability of the stochastic process is an invariant probability of the
deterministic process. The results are valid in discrete and in continuous
time.
"
356,Solving Dense Generalized Eigenproblems on Multi-threaded Architectures,"  We compare two approaches to compute a portion of the spectrum of dense
symmetric definite generalized eigenproblems: one is based on the reduction to
tridiagonal form, and the other on the Krylov-subspace iteration. Two
large-scale applications, arising in molecular dynamics and material science,
are employed to investigate the contributions of the application, architecture,
and parallelism of the method to the performance of the solvers. The
experimental results on a state-of-the-art 8-core platform, equipped with a
graphics processing unit (GPU), reveal that in real applications, iterative
Krylov-subspace methods can be a competitive approach also for the solution of
dense problems.
"
357,"The Potential of Synergistic Static, Dynamic and Speculative Loop Nest
  Optimizations for Automatic Parallelization","  Research in automatic parallelization of loop-centric programs started with
static analysis, then broadened its arsenal to include dynamic
inspection-execution and speculative execution, the best results involving
hybrid static-dynamic schemes. Beyond the detection of parallelism in a
sequential program, scalable parallelization on many-core processors involves
hard and interesting parallelism adaptation and mapping challenges. These
challenges include tailoring data locality to the memory hierarchy, structuring
independent tasks hierarchically to exploit multiple levels of parallelism,
tuning the synchronization grain, balancing the execution load, decoupling the
execution into thread-level pipelines, and leveraging heterogeneous hardware
with specialized accelerators. The polyhedral framework allows to model,
construct and apply very complex loop nest transformations addressing most of
the parallelism adaptation and mapping challenges. But apart from
hardware-specific, back-end oriented transformations (if-conversion, trace
scheduling, value prediction), loop nest optimization has essentially ignored
dynamic and speculative techniques. Research in polyhedral compilation recently
reached a significant milestone towards the support of dynamic, data-dependent
control flow. This opens a large avenue for blending dynamic analyses and
speculative techniques with advanced loop nest optimizations. Selecting
real-world examples from SPEC benchmarks and numerical kernels, we make a case
for the design of synergistic static, dynamic and speculative loop
transformation techniques. We also sketch the embedding of dynamic information,
including speculative assumptions, in the heart of affine transformation search
spaces.
"
358,"Simple and Effective Dynamic Provisioning for Power-Proportional Data
  Centers","  Energy consumption represents a significant cost in data center operation. A
large fraction of the energy, however, is used to power idle servers when the
workload is low. Dynamic provisioning techniques aim at saving this portion of
the energy, by turning off unnecessary servers. In this paper, we explore how
much performance gain can knowing future workload information brings to dynamic
provisioning. In particular, we study the dynamic provisioning problem under
the cost model that a running server consumes a fixed amount energy per unit
time, and develop online solutions with and without future workload information
available. We first reveal an elegant structure of the off-line dynamic
provisioning problem, which allows us to characterize and achieve the optimal
solution in a {}""divide-and-conquer"" manner. We then exploit this insight to
design three online algorithms with competitive ratios $2-\alpha$,
$(e-\alpha)/(e-1)\approx1.58-\alpha/(e-1)$ and $e/(e-1+\alpha)$, respectively,
where $0\leq\alpha\leq1$ is the fraction of a critical window in which future
workload information is available. A fundamental observation is that
\emph{future workload information beyond the critical window will not}
\emph{improve dynamic provisioning performance}. Our algorithms are
decentralized and are simple to implement. We demonstrate their effectiveness
in simulations using real-world traces. We also compare their performance with
state-of-the-art solutions.
"
359,"Performance engineering for the Lattice Boltzmann method on GPGPUs:
  Architectural requirements and performance results","  GPUs offer several times the floating point performance and memory bandwidth
of current standard two socket CPU servers, e.g. NVIDIA C2070 vs. Intel Xeon
Westmere X5650. The lattice Boltzmann method has been established as a flow
solver in recent years and was one of the first flow solvers to be successfully
ported and that performs well on GPUs. We demonstrate advanced optimization
strategies for a D3Q19 lattice Boltzmann based incompressible flow solver for
GPGPUs and CPUs based on NVIDIA CUDA and OpenCL. Since the implemented
algorithm is limited by memory bandwidth, we concentrate on improving memory
access. Basic data layout issues for optimal data access are explained and
discussed. Furthermore, the algorithmic steps are rearranged to improve
scattered access of the GPU memory. The importance of occupancy is discussed as
well as optimization strategies to improve overall concurrency. We arrive at a
well-optimized GPU kernel, which is integrated into a larger framework that can
handle single phase fluid flow simulations as well as particle-laden flows. Our
3D LBM GPU implementation reaches up to 650 MLUPS in single precision and 290
MLUPS in double precision on an NVIDIA Tesla C2070.
"
360,Stabilization of Branching Queueing Networks,"  Queueing networks are gaining attraction for the performance analysis of
parallel computer systems. A Jackson network is a set of interconnected
servers, where the completion of a job at server i may result in the creation
of a new job for server j. We propose to extend Jackson networks by ""branching""
and by ""control"" features. Both extensions are new and substantially expand the
modelling power of Jackson networks. On the other hand, the extensions raise
computational questions, particularly concerning the stability of the networks,
i.e, the ergodicity of the underlying Markov chain. We show for our extended
model that it is decidable in polynomial time if there exists a controller that
achieves stability. Moreover, if such a controller exists, one can efficiently
compute a static randomized controller which stabilizes the network in a very
strong sense; in particular, all moments of the queue sizes are finite.
"
361,"Improving TCP Performance over Wireless Network with Frequent
  Disconnections","  Presented in this paper is the solution to the problem that arises when the
TCP/IP protocol suite is used to provide Internet connectivity through mobile
terminals over emerging 802.11 wireless links. Taking into consideration the
strong drive towards wireless Internet access through mobile terminals, the
problem of frequent disconnections causing serial timeouts is examined and
analyzed, with the help of extensive simulations. After a detailed review of
wireless link loss recovery mechanism and identification of related problems, a
new scheme with modifications at link layer and transport layer is proposed.
The proposed modifications which depend on interaction between two layers (i)
reduce the idle time before transmission at TCP by preventing timeout
occurrences and (ii) decouple the congestion control from recovery of the
losses due to link failure. Results of simulation based experiments demonstrate
considerable performance improvement with the proposed modifications over the
conventional TCP, when a wireless sender is experiencing frequent link
failures.
"
362,A Temporal Approach to Stochastic Network Calculus,"  Stochastic network calculus is a newly developed theory for stochastic
service guarantee analysis of computer networks. In the current stochastic
network calculus literature, its fundamental models are based on the cumulative
amount of traffic or cumulative amount of service. However, there are network
scenarios where direct application of such models is difficult. This paper
presents a temporal approach to stochastic network calculus. The key idea is to
develop models and derive results from the time perspective. Particularly, we
define traffic models and service models based on the cumulative packet
inter-arrival time and the cumulative packet service time, respectively.
Relations among these models as well as with the existing models in the
literature are established. In addition, we prove the basic properties of the
proposed models, such as delay bound and backlog bound, output
characterization, concatenation property and superposition property. These
results form a temporal stochastic network calculus and compliment the existing
results.
"
363,Implementation of a Parallel Tree Method on a GPU,"  The kd-tree is a fundamental tool in computer science. Among other
applications, the application of kd-tree search (by the tree method) to the
fast evaluation of particle interactions and neighbor search is highly
important, since the computational complexity of these problems is reduced from
O(N^2) for a brute force method to O(N log N) for the tree method, where N is
the number of particles. In this paper, we present a parallel implementation of
the tree method running on a graphics processing unit (GPU). We present a
detailed description of how we have implemented the tree method on a Cypress
GPU. An optimization that we found important is localized particle ordering to
effectively utilize cache memory. We present a number of test results and
performance measurements. Our results show that the execution of the tree
traversal in a force calculation on a GPU is practical and efficient.
"
364,"A Study on Using Uncertain Time Series Matching Algorithms in MapReduce
  Applications","  In this paper, we study CPU utilization time patterns of several Map-Reduce
applications. After extracting running patterns of several applications, the
patterns with their statistical information are saved in a reference database
to be later used to tweak system parameters to efficiently execute unknown
applications in future. To achieve this goal, CPU utilization patterns of new
applications along with its statistical information are compared with the
already known ones in the reference database to find/predict their most
probable execution patterns. Because of different patterns lengths, the Dynamic
Time Warping (DTW) is utilized for such comparison; a statistical analysis is
then applied to DTWs' outcomes to select the most suitable candidates.
Moreover, under a hypothesis, another algorithm is proposed to classify
applications under similar CPU utilization patterns. Three widely used text
processing applications (WordCount, Distributed Grep, and Terasort) and another
application (Exim Mainlog parsing) are used to evaluate our hypothesis in
tweaking system parameters in executing similar applications. Results were very
promising and showed effectiveness of our approach on 5-node Map-Reduce
platform
"
365,"Sparse matrix-vector multiplication on GPGPU clusters: A new storage
  format and a scalable implementation","  Sparse matrix-vector multiplication (spMVM) is the dominant operation in many
sparse solvers. We investigate performance properties of spMVM with matrices of
various sparsity patterns on the nVidia ""Fermi"" class of GPGPUs. A new ""padded
jagged diagonals storage"" (pJDS) format is proposed which may substantially
reduce the memory overhead intrinsic to the widespread ELLPACK-R scheme. In our
test scenarios the pJDS format cuts the overall spMVM memory footprint on the
GPGPU by up to 70%, and achieves 95% to 130% of the ELLPACK-R performance.
Using a suitable performance model we identify performance bottlenecks on the
node level that invalidate some types of matrix structures for efficient
multi-GPGPU parallelization. For appropriate sparsity patterns we extend
previous work on distributed-memory parallel spMVM to demonstrate a scalable
hybrid MPI-GPGPU code, achieving efficient overlap of communication and
computation.
"
366,Transmission capacity of wireless networks,"  Transmission capacity (TC) is a performance metric for wireless networks that
measures the spatial intensity of successful transmissions per unit area,
subject to a constraint on the permissible outage probability (where outage
occurs when the SINR at a receiver is below a threshold). This volume gives a
unified treatment of the TC framework that has been developed by the authors
and their collaborators over the past decade. The mathematical framework
underlying the analysis (reviewed in Ch. 2) is stochastic geometry: Poisson
point processes model the locations of interferers, and (stable) shot noise
processes represent the aggregate interference seen at a receiver. Ch. 3
presents TC results (exact, asymptotic, and bounds) on a simple model in order
to illustrate a key strength of the framework: analytical tractability yields
explicit performance dependence upon key model parameters. Ch. 4 presents
enhancements to this basic model --- channel fading, variable link distances,
and multi-hop. Ch. 5 presents four network design case studies well-suited to
TC: i) spectrum management, ii) interference cancellation, iii) signal
threshold transmission scheduling, and iv) power control. Ch. 6 studies the TC
when nodes have multiple antennas, which provides a contrast vs. classical
results that ignore interference.
"
367,"Distributed Multiuser Sequential Channel Sensing Schemes in Multichannel
  Cognitive Radio Networks","  This paper has been withdrawn by the author due to a crucial problem
associated with Figs. 2 and 3.
"
368,"Some Observations on Optimal Frequency Selection in DVFS-based Energy
  Consumption Minimization","  In recent years, the issue of energy consumption in parallel and distributed
computing systems has attracted a great deal of attention. In response to this,
many energy-aware scheduling algorithms have been developed primarily using the
dynamic voltage-frequency scaling (DVFS) capability which has been incorporated
into recent commodity processors. Majority of these algorithms involve two
passes: schedule generation and slack reclamation. The former pass involves the
redistribution of tasks among DVFS-enabled processors based on a given cost
function that includes makespan and energy consumption; and, while the latter
pass is typically achieved by executing individual tasks with slacks at a lower
processor frequency. In this paper, a new slack reclamation algorithm is
proposed by approaching the energy reduction problem from a different angle.
Firstly, the problem of task slack reclamation by using combinations of
processors' frequencies is formulated. Secondly, several proofs are provided to
show that (1) if the working frequency set of processor is assumed to be
continues, the optimal energy will be always achieved by using only one
frequency, (2) for real processors with a discrete set of working frequencies,
the optimal energy is always achieved by using at most two frequencies, and (3)
these two frequencies are adjacent/neighbouring when processor energy
consumption is a convex function of frequency. Thirdly, a novel algorithm to
find the best combination of frequencies to result the optimal energy is
presented. The presented algorithm has been evaluated based on results obtained
from experiments with three different sets of task graphs: 3000 randomly
generated task graphs, and 600 task graphs for two popular applications
(Gauss-Jordan and LU decomposition). The results show the superiority of the
proposed algorithm in comparison with other techniques.
"
369,Early Performance Prediction of Web Services,"  Web Service is an interface which implements business logic. Performance is
an important quality aspect of Web services because of their distributed
nature. Predicting the performance of web services during early stages of
software development is significant. In this paper we model web service using
Unified Modeling Language, Use Case Diagram, Sequence Diagram, Deployment
Diagram. We obtain the Performance metrics by simulating the web services model
using a simulation tool Simulation of Multi-Tier Queuing Architecture. We have
identified the bottle neck resources.
"
370,"Optimizing the Performance of Streaming Numerical Kernels on the IBM
  Blue Gene/P PowerPC 450 Processor","  Several emerging petascale architectures use energy-efficient processors with
vectorized computational units and in-order thread processing. On these
architectures the sustained performance of streaming numerical kernels,
ubiquitous in the solution of partial differential equations, represents a
challenge despite the regularity of memory access. Sophisticated optimization
techniques are required to fully utilize the Central Processing Unit (CPU).
  We propose a new method for constructing streaming numerical kernels using a
high-level assembly synthesis and optimization framework. We describe an
implementation of this method in Python targeting the IBM Blue Gene/P
supercomputer's PowerPC 450 core. This paper details the high-level design,
construction, simulation, verification, and analysis of these kernels utilizing
a subset of the CPU's instruction set.
  We demonstrate the effectiveness of our approach by implementing several
three-dimensional stencil kernels over a variety of cached memory scenarios and
analyzing the mechanically scheduled variants, including a 27-point stencil
achieving a 1.7x speedup over the best previously published results.
"
371,"Reengineering multi tiered enterprise business applications for
  performance enhancement and reciprocal or rectangular hyperbolic relation of
  variation of data transportation time with row pre-fetch size of relational
  database drivers","  Reengineering multi tiered enterprise business applications for performance
enhancement and reciprocal or rectangular hyperbolic relation of variation of
data transportation time with row pre-fetch size of relational database drivers
"
372,"Cross-entropy optimisation of importance sampling parameters for
  statistical model checking","  Statistical model checking avoids the exponential growth of states associated
with probabilistic model checking by estimating properties from multiple
executions of a system and by giving results within confidence bounds. Rare
properties are often very important but pose a particular challenge for
simulation-based approaches, hence a key objective under these circumstances is
to reduce the number and length of simulations necessary to produce a given
level of confidence. Importance sampling is a well-established technique that
achieves this, however to maintain the advantages of statistical model checking
it is necessary to find good importance sampling distributions without
considering the entire state space.
  Motivated by the above, we present a simple algorithm that uses the notion of
cross-entropy to find the optimal parameters for an importance sampling
distribution. In contrast to previous work, our algorithm uses a low
dimensional vector of parameters to define this distribution and thus avoids
the often intractable explicit representation of a transition matrix. We show
that our parametrisation leads to a unique optimum and can produce many orders
of magnitude improvement in simulation efficiency. We demonstrate the efficacy
of our methodology by applying it to models from reliability engineering and
biochemistry.
"
373,A Note on Disk Drag Dynamics,"  The electrical power consumed by typical magnetic hard disk drives (HDD) not
only increases linearly with the number of spindles but, more significantly, it
increases as very fast power-laws of speed (RPM) and diameter. Since the
theoretical basis for this relationship is neither well-known nor readily
accessible in the literature, we show how these exponents arise from
aerodynamic disk drag and discuss their import for green storage capacity
planning.
"
374,Generating a Performance Stochastic Model from UML Specifications,"  Since its initiation by Connie Smith, the process of Software Performance
Engineering (SPE) is becoming a growing concern. The idea is to bring
performance evaluation into the software design process. This suitable
methodology allows software designers to determine the performance of software
during design. Several approaches have been proposed to provide such
techniques. Some of them propose to derive from a UML (Unified Modeling
Language) model a performance model such as Stochastic Petri Net (SPN) or
Stochastic process Algebra (SPA) models. Our work belongs to the same category.
We propose to derive from a UML model a Stochastic Automata Network (SAN) in
order to obtain performance predictions. Our approach is more flexible due to
the SAN modularity and its high resemblance to UML' state-chart diagram.
"
375,"Effect of Packet Delay Variation on Video-Voice over DiffServ-MPLS in
  IPv4-IPv6 Networks","  Over the last years, we have witnessed a rapid deployment of real-time
applications on the Internet as well as many research works about Quality of
Service (QoS), in particular IPv4 (Internet Protocol version 4). The inevitable
exhaustion of the remaining IPv4 address pool has become progressively evident.
As the evolution of Internet Protocol (IP) continues, the deployment of IPv6
QoS is underway. Today, there is limited experience in the deployment of QoS
for IPv6 traffic in MPLS backbone networks in conjunction with DiffServ
(Differentiated Services) support. DiffServ itself does not have the ability to
control the traffic which has been taken for end-to-end path while a number of
links of the path are congested. In contrast, MPLS Traffic Engineering (TE) is
accomplished to control the traffic and can set up end-to-end routing path
before data has been forwarded. From the evolution of IPv4 QoS solutions, we
know that the integration of DiffServ and MPLS TE satisfies the guaranteed QoS
requirement for real-time applications. This paper presents a QoS performance
study of real-time applications such as voice and video conferencing in terms
of Packet Delay Variation (PDV) over DiffServ with or without MPLS TE in
IPv4/IPv6 networks using Optimized Network Engineering Tool (OPNET). We also
study the interaction of Expedited Forwarding (EF), Assured Forwarding (AF)
traffic aggregation, link congestion, as well as the effect of performance
metric such as PDV. The effectiveness of DiffServ and MPLS TE integration in
IPv4/IPv6 network is illustrated and analyzed. This paper shows that IPv6
experiences more PDV than their IPv4 counterparts.
"
376,"Technical Report on Hypergraph-Partitioning-Based Models and Methods for
  Exploiting Cache Locality in Sparse-Matrix Vector Multiplication","  The sparse matrix-vector multiplication (SpMxV) is a kernel operation widely
used in iterative linear solvers. The same sparse matrix is multiplied by a
dense vector repeatedly in these solvers. Matrices with irregular sparsity
patterns make it difficult to utilize cache locality effectively in SpMxV
computations. In this work, we investigate single- and multiple-SpMxV
frameworks for exploiting cache locality in SpMxV computations. For the
single-SpMxV framework, we propose two cache-size-aware top-down
row/column-reordering methods based on 1D and 2D sparse matrix partitioning by
utilizing the column-net and enhancing the row-column-net hypergraph models of
sparse matrices. The multiple-SpMxV framework depends on splitting a given
matrix into a sum of multiple nonzero-disjoint matrices so that the SpMxV
operation is performed as a sequence of multiple input- and output- dependent
SpMxV operations. For an effective matrix splitting required in this framework,
we propose a cache- size-aware top-down approach based on 2D sparse matrix
partitioning by utilizing the row-column-net hypergraph model. For this
framework, we also propose two methods for effective ordering of individual
SpMxV operations. The primary objective in all of the three methods is to
maximize the exploitation of temporal locality. We evaluate the validity of our
models and methods on a wide range of sparse matrices using both cache-miss
simulations and actual runs by using OSKI. Experimental results show that
proposed methods and models outperform state-of-the-art schemes.
"
377,On the Reliability of RAID Systems: An Argument for More Check Drives,"  In this paper we address issues of reliability of RAID systems. We focus on
""big data"" systems with a large number of drives and advanced error correction
schemes beyond \RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and
thus we assume that the RAID consists of $N$ data drives and $M$ check drives.
The RAID fails only if the combined number of failed drives and sector errors
exceeds $M$, a property of Reed-Solomon codes.
  We review a number of models considered in the literature and build upon them
to construct models usable for a large number of data and check drives. We
attempt to account for a significant number of factors that affect RAID
reliability, such as drive replacement or lack thereof, mistakes during service
such as replacing the wrong drive, delayed repair, and the finite duration of
RAID reconstruction. We evaluate the impact of sector failures that do not
result in drive replacement.
  The reader who needs to consider large $M$ and $N$ will find applicable
mathematical techniques concisely summarized here, and should be able to apply
them to similar problems. Most methods are based on the theory of continuous
time Markov chains, but we move beyond this framework when we consider the
fixed time to rebuild broken hard drives, which we model using systems of delay
and partial differential equations.
  One universal statement is applicable across various models: increasing the
number of check drives in all cases increases the reliability of the system,
and is vastly superior to other approaches of ensuring reliability such as
mirroring.
"
378,"Delay Asymptotics with Retransmissions and Incremental Redundancy Codes
  over Erasure Channels","  Recent studies have shown that retransmissions can cause heavy-tailed
transmission delays even when packet sizes are light-tailed. Moreover, the
impact of heavy-tailed delays persists even when packets size are upper
bounded. The key question we study in this paper is how the use of coding
techniques to transmit information, together with different system
configurations, would affect the distribution of delay. To investigate this
problem, we model the underlying channel as a Markov modulated binary erasure
channel, where transmitted bits are either received successfully or erased.
Erasure codes are used to encode information prior to transmission, which
ensures that a fixed fraction of the bits in the codeword can lead to
successful decoding. We use incremental redundancy codes, where the codeword is
divided into codeword trunks and these trunks are transmitted one at a time to
provide incremental redundancies to the receiver until the information is
recovered. We characterize the distribution of delay under two different
scenarios: (I) Decoder uses memory to cache all previously successfully
received bits. (II) Decoder does not use memory, where received bits are
discarded if the corresponding information cannot be decoded. In both cases, we
consider codeword length with infinite and finite support. From a theoretical
perspective, our results provide a benchmark to quantify the tradeoff between
system complexity and the distribution of delay.
"
379,"Performance Evaluation of the Random Replacement Policy for Networks of
  Caches","  The overall performance of content distribution networks as well as recently
proposed information-centric networks rely on both memory and bandwidth
capacities. In this framework, the hit ratio is the key performance indicator
which captures the bandwidth / memory tradeoff for a given global
performance.This paper focuses on the estimation of the hit ratio in a network
of caches that employ the Random replacement policy. Assuming that requests are
independent and identically distributed, general expressions of miss
probabilities for a single Random cache are provided as well as exact results
for specific popularity distributions. Moreover, for any Zipf popularity
distribution with exponent $\alpha$ > 1, we obtain asymptotic equivalents for
the miss probability in the case of large cache size. We extend the analysis to
networks of Random caches, when the topology is either a line or a homogeneous
tree. In that case, approximations for miss probabilities across the network
are derived by assuming that miss events at any node occur independently in
time; the obtained results are compared to the same network using the
Least-Recently-Used discipline, already addressed in the literature. We further
analyze the case of a mixed tandem cache network where the two nodes employ
either Random or Least-Recently-Used policies. In all scenarios, asymptotic
formulas and approximations are extensively compared to simulations and shown
to perform very well. Finally, our results enable us to propose recommendations
for cache replacement disciplines in a network dedicated to content
distribution. These results also hold for a cache using the First-In-First-Out
policy.
"
380,Secure Compressed Reading in Smart Grids,"  Smart Grids measure energy usage in real-time and tailor supply and delivery
accordingly, in order to improve power transmission and distribution. For the
grids to operate effectively, it is critical to collect readings from
massively-installed smart meters to control centers in an efficient and secure
manner. In this paper, we propose a secure compressed reading scheme to address
this critical issue. We observe that our collected real-world meter data
express strong temporal correlations, indicating they are sparse in certain
domains. We adopt Compressed Sensing technique to exploit this sparsity and
design an efficient meter data transmission scheme. Our scheme achieves
substantial efficiency offered by compressed sensing, without the need to know
beforehand in which domain the meter data are sparse. This is in contrast to
traditional compressed-sensing based scheme where such sparse-domain
information is required a priori. We then design specific dependable scheme to
work with our compressed sensing based data transmission scheme to make our
meter reading reliable and secure. We provide performance guarantee for the
correctness, efficiency, and security of our proposed scheme. Through analysis
and simulations, we demonstrate the effectiveness of our schemes and compare
their performance to prior arts.
"
381,Balancing Work and Size with Bounded Buffers,"  We consider the fundamental problem of managing a bounded size queue buffer
where traffic consists of packets of varying size, where each packet requires
several rounds of processing before it can be transmitted from the queue
buffer. The goal in such an environment is to maximize the overall size of
packets that are successfully transmitted. This model is motivated by the
ever-growing ubiquity of network processors architectures, which must deal with
heterogeneously-sized traffic, with heterogeneous processing requirements. Our
work addresses the tension between two conflicting algorithmic approaches in
such settings: the tendency to favor packets with fewer processing
requirements, thus leading to fast contributions to the accumulated throughput,
as opposed to preferring packets of larger size, which imply a large increase
in throughput at each step. We present a model for studying such systems, and
present competitive algorithms whose performance depend on the maximum size a
packet may have, and maximum amount of processing a packet may require. We
further provide lower bounds on algorithms performance in such settings.
"
382,"Efficient Spherical Harmonic Transforms aimed at pseudo-spectral
  numerical simulations","  In this paper, we report on very efficient algorithms for the spherical
harmonic transform (SHT). Explicitly vectorized variations of the algorithm
based on the Gauss-Legendre quadrature are discussed and implemented in the
SHTns library which includes scalar and vector transforms. The main
breakthrough is to achieve very efficient on-the-fly computations of the
Legendre associated functions, even for very high resolutions, by taking
advantage of the specific properties of the SHT and the advanced capabilities
of current and future computers. This allows us to simultaneously and
significantly reduce memory usage and computation time of the SHT. We measure
the performance and accuracy of our algorithms. Even though the complexity of
the algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum
harmonic degree of the transform), they perform much better than any third
party implementation, including lower complexity algorithms, even for
truncations as high as N=1023. SHTns is available at
https://bitbucket.org/nschaeff/shtns as open source software.
"
383,Scaling Datalog for Machine Learning on Big Data,"  In this paper, we present the case for a declarative foundation for
data-intensive machine learning systems. Instead of creating a new system for
each specific flavor of machine learning task, or hardcoding new optimizations,
we argue for the use of recursive queries to program a variety of machine
learning systems. By taking this approach, database query optimization
techniques can be utilized to identify effective execution plans, and the
resulting runtime plans can be executed on a single unified data-parallel query
processing engine. As a proof of concept, we consider two programming
models--Pregel and Iterative Map-Reduce-Update---from the machine learning
domain, and show how they can be captured in Datalog, tuned for a specific
task, and then compiled into an optimized physical plan. Experiments performed
on a large computing cluster with real data demonstrate that this declarative
approach can provide very good performance while offering both increased
generality and programming ease.
"
384,Fluid Model Checking,"  In this paper we investigate a potential use of fluid approximation
techniques in the context of stochastic model checking of CSL formulae. We
focus on properties describing the behaviour of a single agent in a (large)
population of agents, exploiting a limit result known also as fast simulation.
In particular, we will approximate the behaviour of a single agent with a
time-inhomogeneous CTMC which depends on the environment and on the other
agents only through the solution of the fluid differential equation. We will
prove the asymptotic correctness of our approach in terms of satisfiability of
CSL formulae and of reachability probabilities. We will also present a
procedure to model check time-inhomogeneous CTMC against CSL formulae.
"
385,Performance Analysis of l_0 Norm Constraint Least Mean Square Algorithm,"  As one of the recently proposed algorithms for sparse system identification,
$l_0$ norm constraint Least Mean Square ($l_0$-LMS) algorithm modifies the cost
function of the traditional method with a penalty of tap-weight sparsity. The
performance of $l_0$-LMS is quite attractive compared with its various
precursors. However, there has been no detailed study of its performance. This
paper presents all-around and throughout theoretical performance analysis of
$l_0$-LMS for white Gaussian input data based on some reasonable assumptions.
Expressions for steady-state mean square deviation (MSD) are derived and
discussed with respect to algorithm parameters and system sparsity. The
parameter selection rule is established for achieving the best performance.
Approximated with Taylor series, the instantaneous behavior is also derived. In
addition, the relationship between $l_0$-LMS and some previous arts and the
sufficient conditions for $l_0$-LMS to accelerate convergence are set up.
Finally, all of the theoretical results are compared with simulations and are
shown to agree well in a large range of parameter setting.
"
386,"Proof of Convergence and Performance Analysis for Sparse Recovery via
  Zero-point Attracting Projection","  A recursive algorithm named Zero-point Attracting Projection (ZAP) is
proposed recently for sparse signal reconstruction. Compared with the reference
algorithms, ZAP demonstrates rather good performance in recovery precision and
robustness. However, any theoretical analysis about the mentioned algorithm,
even a proof on its convergence, is not available. In this work, a strict proof
on the convergence of ZAP is provided and the condition of convergence is put
forward. Based on the theoretical analysis, it is further proved that ZAP is
non-biased and can approach the sparse solution to any extent, with the proper
choice of step-size. Furthermore, the case of inaccurate measurements in noisy
scenario is also discussed. It is proved that disturbance power linearly
reduces the recovery precision, which is predictable but not preventable. The
reconstruction deviation of $p$-compressible signal is also provided. Finally,
numerical simulations are performed to verify the theoretical analysis.
"
387,"Performance Analysis for Bandwidth Allocation in IEEE 802.16 Broadband
  Wireless Networks using BMAP Queueing","  This paper presents a performance analysis for the bandwidth allocation in
IEEE 802.16 broadband wireless access (BWA) networks considering the
packet-level quality-of-service (QoS) constraints. Adaptive Modulation and
Coding (AMC) rate based on IEEE 802.16 standard is used to adjust the
transmission rate adaptively in each frame time according to channel quality in
order to obtain multiuser diversity gain. To model the arrival process and the
traffic source we use the Batch Markov Arrival Process (BMAP), which enables
more realistic and more accurate traffic modelling. We determine analytically
different performance parameters, such as average queue length, packet dropping
probability, queue throughput and average packet delay. Finally, the analytical
results are validated numerically.
"
388,"A Comprehensive Study and Performance Comparison of M-ary Modulation
  Schemes for an Efficient Wireless Mobile Communication System","  Wireless communications has become one of the fastest growing areas in our
modern life and creates enormous impact on nearly every feature of our daily
life. In this paper, the performance of M-ary modulations schemes (MPSK, MQAM,
MFSK) based wireless communication system on audio signal transmission over
Additive Gaussian Noise (AWGN) channel are analyzed in terms of bit error
probability as a function of SNR. Based on the results obtained in the present
study, MPSK and MQAM are showing better performance for lower modulation order
whereas these are inferior with higher M. The BER value is smaller in MFSK for
higher M, but it is worse due to the distortion in the reproduce signal at the
receiver end. The lossless reproduction of recorded voice signal can be
achieved at the receiver end with a lower modulation order.
"
389,Performance Evaluation of Realistic Vanet Using Traffic Light Scenario,"  Vehicular Ad-hoc Networks (VANETs) is attracting considerable attention from
the research community and the automotive industry to improve the services of
Intelligent Transportation System (ITS). As today's transportation system faces
serious challenges in terms of road safety, efficiency, and environmental
friendliness, the idea of so called ""ITS"" has emerged. Due to the expensive
cost of deployment and complexity of implementing such a system in real world,
research in VANET relies on simulation. This paper attempts to evaluate the
performance of VANET in a realistic environment. The paper contributes by
generating a real world road Map of JNU using existing Google Earth and GIS
tools. Traffic data from a limited region of road Map is collected to capture
the realistic mobility. In this work, the entire region has been divided into
various smaller routes. The realistic mobility model used here considers the
driver's route choice at the run time. It also studies the clustering effect
caused by traffic lights used at the intersection to regulate traffic movement
at different directions. Finally, the performance of the VANET is evaluated in
terms of average delivery ratio, packet loss, and router drop as statistical
measures for driver route choice with traffic light scenario. This experiment
has provided insight into the performance of vehicular traffic communication
for a small realistic scenario.
"
390,"On Modelling and Prediction of Total CPU Usage for Applications in
  MapReduce Environments","  Recently, businesses have started using MapReduce as a popular computation
framework for processing large amount of data, such as spam detection, and
different data mining tasks, in both public and private clouds. Two of the
challenging questions in such environments are (1) choosing suitable values for
MapReduce configuration parameters -e.g., number of mappers, number of
reducers, and DFS block size-, and (2) predicting the amount of resources that
a user should lease from the service provider. Currently, the tasks of both
choosing configuration parameters and estimating required resources are solely
the users' responsibilities. In this paper, we present an approach to provision
the total CPU usage in clock cycles of jobs in MapReduce environment. For a
MapReduce job, a profile of total CPU usage in clock cycles is built from the
job past executions with different values of two configuration parameters e.g.,
number of mappers, and number of reducers. Then, a polynomial regression is
used to model the relation between these configuration parameters and total CPU
usage in clock cycles of the job. We also briefly study the influence of input
data scaling on measured total CPU usage in clock cycles. This derived model
along with the scaling result can then be used to provision the total CPU usage
in clock cycles of the same jobs with different input data size. We validate
the accuracy of our models using three realistic applications (WordCount, Exim
MainLog parsing, and TeraSort). Results show that the predicted total CPU usage
in clock cycles of generated resource provisioning options are less than 8% of
the measured total CPU usage in clock cycles in our 20-node virtual Hadoop
cluster.
"
391,On the Power of Centralization in Distributed Processing,"  In this thesis, we propose and analyze a multi-server model that captures a
performance trade-off between centralized and distributed processing. In our
model, a fraction $p$ of an available resource is deployed in a centralized
manner (e.g., to serve a most-loaded station) while the remaining fraction
$1-p$ is allocated to local servers that can only serve requests addressed
specifically to their respective stations.
  Using a fluid model approach, we demonstrate a surprising phase transition in
the steady-state delay, as $p$ changes: in the limit of a large number of
stations, and when any amount of centralization is available ($p>0$), the
average queue length in steady state scales as $\log_{1/(1-p)} 1/(1-\lambda)$
when the traffic intensity $\lambda$ goes to 1. This is exponentially smaller
than the usual M/M/1-queue delay scaling of $1/(1-\lambda)$, obtained when all
resources are fully allocated to local stations ($p=0$). This indicates a
strong qualitative impact of even a small degree of centralization.
  We prove convergence to a fluid limit, and characterize both the transient
and steady-state behavior of the finite system, in the limit as the number of
stations $N$ goes to infinity. We show that the sequence of queue-length
processes converges to a unique fluid trajectory (over any finite time
interval, as $N$ approaches infinity, and that this fluid trajectory converges
to a unique invariant state $v^I$, for which a simple closed-form expression is
obtained. We also show that the steady-state distribution of the $N$-server
system concentrates on $v^I$ as $N$ goes to infinity.
"
392,"Minimizing Slowdown in Heterogeneous Size-Aware Dispatching Systems
  (full version)","  We consider a system of parallel queues where tasks are assigned (dispatched)
to one of the available servers upon arrival. The dispatching decision is based
on the full state information, i.e., on the sizes of the new and existing jobs.
We are interested in minimizing the so-called mean slowdown criterion
corresponding to the mean of the sojourn time divided by the processing time.
Assuming no new jobs arrive, the shortest-processing-time-product (SPTP)
schedule is known to minimize the slowdown of the existing jobs. The main
contribution of this paper is three-fold: 1) To show the optimality of SPTP
with respect to slowdown in a single server queue under Poisson arrivals; 2) to
derive the so-called size-aware value functions for
M/G/1-FIFO/LIFO/SPTP/SPT/SRPT with general holding costs of which the slowdown
criterion is a special case; and 3) to utilize the value functions to derive
efficient dispatching policies so as to minimize the mean slowdown in a
heterogeneous server system. The derived policies offer a significantly better
performance than e.g., the size-aware-task-assignment with equal load (SITA-E)
and least-work-left (LWL) policies.
"
393,Memory Hierarchy Sensitive Graph Layout,"  Mining large graphs for information is becoming an increasingly important
workload due to the plethora of graph structured data becoming available. An
aspect of graph algorithms that has hitherto not received much interest is the
effect of memory hierarchy on accesses. A typical system today has multiple
levels in the memory hierarchy with differing units of locality; ranging across
cache lines, TLB entries and DRAM pages. We postulate that it is possible to
allocate graph structured data in main memory in a way as to improve the
spatial locality of the data. Previous approaches to improving cache locality
have focused only on a single unit of locality, either the cache line or
virtual memory page. On the other hand cache oblivious algorithms can optimise
layout for all levels of the memory hierarchy but unfortunately need to be
specially designed for individual data structures. In this paper we explore
hierarchical blocking as a technique for closing this gap. We require as input
a specification of the units of locality in the memory hierarchy and lay out
the input graph accordingly by copying its nodes using a hierarchy of breadth
first searches. We start with a basic algorithm that is limited to trees and
then extend it to arbitrary graphs. Our most efficient version requires only a
constant amount of additional space. We have implemented versions of the
algorithm in various environments: for C programs interfaced with macros, as an
extension to the Boost object oriented graph library and finally as a
modification to the traversal phase of the semispace garbage collector in the
Jikes Java virtual machine. Our results show significant improvements in the
access time to graphs of various structure.
"
394,Robust methods for LTE and WiMAX dimensioning,"  This paper proposes an analytic model for dimensioning OFDMA based networks
like WiMAX and LTE systems. In such a system, users require a number of
subchannels which depends on their \SNR, hence of their position and the
shadowing they experience. The system is overloaded when the number of required
subchannels is greater than the number of available subchannels. We give an
exact though not closed expression of the loss probability and then give an
algorithmic method to derive the number of subchannels which guarantees a loss
probability less than a given threshold. We show that Gaussian approximation
lead to optimistic values and are thus unusable. We then introduce Edgeworth
expansions with error bounds and show that by choosing the right order of the
expansion, one can have an approximate dimensioning value easy to compute but
with guaranteed performance. As the values obtained are highly dependent from
the parameters of the system, which turned to be rather undetermined, we
provide a procedure based on concentration inequality for Poisson functionals,
which yields to conservative dimensioning. This paper relies on recent results
on concentration inequalities and establish new results on Edgeworth
expansions.
"
395,Improving Seek Time for Column Store Using MMH Algorithm,"  Hash based search has, proven excellence on large data warehouses stored in
column store. Data distribution has significant impact on hash based search. To
reduce impact of data distribution, we have proposed Memory Managed Hash (MMH)
algorithm that uses shift XOR group for Queries and Transactions in column
store. Our experiments show that MMH improves read and write throughput by 22%
for TPC-H distribution.
"
396,"Mathematical Modeling of Competitive Group Recommendation Systems with
  Application to Peer Review Systems","  In this paper, we present a mathematical model to capture various factors
which may influence the accuracy of a competitive group recommendation system.
We apply this model to peer review systems, i.e., conference or research grants
review, which is an essential component in our scientific community. We explore
number of important questions, i.e., how will the number of reviews per paper
affect the accuracy of the overall recommendation? Will the score aggregation
policy influence the final recommendation? How reviewers' preference may affect
the accuracy of the final recommendation? To answer these important questions,
we formally analyze our model. Through this analysis, we obtain the insight on
how to design a randomized algorithm which is both computationally efficient
and asymptotically accurate in evaluating the accuracy of a competitive group
recommendation system. We obtain number of interesting observations: i.e., for
a medium tier conference, three reviews per paper is sufficient for a high
accuracy recommendation. For prestigious conferences, one may need at least
seven reviews per paper to achieve high accuracy. We also propose a
heterogeneous review strategy which requires equal or less reviewing workload,
but can improve over a homogeneous review strategy in recommendation accuracy
by as much as 30% . We believe our models and methodology are important
building blocks to study competitive group recommendation systems.
"
397,"Effect of Thread Level Parallelism on the Performance of Optimum
  Architecture for Embedded Applications","  According to the increasing complexity of network application and internet
traffic, network processor as a subset of embedded processors have to process
more computation intensive tasks. By scaling down the feature size and emersion
of chip multiprocessors (CMP) that are usually multi-thread processors, the
performance requirements are somehow guaranteed. As multithread processors are
the heir of uni-thread processors and there isn't any general design flow to
design a multithread embedded processor, in this paper we perform a
comprehensive design space exploration for an optimum uni-thread embedded
processor based on the limited area and power budgets. Finally we run multiple
threads on this architecture to find out the maximum thread level parallelism
(TLP) based on performance per power and area optimum uni-thread architecture.
"
398,"On the Computation of the Higher Order Statistics of the Channel
  Capacity over Generalized Fading Channels","  The higher-order statistics (HOS) of the channel capacity
$\mu_n=\mathbb{E}[\log^n(1+\gamma_{end})]$, where $n\in\mathbb{N}$ denotes the
order of the statistics, has received relatively little attention in the
literature, due in part to the intractability of its analysis. In this letter,
we propose a novel and unified analysis, which is based on the moment
generating function (MGF) technique, to exactly compute the HOS of the channel
capacity. More precisely, our mathematical formalism can be readily applied to
maximal-ratio-combining (MRC) receivers operating in generalized fading
environments (i.e., the sum of the correlated noncentral chi-squared
distributions / the correlated generalized Rician distributions). The
mathematical formalism is illustrated by some numerical examples focussing on
the correlated generalized fading environments.
"
399,Stochastic Analysis of Mean Interference for RTS/CTS Mechanism,"  The RTS/CTS handshake mechanism in WLAN is studied using stochastic geometry.
The effect of RTS/CTS is treated as a thinning procedure for a spatially
distributed point process that models the potential transceivers in a WLAN, and
the resulting concurrent transmission processes are described. Exact formulas
for the intensity of the concurrent transmission processes and the mean
interference experienced by a typical receiver are established. The analysis
yields useful results for understanding how the design parameters of RTS/CTS
affect the network interference.
"
400,Revisiting the D-iteration method: runtime comparison,"  In this paper, we revisit the D-iteration algorithm in order to better
explain different performance results that were observed for the numerical
computation of the eigenvector associated to the PageRank score. We revisit
here the practical computation cost based on the execution runtime compared to
the theoretical number of iterations.
"
401,Model for Predicting End User Web Page Response Time,"  Perceived responsiveness of a web page is one of the most important and least
understood metrics of web page design, and is critical for attracting and
maintaining a large audience. Web pages can be designed to meet performance
SLAs early in the product lifecycle if there is a way to predict the apparent
responsiveness of a particular page layout. Response time of a web page is
largely influenced by page layout and various network characteristics. Since
the network characteristics vary widely from country to country, accurately
modeling and predicting the perceived responsiveness of a web page from the end
user's perspective has traditionally proven very difficult. We propose a model
for predicting end user web page response time based on web page, network,
browser download and browser rendering characteristics. We start by
understanding the key parameters that affect perceived response time. We then
model each of these parameters individually using experimental tests and
statistical techniques. Finally, we demonstrate the effectiveness of this model
by conducting an experimental study with Yahoo! web pages in two countries and
compare it with 3rd party measurement application.
"
402,Reliable Generation of High-Performance Matrix Algebra,"  Scientific programmers often turn to vendor-tuned Basic Linear Algebra
Subprograms (BLAS) to obtain portable high performance. However, many numerical
algorithms require several BLAS calls in sequence, and those successive calls
result in suboptimal performance. The entire sequence needs to be optimized in
concert. Instead of vendor-tuned BLAS, a programmer could start with source
code in Fortran or C (e.g., based on the Netlib BLAS) and use a
state-of-the-art optimizing compiler. However, our experiments show that
optimizing compilers often attain only one-quarter the performance of
hand-optimized code. In this paper we present a domain-specific compiler for
matrix algebra, the Build to Order BLAS (BTO), that reliably achieves high
performance using a scalable search algorithm for choosing the best combination
of loop fusion, array contraction, and multithreading for data parallelism. The
BTO compiler generates code that is between 16% slower and 39% faster than
hand-optimized code.
"
403,Performance Measurement of Cloud Computing Services,"  Cloud computing today has now been growing as new technologies and new
business models. In distributed technology perspective, cloud computing most
like client-server services like web-based or web-service but it used virtual
resources to execute. Currently, cloud computing relies on the use of an
elastic virtual machine and the use of network for data exchange. We conduct an
experimental setup to measure the quality of service received by cloud
computing customers. Experimental setup done by creating a HTTP service that
runs in the cloud computing infrastructure. We interest to know about the
impact of increasing the number of users on the average quality received by
users. The qualities received by user measured within two parameters consist of
average response times and the number of requests time out. Experimental
results of this study show that increasing the number of users has increased
the average response time. Similarly, the number of request time out increasing
with increasing number of users. It means that the qualities of service
received by user are decreasing also. We found that the impact of the number of
users on the quality of service is no longer in linear trend. The results of
this study can be used as a reference model for the network operator in
performing services in which a certain number of users in order to obtain
optimal quality services.
"
404,"Mixed-mode implementation of PETSc for scalable linear algebra on
  multi-core processors","  With multi-core processors a ubiquitous building block of modern
supercomputers, it is now past time to enable applications to embrace these
developments in processor design. To achieve exascale performance, applications
will need ways of exploiting the new levels of parallelism that are exposed in
modern high-performance computers. A typical approach to this is to use
shared-memory programming techniques to best exploit multi-core nodes along
with inter-node message passing. In this paper, we describe the addition of
OpenMP threaded functionality to the PETSc library. We highlight some issues
that hinder good performance of threaded applications on modern processors and
describe how to negate them. The OpenMP branch of PETSc was benchmarked using
matrices extracted from Fluidity, a CFD application code, which uses the
library as its linear solver engine. The overall performance of the mixed-mode
implementation is shown to be superior to that of the pure-MPI version.
"
405,"Evaluation of Proactive, Reactive and Hybrid Ad hoc Routing Protocol for
  various Battery models in VANET using Qualnet","  In VANET high speed is the real characteristics which leads frequent
breakdown, interference etc. In this paper we studied various Ad hoc routing
protocols, Reactive, Proactive & Hybrid, taking into consideration various
VANET parameters like speed, altitude etc in real traffic scenario and
evaluated them for various battery models for energy conservation.. The AODV
and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid) protocols are compared
for battery models Duracell AA(MX- 1500),Duracell AAA(MN-2400),Duracell
AAA(MX-2400), Duracell C-MN(MN-1400),Panasonic AA standard using Qualnet as a
Simulation tool. Since Energy conservation is main focus area nowadays. Hence
performance of the protocols with various battery models counts and helps to
make a right selection. Varying parameters of VANET shows that in the real
traffic scenarios proactive protocol performs more efficiently for energy
conservation.
"
406,A Comparative Study on the Performance of the Top DBMS Systems,"  Database management systems are today's most reliable mean to organize data
into collections that can be searched and updated. However, many DBMS systems
are available on the market each having their pros and cons in terms of
reliability, usability, security, and performance. This paper presents a
comparative study on the performance of the top DBMS systems. They are mainly
MS SQL Server 2008, Oracle 11g, IBM DB2, MySQL 5.5, and MS Access 2010. The
testing is aimed at executing different SQL queries with different level of
complexities over the different five DBMSs under test. This would pave the way
to build a head-to-head comparative evaluation that shows the average execution
time, memory usage, and CPU utilization of each DBMS after completion of the
test.
"
407,"A Method for the Characterisation of Observer Effects and its
  Application to OML","  In all measurement campaigns, one needs to assert that the instrumentation
tools do not significantly impact the system being monitored. This is critical
to future claims based on the collected data and is sometimes overseen in
experimental studies. We propose a method to evaluate the potential ""observer
effect"" of an instrumentation system, and apply it to the OMF Measurement
Library (OML). OML allows the instrumentation of almost any software to collect
any type of measurements. As it is increasingly being used in networking
research, it is important to characterise possible biases it may introduce in
the collected metrics. Thus, we study its effect on multiple types of reports
from various applications commonly used in wireless research. To this end, we
designed experiments comparing OML-instrumented software with their original
flavours. Our analyses of the results from these experiments show that, with an
appropriate reporting setup, OML has no significant impact on the instrumented
applications, and may even improve some of their performances in specifics
cases. We discuss our methodology and the implication of using OML, and provide
guidelines on instrumenting off-the-shelf software.
"
408,"Squeezing out the Cloud via Profit-Maximizing Resource Allocation
  Policies","  We study the problem of maximizing the average hourly profit earned by a
Software-as-a-Service (SaaS) provider who runs a software service on behalf of
a customer using servers rented from an Infrastructure-as-a-Service (IaaS)
provider. The SaaS provider earns a fee per successful transaction and incurs
costs proportional to the number of server-hours it uses. A number of resource
allocation policies for this or similar problems have been proposed in previous
work. However, to the best of our knowledge, these policies have not been
comparatively evaluated in a cloud environment. This paper reports on an
empirical evaluation of three policies using a replica of Wikipedia deployed on
the Amazon EC2 cloud. Experimental results show that a policy based on a
solution to an optimization problem derived from the SaaS provider's utility
function outperforms well-known heuristics that have been proposed for similar
problems. It is also shown that all three policies outperform a ""reactive""
allocation approach based on Amazon's auto-scaling feature.
"
409,Relativistic Hydrodynamics on Graphic Cards,"  We show how to accelerate relativistic hydrodynamics simulations using
graphic cards (graphic processing units, GPUs). These improvements are of
highest relevance e.g. to the field of high-energetic nucleus-nucleus
collisions at RHIC and LHC where (ideal and dissipative) relativistic
hydrodynamics is used to calculate the evolution of hot and dense QCD matter.
The results reported here are based on the Sharp And Smooth Transport Algorithm
(SHASTA), which is employed in many hydrodynamical models and hybrid simulation
packages, e.g. the Ultrarelativistic Quantum Molecular Dynamics model (UrQMD).
We have redesigned the SHASTA using the OpenCL computing framework to work on
accelerators like graphic processing units (GPUs) as well as on multi-core
processors. With the redesign of the algorithm the hydrodynamic calculations
have been accelerated by a factor 160 allowing for event-by-event calculations
and better statistics in hybrid calculations.
"
410,"A Cross-Layer Design Based on Geographic Information for Cooperative
  Wireless Networks","  Most of geographic routing approaches in wireless ad hoc and sensor networks
do not take into consideration the medium access control (MAC) and physical
layers when designing a routing protocol. In this paper, we focus on a
cross-layer framework design that exploits the synergies between network, MAC,
and physical layers. In the proposed CoopGeo, we use a beaconless forwarding
scheme where the next hop is selected through a contention process based on the
geographic position of nodes. We optimize this Network-MAC layer interaction
using a cooperative relaying technique with a relay selection scheme also based
on geographic information in order to improve the system performance in terms
of reliability.
"
411,"Power Grid Vulnerability to Geographically Correlated Failures -
  Analysis and Control Implications","  We consider power line outages in the transmission system of the power grid,
and specifically those caused by a natural disaster or a large scale physical
attack. In the transmission system, an outage of a line may lead to overload on
other lines, thereby eventually leading to their outage. While such cascading
failures have been studied before, our focus is on cascading failures that
follow an outage of several lines in the same geographical area. We provide an
analytical model of such failures, investigate the model's properties, and show
that it differs from other models used to analyze cascades in the power grid
(e.g., epidemic/percolation-based models). We then show how to identify the
most vulnerable locations in the grid and perform extensive numerical
experiments with real grid data to investigate the various effects of
geographically correlated outages and the resulting cascades. These results
allow us to gain insights into the relationships between various parameters and
performance metrics, such as the size of the original event, the final number
of connected components, and the fraction of demand (load) satisfied after the
cascade. In particular, we focus on the timing and nature of optimal control
actions used to reduce the impact of a cascade, in real time. We also compare
results obtained by our model to the results of a real cascade that occurred
during a major blackout in the San Diego area on Sept. 2011. The analysis and
results presented in this paper will have implications both on the design of
new power grids and on identifying the locations for shielding, strengthening,
and monitoring efforts in grid upgrades.
"
412,Astrophysical Particle Simulations on Heterogeneous CPU-GPU Systems,"  A heterogeneous CPU-GPU node is getting popular in HPC clusters. We need to
rethink algorithms and optimization techniques for such system depending on the
relative performance of CPU vs. GPU. In this paper, we report a performance
optimized particle simulation code ""OTOO"", that is based on the octree method,
for heterogenous systems. Main applications of OTOO are astrophysical
simulations such as N-body models and the evolution of a violent merger of
stars. We propose optimal task split between CPU and GPU where GPU is only used
to compute the calculation of the particle force. Also, we describe
optimization techniques such as control of the force accuracy, vectorized tree
walk, and work partitioning among multiple GPUs. We used OTOO for modeling a
merger of two white dwarf stars and found that OTOO is powerful and practical
to simulate the fate of the process.
"
413,"Uncertainty Analysis of the Adequacy Assessment Model of a Distributed
  Generation System","  Due to the inherent aleatory uncertainties in renewable generators, the
reliability/adequacy assessments of distributed generation (DG) systems have
been particularly focused on the probabilistic modeling of random behaviors,
given sufficient informative data. However, another type of uncertainty
(epistemic uncertainty) must be accounted for in the modeling, due to
incomplete knowledge of the phenomena and imprecise evaluation of the related
characteristic parameters. In circumstances of few informative data, this type
of uncertainty calls for alternative methods of representation, propagation,
analysis and interpretation. In this study, we make a first attempt to
identify, model, and jointly propagate aleatory and epistemic uncertainties in
the context of DG systems modeling for adequacy assessment. Probability and
possibility distributions are used to model the aleatory and epistemic
uncertainties, respectively. Evidence theory is used to incorporate the two
uncertainties under a single framework. Based on the plausibility and belief
functions of evidence theory, the hybrid propagation approach is introduced. A
demonstration is given on a DG system adapted from the IEEE 34 nodes
distribution test feeder. Compared to the pure probabilistic approach, it is
shown that the hybrid propagation is capable of explicitly expressing the
imprecision in the knowledge on the DG parameters into the final adequacy
values assessed. It also effectively captures the growth of uncertainties with
higher DG penetration levels.
"
414,"Heavy Traffic Optimal Resource Allocation Algorithms for Cloud Computing
  Clusters","  Cloud computing is emerging as an important platform for business, personal
and mobile computing applications. In this paper, we study a stochastic model
of cloud computing, where jobs arrive according to a stochastic process and
request resources like CPU, memory and storage space. We consider a model where
the resource allocation problem can be separated into a routing or load
balancing problem and a scheduling problem. We study the
join-the-shortest-queue routing and power-of-two-choices routing algorithms
with MaxWeight scheduling algorithm. It was known that these algorithms are
throughput optimal. In this paper, we show that these algorithms are queue
length optimal in the heavy traffic limit.
"
415,Fault-tolerant linear solvers via selective reliability,"  Energy increasingly constrains modern computer hardware, yet protecting
computations and data against errors costs energy. This holds at all scales,
but especially for the largest parallel computers being built and planned
today. As processor counts continue to grow, the cost of ensuring reliability
consistently throughout an application will become unbearable. However, many
algorithms only need reliability for certain data and phases of computation.
This suggests an algorithm and system codesign approach. We show that if the
system lets applications apply reliability selectively, we can develop
algorithms that compute the right answer despite faults. These ""fault-tolerant""
iterative methods either converge eventually, at a rate that degrades
gracefully with increased fault rate, or return a clear failure indication in
the rare case that they cannot converge. Furthermore, they store most of their
data unreliably, and spend most of their time in unreliable mode.
  We demonstrate this for the specific case of detected but uncorrectable
memory faults, which we argue are representative of all kinds of faults. We
developed a cross-layer application / operating system framework that
intercepts and reports uncorrectable memory faults to the application, rather
than killing the application, as current operating systems do. The application
in turn can mark memory allocations as subject to such faults. Using this
framework, we wrote a fault-tolerant iterative linear solver using components
from the Trilinos solvers library. Our solver exploits hybrid parallelism (MPI
and threads). It performs just as well as other solvers if no faults occur, and
converges where other solvers do not in the presence of faults. We show
convergence results for representative test problems. Near-term future work
will include performance tests.
"
416,Network Load Analysis and Provisioning of MapReduce Applications,"  In this paper, we study the dependency between configuration parameters and
network load of fixed-size MapReduce applications in shuffle phase and then
propose an analytical method to model this dependency. Our approach consists of
three key phases: profiling, modeling, and prediction. In the first stage, an
application is run several times with different sets of MapReduce configuration
parameters (here number of mappers and number of reducers) to profile the
network load of the application in the shuffle phase on a given cluster. Then,
the relation between these parameters and the network load is modeled by
multivariate linear regression. For evaluation, three applications (WordCount,
Exim Mainlog parsing, and TeraSort) are utilized to evaluate our technique on a
4-node MapReduce private cluster.
"
417,"To Compress or Not To Compress: Processing vs Transmission Tradeoffs for
  Energy Constrained Sensor Networking","  In the past few years, lossy compression has been widely applied in the field
of wireless sensor networks (WSN), where energy efficiency is a crucial concern
due to the constrained nature of the transmission devices. Often, the common
thinking among researchers and implementers is that compression is always a
good choice, because the major source of energy consumption in a sensor node
comes from the transmission of the data. Lossy compression is deemed a viable
solution as the imperfect reconstruction of the signal is often acceptable in
WSN. In this paper, we thoroughly review a number of lossy compression methods
from the literature, and analyze their performance in terms of compression
efficiency, computational complexity and energy consumption. We consider two
different scenarios, namely, wireless and underwater communications, and show
that signal compression may or may not help in the reduction of the overall
energy consumption, depending on factors such as the compression algorithm, the
signal statistics and the hardware characteristics, i.e., micro-controller and
transmission technology. The lesson that we have learned, is that signal
compression may in fact provide some energy savings. However, its usage should
be carefully evaluated, as in quite a few cases processing and transmission
costs are of the same order of magnitude, whereas, in some other cases, the
former may even dominate the latter. In this paper, we show quantitative
comparisons to assess these tradeoffs in the above mentioned scenarios.
Finally, we provide formulas, obtained through numerical fittings, to gauge
computational complexity, overall energy consumption and signal representation
accuracy for the best performing algorithms as a function of the most relevant
system parameters.
"
418,"Best practices for HPM-assisted performance engineering on modern
  multicore processors","  Many tools and libraries employ hardware performance monitoring (HPM) on
modern processors, and using this data for performance assessment and as a
starting point for code optimizations is very popular. However, such data is
only useful if it is interpreted with care, and if the right metrics are chosen
for the right purpose. We demonstrate the sensible use of hardware performance
counters in the context of a structured performance engineering approach for
applications in computational science. Typical performance patterns and their
respective metric signatures are defined, and some of them are illustrated
using case studies. Although these generic concepts do not depend on specific
tools or environments, we restrict ourselves to modern x86-based multicore
processors and use the likwid-perfctr tool under the Linux OS.
"
419,"Block Iterative Eigensolvers for Sequences of Correlated Eigenvalue
  Problems","  In Density Functional Theory simulations based on the LAPW method, each
self-consistent field cycle comprises dozens of large dense generalized
eigenproblems. In contrast to real-space methods, eigenpairs solving for
problems at distinct cycles have either been believed to be independent or at
most very loosely connected. In a recent study [7], it was demonstrated that,
contrary to belief, successive eigenproblems in a sequence are strongly
correlated with one another. In particular, by monitoring the subspace angles
between eigenvectors of successive eigenproblems, it was shown that these
angles decrease noticeably after the first few iterations and become close to
collinear. This last result suggests that we can manipulate the eigenvectors,
solving for a specific eigenproblem in a sequence, as an approximate solution
for the following eigenproblem. In this work we present results that are in
line with this intuition. We provide numerical examples where opportunely
selected block iterative eigensolvers benefit from the reuse of eigenvectors by
achieving a substantial speed-up. The results presented will eventually open
the way to a widespread use of block iterative eigensolvers in ab initio
electronic structure codes based on the LAPW approach.
"
420,"A Multi-State Power Model for Adequacy Assessment of Distributed
  Generation via Universal Generating Function","  The current and future developments of electric power systems are pushing the
boundaries of reliability assessment to consider distribution networks with
renewable generators. Given the stochastic features of these elements, most
modeling approaches rely on Monte Carlo simulation. The computational costs
associated to the simulation approach force to treating mostly small-sized
systems, i.e. with a limited number of lumped components of a given renewable
technology (e.g. wind or solar, etc.) whose behavior is described by a binary
state, working or failed. In this paper, we propose an analytical multi-state
modeling approach for the reliability assessment of distributed generation
(DG). The approach allows looking to a number of diverse energy generation
technologies distributed on the system. Multiple states are used to describe
the randomness in the generation units, due to the stochastic nature of the
generation sources and of the mechanical degradation/failure behavior of the
generation systems. The universal generating function (UGF) technique is used
for the individual component multi-state modeling. A multiplication-type
composition operator is introduced to combine the UGFs for the mechanical
degradation and renewable generation source states into the UGF of the
renewable generator power output. The overall multi-state DG system UGF is then
constructed and classical reliability indices (e.g. loss of load expectation
(LOLE), expected energy not supplied (EENS)) are computed from the DG system
generation and load UGFs. An application of the model is shown on a DG system
adapted from the IEEE 34 nodes distribution test feeder.
"
421,"Proceedings 10th Workshop on Quantitative Aspects of Programming
  Languages and Systems","  This volume contains the proceedings of the Tenth Workshop on Quantitative
Aspects of Programming Languages (QAPL 2012), held in Tallin, Estonia, on March
31 and April 1, 2012. QAPL 2012 is a satellite event of the European Joint
Conferences on Theory and Practice of Software (ETAPS 2012). The workshop theme
is on quantitative aspects of computation. These aspects are related to the use
of physical quantities (storage space, time, bandwidth, etc.) as well as
mathematical quantities (e.g. probability and measures for reliability,
security and trust), and play an important (sometimes essential) role in
characterising the behavior and determining the properties of systems. Such
quantities are central to the definition of both the model of systems
(architecture, language design, semantics) and the methodologies and tools for
the analysis and verification of the systems properties. The aim of this
workshop is to discuss the explicit use of quantitative information such as
time and probabilities either directly in the model or as a tool for the
analysis of systems.
"
422,Hybrid performance modelling of opportunistic networks,"  We demonstrate the modelling of opportunistic networks using the process
algebra stochastic HYPE. Network traffic is modelled as continuous flows,
contact between nodes in the network is modelled stochastically, and
instantaneous decisions are modelled as discrete events. Our model describes a
network of stationary video sensors with a mobile ferry which collects data
from the sensors and delivers it to the base station. We consider different
mobility models and different buffer sizes for the ferries. This case study
illustrates the flexibility and expressive power of stochastic HYPE. We also
discuss the software that enables us to describe stochastic HYPE models and
simulate them.
"
423,"Cramer-Rao Bounds for Joint RSS/DoA-Based Primary-User Localization in
  Cognitive Radio Networks","  Knowledge about the location of licensed primary-users (PU) could enable
several key features in cognitive radio (CR) networks including improved
spatio-temporal sensing, intelligent location-aware routing, as well as aiding
spectrum policy enforcement. In this paper we consider the achievable accuracy
of PU localization algorithms that jointly utilize received-signal-strength
(RSS) and direction-of-arrival (DoA) measurements by evaluating the Cramer-Rao
Bound (CRB). Previous works evaluate the CRB for RSS-only and DoA-only
localization algorithms separately and assume DoA estimation error variance is
a fixed constant or rather independent of RSS. We derive the CRB for joint
RSS/DoA-based PU localization algorithms based on the mathematical model of DoA
estimation error variance as a function of RSS, for a given CR placement. The
bound is compared with practical localization algorithms and the impact of
several key parameters, such as number of nodes, number of antennas and
samples, channel shadowing variance and correlation distance, on the achievable
accuracy are thoroughly analyzed and discussed. We also derive the closed-form
asymptotic CRB for uniform random CR placement, and perform theoretical and
numerical studies on the required number of CRs such that the asymptotic CRB
tightly approximates the numerical integration of the CRB for a given
placement.
"
424,A Generic Library for Stencil Computations,"  In this era of diverse and heterogeneous computer architectures, the
programmability issues, such as productivity and portable efficiency, are
crucial to software development and algorithm design. One way to approach the
problem is to step away from traditional sequential programming languages and
move toward domain specific programming environments to balance between
expressivity and efficiency. In order to demonstrate this principle, we
developed a domain specific C++ generic library for stencil computations, like
PDE solvers. The library features high level constructs to specify computation
and allows the development of parallel stencil computations with very limited
effort. The high abstraction constructs (like do_all and do_reduce) make the
program shorter and cleaner with increased contextual information for better
performance exploitation. The results show good performance from Windows
multicores, to HPC clusters and machines with accelerators, like GPUs.
"
425,"Impact of Different Spreading Codes Using FEC on DWT Based MC-CDMA
  System","  The effect of different spreading codes in DWT based MC-CDMA wireless
communication system is investigated. In this paper, we present the Bit Error
Rate (BER) performance of different spreading codes (Walsh-Hadamard code,
Orthogonal gold code and Golay complementary sequences) using Forward Error
Correction (FEC) of the proposed system. The data is analyzed and is compared
among different spreading codes in both coded and uncoded cases. It is found
via computer simulation that the performance of the proposed coded system is
much better than that of the uncoded system irrespective of the spreading codes
and all the spreading codes show approximately similar nature for both coded
and uncoded in all modulation schemes.
"
426,"Automated Inference System for End-To-End Diagnosis of Network
  Performance Issues in Client-Terminal Devices","  Traditional network diagnosis methods of Client-Terminal Device (CTD)
problems tend to be laborintensive, time consuming, and contribute to increased
customer dissatisfaction. In this paper, we propose an automated solution for
rapidly diagnose the root causes of network performance issues in CTD. Based on
a new intelligent inference technique, we create the Intelligent Automated
Client Diagnostic (IACD) system, which only relies on collection of
Transmission Control Protocol (TCP) packet traces. Using soft-margin Support
Vector Machine (SVM) classifiers, the system (i) distinguishes link problems
from client problems and (ii) identifies characteristics unique to the specific
fault to report the root cause. The modular design of the system enables
support for new access link and fault types. Experimental evaluation
demonstrated the capability of the IACD system to distinguish between faulty
and healthy links and to diagnose the client faults with 98% accuracy. The
system can perform fault diagnosis independent of the user's specific TCP
implementation, enabling diagnosis of diverse range of client devices
"
427,"Performance Analysis of Wavelet Based MC-CDMA System with Implementation
  of Various Antenna Diversity Schemes","  The impact of using wavelet based technique on the performance of a MC-CDMA
wireless communication system has been investigated. The system under proposed
study incorporates Walsh Hadamard codes to discriminate the message signal for
individual user. A computer program written in Mathlab source code is developed
and this simulation study is made with implementation of various antenna
diversity schemes and fading (Rayleigh and Rician) channel. Computer simulation
results demonstrate that the proposed wavelet based MC-CDMA system outperforms
in Alamouti (two transmit antenna and one receive antenna) under AWGN and
Rician channel.
"
428,"Transmission of Voice Signal: BER Performance Analysis of Different FEC
  Schemes Based OFDM System over Various Channels","  In this paper, we investigate the impact of Forward Error Correction (FEC)
codes namely Cyclic Redundancy Code and Convolution Code on the performance of
OFDM wireless communication system for speech signal transmission over both
AWGN and fading (Rayleigh and Rician) channels in term of Bit Error
Probability. The simulation has been done in conjunction with QPSK digital
modulation and compared with uncoded resultstal modulation. In the fading
channels, it is found via computer simulation that the performance of the
Convolution interleaved based OFDM systems outperform than that of CRC
interleaved OFDM system as well as uncoded OFDM channels.
"
429,Cluster Based Hierarchical Routing Protocol for Wireless Sensor Network,"  The efficient use of energy source in a sensor node is most desirable
criteria for prolong the life time of wireless sensor network. In this paper,
we propose a two layer hierarchical routing protocol called Cluster Based
Hierarchical Routing Protocol (CBHRP). We introduce a new concept called
head-set, consists of one active cluster head and some other associate cluster
heads within a cluster. The head-set members are responsible for control and
management of the network. Results show that this protocol reduces energy
consumption quite significantly and prolongs the life time of sensor network as
compared to LEACH.
"
430,"WEP: An Energy Efficient Protocol for Cluster Based Heterogeneous
  Wireless Sensor Network","  We develop an energy-efficient routing protocol in order to enhance the
stability period of wireless sensor networks. This protocol is called weighted
election protocol (WEP). It introduces a scheme to combine clustering strategy
with chain routing algorithm for satisfy both energy and stable period
constrains under heterogeneous environment in WSNs. Simulation results show
that new one performs better than LEACH, SEP and HEARP in terms of stability
period and network lifetime. It is also found that longer stability period
strongly depend on higher values of extra energy during its heterogeneous
settings.
"
431,"Effect of Interleaved FEC Code on Wavelet Based MC-CDMA System with
  Alamouti STBC in Different Modulation Schemes","  In this paper, the impact of Forward Error Correction (FEC) code namely
Trellis code with interleaver on the performance of wavelet based MC-CDMA
wireless communication system with the implementation of Alamouti antenna
diversity scheme has been investigated in terms of Bit Error Rate (BER) as a
function of Signal-to-Noise Ratio (SNR) per bit. Simulation of the system under
proposed study has been done in M-ary modulation schemes (MPSK, MQAM and DPSK)
over AWGN and Rayleigh fading channel incorporating Walsh Hadamard code as
orthogonal spreading code to discriminate the message signal for individual
user. It is observed via computer simulation that the performance of the
interleaved coded based proposed system outperforms than that of the uncoded
system in all modulation schemes over Rayleigh fading channel.
"
432,"Hierarchical Performance Modeling for Ranking Dense Linear Algebra
  Algorithms","  A large class of dense linear algebra operations, such as LU decomposition or
inversion of a triangular matrix, are usually performed by blocked algorithms.
For one such operation, typically, not only one but many algorithmic variants
exist; depending on computing architecture, libraries and problem size, each
variant attains a different performances. We propose methods and tools to rank
the algorithmic variants according to their performance for a given scenario
without executing them.
  For this purpose, we identify the routines upon which the algorithms are
built. A first tool - the Sampler - measures the performance of these routines.
Using the Sampler, a second tool models their performance. The generated models
are then used to predict the performance of the considered algorithms. For a
given scenario, these predictions allow us to correctly rank the algorithms
according to their performance without executing them. With the help of the
same tools, algorithmic parameters such as block-size can be optimally tuned.
"
433,"Characterizing the Impact of the Workload on the Value of Dynamic
  Resizing in Data Centers","  Energy consumption imposes a significant cost for data centers; yet much of
that energy is used to maintain excess service capacity during periods of
predictably low load. Resultantly, there has recently been interest in
developing designs that allow the service capacity to be dynamically resized to
match the current workload. However, there is still much debate about the value
of such approaches in real settings. In this paper, we show that the value of
dynamic resizing is highly dependent on statistics of the workload process. In
particular, both slow time-scale non-stationarities of the workload (e.g., the
peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness
of arrivals) play key roles. To illustrate the impact of these factors, we
combine optimization-based modeling of the slow time-scale with stochastic
modeling of the fast time scale. Within this framework, we provide both
analytic and numerical results characterizing when dynamic resizing does (and
does not) provide benefits.
"
434,"Criticality of Large Delay Tolerant Networks via Directed Continuum
  Percolation in Space-Time","  We study delay tolerant networking (DTN) and in particular, its capacity to
store, carry and forward messages so that the messages eventually reach their
final destinations. We approach this broad question in the framework of
percolation theory. To this end, we assume an elementary mobility model, where
nodes arrive to an infinite plane according to a Poisson point process, move a
certain distance L, and then depart. In this setting, we characterize the mean
density of nodes required to support DTN style networking. In particular, under
the given assumptions, we show that DTN is feasible when the mean node degree
is greater than 4 e(g), where parameter g=L/d is the ratio of the distance L to
the transmission range d, and e(g) is the critical reduced number density of
tilted cylinders in a directed continuum percolation model. By means of Monte
Carlo simulations, we give numerical values for e(g). The asymptotic behavior
of e(g) when g tends to infinity is also derived from a fluid flow analysis.
"
435,An Upper Bound on the Convergence Time for Distributed Binary Consensus,"  The problem addressed in this paper is the analysis of a distributed
consensus algorithm for arbitrary networks, proposed by B\'en\'ezit et al.. In
the initial setting, each node in the network has one of two possible states
(""yes"" or ""no""). Nodes can update their states by communicating with their
neighbors via a 2-bit message in an asynchronous clock setting. Eventually, all
nodes reach consensus on the majority states. We use the theory of electric
networks, random walks, and couplings of Markov chains to derive an O(N4 logN)
upper bound for the expected convergence time on an arbitrary graph of size N.
"
436,An Upper Bound on the Convergence Time for Quantized Consensus,"  We analyze a class of distributed quantized consen- sus algorithms for
arbitrary networks. In the initial setting, each node in the network has an
integer value. Nodes exchange their current estimate of the mean value in the
network, and then update their estimation by communicating with their neighbors
in a limited capacity channel in an asynchronous clock setting. Eventually, all
nodes reach consensus with quantized precision. We start the analysis with a
special case of a distributed binary voting algorithm, then proceed to the
expected convergence time for the general quantized consensus algorithm
proposed by Kashyap et al. We use the theory of electric networks, random
walks, and couplings of Markov chains to derive an O(N^3log N) upper bound for
the expected convergence time on an arbitrary graph of size N, improving on the
state of art bound of O(N^4logN) for binary consensus and O(N^5) for quantized
consensus algorithms. Our result is not dependent on graph topology.
Simulations on special graphs such as star networks, line graphs, lollipop
graphs, and Erd\""os-R\'enyi random graphs are performed to validate the
analysis. This work has applications to load balancing, coordination of
autonomous agents, estimation and detection, decision-making networks,
peer-to-peer systems, etc.
"
437,"Doing More for Less -- Cache-Aware Parallel Contraction Hierarchies
  Preprocessing","  Contraction Hierarchies is a successful speedup-technique to Dijkstra's
seminal shortest path algorithm that has a convenient trade-off between
preprocessing and query times. We investigate a shared-memory parallel
implementation that uses $O(n+m)$ space for storing the graph and O(1) space
for each core during preprocessing. The presented data structures and
algorithms consequently exploits cache locality and thus exhibit competitive
preprocessing times. The presented implementation is especially suitable for
preprocessing graphs of planet-wide scale in practice. Also, our experiments
show that optimal data structures in the PRAM model can be beaten in practice
by exploiting memory cache hierarchies.
"
438,"Exploring performance and power properties of modern multicore chips via
  simple machine models","  Modern multicore chips show complex behavior with respect to performance and
power. Starting with the Intel Sandy Bridge processor, it has become possible
to directly measure the power dissipation of a CPU chip and correlate this data
with the performance properties of the running code. Going beyond a simple
bottleneck analysis, we employ the recently published Execution-Cache-Memory
(ECM) model to describe the single- and multi-core performance of streaming
kernels. The model refines the well-known roofline model, since it can predict
the scaling and the saturation behavior of bandwidth-limited loop kernels on a
multicore chip. The saturation point is especially relevant for considerations
of energy consumption. From power dissipation measurements of benchmark
programs with vastly different requirements to the hardware, we derive a
simple, phenomenological power model for the Sandy Bridge processor. Together
with the ECM model, we are able to explain many peculiarities in the
performance and power behavior of multicore processors, and derive guidelines
for energy-efficient execution of parallel programs. Finally, we show that the
ECM and power models can be successfully used to describe the scaling and power
behavior of a lattice-Boltzmann flow solver code.
"
439,Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility,"  Data mining involves the systematic analysis of large data sets, and data
mining in agricultural soil datasets is exciting and modern research area. The
productive capacity of a soil depends on soil fertility. Achieving and
maintaining appropriate levels of soil fertility, is of utmost importance if
agricultural land is to remain capable of nourishing crop production. In this
research, Steps for building a predictive model of soil fertility have been
explained.
  This paper aims at predicting soil fertility class using decision tree
algorithms in data mining . Further, it focuses on performance tuning of J48
decision tree algorithm with the help of meta-techniques such as attribute
selection and boosting.
"
440,On Benchmarking Embedded Linux Flash File Systems,"  Due to its attractive characteristics in terms of performance, weight and
power consumption, NAND flash memory became the main non volatile memory (NVM)
in embedded systems. Those NVMs also present some specific
characteristics/constraints: good but asymmetric I/O performance, limited
lifetime, write/erase granularity asymmetry, etc. Those peculiarities are
either managed in hardware for flash disks (SSDs, SD cards, USB sticks, etc.)
or in software for raw embedded flash chips. When managed in software, flash
algorithms and structures are implemented in a specific flash file system
(FFS). In this paper, we present a performance study of the most widely used
FFSs in embedded Linux: JFFS2, UBIFS,and YAFFS. We show some very particular
behaviors and large performance disparities for tested FFS operations such as
mounting, copying, and searching file trees, compression, etc.
"
441,"A Hardware Time Manager Implementation for the Xenomai Real-Time Kernel
  of Embedded Linux","  Nowadays, the use of embedded operating systems in different embedded
projects is subject to a tremendous growth. Embedded Linux is becoming one of
those most popular EOSs due to its modularity, efficiency, reliability, and
cost. One way to make it hard real-time is to include a real-time kernel like
Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS)
is its ability to meet execution time deadlines deterministically. So, the more
precise and flexible the time management can be, the better it can handle
efficiently the determinism for different embedded applications. RTOS time
precision is characterized by a specific periodic interrupt service controlled
by a software time manager. The smaller the period of the interrupt, the better
the precision of the RTOS, the more it overloads the CPU, and though reduces
the overall efficiency of the RTOS. In this paper, we propose to drastically
reduce these overheads by migrating the time management service of Xenomai into
a configurable hardware component to relieve the CPU. The hardware component is
implemented in a Field Programmable Gate Array coupled to the CPU. This work
was achieved in a Master degree project where students could apprehend many
fields of embedded systems: RTOS programming, hardware design, performance
evaluation, etc.
"
442,"Channel Assignment in Dense MC-MR Wireless Networks: Scaling Laws and
  Algorithms","  We investigate optimal channel assignment algorithms that maximize per node
throughput in dense multichannel multi-radio (MC-MR) wireless networks.
Specifically, we consider an MC-MR network where all nodes are within the
transmission range of each other. This situation is encountered in many
real-life settings such as students in a lecture hall, delegates attending a
conference, or soldiers in a battlefield. In this scenario, we show that
intelligent assignment of the available channels results in a significantly
higher per node throughput. We first propose a class of channel assignment
algorithms, parameterized by T (the number of transceivers per node), that can
achieve $\Theta(1/N^{1/T})$ per node throughput using $\Theta(TN^{1-1/T})$
channels. In view of practical constraints on $T$, we then propose another
algorithm that can achieve $\Theta(1/(\log_2 N)^2)$ per node throughput using
only two transceivers per node. Finally, we identify a fundamental relationship
between the achievable per node throughput, the total number of channels used,
and the network size under any strategy. Using analysis and simulations, we
show that our algorithms achieve close to optimal performance at different
operating points on this curve. Our work has several interesting implications
on the optimal network design for dense MC-MR wireless networks.
"
443,"Evaluating the SiteStory Transactional Web Archive With the ApacheBench
  Tool","  Conventional Web archives are created by periodically crawling a web site and
archiving the responses from the Web server. Although easy to implement and
common deployed, this form of archiving typically misses updates and may not be
suitable for all preservation scenarios, for example a site that is required
(perhaps for records compliance) to keep a copy of all pages it has served. In
contrast, transactional archives work in conjunction with a Web server to
record all pages that have been served. Los Alamos National Laboratory has
developed SiteSory, an open-source transactional archive written in Java
solution that runs on Apache Web servers, provides a Memento compatible access
interface, and WARC file export features. We used the ApacheBench utility on a
pre-release version of to measure response time and content delivery time in
different environments and on different machines. The performance tests were
designed to determine the feasibility of SiteStory as a production-level
solution for high fidelity automatic Web archiving. We found that SiteStory
does not significantly affect content server performance when it is performing
transactional archiving. Content server performance slows from 0.076 seconds to
0.086 seconds per Web page access when the content server is under load, and
from 0.15 seconds to 0.21 seconds when the resource has many embedded and
changing resources.
"
444,Toward a New Protocol to Evaluate Recommender Systems,"  In this paper, we propose an approach to analyze the performance and the
added value of automatic recommender systems in an industrial context. We show
that recommender systems are multifaceted and can be organized around 4
structuring functions: help users to decide, help users to compare, help users
to discover, help users to explore. A global off line protocol is then proposed
to evaluate recommender systems. This protocol is based on the definition of
appropriate evaluation measures for each aforementioned function. The
evaluation protocol is discussed from the perspective of the usefulness and
trust of the recommendation. A new measure called Average Measure of Impact is
introduced. This measure evaluates the impact of the personalized
recommendation. We experiment with two classical methods, K-Nearest Neighbors
(KNN) and Matrix Factorization (MF), using the well known dataset: Netflix. A
segmentation of both users and items is proposed to finely analyze where the
algorithms perform well or badly. We show that the performance is strongly
dependent on the segments and that there is no clear correlation between the
RMSE and the quality of the recommendation.
"
445,Performance Modeling for Dense Linear Algebra,"  It is well known that the behavior of dense linear algebra algorithms is
greatly influenced by factors like target architecture, underlying libraries
and even problem size; because of this, the accurate prediction of their
performance is a real challenge. In this article, we are not interested in
creating accurate models for a given algorithm, but in correctly ranking a set
of equivalent algorithms according to their performance. Aware of the
hierarchical structure of dense linear algebra routines, we approach the
problem by developing a framework for the automatic generation of statistical
performance models for BLAS and LAPACK libraries. This allows us to obtain
predictions through evaluating and combining such models. We demonstrate that
our approach is successful in both single- and multi-core environments, not
only in the ranking of algorithms but also in tuning their parameters.
"
446,"Acquisition probability of multi-user UWB systems in the presence of a
  novel synchronization approach","  In this paper, to synchronize Ultra Wideband (UWB) systems in ad-hoc
multi-user environments, we propose a new timing acquisition approach for
achieving a good performance despite the difficulties to get there.
Synchronization constraints are caused by the ultra-short emitted waveforms
nature of UWB signals. Used in [1, 2] for single-user environments, our timing
acquisition approach is based on two successive stages or floors. Extended for
multi-user environments, the used algorithm is a combination between coarse
synchronization based on timing with dirty templates (TDT) acquisition scheme
and a new fine synchronization scheme developed in [3-6] which conduct to an
improved estimate of timing offset. In this work, we develop and test this
method in both data-aided (DA) and non-data-aided (NDA) modes. Simulation
results and comparisons are also given to confirm performance improvement of
our approach (in terms of mean square error and acquisition probability)
compared to the original TDT algorithm in multi-user environments, especially
in the NDA mode.
"
447,"A framework for the analytical performance assessment of matrix and
  tensor-based ESPRIT-type algorithms","  In this paper we present a generic framework for the asymptotic performance
analysis of subspace-based parameter estimation schemes. It is based on earlier
results on an explicit first-order expansion of the estimation error in the
signal subspace obtained via an SVD of the noisy observation matrix. We extend
these results in a number of aspects. Firstly, we derive an explicit
first-order expansion of the Higher- Order SVD (HOSVD)-based subspace estimate.
Secondly, we show how to obtain explicit first-order expansions of the
estimation error of ESPRIT-type algorithms and provide the expressions for
matrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT. Thirdly, we
derive closed-form expressions for the mean square error (MSE) and show that
they only depend on the second-order moments of the noise. Hence, we only need
the noise to be zero mean and possess finite second order moments. Fourthly, we
investigate the effect of using Structured Least Squares (SLS) to solve the
overdetermined shift invariance equations in ESPRIT and provide an explicit
first-order expansion as well as a closed-form MSE expression. Finally, we
simplify the MSE for the special case of a single source and compute the
asymptotic efficiency of the investigated ESPRIT-type algorithms in compact
closed-form expressions which only depend on the array size and the effective
SNR. Our results are more general than existing results on the performance
analysis of ESPRIT-type algorithms since (a) we do not need any assumptions
about the noise except for the mean to be zero and the second-order moments to
be finite (in contrast to earlier results that require Gaussianity or
second-order circular symmetry); (b) our results are asymptotic in the
effective SNR, i.e., we do not require the number of samples to be large; (c)
we present a framework that incorporates various ESPRIT-type algorithms in one
unified manner.
"
448,"Storage Workload Modelling by Hidden Markov Models: Application to FLASH
  Memory","  A workload analysis technique is presented that processes data from operation
type traces and creates a Hidden Markov Model (HMM) to represent the workload
that generated those traces. The HMM can be used to create representative
traces for performance models, such as simulators, avoiding the need to
repeatedly acquire suitable traces. It can also be used to estimate directly
the transition probabilities and rates of a Markov modulated arrival process,
for use as input to an analytical performance model of Flash memory. The HMMs
obtained from industrial workloads are validated by comparing their
autocorrelation functions and other statistics with those of the corresponding
monitored time series. Further, the performance model applications are
illustrated by numerical examples.
"
449,"Congestion Control of TCP Flows in Internet Routers by Means of Index
  Policy","  In this paper we address the problem of fast and fair transmission of flows
in a router, which is a fundamental issue in networks like the Internet. We
model the interaction between a TCP source and a bottleneck queue with the
objective of designing optimal packet admission controls in the bottleneck
queue. We focus on the relaxed version of the problem obtained by relaxing the
fixed buffer capacity constraint that must be satisfied at all time epoch. The
relaxation allows us to reduce the multi-flow problem into a family of
single-flow problems, for which we can analyze both theoretically and
numerically the existence of optimal control policies of special structure. In
particular, we show that for a variety of parameters, TCP flows can be
optimally controlled in routers by so-called index policies, but not always by
threshold policies. We have also implemented index policies in Network
Simulator-3 and tested in a simple topology their applicability in real
networks. The simulation results show that the index policy covers a big range
of desirable properties with respect to fairness between different versions of
TCP models, across users with different round-trip-time and minimum buffer
required to achieve full utility of the queue.
"
450,"A Resource Intensive Traffic-Aware Scheme for Cluster-based Energy
  Conservation in Wireless Devices","  Wireless traffic that is destined for a certain device in a network, can be
exploited in order to minimize the availability and delay trade-offs, and
mitigate the Energy consumption. The Energy Conservation (EC) mechanism can be
node-centric by considering the traversed nodal traffic in order to prolong the
network lifetime. This work describes a quantitative traffic-based approach
where a clustered Sleep-Proxy mechanism takes place in order to enable each
node to sleep according to the time duration of the active traffic that each
node expects and experiences. Sleep-proxies within the clusters are created
according to pairwise active-time comparison, where each node expects during
the active periods, a requested traffic. For resource availability and recovery
purposes, the caching mechanism takes place in case where the node for which
the traffic is destined is not available. The proposed scheme uses Role-based
nodes which are assigned to manipulate the traffic in a cluster, through the
time-oriented backward difference traffic evaluation scheme. Simulation study
is carried out for the proposed backward estimation scheme and the
effectiveness of the end-to-end EC mechanism taking into account a number of
metrics and measures for the effects while incrementing the sleep time duration
under the proposed framework. Comparative simulation results show that the
proposed scheme could be applied to infrastructure-less systems, providing
energy-efficient resource exchange with significant minimization in the power
consumption of each device.
"
451,Delay-Optimal Data Forwarding in Vehicular Sensor Networks,"  Vehicular Sensor Network (VSN) is emerging as a new solution for monitoring
urban environments such as Intelligent Transportation Systems and air
pollution. One of the crucial factors that determine the service quality of
urban monitoring applications is the delivery delay of sensing data packets in
the VSN. In this paper, we study the problem of routing data packets with
minimum delay in the VSN, by exploiting i) vehicle traffic statistics, ii)
anycast routing and iii) knowledge of future trajectories of vehicles such as
buses. We first introduce a novel road network graph model that incorporates
the three factors into the routing metric. We then characterize the packet
delay on each edge as a function of the vehicle density, speed and the length
of the edge. Based on the network model and delay function, we formulate the
packet routing problem as a Markov Decision Process (MDP) and develop an
optimal routing policy by solving the MDP. Evaluations using real vehicle
traces in a city show that our routing policy significantly improves the delay
performance compared to existing routing protocols.
"
452,"Distributing an Exact Algorithm for Maximum Clique: maximising the
  costup","  We take an existing implementation of an algorithm for the maximum clique
problem and modify it so that we can distribute it over an ad-hoc cluster of
machines. Our goal was to achieve a significant speedup in performance with
minimal development effort, i.e. a maximum costup. We present a simple
modification to a state-of-the-art exact algorithm for maximum clique that
allows us to distribute it across many machines. An empirical study over large
hard benchmarks shows that speedups of an order of magnitude are routine for 25
or more machines.
"
453,Locating Disruptions on Internet Paths through End-to-End Measurements,"  In backbone networks carrying heavy traffic loads, unwanted and unusual
end-to-end delay changes can happen, though possibly rarely. In order to
understand and manage the network to potentially avoid such abrupt changes, it
is crucial and challenging to locate where in the network lies the cause of
such delays so that some corresponding actions may be taken. To tackle this
challenge, the present paper proposes a simple and novel approach. The proposed
approach relies only on end-to-end measurements, unlike literature approaches
that often require a distributed and possibly complicated monitoring /
measurement infrastructure. Here, the key idea of the proposed approach is to
make use of compressed sensing theory to estimate delays on each hop between
the two nodes where end-to-end delay measurement is conducted, and infer
critical hops that contribute to the abrupt delay increases. To demonstrate its
effectiveness, the proposed approach is applied to a real network. The results
are encouraging, showing that the proposed approach is able to locate the hops
that have the most significant impact on or contribute the most to abrupt
increases on the end-to-end delay.
"
454,Fast Packed String Matching for Short Patterns,"  Searching for all occurrences of a pattern in a text is a fundamental problem
in computer science with applications in many other fields, like natural
language processing, information retrieval and computational biology. In the
last two decades a general trend has appeared trying to exploit the power of
the word RAM model to speed-up the performances of classical string matching
algorithms. In this model an algorithm operates on words of length w, grouping
blocks of characters, and arithmetic and logic operations on the words take one
unit of time. In this paper we use specialized word-size packed string matching
instructions, based on the Intel streaming SIMD extensions (SSE) technology, to
design very fast string matching algorithms in the case of short patterns. From
our experimental results it turns out that, despite their quadratic worst case
time complexity, the new presented algorithms become the clear winners on the
average for short patterns, when compared against the most effective algorithms
known in literature.
"
455,"Scalable Analysis for Large Social Networks: the data-aware mean-field
  approach","  Studies on social networks have proved that endogenous and exogenous factors
influence dynamics. Two streams of modeling exist on explaining the dynamics of
social networks: 1) models predicting links through network properties, and 2)
models considering the effects of social attributes. In this interdisciplinary
study we work to overcome a number of computational limitations within these
current models. We employ a mean-field model which allows for the construction
of a population-specific socially informed model for predicting links from both
network and social properties in large social networks. The model is tested on
a population of conference coauthorship behavior, considering a number of
parameters from available Web data. We address how large social networks can be
modeled preserving both network and social parameters. We prove that the
mean-field model, using a data-aware approach, allows us to overcome
computational burdens and thus scalability issues in modeling large social
networks in terms of both network and social parameters. Additionally, we
confirm that large social networks evolve through both network and
social-selection decisions; asserting that the dynamics of networks cannot
singly be studied from a single perspective but must consider effects of social
parameters.
"
456,"Quantum Monte Carlo for large chemical systems: Implementing efficient
  strategies for petascale platforms and beyond","  Various strategies to implement efficiently QMC simulations for large
chemical systems are presented. These include: i.) the introduction of an
efficient algorithm to calculate the computationally expensive Slater matrices.
This novel scheme is based on the use of the highly localized character of
atomic Gaussian basis functions (not the molecular orbitals as usually done),
ii.) the possibility of keeping the memory footprint minimal, iii.) the
important enhancement of single-core performance when efficient optimization
tools are employed, and iv.) the definition of a universal, dynamic,
fault-tolerant, and load-balanced computational framework adapted to all kinds
of computational platforms (massively parallel machines, clusters, or
distributed grids). These strategies have been implemented in the QMC=Chem code
developed at Toulouse and illustrated with numerical applications on small
peptides of increasing sizes (158, 434, 1056 and 1731 electrons). Using 10k-80k
computing cores of the Curie machine (GENCI-TGCC-CEA, France) QMC=Chem has been
shown to be capable of running at the petascale level, thus demonstrating that
for this machine a large part of the peak performance can be achieved.
Implementation of large-scale QMC simulations for future exascale platforms
with a comparable level of efficiency is expected to be feasible.
"
457,PCNM: A New Platform for Cellular Networks Measurements and Optimization,"  In this paper, we present PCNM, a new mobile platform for cellular networks
measurements. PCNM is based on a set of techniques that tailors theoretical
calculations and simulations to the real cellular network environment. It
includes: (a) modules that measure different parameters of a base station (BS)
such as localization, cells identification, time advance information, reception
level and quality, (b) a new protocol that optimizes the task of network
measurement by monitoring a set of mobile nodes and finally (c) the ability to
extend an existing cellular network by adding new base stations. We evaluate
our genetic algorithm used to reduce the nodes mobility and optimize the
measurement extraction of N base stations using k mobile sensors (k >= 1). We
show how connecting real measurements (using mobile sensors in a collaborative
way) to theoretical and prediction methods is of high benefits for cellular
networks maintenance, extension and performances evaluation.
"
458,"Wireless Network Coding via Modified 802.11 MAC/PHY: Design and
  Implementation on SDR","  Network coding (NC), in principle, is a Layer-3 innovation that improves
network throughput in wired networks for multicast/broadcast scenarios. Due to
the fundamental differences between wired and wireless networks, extending NC
to wireless networks generates several new and significant practical
challenges. Two-way information exchange (both symmetric and asymmetric).
Network coding (NC), in principle, is a Layer-3 innovation that improves
network throughput in wired networks for multicast/broadcast scenarios. Due to
the fundamental differences between wired and wireless networks, extending NC
to wireless networks generates several new and significant practical
challenges. Two-way information exchange (both symmetric and asymmetric)
between a pair of 802.11 sources/sinks using an intermediate relay node is a
canonical scenario for evaluating the effectiveness of Wireless Network Coding
(WNC) in a practical setting. Our primary objective in this work is to suggest
pragmatic and novel modifications at the MAC and PHY layers of the 802.11
protocol stack on a Software Radio (SORA) platform to support WNC and obtain
achievable throughput estimates via lab-scale experiments. Our results show
that network coding (at the MAC or PHY layer) increases system
throughput-typically by 20-30%%.
"
459,Exploiting Network Cooperation in Green Wireless Communication,"  There is a growing interest in energy efficient or so-called ""green"" wireless
communication to reduce the energy consumption in cellular networks. Since
today's wireless terminals are typically equipped with multiple network access
interfaces such as Bluetooth, Wi-Fi, and cellular networks, this paper
investigates user terminals cooperating with each other in transmitting their
data packets to the base station (BS), by exploiting the multiple network
access interfaces, called inter-network cooperation. We also examine the
conventional schemes without user cooperation and with intra-network
cooperation for comparison. Given target outage probability and data rate
requirements, we analyze the energy consumption of conventional schemes as
compared to the proposed inter-network cooperation by taking into account both
physical-layer channel impairments (including path loss, fading, and thermal
noise) and upper-layer protocol overheads. It is shown that distances between
different network entities (i.e., user terminals and BS) have a significant
influence on the energy efficiency of proposed inter-network cooperation
scheme. Specifically, when the cooperating users are close to BS or the users
are far away from each other, the inter-network cooperation may consume more
energy than conventional schemes without user cooperation or with intra-network
cooperation. However, as the cooperating users move away from BS and the
inter-user distance is not too large, the inter-network cooperation
significantly reduces the energy consumption over conventional schemes.
"
460,Solid State Disk Object-Based Storage with Trim Commands,"  This paper presents a model of NAND flash SSD utilization and write
amplification when the ATA/ATAPI SSD Trim command is incorporated into
object-based storage under a variety of user workloads, including a uniform
random workload with objects of fixed size and a uniform random workload with
objects of varying sizes. We first summarize the existing models for write
amplification in SSDs for workloads with and without the Trim command, then
propose an alteration of the models that utilizes a framework of object-based
storage. The utilization of objects and pages in the SSD is derived, with the
analytic results compared to simulation. Finally, the effect of objects on
write amplification and its computation is discussed along with a potential
application to optimization of SSD usage through object storage metadata
servers that allocate object classes of distinct object size.
"
461,Performance Evaluation: Ball-Tree and KD-Tree in the Context of MST,"  Now a days many algorithms are invented or being inventing to find the
solution for Euclidean Minimum Spanning Tree, EMST, problem, as its
applicability is increasing in much wide range of fields containing spatial or
spatio temporal data viz. astronomy which consists of millions of spatial data.
To solve this problem, we are presenting a technique by adopting the dual tree
algorithm for finding efficient EMST and experimented on a variety of real time
and synthetic datasets. This paper presents the observed experimental
observations and the efficiency of the dual tree framework, in the context of
kdtree and ball tree on spatial datasets of different dimensions.
"
462,"Computing Petaflops over Terabytes of Data: The Case of Genome-Wide
  Association Studies","  In many scientific and engineering applications, one has to solve not one but
a sequence of instances of the same problem. Often times, the problems in the
sequence are linked in a way that allows intermediate results to be reused. A
characteristic example for this class of applications is given by the
Genome-Wide Association Studies (GWAS), a widely spread tool in computational
biology. GWAS entails the solution of up to trillions ($10^{12}$) of correlated
generalized least-squares problems, posing a daunting challenge: the
performance of petaflops ($10^{15}$ floating-point operations) over terabytes
of data.
  In this paper, we design an algorithm for performing GWAS on multi-core
architectures. This is accomplished in three steps. First, we show how to
exploit the relation among successive problems, thus reducing the overall
computational complexity. Then, through an analysis of the required data
transfers, we identify how to eliminate any overhead due to input/output
operations. Finally, we study how to decompose computation into tasks to be
distributed among the available cores, to attain high performance and
scalability. With our algorithm, a GWAS that currently requires the use of a
supercomputer may now be performed in matter of hours on a single multi-core
node.
  The discussion centers around the methodology to develop the algorithm rather
than the specific application. We believe the paper contributes valuable
guidelines of general applicability for computational scientists on how to
develop and optimize numerical algorithms.
"
463,"Eigenvalue-based Cyclostationary Spectrum Sensing Using Multiple
  Antennas","  In this paper, we propose a signal-selective spectrum sensing method for
cognitive radio networks and specifically targeted for receivers with
multiple-antenna capability. This method is used for detecting the presence or
absence of primary users based on the eigenvalues of the cyclic covariance
matrix of received signals. In particular, the cyclic correlation significance
test is used to detect a specific signal-of-interest by exploiting knowledge of
its cyclic frequencies. The analytical threshold for achieving constant false
alarm rate using this detection method is presented, verified through
simulations, and shown to be independent of both the number of samples used and
the noise variance, effectively eliminating the dependence on accurate noise
estimation. The proposed method is also shown, through numerical simulations,
to outperform existing multiple-antenna cyclostationary-based spectrum sensing
algorithms under a quasi-static Rayleigh fading channel, in both spatially
correlated and uncorrelated noise environments. The algorithm also has
significantly lower computational complexity than these other approaches.
"
464,"Performance Indicator for MIMO MMSE Receivers in the Presence of Channel
  Estimation Error","  We present the derivation of post-processing SNR for
Minimum-Mean-Squared-Error (MMSE) receivers with imperfect channel estimates,
and show that it is an accurate indicator of the error rate performance of MIMO
systems in the presence of channel estimation error. Simulation results show
the tightness of the analysis.
"
465,Distribution of the Number of Retransmissions of Bounded Documents,"  Retransmission-based failure recovery represents a primary approach in
existing communication networks that guarantees data delivery in the presence
of channel failures. Recent work has shown that, when data sizes have infinite
support, retransmissions can cause long (-tailed) delays even if all traffic
and network characteristics are light-tailed. In this paper we investigate the
practically important case of bounded data units 0 <= L_b <= b under the
condition that the hazard functions of the distributions of data sizes and
channel statistics are proportional. To this end, we provide an explicit and
uniform characterization of the entire body of the retransmission distribution
Pr[N_b > n] in both n and b. Our main discovery is that this distribution can
be represented as the product of a power law and Gamma distribution. This
rigorous approximation clearly demonstrates the coupling of a power law
distribution, dominating the main body, and the Gamma distribution, determining
the exponential tail. Our results are validated via simulation experiments and
can be useful for designing retransmission-based systems with the required
performance characteristics. From a broader perspective, this study applies to
any other system, e.g., computing, where restart mechanisms are employed after
a job processing failure.
"
466,"Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic
  Correlation Significance Test","  In this paper, we propose and analyze a spectrum sensing method based on
cyclostationarity specifically targeted for receivers with multiple antennas.
This detection method is used for determining the presence or absence of
primary users in cognitive radio networks based on the eigenvalues of the
cyclic covariance matrix of received signals. In particular, the cyclic
correlation significance test is used to detect a specific signal-of-interest
by exploiting knowledge of its cyclic frequencies. Analytical expressions for
the probability of detection and probability of false-alarm under both
spatially uncorrelated or spatially correlated noise are derived and verified
by simulation. The detection performance in a Rayleigh flat-fading environment
is found and verified through simulations. One of the advantages of the
proposed method is that the detection threshold is shown to be independent of
both the number of samples and the noise covariance, effectively eliminating
the dependence on accurate noise estimation. The proposed method is also shown
to provide higher detection probability and better robustness to noise
uncertainty than existing multiple-antenna cyclostationary-based spectrum
sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading
channel.
"
467,Stochastic Superoptimization,"  We formulate the loop-free, binary superoptimization task as a stochastic
search problem. The competing constraints of transformation correctness and
performance improvement are encoded as terms in a cost function, and a Markov
Chain Monte Carlo sampler is used to rapidly explore the space of all possible
programs to find one that is an optimization of a given target program.
Although our method sacrifices com- pleteness, the scope of programs we are
able to reason about, and the quality of the programs we produce, far exceed
those of existing superoptimizers. Beginning from binaries com- piled by llvm
-O0 for 64-bit X86, our prototype implemen- tation, STOKE, is able to produce
programs which either match or outperform the code sequences produced by gcc
with full optimizations enabled, and, in some cases, expert handwritten
assembly.
"
468,Queuing with future information,"  We study an admissions control problem, where a queue with service rate $1-p$
receives incoming jobs at rate $\lambda\in(1-p,1)$, and the decision maker is
allowed to redirect away jobs up to a rate of $p$, with the objective of
minimizing the time-average queue length. We show that the amount of
information about the future has a significant impact on system performance, in
the heavy-traffic regime. When the future is unknown, the optimal average queue
length diverges at rate $\sim\log_{1/(1-p)}\frac{1}{1-\lambda}$, as $\lambda\to
1$. In sharp contrast, when all future arrival and service times are revealed
beforehand, the optimal average queue length converges to a finite constant,
$(1-p)/p$, as $\lambda\to1$. We further show that the finite limit of $(1-p)/p$
can be achieved using only a finite lookahead window starting from the current
time frame, whose length scales as $\mathcal{O}(\log\frac{1}{1-\lambda})$, as
$\lambda\to1$. This leads to the conjecture of an interesting duality between
queuing delay and the amount of information about the future.
"
469,Performance of SSE and AVX Instruction Sets,"  SSE (streaming SIMD extensions) and AVX (advanced vector extensions) are SIMD
(single instruction multiple data streams) instruction sets supported by recent
CPUs manufactured in Intel and AMD. This SIMD programming allows parallel
processing by multiple cores in a single CPU. Basic arithmetic and data
transfer operations such as sum, multiplication and square root can be
processed simultaneously. Although popular compilers such as GNU compilers and
Intel compilers provide automatic SIMD optimization options, one can obtain
better performance by a manual SIMD programming with proper optimization: data
packing, data reuse and asynchronous data transfer. In particular, linear
algebraic operations of vectors and matrices can be easily optimized by the
SIMD programming. Typical calculations in lattice gauge theory are composed of
linear algebraic operations of gauge link matrices and fermion vectors, and so
can adopt the manual SIMD programming to improve the performance.
"
470,Algorithm Runtime Prediction: Methods & Evaluation,"  Perhaps surprisingly, it is possible to predict how long an algorithm will
take to run on a previously unseen input, using machine learning techniques to
build a model of the algorithm's runtime as a function of problem-specific
instance features. Such models have important applications to algorithm
analysis, portfolio-based algorithm selection, and the automatic configuration
of parameterized algorithms. Over the past decade, a wide variety of techniques
have been studied for building such models. Here, we describe extensions and
improvements of existing models, new families of models, and -- perhaps most
importantly -- a much more thorough treatment of algorithm parameters as model
inputs. We also comprehensively describe new and existing features for
predicting algorithm runtime for propositional satisfiability (SAT), travelling
salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate
these innovations through the largest empirical analysis of its kind, comparing
to a wide range of runtime modelling techniques from the literature. Our
experiments consider 11 algorithms and 35 instance distributions; they also
span a very wide range of SAT, MIP, and TSP instances, with the least
structured having been generated uniformly at random and the most structured
having emerged from real industrial applications. Overall, we demonstrate that
our new models yield substantially better runtime predictions than previous
approaches in terms of their generalization to new problem instances, to new
algorithms from a parameterized space, and to both simultaneously.
"
471,Data-parallel programming with Intel Array Building Blocks (ArBB),"  Intel Array Building Blocks is a high-level data-parallel programming
environment designed to produce scalable and portable results on existing and
upcoming multi- and many-core platforms.
  We have chosen several mathematical kernels - a dense matrix-matrix
multiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a
conjugate gradients solver - as synthetic benchmarks and representatives of
scientific codes and ported them to ArBB. This whitepaper describes the ArBB
ports and presents performance and scaling measurements on the Westmere-EX
based system SuperMIG at LRZ in comparison with OpenMP and MKL.
"
472,Hybrid Behaviour of Markov Population Models,"  We investigate the behaviour of population models written in Stochastic
Concurrent Constraint Programming (sCCP), a stochastic extension of Concurrent
Constraint Programming. In particular, we focus on models from which we can
define a semantics of sCCP both in terms of Continuous Time Markov Chains
(CTMC) and in terms of Stochastic Hybrid Systems, in which some populations are
approximated continuously, while others are kept discrete. We will prove the
correctness of the hybrid semantics from the point of view of the limiting
behaviour of a sequence of models for increasing population size. More
specifically, we prove that, under suitable regularity conditions, the sequence
of CTMC constructed from sCCP programs for increasing population size converges
to the hybrid system constructed by means of the hybrid semantics. We
investigate in particular what happens for sCCP models in which some
transitions are guarded by boolean predicates or in the presence of
instantaneous transitions.
"
473,"Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body
  Simulation","  Clusters of SMP nodes provide support for a wide diversity of parallel
programming paradigms. Combining both shared memory and message passing
parallelizations within the same application, the hybrid MPI-OpenMP paradigm is
an emerging trend for parallel programming to fully exploit distributed
shared-memory architecture. In this paper, we improve the performance of MPEG-2
encoder and n-body simulation by employing the hybrid MPI-OpenMP programming
paradigm on SMP clusters. The hierarchical image data structure of the MPEG
bit-stream is eminently suitable for the hybrid model to achieve multiple
levels of parallelism: MPI for parallelism at the group of pictures level
across SMP nodes and OpenMP for parallelism within pictures at the slice level
within each SMP node. Similarly, the work load of the force calculation which
accounts for upwards of 90% of the cycles in typical computations in the n-body
simulation is shared among OpenMP threads after ORB domain decomposition among
MPI processes. Besides, loop scheduling of OpenMP threads is adopted with
appropriate chunk size to provide better load balance of work, leading to
enhanced performance. With the n-body simulation, experimental results
demonstrate that the hybrid MPI-OpenMP program outperforms the corresponding
pure MPI program by average factors of 1.52 on a 4-way cluster and 1.21 on a
2-way cluster. Likewise, the hybrid model offers a performance improvement of
18% compared to the MPI model for the MPEG-2 encoder.
"
474,"Performance Evaluation of Treecode Algorithm for N-Body Simulation Using
  GridRPC System","  This paper is aimed at improving the performance of the treecode algorithm
for N-Body simulation by employing the NetSolve GridRPC programming model to
exploit the use of multiple clusters. N-Body is a classical problem, and
appears in many areas of science and engineering, including astrophysics,
molecular dynamics, and graphics. In the simulation of N-Body, the specific
routine for calculating the forces on the bodies which accounts for upwards of
90% of the cycles in typical computations is eminently suitable for obtaining
parallelism with GridRPC calls. It is divided among the compute nodes by
simultaneously calling multiple GridRPC requests to them. The performance of
the GridRPC implementation is then compared to that of the MPI version and
hybrid MPI-OpenMP version for the treecode algorithm on individual clusters.
"
475,"Deriving Pareto-optimal performance bounds for 1 and 2-relay wireless
  networks","  This work addresses the problem of deriving fundamental trade-off bounds for
a 1-relay and a 2-relay wireless network when multiple performance criteria are
of interest. It proposes a simple MultiObjective (MO) performance evaluation
framework composed of a broadcast and interference-limited network model;
capacity, delay and energy performance metrics and an associated MO
optimization problem. Pareto optimal performance bounds between end-to-end
delay and energy for a capacity-achieving network are given for 1-relay and
2-relay topologies and assessed through simulations. Moreover, we also show in
this paper that these bounds are tight since they can be reached by simple
practical coding strategies performed by the source and the relays. Two
different types of network coding strategies are investigated. Practical
performance bounds for both strategies are compared to the theoretical upper
bound. Results confirm that the proposed upper bound on delay and energy
performance is tight and can be reached with the proposed combined source and
network coding strategies.
"
476,The Universe at Extreme Scale: Multi-Petaflop Sky Simulation on the BG/Q,"  Remarkable observational advances have established a compelling
cross-validated model of the Universe. Yet, two key pillars of this model --
dark matter and dark energy -- remain mysterious. Sky surveys that map billions
of galaxies to explore the `Dark Universe', demand a corresponding
extreme-scale simulation capability; the HACC (Hybrid/Hardware Accelerated
Cosmology Code) framework has been designed to deliver this level of
performance now, and into the future. With its novel algorithmic structure,
HACC allows flexible tuning across diverse architectures, including accelerated
and multi-core systems.
  On the IBM BG/Q, HACC attains unprecedented scalable performance -- currently
13.94 PFlops at 69.2% of peak and 90% parallel efficiency on 1,572,864 cores
with an equal number of MPI ranks, and a concurrency of 6.3 million. This level
of performance was achieved at extreme problem sizes, including a benchmark run
with more than 3.6 trillion particles, significantly larger than any
cosmological simulation yet performed.
"
477,Critical Utility Infrastructural Resilience,"  The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a
European project within the research area of Critical Information
Infrastructure Protection, with a specific focus on the infrastructures
operated by power utilities, widely recognized as fundamental to national and
international economy, security and quality of life. Such infrastructures faced
with the recent market deregulations and the multiple interdependencies with
other infrastructures are becoming more and more vulnerable to various threats,
including accidental failures and deliberate sabotage and malicious attacks.
The subject of CRUTIAL research are small scale networked ICT systems used to
control and manage the electric power grid, in which artifacts controlling the
physical process of electricity transportation need to be connected with
corporate and societal applications performing management and maintenance
functionality. The peculiarity of such ICT-supported systems is that they are
related to the power system dynamics and its emergency conditions. Specific
effort need to be devoted by the Electric Power community and by the
Information Technology community to influence the technological progress in
order to allow commercial intelligent electronic devices to be effectively
deployed for the protection of citizens against cyber threats to electric power
management and control systems. A well-founded know-how needs to be built
inside the industrial power sector to allow all the involved stakeholders to
achieve their service objectives without compromising the resilience properties
of the logical and physical assets that support the electric power provision.
"
478,Modeling the resilience of large and evolving systems,"  This paper summarizes the state of knowledge and ongoing research on methods
and techniques for resilience evaluation, taking into account the
resilience-scaling challenges and properties related to the ubiquitous
computerized systems. We mainly focus on quantitative evaluation approaches
and, in particular, on model-based evaluation techniques that are commonly used
to evaluate and compare, from the dependability point of view, different
architecture alternatives at the design stage. We outline some of the main
modeling techniques aiming at mastering the largeness of analytical
dependability models at the construction level. Actually, addressing the model
largeness problem is important with respect to the investigation of the
scalability of current techniques to meet the complexity challenges of
ubiquitous systems. Finally we present two case studies in which some of the
presented techniques are applied for modeling web services and General Packet
Radio Service (GPRS) mobile telephone networks, as prominent examples of large
and evolving systems.
"
479,"Optimal Power Allocation for Outage Minimization in Fading Channels with
  Energy Harvesting Constraints","  This paper studies the optimal power allocation for outage minimization in
point-to-point fading channels with the energy-harvesting constraints and
channel distribution information (CDI) at the transmitter. Both the cases with
non-causal and causal energy state information (ESI) are considered, which
correspond to the energy harvesting rates being known and unknown prior to the
transmissions, respectively. For the non-causal ESI case, the average outage
probability minimization problem over a finite horizon is shown to be
non-convex for a large class of practical fading channels. However, the
globally optimal ""offline"" power allocation is obtained by a forward search
algorithm with at most $N$ one-dimensional searches, and the optimal power
profile is shown to be non-decreasing over time and have an interesting
""save-then-transmit"" structure. In particular, for the special case of N=1, our
result revisits the classic outage capacity for fading channels with uniform
power allocation. Moreover, for the case with causal ESI, we propose both the
optimal and suboptimal ""online"" power allocation algorithms, by applying the
technique of dynamic programming and exploring the structure of optimal offline
solutions, respectively.
"
480,The Cost of Address Translation,"  Modern computers are not random access machines (RAMs). They have a memory
hierarchy, multiple cores, and virtual memory. In this paper, we address the
computational cost of address translation in virtual memory. Starting point for
our work is the observation that the analysis of some simple algorithms (random
scan of an array, binary search, heapsort) in either the RAM model or the EM
model (external memory model) does not correctly predict growth rates of actual
running times. We propose the VAT model (virtual address translation) to
account for the cost of address translations and analyze the algorithms
mentioned above and others in the model. The predictions agree with the
measurements. We also analyze the VAT-cost of cache-oblivious algorithms.
"
481,"Achieving Optimal Throughput and Near-Optimal Asymptotic Delay
  Performance in Multi-Channel Wireless Networks with Low Complexity: A
  Practical Greedy Scheduling Policy","  In this paper, we focus on the scheduling problem in multi-channel wireless
networks, e.g., the downlink of a single cell in fourth generation (4G)
OFDM-based cellular networks. Our goal is to design practical scheduling
policies that can achieve provably good performance in terms of both throughput
and delay, at a low complexity. While a class of $O(n^{2.5} \log n)$-complexity
hybrid scheduling policies are recently developed to guarantee both
rate-function delay optimality (in the many-channel many-user asymptotic
regime) and throughput optimality (in the general non-asymptotic setting),
their practical complexity is typically high. To address this issue, we develop
a simple greedy policy called Delay-based Server-Side-Greedy (D-SSG) with a
\lower complexity $2n^2+2n$, and rigorously prove that D-SSG not only achieves
throughput optimality, but also guarantees near-optimal asymptotic delay
performance. Specifically, we show that the rate-function attained by D-SSG for
any delay-violation threshold $b$, is no smaller than the maximum achievable
rate-function by any scheduling policy for threshold $b-1$. Thus, we are able
to achieve a reduction in complexity (from $O(n^{2.5} \log n)$ of the hybrid
policies to $2n^2 + 2n$) with a minimal drop in the delay performance. More
importantly, in practice, D-SSG generally has a substantially lower complexity
than the hybrid policies that typically have a large constant factor hidden in
the $O(\cdot)$ notation. Finally, we conduct numerical simulations to validate
our theoretical results in various scenarios. The simulation results show that
D-SSG not only guarantees a near-optimal rate-function, but also empirically is
virtually indistinguishable from delay-optimal policies.
"
482,Power Consumption Analysis of a Modern Smartphone,"  This paper presents observations about power consumption of a latest
smartphone. Modern smartphones are powerful devices with different choices of
data connections and other functional modes. This paper provides analysis of
power utilization for these different operation modes. Also, we present power
consumption by vital operating system (OS) components.
"
483,"Information model for model driven safety requirements management of
  complex systems","  The aim of this paper is to propose a rigorous and complete design framework
for complex system based on system engineering (SE) principles. The SE standard
EIA-632 is used to guide the approach. Within this framework, two aspects are
presented. The first one concerns the integration of safety requirements and
management in system engineering process. The objective is to help designers
and engineers in managing safety of complex systems. The second aspect concerns
model driven design through the definition of an information model. This model
is based on SysML (System Modeling Language) to address requirements definition
and their traceability towards the solution and the Verification and Validation
(V&V) elements.
"
484,"Opportunistic Relaying in Wireless Body Area Networks: Coexistence
  Performance","  In this paper, a cooperative two-hop communication scheme, together with
opportunistic relaying (OR), is applied within a mobile wireless body area
network (WBAN). Its effectiveness in interference mitigation is investigated in
a scenario where there are multiple closely-located networks. Due to a typical
WBAN's nature, no coordination is used among different WBANs. A suitable
time-division-multiple-access (TDMA) is adopted as both an intra-network and
also an inter-network access scheme. Extensive on-body and off-body channel
gain measurements are employed to gauge performance, which are overlaid to
simulate a realistic WBAN working environment. It is found that opportunistic
relaying is able to improve the signal-to-interference-and-noise ratio (SINR)
threshold value at outage probability of 10% by an average of 5 dB, and it is
also shown that it can reduce level crossing rate (LCR) significantly at a low
SINR threshold value. Furthermore, this scheme is more efficient when on-body
channels fade less slowly.
"
485,Operational semantics for product-form solution,"  In this paper we present product-form solutions from the point of view of
stochastic process algebra. In previous work we have shown how to derive
product-form solutions for a formalism called Labelled Markov Automata (LMA).
LMA are very useful as their relation with the Continuous Time Markov Chains is
very direct. The disadvantage of using LMA is that the proofs of properties are
cumbersome. In fact, in LMA it is not possible to use the inductive structure
of the language in a proof. In this paper we consider a simple stochastic
process algebra that has the great advantage of simplifying the proofs. This
simple language has been inspired by PEPA, however, detailed analysis of the
semantics of cooperation will show the differences between the two formalisms.
It will also be shown that the semantics of the cooperation in process algebra
influences the correctness of the derivation of the product-form solutions.
"
486,A Blind Time-Reversal Detector in the Presence of Channel Correlation,"  A blind target detector using the time reversal transmission is proposed in
the presence of channel correlation. We calculate the exact moments of the test
statistics involved. The derived moments are used to construct an accurate
approximative Likelihood Ratio Test (LRT) based on multivariate Edgeworth
expansion. Performance gain over an existing detector is observed in scenarios
with channel correlation and relatively strong target signal.
"
487,"Exploring mutexes, the Oracle RDBMS retrial spinlocks","  Spinlocks are widely used in database engines for processes synchronization.
KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions
for submicrosecond synchronization. The mutex contention is frequently observed
in highly concurrent OLTP environments.
  This work explores how Oracle mutexes operate, spin, and sleep. It develops
predictive mathematical model and discusses parameters and statistics related
to mutex performance tuning, as well as results of contention experiments.
"
488,A Multi-GPU Programming Library for Real-Time Applications,"  We present MGPU, a C++ programming library targeted at single-node multi-GPU
systems. Such systems combine disproportionate floating point performance with
high data locality and are thus well suited to implement real-time algorithms.
We describe the library design, programming interface and implementation
details in light of this specific problem domain. The core concepts of this
work are a novel kind of container abstraction and MPI-like communication
methods for intra-system communication. We further demonstrate how MGPU is used
as a framework for porting existing GPU libraries to multi-device
architectures. Putting our library to the test, we accelerate an iterative
non-linear image reconstruction algorithm for real-time magnetic resonance
imaging using multiple GPUs. We achieve a speed-up of about 1.7 using 2 GPUs
and reach a final speed-up of 2.1 with 4 GPUs. These promising results lead us
to conclude that multi-GPU systems are a viable solution for real-time MRI
reconstruction as well as signal-processing applications in general.
"
489,Load-aware Channel Selection for 802.11 WLANs with Limited Measurement,"  It has been known that load unaware channel selection in 802.11 networks
results in high level interference, and can significantly reduce the network
throughput. In current implementation, the only way to determine the traffic
load on a channel is to measure that channel for a certain duration of time.
Therefore, in order to find the best channel with the minimum load all channels
have to be measured, which is costly and can cause unacceptable communication
interruptions between the AP and the stations. In this paper, we propose a
learning based approach which aims to find the channel with the minimum load by
measuring only limited number of channels. Our method uses Gaussian Process
Regressing to accurately track the traffic load on each channel based on the
previous measured load. We confirm the performance of our algorithm by using
experimental data, and show that the time consumed for the load measurement can
be reduced up to 46% compared to the case where all channels are monitored.
"
490,"Technical Report: Beaconless Geo-Routing Under The Spotlight: Practical
  Link Models and Application Scenarios","  Analysis and simulation of beaconless geo-routing protocols have been
traditionally conducted assuming equal communication ranges for the data and
control packets. In reality, this is not true since the communication range is
actually function of the packet length. Control packets are typically much
shorter than data packets. As a consequence, a substantial discrepancy exists
in practice between their respective communication ranges. In this paper, we
devise a practical link model for computing the effective communication range.
We further introduce two simple strategies for bridging the gap between the
control and data packet communication ranges. Our primary objective in this
paper is to construct a realistic analytical framework describing the
end-to-end performance of beaconless geo-routing protocols. Two flagship
protocols are selected in this paper for further investigation under the
developed framework. For a better perspective, the two protocols are actually
compared to a hypothetical limit case; one which offers optimal energy and
latency performance. Finally, we present four different application scenarios.
For each scenario, we highlight the geo-routing protocol which performs the
best and discuss the reasons behind it.
"
491,"Video Tester -- A multiple-metric framework for video quality assessment
  over IP networks","  This paper presents an extensible and reusable framework which addresses the
problem of video quality assessment over IP networks. The proposed tool
(referred to as Video-Tester) supports raw uncompressed video encoding and
decoding. It also includes different video over IP transmission methods (i.e.:
RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it
is furnished with a rich set of offline analysis capabilities. Video-Tester
analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,
packet inter-arrival time, jitter and loss rate, as well as GOP size and
I-frame loss rate). Our design facilitates the integration of virtually any
existing video quality metric thanks to the adopted Python-based modular
approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video
quality metric, DIV and PSNR-based MOS estimations. In order to promote its use
and extension, Video-Tester is open and publicly available.
"
492,"Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel
  Xeon Phi","  Intel Xeon Phi is a recently released high-performance coprocessor which
features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD
registers achieving a peak theoretical performance of 1Tflop/s in double
precision. Many scientific applications involve operations on large sparse
matrices such as linear solvers, eigensolver, and graph mining algorithms. The
core of most of these applications involves the multiplication of a large,
sparse matrix with a dense vector (SpMV). In this paper, we investigate the
performance of the Xeon Phi coprocessor for SpMV. We first provide a
comprehensive introduction to this new architecture and analyze its peak
performance with a number of micro benchmarks. Although the design of a Xeon
Phi core is not much different than those of the cores in modern processors,
its large number of cores and hyperthreading capability allow many application
to saturate the available memory bandwidth, which is not the case for many
cutting-edge processors. Yet, our performance studies show that it is the
memory latency not the bandwidth which creates a bottleneck for SpMV on this
architecture. Finally, our experiments show that Xeon Phi's sparse kernel
performance is very promising and even better than that of cutting-edge general
purpose processors and GPUs.
"
493,On a Catalogue of Metrics for Evaluating Commercial Cloud Services,"  Given the continually increasing amount of commercial Cloud services in the
market, evaluation of different services plays a significant role in
cost-benefit analysis or decision making for choosing Cloud Computing. In
particular, employing suitable metrics is essential in evaluation
implementations. However, to the best of our knowledge, there is not any
systematic discussion about metrics for evaluating Cloud services. By using the
method of Systematic Literature Review (SLR), we have collected the de facto
metrics adopted in the existing Cloud services evaluation work. The collected
metrics were arranged following different Cloud service features to be
evaluated, which essentially constructed an evaluation metrics catalogue, as
shown in this paper. This metrics catalogue can be used to facilitate the
future practice and research in the area of Cloud services evaluation.
Moreover, considering metrics selection is a prerequisite of benchmark
selection in evaluation implementations, this work also supplements the
existing research in benchmarking the commercial Cloud services.
"
494,"Towards a Taxonomy of Performance Evaluation of Commercial Cloud
  Services","  Cloud Computing, as one of the most promising computing paradigms, has become
increasingly accepted in industry. Numerous commercial providers have started
to supply public Cloud services, and corresponding performance evaluation is
then inevitably required for Cloud provider selection or cost-benefit analysis.
Unfortunately, inaccurate and confusing evaluation implementations can be often
seen in the context of commercial Cloud Computing, which could severely
interfere and spoil evaluation-related comprehension and communication. This
paper introduces a taxonomy to help profile and standardize the details of
performance evaluation of commercial Cloud services. Through a systematic
literature review, we constructed the taxonomy along two dimensions by
arranging the atomic elements of Cloud-related performance evaluation. As such,
this proposed taxonomy can be employed both to analyze existing evaluation
practices through decomposition into elements and to design new experiments
through composing elements for evaluating performance of commercial Cloud
services. Moreover, through smooth expansion, we can continually adapt this
taxonomy to the more general area of evaluation of Cloud Computing.
"
495,"Exploiting the Past to Reduce Delay in CSMA Scheduling: A High-order
  Markov Chain Approach","  Recently several CSMA algorithms based on the Glauber dynamics model have
been proposed for multihop wireless scheduling, as viable solutions to achieve
the throughput optimality, yet are simple to implement. However, their delay
performances still remain unsatisfactory, mainly due to the nature of the
underlying Markov chains that imposes a fundamental constraint on how the link
state can evolve over time. In this paper, we propose a new approach toward
better queueing and delay performance, based on our observation that the
algorithm needs not be Markovian, as long as it can be implemented in a
distributed manner, achieve the same throughput optimality, while offering far
better delay performance for general network topologies. Our approach hinges
upon utilizing past state information observed by local link and then
constructing a high-order Markov chain for the evolution of the feasible link
schedules. We show in theory and simulation that our proposed algorithm, named
delayed CSMA, adds virtually no additional overhead onto the existing
CSMA-based algorithms, achieves the throughput optimality under the usual
choice of link weight as a function of local queue length, and also provides
much better delay performance by effectively `de-correlating' the link state
process (thus removing link starvation) under any arbitrary network topology.
From our extensive simulations we observe that the delay under our algorithm
can be often reduced by a factor of 20 over a wide range of scenarios, compared
to the standard Glauber-dynamics-based CSMA algorithm.
"
496,"Comprehensive Resource Measurement and Analysis for HPC Systems with
  TACC_Stats","  High-performance computing (HPC) systems are a complex combination of
software, processors, memory, networks, and storage systems characterized by
frequent disruptive technological advances. Anomalous behavior has to be
manually diagnosed and remedied with incomplete and sparse data. It also has
been effort-intensive for users to assess the effectiveness with which they are
using the available resources. The data available for system level analyses
appear from multiple sources and in disparate formats (from Linux ""sysstat"" and
accounting to scheduler/kernel logs). Sysstat does not resolve its measurements
by job so that job-oriented analyses require individual measurements. There are
many user-oriented performance instrumentation and profiling tools but they
require extensive system knowledge, code changes and recompilation, and thus
are not widely used. To address this issue, we develop TACC_Stats, a
job-oriented and logically structured version of the conventional Linux
""sysstat/sar"" system-wide performance monitor. We use TACC_Stats-collected data
from a supercomputer ""Ranger"" to demonstrate its effectiveness in two case
studies.
"
497,"Impact of Pointing Errors on the Performance of Mixed RF/FSO Dual-Hop
  Transmission Systems","  In this work, the performance analysis of a dual-hop relay transmission
system composed of asymmetric radio-frequency (RF)/free-space optical (FSO)
links with pointing errors is presented. More specifically, we build on the
system model presented in [1] to derive new exact closed-form expressions for
the cumulative distribution function, probability density function, moment
generating function, and moments of the end-to-end signal-to-noise ratio in
terms of the Meijer's G function. We then capitalize on these results to offer
new exact closed-form expressions for the higher-order amount of fading,
average error rate for binary and M-ary modulation schemes, and the ergodic
capacity, all in terms of Meijer's G functions. Our new analytical results were
also verified via computer-based Monte-Carlo simulation results.
"
498,Asynchronous MPI for the Masses,"  We present a simple library which equips MPI implementations with truly
asynchronous non-blocking point-to-point operations, and which is independent
of the underlying communication infrastructure. It utilizes the MPI profiling
interface (PMPI) and the MPI_THREAD_MULTIPLE thread compatibility level, and
works with current versions of Intel MPI, Open MPI, MPICH2, MVAPICH2, Cray MPI,
and IBM MPI. We show performance comparisons on a commodity InfiniBand cluster
and two tier-1 systems in Germany, using low-level and application benchmarks.
Issues of thread/process placement and the peculiarities of different MPI
implementations are discussed in detail. We also identify the MPI libraries
that already support asynchronous operations. Finally we show how our ideas can
be extended to MPI-IO.
"
499,Predicting Intermediate Storage Performance for Workflow Applications,"  Configuring a storage system to better serve an application is a challenging
task complicated by a multidimensional, discrete configuration space and the
high cost of space exploration (e.g., by running the application with different
storage configurations). To enable selecting the best configuration in a
reasonable time, we design an end-to-end performance prediction mechanism that
estimates the turn-around time of an application using storage system under a
given configuration. This approach focuses on a generic object-based storage
system design, supports exploring the impact of optimizations targeting
workflow applications (e.g., various data placement schemes) in addition to
other, more traditional, configuration knobs (e.g., stripe size or replication
level), and models the system operation at data-chunk and control message
level.
  This paper presents our experience to date with designing and using this
prediction mechanism. We evaluate this mechanism using micro- as well as
synthetic benchmarks mimicking real workflow applications, and a real
application.. A preliminary evaluation shows that we are on a good track to
meet our objectives: it can scale to model a workflow application run on an
entire cluster while offering an over 200x speedup factor (normalized by
resource) compared to running the actual application, and can achieve, in the
limited number of scenarios we study, a prediction accuracy that enables
identifying the best storage system configuration.
"
500,"Optimal Discriminant Functions Based On Sampled Distribution Distance
  for Modulation Classification","  In this letter, we derive the optimal discriminant functions for modulation
classification based on the sampled distribution distance. The proposed method
classifies various candidate constellations using a low complexity approach
based on the distribution distance at specific testpoints along the cumulative
distribution function. This method, based on the Bayesian decision criteria,
asymptotically provides the minimum classification error possible given a set
of testpoints. Testpoint locations are also optimized to improve classification
performance. The method provides significant gains over existing approaches
that also use the distribution of the signal features.
"
501,"Software model refactoring based on performance analysis: better working
  on software or performance side?","  Several approaches have been introduced in the last few years to tackle the
problem of interpreting model-based performance analysis results and
translating them into architectural feedback. Typically the interpretation can
take place by browsing either the software model or the performance model. In
this paper, we compare two approaches that we have recently introduced for this
goal: one based on the detection and solution of performance antipatterns, and
another one based on bidirectional model transformations between software and
performance models. We apply both approaches to the same example in order to
illustrate the differences in the obtained performance results. Thereafter, we
raise the level of abstraction and we discuss the pros and cons of working on
the software side and on the performance side.
"
502,Blind Estimation of Primary User Traffic Parameters Under Sensing Errors,"  In this work we investigate the bounds on the estimation accuracy of Primary
User (PU) traffic parameters with exponentially distributed busy and idle
times. We derive closed-form expressions for the Cramer-Rao bounds on the mean
squared estimation error for the blind joint estimation of the PU traffic
parameters, specifically, the duty cycle, and the mean arrival and departure
rates. Moreover, we present the corresponding maximum-likelihood estimators for
the traffic parameters. In addition, we derive a modified likelihood function
for the joint estimation of traffic parameters when spectrum sensing errors are
considered, and we present the impact of spectrum sensing errors on the
estimation error via simulations. Finally, we consider a duty cycle estimator,
common in traffic estimation literature, that is based on averaging the traffic
samples. We derive, in closed-form, the mean squared estimation error of the
considered estimator under spectrum sensing errors.
"
503,Approximately Optimal Scheduling of an M/G/1 Queue with Heavy Tails,"  Distributions with a heavy tail are difficult to estimate. If the design of a
scheduling policy is sensitive to the details of heavy tail distributions of
the service times, an approximately optimal solution is difficult to obtain.
This paper shows that the optimal scheduling of an M/G/1 queue with heavy
tailed service times does not present this difficulty and that an approximately
optimal strategy can be derived by truncating the distributions.
"
504,"Queuing Theoretic Analysis of Power-performance Tradeoff in
  Power-efficient Computing","  In this paper we study the power-performance relationship of power-efficient
computing from a queuing theoretic perspective. We investigate the interplay of
several system operations including processing speed, system on/off decisions,
and server farm size. We identify that there are oftentimes ""sweet spots"" in
power-efficient operations: there exist optimal combinations of processing
speed and system settings that maximize power efficiency. For the single server
case, a widely deployed threshold mechanism is studied. We show that there
exist optimal processing speed and threshold value pairs that minimize the
power consumption. This holds for the threshold mechanism with job batching.
For the multi-server case, it is shown that there exist best processing speed
and server farm size combinations.
"
505,"Model-guided Performance Analysis of the Sparse Matrix-Matrix
  Multiplication","  Achieving high efficiency with numerical kernels for sparse matrices is of
utmost importance, since they are part of many simulation codes and tend to use
most of the available compute time and resources. In addition, especially in
large scale simulation frameworks the readability and ease of use of
mathematical expressions are essential components for the continuous
maintenance, modification, and extension of software. In this context, the
sparse matrix-matrix multiplication is of special interest. In this paper we
thoroughly analyze the single-core performance of sparse matrix-matrix
multiplication kernels in the Blaze Smart Expression Template (SET) framework.
We develop simple models for estimating the achievable maximum performance, and
use them to assess the efficiency of our implementations. Additionally, we
compare these kernels with several commonly used SET-based C++ libraries,
which, just as Blaze, aim at combining the requirements of high performance
with an elegant user interface. For the different sparse matrix structures
considered here, we show that our implementations are competitive or faster
than those of the other SET libraries for most problem sizes on a current Intel
multicore processor.
"
506,Stochastic Service Curve and Delay Bound Analysis: A Single Node Case,"  A packet-switched network node with constant capacity (in bps) is considered,
where packets within each flow are served in the first in first out (FIFO)
manner. While this single node system is perhaps the simplest computer
communication system, its stochastic service curve characterization and
independent case analysis in the context of stochastic network calculus
(snetcal) are still basic and many crucial questions surprisingly remain open.
Specifically, when the input is a single flow, what stochastic service curve
and delay bound does the node provide? When the considered flow shares the node
with another flow, what stochastic service curve and delay bound does the node
provide to the considered flow, and if the two flows are independent, can this
independence be made use of and how? The aim of this paper is to provide
answers to these fundamental questions.
"
507,"Statistical Regression to Predict Total Cumulative CPU Usage of
  MapReduce Jobs","  Recently, businesses have started using MapReduce as a popular computation
framework for processing large amount of data, such as spam detection, and
different data mining tasks, in both public and private clouds. Two of the
challenging questions in such environments are (1) choosing suitable values for
MapReduce configuration parameters e.g., number of mappers, number of reducers,
and DFS block size, and (2) predicting the amount of resources that a user
should lease from the service provider. Currently, the tasks of both choosing
configuration parameters and estimating required resources are solely the users
responsibilities. In this paper, we present an approach to provision the total
CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce
job, a profile of total CPU usage in clock cycles is built from the job past
executions with different values of two configuration parameters e.g., number
of mappers, and number of reducers. Then, a polynomial regression is used to
model the relation between these configuration parameters and total CPU usage
in clock cycles of the job. We also briefly study the influence of input data
scaling on measured total CPU usage in clock cycles. This derived model along
with the scaling result can then be used to provision the total CPU usage in
clock cycles of the same jobs with different input data size. We validate the
accuracy of our models using three realistic applications (WordCount, Exim
MainLog parsing, and TeraSort). Results show that the predicted total CPU usage
in clock cycles of generated resource provisioning options are less than 8% of
the measured total CPU usage in clock cycles in our 20-node virtual Hadoop
cluster.
"
508,Sharp Bounds in Stochastic Network Calculus,"  The practicality of the stochastic network calculus (SNC) is often questioned
on grounds of potential looseness of its performance bounds. In this paper it
is uncovered that for bursty arrival processes (specifically Markov-Modulated
On-Off (MMOO)), whose amenability to \textit{per-flow} analysis is typically
proclaimed as a highlight of SNC, the bounds can unfortunately indeed be very
loose (e.g., by several orders of magnitude off). In response to this uncovered
weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving
a general sample-path bound, using martingale based techniques, which
accommodates FIFO, SP, EDF, and GPS scheduling. The obtained (Martingale)
bounds gain an exponential decay factor of ${\mathcal{O}}(e^{-\alpha n})$ in
the number of flows $n$. Moreover, numerical comparisons against simulations
show that the Martingale bounds are remarkably accurate for FIFO, SP, and EDF
scheduling; for GPS scheduling, although the Martingale bounds substantially
improve the Standard bounds, they are numerically loose, demanding for
improvements in the core SNC analysis of GPS.
"
509,Optimization of FASTEST-3D for Modern Multicore Systems,"  FASTEST-3D is an MPI-parallel finite-volume flow solver based on
block-structured meshes that has been developed at the University of
Erlangen-Nuremberg since the early 1990s. It can be used to solve the laminar
or turbulent incompressible Navier-Stokes equations. Up to now its scalability
was strongly limited by a rather rigid communication infrastructure, which led
to a dominance of MPI time already at small process counts.
  This paper describes several optimizations to increase the performance,
scalability, and flexibility of FASTEST-3D. First, a node-level performance
analysis is carried out in order to pinpoint the main bottlenecks and identify
sweet spots for energy-efficient execution. In addition, a single-precision
version of the solver for the linear equation system arising from the
discretization of the governing equations is devised, which significantly
increases the single-core performance. Then the communication mechanisms in
FASTEST-3D are analyzed and a new communication strategy based on non-blocking
calls is implemented. Performance results with the revised version show
significantly increased single-node performance and considerably improved
communication patterns along with much better parallel scalability. In this
context we discuss the concept of ""acceptable parallel efficiency"" and how it
influences the real gain of the optimizations. Scaling measurements are carried
out on a modern petascale system. The obtained improvements are of major
importance for the use of FASTEST-3D on current high-performance computer
clusters and will help to perform simulations with much higher spatial and
temporal resolution to tackle turbulent flow in technical applications.
"
510,"Stochastic Modeling of Large-Scale Solid-State Storage Systems:
  Analysis, Design Tradeoffs and Optimization","  Solid state drives (SSDs) have seen wide deployment in mobiles, desktops, and
data centers due to their high I/O performance and low energy consumption. As
SSDs write data out-of-place, garbage collection (GC) is required to erase and
reclaim space with invalid data. However, GC poses additional writes that
hinder the I/O performance, while SSD blocks can only endure a finite number of
erasures. Thus, there is a performance-durability tradeoff on the design space
of GC. To characterize the optimal tradeoff, this paper formulates an
analytical model that explores the full optimal design space of any GC
algorithm. We first present a stochastic Markov chain model that captures the
I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive
the asymptotic steady-state performance. We further prove the model convergence
and generalize the model for all types of workload. Inspired by this model, we
propose a randomized greedy algorithm (RGA) that can operate along the optimal
tradeoff curve with a tunable parameter. Using trace-driven simulation on
DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to
realize the performance-durability tradeoff.
"
511,"Identifying Compiler Options to Minimise Energy Consumption for Embedded
  Platforms","  This paper presents an analysis of the energy consumption of an extensive
number of the optimisations a modern compiler can perform. Using GCC as a test
case, we evaluate a set of ten carefully selected benchmarks for five different
embedded platforms.
  A fractional factorial design is used to systematically explore the large
optimisation space (2^82 possible combinations), whilst still accurately
determining the effects of optimisations and optimisation combinations.
Hardware power measurements on each platform are taken to ensure all
architectural effects on the energy consumption are captured.
  We show that fractional factorial design can find more optimal combinations
than relying on built in compiler settings. We explore the relationship between
run-time and energy consumption, and identify scenarios where they are and are
not correlated.
  A further conclusion of this study is the structure of the benchmark has a
larger effect than the hardware architecture on whether the optimisation will
be effective, and that no single optimisation is universally beneficial for
execution time or energy consumption.
"
512,Towards a Workload for Evolutionary Analytics,"  Emerging data analysis involves the ingestion and exploration of new data
sets, application of complex functions, and frequent query revisions based on
observing prior query answers. We call this new type of analysis evolutionary
analytics and identify its properties. This type of analysis is not well
represented by current benchmark workloads. In this paper, we present a
workload and identify several metrics to test system support for evolutionary
analytics. Along with our metrics, we present methodologies for running the
workload that capture this analytical scenario.
"
513,Stochastic Analysis on RAID Reliability for Solid-State Drives,"  Solid-state drives (SSDs) have been widely deployed in desktops and data
centers. However, SSDs suffer from bit errors, and the bit error rate is time
dependent since it increases as an SSD wears down. Traditional storage systems
mainly use parity-based RAID to provide reliability guarantees by striping
redundancy across multiple devices, but the effectiveness of RAID in SSDs
remains debatable as parity updates aggravate the wearing and bit error rates
of SSDs. In particular, an open problem is that how different parity
distributions over multiple devices, such as the even distribution suggested by
conventional wisdom, or uneven distributions proposed in recent RAID schemes
for SSDs, may influence the reliability of an SSD RAID array. To address this
fundamental problem, we propose the first analytical model to quantify the
reliability dynamics of an SSD RAID array. Specifically, we develop a
""non-homogeneous"" continuous time Markov chain model, and derive the transient
reliability solution. We validate our model via trace-driven simulations and
conduct numerical analysis to provide insights into the reliability dynamics of
SSD RAID arrays under different parity distributions and subject to different
bit error rates and array configurations. Designers can use our model to decide
the appropriate parity distribution based on their reliability requirements.
"
514,"A Performance Comparison of Different Graphics Processing Units Running
  Direct N-Body Simulations","  Hybrid computational architectures based on the joint power of Central
Processing Units and Graphic Processing Units (GPUs) are becoming popular and
powerful hardware tools for a wide range of simulations in biology, chemistry,
engineering, physics, etc..
  In this paper we present a comparison of performance of various GPUs
available on market when applied to the numerical integration of the classic,
gravitational, N-body problem. To do this, we developed an OpenCL version of
the parallel code (HiGPUs) to use for these tests, because this version is the
only apt to work on GPUs of different makes.
  The main general result is that we confirm the reliability, speed and
cheapness of GPUs when applied to the examined kind of problems (i.e. when the
forces to evaluate are dependent on the mutual distances, as it happens in
gravitational physics and molecular dynamics). More specifically, we find that
also the cheap GPUs built to be employed just for gaming applications are very
performant in terms of computing speed also in scientific applications and,
although with some limitations in central memory and in bandwidth, can be a
good choice to implement a machine for scientific use at a very good
performance to cost ratio.
"
515,Throughput Optimal Scheduling Policies in Networks of Interacting Queues,"  This report considers a fairly general model of constrained queuing networks
that allows us to represent both MMBP (Markov Modulated Bernoulli Processes)
arrivals and time-varying service constraints. We derive a set of sufficient
conditions for throughput optimality of scheduling policies that encompass and
generalize all the previously obtained results in the field. This leads to the
definition of new classes of (non diagonal) throughput optimal scheduling
policies. We prove the stability of queues by extending the traditional
Lyapunov drift criteria methodology.
"
516,"A Taxonomy of Performance Assurance Methodologies and its Application in
  High Performance Computer Architectures","  This paper presents a systematic approach to the complex problem of high
confidence performance assurance of high performance architectures based on
methods used over several generations of industrial microprocessors. A taxonomy
is presented for performance assurance through three key stages of a product
life cycle-high level performance, RTL performance, and silicon performance.
The proposed taxonomy includes two components-independent performance assurance
space for each stage and a correlation performance assurance space between
stages. It provides a detailed insight into the performance assurance space in
terms of coverage provided taking into account capabilities and limitations of
tools and methodologies used at each stage. An application of the taxonomy to
cases described in the literature and to high performance Intel architectures
is shown. The proposed work should be of interest to manufacturers of high
performance microprocessor/chipset architectures and has not been discussed in
the literature.
"
517,Multithreaded Input-Sensitive Profiling,"  Input-sensitive profiling is a recent performance analysis technique that
makes it possible to estimate the empirical cost function of individual
routines of a program, helping developers understand how performance scales to
larger inputs and pinpoint asymptotic bottlenecks in the code. A current
limitation of input-sensitive profilers is that they specifically target
sequential computations, ignoring any communication between threads. In this
paper we show how to overcome this limitation, extending the range of
applicability of the original approach to multithreaded applications and to
applications that operate on I/O streams. We develop new metrics for
automatically estimating the size of the input given to each routine
activation, addressing input produced by non-deterministic memory stores
performed by other threads as well as by the OS kernel (e.g., in response to
I/O or network operations). We provide real case studies, showing that our
extension allows it to characterize the behavior of complex applications more
precisely than previous approaches. An extensive experimental investigation on
a variety of benchmark suites (including the SPEC OMP2012 and the PARSEC
benchmarks) shows that our Valgrind-based input-sensitive profiler incurs an
overhead comparable to other prominent heavyweight analysis tools, while
collecting significantly more performance points from each profiling session
and correctly characterizing both thread-induced and external input.
"
518,"Investigating Randomly Generated Adjacency Matrices For Their Use In
  Modeling Wireless Topologies","  Generation of realistic topologies plays an important role in determining the
accuracy and validity of simulation studies. This study presents a discussion
to justify why, and how often randomly generated adjacency matrices may not not
conform to wireless topologies in the physical world. Specifically, it shows
through analysis and random trials that, more than 90% of times, a randomly
generated adjacency matrix will not conform to a valid wireless topology, when
it has more than 3 nodes. By showing that node triplets in the adjacency graph
need to adhere to rules of a geometric vector space, the study shows that the
number of randomly chosen node triplets failing consistency checks grow at the
order of O(base^3), where base is the granularity of the distance metric.
Further, the study models and presents a probability estimate with which any
randomly generated adjacency matrix would fail realization. This information
could be used to design simpler algorithms for generating k-connected wireless
topologies.
"
519,Ball-Larus Path Profiling Across Multiple Loop iterations,"  Identifying the hottest paths in the control flow graph of a routine can
direct optimizations to portions of the code where most resources are consumed.
This powerful methodology, called path profiling, was introduced by Ball and
Larus in the mid 90s and has received considerable attention in the last 15
years for its practical relevance. A shortcoming of Ball-Larus path profiling
was the inability to profile cyclic paths, making it difficult to mine
interesting execution patterns that span multiple loop iterations. Previous
results, based on rather complex algorithms, have attempted to circumvent this
limitation at the price of significant performance losses already for a small
number of iterations. In this paper, we present a new approach to multiple
iterations path profiling, based on data structures built on top of the
original Ball-Larus numbering technique. Our approach allows it to profile all
executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths,
where k is a user-defined parameter. An extensive experimental investigation on
a large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,
our approach can be even faster than Ball-Larus due to fewer operations on
smaller hash tables, producing compact representations of cyclic paths even for
large values of k.
"
520,"""Superluminal"" FITS File Processing on Multiprocessors: Zero Time Endian
  Conversion Technique","  The FITS is the standard file format in astronomy, and it has been extended
to agree with astronomical needs of the day. However, astronomical datasets
have been inflating year by year. In case of ALMA telescope, a ~ TB scale
4-dimensional data cube may be produced for one target. Considering that
typical Internet bandwidth is a few 10 MB/s at most, the original data cubes in
FITS format are hosted on a VO server, and the region which a user is
interested in should be cut out and transferred to the user (Eguchi et al.,
2012). The system will equip a very high-speed disk array to process a TB scale
data cube in a few 10 seconds, and disk I/O speed, endian conversion and data
processing one will be comparable. Hence to reduce the endian conversion time
is one of issues to realize our system. In this paper, I introduce a technique
named ""just-in-time endian conversion"", which delays the endian conversion for
each pixel just before it is really needed, to sweep out the endian conversion
time; by applying this method, the FITS processing speed increases 20% for
single threading, and 40% for multi-threading compared to CFITSIO. The speed-up
by the method tightly relates to modern CPU architecture to improve the
efficiency of instruction pipelines due to break of ""causality"", a programmed
instruction code sequence.
"
521,Paging with dynamic memory capacity,"  We study a generalization of the classic paging problem that allows the
amount of available memory to vary over time - capturing a fundamental property
of many modern computing realities, from cloud computing to multi-core and
energy-optimized processors. It turns out that good performance in the
""classic"" case provides no performance guarantees when memory capacity
fluctuates: roughly speaking, moving from static to dynamic capacity can mean
the difference between optimality within a factor 2 in space and time, and
suboptimality by an arbitrarily large factor. More precisely, adopting the
competitive analysis framework, we show that some online paging algorithms,
despite having an optimal (h,k)-competitive ratio when capacity remains
constant, are not (3,k)-competitive for any arbitrarily large k in the presence
of minimal capacity fluctuations. In this light it is surprising that several
classic paging algorithms perform remarkably well even if memory capacity
changes adversarially - even without taking those changes into explicit
account! In particular, we prove that LFD still achieves the minimum number of
faults, and that several classic online algorithms such as LRU have a ""dynamic""
(h,k)-competitive ratio that is the best one can achieve without knowledge of
future page requests, even if one had perfect knowledge of future capacity
fluctuations (an exact characterization of this ratio shows it is almost,
albeit not quite, equal to the ""classic"" ratio k/(k-h+1)). In other words, with
careful management, knowing/predicting future memory resources appears far less
crucial to performance than knowing/predicting future data accesses.
"
522,"Chip-level and multi-node analysis of energy-optimized lattice-Boltzmann
  CFD simulations","  Memory-bound algorithms show complex performance and energy consumption
behavior on multicore processors. We choose the lattice-Boltzmann method (LBM)
on an Intel Sandy Bridge cluster as a prototype scenario to investigate if and
how single-chip performance and power characteristics can be generalized to the
highly parallel case. First we perform an analysis of a sparse-lattice LBM
implementation for complex geometries. Using a single-core performance model,
we predict the intra-chip saturation characteristics and the optimal operating
point in terms of energy to solution as a function of implementation details,
clock frequency, vectorization, and number of active cores per chip. We show
that high single-core performance and a correct choice of the number of active
cores per chip are the essential optimizations for lowest energy to solution at
minimal performance degradation. Then we extrapolate to the MPI-parallel level
and quantify the energy-saving potential of various optimizations and execution
modes, where we find these guidelines to be even more important, especially
when communication overhead is non-negligible. In our setup we could achieve
energy savings of 35% in this case, compared to a naive approach. We also
demonstrate that a simple non-reflective reduction of the clock speed leaves
most of the energy saving potential unused.
"
523,"Performance of a Multiple-Access DCSK-CC System over Nakagami-$m$ Fading
  Channels","  In this paper, we propose a novel cooperative scheme to enhance the
performance of multiple-access (MA) differential-chaos-shift-keying (DCSK)
systems. We provide the bit-error-rate (BER) performance and throughput
analyses for the new system with a decode-and-forward (DF) protocol over
Nakagami-$m$ fading channels. Our simulated results not only show that this
system significantly improves the BER performance as compared to the existing
DCSK non-cooperative (DCSK-NC) system and the multiple-input multiple-output
DCSK (MIMO-DCSK) system, but also verify the theoretical analyses. Furthermore,
we show that the throughput of this system approximately equals that of the
DCSK-NC system, both of which have prominent improvements over the MIMO-DCSK
system. We thus believe that the proposed system can be a good framework for
chaos-modulation-based wireless communications.
"
524,"Real-Time Welfare-Maximizing Regulation Allocation in Dynamic
  Aggregator-EVs System","  The concept of vehicle-to-grid (V2G) has gained recent interest as more and
more electric vehicles (EVs) are put to use. In this paper, we consider a
dynamic aggregator-EVs system, where an aggregator centrally coordinates a
large number of dynamic EVs to perform regulation service. We propose a
Welfare-Maximizing Regulation Allocation (WMRA) algorithm for the aggregator to
fairly allocate the regulation amount among its EVs. Compared to previous
works, WMRA accommodates a wide spectrum of vital system characteristics,
including dynamics of EV, limited EV battery size, EV battery degradation cost,
and the cost of using external energy sources for the aggregator. The algorithm
operates in real time and does not require any prior knowledge of the
statistical information of the system. Theoretically, we demonstrate that WMRA
is away from the optimum by O(1/V), where V is a controlling parameter
depending on EV's battery size. In addition, our simulation results indicate
that WMRA can substantially outperform a suboptimal greedy algorithm.
"
525,Stationary analysis of the Shortest Queue First service policy,"  We analyze the so-called Shortest Queue First (SQF) queueing discipline
whereby a unique server addresses queues in parallel by serving at any time
that queue with the smallest workload. Considering a stationary system composed
of two parallel queues and assuming Poisson arrivals and general service time
distributions, we first establish the functional equations satisfied by the
Laplace transforms of the workloads in each queue. We further specialize these
equations to the so-called ""symmetric case"", with same arrival rates and
identical exponential service time distributions at each queue; we then obtain
a functional equation $$ M(z) = q(z) \cdot M \circ h(z) + L(z) $$ for unknown
function $M$, where given functions $q$, $L$ and $h$ are related to one branch
of a cubic polynomial equation. We study the analyticity domain of function $M$
and express it by a series expansion involving all iterates of function $h$.
This allows us to determine empty queue probabilities along with the tail of
the workload distribution in each queue. This tail appears to be identical to
that of the Head-of-Line preemptive priority system, which is the key feature
desired for the SQF discipline.
"
526,"Stationary analysis of the ""Shortest Queue First"" service policy: the
  asymmetric case","  As a follow-up to a recent paper considering two symmetric queues, the
\textit{Shortest Queue First} service discipline is presently analysed for two
general asymmetric queues. Using the results previously established and
assuming exponentially distributed service times, the bivariate Laplace
transform of workloads in each queue is shown to depend on the solution
$\mathbf{M}$ to a two-dimensional functional equation $$ \mathbf{M} = Q_1 \cdot
\mathbf{M}\circ h_1 + Q_2 \cdot \mathbf{M}\circ h_2 + \mathbf{L} $$ with given
matrices $Q_1$, $Q_2$ and vector $\mathbf{L}$ and where functions $h_1$ and
$h_2$ are defined each on some rational curve; solution $\mathbf{M}$ can then
represented by a series expansion involving the semi-group $< h_1, h_2 >$
generated by these two functions. The empty queue probabilities along with the
tail behaviour of the workload distribution at each queue are characterised.
"
527,Analysis of a non-work conserving Generalized Processor Sharing queue,"  We consider in this paper a non work-conserving Generalized Processor Sharing
(GPS) system composed of two queues with Poisson arrivals and exponential
service times. Using general results due to Fayolle \emph{et al}, we first
establish the stability condition for this system. We then determine the
functional equation satisfied by the generating function of the numbers of jobs
in both queues and the associated Riemann-Hilbert problem. We prove the
existence and the uniqueness of the solution. This allows us to completely
characterize the system, in particular to compute the empty queue probability.
We finally derive the tail asymptotics of the number of jobs in one queue.
"
528,Search in Random Media with L\'evy Flights,"  We review some of our work regarding search which has been motivated by a
variety of applications in engineering and technology, including traffic
routing and security in communication networks, explosive mine detection and
removal, tactical operations as well as emergency management. We develop the
basic mathematical model representing N searchers that proceed independently,
and which can be affected by destruction or loss, and time-outs. An approach
based on Laplace transform is then developed for the case where the success of
the search requires that k out of the N searchers be successful, and we
estimate the amount of energy expended and the number of searchers that are
required to conduct a successful search in time B, provided B is large.
Finally, we present an iterative numerical solution approach that allows us to
analyse the search time and the energy needed to find an object when the search
space is non-homogeneous.
"
529,"On the Delay-Storage Trade-off in Content Download from Coded
  Distributed Storage Systems","  In this paper we study how coding in distributed storage reduces expected
download time, in addition to providing reliability against disk failures. The
expected download time is reduced because when a content file is encoded to add
redundancy and distributed across multiple disks, reading only a subset of the
disks is sufficient to reconstruct the content. For the same total storage
used, coding exploits the diversity in storage better than simple replication,
and hence gives faster download. We use a novel fork-join queuing framework to
model multiple users requesting the content simultaneously, and derive bounds
on the expected download time. Our system model and results are a novel
generalization of the fork-join system that is studied in queueing theory
literature. Our results demonstrate the fundamental trade-off between the
expected download time and the amount of storage space. This trade-off can be
used for design of the amount of redundancy required to meet the delay
constraints on content delivery.
"
530,"On the importance of nonlinear modeling in computer performance
  prediction","  Computers are nonlinear dynamical systems that exhibit complex and sometimes
even chaotic behavior. The models used in the computer systems community,
however, are linear. This paper is an exploration of that disconnect: when
linear models are adequate for predicting computer performance and when they
are not. Specifically, we build linear and nonlinear models of the processor
load of an Intel i7-based computer as it executes a range of different
programs. We then use those models to predict the processor loads forward in
time and compare those forecasts to the true continuations of the time series
"
531,"Determinism, Complexity, and Predictability in Computer Performance","  Computers are deterministic dynamical systems (CHAOS 19:033124, 2009). Among
other things, that implies that one should be able to use deterministic
forecast rules to predict their behavior. That statement is sometimes-but not
always-true. The memory and processor loads of some simple programs are easy to
predict, for example, but those of more-complex programs like compilers are
not. The goal of this paper is to determine why that is the case. We conjecture
that, in practice, complexity can effectively overwhelm the predictive power of
deterministic forecast models. To explore that, we build models of a number of
performance traces from different programs running on different Intel-based
computers. We then calculate the permutation entropy-a temporal entropy metric
that uses ordinal analysis-of those traces and correlate those values against
the prediction success
"
532,"A class of equivalent idle-time-order-based routing policies for
  heterogeneous multi-server systems","  We consider an M/M/N/K/FCFS system (N>0, K>=N), where the servers operate at
(possibly) heterogeneous service rates. In this situation, the steady state
behavior depends on the routing policy that is used to select which idle server
serves the next job in queue. We define a class of idle-time-order-based
policies (including, for example, Longest Idle Server First (LISF)) and show
that all policies in this class result in the same steady state behavior. In
particular, they are all equivalent to the naive Random routing policy.
"
533,HTTPI Based Web Service Security over SOAP,"  Now a days, a new family of web applications open applications, are emerging
(e.g., Social Networking, News and Blogging). Generally, these open
applications are non-confidential. The security needs of these applications are
only client/server authentication and data integrity. For securing these open
applications, effectively and efficiently, HTTPI, a new transport protocol is
proposed, which ensures the entire security requirements of open applications.
Benefit of using the HTTPI is that it is economical in use, well-suited for
cache proxies, like HTTP is, and provides security against many Internet
attacks (Server Impersonation and Message Modification) like HTTPS does. In
terms of performance HTTPI is very close to the HTTP, but much better than
HTTPS. A Web service is a method of communication between two ends over the
Internet. These web services are developed over XML and HTTP. Today, most of
the open applications use web services for most of their operations. For
securing these web services, security design based on HTTPI is proposed. Our
work involves securing the web services over SOAP, based on the HTTPI. This
secure web service might be applicable for open applications, where
authentication and integrity is needed, but no confidentiality required. In our
paper, we introduce a web service security model based on HTTPI protocol over
SOAP and develop a preliminary implementation of this model. We also analyze
the performance of our approach through an experiment and show that our
proposed approach provides higher throughput, lower average response time and
lower response size than HTTPS based web service security approach.
"
534,"Proceedings 11th International Workshop on Quantitative Aspects of
  Programming Languages and Systems","  Quantitative aspects of computation are important and sometimes essential in
characterising the behavior and determining the properties of systems. They are
related to the use of physical quantities (storage space, time, bandwidth,
etc.) as well as mathematical quantities (e.g. probability and measures for
reliability, security and trust). Such quantities play a central role in
defining both the model of systems (architecture, language design, semantics)
and the methodologies and tools for the analysis and verification of system
properties. The aim of this workshop is to discuss the explicit use of
quantitative information such as time and probabilities either directly in the
model or as a tool for the analysis of systems.
"
535,"Ber Performance Analysis of WiMAX PHY Layer under different channel
  conditions","  This paper gives an introduction on the IEEE 802.16 standard WIMAX or
Worldwide Interoperability for Microwave Access. The different parts give
details on the architectural specifications of WiMAX networks and also on the
working principle of WiMAX networks including its services provided. It also
provides brief descriptions on its salient features of this technology and how
it benefits the networking industry. A brief outline of the basic building
blocks or equipment of WiMAX architecture is also provided. This paper also
evaluates the simulation performance of IEEE 802.16 OFDM PHY layer. The
Stanford University Interim (SUI) channel model under varying parameters is
selected for the wireless channel in the simulation. The performance
measurements and analysis was done in simulation developed in MATLAB.
"
536,"The Effect of Communication and Synchronization on Amdahl Law in
  Multicore Systems","  This work analyses the effects of sequential-to-parallel synchronization and
inter-core communication on multicore performance, speedup and scaling. A
modification of Amdahl law is formulated, to reflect the finding that parallel
speedup is lower than originally predicted, due to these effects. In
applications with high inter-core communication requirements, the workload
should be executed on a small number of cores, and applications of high
sequential-to-parallel synchronization requirements may better be executed by
the sequential core, even when f, the Amdahl fraction of parallelization, is
very close to 1. To improve the scalability and performance speedup of a
multicore, it is as important to address the synchronization and connectivity
intensities of parallel algorithms as their parallelization factor.
"
537,A Simple Policy for Multiple Queues with Size-Independent Service Times,"  We consider a service system with two Poisson arrival queues. A server
chooses which queue to serve at each moment. Once a queue is served, all the
customers will be served within a fixed amount of time. This model is useful in
studying airport shuttling or certain online computing systems. We propose a
simple yet optimal state-independent policy for this problem which is not only
easy to implement, but also performs very well.
"
538,"Performance Bounds for Multiclass FIFO in Communication Networks: A
  Deterministic Case","  Multiclass FIFO is used in communication networks such as in input-queueing
routers/switches and in wireless networks. For the concern of providing service
guarantees in such networks, it is crucial to have analytical results, e.g.
bounds, on the performance of multi-class FIFO. Surprisingly, there are few
such results in the literature. This paper is devoted to filling the gap.
Specifically, a single hop deterministic case is studied, for which, delay and
backlog bounds are derived, in addition to guaranteed rate and service curve
characterizations that may be exploited to extend the analysis to network
cases.
"
539,Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things,"  Numerous energy harvesting wireless devices that will serve as building
blocks for the Internet of Things (IoT) are currently under development.
However, there is still only limited understanding of the properties of various
energy sources and their impact on energy harvesting adaptive algorithms.
Hence, we focus on characterizing the kinetic (motion) energy that can be
harvested by a wireless node with an IoT form factor and on developing energy
allocation algorithms for such nodes. In this paper, we describe methods for
estimating harvested energy from acceleration traces. To characterize the
energy availability associated with specific human activities (e.g., relaxing,
walking, cycling), we analyze a motion dataset with over 40 participants. Based
on acceleration measurements that we collected for over 200 hours, we study
energy generation processes associated with day-long human routines. We also
briefly summarize our experiments with moving objects. We develop energy
allocation algorithms that take into account practical IoT node design
considerations, and evaluate the algorithms using the collected measurements.
Our observations provide insights into the design of motion energy harvesters,
IoT nodes, and energy harvesting adaptive algorithms.
"
540,"Toward a Unified Performance and Power Consumption NAND Flash Memory
  Model of Embedded and Solid State Secondary Storage Systems","  This paper presents a set of models dedicated to describe a flash storage
subsystem structure, functions, performance and power consumption behaviors.
These models cover a large range of today's NAND flash memory applications.
They are designed to be implemented in simulation tools allowing to estimate
and compare performance and power consumption of I/O requests on flash memory
based storage systems. Such tools can also help in designing and validating new
flash storage systems and management mechanisms. This work is integrated in a
global project aiming to build a framework simulating complex flash storage
hierarchies for performance and power consumption analysis. This tool will be
highly configurable and modular with various levels of usage complexity
according to the required aim: from a software user point of view for
simulating storage systems, to a developer point of view for designing, testing
and validating new flash storage management systems.
"
541,"""The tail wags the dog"": A study of anomaly detection in commercial
  application performance","  The IT industry needs systems management models that leverage available
application information to detect quality of service, scalability and health of
service. Ideally this technique would be common for varying application types
with different n-tier architectures under normal production conditions of
varying load, user session traffic, transaction type, transaction mix, and
hosting environment.
  This paper shows that a whole of service measurement paradigm utilizing a
black box M/M/1 queuing model and auto regression curve fitting of the
associated CDF are an accurate model to characterize system performance
signatures. This modeling method is also used to detect application slow down
events. The technique was shown to work for a diverse range of workloads
ranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on
customizations specific to the n-tier architecture of the systems being
analyzed and so the performance anomaly detection technique was shown to be
platform and configuration agnostic.
"
542,"Dynamic Partial Cooperative MIMO System for Delay-Sensitive Applications
  with Limited Backhaul Capacity","  Considering backhaul consumption in practical systems, it may not be the best
choice to engage all the time in full cooperative MIMO for interference
mitigation. In this paper, we propose a novel downlink partial cooperative MIMO
(Pco-MIMO) physical layer (PHY) scheme, which allows flexible tradeoff between
the partial data cooperation level and the backhaul consumption. Based on this
Pco-MIMO scheme, we consider dynamic transmit power and rate allocation
according to the imperfect channel state information at transmitters (CSIT) and
the queue state information (QSI) to minimize the average delay cost subject to
average backhaul consumption constraints and average power constraints. The
delay-optimal control problem is formulated as an infinite horizon average cost
constrained partially observed Markov decision process (CPOMDP). By exploiting
the special structure in our problem, we derive an equivalent Bellman Equation
to solve the CPOMDP. To reduce computational complexity and facilitate
distributed implementation, we propose a distributed online learning algorithm
to estimate the per-flow potential functions and Lagrange multipliers (LMs) and
a distributed online stochastic partial gradient algorithm to obtain the power
and rate control policy. The proposed low-complexity distributed solution is
based on local observations of the system states at the BSs and is very robust
against model variations. We also prove the convergence and the asymptotic
optimality of the proposed solution.
"
543,Measuring the Optimality of Hadoop Optimization,"  In recent years, much research has focused on how to optimize Hadoop jobs.
Their approaches are diverse, ranging from improving HDFS and Hadoop job
scheduler to optimizing parameters in Hadoop configurations. Despite their
success in improving the performance of Hadoop jobs, however, very little is
known about the limit of their optimization performance. That is, how optimal
is a given Hadoop optimization? When a Hadoop optimization method X improves
the performance of a job by Y %, how do we know if this improvement is as good
as it can be? To answer this question, in this paper, we first examine the
ideal best case, the lower bound, of running time for Hadoop jobs and develop a
measure to accurately estimate how optimal a given Hadoop optimization is with
respect to the lower bound. Then, we demonstrate how one may exploit the
proposed measure to improve the optimization of Hadoop jobs.
"
544,A Performance Comparison of Network Simulators for Wireless Networks,"  Network simulation is the most useful and common methodology used to evaluate
different network to-pologies without real world implementation. Network
simulators are widely used by the research community to evaluate new theories
and hypotheses. There are a number of network simulators, for instance, ns-2,
ns-3, OMNET++, SWAN, OPNET, Jist, and GloMoSiM etc. Therefore, the selection of
a network simulator for evaluating research work is a crucial task for
researchers. The main focus of this paper is to compare the state-of-the-art,
open source network simulators based on the following parameters: CPU
utilization, memory usage, computational time, and scalability by simulating a
MANET routing protocol, to identify an optimal network simulator for the
research community.
"
545,Spatial Fluid Limits for Stochastic Mobile Networks,"  We consider Markov models of large-scale networks where nodes are
characterized by their local behavior and by a mobility model over a
two-dimensional lattice. By assuming random walk, we prove convergence to a
system of partial differential equations (PDEs) whose size depends neither on
the lattice size nor on the population of nodes. This provides a macroscopic
view of the model which approximates discrete stochastic movements with
continuous deterministic diffusions. We illustrate the practical applicability
of this result by modeling a network of mobile nodes with on/off behavior
performing file transfers with connectivity to 802.11 access points. By means
of an empirical validation against discrete-event simulation we show high
quality of the PDE approximation even for low populations and coarse lattices.
In addition, we confirm the computational advantage in using the PDE limit over
a traditional ordinary differential equation limit where the lattice is modeled
discretely, yielding speed-ups of up to two orders of magnitude.
"
546,Smart Streaming for Online Video Services,"  Bandwidth consumption is a significant concern for online video service
providers. Practical video streaming systems usually use some form of HTTP
streaming (progressive download) to let users download the video at a faster
rate than the video bitrate. Since users may quit before viewing the complete
video, however, much of the downloaded video will be ""wasted"". To the extent
that users' departure behavior can be predicted, we develop smart streaming
that can be used to improve user QoE with limited server bandwidth or save
bandwidth cost with unlimited server bandwidth. Through measurement, we extract
certain user behavior properties for implementing such smart streaming, and
demonstrate its advantage using prototype implementation as well as
simulations.
"
547,"Effect of Spatial Interference Correlation on the Performance of Maximum
  Ratio Combining","  While the performance of maximum ratio combining (MRC) is well understood for
a single isolated link, the same is not true in the presence of interference,
which is typically correlated across antennas due to the common locations of
interferers. For tractability, prior work focuses on the two extreme cases
where the interference power across antennas is either assumed to be fully
correlated or fully uncorrelated. In this paper, we address this shortcoming
and characterize the performance of MRC in the presence of spatially-correlated
interference across antennas. Modeling the interference field as a Poisson
point process, we derive the exact distribution of the signal-to-interference
ratio (SIR) for the case of two receive antennas, and upper and lower bounds
for the general case. Using these results, we study the diversity behavior of
MRC and characterize the critical density of simultaneous transmissions for a
given outage constraint. The exact SIR distribution is also useful in
benchmarking simpler correlation models. We show that the full-correlation
assumption is considerably pessimistic (up to 30% higher outage probability for
typical values) and the no-correlation assumption is significantly optimistic
compared to the true performance.
"
548,A unified approach to the performance analysis of caching systems,"  We propose a unified methodology to analyse the performance of caches (both
isolated and interconnected), by extending and generalizing a decoupling
technique originally known as Che's approximation, which provides very accurate
results at low computational cost. We consider several caching policies, taking
into account the effects of temporal locality. In the case of interconnected
caches, our approach allows us to do better than the Poisson approximation
commonly adopted in prior work. Our results, validated against simulations and
trace-driven experiments, provide interesting insights into the performance of
caching systems.
"
549,"Performance study and simulation of an anycast protocol for wireless
  mobile ad hoc networks","  This paper conducts a detailed simulation study of stateless anycast routing
in a mobile wireless ad hoc network. The model covers all the fundamental
aspects of such networks with a routing mechanism using a scheme of
orientation-dependent inter-node communication links. The simulation system
Winsim is used which explicitly represents parallelism of events and processes
in the network. The purpose of these simulations is to investigate the effect
of node s maximum speed, and different TTL over the network performance under
two different scenarios. Simulation study investigates five practically
important performance metrics of a wireless mobile ad hoc network and shows the
dependence of this metrics on the transmission radius, link availability, and
maximal possible node speed.
"
550,"On the Catalyzing Effect of Randomness on the Per-Flow Throughput in
  Wireless Networks","  This paper investigates the throughput capacity of a flow crossing a
multi-hop wireless network, whose geometry is characterized by general
randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both
the nodes' densities and the number of hops. The key contribution is to
demonstrate \textit{how} the \textit{per-flow throughput} depends on the
distribution of 1) the number of nodes $N_j$ inside hops' interference sets, 2)
the number of hops $K$, and 3) the degree of spatial correlations. The
randomness in both $N_j$'s and $K$ is advantageous, i.e., it can yield larger
scalings (as large as $\Theta(n)$) than in non-random settings. An interesting
consequence is that the per-flow capacity can exhibit the opposite behavior to
the network capacity, which was shown to suffer from a logarithmic decrease in
the presence of randomness. In turn, spatial correlations along the end-to-end
path are detrimental by a logarithmic term.
"
551,"RepFlow: Minimizing Flow Completion Times with Replicated Flows in Data
  Centers","  Short TCP flows that are critical for many interactive applications in data
centers are plagued by large flows and head-of-line blocking in switches.
Hash-based load balancing schemes such as ECMP aggravate the matter and result
in long-tailed flow completion times (FCT). Previous work on reducing FCT
usually requires custom switch hardware and/or protocol changes. We propose
RepFlow, a simple yet practically effective approach that replicates each short
flow to reduce the completion times, without any change to switches or host
kernels. With ECMP the original and replicated flows traverse distinct paths
with different congestion levels, thereby reducing the probability of having
long queueing delay. We develop a simple analytical model to demonstrate the
potential improvement of RepFlow. Extensive NS-3 simulations and Mininet
implementation show that RepFlow provides 50%--70% speedup in both mean and
99-th percentile FCT for all loads, and offers near-optimal FCT when used with
DCTCP.
"
552,"Towards a System Theoretic Approach to Wireless Network Capacity in
  Finite Time and Space","  In asymptotic regimes, both in time and space (network size), the derivation
of network capacity results is grossly simplified by brushing aside queueing
behavior in non-Jackson networks. This simplifying double-limit model, however,
lends itself to conservative numerical results in finite regimes. To properly
account for queueing behavior beyond a simple calculus based on average rates,
we advocate a system theoretic methodology for the capacity problem in finite
time and space regimes. This methodology also accounts for spatial correlations
arising in networks with CSMA/CA scheduling and it delivers rigorous
closed-form capacity results in terms of probability distributions. Unlike
numerous existing asymptotic results, subject to anecdotal practical concerns,
our transient one can be used in practical settings: for example, to compute
the time scales at which multi-hop routing is more advantageous than single-hop
routing.
"
553,"The Implications of Diverse Applications and Scalable Data Sets in
  Benchmarking Big Data Systems","  Now we live in an era of big data, and big data applications are becoming
more and more pervasive. How to benchmark data center computer systems running
big data applications (in short big data systems) is a hot topic. In this
paper, we focus on measuring the performance impacts of diverse applications
and scalable volumes of data sets on big data systems. For four typical data
analysis applications---an important class of big data applications, we find
two major results through experiments: first, the data scale has a significant
impact on the performance of big data systems, so we must provide scalable
volumes of data sets in big data benchmarks. Second, for the four applications,
even all of them use the simple algorithms, the performance trends are
different with increasing data scales, and hence we must consider not only
variety of data sets but also variety of applications in benchmarking big data
systems.
"
554,Characterizing Data Analysis Workloads in Data Centers,"  As the amount of data explodes rapidly, more and more corporations are using
data centers to make effective decisions and gain a competitive edge. Data
analysis applications play a significant role in data centers, and hence it has
became increasingly important to understand their behaviors in order to further
improve the performance of data center computer systems. In this paper, after
investigating three most important application domains in terms of page views
and daily visitors, we choose eleven representative data analysis workloads and
characterize their micro-architectural characteristics by using hardware
performance counters, in order to understand the impacts and implications of
data analysis workloads on the systems equipped with modern superscalar
out-of-order processors. Our study on the workloads reveals that data analysis
applications share many inherent characteristics, which place them in a
different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,
including traditional server workloads (SPECweb2005) and scale-out service
workloads (four among six benchmarks in CloudSuite), and accordingly we give
several recommendations for architecture and system optimizations. On the basis
of our workload characterization work, we released a benchmark suite named
DCBench for typical datacenter workloads, including data analysis and service
workloads, with an open-source license on our project home page on
http://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing
architecture and small-to-medium scale system researches for datacenter
computing.
"
555,"TOFEC: Achieving Optimal Throughput-Delay Trade-off of Cloud Storage
  Using Erasure Codes","  Our paper presents solutions using erasure coding, parallel connections to
storage cloud and limited chunking (i.e., dividing the object into a few
smaller segments) together to significantly improve the delay performance of
uploading and downloading data in and out of cloud storage.
  TOFEC is a strategy that helps front-end proxy adapt to level of workload by
treating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring
admission control. Under light workloads, TOFEC creates more smaller chunks and
uses more parallel connections per file, minimizing service delay. Under heavy
workloads, TOFEC automatically reduces the level of chunking (fewer chunks with
increased size) and uses fewer parallel connections to reduce overhead,
resulting in higher throughput and preventing queueing delay. Our trace-driven
simulation results show that TOFEC's adaptation mechanism converges to an
appropriate code that provides the optimal delay-throughput trade-off without
reducing system capacity. Compared to a non-adaptive strategy optimized for
throughput, TOFEC delivers 2.5x lower latency under light workloads; compared
to a non-adaptive strategy optimized for latency, TOFEC can scale to support
over 3x as many requests.
"
556,"Technical Report: An MGF-based Unified Framework to Determine the Joint
  Statistics of Partial Sums of Ordered i.n.d. Random Variables","  The joint statistics of partial sums of ordered random variables (RVs) are
often needed for the accurate performance characterization of a wide variety of
wireless communication systems. A unified analytical framework to determine the
joint statistics of partial sums of ordered independent and identically
distributed (i.i.d.) random variables was recently presented. However, the
identical distribution assumption may not be valid in several real-world
applications. With this motivation in mind, we consider in this paper the more
general case in which the random variables are independent but not necessarily
identically distributed (i.n.d.). More specifically, we extend the previous
analysis and introduce a new more general unified analytical framework to
determine the joint statistics of partial sums of ordered i.n.d. RVs. Our
mathematical formalism is illustrated with an application on the exact
performance analysis of the capture probability of generalized selection
combining (GSC)-based RAKE receivers operating over frequency-selective fading
channels with a non-uniform power delay profile. We also discussed a couple of
other sample applications of the generic results presented in this work.
"
557,"Performance comparison of IEEE 802.11g and IEEE 802.11n in the presence
  of interference from 802.15.4 networks","  In this paper we compare the packet error rate (PER) and maximum throughput
of IEEE 802.11n and IEEE 802.11g under interference from IEEE 802.15.4 by using
MATLAB to simulate the IEEE PHY for 802.11n and 802.11g networks.
"
558,"RBioCloud: A Light-weight Framework for Bioconductor and R-based Jobs on
  the Cloud","  Large-scale ad hoc analytics of genomic data is popular using the
R-programming language supported by 671 software packages provided by
Bioconductor. More recently, analytical jobs are benefitting from on-demand
computing and storage, their scalability and their low maintenance cost, all of
which are offered by the cloud. While Biologists and Bioinformaticists can take
an analytical job and execute it on their personal workstations, it remains
challenging to seamlessly execute the job on the cloud infrastructure without
extensive knowledge of the cloud dashboard. How analytical jobs can not only
with minimum effort be executed on the cloud, but also how both the resources
and data required by the job can be managed is explored in this paper. An
open-source light-weight framework for executing R-scripts using Bioconductor
packages, referred to as `RBioCloud', is designed and developed. RBioCloud
offers a set of simple command-line tools for managing the cloud resources, the
data and the execution of the job. Three biological test cases validate the
feasibility of RBioCloud. The framework is publicly available from
http://www.rbiocloud.com.
"
559,Parallel Simulations for Analysing Portfolios of Catastrophic Event Risk,"  At the heart of the analytical pipeline of a modern quantitative
insurance/reinsurance company is a stochastic simulation technique for
portfolio risk analysis and pricing process referred to as Aggregate Analysis.
Support for the computation of risk measures including Probable Maximum Loss
(PML) and the Tail Value at Risk (TVAR) for a variety of types of complex
property catastrophe insurance contracts including Cat eXcess of Loss (XL), or
Per-Occurrence XL, and Aggregate XL, and contracts that combine these measures
is obtained in Aggregate Analysis.
  In this paper, we explore parallel methods for aggregate risk analysis. A
parallel aggregate risk analysis algorithm and an engine based on the algorithm
is proposed. This engine is implemented in C and OpenMP for multi-core CPUs and
in C and CUDA for many-core GPUs. Performance analysis of the algorithm
indicates that GPUs offer an alternative HPC solution for aggregate risk
analysis that is cost effective. The optimised algorithm on the GPU performs a
1 million trial aggregate simulation with 1000 catastrophic events per trial on
a typical exposure set and contract structure in just over 20 seconds which is
approximately 15x times faster than the sequential counterpart. This can
sufficiently support the real-time pricing scenario in which an underwriter
analyses different contractual terms and pricing while discussing a deal with a
client over the phone.
"
560,First experiences with the Intel MIC architecture at LRZ,"  With the rapidly growing demand for computing power new accelerator based
architectures have entered the world of high performance computing since around
5 years. In particular GPGPUs have recently become very popular, however
programming GPGPUs using programming languages like CUDA or OpenCL is
cumbersome and error-prone. Trying to overcome these difficulties, Intel
developed their own Many Integrated Core (MIC) architecture which can be
programmed using standard parallel programming techniques like OpenMP and MPI.
In the beginning of 2013, the first production-level cards named Intel Xeon Phi
came on the market. LRZ has been considered by Intel as a leading research
centre for evaluating coprocessors based on the MIC architecture since 2010
under strict NDA. Since the Intel Xeon Phi is now generally available, we can
share our experience on programming Intel's new MIC architecture.
"
561,"Performance Analysis of Connection Admission Control Scheme in IEEE
  802.16 OFDMA Networks","  IEEE 802.16 OFDMA (Orthogonal Frequency Division Multiple Access) technology
has emerged as a promising technology for broadband access in a Wireless
Metropolitan Area Network (WMAN) environment. In this paper, we address the
problem of queueing theoretic performance modeling and analysis of OFDMA under
broad-band wireless networks. We consider a single-cell IEEE 802.16 environment
in which the base station allocates subchannels to the subscriber stations in
its coverage area. The subchannels allocated to a subscriber station are shared
by multiple connections at that subscriber station. To ensure the Quality of
Service (QoS) performances, a Connection Admission Control (CAC) scheme is
considered at a subscriber station. A queueing analytical framework for these
admission control schemes is presented considering OFDMA-based transmission at
the physical layer. Then, based on the queueing model, both the
connection-level and the packet-level performances are studied and compared
with their analogues in the case without CAC. The connection arrival is modeled
by a Poisson process and the packet arrival for a connection by a two-state
Markov Modulated Poisson Process (MMPP). We determine analytically and
numerically different performance parameters, such as connection blocking
probability, average number of ongoing connections, average queue length,
packet dropping probability, queue throughput and average packet delay.
"
562,"Measurement and Prediction of Centrical/Peripheral Network Properties
  based on Regression Analysis - A Parametric Foundation for Performance
  Self-Management in WSNs","  Predicting performance-related behavior of the underlying network structure
becomes more and more indispensable in terms of the aspired application outcome
quality. However, the reliable forecast of QoS metrics like packet transfer
delay in wireless network systems is still a challenging task. Even though
existing approaches are technically capable of determining such network
properties under certain assumptions, they mostly abstract away from primal
aspects that inherently have an essential impact on temporal network
performance dynamics. Also, they usually require auxiliary resources to be
implemented and deployed along with the actual network components. In the
course of developing a lightweight measurement-based alternative for the
self-inspection and prediction of volatile performance characteristics in
environments of any kind, we selectively investigate the duration of message
delivery and packet loss rate against various parameters peculiar to common
radio network technologies like Wireless Sensor Networks (WSNs). Our hands-on
experiments reveal the relations between the oftentimes underestimated medium
access delay and a variety of main influencing factors including packet size,
backoff period, and number of neighbor nodes contending for the communication
medium. A closed formulation of selected weighted drivers facilitates the
average-case prediction of inter-node packet transfer delays for arbitrary
configurations of given network parameters even on resource-scarce WSN devices.
We validate our prediction method against basic multi-hop networking scenarios.
Yield field test results proof the basic feasibility and high precision of our
approach to network property estimation in virtue of self-governed local
measurements and regression-based calculations paving the way for a prospective
self-management of network properties based upon autonomous distributed
coordination.
"
563,"A Computational Framework for the Mixing Times in the QBD Processes with
  Infinitely-Many Levels","  In this paper, we develop some matrix Poisson's equations satisfied by the
mean and variance of the mixing time in an irreducible positive-recurrent
discrete-time Markov chain with infinitely-many levels, and provide a
computational framework for the solution to the matrix Poisson's equations by
means of the UL-type of $RG$-factorization as well as the generalized inverses.
In an important special case: the level-dependent QBD processes, we provide a
detailed computation for the mean and variance of the mixing time. Based on
this, we give new highlight on computation of the mixing time in the
block-structured Markov chains with infinitely-many levels through the
matrix-analytic method.
"
564,BEEBS: Open Benchmarks for Energy Measurements on Embedded Platforms,"  This paper presents and justifies an open benchmark suite named BEEBS,
targeted at evaluating the energy consumption of embedded processors.
  We explore the possible sources of energy consumption, then select individual
benchmarks from contemporary suites to cover these areas. Version one of BEEBS
is presented here and contains 10 benchmarks that cover a wide range of typical
embedded applications. The benchmark suite is portable across diverse
architectures and is freely available.
  The benchmark suite is extensively evaluated, and the properties of its
constituent programs are analysed. Using real hardware platforms we show case
examples which illustrate the difference in power dissipation between three
processor architectures and their related ISAs. We observe significant
differences in the average instruction dissipation between the architectures of
4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and
88uW/MHz (XMOS XS1-L1).
"
565,"Using Chip Multithreading to Speed Up Scenario-Based Design Space
  Exploration","  To cope with the complex embedded system design, early design space
exploration (DSE) is used to make design decisions early in the design phase.
For early DSE it is crucial that the running time of the exploration is as
small as possible. In this paper, we describe both the porting of our
scenario-based DSE to the SPARC T3-4 server and the analysis of its performance
behavior.
"
566,Accelerating a Cloud-Based Software GNSS Receiver,"  In this paper we discuss ways to reduce the execution time of a software
Global Navigation Satellite System (GNSS) receiver that is meant for offline
operation in a cloud environment. Client devices record satellite signals they
receive, and send them to the cloud, to be processed by this software. The goal
of this project is for each client request to be processed as fast as possible,
but also to increase total system throughput by making sure as many requests as
possible are processed within a unit of time. The characteristics of our
application provided both opportunities and challenges for increasing
performance. We describe the speedups we obtained by enabling the software to
exploit multi-core CPUs and GPGPUs. We mention which techniques worked for us
and which did not. To increase throughput, we describe how we control the
resources allocated to each invocation of the software to process a client
request, such that multiple copies of the application can run at the same time.
We use the notion of effective running time to measure the system's throughput
when running multiple instances at the same time, and show how we can determine
when the system's computing resources have been saturated.
"
567,"Machines are benchmarked by code, not algorithms","  This article highlights how small modifications to either the source code of
a benchmark program or the compilation options may impact its behavior on a
specific machine. It argues that for evaluating machines, benchmark providers
and users be careful to ensure reproducibility of results based on the machine
code actually running on the hardware and not just source code. The article
uses color to grayscale conversion of digital images as a running example.
"
568,"Optimizing the performance of Lattice Gauge Theory simulations with
  Streaming SIMD extensions","  Two factors, which affect simulation quality are the amount of computing
power and implementation. The Streaming SIMD (single instruction multiple data)
extensions (SSE) present a technique for influencing both by exploiting the
processor's parallel functionalism. In this paper, we show how SSE improves
performance of lattice gauge theory simulations. We identified two significant
trends through an analysis of data from various runs. The speed-ups were higher
for single precision than double precision floating point numbers. Notably,
though the use of SSE significantly improved simulation time, it did not
deliver the theoretical maximum. There are a number of reasons for this:
architectural constraints imposed by the FSB speed, the spatial and temporal
patterns of data retrieval, ratio of computational to non-computational
instructions, and the need to interleave miscellaneous instructions with
computational instructions. We present a model for analyzing the SSE
performance, which could help factor in the bottlenecks or weaknesses in the
implementation, the computing architecture, and the mapping of software to the
computing substrate while evaluating the improvement in efficiency. The model
or framework would be useful in evaluating the use of other computational
frameworks, and in predicting the benefits that can be derived from future
hardware or architectural improvements.
"
569,"Product-form solutions for integrated services packet networks and cloud
  computing systems","  We iteratively derive the product-form solutions of stationary distributions
of priority multiclass queueing networks with multi-sever stations. The
networks are Markovian with exponential interarrival and service time
distributions. These solutions can be used to conduct performance analysis or
as comparison criteria for approximation and simulation studies of large scale
networks with multi-processor shared-memory switches and cloud computing
systems with parallel-server stations. Numerical comparisons with existing
Brownian approximating model are provided to indicate the effectiveness of our
algorithm.
"
570,"A General, Tractable and Accurate Model for a Cascade of Caches","  Performance evaluation of caching systems is an old and widely investigated
research topic. The research community is once again actively working on this
topic because the Internet is evolving towards new transfer modes, which
envisage to cache both contents and instructions within the network. In
particular, there is interest in characterizing multi-cache systems, in which
requests not satisfied by a cache are forwarded to other caches.
  In this field, this paper contributes as follows. First, we devise a simple
but accurate approximate analysis for caches fed by general ""renewal"" traffic
patterns. Second, we characterize and model the traffic statistics for the
output (miss) stream. Third, we show in the simple example case of tandem
caches how the resulting output stream model can be conveniently exploited to
analyze the performance of subsequent cache stages. The main novelty of our
work stems in the ability to handle traffic patterns beyond the traditional
independent reference model, thus permitting simple assessment of cascade of
caches as well as improved understanding of the phenomena involved in cache
hierarchies.
"
571,When Backpressure Meets Predictive Scheduling,"  Motivated by the increasing popularity of learning and predicting human user
behavior in communication and computing systems, in this paper, we investigate
the fundamental benefit of predictive scheduling, i.e., predicting and
pre-serving arrivals, in controlled queueing systems. Based on a lookahead
window prediction model, we first establish a novel equivalence between the
predictive queueing system with a \emph{fully-efficient} scheduling scheme and
an equivalent queueing system without prediction. This connection allows us to
analytically demonstrate that predictive scheduling necessarily improves system
delay performance and can drive it to zero with increasing prediction power. We
then propose the \textsf{Predictive Backpressure (PBP)} algorithm for achieving
optimal utility performance in such predictive systems. \textsf{PBP}
efficiently incorporates prediction into stochastic system control and avoids
the great complication due to the exponential state space growth in the
prediction window size. We show that \textsf{PBP} can achieve a utility
performance that is within $O(\epsilon)$ of the optimal, for any $\epsilon>0$,
while guaranteeing that the system delay distribution is a
\emph{shifted-to-the-left} version of that under the original Backpressure
algorithm. Hence, the average packet delay under \textsf{PBP} is strictly
better than that under Backpressure, and vanishes with increasing prediction
window size. This implies that the resulting utility-delay tradeoff with
predictive scheduling beats the known optimal $[O(\epsilon),
O(\log(1/\epsilon))]$ tradeoff for systems without prediction.
"
572,Mixed Polling with Rerouting and Applications,"  Queueing systems with a single server in which customers wait to be served at
a finite number of distinct locations (buffers/queues) are called discrete
polling systems. Polling systems in which arrivals of users occur anywhere in a
continuum are called continuous polling systems. Often one encounters a
combination of the two systems: the users can either arrive in a continuum or
wait in a finite set (i.e. wait at a finite number of queues). We call these
systems mixed polling systems. Also, in some applications, customers are
rerouted to a new location (for another service) after their service is
completed. In this work, we study mixed polling systems with rerouting. We
obtain their steady state performance by discretization using the known pseudo
conservation laws of discrete polling systems. Their stationary expected
workload is obtained as a limit of the stationary expected workload of a
discrete system. The main tools for our analysis are: a) the fixed point
analysis of infinite dimensional operators and; b) the convergence of Riemann
sums to an integral.
  We analyze two applications using our results on mixed polling systems and
discuss the optimal system design. We consider a local area network, in which a
moving ferry facilitates communication (data transfer) using a wireless link.
We also consider a distributed waste collection system and derive the optimal
collection point. In both examples, the service requests can arrive anywhere in
a subset of the two dimensional plane. Namely, some users arrive in a
continuous set while others wait for their service in a finite set. The only
polling systems that can model these applications are mixed systems with
rerouting as introduced in this manuscript.
"
573,"An Aggregation Technique For Large-Scale PEPA Models With Non-Uniform
  Populations","  Performance analysis based on modelling consists of two major steps: model
construction and model analysis. Formal modelling techniques significantly aid
model construction but can exacerbate model analysis. In particular, here we
consider the analysis of large-scale systems which consist of one or more
entities replicated many times to form large populations. The replication of
entities in such models can cause their state spaces to grow exponentially to
the extent that their exact stochastic analysis becomes computationally
expensive or even infeasible.
  In this paper, we propose a new approximate aggregation algorithm for a class
of large-scale PEPA models. For a given model, the method quickly checks if it
satisfies a syntactic condition, indicating that the model may be solved
approximately with high accuracy. If so, an aggregated CTMC is generated
directly from the model description. This CTMC can be used for efficient
derivation of an approximate marginal probability distribution over some of the
model's populations. In the context of a large-scale client-server system, we
demonstrate the usefulness of our method.
"
574,"Flashmon V2: Monitoring Raw NAND Flash Memory I/O Requests on Embedded
  Linux","  This paper presents Flashmon version 2, a tool for monitoring embedded Linux
NAND flash memory I/O requests. It is designed for embedded boards based
devices containing raw flash chips. Flashmon is a kernel module and stands for
""flash monitor"". It traces flash I/O by placing kernel probes at the NAND
driver level. It allows tracing at runtime the 3 main flash operations: page
reads / writes and block erasures. Flashmon is (1) generic as it was
successfully tested on the three most widely used flash file systems that are
JFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non
intrusive, (3) has a controllable memory footprint, and (4) exhibits a low
overhead (<6%) on the traced system. Finally, it is (5) simple to integrate and
used as a standalone module or as a built-in function / module in existing
kernel sources. Monitoring flash memory operations allows a better
understanding of existing flash management systems by studying and analyzing
their behavior. Moreover it is useful in development phase for prototyping and
validating new solutions.
"
575,Software Autotuning for Sustainable Performance Portability,"  Scientific software applications are increasingly developed by large
interdiscplinary teams operating on functional modules organized around a
common software framework, which is capable of integrating new functional
capabilities without modifying the core of the framework. In such environment,
software correctness and modularity take precedence at the expense of code
performance, which is an important concern during execution on supercomputing
facilities, where the allocation of core-hours is a valuable resource. To
alleviate the performance problems, we propose automated performance tuning
(autotuning) of software to extract the maximum performance on a given hardware
platform and to enable performance portability across heterogeneous hardware
platforms. The resulting code remains generic without committing to a
particular software stack and yet is compile-time specializable for maximal
sustained performance.
"
576,"Memory transfer optimization for a lattice Boltzmann solver on Kepler
  architecture nVidia GPUs","  The Lattice Boltzmann method (LBM) for solving fluid flow is naturally well
suited to an efficient implementation for massively parallel computing, due to
the prevalence of local operations in the algorithm. This paper presents and
analyses the performance of a 3D lattice Boltzmann solver, optimized for third
generation nVidia GPU hardware, also known as `Kepler'. We provide a review of
previous optimisation strategies and analyse data read/write times for
different memory types. In LBM, the time propagation step (known as streaming),
involves shifting data to adjacent locations and is central to parallel
performance; here we examine three approaches which make use of different
hardware options. Two of which make use of `performance enhancing' features of
the GPU; shared memory and the new shuffle instruction found in Kepler based
GPUs. These are compared to a standard transfer of data which relies instead on
optimised storage to increase coalesced access. It is shown that the more
simple approach is most efficient; since the need for large numbers of
registers per thread in LBM limits the block size and thus the efficiency of
these special features is reduced. Detailed results are obtained for a D3Q19
LBM solver, which is benchmarked on nVidia K5000M and K20C GPUs. In the latter
case the use of a read-only data cache is explored, and peak performance of
over 1036 Million Lattice Updates Per Second (MLUPS) is achieved. The
appearance of a periodic bottleneck in the solver performance is also reported,
believed to be hardware related; spikes in iteration-time occur with a
frequency of around 11Hz for both GPUs, independent of the size of the problem.
"
577,"On the tradeoff of average delay and average power for fading
  point-to-point links with monotone policies","  We consider a fading point-to-point link with packets arriving randomly at
rate $\lambda$ per slot to the transmitter queue. We assume that the
transmitter can control the number of packets served in a slot by varying the
transmit power for the slot. We restrict to transmitter scheduling policies
that are monotone and stationary, i.e., the number of packets served is a
non-decreasing function of the queue length at the beginning of the slot for
every slot fade state. For such policies, we obtain asymptotic lower bounds for
the minimum average delay of the packets, when average transmitter power is a
small positive quantity $V$ more than the minimum average power required for
transmitter queue stability. We show that the minimum average delay grows
either to a finite value or as $\Omega\brap{\log(1/V)}$ or $\Omega\brap{1/V}$
when $V \downarrow 0$, for certain sets of values of $\lambda$. These sets are
determined by the distribution of fading gain, the maximum number of packets
which can be transmitted in a slot, and the transmit power function of the
fading gain and the number of packets transmitted that is assumed. We identify
a case where the above behaviour of the tradeoff differs from that obtained
from a previously considered approximate model, in which the random queue
length process is assumed to evolve on the non-negative real line, and the
transmit power function is strictly convex. We also consider a fading
point-to-point link, where the transmitter, in addition to controlling the
number of packets served, can also control the number of packets admitted in
every slot. Our approach, which uses bounds on the stationary probability
distribution of the queue length, also leads to an intuitive explanation of the
asymptotic behaviour of average delay in the regime where $V \downarrow 0$.
"
578,"Lower Bound on the BER of a Decode-and-Forward Relay Network Under Chaos
  Shift Keying Communication System","  This paper carries out the first-ever investigation of the analysis of a
cooperative Decode-and-Forward (DF) relay network with Chaos Shift Keying (CSK)
modulation. The performance analysis of DF-CSK in this paper takes into account
the dynamical nature of chaotic signal, which is not similar to a conventional
binary modulation performance computation methodology. The expression of a
lower bound bit error rate (BER) is derived in order to investigate the
performance of the cooperative system under independently and identically
distributed (i.i.d.) Gaussian fading wireless environments. The effect of the
non-periodic nature of chaotic sequence leading to a non constant bit energy of
the considered modulation is also investigated. A computation approach of the
BER expression based on the probability density function of the bit energy of
the chaotic sequence, channel distribution, and number of relays is presented.
Simulation results prove the accuracy of our BER computation methodology.
"
579,"Analysis of Optimization Techniques to Improve User Response Time of Web
  Applications and Their Implementation for MOODLE","  Analysis of seven optimization techniques grouped under three categories
(hardware, back-end, and front-end) is done to study the reduction in average
user response time for Modular Object Oriented Dynamic Learning Environment
(Moodle), a Learning Management System which is scripted in PHP5, runs on
Apache web server and utilizes MySQL database software. Before the
implementation of these techniques, performance analysis of Moodle is performed
for varying number of concurrent users. The results obtained for each
optimization technique are then reported in a tabular format. The maximum
reduction in end user response time was achieved for hardware optimization
which requires Moodle server and database to be installed on solid state disk.
"
580,"Improved Spectrum Mobility using Virtual Reservation in Collaborative
  Cognitive Radio Networks","  Cognitive radio technology would enable a set of secondary users (SU) to
opportunistically use the spectrum licensed to a primary user (PU). On the
appearance of this PU on a specific frequency band, any SU occupying this band
should free it for PUs. Typically, SUs may collaborate to reduce the impact of
cognitive users on the primary network and to improve the performance of the
SUs. In this paper, we propose and analyze the performance of virtual
reservation in collaborative cognitive networks. Virtual reservation is a novel
link maintenance strategy that aims to maximize the throughput of the cognitive
network through full spectrum utilization. Our performance evaluation shows
significant improvements not only in the SUs blocking and forced termination
probabilities but also in the throughput of cognitive users.
"
581,Efficient Modular Arithmetic for SIMD Devices,"  This paper describes several new improvements of modular arithmetic and how
to exploit them in order to gain more efficient implementations of commonly
used algorithms, especially in cryptographic applications. We further present a
new record for modular multiplications per second on a single desktop computer
as well as a new record for the ECM factoring algorithm. This new results allow
building personal computers which can handle more than 3 billion modular
multiplications per second for a 192 bit module at moderate costs using modern
graphic cards.
"
582,Extreme Scaling of Lattice Quantum Chromodynamics,"  As the complexity and size of challenges in science and engineering are
continually increasing, it is highly important that applications are able to
scale strongly to very large numbers of cores (>100,000 cores) to enable HPC
systems to be utilised efficiently. This paper presents results of strong
scaling tests performed with an MPI only and a hybrid MPI + OpenMP version of
the Lattice QCD application BQCD on the European Tier-0 system SuperMUC at LRZ.
"
583,An Empirical Study of Intel Xeon Phi,"  With at least 50 cores, Intel Xeon Phi is a true many-core architecture.
Featuring fairly powerful cores, two cache levels, and very fast
interconnections, the Xeon Phi can get a theoretical peak of 1000 GFLOPs and
over 240 GB/s. These numbers, as well as its flexibility - it can be used both
as a coprocessor or as a stand-alone processor - are very tempting for parallel
applications looking for new performance records.
  In this paper, we present an empirical study of Xeon Phi, stressing its
performance limits and relevant performance factors, ultimately aiming to
present a simplified view of the machine for regular programmers in search for
performance.
  To do so, we have micro-benchmarked the main hardware components of the
processor - the cores, the memory hierarchies, the ring interconnect, and the
PCIe connection. We show that, in ideal microbenchmarking conditions, the
performance that can be achieved is very close to the theoretical peak, as
given in the official programmer's guide. We have also identified and
quantified several causes for significant performance penalties. Our findings
have been captured in four optimization guidelines, and used to build a
simplified programmer's view of Xeon Phi, eventually enable the design and
prototyping of applications on a functionality-based model of the architecture.
"
584,"Self-Organizing Mobility Robustness Optimization in LTE Networks with
  eICIC","  We address the problem of Mobility Robustness Optimization (MRO) and describe
centralized Self Organizing Network (SON) solutions that can optimize
connected-mode mobility Key Performance Indicators (KPIs). Our solution extends
the earlier work of eICIC parameter optimization [7], to heterogeneous networks
with mobility, and outline methods of progressive complexity that optimize the
Retaining/Offloading Bias which are macro/pico views of Cell Individual Offset
parameters. Simulation results under real LTE network deployment assumptions of
a US metropolitan area demonstrate the effects of such solutions on the
mobility KPIs. To our knowledge, this solution is the first that demonstrates
the joint optimization of eICIC and MRO.
"
585,The Implications from Benchmarking Three Big Data Systems,"  Along with today's data explosion and application diversification, a variety
of hardware platforms for big data are emerging, attracting interests from both
industry and academia. The existing hardware platforms represent a wide range
of implementation approaches, and different hardware platforms have different
strengths. In this paper, we conduct comprehensive evaluations on three
representative big data systems: Intel Xeon, Atom (low power processors), and
many-core Tilera using BigDataBench - a big data benchmark suite. Then we
explore the relative performance of the three implementation approaches by
running BigDataBench, and provide strong guidance for the big data systems
construction. Through our experiments, we have inferred that a big data system
based on specific hardware has different performance in the context of
different applications and data volumes. When we construct a system, we should
take into account not only the performance or energy consumption of the pure
hardware, but also the characteristics of applications running on them. Data
scale, application type and complexity should be considered comprehensively
when researchers or architects plan to choose fundamental components for their
big data systems.
"
586,Staying Alive: System Design for Self-Sufficient Sensor Networks,"  Self-sustainability is a crucial step for modern sensor networks. Here, we
offer an original and comprehensive framework for autonomous sensor networks
powered by renewable energy sources. We decompose our design into two nested
optimization steps: the inner step characterizes the optimal network operating
point subject to an average energy consumption constraint, while the outer step
provides online energy management policies making the system energetically
self-sufficient in the presence of unpredictable and intermittent energy
sources. Our framework sheds new light into the design of pragmatic schemes for
the control of energy harvesting sensor networks} and permits to gauge the
impact of key sensor network parameters, such as the battery capacity, the
harvester size, the information transmission rate and the radio duty cycle. We
analyze the robustness of the obtained energy management policies in the cases
where the nodes have differing energy inflow statistics and where topology
changes may occur, devising effective heuristics. Our energy management
policies are finally evaluated considering real solar radiation traces,
validating them against state of the art solutions and describing the impact of
relevant design choices in terms of achievable network throughput and battery
level dynamics.
"
587,"EPOBF: Energy Efficient Allocation of Virtual Machines in High
  Performance Computing Cloud","  Cloud computing has become more popular in provision of computing resources
under virtual machine (VM) abstraction for high performance computing (HPC)
users to run their applications. A HPC cloud is such cloud computing
environment. One of challenges of energy efficient resource allocation for VMs
in HPC cloud is tradeoff between minimizing total energy consumption of
physical machines (PMs) and satisfying Quality of Service (e.g. performance).
On one hand, cloud providers want to maximize their profit by reducing the
power cost (e.g. using the smallest number of running PMs). On the other hand,
cloud customers (users) want highest performance for their applications. In
this paper, we focus on the scenario that scheduler does not know global
information about user jobs and user applications in the future. Users will
request shortterm resources at fixed start times and non interrupted durations.
We then propose a new allocation heuristic (named Energy-aware and Performance
per watt oriented Bestfit (EPOBF)) that uses metric of performance per watt to
choose which most energy-efficient PM for mapping each VM (e.g. maximum of MIPS
per Watt). Using information from Feitelson's Parallel Workload Archive to
model HPC jobs, we compare the proposed EPOBF to state of the art heuristics on
heterogeneous PMs (each PM has multicore CPU). Simulations show that the EPOBF
can reduce significant total energy consumption in comparison with state of the
art allocation heuristics.
"
588,"Comparative Performance Analysis of Intel Xeon Phi, GPU, and CPU","  We investigate and characterize the performance of an important class of
operations on GPUs and Many Integrated Core (MIC) architectures. Our work is
motivated by applications that analyze low-dimensional spatial datasets
captured by high resolution sensors, such as image datasets obtained from whole
slide tissue specimens using microscopy image scanners. We identify the data
access and computation patterns of operations in object segmentation and
feature computation categories. We systematically implement and evaluate the
performance of these core operations on modern CPUs, GPUs, and MIC systems for
a microscopy image analysis application. Our results show that (1) the data
access pattern and parallelization strategy employed by the operations strongly
affect their performance. While the performance on a MIC of operations that
perform regular data access is comparable or sometimes better than that on a
GPU; (2) GPUs are significantly more efficient than MICs for operations and
algorithms that irregularly access data. This is a result of the low
performance of the latter when it comes to random data access; (3) adequate
coordinated execution on MICs and CPUs using a performance aware task
scheduling strategy improves about 1.29x over a first-come-first-served
strategy. The example application attained an efficiency of 84% in an execution
with of 192 nodes (3072 CPU cores and 192 MICs).
"
589,"On the optimal tradeoff of average service cost rate, average utility
  rate, and average delay for the state dependent M/M/1 queue","  The optimal tradeoff between average service cost rate, average utility rate,
and average delay is addressed for a state dependent M/M/1 queueing model, with
controllable queue length dependent service rates and arrival rates. For a
model with a constant arrival rate $\lambda$ for all queue lengths, we obtain
an asymptotic characterization of the minimum average delay, when the average
service cost rate is a small positive quantity, $V$, more than the minimum
average service cost rate required for queue stability. We show that depending
on the value of the arrival rate $\lambda$, the assumed service cost rate
function, and the possible values of the service rates, the minimum average
delay either: a) increases only to a finite value, b) increases without bound
as $\log\frac{1}{V}$, c) increases without bound as $\frac{1}{V}$, or d)
increases without bound as $\frac{1}{\sqrt{V}}$, when $V \downarrow 0$. We then
extend our analysis to (i) a complementary problem, where the tradeoff of
average utility rate and average delay is analysed for a M/M/1 queueing model,
with controllable queue length dependent arrival rates, but a constant service
rate $\mu$ for all queue lengths, and (ii) a M/M/1 queueing model, with
controllable queue length dependent service rates and arrival rates, for which
we obtain an asymptotic characterization of the minimum average delay under
constraints on both the average service cost rate as well as the average
utility rate. The results that we obtain are useful in obtaining intuition as
well guidance for the derivation of similar asymptotic lower bounds, such as
the Berry-Gallager asymptotic lower bound, for discrete time queueing models.
"
590,"The abstract Cauchy problem for the non-stationary bulk queue M(t)|M[k,
  B]|1","  We derived state probability equations describing the queue M(t)|M[k, B]|1
and formulated as an abstract Cauchy problem to investigate by means of the
semi-group theory of bounded linear operators in functional analysis. For the
abstract Cauchy problem of this queue, we determined the eigenfunctions of
maximal operator and showed some properties of the Dirichlet operator.
"
591,Performance Modeling of BitTorrent Peer-to-Peer File Sharing Networks,"  BitTorrent is undoubtedly the most popular P2P file sharing application on
today's Internet. The widespread popularity of BitTorrent has attracted a great
deal of attention from networking researchers who conducted various performance
studies on it. This paper presents a comprehensive survey of analytical
performance modeling techniques for BitTorrent networks. The performance models
examined in this study include deterministic models, Markov chain models, fluid
flow models, and queuing network models. These models evaluate the performance
metrics of BitTorrent networks at different regimes with various realistic
factors considered. Furthermore, a comparative analysis is conducted on those
modeling techniques in the aspects of complexity, accuracy, extensibility, and
scalability.
"
592,Significance Relations for the Benchmarking of Meta-Heuristic Algorithms,"  The experimental analysis of meta-heuristic algorithm performance is usually
based on comparing average performance metric values over a set of algorithm
instances. When algorithms getting tight in performance gains, the additional
consideration of significance of a metric improvement comes into play. However,
from this moment the comparison changes from an absolute to a relative mode.
Here the implications of this paradigm shift are investigated. Significance
relations are formally established. Based on this, a trade-off between
increasing cycle-freeness of the relation and small maximum sets can be
identified, allowing for the selection of a proper significance level and
resulting ranking of a set of algorithms. The procedure is exemplified on the
CEC'05 benchmark of real parameter single objective optimization problems. The
significance relation here is based on awarding ranking points for relative
performance gains, similar to the Borda count voting method or the Wilcoxon
signed rank test. In the particular CEC'05 case, five ranks for algorithm
performance can be clearly identified.
"
593,"Proactive bottleneck performance analysis in parallel computing using
  openMP","  The aim of parallel computing is to increase an application performance by
executing the application on multiple processors. OpenMP is an API that
supports multi platform shared memory programming model and shared-memory
programs are typically executed by multiple threads. The use of multi threading
can enhance the performance of application but its excessive use can degrade
the performance. This paper describes a novel approach to avoid bottlenecks in
application and provide some techniques to improve performance in OpenMP
application. This paper analyzes bottleneck performance as bottleneck inhibits
performance. Performance of multi threaded applications is limited by a variety
of bottlenecks, e.g. critical sections, barriers and so on. This paper provides
some tips how to avoid performance bottleneck problems. This paper focuses on
how to reduce overheads and overall execution time to get better performance of
application.
"
594,When Do Redundant Requests Reduce Latency ?,"  Several systems possess the flexibility to serve requests in more than one
way. For instance, a distributed storage system storing multiple replicas of
the data can serve a request from any of the multiple servers that store the
requested data, or a computational task may be performed in a compute-cluster
by any one of multiple processors. In such systems, the latency of serving the
requests may potentially be reduced by sending ""redundant requests"": a request
may be sent to more servers than needed, and it is deemed served when the
requisite number of servers complete service. Such a mechanism trades off the
possibility of faster execution of at least one copy of the request with the
increase in the delay due to an increased load on the system. Due to this
tradeoff, it is unclear when redundant requests may actually help. Several
recent works empirically evaluate the latency performance of redundant requests
in diverse settings.
  This work aims at an analytical study of the latency performance of redundant
requests, with the primary goals of characterizing under what scenarios sending
redundant requests will help (and under what scenarios they will not help), as
well as designing optimal redundant-requesting policies. We first present a
model that captures the key features of such systems. We show that when service
times are i.i.d. memoryless or ""heavier"", and when the additional copies of
already-completed jobs can be removed instantly, redundant requests reduce the
average latency. On the other hand, when service times are ""lighter"" or when
service times are memoryless and removal of jobs is not instantaneous, then not
having any redundancy in the requests is optimal under high loads. Our results
hold for arbitrary arrival processes.
"
595,Performance Evaluation of Java File Security System (JFSS),"  Security is a critical issue of the modern file and storage systems, it is
imperative to protect the stored data from unauthorized access. We have
developed a file security system named as Java File Security System (JFSS) [1]
that guarantee the security to files on the demand of all users. It has been
developed on Java platform. Java has been used as programming language in order
to provide portability, but it enforces some performance limitations. It is
developed in FUSE (File System in User space) [3]. Many efforts have been done
over the years for developing file systems in user space (FUSE). All have their
own merits and demerits. In this paper we have evaluated the performance of
Java File Security System (JFSS). Over and over again, the increased security
comes at the expense of user convenience, performance or compatibility with
other systems. JFSS system performance evaluations show that encryption
overheads are modest as compared to security.
"
596,HS06 Benchmark for an ARM Server,"  We benchmarked an ARM cortex-A9 based server system with a four-core CPU
running at 1.1 GHz. The system used Ubuntu 12.04 as operating system and the
HEPSPEC 2006 (HS06) benchmarking suite was compiled natively with gcc-4.4 on
the system. The benchmark was run for various settings of the relevant gcc
compiler options. We did not find significant influence from the compiler
options on the benchmark result. The final HS06 benchmark result is 10.4.
"
597,"Distributed Multiscale Computing with MUSCLE 2, the Multiscale Coupling
  Library and Environment","  We present the Multiscale Coupling Library and Environment: MUSCLE 2. This
multiscale component-based execution environment has a simple to use Java, C++,
C, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We
demonstrate its local and distributed computing capabilities and compare its
performance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local
throughput of MPI is about two times higher, so very tightly coupled code
should use MPI as a single submodel of MUSCLE 2; the distributed performance of
GridFTP is lower, especially for small messages. We test the performance of a
canal system model with MUSCLE 2, where it introduces an overhead as small as
5% compared to MPI.
"
598,"Analysis of Load Balancing in Large Heterogeneous Processor Sharing
  Systems","  We analyze randomized dynamic load balancing schemes for multi-server
processor sharing systems when the number of servers in the system is large and
the servers have heterogeneous service rates. In particular, we focus on the
classical power-of-two load balancing scheme and a variant of it in which a
newly arrived job is assigned to the server having the least instantaneous
Lagrange shadow cost among two randomly chosen servers. The instantaneous
Lagrange shadow cost at a server is given by the ratio of the number of
unfinished jobs at the server to the capacity of the server. Two different
approaches of analysis are presented for each scheme. For exponential job
length distribution, the analysis is done using the mean field approach and for
more general job length distributions the analysis is carried out assuming an
asymptotic independence property. Analytical expressions to compute mean
sojourn time of jobs are found for both schemes. Asymptotic insensitivity of
the schemes to the type of job length distribution is established. Numerical
results are presented to validate the theoretical results and to show that,
unlike the homogeneous scenario, the power-of-two type schemes considered in
this paper may not always result in better behaviour in terms of the mean
sojourn time of jobs.
"
599,"Energy Efficient Spectrum Sensing and Handoff Strategies in Cognitive
  Radio Networks","  The limited spectrum resources and dramatic growth of high data rate
communications have motivated opportunistic spectrum access using the promising
concept of cognitive radio networks. Although this concept has emerged
primarily to enhance spectrum utilization, the importance of energy consumption
poses new challenges, because energy efficiency and communication performance
can be at odds. In this paper, the existing approaches to energy efficiency
spectrum sensing and handoff are classified. The tradeoff between energy
consumption and throughput is established as function of the numerous design
parameters of cognitive radio networks, both in the case of local and of
cooperative spectrum sensing. It is argued that a number of important aspects
still needs to be researched, such as fairness, dynamic behavior, reactive and
proactive schemes for energy efficiency.
"
600,"High Throughput Virtual Screening with Data Level Parallelism in
  Multi-core Processors","  Improving the throughput of molecular docking, a computationally intensive
phase of the virtual screening process, is a highly sought area of research
since it has a significant weight in the drug designing process. With such
improvements, the world might find cures for incurable diseases like HIV
disease and Cancer sooner. Our approach presented in this paper is to utilize a
multi-core environment to introduce Data Level Parallelism (DLP) to the
Autodock Vina software, which is a widely used for molecular docking software.
Autodock Vina already exploits Instruction Level Parallelism (ILP) in
multi-core environments and therefore optimized for such environments. However,
with the results we have obtained, it can be clearly seen that our approach has
enhanced the throughput of the already optimized software by more than six
times. This will dramatically reduce the time consumed for the lead
identification phase in drug designing along with the shift in the processor
technology from multi-core to many-core of the current era. Therefore, we
believe that the contribution of this project will effectively make it possible
to expand the number of small molecules docked against a drug target and
improving the chances to design drugs for incurable diseases.
"
601,Scalability of the plasma physics code GEM,"  We discuss a detailed weak scaling analysis of GEM, a 3D MPI-parallelised
gyrofluid code used in theoretical plasma physics at the Max Planck Institute
of Plasma Physics, IPP at Garching b. M\""unchen, Germany. Within a PRACE
Preparatory Access Project various versions of the code have been analysed on
the HPC systems SuperMUC at LRZ and JUQUEEN at J\""ulich Supercomputing Centre
(JSC) to improve the parallel scalability of the application. The diagnostic
tool Scalasca has been used to filter out suboptimal routines. The code uses
the electromagnetic gyrofluid model which is a superset of magnetohydrodynamic
and drift-Alfv\'en microturbulance and also includes several relevant kinetic
processes. GEM can be used with different geometries depending on the targeted
use case, and has been proven to show good scalability when the computational
domain is distributed amongst two dimensions. Such a distribution allows grids
with sufficient size to describe small scale tokamak devices. In order to
enable simulation of very large tokamaks (such as the next generation nuclear
fusion device ITER in Cadarache, France) the third dimension has been
parallelised and weak scaling has been achieved for significantly larger grids.
"
602,"Dominant block guided optimal cache size estimation to maximize IPC of
  embedded software","  Embedded system software is highly constrained from performance, memory
footprint, energy consumption and implementing cost view point. It is always
desirable to obtain better Instructions per Cycle. Instruction cache has major
contribution in improving IPC. Cache memories are realized on the same chip
where the processor is running. This considerably increases the system cost as
well. Hence, it is required to maintain a trade off between cache sizes and
performance improvement offered. Determining the number of cache lines and size
of cache line are important parameters for cache designing. The design space
for cache is quite large. It is time taking to execute the given application
with different cache sizes on an instruction set simulator to figure out the
optimal cache size. In this paper, a technique is proposed to identify a number
of cache lines and cache line size for the L1 instruction cache that will offer
best or nearly best IPC. Cache size is derived, at a higher abstraction level,
from basic block analysis in the Low Level Virtual Machine environment. The
cache size estimated is cross validated by simulating the set of benchmark
applications with different cache sizes in simple scalar simulator. The
proposed method seems to be superior in terms of estimation accuracy and
estimation time as compared to the existing methods for estimation of optimal
cache size parameters like cache line size, number of cache lines.
"
603,"Advanced Antenna Techniques and High Order Sectorization with Novel
  Network Tessellation for Enhancing Macro Cell Capacity in DC-HSDPA Network","  Mobile operators commonly use macro cells with traditional wide beam antennas
for wider coverage in the cell, but future capacity demands cannot be achieved
by using them only. It is required to achieve maximum practical capacity from
macro cells by employing higher order sectorization and by utilizing all
possible antenna solutions including smart antennas. This paper presents
enhanced tessellation for 6-sector sites and proposes novel layout for
12-sector sites. The main target of this paper is to compare the performance of
conventional wide beam antenna, switched beam smart antenna, adaptive beam
antenna and different network layouts in terms of offering better received
signal quality and user throughput. Splitting macro cell into smaller micro or
pico cells can improve the capacity of network, but this paper highlights the
importance of higher order sectorization and advance antenna techniques to
attain high Signal to Interference plus Noise Ratio (SINR), along with improved
network capacity. Monte Carlo simulations at system level were done for Dual
Cell High Speed Downlink Packet Access (DC-HSDPA) technology with multiple
(five) users per Transmission Time Interval (TTI) at different Intersite
Distance (ISD). The obtained results validate and estimate the gain of using
smart antennas and higher order sectorization with proposed network layout.
"
604,A Survey of Embedded Software Profiling Methodologies,"  Embedded Systems combine one or more processor cores with dedicated logic
running on an ASIC or FPGA to meet design goals at reasonable cost. It is
achieved by profiling the application with variety of aspects like performance,
memory usage, cache hit versus cache miss, energy consumption, etc. Out of
these, performance estimation is more important than others. With ever
increasing system complexities, it becomes quite necessary to carry out
performance estimation of embedded software implemented in a particular
processor for fast design space exploration. Such profiled data also guides the
designer how to partition the system for Hardware (HW) and Software (SW)
environments. In this paper, we propose a classification for currently
available Embedded Software Profiling Tools, and we present different academic
and industrial approaches in this context. Based on these observations, it will
be easy to identify such common principles and needs which are required for a
true Software Profiling Tool for a particular application.
"
605,Low Latency Datacenter Networking: A Short Survey,"  Datacenters are the cornerstone of the big data infrastructure supporting
numerous online services. The demand for interactivity, which significantly
impacts user experience and provider revenue, is translated into stringent
timing requirements for flows in datacenter networks. Thus low latency
networking is becoming a major concern of both industry and academia.
  We provide a short survey of recent progress made by the networking community
for low latency datacenter networks. We propose a taxonomy to categorize
existing work based on four main techniques, reducing queue length,
accelerating retransmissions, prioritizing mice flows, and exploiting
multi-path. Then we review select papers, highlight the principal ideas, and
discuss their pros and cons. We also present our perspectives of the research
challenges and opportunities, hoping to aspire more future work in this space.
"
606,Building An Information System for a Distributed Testbed,"  This paper describes an information system designed to support the large
volume of monitoring information generated by a distributed testbed. This
monitoring information is produced by several subsystems and consists of status
and performance data that needs to be federated, distributed, and stored in a
timely and easy to use manner. Our approach differs from existing approaches
because it federates and distributes information at a low architectural level
via messaging; a natural match to many of the producers and consumers of
information. In addition, a database is easily layered atop the messaging layer
for consumers that want to query and search the information. Finally, a common
language to represent information in all layers of the information system makes
it significantly easier for users to consume information. Performance data
shows that this approach meets the significant needs of FutureGrid and would
meet the needs of an experimental infrastructure twice the size of FutureGrid.
In addition, this design also meets the needs of existing distributed
scientific infrastructures.
"
607,"On the Effectiveness of Polynomial Realization of Reed-Solomon Codes for
  Storage Systems","  There are different ways to realize Reed Solomon (RS) codes. While in the
storage community, using the generator matrices to implement RS codes is more
popular, in the coding theory community the generator polynomials are typically
used to realize RS codes. Prominent exceptions include HDFS-RAID, which uses
generator polynomial based erasure codes, and extends the Apache Hadoop's file
system.
  In this paper we evaluate the performance of an implementation of polynomial
realization of Reed-Solomon codes, along with our optimized version of it,
against that of a widely-used library (Jerasure) that implements the main
matrix realization alternatives. Our experimental study shows that despite
significant performance gains yielded by our optimizations, the polynomial
implementations' performance is constantly inferior to those of matrix
realization alternatives in general, and that of Cauchy bit matrices in
particular.
"
608,"Analysis and Optimization of Random Sensing Order in Cognitive Radio
  Networks","  Developing an efficient spectrum access policy enables cognitive radios to
dramatically increase spectrum utilization while ensuring predetermined quality
of service levels for primary users. In this paper, modeling, performance
analysis, and optimization of a distributed secondary network with random
sensing order policy are studied. Specifically, the secondary users create a
random order of available channels upon primary users return, and then find
optimal transmission and handoff opportunities in a distributed manner. By a
Markov chain analysis, the average throughputs of the secondary users and
average interference level among the secondary and primary users are
investigated. A maximization of the secondary network performance in terms of
the throughput while keeping under control the average interference is
proposed. It is shown that despite of traditional view, non-zero false alarm in
the channel sensing can increase channel utilization, especially in a dense
secondary network where the contention is too high. Then, two simple and
practical adaptive algorithms are established to optimize the network. The
second algorithm follows the variations of the wireless channels in
non-stationary conditions and outperforms even static brute force optimization,
while demanding few computations. The convergence of the distributed algorithms
are theoretically investigated based on the analytical performance indicators
established by the Markov chain analysis. Finally, numerical results validate
the analytical derivations and demonstrate the efficiency of the proposed
schemes. It is concluded that fully distributed sensing order algorithms can
lead to substantial performance improvements in cognitive radio networks
without the need of centralized management or message passing among the users.
"
609,"On the Stability of Random Multiple Access with Feedback Exploitation
  and Queue Priority","  In this paper, we study the stability of two interacting queues under random
multiple access in which the queues leverage the feedback information. We
derive the stability region under random multiple access where one of the two
queues exploits the feedback information and backs off under negative
acknowledgement (NACK) and the other, higher priority, queue will access the
channel with probability one. We characterize the stability region of this
feedback-based random access protocol and prove that this derived stability
region encloses the stability region of the conventional random access (RA)
scheme that does not exploit the feedback information.
"
610,Removing Dynamic Type Tests with Context-Driven Basic Block Versioning,"  Dynamic typing is an important feature of dynamic programming languages.
Primitive operators such as those for performing arithmetic and comparisons
typically operate on a wide variety of in put value types, and as such, must
internally implement some form of dynamic type dispatch and type checking.
Removing such type tests is important for an efficient implementation.
  In this paper, we examine the effectiveness of a novel approach to reducing
the number of dynamically executed type tests called context-driven basic block
versioning. This simple technique clones and specializes basic blocks in such a
way as to allow the compiler to accumulate type information while machine code
is generated, without a separate type analysis pass. The accumulated
information allows the removal of some redundant type tests, particularly in
performance-critical paths.
  We have implemented intraprocedural context-driven basic block versioning in
a JavaScript JIT compiler. For comparison, we have also implemented a classical
flow-based type analysis operating on the same concrete types. Our results show
that basic block versioning performs better on most benchmarks and removes a
large fraction of type tests at the expense of a moderate code size increase.
We believe that this technique offers a good tradeoff between implementation
complexity and performance, and is suitable for integration in production JIT
compilers.
"
611,Self-Optimizing Mechanisms for EMF Reduction in Heterogeneous Networks,"  This paper focuses on the exposure to Radio Frequency (RF) Electromagnetic
Fields (EMF) and on optimization methods to reduce it. Within the FP7 LEXNET
project, an Exposure Index (EI) has been defined that aggregates the essential
components that impact exposure to EMF. The EI includes, among other, downlink
(DL) exposure induced by the base stations (BSs) and access points, the uplink
(UL) exposure induced by the devices in communication, and the corresponding
exposure time. Motivated by the EI definition, this paper develops stochastic
approximation based self-optimizing algorithm that dynamically adapts the
network to reduce the EI in a heterogeneous network with macro- and small
cells. It is argued that the increase of the small cells' coverage can, to a
certain extent, reduce the EI, but above a certain limit, will deteriorate DL
QoS. A load balancing algorithm is formulated that adapts the small cell'
coverage based on UL loads and a DL QoS indicator. The proof of convergence of
the algorithm is provided and its performance in terms of EI reduction is
illustrated through extensive numerical simulations.
"
612,"Performance Engineering for a Medical Imaging Application on the Intel
  Xeon Phi Accelerator","  We examine the Xeon Phi, which is based on Intel's Many Integrated Cores
architecture, for its suitability to run the FDK algorithm--the most commonly
used algorithm to perform the 3D image reconstruction in cone-beam computed
tomography. We study the challenges of efficiently parallelizing the
application and means to enable sensible data sharing between threads despite
the lack of a shared last level cache. Apart from parallelization, SIMD
vectorization is critical for good performance on the Xeon Phi; we perform
various micro-benchmarks to investigate the platform's new set of vector
instructions and put a special emphasis on the newly introduced vector gather
capability. We refine a previous performance model for the application and
adapt it for the Xeon Phi to validate the performance of our optimized
hand-written assembly implementation, as well as the performance of several
different auto-vectorization approaches.
"
613,"Power Aware Wireless File Downloading: A Constrained Restless Bandit
  Approach","  This paper treats power-aware throughput maximization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy and its effectiveness is shown via
simulation. For simple 3-user cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
"
614,"An algorithm for calculating steady state probabilities of $M|E_r|c|K$
  queueing systems","  This paper presents a method for calculating steady state probabilities of
$M|E_r|c|K$ queueing systems. The infinitesimal generator matrix is used to
define all possible states in the system and their transition probabilities.
While this matrix can be written down immediately for many other $M|PH|c|K$
queueing systems with phase-type service times (e.g. Coxian, Hypoexponential,
\ldots), it requires a more careful analysis for systems with Erlangian service
times. The constructed matrix may then be used to calculate steady state
probabilities using an iterative algorithm. The resulting steady state
probabilities can be used to calculate various performance measures, e.g. the
average queue length. Additionally, computational issues of the implementation
are discussed and an example from the field of telecommunication call-center
queue length will be outlined to substantiate the applicability of these
efforts. In the appendix, tables of the average queueing length given a
specific number of service channels, traffic density, and system size are
presented.
"
615,MRRR-based Eigensolvers for Multi-core Processors and Supercomputers,"  The real symmetric tridiagonal eigenproblem is of outstanding importance in
numerical computations; it arises frequently as part of eigensolvers for
standard and generalized dense Hermitian eigenproblems that are based on a
reduction to tridiagonal form. For its solution, the algorithm of Multiple
Relatively Robust Representations (MRRR or MR3 in short) - introduced in the
late 1990s - is among the fastest methods. To compute k eigenpairs of a real
n-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in
contrast, all the other practical methods require O(k^2 n) or O(n^3) operations
in the worst case. This thesis centers around the performance and accuracy of
MRRR.
"
616,A Brief Review on Models for Performance Evaluation in DSS Architecture,"  Distributed Software Systems are used these days by many people in the real
time operations and modern enterprise applications. One of the most important
and essential attributes of measurements for the quality of service of
distributed software is performance. Performance models can be employed at
early stages of the software development cycle to characterize the quantitative
behavior of software systems. In this research, performance models based on
fuzzy logic approach, queuing network approach and Petri net approach have been
reviewed briefly. One of the most common ways in performance analysis of
distributed software systems is translating the UML diagrams to mathematical
modeling languages for the description of distributed systems such as queuing
networks or Petri nets. In this paper, some of these approaches are reviewed
briefly. Attributes which are used for performance modeling in the literature
are mostly machine based. On the other hand, end users and client parameters
for performance evaluation are not covered extensively. In this way, future
research could be based on developing hybrid models to capture user decision
variables which make system performance evaluation more user driven.
"
617,"Performance Impact of Lock-Free Algorithms on Multicore Communication
  APIs","  Data race conditions in multi-tasking software applications are prevented by
serializing access to shared memory resources, ensuring data consistency and
deterministic behavior. Traditionally tasks acquire and release locks to
synchronize operations on shared memory. Unfortunately, lock management can add
significant processing overhead especially for multicore deployments where
tasks on different cores convoy in queues waiting to acquire a lock.
Implementing more than one lock introduces the risk of deadlock and using
spinlocks constrains which cores a task can run on. The better alternative is
to eliminate locks and validate that real-time properties are met, which is not
directly considered in many embedded applications. Removing the locks is
non-trivial and packaging lock-free algorithms for developers reduces the
possibility of concurrency defects. This paper details how a multicore
communication API implementation is enhanced to support lock-free messaging and
the impact this has on data exchange latency between tasks. Throughput and
latency are compared on Windows and Linux between lock-based and lock-free
implementations for data exchange of messages, packets, and scalars. A model of
the lock-free exchange predicts performance at the system architecture level
and provides a stop criterion for the refactoring. The results show that
migration from single to multicore hardware architectures degrades lock-based
performance, and increases lock-free performance.
"
618,SIMD Compression and the Intersection of Sorted Integers,"  Sorted lists of integers are commonly used in inverted indexes and database
systems. They are often compressed in memory. We can use the SIMD instructions
available in common processors to boost the speed of integer compression
schemes. Our S4-BP128-D4 scheme uses as little as 0.7 CPU cycles per decoded
integer while still providing state-of-the-art compression.
  However, if the subsequent processing of the integers is slow, the effort
spent on optimizing decoding speed can be wasted. To show that it does not have
to be so, we (1) vectorize and optimize the intersection of posting lists; (2)
introduce the SIMD Galloping algorithm. We exploit the fact that one SIMD
instruction can compare 4 pairs of integers at once.
  We experiment with two TREC text collections, GOV2 and ClueWeb09 (Category
B), using logs from the TREC million-query track. We show that using only the
SIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive
queries can double the speed of a state-of-the-art approach.
"
619,"BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking
  Framework","  Big Data is considered proprietary asset of companies, organizations, and
even nations. Turning big data into real treasure requires the support of big
data systems. A variety of commercial and open source products have been
unleashed for big data storage and processing. While big data users are facing
the choice of which system best suits their needs, big data system developers
are facing the question of how to evaluate their systems with regard to general
big data processing needs. System benchmarking is the classic way of meeting
the above demands. However, existent big data benchmarks either fail to
represent the variety of big data processing requirements, or target only one
specific platform, e.g. Hadoop.
  In this paper, with our industrial partners, we present BigOP, an end-to-end
system benchmarking framework, featuring the abstraction of representative
Operation sets, workload Patterns, and prescribed tests. BigOP is part of an
open-source big data benchmarking project, BigDataBench. BigOP's abstraction
model not only guides the development of BigDataBench, but also enables
automatic generation of tests with comprehensive workloads.
  We illustrate the feasibility of BigOP by implementing an automatic test
generation tool and benchmarking against three widely used big data processing
systems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three
different application scenarios are prescribed. The tests involve relational
data, text data and graph data, as well as all operations and workload
patterns. We report results following test specifications.
"
620,"Comparing the Performance of Different x86 SIMD Instruction Sets for a
  Medical Imaging Application on Modern Multi- and Manycore Chips","  Single Instruction, Multiple Data (SIMD) vectorization is a major driver of
performance in current architectures, and is mandatory for achieving good
performance with codes that are limited by instruction throughput. We
investigate the efficiency of different SIMD-vectorized implementations of the
RabbitCT benchmark. RabbitCT performs 3D image reconstruction by back
projection, a vital operation in computed tomography applications. The
underlying algorithm is a challenge for vectorization because it consists,
apart from a streaming part, also of a bilinear interpolation requiring
scattered access to image data. We analyze the performance of SSE (128 bit),
AVX (256 bit), AVX2 (256 bit), and IMCI (512 bit) implementations on recent
Intel x86 systems. A special emphasis is put on the vector gather
implementation on Intel Haswell and Knights Corner microarchitectures. Finally
we discuss why GPU implementations perform much better for this specific
algorithm.
"
621,"A Monte-Carlo Approach to Lifespan Failure Performance Analysis of the
  Network Fabric in Modular Data Centers","  Data centers have been evolved from a passive element of compute
infrastructure to become an active, core part of any ICT solution. In
particular, modular data centers (MDCs), which are a promising design approach
to improve resiliency of data centers, can play a key role in deploying ICT
infrastructure in remote and inhospitable environments in order to take
advantage of low temperatures and hydro- and wind-electric capabilities. This
is because of capability of the modular data centers to survive even in lack of
continuous on-site maintenance and support. The most critical part of a data
center is its network fabric that could impede the whole system even if all
other components are fully functional, assuming that other analyses has been
already performed to ensure the reliability of the underlying infrastructure
and support systems. In this work, a complete failure analysis of modular data
centers using failure models of various components including servers, switches,
and links is performed using a proposed Monte-Carlo approach. The proposed
Monte-Carlo approach, which is based on the concept of snapshots, allows us to
effectively calculate the performance of a design along its lifespan even up to
the terminal stages. To show the capabilities of the proposed approach, various
network topologies, such as FatTree, BCube, MDCube, and their modifications are
considered. The performance and also the lifespan of each topology design in
presence of failures of their components are studied against the topology
parameters.
"
622,Modelling Load Balancing and Carrier Aggregation in Mobile Networks,"  In this paper, we study the performance of multicarrier mobile networks.
Specifically, we analyze the flow-level performance of two inter-carrier load
balancing schemes and the gain engendered by Carrier Aggregation (CA). CA is
one of the most important features of HSPA+ and LTE-A networks; it allows
devices to be served simultaneously by several carriers. We propose two load
balancing schemes, namely Join the Fastest Queue (JFQ) and Volume Balancing
(VB), that allow the traffic of CA and non-CA users to be distributed over the
aggregated carriers. We then evaluate the performance of these schemes by means
of analytical modeling. We show that the proposed schemes achieve quasi-ideal
load balancing. We also investigate the impact of mixing traffic of CA and
non-CA users in the same cell and show that performance is practically
insensitive to the traffic mix.
"
623,"A Measurement-based Analysis of the Energy Consumption of Data Center
  Servers","  Energy consumption is a growing issue in data centers, impacting their
economic viability and their public image. In this work we empirically
characterize the power and energy consumed by different types of servers. In
particular, in order to understand the behavior of their energy and power
consumption, we perform measurements in different servers. In each of them, we
exhaustively measure the power consumed by the CPU, the disk, and the network
interface under different configurations, identifying the optimal operational
levels. One interesting conclusion of our study is that the curve that defines
the minimal CPU power as a function of the load is neither linear nor purely
convex as has been previously assumed. Moreover, we find that the efficiency of
the various server components can be maximized by tuning the CPU frequency and
the number of active cores as a function of the system and network load, while
the block size of I/O operations should be always maximized by applications. We
also show how to estimate the energy consumed by an application as a function
of some simple parameters, like the CPU load, and the disk and network
activity. We validate the proposed approach by accurately estimating the energy
of a map-reduce computation in a Hadoop platform.
"
624,"Constructing Performance Models for Dense Linear Algebra Algorithms on
  Cray XE Systems","  Hiding or minimizing the communication cost is key in order to obtain good
performance on large-scale systems. While communication overlapping attempts to
hide communications cost, 2.5D communication avoiding algorithms improve
performance scalability by reducing the volume of data transfers at the cost of
extra memory usage. Both approaches can be used together or separately and the
best choice depends on the machine, the algorithm and the problem size. Thus,
the development of performance models is crucial to determine the best option
for each scenario. In this paper, we present a methodology for constructing
performance models for parallel numerical routines on Cray XE systems. Our
models use portable benchmarks that measure computational cost and network
characteristics, as well as performance degradation caused by simultaneous
accesses to the network. We validate our methodology by constructing the
performance models for the 2D and 2.5D approaches, with and without
overlapping, of two matrix multiplication algorithms (Cannon's and SUMMA),
triangular solve (TRSM) and Cholesky. We compare the estimations provided by
these models with the experimental results using up to 24,576 cores of a Cray
XE6 system and predict the performance of the algorithms on larger systems.
Results prove that the estimations significantly improve when taking into
account network contention.
"
625,Characterizing Workload of Web Applications on Virtualized Servers,"  With the ever increasing demands of cloud computing services, planning and
management of cloud resources has become a more and more important issue which
directed affects the resource utilization and SLA and customer satisfaction.
But before any management strategy is made, a good understanding of
applications' workload in virtualized environment is the basic fact and
principle to the resource management methods. Unfortunately, little work has
been focused on this area. Lack of raw data could be one reason; another reason
is that people still use the traditional models or methods shared under
non-virtualized environment. The study of applications' workload in virtualized
environment should take on some of its peculiar features comparing to the
non-virtualized environment. In this paper, we are open to analyze the workload
demands that reflect applications' behavior and the impact of virtualization.
The results are obtained from an experimental cloud testbed running web
applications, specifically the RUBiS benchmark application. We profile the
workload dynamics on both virtualized and non-virtualized environments and
compare the findings. The experimental results are valuable for us to estimate
the performance of applications on computer architectures, to predict SLA
compliance or violation based on the projected application workload and to
guide the decision making to support applications with the right hardware.
"
626,On Big Data Benchmarking,"  Big data systems address the challenges of capturing, storing, managing,
analyzing, and visualizing big data. Within this context, developing benchmarks
to evaluate and compare big data systems has become an active topic for both
research and industry communities. To date, most of the state-of-the-art big
data benchmarks are designed for specific types of systems. Based on our
experience, however, we argue that considering the complexity, diversity, and
rapid evolution of big data systems, for the sake of fairness, big data
benchmarks must include diversity of data and workloads. Given this motivation,
in this paper, we first propose the key requirements and challenges in
developing big data benchmarks from the perspectives of generating data with 4V
properties (i.e. volume, velocity, variety and veracity) of big data, as well
as generating tests with comprehensive workloads for big data systems. We then
present the methodology on big data benchmarking designed to address these
challenges. Next, the state-of-the-art are summarized and compared, following
by our vision for future research directions.
"
627,Exact Simulation for Assemble-To-Order Systems,"  We develop exact simulation (also known as perfect sampling) algorithms for a
family of assemble-to-order systems. Due to the finite capacity, and coupling
in demands and replenishments, known solving techniques are inefficient for
larger problem instances. We first consider the case with individual
replenishments of items, and derive an event based representation of the Markov
chain that allows applying existing exact simulation techniques, using the
monotonicity properties or bounding chains. In the case of joint
replenishments, the state space becomes intractable for the existing methods.
We propose new exact simulation algorithms, based on aggregation and bounding
chains, that allow a significant reduction of the state space of the Markov
chain. We also discuss the coupling times of considered models and provide
sufficient conditions for linear (in the single server replenishment case) or
quadratic (many server case) complexity of our algorithms in terms of the total
capacity in the system.
"
628,"A Study on the Influence of Caching: Sequences of Dense Linear Algebra
  Kernels","  It is universally known that caching is critical to attain high- performance
implementations: In many situations, data locality (in space and time) plays a
bigger role than optimizing the (number of) arithmetic floating point
operations. In this paper, we show evidence that at least for linear algebra
algorithms, caching is also a crucial factor for accurate performance modeling
and performance prediction.
"
629,"Exact Analysis of TTL Cache Networks: The Case of Caching Policies
  driven by Stopping Times","  TTL caching models have recently regained significant research interest,
largely due to their ability to fit popular caching policies such as LRU. This
paper advances the state-of-the-art analysis of TTL-based cache networks by
developing two exact methods with orthogonal generality and computational
complexity. The first method generalizes existing results for line networks
under renewal requests to the broad class of caching policies whereby evictions
are driven by stopping times. The obtained results are further generalized,
using the second method, to feedforward networks with Markov arrival processes
(MAP) requests. MAPs are particularly suitable for non-line networks because
they are closed not only under superposition and splitting, as known, but also
under input-output caching operations as proven herein for phase-type TTL
distributions. The crucial benefit of the two closure properties is that they
jointly enable the first exact analysis of feedforward networks of TTL caches
in great generality.
"
630,Perfect Simulation of $M/G/c$ Queues,"  In this paper we describe a perfect simulation algorithm for the stable
$M/G/c$ queue. Sigman (2011: Exact Simulation of the Stationary Distribution of
the FIFO M/G/c Queue. Journal of Applied Probability, 48A, 209--213) showed how
to build a dominated CFTP algorithm for perfect simulation of the super-stable
$M/G/c$ queue operating under First Come First Served discipline, with
dominating process provided by the corresponding $M/G/1$ queue (using Wolff's
sample path monotonicity, which applies when service durations are coupled in
order of initiation of service), and exploiting the fact that the workload
process for the $M/G/1$ queue remains the same under different queueing
disciplines, in particular under the Processor Sharing discipline, for which a
dynamic reversibility property holds. We generalize Sigman's construction to
the stable case by comparing the $M/G/c$ queue to a copy run under Random
Assignment. This allows us to produce a naive perfect simulation algorithm
based on running the dominating process back to the time it first empties. We
also construct a more efficient algorithm that uses sandwiching by lower and
upper processes constructed as coupled $M/G/c$ queues started respectively from
the empty state and the state of the $M/G/c$ queue under Random Assignment. A
careful analysis shows that appropriate ordering relationships can still be
maintained, so long as service durations continue to be coupled in order of
initiation of service. We summarize statistical checks of simulation output,
and demonstrate that the mean run-time is finite so long as the second moment
of the service duration distribution is finite.
"
631,Theoretical Evaluation of Offloading through Wireless LANs,"  Offloading of cellular traffic through a wireless local area network (WLAN)
is theoretically evaluated. First, empirical data sets of the locations of WLAN
internet access points are analyzed and an inhomogeneous Poisson process
consisting of high, normal, and low density regions is proposed as a spatial
point process model for these configurations. Second, performance metrics, such
as mean available bandwidth for a user and the number of vertical handovers,
are evaluated for the proposed model through geometric analysis. Explicit
formulas are derived for the metrics, although they depend on many parameters
such as the number of WLAN access points, the shape of each WLAN coverage
region, the location of each WLAN access point, the available bandwidth (bps)
of the WLAN, and the shape and available bandwidth (bps) of each subregion
identified by the channel quality indicator in a cell of the cellular network.
Explicit formulas strongly suggest that the bandwidth a user experiences does
not depend on the user mobility. This is because the bandwidth available by a
user who does not move and that available by a user who moves are the same or
approximately the same as a probabilistic distribution. Numerical examples show
that parameters, such as the size of regions where placement of WLAN access
points is not allowed and the mean density of WLANs in high density regions,
have a large impact on performance metrics. In particular, a homogeneous
Poisson process model as the WLAN access point location model largely
overestimates the mean available bandwidth for a user and the number of
vertical handovers. The overestimated mean available bandwidth is, for example,
about 50% in a certain condition.
"
632,Bandwidth-Aware Scheduling with SDN in Hadoop: A New Trend for Big Data,"  Software Defined Networking (SDN) is a revolutionary network architecture
that separates out network control functions from the underlying equipment and
is an increasingly trend to help enterprises build more manageable data centers
where big data processing emerges as an important part of applications. To
concurrently process large-scale data, MapReduce with an open source
implementation named Hadoop is proposed. In practical Hadoop systems one kind
of issue that vitally impacts the overall performance is know as the
NP-complete minimum make span problem. One main solution is to assign tasks on
data local nodes to avoid link occupation since network bandwidth is a scarce
resource. Many methodologies for enhancing data locality are proposed such as
the HDS and state-of-the-art scheduler BAR. However, all of them either ignore
allocating tasks in a global view or disregard available bandwidth as the basis
for scheduling. In this paper we propose a heuristic bandwidth-aware task
scheduler BASS to combine Hadoop with SDN. It is not only able to guarantee
data locality in a global view but also can efficiently assign tasks in an
optimized way. Both examples and experiments demonstrate that BASS has the best
performance in terms of job completion time. To our knowledge, BASS is the
first to exploit talent of SDN for big data processing and we believe it points
out a new trend for large-scale data processing.
"
633,Performance Benefits of DataMPI: A Case Study with BigDataBench,"  Apache Hadoop and Spark are gaining prominence in Big Data processing and
analytics. Both of them are widely deployed on Internet companies. On the other
hand, high-performance data analysis requirements are causing academical and
industrial communities to adopt state-of-the-art technologies in HPC to solve
Big Data problems. Recently, we have proposed a key-value pair based
communication library, DataMPI, which is extending MPI to support
Hadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,
a Big Data benchmark suite, to do comprehensive studies on performance and
resource utilization characterizations of Hadoop, Spark and DataMPI. From our
experiments, we observe that the job execution time of DataMPI has up to 55%
and 39% speedups compared with those of Hadoop and Spark, respectively. Most of
the benefits come from the high-efficiency communication mechanisms in DataMPI.
We also notice that the resource (CPU, memory, disk and network I/O)
utilizations of DataMPI are also more efficient than those of the other two
frameworks.
"
634,"Catalog Dynamics: Impact of Content Publishing and Perishing on the
  Performance of a LRU Cache","  The Internet heavily relies on Content Distribution Networks and transparent
caches to cope with the ever-increasing traffic demand of users. Content,
however, is essentially versatile: once published at a given time, its
popularity vanishes over time. All requests for a given document are then
concentrated between the publishing time and an effective perishing time.
  In this paper, we propose a new model for the arrival of content requests,
which takes into account the dynamical nature of the content catalog. Based on
two large traffic traces collected on the Orange network, we use the
semi-experimental method and determine invariants of the content request
process. This allows us to define a simple mathematical model for content
requests; by extending the so-called ""Che approximation"", we then compute the
performance of a LRU cache fed with such a request process, expressed by its
hit ratio. We numerically validate the good accuracy of our model by comparison
to trace-based simulation.
"
635,A Survey on Network Tomography with Network Coding,"  The overhead of internal network monitoring motivates techniques of network
tomography. Network coding (NC) presents a new opportunity for network
tomography as NC introduces topology-dependent correlation that can be further
exploited in topology estimation. Compared with traditional methods, network
tomography with NC has many advantages such as the improvement of tomography
accuracy and the reduction of complexity in choosing monitoring paths. In this
paper we first introduce the problem of tomography with NC and then propose the
taxonomy criteria to classify various methods. We also present existing
solutions and future trend. We expect that our comprehensive review on network
tomography with NC can serve as a good reference for researchers and
practitioners working in the area.
"
636,Acceleration of a Full-scale Industrial CFD Application with OP2,"  Hydra is a full-scale industrial CFD application used for the design of
turbomachinery at Rolls Royce plc. It consists of over 300 parallel loops with
a code base exceeding 50K lines and is capable of performing complex
simulations over highly detailed unstructured mesh geometries. Unlike simpler
structured-mesh applications, which feature high speed-ups when accelerated by
modern processor architectures, such as multi-core and many-core processor
systems, Hydra presents major challenges in data organization and movement that
need to be overcome for continued high performance on emerging platforms. We
present research in achieving this goal through the OP2 domain-specific
high-level framework. OP2 targets the domain of unstructured mesh problems and
follows the design of an active library using source-to-source translation and
compilation to generate multiple parallel implementations from a single
high-level application source for execution on a range of back-end hardware
platforms. We chart the conversion of Hydra from its original hand-tuned
production version to one that utilizes OP2, and map out the key difficulties
encountered in the process. To our knowledge this research presents the first
application of such a high-level framework to a full scale production code.
Specifically we show (1) how different parallel implementations can be achieved
with an active library framework, even for a highly complicated industrial
application such as Hydra, and (2) how different optimizations targeting
contrasting parallel architectures can be applied to the whole application,
seamlessly, reducing developer effort and increasing code longevity.
Performance results demonstrate that not only the same runtime performance as
that of the hand-tuned original production code could be achieved, but it can
be significantly improved on conventional processor systems. Additionally, we
achieve further...
"
637,Cache-aware Parallel Programming for Manycore Processors,"  With rapidly evolving technology, multicore and manycore processors have
emerged as promising architectures to benefit from increasing transistor
numbers. The transition towards these parallel architectures makes today an
exciting time to investigate challenges in parallel computing. The TILEPro64 is
a manycore accelerator, composed of 64 tiles interconnected via multiple 8x8
mesh networks. It contains per-tile caches and supports cache-coherent shared
memory by default. In this paper we present a programming technique to take
advantages of distributed caching facilities in manycore processors. However,
unlike other work in this area, our approach does not use architecture-specific
libraries. Instead, we provide the programmer with a novel technique on how to
program future Non-Uniform Cache Architecture (NUCA) manycore systems, bearing
in mind their caching organisation. We show that our localised programming
approach can result in a significant improvement of the parallelisation
efficiency (speed-up).
"
638,"An Efficient Thread Mapping Strategy for Multiprogramming on Manycore
  Processors","  The emergence of multicore and manycore processors is set to change the
parallel computing world. Applications are shifting towards increased
parallelism in order to utilise these architectures efficiently. This leads to
a situation where every application creates its desirable number of threads,
based on its parallel nature and the system resources allowance. Task
scheduling in such a multithreaded multiprogramming environment is a
significant challenge. In task scheduling, not only the order of the execution,
but also the mapping of threads to the execution resources is of a great
importance. In this paper we state and discuss some fundamental rules based on
results obtained from selected applications of the BOTS benchmarks on the
64-core TILEPro64 processor. We demonstrate how previously efficient mapping
policies such as those of the SMP Linux scheduler become inefficient when the
number of threads and cores grows. We propose a novel, low-overhead technique,
a heuristic based on the amount of time spent by each CPU doing some useful
work, to fairly distribute the workloads amongst the cores in a
multiprogramming environment. Our novel approach could be implemented as a
pragma similar to those in the new task-based OpenMP versions, or can be
incorporated as a distributed thread mapping mechanism in future manycore
programming frameworks. We show that our thread mapping scheme can outperform
the native GNU/Linux thread scheduler in both single-programming and
multiprogramming environments.
"
639,"A model-driven approach to broaden the detection of software performance
  antipatterns at runtime","  Performance antipatterns document bad design patterns that have negative
influence on system performance. In our previous work we formalized such
antipatterns as logical predicates that predicate on four views: (i) the static
view that captures the software elements (e.g. classes, components) and the
static relationships among them; (ii) the dynamic view that represents the
interaction (e.g. messages) that occurs between the software entities elements
to provide the system functionalities; (iii) the deployment view that describes
the hardware elements (e.g. processing nodes) and the mapping of the software
entities onto the hardware platform; (iv) the performance view that collects
specific performance indices. In this paper we present a lightweight
infrastructure that is able to detect performance antipatterns at runtime
through monitoring. The proposed approach precalculates such predicates and
identifies antipatterns whose static, dynamic and deployment sub-predicates are
validated by the current system configuration and brings at runtime the
verification of performance sub-predicates. The proposed infrastructure
leverages model-driven techniques to generate probes for monitoring the
performance sub-predicates and detecting antipatterns at runtime.
"
640,Analysis of Petri Net Models through Stochastic Differential Equations,"  It is well known, mainly because of the work of Kurtz, that density dependent
Markov chains can be approximated by sets of ordinary differential equations
(ODEs) when their indexing parameter grows very large. This approximation
cannot capture the stochastic nature of the process and, consequently, it can
provide an erroneous view of the behavior of the Markov chain if the indexing
parameter is not sufficiently high. Important phenomena that cannot be revealed
include non-negligible variance and bi-modal population distributions. A
less-known approximation proposed by Kurtz applies stochastic differential
equations (SDEs) and provides information about the stochastic nature of the
process. In this paper we apply and extend this diffusion approximation to
study stochastic Petri nets. We identify a class of nets whose underlying
stochastic process is a density dependent Markov chain whose indexing parameter
is a multiplicative constant which identifies the population level expressed by
the initial marking and we provide means to automatically construct the
associated set of SDEs. Since the diffusion approximation of Kurtz considers
the process only up to the time when it first exits an open interval, we extend
the approximation by a machinery that mimics the behavior of the Markov chain
at the boundary and allows thus to apply the approach to a wider set of
problems. The resulting process is of the jump-diffusion type. We illustrate by
examples that the jump-diffusion approximation which extends to bounded domains
can be much more informative than that based on ODEs as it can provide accurate
quantity distributions even when they are multi-modal and even for relatively
small population levels. Moreover, we show that the method is faster than
simulating the original Markov chain.
"
641,Batch Arrival Multiserver Queue with Setup Time,"  Queues with setup time are extensively studied because they have application
in performance evaluation of power-saving data centers. In a data center, there
are a huge number of servers which consume a large amount of energy. In the
current technology, an idle server still consumes about 60\% of its peak
processing a job. Thus, the only way to save energy is to turn off servers
which are not processing a job. However, when there are some waiting jobs, we
have to turn on the OFF servers. A server needs some setup time to be active
during which it consumes energy but cannot process a job. Therefore, there
exists a trade-off between power consumption and delay performance. Gandhi et
al. \cite{Gandhi10a,Gandhi10} analyze this trade-off using an M/M/$c$ queue
with staggered setup (one server in setup at a time). In this paper, using an
alternative approach, we obtain generating functions for the joint stationary
distribution of the number of active servers and that of jobs in the system for
a more general model with batch arrivals and state-dependent setup time. We
further obtain moments for the queue size. Numerical results reveal that
keeping the same traffic intensity, the mean power consumption decreases with
the mean batch size for the case of fixed batch size. One of the main
theoretical contribution is a new conditional decomposition formula showing
that the number of waiting customers under the condition that all servers are
busy can be decomposed to the sum of two independent random variables where the
first is the same quantity in the corresponding model without setup time while
the second is the number of waiting customers before an arbitrary customer.
"
642,Enhanced Cluster Computing Performance Through Proportional Fairness,"  The performance of cluster computing depends on how concurrent jobs share
multiple data center resource types like CPU, RAM and disk storage. Recent
research has discussed efficiency and fairness requirements and identified a
number of desirable scheduling objectives including so-called dominant resource
fairness (DRF). We argue here that proportional fairness (PF), long recognized
as a desirable objective in sharing network bandwidth between ongoing flows, is
preferable to DRF. The superiority of PF is manifest under the realistic
modelling assumption that the population of jobs in progress is a stochastic
process. In random traffic the strategy-proof property of DRF proves
unimportant while PF is shown by analysis and simulation to offer a
significantly better efficiency-fairness tradeoff.
"
643,"Joint Channel Assignment and Opportunistic Routing for Maximizing
  Throughput in Cognitive Radio Networks","  In this paper, we consider the joint opportunistic routing and channel
assignment problem in multi-channel multi-radio (MCMR) cognitive radio networks
(CRNs) for improving aggregate throughput of the secondary users. We first
present the nonlinear programming optimization model for this joint problem,
taking into account the feature of CRNs-channel uncertainty. Then considering
the queue state of a node, we propose a new scheme to select proper forwarding
candidates for opportunistic routing. Furthermore, a new algorithm for
calculating the forwarding probability of any packet at a node is proposed,
which is used to calculate how many packets a forwarder should send, so that
the duplicate transmission can be reduced compared with MAC-independent
opportunistic routing & encoding (MORE) [11]. Our numerical results show that
the proposed scheme performs significantly better that traditional routing and
opportunistic routing in which channel assignment strategy is employed.
"
644,"Control of parallel non-observable queues: asymptotic equivalence and
  optimality of periodic policies","  We consider a queueing system composed of a dispatcher that routes
deterministically jobs to a set of non-observable queues working in parallel.
In this setting, the fundamental problem is which policy should the dispatcher
implement to minimize the stationary mean waiting time of the incoming jobs. We
present a structural property that holds in the classic scaling of the system
where the network demand (arrival rate of jobs) grows proportionally with the
number of queues. Assuming that each queue of type $r$ is replicated $k$ times,
we consider a set of policies that are periodic with period $k \sum_r p_r$ and
such that exactly $p_r$ jobs are sent in a period to each queue of type $r$.
When $k\to\infty$, our main result shows that all the policies in this set are
equivalent, in the sense that they yield the same mean stationary waiting time,
and optimal, in the sense that no other policy having the same aggregate
arrival rate to \emph{all} queues of a given type can do better in minimizing
the stationary mean waiting time. This property holds in a strong probabilistic
sense. Furthermore, the limiting mean waiting time achieved by our policies is
a convex function of the arrival rate in each queue, which facilitates the
development of a further optimization aimed at solving the fundamental problem
above for large systems.
"
645,"On Time-Sensitive Revenue Management and Energy Scheduling in Green Data
  Centers","  In this paper, we design an analytically and experimentally better online
energy and job scheduling algorithm with the objective of maximizing net profit
for a service provider in green data centers. We first study the previously
known algorithms and conclude that these online algorithms have provable poor
performance against their worst-case scenarios. To guarantee an online
algorithm's performance in hindsight, we design a randomized algorithm to
schedule energy and jobs in the data centers and prove the algorithm's expected
competitive ratio in various settings. Our algorithm is theoretical-sound and
it outperforms the previously known algorithms in many settings using both real
traces and simulated data. An optimal offline algorithm is also implemented as
an empirical benchmark.
"
646,"SleepScale: Runtime Joint Speed Scaling and Sleep States Management for
  Power Efficient Data Centers","  Power consumption in data centers has been growing significantly in recent
years. To reduce power, servers are being equipped with increasingly
sophisticated power management mechanisms. Different mechanisms offer
dramatically different trade-offs between power savings and performance
penalties. Considering the complexity, variety, and temporally varying nature
of the applications hosted in a typical data center, intelligently determining
which power management policy to use and when is a complicated task.
  In this paper we analyze a system model featuring both performance scaling
and low-power states. We reveal the interplay between performance scaling and
low-power states via intensive simulation and analytic verification. Based on
the observations, we present SleepScale, a runtime power management tool
designed to efficiently exploit existing power control mechanisms. At run time,
SleepScale characterizes power consumption and quality-of-service (QoS) for
each low-power state and frequency setting, and selects the best policy for a
given QoS constraint. We evaluate SleepScale using workload traces from data
centers and achieve significant power savings relative to conventional power
management strategies.
"
647,Degradation Analysis of Probabilistic Parallel Choice Systems,"  Degradation analysis is used to analyze the useful lifetimes of systems,
their failure rates, and various other system parameters like mean time to
failure (MTTF), mean time between failures (MTBF), and the system failure rate
(SFR). In many systems, certain possible parallel paths of execution that have
greater chances of success are preferred over others. Thus we introduce here
the concept of probabilistic parallel choice. We use binary and $n$-ary
probabilistic choice operators in describing the selections of parallel paths.
These binary and $n$-ary probabilistic choice operators are considered so as to
represent the complete system (described as a series-parallel system) in terms
of the probabilities of selection of parallel paths and their relevant
parameters. Our approach allows us to derive new and generalized formulae for
system parameters like MTTF, MTBF, and SFR. We use a generalized exponential
distribution, allowing distinct installation times for individual components,
and use this model to derive expressions for such system parameters.
"
648,A Parallel Task-based Approach to Linear Algebra,"  Processors with large numbers of cores are becoming commonplace. In order to
take advantage of the available resources in these systems, the programming
paradigm has to move towards increased parallelism. However, increasing the
level of concurrency in the program does not necessarily lead to better
performance. Parallel programming models have to provide flexible ways of
defining parallel tasks and at the same time, efficiently managing the created
tasks. OpenMP is a widely accepted programming model for shared-memory
architectures. In this paper we highlight some of the drawbacks in the OpenMP
tasking approach, and propose an alternative model based on the Glasgow
Parallel Reduction Machine (GPRM) programming framework. As the main focus of
this study, we deploy our model to solve a fundamental linear algebra problem,
LU factorisation of sparse matrices. We have used the SparseLU benchmark from
the BOTS benchmark suite, and compared the results obtained from our model to
those of the OpenMP tasking approach. The TILEPro64 system has been used to run
the experiments. The results are very promising, not only because of the
performance improvement for this particular problem, but also because they
verify the task management efficiency, stability, and flexibility of our model,
which can be applied to solve problems in future many-core systems.
"
649,"When Queueing Meets Coding: Optimal-Latency Data Retrieving Scheme in
  Storage Clouds","  In this paper, we study the problem of reducing the delay of downloading data
from cloud storage systems by leveraging multiple parallel threads, assuming
that the data has been encoded and stored in the clouds using fixed rate
forward error correction (FEC) codes with parameters (n, k). That is, each file
is divided into k equal-sized chunks, which are then expanded into n chunks
such that any k chunks out of the n are sufficient to successfully restore the
original file. The model can be depicted as a multiple-server queue with
arrivals of data retrieving requests and a server corresponding to a thread.
However, this is not a typical queueing model because a server can terminate
its operation, depending on when other servers complete their service (due to
the redundancy that is spread across the threads). Hence, to the best of our
knowledge, the analysis of this queueing model remains quite uncharted.
  Recent traces from Amazon S3 show that the time to retrieve a fixed size
chunk is random and can be approximated as a constant delay plus an i.i.d.
exponentially distributed random variable. For the tractability of the
theoretical analysis, we assume that the chunk downloading time is i.i.d.
exponentially distributed. Under this assumption, we show that any
work-conserving scheme is delay-optimal among all on-line scheduling schemes
when k = 1. When k > 1, we find that a simple greedy scheme, which allocates
all available threads to the head of line request, is delay optimal among all
on-line scheduling schemes. We also provide some numerical results that point
to the limitations of the exponential assumption, and suggest further research
directions.
"
650,Separation of timescales in a two-layered network,"  We investigate a computer network consisting of two layers occurring in, for
example, application servers. The first layer incorporates the arrival of jobs
at a network of multi-server nodes, which we model as a many-server Jackson
network. At the second layer, active servers at these nodes act now as
customers who are served by a common CPU. Our main result shows a separation of
time scales in heavy traffic: the main source of randomness occurs at the
(aggregate) CPU layer; the interactions between different types of nodes at the
other layer is shown to converge to a fixed point at a faster time scale; this
also yields a state-space collapse property. Apart from these fundamental
insights, we also obtain an explicit approximation for the joint law of the
number of jobs in the system, which is provably accurate for heavily loaded
systems and performs numerically well for moderately loaded systems. The
obtained results for the model under consideration can be applied to
thread-pool dimensioning in application servers, while the technique seems
applicable to other layered systems too.
"
651,"Asymptotic and Numerical Analysis of Multiserver Retrial Queue with
  Guard Channel for Cellular Networks","  This paper considers a retrial queueing model for a base station in cellular
networks where fresh calls and handover calls are available. Fresh calls are
initiated from the cell of the base station. On the other hand, a handover call
has been connecting to a base station and moves to another one. In order to
keep the continuation of the communication, it is desired that an available
channel in the new base station is immediately assigned to the handover call.
To this end, a channel is reserved as the guard channel for handover calls in
base stations. Blocked fresh and handover calls join a virtual orbit and repeat
their attempts in a later time. We assume that a base station can recognize
retrial calls and give them the same priority as that of handover calls. We
model a base station by a multiserver retrial queue with priority customers for
which a level-dependent QBD process is formulated. We obtain Taylor series
expansion for the nonzero elements of the rate matrices of the level-dependent
QBD. Using the expansion results, we obtain an asymptotic upper bound for the
joint stationary distribution of the number of busy channels and that of
customers in the orbit. Furthermore, we derive an efficient numerical algorithm
to calculate the joint stationary distribution.
"
652,"Performance Provisioning and Energy Efficiency in Cloud and Distributed
  Computing Systems","  In recent years, the issue of energy consumption in high performance
computing (HPC) systems has attracted a great deal of attention. In response to
this, many energy-aware algorithms have been developed in different layers of
HPC systems, including the hardware layer, service layer and system layer.
These algorithms are of two types: first, algorithms which directly try to
improve the energy by tweaking frequency operation or scheduling algorithms;
and second, algorithms which focus on improving the performance of the system,
with the assumption that efficient running of a system may indirectly save more
energy.
  In this thesis, we develop algorithms in both layers. First, we introduce
three algorithms to directly improve the energy of scheduled tasks at the
hardware level by using Dynamic Voltage Frequency Scaling (DVFS). Second, we
propose two algorithms for modelling and resource provisioning of MapReduce
applications (a well-known parametric distributed framework currently used by
Google, Yahoo, Facebook and LinkedIn) based on its configuration parameters.
Certainly, estimating the performance (e.g., execution time or CPU clock ticks)
of a MapReduce application can be later used for smart scheduling of such
applications in clouds or clusters.
  To evaluate the algorithms, we have conducted extensive simulation and real
experiments on a 5-node physical cluster with up to 25 virtual nodes, using
both synthetic and real world applications. Also, the proposed new algorithms
are compared with existing algorithms by experimentation, and the experimental
results reveal new information on the performance of these algorithms, as well
as on the properties of MapReduce and DVFS. In the end, three open problems are
revealed by the experimental observations, and their importance is explained.
"
653,Signalling Storms in 3G Mobile Networks,"  We review the characteristics of signalling storms that have been caused by
certain common apps and recently observed in cellular networks, leading to
system outages. We then develop a mathematical model of a mobile user's
signalling behaviour which focuses on the potential of causing such storms, and
represent it by a large Markov chain. The analysis of this model allows us to
determine the key parameters of mobile user device behaviour that can lead to
signalling storms. We then identify the parameter values that will lead to
worst case load for the network itself in the presence of such storms. This
leads to explicit results regarding the manner in which individual mobile
behaviour can cause overload conditions on the network and its signalling
servers, and provides insight into how this may be avoided.
"
654,Automatic Detection of Performance Anomalies in Task-Parallel Programs,"  To efficiently exploit the resources of new many-core architectures,
integrating dozens or even hundreds of cores per chip, parallel programming
models have evolved to expose massive amounts of parallelism, often in the form
of fine-grained tasks. Task-parallel languages, such as OpenStream, X10,
Habanero Java and C or StarSs, simplify the development of applications for new
architectures, but tuning task-parallel applications remains a major challenge.
Performance bottlenecks can occur at any level of the implementation, from the
algorithmic level (e.g., lack of parallelism or over-synchronization), to
interactions with the operating and runtime systems (e.g., data placement on
NUMA architectures), to inefficient use of the hardware (e.g., frequent cache
misses or misaligned memory accesses); detecting such issues and determining
the exact cause is a difficult task.
  In previous work, we developed Aftermath, an interactive tool for trace-based
performance analysis and debugging of task-parallel programs and run-time
systems. In contrast to other trace-based analysis tools, such as Paraver or
Vampir, Aftermath offers native support for tasks, i.e., visualization,
statistics and analysis tools adapted for performance debugging at task
granularity. However, the tool currently does not provide support for the
automatic detection of performance bottlenecks and it is up to the user to
investigate the relevant aspects of program execution by focusing the
inspection on specific slices of a trace file. In this paper, we present
ongoing work on two extensions that guide the user through this process.
"
655,"Mean-Field approximation and Quasi-Equilibrium reduction of Markov
  Population Models","  Markov Population Model is a commonly used framework to describe stochastic
systems. Their exact analysis is unfeasible in most cases because of the state
space explosion. Approximations are usually sought, often with the goal of
reducing the number of variables. Among them, the mean field limit and the
quasi-equilibrium approximations stand out. We view them as techniques that are
rooted in independent basic principles. At the basis of the mean field limit is
the law of large numbers. The principle of the quasi-equilibrium reduction is
the separation of temporal scales. It is common practice to apply both limits
to an MPM yielding a fully reduced model. Although the two limits should be
viewed as completely independent options, they are applied almost invariably in
a fixed sequence: MF limit first, QE-reduction second. We present a framework
that makes explicit the distinction of the two reductions, and allows an
arbitrary order of their application. By inverting the sequence, we show that
the double limit does not commute in general: the mean field limit of a
time-scale reduced model is not the same as the time-scale reduced limit of a
mean field model. An example is provided to demonstrate this phenomenon.
Sufficient conditions for the two operations to be freely exchangeable are also
provided.
"
656,Parallelism Via Concurrency at Multiple Levels,"  In this paper we examine the key elements determining the best performance of
computing by increasing the frequency of a single chip and to get the minimum
latency during execution of the programs to achieve best possible output. It is
not enough to provide concurrent improvements in the hardware as Software also
have to introduce concurrency in order to exploit the parallelism. The software
parallelism is defined by the control and data dependency of programs whereas
Hardware refers to the type of parallelism defined by the machine architecture
and hardware multiplicity.
"
657,Block-Structured Supermarket Models,"  Supermarket models are a class of parallel queueing networks with an adaptive
control scheme that play a key role in the study of resource management of,
such as, computer networks, manufacturing systems and transportation networks.
When the arrival processes are non-Poisson and the service times are
non-exponential, analysis of such a supermarket model is always limited,
interesting, and challenging.
  This paper describes a supermarket model with non-Poisson inputs: Markovian
Arrival Processes (MAPs) and with non-exponential service times: Phase-type
(PH) distributions, and provides a generalized matrix-analytic method which is
first combined with the operator semigroup and the mean-field limit. When
discussing such a more general supermarket model, this paper makes some new
results and advances as follows: (1) Providing a detailed probability analysis
for setting up an infinite-dimensional system of differential vector equations
satisfied by the expected fraction vector, where ""the invariance of environment
factors"" is given as an important result. (2) Introducing the phase-type
structure to the operator semigroup and to the mean-field limit, and a
Lipschitz condition can be obtained by means of a unified matrix-differential
algorithm. (3) The matrix-analytic method is used to compute the fixed point
which leads to performance computation of this system. Finally, we use some
numerical examples to illustrate how the performance measures of this
supermarket model depend on the non-Poisson inputs and on the non-exponential
service times. Thus the results of this paper give new highlight on
understanding influence of non-Poisson inputs and of non-exponential service
times on performance measures of more general supermarket models.
"
658,"Approximate analysis of biological systems by hybrid switching jump
  diffusion","  In this paper we consider large state space continuous time Markov chains
(MCs) arising in the field of systems biology. For density dependent families
of MCs that represent the interaction of large groups of identical objects,
Kurtz has proposed two kinds of approximations. One is based on ordinary
differential equations, while the other uses a diffusion process. The
computational cost of the deterministic approximation is significantly lower,
but the diffusion approximation retains stochasticity and is able to reproduce
relevant random features like variance, bimodality, and tail behavior. In a
recent paper, for particular stochastic Petri net models, we proposed a jump
diffusion approximation that aims at being applicable beyond the limits of
Kurtz's diffusion approximation, namely when the process reaches the boundary
with non-negligible probability. Other limitations of the diffusion
approximation in its original form are that it can provide inaccurate results
when the number of objects in some groups is often or constantly low and that
it can be applied only to pure density dependent Markov chains. In order to
overcome these drawbacks, in this paper we propose to apply the jump-diffusion
approximation only to those components of the model that are in density
dependent form and are associated with high population levels. The remaining
components are treated as discrete quantities. The resulting process is a
hybrid switching jump diffusion. We show that the stochastic differential
equations that characterize this process can be derived automatically both from
the description of the original Markov chains or starting from a higher level
description language, like stochastic Petri nets. The proposed approach is
illustrated on three models: one modeling the so called crazy clock reaction,
one describing viral infection kinetics and the last considering transcription
regulation.
"
659,"Proceedings Twelfth International Workshop on Quantitative Aspects of
  Programming Languages and Systems","  This volume contains the proceedings of the Twelfth Workshop on Quantitative
Aspects of Programming Languages and Systems (QAPL 2014), held in Grenoble,
France, on 12 and 13 April, 2014. QAPL 2014 was a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS). The
central theme of the workshop is that of quantitative aspects of computation.
These aspects are related to the use of physical quantities (storage space,
time, bandwidth, etc.) as well as mathematical quantities (e.g. probability and
measures for reliability, security and trust), and play an important (sometimes
essential) role in characterising the behaviour and determining the properties
of systems. Such quantities are central to the definition of both the model of
systems (architecture, language design, semantics) and the methodologies and
tools for the analysis and verification of the systems properties. The aim of
this workshop is to discuss the explicit use of quantitative information such
as time and probabilities either directly in the model or as a tool for the
analysis of systems.
"
660,"Extended Differential Aggregations in Process Algebra for Performance
  and Biology","  We study aggregations for ordinary differential equations induced by fluid
semantics for Markovian process algebra which can capture the dynamics of
performance models and chemical reaction networks. Whilst previous work has
required perfect symmetry for exact aggregation, we present approximate fluid
lumpability, which makes nearby processes perfectly symmetric after a
perturbation of their parameters. We prove that small perturbations yield
nearby differential trajectories. Numerically, we show that many heterogeneous
processes can be aggregated with negligible errors.
"
661,"Patch-based Hybrid Modelling of Spatially Distributed Systems by Using
  Stochastic HYPE - ZebraNet as an Example","  Individual-based hybrid modelling of spatially distributed systems is usually
expensive. Here, we consider a hybrid system in which mobile agents spread over
the space and interact with each other when in close proximity. An
individual-based model for this system needs to capture the spatial attributes
of every agent and monitor the interaction between each pair of them. As a
result, the cost of simulating this model grows exponentially as the number of
agents increases. For this reason, a patch-based model with more abstraction
but better scalability is advantageous. In a patch-based model, instead of
representing each agent separately, we model the agents in a patch as an
aggregation. This property significantly enhances the scalability of the model.
In this paper, we convert an individual-based model for a spatially distributed
network system for wild-life monitoring, ZebraNet, to a patch-based stochastic
HYPE model with accurate performance evaluation. We show the ease and
expressiveness of stochastic HYPE for patch-based modelling of hybrid systems.
Moreover, a mean-field analytical model is proposed as the fluid flow
approximation of the stochastic HYPE model, which can be used to investigate
the average behaviour of the modelled system over an infinite number of
simulation runs of the stochastic HYPE model.
"
662,Formal and Informal Methods for Multi-Core Design Space Exploration,"  We propose a tool-supported methodology for design-space exploration for
embedded systems. It provides means to define high-level models of applications
and multi-processor architectures and evaluate the performance of different
deployment (mapping, scheduling) strategies while taking uncertainty into
account. We argue that this extension of the scope of formal verification is
important for the viability of the domain.
"
663,Exact Solutions for M/M/c/Setup Queues,"  Recently multiserver queues with setup times have been extensively studied
because they have applications in power-saving data centers. The most
challenging model is the M/M/$c$/Setup queue where a server is turned off when
it is idle and is turned on if there are some waiting jobs. Recently, Gandhi et
al.~(SIGMETRICS 2013, QUESTA 2014) present the recursive renewal reward
approach as a new mathematical tool to analyze the model. In this paper, we
derive exact solutions for the same model using two alternative methodologies:
generating function approach and matrix analytic method. The former yields
several theoretical insights into the systems while the latter provides an
exact recursive algorithm to calculate the joint stationary distribution and
then some performance measures so as to give new application insights.
"
664,A hybrid neuro--wavelet predictor for QoS control and stability,"  For distributed systems to properly react to peaks of requests, their
adaptation activities would benefit from the estimation of the amount of
requests. This paper proposes a solution to produce a short-term forecast based
on data characterising user behaviour of online services. We use \emph{wavelet
analysis}, providing compression and denoising on the observed time series of
the amount of past user requests; and a \emph{recurrent neural network} trained
with observed data and designed so as to provide well-timed estimations of
future requests. The said ensemble has the ability to predict the amount of
future user requests with a root mean squared error below 0.06\%. Thanks to
prediction, advance resource provision can be performed for the duration of a
request peak and for just the right amount of resources, hence avoiding
over-provisioning and associated costs. Moreover, reliable provision lets users
enjoy a level of availability of services unaffected by load variations.
"
665,"Near-Optimal Virtual Machine Packing Based on Resource Requirement of
  Service Demands Using Pattern Clustering","  Upon the expansion of Cloud Computing and the positive outlook of
organizations with regard to the movements towards using cloud computing and
their expanding utilization of such valuable processing method, as well as the
solutions provided by the cloud infrastructure providers with regard to the
reduction of the costs of processing resources, the problem of organizing
resources in a cloud environment gained a high importance. One of the major
preoccupations of the minds of cloud infrastructure clients is their lack of
knowledge on the quantity of their required processing resources in different
periods of time. The managers and technicians are trying to make the most use
of scalability and the flexibility of the resources in cloud computing. The
main challenge is with calculating the amount of the required processing
resources per moment with regard to the quantity of incoming requests of the
service. Through deduction of the accurate amount of these items, one can have
an accurate estimation of the requests per moment. This paper aims at
introducing a model for automatic scaling of the cloud resources that would
reduce the cost of renting the resources for the clients of cloud
infrastructure. Thus, first we start with a thorough explanation of the
proposal and the major components of the model. Then through calculating the
incomings of the model through clustering and introducing the way that each of
these components work in different phases,...
"
666,Dealing with Zero Density Using Piecewise Phase-type Approximation,"  Every probability distribution can be approximated up to a given precision by
a phase-type distribution, i.e. a distribution encoded by a continuous time
Markov chain (CTMC). However, an excessive number of states in the
corresponding CTMC is needed for some standard distributions, in particular
most distributions with regions of zero density such as uniform or shifted
distributions. Addressing this class of distributions, we suggest an
alternative representation by CTMC extended with discrete-time transitions.
Using discrete-time transitions we split the density function into multiple
intervals. Within each interval, we then approximate the density with standard
phase-type fitting. We provide an experimental evidence that our method
requires only a moderate number of states to approximate such distributions
with regions of zero density. Furthermore, the usage of CTMC with discrete-time
transitions is supported by a number of techniques for their analysis. Thus,
our results promise an efficient approach to the transient analysis of a class
of non-Markovian models.
"
667,"Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist
  Genetic Algorithm","  Exploration of task mappings plays a crucial role in achieving high
performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.
The problem of optimally mapping a set of tasks onto a set of given
heterogeneous processors for maximal throughput has been known, in general, to
be NP-complete. The problem is further exacerbated when multiple applications
(i.e., bigger task sets) and the communication between tasks are also
considered. Previous research has shown that Genetic Algorithms (GA) typically
are a good choice to solve this problem when the solution space is relatively
small. However, when the size of the problem space increases, classic genetic
algorithms still suffer from the problem of long evolution times. To address
this problem, this paper proposes a novel bias-elitist genetic algorithm that
is guided by domain-specific heuristics to speed up the evolution process.
Experimental results reveal that our proposed algorithm is able to handle large
scale task mapping problems and produces high-quality mapping solutions in only
a short time period.
"
668,COFFEE: an Optimizing Compiler for Finite Element Local Assembly,"  The numerical solution of partial differential equations using the finite
element method is one of the key applications of high performance computing.
Local assembly is its characteristic operation. This entails the execution of a
problem-specific kernel to numerically evaluate an integral for each element in
the discretized problem domain. Since the domain size can be huge, executing
efficient kernels is fundamental. Their op- timization is, however, a
challenging issue. Even though affine loop nests are generally present, the
short trip counts and the complexity of mathematical expressions make it hard
to determine a single or unique sequence of successful transformations.
Therefore, we present the design and systematic evaluation of COF- FEE, a
domain-specific compiler for local assembly kernels. COFFEE manipulates
abstract syntax trees generated from a high-level domain-specific language for
PDEs by introducing domain-aware composable optimizations aimed at improving
instruction-level parallelism, especially SIMD vectorization, and register
locality. It then generates C code including vector intrinsics. Experiments
using a range of finite-element forms of increasing complexity show that
significant performance improvement is achieved.
"
669,"Analysis and Approximation of Dual Tandem Queues with Finite Buffer
  Capacity","  Tandem queues with finite buffer capacity commonly exist in practical
applications. By viewing a tandem queue as an integrated system, an innovative
approach has been developed to analyze its performance through the insight from
reduction method. In our approach, the starvation at the bottleneck caused by
service time randomness is modeled and captured by interruptions. Fundamental
properties of tandem queues with finite buffer capacity are examined. We show
that in general system service rate of a dual tandem queue with finite buffer
capacity is equal or smaller than its bottleneck service rate, and virtual
interruptions, which are the extra idle period at the bottleneck caused by the
non-bottlenecks, depend on arrival rates. Hence, system service rate is a
function of arrival rate when the buffer capacity of a tandem queue is finite.
Approximation for the mean queue time of a dual tandem queue has been developed
through the concept of virtual interruptions.
"
670,"Optimizing Performance of Continuous-Time Stochastic Systems using
  Timeout Synthesis","  We consider parametric version of fixed-delay continuous-time Markov chains
(or equivalently deterministic and stochastic Petri nets, DSPN) where
fixed-delay transitions are specified by parameters, rather than concrete
values. Our goal is to synthesize values of these parameters that, for a given
cost function, minimise expected total cost incurred before reaching a given
set of target states. We show that under mild assumptions, optimal values of
parameters can be effectively approximated using translation to a Markov
decision process (MDP) whose actions correspond to discretized values of these
parameters.
"
671,"Synthetic Generation of Solar States for Smart Grid: A Multiple Segment
  Markov Chain Apptoach","  The use of photovoltaic (PV) sources is becoming very popular in smart grid
for their ecological benefits, with higher scalability and utilization for
local generation and delivery. PV can also potentially avoid the energy losses
that are normally associated with long-range grid distribution. The increased
penetration of solar panels, however, has introduced a need for solar energy
models that are capable of producing realistic synthetic data with small error
margins. Such models, for instance, can be used to design the appropriate size
of energy storage devices or to determine the maximum charging rate of a
PV-powered electric vehicle (EV) charging station. In this regard, this paper
proposes a stochastic model for solar generation using a Markov chain approach.
Based on real data, it is first shown that the solar states are
inter-dependent, and thus suitable for modeling using a Markov model. Then, the
probabilities of transition between states are shown to be heterogeneous over
different time segments. A model is proposed that captures the inter temporal
dependency of solar irradiance through segmentation of the Markov chain across
different times of the day. In the studied model, different state transition
matrices are constructed for different time segments, which the proposed
algorithm then uses to generate the solar states for different times of the
day. Numerical examples are provided to show the effectiveness of the proposed
synthetic generator.
"
672,Program Synthesis and Linear Operator Semantics,"  For deterministic and probabilistic programs we investigate the problem of
program synthesis and program optimisation (with respect to non-functional
properties) in the general setting of global optimisation. This approach is
based on the representation of the semantics of programs and program fragments
in terms of linear operators, i.e. as matrices. We exploit in particular the
fact that we can automatically generate the representation of the semantics of
elementary blocks. These can then can be used in order to compositionally
assemble the semantics of a whole program, i.e. the generator of the
corresponding Discrete Time Markov Chain (DTMC). We also utilise a generalised
version of Abstract Interpretation suitable for this linear algebraic or
functional analytical framework in order to formulate semantical constraints
(invariants) and optimisation objectives (for example performance
requirements).
"
673,"Parallelism-Aware Memory Interference Delay Analysis for COTS Multicore
  Systems","  In modern Commercial Off-The-Shelf (COTS) multicore systems, each core can
generate many parallel memory requests at a time. The processing of these
parallel requests in the DRAM controller greatly affects the memory
interference delay experienced by running tasks on the platform. In this paper,
we model a modern COTS multicore system which has a nonblocking last-level
cache (LLC) and a DRAM controller that prioritizes reads over writes. To
minimize interference, we focus on LLC and DRAM bank partitioned systems. Based
on the model, we propose an analysis that computes a safe upper bound for the
worst-case memory interference delay. We validated our analysis on a real COTS
multicore platform with a set of carefully designed synthetic benchmarks as
well as SPEC2006 benchmarks. Evaluation results show that our analysis is more
accurately capture the worst-case memory interference delay and provides safer
upper bounds compared to a recently proposed analysis which significantly
under-estimate the delay.
"
674,"Quantifying the Effect of Matrix Structure on Multithreaded Performance
  of the SpMV Kernel","  Sparse matrix-vector multiplication (SpMV) is the core operation in many
common network and graph analytics, but poor performance of the SpMV kernel
handicaps these applications. This work quantifies the effect of matrix
structure on SpMV performance, using Intel's VTune tool for the Sandy Bridge
architecture. Two types of sparse matrices are considered: finite difference
(FD) matrices, which are structured, and R-MAT matrices, which are
unstructured. Analysis of cache behavior and prefetcher activity reveals that
the SpMV kernel performs far worse with R-MAT matrices than with FD matrices,
due to the difference in matrix structure. To address the problems caused by
unstructured matrices, novel architecture improvements are proposed.
"
675,An Alternating Direction Method Approach to Cloud Traffic Management,"  In this paper, we introduce a unified framework for studying various cloud
traffic management problems, ranging from geographical load balancing to
backbone traffic engineering. We first abstract these real-world problems as a
multi-facility resource allocation problem, and then present two distributed
optimization algorithms by exploiting the special structure of the problem. Our
algorithms are inspired by Alternating Direction Method of Multipliers (ADMM),
enjoying a number of unique features. Compared to dual decomposition, they
converge with non-strictly convex objective functions; compared to other
ADMM-type algorithms, they not only achieve faster convergence under weaker
assumptions, but also have lower computational complexity and lower
message-passing overhead. The simulation results not only confirm these
desirable features of our algorithms, but also highlight several additional
advantages, such as scalability and fault-tolerance.
"
676,A Two-Queue Polling Model with Two Priority Levels in the First Queue,"  In this paper we consider a single-server cyclic polling system consisting of
two queues. Between visits to successive queues, the server is delayed by a
random switch-over time. Two types of customers arrive at the first queue: high
and low priority customers. For this situation the following service
disciplines are considered: gated, globally gated, and exhaustive. We study the
cycle time distribution, the waiting times for each customer type, the joint
queue length distribution at polling epochs, and the steady-state marginal
queue length distributions for each customer type.
"
677,Mixed Gated/Exhaustive Service in a Polling Model with Priorities,"  In this paper we consider a single-server polling system with switch-over
times. We introduce a new service discipline, mixed gated/exhaustive service,
that can be used for queues with two types of customers: high and low priority
customers. At the beginning of a visit of the server to such a queue, a gate is
set behind all customers. High priority customers receive priority in the sense
that they are always served before any low priority customers. But high
priority customers have a second advantage over low priority customers. Low
priority customers are served according to the gated service discipline, i.e.
only customers standing in front of the gate are served during this visit. In
contrast, high priority customers arriving during the visit period of the queue
are allowed to pass the gate and all low priority customers before the gate.
  We study the cycle time distribution, the waiting time distributions for each
customer type, the joint queue length distribution of all priority classes at
all queues at polling epochs, and the steady-state marginal queue length
distributions for each customer type. Through numerical examples we illustrate
that the mixed gated/exhaustive service discipline can significantly decrease
waiting times of high priority jobs. In many cases there is a minimal negative
impact on the waiting times of low priority customers but, remarkably, it turns
out that in polling systems with larger switch-over times there can be even a
positive impact on the waiting times of low priority customers.
"
678,A Polling Model with Reneging at Polling Instants,"  In this paper we consider a single-server, cyclic polling system with
switch-over times and Poisson arrivals. The service disciplines that are
discussed, are exhaustive and gated service. The novel contribution of the
present paper is that we consider the reneging of customers at polling
instants. In more detail, whenever the server starts or ends a visit to a
queue, some of the customers waiting in each queue leave the system before
having received service. The probability that a certain customer leaves the
queue, depends on the queue in which the customer is waiting, and on the
location of the server. We show that this system can be analysed by introducing
customer subtypes, depending on their arrival periods, and keeping track of the
moment when they abandon the system. In order to determine waiting time
distributions, we regard the system as a polling model with varying arrival
rates, and apply a generalised version of the distributional form of Little's
law. The marginal queue length distribution can be found by conditioning on the
state of the system (position of the server, and whether it is serving or
switching).
"
679,Closed-Form Waiting Time Approximations for Polling Systems,"  A typical polling system consists of a number of queues, attended by a single
server in a fixed order. The vast majority of papers on polling systems
focusses on Poisson arrivals, whereas very few results are available for
general arrivals. The current study is the first one presenting simple
closed-form approximations for the mean waiting times in polling systems with
renewal arrival processes, performing well for ALL workloads. The
approximations are constructed using heavy traffic limits and newly developed
light traffic limits. The closed-form approximations may prove to be extremely
useful for system design and optimisation in application areas as diverse as
telecommunication, maintenance, manufacturing and transportation.
"
680,Applications of polling systems,"  Since the first paper on polling systems, written by Mack in 1957, a huge
number of papers on this topic has been written. A typical polling system
consists of a number of queues, attended by a single server. In several
surveys, the most notable ones written by Takagi, detailed and comprehensive
descriptions of the mathematical analysis of polling systems are provided. The
goal of the present survey paper is to complement these papers by putting the
emphasis on \emph{applications} of polling models. We discuss not only the
capabilities, but also the limitations of polling models in representing
various applications. The present survey is directed at both academicians and
practitioners.
"
681,Delays at signalised intersections with exhaustive traffic control,"  In this paper we study a traffic intersection with vehicle-actuated traffic
signal control. Traffic lights stay green until all lanes within a group are
emptied. Assuming general renewal arrival processes, we derive exact limiting
distributions of the delays under Heavy Traffic (HT) conditions. Furthermore,
we derive the Light Traffic (LT) limit of the mean delays for intersections
with Poisson arrivals, and develop a heuristic adaptation of this limit to
capture the LT behaviour for other interarrival-time distributions. We combine
the LT and HT results to develop closed-form approximations for the mean delays
of vehicles in each lane. These closed-form approximations are quite accurate,
very insightful and simple to implement.
"
682,On open problems in polling systems,"  In the present paper we address two open problems concerning polling systems,
viz., queueing systems consisting of multiple queues attended by a single
server that visits the queues one at a time. The first open problem deals with
a system consisting of two queues, one of which has gated service, while the
other receives 1-limited service. The second open problem concerns polling
systems with general (renewal) arrivals and deterministic switch-over times
that become infinitely large. We discuss related, known results for both
problems, and the difficulties encountered when trying to solve them.
"
683,Waiting times in queueing networks with a single shared server,"  We study a queueing network with a single shared server that serves the
queues in a cyclic order. External customers arrive at the queues according to
independent Poisson processes. After completing service, a customer either
leaves the system or is routed to another queue. This model is very generic and
finds many applications in computer systems, communication networks,
manufacturing systems, and robotics. Special cases of the introduced network
include well-known polling models, tandem queues, systems with a waiting room,
multi-stage models with parallel queues, and many others. A complicating factor
of this model is that the internally rerouted customers do not arrive at the
various queues according to a Poisson process, causing standard techniques to
find waiting-time distributions to fail. In this paper we develop a new method
to obtain exact expressions for the Laplace-Stieltjes transforms of the
steady-state waiting-time distributions. This method can be applied to a wide
variety of models which lacked an analysis of the waiting-time distribution
until now.
"
684,Queueing networks with a single shared server: light and heavy traffic,"  We study a queueing network with a single shared server, that serves the
queues in a cyclic order according to the gated service discipline. External
customers arrive at the queues according to independent Poisson processes.
After completing service, a customer either leaves the system or is routed to
another queue. This model is very generic and finds many applications in
computer systems, communication networks, manufacturing systems and robotics.
Special cases of the introduced network include well-known polling models and
tandem queues. We derive exact limits of the mean delays under both
heavy-traffic and light-traffic conditions. By interpolating between these
asymptotic regimes, we develop simple closed-form approximations for the mean
delays for arbitrary loads.
"
685,A Polling Model with Multiple Priority Levels,"  In this paper we consider a single-server cyclic polling system. Between
visits to successive queues, the server is delayed by a random switch-over
time. The order in which customers are served in each queue is determined by a
priority level that is assigned to each customer at his arrival. For this
situation the following service disciplines are considered: gated, exhaustive,
and globally gated. We study the cycle time distribution, the waiting times for
each customer type, the joint queue length distribution of all priority classes
at all queues at polling epochs, and the steady-state marginal queue length
distributions for each customer type.
"
686,High Level Programming for Heterogeneous Architectures,"  This work presents an effort to bridge the gap between abstract high level
programming and OpenCL by extending an existing high level Java programming
framework (APARAPI), based on OpenCL, so that it can be used to program FPGAs
at a high level of abstraction and increased ease of programmability. We run
several real world algorithms to assess the performance of the framework on
both a low end and a high end system. On the low end and high end systems
respectively we observed up to 78-80 percent power reduction and 4.8X-5.3X
speed increase running NBody simulation, as well as up to 65-80 percent power
reduction and 6.2X-7X speed increase for a KMeans, MapReduce algorithm running
on top of the Hadoop framework and APARAPI.
"
687,"A Domain Specific Approach to Heterogeneous Computing: From Availability
  to Accessibility","  We advocate a domain specific software development methodology for
heterogeneous computing platforms such as Multicore CPUs, GPUs and FPGAs. We
argue that three specific benefits are realised from adopting such an approach:
portable, efficient implementations across heterogeneous platforms; domain
specific metrics of quality that characterise platforms in a form software
developers will understand; automatic, optimal partitioning across the
available computing resources. These three benefits allow a development
methodology for software developers where they describe their computational
problems in a single, easy to understand form, and after a modeling procedure
on the available resources, select how they would like to trade between various
domain specific metrics. Our work on the Forward Financial Framework ($F^3$)
demonstrates this methodology in practise. We are able to execute a range of
computational finance option pricing tasks efficiently upon a wide range of
CPU, GPU and FPGA computing platforms. We can also create accurate financial
domain metric models of walltime latency and statistical confidence.
Furthermore, we believe that we can support automatic, optimal partitioning
using this execution and modelling capability.
"
688,"Accelerating unstructured finite volume computations on
  field-programmable gate arrays","  Accurate simulations of various physical processes on digital computers
requires huge computing performance, therefore accelerating these scientific
and engineering applications has a great importance. Density of programmable
logic devices doubles in every 18 months according to Moore's Law. On the
recent devices around one hundred double precision floating-point adders and
multipliers can be implemented. In the paper an FPGA based framework is
described to efficiently utilize this huge computing power to accelerate
simulation of complex physical spatiotemporal phenomena. Simulating complicated
geometries requires unstructured spatial discretization which results in
irregular memory access patterns severely limiting computing performance. Data
locality is improved by mesh node renumbering technique which results in
predictable memory access pattern. Additionally storing a small window of node
data in the on-chip memory of the FPGA can increase data reuse and decrease
memory bandwidth requirements. Generation of the floating-point data path and
control structure of the arithmetic unit containing dozens of operators is a
very challenging task when the goal is high operating frequency. Long and high
fanout control lines and improper placement can severely affect computing
performance. In the paper an automatic data path generation and partitioning
algorithm is presented to eliminate long delays and aid placement of the
circuit. Efficiency and use of the framework is described by a case study
solving the Euler equations on an unstructured mesh using finite volume
technique. On the currently available largest FPGA the generated architecture
contains three processing elements working in parallel providing 90 times
speedup compared to a high performance microprocessor core.
"
689,"Performance Analysis of Linear-Equality-Constrained Least-Squares
  Estimation","  We analyze the performance of a linear-equality-constrained least-squares
(CLS) algorithm and its relaxed version, called rCLS, that is obtained via the
method of weighting. The rCLS algorithm solves an unconstrained least-squares
problem that is augmented by incorporating a weighted form of the linear
constraints. As a result, unlike the CLS algorithm, the rCLS algorithm is
amenable to our approach to performance analysis presented here, which is akin
to the energy-conservation-based methodology. Therefore, we initially inspect
the convergence properties and evaluate the precision of estimation as well as
satisfaction of the constraints for the rCLS algorithm in both mean and
mean-square senses. Afterwards, we examine the performance of the CLS algorithm
by evaluating the limiting performance of the rCLS algorithm as the relaxation
parameter (weight) approaches infinity. Numerical examples verify the accuracy
of the theoretical findings.
"
690,"Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,
  September 2, 2014","  This is the Proceedings of the 1st OMNeT++ Community Summit, which was held
in Hamburg, Germany, September 2, 2014.
"
691,Performance Portability Study of Linear Algebra Kernels in OpenCL,"  The performance portability of OpenCL kernel implementations for common
memory bandwidth limited linear algebra operations across different hardware
generations of the same vendor as well as across vendors is studied. Certain
combinations of kernel implementations and work sizes are found to exhibit good
performance across compute kernels, hardware generations, and, to a lesser
degree, vendors. As a consequence, it is demonstrated that the optimization of
a single kernel is often sufficient to obtain good performance for a large
class of more complicated operations.
"
692,Characterizing and Subsetting Big Data Workloads,"  Big data benchmark suites must include a diversity of data and workloads to
be useful in fairly evaluating big data systems and architectures. However,
using truly comprehensive benchmarks poses great challenges for the
architecture community. First, we need to thoroughly understand the behaviors
of a variety of workloads. Second, our usual simulation-based research methods
become prohibitively expensive for big data. As big data is an emerging field,
more and more software stacks are being proposed to facilitate the development
of big data applications, which aggravates hese challenges. In this paper, we
first use Principle Component Analysis (PCA) to identify the most important
characteristics from 45 metrics to characterize big data workloads from
BigDataBench, a comprehensive big data benchmark suite. Second, we apply a
clustering technique to the principle components obtained from the PCA to
investigate the similarity among big data workloads, and we verify the
importance of including different software stacks for big data benchmarking.
Third, we select seven representative big data workloads by removing redundant
ones and release the BigDataBench simulation version, which is publicly
available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.
"
693,Network calculus for parallel processing,"  In this note, we present preliminary results on the use of ""network calculus""
for parallel processing systems, specifically MapReduce.
"
694,Primary User Traffic Classification in Dynamic Spectrum Access Networks,"  This paper focuses on analytical studies of the primary user (PU) traffic
classification problem. Observing that the gamma distribution can represent
positively skewed data and exponential distribution (popular in communication
networks performance analysis literature) it is considered here as the PU
traffic descriptor. We investigate two PU traffic classifiers utilizing
perfectly measured PU activity (busy) and inactivity (idle) periods: (i)
maximum likelihood classifier (MLC) and (ii) multi-hypothesis sequential
probability ratio test classifier (MSPRTC). Then, relaxing the assumption on
perfect period measurement, we consider a PU traffic observation through
channel sampling. For a special case of negligible probability of PU state
change in between two samplings, we propose a minimum variance PU busy/idle
period length estimator. Later, relaxing the assumption of the complete
knowledge of the parameters of the PU period length distribution, we propose
two PU traffic classification schemes: (i) estimate-then-classify (ETC), and
(ii) average likelihood function (ALF) classifiers considering time domain
fluctuation of the PU traffic parameters. Numerical results show that both MLC
and MSPRTC are sensitive to the periods measurement errors when the distance
among distribution hypotheses is small, and to the distribution parameter
estimation errors when the distance among hypotheses is large. For PU traffic
parameters with a partial prior knowledge of the distribution, the ETC
outperforms ALF when the distance among hypotheses is small, while the opposite
holds when the distance is large.
"
695,Quality of Service Improvement for High-Speed Railway Communications,"  With the fast development of high-speed railways, a call for fulfilling the
notion of communication at ""anytime, anywhere"" for high-speed train passengers
in the Train Operating Control System is on the way. In order to make a
realization of that, new railway wireless communication networks are needed.
The most promising one is the Long Term Evolution for Railway which will
provide broadband access, fast handover, and reliable communication for high
mobility users. However, with the increase of speed, the system is subjected to
high bit error rate, Doppler frequency shift and handover failure just like
other system does. This paper is trying to solve these problems by employing
MIMO technique. Specifically, the goal is to provide higher data rate, higher
reliability, less delay, and other relative quality of services for passengers.
MIMO performance analysis, resource allocation, and access control for handover
and various services in a two-hop model are proposed in this paper. Analytical
results and simulation results show that the proposed model and schemes perform
well in improving the system performances.
"
696,Heavy Traffic Limits for GI/H/n Queues: Theory and Application,"  We consider a GI/H/n queueing system. In this system, there are multiple
servers in the queue. The inter-arrival time is general and independent, and
the service time follows hyper-exponential distribution. Instead of stochastic
differential equations, we propose two heavy traffic limits for this system,
which can be easily applied in practical systems. In applications, we show how
to use these heavy traffic limits to design a power efficient cloud computing
environment based on different QoS requirements.
"
697,DW&C:Dollops Wise Curtail IPv4/IPv6 Transition Mechanism using NS2,"  BD-SIIT and DSTM are widely deployed IPv4/IPv6 Transition mechanism to
improve the performance of the computer network in terms of Throughput,End to
End Delay(EED) and Packet Drop Rate(PDR).In this journal paper we have
Implemented and Compared the Performance Issues of our newly proposed Dollops
Wise Curtail(DW&C)IPv4/IPv6 Migration Mechanism with BD-SIIT and DSTM in NS2.
Implementation and Comparison Performance Analysis between Dollops Wise
Curtail,BD-SIIT and DSTM shows that Dollops Wise Curtail IPv4/IPv6 migration
algorithm performance outperforms than BD-SIIT and DSTM.Based on extensive
simulations,we show that DW&C algorithm reduces the Packet Drop Rate(PDR),End
to End Delay(EED) and achieves better Throughput than BD-SIIT and DSTM.In our
research work observation,the performance metrics such as Throughput,EED and
PLR for DW&C,BD-SIIT and DSTM are measured using TCP,UDP,FTP and CBR Traffics
"
698,"Performance analysis of a 240 thread tournament level MCTS Go program on
  the Intel Xeon Phi","  In 2013 Intel introduced the Xeon Phi, a new parallel co-processor board. The
Xeon Phi is a cache-coherent many-core shared memory architecture claiming
CPU-like versatility, programmability, high performance, and power efficiency.
The first published micro-benchmark studies indicate that many of Intel's
claims appear to be true. The current paper is the first study on the Phi of a
complex artificial intelligence application. It contains an open source MCTS
application for playing tournament quality Go (an oriental board game). We
report the first speedup figures for up to 240 parallel threads on a real
machine, allowing a direct comparison to previous simulation studies. After a
substantial amount of work, we observed that performance scales well up to 32
threads, largely confirming previous simulation results of this Go program,
although the performance surprisingly deteriorates between 32 and 240 threads.
Furthermore, we report (1) unexpected performance anomalies between the Xeon
Phi and Xeon CPU for small problem sizes and small numbers of threads, and (2)
that performance is sensitive to scheduling choices. Achieving good performance
on the Xeon Phi for complex programs is not straightforward; it requires a deep
understanding of (1) search patterns, (2) of scheduling, and (3) of the
architecture and its many cores and caches. In practice, the Xeon Phi is less
straightforward to program for than originally envisioned by Intel.
"
699,Store-Forward and its implications for Proportional Scheduling,"  The Proportional Scheduler was recently proposed as a scheduling algorithm
for multi-hop switch networks. For these networks, the BackPressure scheduler
is the classical benchmark. For networks with fixed routing, the Proportional
Scheduler is maximum stable, myopic and, furthermore, will alleviate certain
scaling issued found in BackPressure for large networks. Nonetheless, the
equilibrium and delay properties of the Proportional Scheduler has not been
fully characterized.
  In this article, we postulate on the equilibrium behaviour of the
Proportional Scheduler though the analysis of an analogous rule called the
Store-Forward allocation. It has been shown that Store-Forward has
asymptotically allocates according to the Proportional Scheduler. Further, for
Store-Forward networks, numerous equilibrium quantities are explicitly
calculable. For FIFO networks under Store-Forward, we calculate the policies
stationary distribution and end-to-end route delay. We discuss network
topologies when the stationary distribution is product-form, a phenomenon which
we call \emph{product form resource pooling}. We extend this product form
notion to independent set scheduling on perfect graphs, where we show that
non-neighbouring queues are statistically independent. Finally, we analyse the
large deviations behaviour of the equilibrium distribution of Store-Forward
networks in order to construct Lyapunov functions for FIFO switch networks.
"
700,"Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power
  Management","  Modern DRAM architectures allow a number of low-power states on individual
memory ranks for advanced power management. Many previous studies have taken
advantage of demotions on low-power states for energy saving. However, most of
the demotion schemes are statically performed on a limited number of
pre-selected low-power states, and are suboptimal for different workloads and
memory architectures. Even worse, the idle periods are often too short for
effective power state transitions, especially for memory intensive
applications. Wrong decisions on power state transition incur significant
energy and delay penalties. In this paper, we propose a novel memory system
design named RAMZzz with rank-aware energy saving optimizations including
dynamic page migrations and adaptive demotions. Specifically, we group the
pages with similar access locality into the same rank with dynamic page
migrations. Ranks have their hotness: hot ranks are kept busy for high
utilization and cold ranks can have more lengthy idle periods for power state
transitions. We further develop adaptive state demotions by considering all
low-power states for each rank and a prediction model to estimate the
power-down timeout among states. We experimentally compare our algorithm with
other energy saving policies with cycle-accurate simulation. Experiments with
benchmark workloads show that RAMZzz achieves significant improvement on
energy-delay2 and energy consumption over other energy saving techniques.
"
701,Instability of Sharing Systems in the Presence of Retransmissions,"  Retransmissions represent a primary failure recovery mechanism on all layers
of communication network architecture. Similarly, fair sharing, e.g. processor
sharing (PS), is a widely accepted approach to resource allocation among
multiple users. Recent work has shown that retransmissions in failure-prone,
e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this
paper, we discover a new phenomenon showing that PS-based scheduling induces
complete instability with zero throughput in the presence of retransmissions,
regardless of how low the traffic load may be. This phenomenon occurs even when
the job sizes are bounded/fragmented, e.g. deterministic. Our analytical
results are further validated via simulation experiments. Moreover, our work
demonstrates that scheduling one job at a time, such as first-come-first-serve,
achieves stability and should be preferred in these systems.
"
702,"Intel Cilk Plus for Complex Parallel Algorithms: ""Enormous Fast Fourier
  Transform"" (EFFT) Library","  In this paper we demonstrate the methodology for parallelizing the
computation of large one-dimensional discrete fast Fourier transforms (DFFTs)
on multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey
method have to control cache utilization, memory bandwidth and vector hardware
usage, and at the same time scale across multiple threads or compute nodes. Our
method builds on single-threaded Intel Math Kernel Library (MKL) implementation
of DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We
demonstrate the ability of Intel Cilk Plus to handle parallel recursion with
nested loop-centric parallelism without tuning the code to the number of cores
or cache metrics. The result of our work is a library called EFFT that performs
1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel
DFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code
of EFFT is available for free download under the GPLv3 license. This work
provides a new efficient DFFT implementation, and at the same time demonstrates
an educational example of how computer science problems with complex parallel
patterns can be optimized for high performance using the Intel Cilk Plus
framework.
"
703,"A Queueing Network Approach to the Analysis and Control of
  Mobility-On-Demand Systems","  This paper presents a queueing network approach to the analysis and control
of mobility-on-demand (MoD) systems for urban personal transportation. A MoD
system consists of a fleet of vehicles providing one-way car sharing service
and a team of drivers to rebalance such vehicles. The drivers then rebalance
themselves by driving select customers similar to a taxi service. We model the
MoD system as two coupled closed Jackson networks with passenger loss. We show
that the system can be approximately balanced by solving two decoupled linear
programs and exactly balanced through nonlinear optimization. The rebalancing
techniques are applied to a system sizing example using taxi data in three
neighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver
ratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time
closed-loop rebalancing policy for drivers and demonstrate its stability (in
terms of customer wait times) for typical system loads.
"
704,"Tolls and Welfare Optimization for Multiclass Traffic in Multiqueue
  Systems","  We consider a queueing system with multiple heterogeneous servers serving a
multiclass population. The classes are distinguished by the time costs. All
customers have i.i.d. service requirements. Arriving customers do not see the
instantaneous queue occupancy. Arrivals are randomly routed to one of the
servers and the routing probabilities are determined centrally to optimize the
expected waiting cost. This is, in general, a difficult optimization problem
and we obtain the structure of the routing matrix. Next we consider a system in
which each queue charges an admission price. The arrivals are routed randomly
to minimize an individual objective function that includes the expected waiting
cost and the admission price. Once again, we obtain the structure of the
equilibrium routing matrix for this case. Finally, we determine the admission
prices to make the equilibrium routing probability matrix equal to a given
optimal routing probability matrix.
"
705,Cache-aware Performance Modeling and Prediction for Dense Linear Algebra,"  Countless applications cast their computational core in terms of dense linear
algebra operations. These operations can usually be implemented by combining
the routines offered by standard linear algebra libraries such as BLAS and
LAPACK, and typically each operation can be obtained in many alternative ways.
Interestingly, identifying the fastest implementation -- without executing it
-- is a challenging task even for experts. An equally challenging task is that
of tuning each routine to performance-optimal configurations. Indeed, the
problem is so difficult that even the default values provided by the libraries
are often considerably suboptimal; as a solution, normally one has to resort to
executing and timing the routines, driven by some form of parameter search. In
this paper, we discuss a methodology to solve both problems: identifying the
best performing algorithm within a family of alternatives, and tuning
algorithmic parameters for maximum performance; in both cases, we do not
execute the algorithms themselves. Instead, our methodology relies on timing
and modeling the computational kernels underlying the algorithms, and on a
technique for tracking the contents of the CPU cache. In general, our
performance predictions allow us to tune dense linear algebra algorithms within
few percents from the best attainable results, thus allowing computational
scientists and code developers alike to efficiently optimize their linear
algebra routines and codes.
"
706,On the Performance Prediction of BLAS-based Tensor Contractions,"  Tensor operations are surging as the computational building blocks for a
variety of scientific simulations and the development of high-performance
kernels for such operations is known to be a challenging task. While for
operations on one- and two-dimensional tensors there exist standardized
interfaces and highly-optimized libraries (BLAS), for higher dimensional
tensors neither standards nor highly-tuned implementations exist yet. In this
paper, we consider contractions between two tensors of arbitrary dimensionality
and take on the challenge of generating high-performance implementations by
resorting to sequences of BLAS kernels. The approach consists in breaking the
contraction down into operations that only involve matrices or vectors. Since
in general there are many alternative ways of decomposing a contraction, we are
able to methodically derive a large family of algorithms. The main contribution
of this paper is a systematic methodology to accurately identify the fastest
algorithms in the bunch, without executing them. The goal is instead
accomplished with the help of a set of cache-aware micro-benchmarks for the
underlying BLAS kernels. The predictions we construct from such benchmarks
allow us to reliably single out the best-performing algorithms in a tiny
fraction of the time taken by the direct execution of the algorithms.
"
707,"Multi-step Uniformization with Steady-State Detection in Nonstationary
  M/M/s Queuing Systems","  A new approach to the steady state detection in the uniformization method of
solving continuous time Markov chains is introduced. The method is particularly
useful in solving inhomogenous CTMC's in multiple steps, where the desired
error bound of the whole solution can be distributed not proportionally to the
lengths of the respective intervals, but rather in a way, that maximizes the
chances of detecting a steady state. Additionally, the convergence properties
of the underlying DTMC are used to further enhance the computational savings
due to the steady state detection. The method is applied to the problem of
modeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)
queuing system.
"
708,"Introducing SLAMBench, a performance and accuracy benchmarking
  methodology for SLAM","  Real-time dense computer vision and SLAM offer great potential for a new
level of scene modelling, tracking and real environmental interaction for many
types of robot, but their high computational requirements mean that use on mass
market embedded platforms is challenging. Meanwhile, trends in low-cost,
low-power processing are towards massive parallelism and heterogeneity, making
it difficult for robotics and vision researchers to implement their algorithms
in a performance-portable way. In this paper we introduce SLAMBench, a
publicly-available software framework which represents a starting point for
quantitative, comparable and validatable experimental research to investigate
trade-offs in performance, accuracy and energy consumption of a dense RGB-D
SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,
OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D
sequences with trajectory and scene ground truth for reliable accuracy
comparison of different implementation and algorithms. We present an analysis
and breakdown of the constituent algorithmic elements of KinectFusion, and
experimentally investigate their execution time on a variety of multicore and
GPUaccelerated platforms. For a popular embedded platform, we also present an
analysis of energy efficiency for different configuration alternatives.
"
709,Efficient State-based CRDTs by Delta-Mutation,"  CRDTs are distributed data types that make eventual consistency of a
distributed object possible and non ad-hoc. Specifically, state-based CRDTs
ensure convergence through disseminating the en- tire state, that may be large,
and merging it to other replicas; whereas operation-based CRDTs disseminate
operations (i.e., small states) assuming an exactly-once reliable dissemination
layer. We introduce Delta State Conflict-Free Replicated Datatypes
({\delta}-CRDT) that can achieve the best of both worlds: small messages with
an incremental nature, as in operation-based CRDTs, disseminated over
unreliable communication channels, as in traditional state-based CRDTs. This is
achieved by defining {\delta}-mutators to return a delta-state, typically with
a much smaller size than the full state, that is joined to both: local and
remote states. We introduce the {\delta}-CRDT framework, and we explain it
through establishing a correspondence to current state-based CRDTs. In
addition, we present an anti-entropy algorithm that ensures causal consistency,
and we introduce two {\delta}-CRDT specifications of well-known replicated
datatypes.
"
710,"Architecture, implementation and parallelization of the software to
  search for periodic gravitational wave signals","  The parallelization, design and scalability of the \sky code to search for
periodic gravitational waves from rotating neutron stars is discussed. The code
is based on an efficient implementation of the F-statistic using the Fast
Fourier Transform algorithm. To perform an analysis of data from the advanced
LIGO and Virgo gravitational wave detectors' network, which will start
operating in 2015, hundreds of millions of CPU hours will be required - the
code utilizing the potential of massively parallel supercomputers is therefore
mandatory. We have parallelized the code using the Message Passing Interface
standard, implemented a mechanism for combining the searches at different
sky-positions and frequency bands into one extremely scalable program. The
parallel I/O interface is used to escape bottlenecks, when writing the
generated data into file system. This allowed to develop a highly scalable
computation code, which would enable the data analysis at large scales on
acceptable time scales. Benchmarking of the code on a Cray XE6 system was
performed to show efficiency of our parallelization concept and to demonstrate
scaling up to 50 thousand cores in parallel.
"
711,"Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing
  Units","  We revisit the implementation of iterative solvers on discrete graphics
processing units and demonstrate the benefit of implementations using extensive
kernel fusion for pipelined formulations over conventional implementations of
classical formulations. The proposed implementations with both CUDA and OpenCL
are freely available in ViennaCL and are shown to be competitive with or even
superior to other solver packages for graphics processing units. Highest
performance gains are obtained for small to medium-sized systems, while our
implementations are on par with vendor-tuned implementations for very large
systems. Our results are especially beneficial for transient problems, where
many small to medium-sized systems instead of a single big system need to be
solved.
"
712,"Efficient HTTP based I/O on very large datasets for high performance
  computing with the libdavix library","  Remote data access for data analysis in high performance computing is
commonly done with specialized data access protocols and storage systems. These
protocols are highly optimized for high throughput on very large datasets,
multi-streams, high availability, low latency and efficient parallel I/O. The
purpose of this paper is to describe how we have adapted a generic protocol,
the Hyper Text Transport Protocol (HTTP) to make it a competitive alternative
for high performance I/O and data analysis applications in a global computing
grid: the Worldwide LHC Computing Grid. In this work, we first analyze the
design differences between the HTTP protocol and the most common high
performance I/O protocols, pointing out the main performance weaknesses of
HTTP. Then, we describe in detail how we solved these issues. Our solutions
have been implemented in a toolkit called davix, available through several
recent Linux distributions. Finally, we describe the results of our benchmarks
where we compare the performance of davix against a HPC specific protocol for a
data analysis use case.
"
713,"Quantifying performance bottlenecks of stencil computations using the
  Execution-Cache-Memory model","  Stencil algorithms on regular lattices appear in many fields of computational
science, and much effort has been put into optimized implementations. Such
activities are usually not guided by performance models that provide estimates
of expected speedup. Understanding the performance properties and bottlenecks
by performance modeling enables a clear view on promising optimization
opportunities. In this work we refine the recently developed
Execution-Cache-Memory (ECM) model and use it to quantify the performance
bottlenecks of stencil algorithms on a contemporary Intel processor. This
includes applying the model to arrive at single-core performance and
scalability predictions for typical corner case stencil loop kernels. Guided by
the ECM model we accurately quantify the significance of ""layer conditions,""
which are required to estimate the data traffic through the memory hierarchy,
and study the impact of typical optimization approaches such as spatial
blocking, strength reduction, and temporal blocking for their expected
benefits. We also compare the ECM model to the widely known Roofline model.
"
714,"On Bootstrapping Machine Learning Performance Predictors via Analytical
  Models","  Performance modeling typically relies on two antithetic methodologies: white
box models, which exploit knowledge on system's internals and capture its
dynamics using analytical approaches, and black box techniques, which infer
relations among the input and output variables of a system based on the
evidences gathered during an initial training phase. In this paper we
investigate a technique, which we name Bootstrapping, which aims at reconciling
these two methodologies and at compensating the cons of the one with the pros
of the other. We thoroughly analyze the design space of this gray box modeling
technique, and identify a number of algorithmic and parametric trade-offs which
we evaluate via two realistic case studies, a Key-Value Store and a Total Order
Broadcast service.
"
715,"Performance Engineering of the Kernel Polynomial Method on Large-Scale
  CPU-GPU Systems","  The Kernel Polynomial Method (KPM) is a well-established scheme in quantum
physics and quantum chemistry to determine the eigenvalue density and spectral
properties of large sparse matrices. In this work we demonstrate the high
optimization potential and feasibility of peta-scale heterogeneous CPU-GPU
implementations of the KPM. At the node level we show that it is possible to
decouple the sparse matrix problem posed by KPM from main memory bandwidth both
on CPU and GPU. To alleviate the effects of scattered data access we combine
loosely coupled outer iterations with tightly coupled block sparse matrix
multiple vector operations, which enables pure data streaming. All
optimizations are guided by a performance analysis and modelling process that
indicates how the computational bottlenecks change with each optimization step.
Finally we use the optimized node-level KPM with a hybrid-parallel framework to
perform large scale heterogeneous electronic structure calculations for novel
topological materials on a petascale-class Cray XC30 system.
"
716,"Towards energy efficiency and maximum computational intensity for
  stencil algorithms using wavefront diamond temporal blocking","  We study the impact of tunable parameters on computational intensity (i.e.,
inverse code balance) and energy consumption of multicore-optimized wavefront
diamond temporal blocking (MWD) applied to different stencil-based update
schemes. MWD combines the concepts of diamond tiling and multicore-aware
wavefront blocking in order to achieve lower cache size requirements than
standard single-core wavefront temporal blocking. We analyze the impact of the
cache block size on the theoretical and observed code balance, introduce loop
tiling in the leading dimension to widen the range of applicable diamond sizes,
and show performance results on a contemporary Intel CPU. The impact of code
balance on power dissipation on the CPU and in the DRAM is investigated and
shows that DRAM power is a decisive factor for energy consumption, which is
strongly influenced by the code balance. Furthermore we show that highest
performance does not necessarily lead to lowest energy even if the clock speed
is fixed.
"
717,"QoE Modelling, Measurement and Prediction: A Review","  In mobile computing systems, users can access network services anywhere and
anytime using mobile devices such as tablets and smart phones. These devices
connect to the Internet via network or telecommunications operators. Users
usually have some expectations about the services provided to them by different
operators. Users' expectations along with additional factors such as cognitive
and behavioural states, cost, and network quality of service (QoS) may
determine their quality of experience (QoE). If users are not satisfied with
their QoE, they may switch to different providers or may stop using a
particular application or service. Thus, QoE measurement and prediction
techniques may benefit users in availing personalized services from service
providers. On the other hand, it can help service providers to achieve lower
user-operator switchover. This paper presents a review of the state-the-art
research in the area of QoE modelling, measurement and prediction. In
particular, we investigate and discuss the strengths and shortcomings of
existing techniques. Finally, we present future research directions for
developing novel QoE measurement and prediction techniques
"
718,Energy-Aware Lease Scheduling in Virtualized Data Centers,"  Energy efficiency has become an important measurement of scheduling
algorithms in virtualized data centers. One of the challenges of
energy-efficient scheduling algorithms, however, is the trade-off between
minimizing energy consumption and satisfying quality of service (e.g.
performance, resource availability on time for reservation requests). We
consider resource needs in the context of virtualized data centers of a private
cloud system, which provides resource leases in terms of virtual machines (VMs)
for user applications. In this paper, we propose heuristics for scheduling VMs
that address the above challenge. On performance evaluation, simulated results
have shown a significant reduction on total energy consumption of our proposed
algorithms compared with an existing First-Come-First-Serve (FCFS) scheduling
algorithm with the same fulfillment of performance requirements. We also
discuss the improvement of energy saving when additionally using migration
policies to the above mentioned algorithms.
"
719,Estimating the Spatial Reuse with Configuration Models,"  We propose a new methodology to estimate the spatial reuse of CSMA-like
scheduling. Instead of focusing on spatial configurations of users, we model
the interferences between users as a random graph. Using configuration models
for random graphs, we show how the properties of the medium access mechanism
are captured by some deterministic differential equations, when the size of the
graph gets large. Performance indicators such as the probability of connection
of a given node can then be efficiently computed from these equations. We also
perform simulations to illustrate the results on different types of random
graphs. Even on spatial structures, these estimates get very accurate as soon
as the variance of the interference is not negligible.
"
720,Cloud Benchmarking for Performance,"  How can applications be deployed on the cloud to achieve maximum performance?
This question has become significant and challenging with the availability of a
wide variety of Virtual Machines (VMs) with different performance capabilities
in the cloud. The above question is addressed by proposing a six step
benchmarking methodology in which a user provides a set of four weights that
indicate how important each of the following groups: memory, processor,
computation and storage are to the application that needs to be executed on the
cloud. The weights along with cloud benchmarking data are used to generate a
ranking of VMs that can maximise performance of the application. The rankings
are validated through an empirical analysis using two case study applications;
the first is a financial risk application and the second is a molecular
dynamics simulation, which are both representative of workloads that can
benefit from execution on the cloud. Both case studies validate the feasibility
of the methodology and highlight that maximum performance can be achieved on
the cloud by selecting the top ranked VMs produced by the methodology.
"
721,Branch-Avoiding Graph Algorithms,"  This paper quantifies the impact of branches and branch mispredictions on the
single-core performance for two classes of graph problems. Specifically, we
consider classical algorithms for computing connected components and
breadth-first search (BFS). We show that branch mispredictions are costly and
can reduce performance by as much as 30%-50%. This insight suggests that one
should seek graph algorithms and implementations that avoid branches.
  As a proof-of-concept, we devise such implementations for both the classic
top-down algorithm for BFS and the Shiloach-Vishkin algorithm for connected
components. We evaluate these implementations on current x86 and ARM-based
processors to show the efficacy of the approach. Our results suggest how both
compiler writers and architects might exploit this insight to improve graph
processing systems more broadly and create better systems for such problems.
"
722,"Dimensioning of PA for massive MIMO system with load adaptive number of
  antennas","  This paper takes into consideration the non-ideal efficiency characteristics
of realistic power amplifiers (PAs) along with the daily traffic profile in
order to investigate the impact of PA dimensioning on the energy efficiency
(EE) of load adaptive massive MIMO system. A multicellular system has been
considered where each base station (BS) is equipped with a large number of
antennas to serve many single antenna users. For a given number of users in a
cell, the optimum number of active antennas maximizing EE has been derived
where total BS downlink power is assumed to be fixed. Under the same
assumption, the PAs have been dimensioned in a way that maximizes network EE
not only for a single time snapshot but over twenty four hours of operation
while considering dynamic efficiency characteristics of the PAs. In order to
incorporate this daily load profile, each BS has been modeled as an M/G/m/m
state dependent queue under the assumption that the network is dimensioned to
serve a maximum number of users at a time corresponding to 100% cell traffic
load. This load adaptive system along with the optimized PA dimensioning
achieves 30% higher energy efficiency compared to a base line system where the
BSs always run with a fixed number of active antennas which are most energy
efficient while serving 100% traffic load.
"
723,"Investigation of the relationship between code change set n-grams and
  change in energy consumption","  The amount of software running on mobile devices is constantly growing as
consumers and industry purchase more battery powered devices. On the other
hand, tools that provide developers with feed- back on how their software
changes affect battery life are not widely available. This work employs Green
Mining, the study of the rela- tionship between energy consumption and software
changesets, and n-gram language models to evaluate if source code changeset
perplex- ity correlates with change in energy consumption. A correlation be-
tween perplexity and change in energy consumption would permit the development
of a tool that predicts the impact a code changeset may have on a software
applications energy consumption. The case study results show that there is weak
to no correlation between cross en- tropy and change in energy consumption.
Therefore, future areas of investigation are proposed.
"
724,"Optimization of Discrete-parameter Multiprocessor Systems using a Novel
  Ergodic Interpolation Technique","  Modern multi-core systems have a large number of design parameters, most of
which are discrete-valued, and this number is likely to keep increasing as chip
complexity rises. Further, the accurate evaluation of a potential design choice
is computationally expensive because it requires detailed cycle-accurate system
simulation. If the discrete parameter space can be embedded into a larger
continuous parameter space, then continuous space techniques can, in principle,
be applied to the system optimization problem. Such continuous space techniques
often scale well with the number of parameters.
  We propose a novel technique for embedding the discrete parameter space into
an extended continuous space so that continuous space techniques can be applied
to the embedded problem using cycle accurate simulation for evaluating the
objective function. This embedding is implemented using simulation-based
ergodic interpolation, which, unlike spatial interpolation, produces the
interpolated value within a single simulation run irrespective of the number of
parameters. We have implemented this interpolation scheme in a cycle-based
system simulator. In a characterization study, we observe that the interpolated
performance curves are continuous, piece-wise smooth, and have low statistical
error. We use the ergodic interpolation-based approach to solve a large
multi-core design optimization problem with 31 design parameters. Our results
indicate that continuous space optimization using ergodic interpolation-based
embedding can be a viable approach for large multi-core design optimization
problems.
"
725,"Buffer occupancy asymptotics in rate proportional sharing networks with
  heterogeneous long-tailed inputs","  In this paper, we consider a network of rate proportional processor sharing
servers in which sessions with long-tailed duration arrive as Poisson
processes. In particular, we assume that a session of type $n$ transmits at a
rate $r_n$ bits per unit time and lasts for a random time $\tau_n$ with a
generalized Pareto distribution given by $P \{\tau_n > x\} \sim \alpha_n
x^{-(1+\beta_n)}$ for large $x$, where $\alpha_n, \beta_n > 0$. The weights are
taken to be the rates of the flows. The network is assumed to be loop-free with
respect to source-destination routes. We characterize the order $O-$asymptotics
of the complementary buffer occupancy distribution at each node in terms of the
input characteristics of the sessions. In particular, we show that the
distributions obey a power law whose exponent can be calculated via solving a
fixed point and deterministic knapsack problem. The paper concludes with some
canonical examples.
"
726,"Performance Analysis for Energy Harvesting Communication Protocols with
  Fixed Rate Transmission","  Energy Harvesting (EH) has emerged as a promising technique for Green
Communications and it is a novel technique to prolong the lifetime of the
wireless networks with replenishable nodes. In this paper, we consider the
energy shortage analysis of fixed rate transmission in communication systems
with energy harvesting nodes. First, we study the finite-horizon transmission
and provide the general formula for the energy shortage probability. We also
give some examples as benchmarks. Then, we continue to derive a closed-form
expression for infinite-horizon transmission, which is a lower bound for the
energy shortage probability of any finite-horizon transmission. These results
are proposed for both Additive White Gaussian Noise (AWGN) and fading channels.
Moreover, we show that even under \emph{random energy arrival}, one can
transmit at a fixed rate equal to capacity in the AWGN channels with negligible
aggregate shortage time. We achieve this result using our practical
transmission schemes, proposed for finite-horizon. Also, comprehensive
numerical simulations are performed in AWGN and fading channels with no Channel
State Information (CSI) available at the transmitter, which corroborate our
theoretical findings. Furthermore, we improve the performance of our
transmission schemes in the fading channel with no CSI at the transmitter by
optimizing the transmission initiation threshold.
"
727,The Implementation of a Real-Time Polyphase Filter,"  In this article we study the suitability of dierent computational
accelerators for the task of real-time data processing. The algorithm used for
comparison is the polyphase filter, a standard tool in signal processing and a
well established algorithm. We measure performance in FLOPs and execution time,
which is a critical factor for real-time systems. For our real-time studies we
have chosen a data rate of 6.5GB/s, which is the estimated data rate for a
single channel on the SKAs Low Frequency Aperture Array. Our findings how that
GPUs are the most likely candidate for real-time data processing. GPUs are
better in both performance and power consumption.
"
728,Stochastic HYPE: Flow-based modelling of stochastic hybrid systems,"  Stochastic HYPE is a novel process algebra that models stochastic,
instantaneous and continuous behaviour. It develops the flow-based approach of
the hybrid process algebra HYPE by replacing non-urgent events with events with
exponentially-distributed durations and also introduces random resets. The
random resets allow for general stochasticity, and in particular allow for the
use of event durations drawn from distributions other than the exponential
distribution. To account for stochasticity, the semantics of stochastic HYPE
target piecewise deterministic Markov processes (PDMPs), via intermediate
transition-driven stochastic hybrid automata (TDSHA) in contrast to the hybrid
automata used as semantic target for HYPE. Stochastic HYPE models have a
specific structure where the controller of a system is separate from the
continuous aspect of this system providing separation of concerns and
supporting reasoning. A novel equivalence is defined which captures when two
models have the same stochastic behaviour (as in stochastic bisimulation),
instantaneous behaviour (as in classical bisimulation) and continuous
behaviour. These techniques are illustrated via an assembly line example.
"
729,On The Modeling of OpenFlow-based SDNs: The Single Node Case,"  OpenFlow is one of the most commonly used protocols for communication between
the controller and the forwarding element in a software defined network (SDN).
A model based on M/M/1 queues is proposed in [1] to capture the communication
between the forwarding element and the controller. Albeit the model provides
useful insight, it is accurate only for the case when the probability of
expecting a new flow is small. Secondly, it is not straight forward to extend
the model in [1] to more than one forwarding element in the data plane. In this
work we propose a model which addresses both these challenges. The model is
based on Jackson assumption but with corrections tailored to the OpenFlow based
SDN network. Performance analysis using the proposed model indicates that the
model is accurate even for the case when the probability of new flow is quite
large. Further we show by a toy example that the model can be extended to more
than one node in the data plane.
"
730,Modeling LRU caches with Shot Noise request processes,"  In this paper we analyze Least Recently Used (LRU) caches operating under the
Shot Noise requests Model (SNM). The SNM was recently proposed to better
capture the main characteristics of today Video on Demand (VoD) traffic. We
investigate the validity of Che's approximation through an asymptotic analysis
of the cache eviction time. In particular, we provide a large deviation
principle, a law of large numbers and a central limit theorem for the cache
eviction time, as the cache size grows large. Finally, we derive upper and
lower bounds for the ""hit"" probability in tandem networks of caches under Che's
approximation.
"
731,"Resource Allocation Frameworks for Network-coded Layered Multimedia
  Multicast Services","  The explosive growth of content-on-the-move, such as video streaming to
mobile devices, has propelled research on multimedia broadcast and multicast
schemes. Multi-rate transmission strategies have been proposed as a means of
delivering layered services to users experiencing different downlink channel
conditions. In this paper, we consider Point-to-Multipoint layered service
delivery across a generic cellular system and improve it by applying different
random linear network coding approaches. We derive packet error probability
expressions and use them as performance metrics in the formulation of resource
allocation frameworks. The aim of these frameworks is both the optimization of
the transmission scheme and the minimization of the number of broadcast packets
on each downlink channel, while offering service guarantees to a predetermined
fraction of users. As a case of study, our proposed frameworks are then adapted
to the LTE-A standard and the eMBMS technology. We focus on the delivery of a
video service based on the H.264/SVC standard and demonstrate the advantages of
layered network coding over multi-rate transmission. Furthermore, we establish
that the choice of both the network coding technique and resource allocation
method play a critical role on the network footprint, and the quality of each
received video layer.
"
732,Energy-Efficient Strategies for Cooperative Multi-Channel MAC Protocols,"  Distributed Information SHaring (DISH) is a new cooperative approach to
designing multi-channel MAC protocols. It aids nodes in their decision making
processes by compensating for their missing information via information sharing
through other neighboring nodes. This approach was recently shown to
significantly boost the throughput of multi-channel MAC protocols. However, a
critical issue for ad hoc communication devices, i.e., energy efficiency, has
yet to be addressed. In this paper, we address this issue by developing simple
solutions which (1) reduce the energy consumption (2) without compromising the
throughput performance, and meanwhile (3) maximize cost efficiency. We propose
two energy-efficient strategies: in-situ energy conscious DISH which uses
existing nodes only, and altruistic DISH which needs additional nodes called
altruists. We compare five protocols with respect to the strategies and
identify altruistic DISH to be the right choice in general: it (1) conserves
40-80% of energy, (2) maintains the throughput advantage gained from the DISH
approach, and (3) more than doubles the cost efficiency compared to protocols
without applying the strategy. On the other hand, our study shows that in-situ
energy conscious DISH is suitable only in certain limited scenarios.
"
733,Analyzing DISH for Multi-Channel MAC Protocols in Wireless Networks,"  For long, node cooperation has been exploited as a data relaying mechanism.
However, the wireless channel allows for much richer interaction between nodes.
One such scenario is in a multi-channel environment, where transmitter-receiver
pairs may make incorrect decisions (e.g., in selecting channels) but idle
neighbors could help by sharing information to prevent undesirable consequences
(e.g., data collisions). This represents a Distributed Information SHaring
(DISH) mechanism for cooperation and suggests new ways of designing cooperative
protocols. However, what is lacking is a theoretical understanding of this new
notion of cooperation. In this paper, we view cooperation as a network resource
and evaluate the availability of cooperation via a metric, $p_{co}$, the
probability of obtaining cooperation. First, we analytically evaluate $p_{co}$
in the context of multi-channel multi-hop wireless networks. Second, we verify
our analysis via simulations and the results show that our analysis accurately
characterizes the behavior of $p_{co}$ as a function of underlying network
parameters. This step also yields important insights into DISH with respect to
network dynamics. Third, we investigate the correlation between $p_{co}$ and
network performance in terms of collision rate, packet delay, and throughput.
The results indicate a near-linear relationship, which may significantly
simplify performance analysis for cooperative networks and suggests that
$p_{co}$ be used as an appropriate performance indicator itself. Throughout
this work, we utilize, as appropriate, three different DISH contexts ---
model-based DISH, ideal DISH, and real DISH --- to explore $p_{co}$.
"
734,"A Metric for DISH Networks: Analysis, Implications, and Applications","  In wireless networks, node cooperation has been exploited as a data relaying
mechanism for decades. However, the wireless channel allows for much richer
interaction among nodes. In particular, Distributed Information SHaring (DISH)
represents a new improvement to multi-channel MAC protocol design by using a
cooperative element at the control plane. In this approach, nodes exchange
control information to make up for other nodes' insufficient knowledge about
the environment, and thereby aid in their decision making. To date, what is
lacking is a theoretical understanding of DISH. In this paper, we view
cooperation as a network resource and evaluate the availability of cooperation,
$p_{co}$. We first analyze $p_{co}$ in the context of a multi-channel multi-hop
wireless network, and then perform simulations which show that the analysis
accurately characterizes $p_{co}$ as a function of underlying network
parameters. Next, we investigate the correlation between $p_{co}$ and network
metrics such as collision rate, packet delay, and throughput. We find a
near-linear relationship between $p_{co}$ and the metrics, which suggests that
$p_{co}$ can be used as an appropriate performance indicator itself. Finally,
we apply our analysis to solving a channel bandwidth allocation problem, where
we derive optimal schemes and provide general guidelines on bandwidth
allocation for DISH networks.
"
735,"Efficient analysis of caching strategies under dynamic content
  popularity","  In this paper we develop a novel technique to analyze both isolated and
interconnected caches operating under different caching strategies and
realistic traffic conditions. The main strength of our approach is the ability
to consider dynamic contents which are constantly added into the system
catalogue, and whose popularity evolves over time according to desired
profiles. We do so while preserving the simplicity and computational efficiency
of models developed under stationary popularity conditions, which are needed to
analyze several caching strategies. Our main achievement is to show that the
impact of content popularity dynamics on cache performance can be effectively
captured into an analytical model based on a fixed content catalogue (i.e., a
catalogue whose size and objects' popularity do not change over time).
"
736,"A Flexible Framework for Accurate Simulation of Cloud In-Memory Data
  Stores","  In-memory (transactional) data stores are recognized as a first-class data
management technology for cloud platforms, thanks to their ability to match the
elasticity requirements imposed by the pay-as-you-go cost model. On the other
hand, defining the well-suited amount of cache servers to be deployed, and the
degree of in-memory replication of slices of data, in order to optimize
reliability/availability and performance tradeoffs, is far from being a trivial
task. Yet, it is an essential aspect of the provisioning process of cloud
platforms, given that it has an impact on how well cloud resources are actually
exploited. To cope with the issue of determining optimized configurations of
cloud in-memory data stores, in this article we present a flexible simulation
framework offering skeleton simulation models that can be easily specialized in
order to capture the dynamics of diverse data grid systems, such as those
related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of
the framework lies in that it integrates simulation and machine-learning
(black-box) techniques, the latter being essentially used to capture the
dynamics of the data-exchange layer (e.g. the message passing layer) across the
cache servers. This is a relevant aspect when considering that the actual
data-transport/networking infrastructure on top of which the data grid is
deployed might be unknown, hence being not feasible to be modeled via white-box
(namely purely simulative) approaches. We also provide an extended experimental
study aimed at validating instances of simulation models supported by our
framework against execution dynamics of real data grid systems deployed on top
of either private or public cloud infrastructures.
"
737,FairCache: Introducing Fairness to ICN Caching - Technical Report,"  Information-centric networking extensively uses universal in-network caching.
However, developing an efficient and fair collaborative caching algorithm for
selfish caches is still an open question. In addition, the communication
overhead induced by collaboration is especially poorly understood in a general
network setting such as realistic ISP and Autonomous System networks. In this
paper, we address these two problems by modeling the in-network caching problem
as a Nash bargaining game. We show that the game is a convex optimization
problem and further derive the corresponding distributed algorithm. We
analytically investigate the collaboration overhead on general graph
topologies, and theoretically show that collaboration has to be constrained
within a small neighborhood due to its cost growing exponentially. Our proposed
algorithm achieves at least 16% performance gain over its competitors on
different network topologies in the evaluation, and guarantees provable
convergence, Pareto efficiency and proportional fairness.
"
738,Optimizing Large-Scale ODE Simulations,"  We present a strategy to speed up Runge-Kutta-based ODE simulations of large
systems with nearest-neighbor coupling. We identify the cache/memory bandwidth
as the crucial performance bottleneck. To reduce the required bandwidth, we
introduce a granularity in the simulation and identify the optimal cluster size
in a performance study. This leads to a considerable performance increase and
transforms the algorithm from bandwidth bound to CPU bound. By additionally
employing SIMD instructions we are able to boost the efficiency even further.
In the end, a total performance increase of up to a factor three is observed
when using cache optimization and SIMD instructions compared to a standard
implementation. All simulation codes are written in C++ and made publicly
available. By using the modern C++ libraries Boost.odeint and Boost.SIMD, these
optimizations can be implemented with minimal programming effort.
"
739,On Transitory Queueing,"  We introduce a framework and develop a theory of transitory queueing models.
These are models that are not only non-stationary and time-varying but also
have other features such as the queueing system operates over finite time, or
only a finite population arrives. Such models are relevant in many real-world
settings, from queues at post-offces, DMV, concert halls and stadia to
out-patient departments at hospitals. We develop fluid and diffusion limits for
a large class of transitory queueing models. We then introduce three specific
models that fit within this framework, namely, the Delta(i)/GI/1 model, the
conditioned G/GI/1 model, and an arrival model of scheduled traffic with epoch
uncertainty. We show that asymptotically these models are distributionally
equivalent, i.e., they have the same fluid and diffusion limits. We note that
our framework provides the first ever way of analyzing the standard G/GI/1
model when we condition on the number of arrivals. In obtaining these results,
we provide generalizations and extensions of the Glivenko-Cantelli and Donskers
Theorem for empirical processes with triangular arrays. Our analysis uses the
population acceleration technique that we introduce and develop. This may be
useful in analysis of other non-stationary and non-ergodic queuing models.
"
740,"Proceedings of the 5th International Workshop on Adaptive Self-tuning
  Computing Systems 2015 (ADAPT'15)","  This is the proceedings of the 5th International Workshop on Adaptive
Self-tuning Computing Systems 2015 (ADAPT'15).
"
741,Fast Product-Matrix Regenerating Codes,"  Distributed storage systems support failures of individual devices by the use
of replication or erasure correcting codes. While erasure correcting codes
offer a better storage efficiency than replication for similar fault tolerance,
they incur higher CPU consumption, higher network consumption and higher disk
I/Os. To address these issues, codes specific to storage systems have been
designed. Their main feature is the ability to repair a single lost disk
efficiently. In this paper, we focus on one such class of codes that minimize
network consumption during repair, namely regenerating codes. We implement the
original Product-Matrix Regenerating codes as well as a new optimization we
propose and show that the resulting optimized codes allow achieving 790 MB/s
for encoding in typical settings. Reported speeds are significantly higher than
previous studies, highlighting that regenerating codes can be used with little
CPU penalty.
"
742,"Dynamic Channel Allocation for Class-Based QoS Provisioning and Call
  Admission in Visible Light Communication","  Provisioning of quality of service (QoS) is a key issue in visible light
communication (VLC) system as well as in other wireless communication systems.
Due to the fact that QoS requirements are not as strict for all traffic types,
more calls of higher priority traffic classes can be accommodated by blocking
some more calls of lower priority traffic classes. Diverse types of high data
rate traffic are supported by existing wireless communication systems while the
resource is limited. Hence, priority based resource allocation can ensure the
service quality for the calls of important traffic class. The fixed guard
channels to prioritize any class of calls always reduce the channel
utilization. In this paper we propose a priority based dynamic channel
reservation scheme for higher priority calls that does not reduce the channel
utilization significantly. The number of reserved channels for each of the
individual traffic classes is calculated using real-time observation of the
call arrival rates of all the traffic classes. The features of the scheme allow
reduction of the call blocking probability of higher priority calls along with
the increase of the channel utilization. The proposed Markov Chain model is
expected to be very much effective for the queuing analysis especially for the
priority scheme of any number of traffic classes. The numerical results show
that the proposed scheme is able to attain reasonable call blocking probability
of higher priority calls without sacrificing channel utilization.
"
743,"Class-Based Service Connectivity using Multi-Level Bandwidth Adaptation
  in Multimedia Wireless Networks","  Due to the fact that quality of service requirements are not very strict for
all traffic types, more calls of higher priority can be accommodated by
reducing some bandwidth allocation for the bandwidth adaptive calls. The
bandwidth adaptation to accept a higher priority call is more than that of a
lower priority call. Therefore, the multi-level bandwidth adaptation technique
improves the overall forced call termination probability as well as provides
priority of the traffic classes in terms of call blocking probability without
reducing the bandwidth utilization. We propose a novel bandwidth adaptation
model that releases multi-level of bandwidth from the existing multimedia
traffic calls. The amount of released bandwidth is decided based on the
priority of the requesting traffic calls and the number of existing bandwidth
adaptive calls. This prioritization of traffic classes does not reduce the
bandwidth utilization. Moreover, our scheme reduces the overall forced call
termination probability significantly. The proposed scheme is modeled using the
Markov Chain. The numerical results show that the proposed scheme is able to
provide negligible handover call dropping probability as well as significantly
reduced new call blocking probability of higher priority calls without
increasing the overall forced call termination probability.
"
744,"Modelling common cause failures of large digital I&C systems with
  coloured Petri nets","  The purpose of this study is the representation of Common Cause Failures
(CCF) in large digital systems. The system under study is representative of a
control system of a nuclear plant. The model for CCF is the generalized Atwood
model. It can represent independent failures, CCF non-lethal for some system
elements and CCF lethal to all. The Atwood model was modified to ""direct""
non-lethal DCC on certain parts of the system and take into account the
different possible origins of DCC. Maintenance and repairs are taken into
account in the model that is thus dynamic. The main evaluation results are
probabilistic, the considered indicator is the probability of failure on demand
(PFD). A comparison is made between the estimator of the PFD taking into
account all the failures and the estimator taking into account only the
detected failures.
"
745,Easy-to-Use On-the-Fly Binary Program Acceleration on Many-Cores,"  This paper introduces Binary Acceleration At Runtime (BAAR), an easy-to-use
on-the-fly binary acceleration mechanism which aims to tackle the problem of
enabling existent software to automatically utilize accelerators at runtime.
BAAR is based on the LLVM Compiler Infrastructure and has a client-server
architecture. The client runs the program to be accelerated in an environment
which allows program analysis and profiling. Program parts which are identified
as suitable for the available accelerator are exported and sent to the server.
The server optimizes these program parts for the accelerator and provides RPC
execution for the client. The client transforms its program to utilize
accelerated execution on the server for offloaded program parts.
  We evaluate our work with a proof-of-concept implementation of BAAR that uses
an Intel Xeon Phi 5110P as the acceleration target and performs automatic
offloading, parallelization and vectorization of suitable program parts. The
practicality of BAAR for real-world examples is shown based on a study of
stencil codes. Our results show a speedup of up to 4x without any
developer-provided hints and 5.77x with hints over the same code compiled with
the Intel Compiler at optimization level O2 and running on an Intel Xeon
E5-2670 machine. Based on our insights gained during implementation and
evaluation we outline future directions of research, e.g., offloading more
fine-granular program parts than functions, a more sophisticated communication
mechanism or introducing on-stack-replacement.
"
746,"Graph-Based Minimum Dwell Time and Average Dwell Time Computations for
  Discrete-Time Switched Linear Systems","  Discrete-time switched linear systems where switchings are governed by a
digraph are considered. The minimum (or average) dwell time that guarantees the
asymptotic stability can be computed by calculating the maximum cycle ratio (or
maximum cycle mean) of a doubly weighted digraph where weights depend on the
eigenvalues and eigenvectors of subsystem matrices. The graph-based method is
applied to systems with defective subsystem matrices using Jordan
decomposition. In the case of bimodal switched systems scaling algorithms that
minimizes the condition number can be used to give a better minimum (or
average) dwell time estimates.
"
747,Renewable Energy-Aware Information-Centric Networking,"  The ICT industry today is placed as one of the major consumers of energy,
where recent reports have also shown that the industry is a major contributor
to global carbon emissions. While renewable energy-aware data centers have been
proposed, these solutions have certain limitations. The primary limitation is
due to the design of data centers which focus on large-size facilities located
in selected locations. This paper addresses this problem, by utilizing
in-network caching with each router having storage and being powered by
renewable energy sources (wind and solar). Besides placing contents closer to
end users, utilizing in-network caching could potentially increase probability
of capturing renewable energy in diverse geographical locations. Our proposed
solution is dual- layered: on the first layer a distributed gradient-based
routing protocol is used to discover the paths along routers that are powered
by the highest renewable energy, and on the second layer, a caching mechanism
will pull the contents from the data centre and place them on routers of the
paths that are discovered by our routing protocol. Through our experiments on a
testbed utilizing real meteorological data, our proposed solution has
demonstrated increased quantity of renewable energy consumption, while reducing
the workload on the data centers.
"
748,"Performance comparison between Java and JNI for optimal implementation
  of computational micro-kernels","  General purpose CPUs used in high performance computing (HPC) support a
vector instruction set and an out-of-order engine dedicated to increase the
instruction level parallelism. Hence, related optimizations are currently
critical to improve the performance of applications requiring numerical
computation. Moreover, the use of a Java run-time environment such as the
HotSpot Java Virtual Machine (JVM) in high performance computing is a promising
alternative. It benefits from its programming flexibility, productivity and the
performance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT
compiler suffers from two main drawbacks. First, the JIT is a black box for
developers. We have no control over the generated code nor any feedback from
its optimization phases like vectorization. Secondly, the time constraint
narrows down the degree of optimization compared to static compilers like GCC
or LLVM. So, it is compelling to use statically compiled code since it benefits
from additional optimization reducing performance bottlenecks. Java enables to
call native code from dynamic libraries through the Java Native Interface
(JNI). Nevertheless, JNI methods are not inlined and require an additional cost
to be invoked compared to Java ones. Therefore, to benefit from better static
optimization, this call overhead must be leveraged by the amount of computation
performed at each JNI invocation. In this paper we tackle this problem and we
propose to do this analysis for a set of micro-kernels. Our goal is to select
the most efficient implementation considering the amount of computation defined
by the calling context. We also investigate the impact on performance of
several different optimization schemes which are vectorization, out-of-order
optimization, data alignment, method inlining and the use of native memory for
JNI methods.
"
749,Accelerating Correlation Power Analysis Using Graphics Processing Units,"  Correlation Power Analysis (CPA) is a type of power analysis based side
channel attack that can be used to derive the secret key of encryption
algorithms including DES (Data Encryption Standard) and AES (Advanced
Encryption Standard). A typical CPA attack on unprotected AES is performed by
analysing a few thousand power traces that requires about an hour of
computational time on a general purpose CPU. Due to the severity of this
situation, a large number of researchers work on countermeasures to such
attacks. Verifying that a proposed countermeasure works well requires
performing the CPA attack on about 1.5 million power traces. Such processing,
even for a single attempt of verification on commodity hardware would run for
several days making the verification process infeasible. Modern Graphics
Processing Units (GPUs) have support for thousands of light weight threads,
making them ideal for parallelizable algorithms like CPA. While the cost of a
GPU being lesser than a high performance multicore server, still the GPU
performance for this algorithm is many folds better than that of a multicore
server. We present an algorithm and its implementation on GPU for CPA on
128-bit AES that is capable of executing 1300x faster than that on a single
threaded CPU and more than 60x faster than that on a 32 threaded multicore
server. We show that an attack that would take hours on the multicore server
would take even less than a minute on a much cost effective GPU.
"
750,"To Use or Not to Use: Graphics Processing Units for Pattern Matching
  Algorithms","  String matching is an important part in today's computer applications and
Aho-Corasick algorithm is one of the main string matching algorithms used to
accomplish this. This paper discusses that when can the GPUs be used for string
matching applications using the Aho-Corasick algorithm as a benchmark. We have
to identify the best unit to run our string matching algorithm according to the
performance of our devices and the applications. Sometimes CPU gives better
performance than GPU and sometimes GPU gives better performance than CPU.
Therefore, identifying this critical point is significant task for researchers
who are using GPUs to improve the performance of their string matching
applications based on string matching algorithms.
"
751,"A Structured Hardware Software Architecture for Peptide Based Diagnosis
  - Sub-string Matching Problem with Limited Tolerance (ICIAfS14)","  The problem of inferring proteins from complex peptide samples in shotgun
proteomic workflow sets extreme demands on computational resources. This is
exacerbated by the fact that, in general, a given protein cannot be defined by
a fixed sequence of amino acids due to the existence of splice variants and
isoforms of that protein. Therefore, the problem of protein inference could be
considered as one of identifying sequences of amino acids with some limited
tolerance. Two problems arise from this: a) due to these variations, the
applicability of exact string matching methodologies could be questioned and b)
the difficulty of defining a reference sequence for a particular set of
proteins that are functionally indistinguishable, but with some variation in
features. This paper presents a model-based inference approach that is
developed and validated to solve the inference problem. Our approach starts
from an examination of the known set of splice variants and isoforms of a
target protein to identify the Greatest Common Stable Substring (GCSS) of amino
acids and the Substrings Subjects to Limited Variation (SSLV) and their
respective locations on the GCSS. Then we define and solve the Sub-string
Matching Problem with Limited Tolerance (SMPLT). This approach is validated on
identified peptides in a labelled and clustered data set from UNIPROT.
Identification of Baylisascaris Procyonis infection was used as an application
instance that achieved up to 70 times speedup compared to a software only
system. This workflow can be generalised to any inexact multiple pattern
matching application by replacing the patterns in a clustered and distributed
environment which permits a distance between member strings to account for
permitted deviations such as substitutions, insertions and deletions.
"
752,"A Case Study: Task Scheduling Methodologies for High Speed Computing
  Systems","  High Speed computing meets ever increasing real-time computational demands
through the leveraging of flexibility and parallelism. The flexibility is
achieved when computing platform designed with heterogeneous resources to
support multifarious tasks of an application where as task scheduling brings
parallel processing. The efficient task scheduling is critical to obtain
optimized performance in heterogeneous computing Systems (HCS). In this paper,
we brought a review of various application scheduling models which provide
parallelism for homogeneous and heterogeneous computing systems. In this paper,
we made a review of various scheduling methodologies targeted to high speed
computing systems and also prepared summary chart. The comparative study of
scheduling methodologies for high speed computing systems has been carried out
based on the attributes of platform & application as well. The attributes are
execution time, nature of task, task handling capability, type of host &
computing platform. Finally a summary chart has been prepared and it
demonstrates that the need of developing scheduling methodologies for
Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high
speed computing platform for real time applications.
"
753,"Beam-searching and Transmission Scheduling in Millimeter Wave
  Communications","  Millimeter wave (mmW) wireless networks are capable to support multi-gigabit
data rates, by using directional communications with narrow beams. However,
existing mmW communications standards are hindered by two problems: deafness
and single link scheduling. The deafness problem, that is, a misalignment
between transmitter and receiver beams, demands a time consuming beam-searching
operation, which leads to an alignment-throughput tradeoff. Moreover, the
existing mmW standards schedule a single link in each time slot and hence do
not fully exploit the potential of mmW communications, where directional
communications allow multiple concurrent transmissions. These two problems are
addressed in this paper, where a joint beamwidth selection and power allocation
problem is formulated by an optimization problem for short range mmW networks
with the objective of maximizing effective network throughput. This
optimization problem allows establishing the fundamental alignment-throughput
tradeoff, however it is computationally complex and requires exact knowledge of
network topology, which may not be available in practice. Therefore, two
standard-compliant approximation solution algorithms are developed, which rely
on underestimation and overestimation of interference. The first one exploits
directionality to maximize the reuse of available spectrum and thereby
increases the network throughput, while imposing almost no computational
complexity. The second one is a more conservative approach that protects all
active links from harmful interference, yet enhances the network throughput by
100% compared to the existing standards. Extensive performance analysis
provides useful insights on the directionality level and the number of
concurrent transmissions that should be pursued. Interestingly, extremely
narrow beams are in general not optimal.
"
754,Binary Systematic Network Coding for Progressive Packet Decoding,"  We consider binary systematic network codes and investigate their capability
of decoding a source message either in full or in part. We carry out a
probability analysis, derive closed-form expressions for the decoding
probability and show that systematic network coding outperforms conventional
network coding. We also develop an algorithm based on Gaussian elimination that
allows progressive decoding of source packets. Simulation results show that the
proposed decoding algorithm can achieve the theoretical optimal performance.
Furthermore, we demonstrate that systematic network codes equipped with the
proposed algorithm are good candidates for progressive packet recovery owing to
their overall decoding delay characteristics.
"
755,"Sleep Period Optimization Model For Layered Video Service Delivery Over
  eMBMS Networks","  Long Term Evolution-Advanced (LTE-A) and the evolved Multimedia Broadcast
Multicast System (eMBMS) are the most promising technologies for the delivery
of highly bandwidth demanding applications. In this paper we propose a green
resource allocation strategy for the delivery of layered video streams to users
with different propagation conditions. The goal of the proposed model is to
minimize the user energy consumption. That goal is achieved by minimizing the
time required by each user to receive the broadcast data via an efficient power
transmission allocation model. A key point in our system model is that the
reliability of layered video communications is ensured by means of the Random
Linear Network Coding (RLNC) approach. Analytical results show that the
proposed resource allocation model ensures the desired quality of service
constraints, while the user energy footprint is significantly reduced.
"
756,Optimized Network-coded Scalable Video Multicasting over eMBMS Networks,"  Delivery of multicast video services over fourth generation (4G) networks
such as 3GPP Long Term Evolution-Advanced (LTE-A) is gaining momentum. In this
paper, we address the issue of efficiently multicasting layered video services
by defining a novel resource allocation framework that aims to maximize the
service coverage whilst keeping the radio resource footprint low. A key point
in the proposed system mode is that the reliability of multicast video services
is ensured by means of an Unequal Error Protection implementation of the
Network Coding (UEP-NC) scheme. In addition, both the communication parameters
and the UEP-NC scheme are jointly optimized by the proposed resource allocation
framework. Numerical results show that the proposed allocation framework can
significantly increase the service coverage when compared to a conventional
Multi-rate Transmission (MrT) strategy.
"
757,"Solving the Klein-Gordon equation using Fourier spectral methods: A
  benchmark test for computer performance","  The cubic Klein-Gordon equation is a simple but non-trivial partial
differential equation whose numerical solution has the main building blocks
required for the solution of many other partial differential equations. In this
study, the library 2DECOMP&FFT is used in a Fourier spectral scheme to solve
the Klein-Gordon equation and strong scaling of the code is examined on
thirteen different machines for a problem size of 512^3. The results are useful
in assessing likely performance of other parallel fast Fourier transform based
programs for solving partial differential equations. The problem is chosen to
be large enough to solve on a workstation, yet also of interest to solve
quickly on a supercomputer, in particular for parametric studies. Unlike other
high performance computing benchmarks, for this problem size, the time to
solution will not be improved by simply building a bigger supercomputer.
"
758,"Global finite element matrix construction based on a CPU-GPU
  implementation","  The finite element method (FEM) has several computational steps to
numerically solve a particular problem, to which many efforts have been
directed to accelerate the solution stage of the linear system of equations.
However, the finite element matrix construction, which is also time-consuming
for unstructured meshes, has been less investigated. The generation of the
global finite element matrix is performed in two steps, computing the local
matrices by numerical integration and assembling them into a global system,
which has traditionally been done in serial computing. This work presents a
fast technique to construct the global finite element matrix that arises by
solving the Poisson's equation in a three-dimensional domain. The proposed
methodology consists in computing the numerical integration, due to its
intrinsic parallel opportunities, in the graphics processing unit (GPU) and
computing the matrix assembly, due to its intrinsic serial operations, in the
central processing unit (CPU). In the numerical integration, only the lower
triangular part of each local stiffness matrix is computed thanks to its
symmetry, which saves GPU memory and computing time. As a result of symmetry,
the global sparse matrix also contains non-zero elements only in its lower
triangular part, which reduces the assembly operations and memory usage. This
methodology allows generating the global sparse matrix from any unstructured
finite element mesh size on GPUs with little memory capacity, only limited by
the CPU memory.
"
759,A Roofline Visualization Framework,"  The Roofline Model and its derivatives provide an intuitive representation of
the best achievable performance on a given architecture. The Roofline Toolkit
project is a collaboration among researchers at Argonne National Laboratory,
Lawrence Berkeley National Laboratory, and the University of Oregon and
consists of three main parts: hardware characterization, software
characterization, and data manipulation and visualization interface. These
components address the different aspects of performance data acquisition and
manipulation required for performance analysis, modeling and optimization of
codes on existing and emerging architectures. In this paper we introduce an
initial implementation of the third component, a system for visualizing
roofline charts and managing roofline performance analysis data. We discuss the
implementation and rationale for the integration of the roofline visualization
system into the Eclipse IDE. An overview of our continuing efforts and goals in
the development of this project is provided.
"
760,"On Optimization of Network-coded Scalable Multimedia Service
  Multicasting","  In the near future, the delivery of multimedia multicast services over
next-generation networks is likely to become one of the main pillars of future
cellular networks. In this extended abstract, we address the issue of
efficiently multicasting layered video services by defining a novel
optimization paradigm that is based on an Unequal Error Protection
implementation of Random Linear Network Coding, and aims to ensure target
service coverages by using a limited amount of radio resources.
"
761,"A Model Predictive Control Approach for Low-Complexity Electric Vehicle
  Charging Scheduling: Optimality and Scalability","  With the increasing adoption of plug-in electric vehicles (PEVs), it is
critical to develop efficient charging coordination mechanisms that minimize
the cost and impact of PEV integration to the power grid. In this paper, we
consider the optimal PEV charging scheduling, where the non-causal information
about future PEV arrivals is not known in advance, but its statistical
information can be estimated. This leads to an ""online"" charging scheduling
problem that is naturally formulated as a finite-horizon dynamic programming
with continuous state space and action space. To avoid the prohibitively high
complexity of solving such a dynamic programming problem, we provide a Model
Predictive Control (MPC) based algorithm with computational complexity
$O(T^3)$, where $T$ is the total number of time stages. We rigorously analyze
the performance gap between the near-optimal solution of the MPC-based approach
and the optimal solution for any distributions of exogenous random variables.
Furthermore, our rigorous analysis shows that when the random process
describing the arrival of charging demands is first-order periodic, the
complexity of proposed algorithm can be reduced to $O(1)$, which is independent
of $T$. Extensive simulations show that the proposed online algorithm performs
very closely to the optimal online algorithm. The performance gap is smaller
than $0.4\%$ in most cases.
"
762,Effective RAT Selection Approach for 5G Dense Wireless Networks,"  Dense Networks (DenseNet) and Multi-Radio Access Technologies (Multi-RATs)
are considered as key features of the emerging fifth generation (5G) wireless
systems. A Multi-RAT DenseNet is characterized by a very dense deployment of
low-power base stations (BSs) and by a multi-tier architecture consisting of
heterogeneous radio access technologies. Such a network aims to guarantee high
data-rates, low latency and low energy consumption. Although the usage of a
Multi RAT DenseNet solves problems such as coverage holes and low performance
at the cell edge, frequent and unnecessary RAT handovers may occur with a
consequent high signaling load. In this work, we propose an effective RAT
selection algorithm that efficiently manages the RAT handover procedure by
\emph{(i)} choosing the most suitable RAT that guarantees high system and user
performance, and \emph{(ii)} reducing unnecessary handover events. In
particular, the decision to trigger a handover is based on a new system
parameter named Reference Base Station Efficiency (RBSE). This parameter takes
into account metrics related to both the system and the user: the BS
transmitted power, the BS traffic load and the users' spectral efficiency. We
compare, by simulation, the proposed scheme with the standardized 3GPP
policies. Results show that the proposed RAT selection scheme significantly
reduces the number of handovers and the end-to-end delay while maintaining high
system throughput and user spectral efficiency.
"
763,"Patterns and Rewrite Rules for Systematic Code Generation (From
  High-Level Functional Patterns to High-Performance OpenCL Code)","  Computing systems have become increasingly complex with the emergence of
heterogeneous hardware combining multicore CPUs and GPUs. These parallel
systems exhibit tremendous computational power at the cost of increased
programming effort. This results in a tension between achieving performance and
code portability. Code is either tuned using device-specific optimizations to
achieve maximum performance or is written in a high-level language to achieve
portability at the expense of performance.
  We propose a novel approach that offers high-level programming, code
portability and high-performance. It is based on algorithmic pattern
composition coupled with a powerful, yet simple, set of rewrite rules. This
enables systematic transformation and optimization of a high-level program into
a low-level hardware specific representation which leads to high performance
code.
  We test our design in practice by describing a subset of the OpenCL
programming model with low-level patterns and by implementing a compiler which
generates high performance OpenCL code. Our experiments show that we can
systematically derive high-performance device-specific implementations from
simple high-level algorithmic expressions. The performance of the generated
OpenCL code is on par with highly tuned implementations for multicore CPUs and
GPUs written by experts
"
764,OMP2MPI: Automatic MPI code generation from OpenMP programs,"  In this paper, we present OMP2MPI a tool that generates automatically MPI
source code from OpenMP. With this transformation the original program can be
adapted to be able to exploit a larger number of processors by surpassing the
limits of the node level on large HPC clusters. The transformation can also be
useful to adapt the source code to execute in distributed memory many-cores
with message passing support. In addition, the resulting MPI code can be used
as an starting point that still can be further optimized by software engineers.
The transformation process is focused on detecting OpenMP parallel loops and
distributing them in a master/worker pattern. A set of micro-benchmarks have
been used to verify the correctness of the the transformation and to measure
the resulting performance. Surprisingly not only the automatically generated
code is correct by construction, but also it often performs faster even when
executed with MPI.
"
765,"Dynamic Bandwidth-Efficient BCube Topologies for Virtualized Data Center
  Networks","  Network virtualization enables computing networks and data center (DC)
providers to manage their networking resources in a flexible manner using
software running on physical computers. In this paper, we address the existing
issues with the classic DC network topologies in virtualized environment, and
investigate a set of DC network topologies with the capability of providing
dynamic structures according to the service-level required by the active
traffic in a virtual DC network. In particular, we propose three main
approaches to modify the structure of a classic BCube topology as a topology
benchmark, and investigate their associated structural features and maximum
achievable interconnected bandwidth for different routing scenarios. Finally,
we run an extensive simulation program to check the performance of the proposed
modified topologies in a simulation environment which considers failure
analysis and also traffic congestion. Our simulation experiments, which are
consistent to our design goals, show the efficiency of the proposed modified
topologies comparing to the classic BCube in terms of bandwidth availability
and failure resiliency.
"
766,Numerical simulation of skin transport using Parareal,"  In-silico investigation of skin permeation is an important but also
computationally demanding problem. To resolve all scales involved in full
detail will not only require exascale computing capacities but also suitable
parallel algorithms. This article investigates the applicability of the
time-parallel Parareal algorithm to a brick and mortar setup, a precursory
problem to skin permeation. The C++ library Lib4PrM implementing Parareal is
combined with the UG4 simulation framework, which provides the spatial
discretization and parallelization. The combination's performance is studied
with respect to convergence and speedup. It is confirmed that anisotropies in
the domain and jumps in diffusion coefficients only have a minor impact on
Parareal's convergence. The influence of load imbalances in time due to
differences in number of iterations required by the spatial solver as well as
spatio-temporal weak scaling is discussed.
"
767,"Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of
  Shared Servers for Low Delay","  We consider the job assignment problem in a multi-server system consisting of
$N$ parallel processor sharing servers, categorized into $M$ ($\ll N$)
different types according to their processing capacity or speed. Jobs of random
sizes arrive at the system according to a Poisson process with rate $N
\lambda$. Upon each arrival, a small number of servers from each type is
sampled uniformly at random. The job is then assigned to one of the sampled
servers based on a selection rule. We propose two schemes, each corresponding
to a specific selection rule that aims at reducing the mean sojourn time of
jobs in the system.
  We first show that both methods achieve the maximal stability region. We then
analyze the system operating under the proposed schemes as $N \to \infty$ which
corresponds to the mean field. Our results show that asymptotic independence
among servers holds even when $M$ is finite and exchangeability holds only
within servers of the same type. We further establish the existence and
uniqueness of stationary solution of the mean field and show that the tail
distribution of server occupancy decays doubly exponentially for each server
type. When the estimates of arrival rates are not available, the proposed
schemes offer simpler alternatives to achieving lower mean sojourn time of
jobs, as shown by our numerical studies.
"
768,"Estimating the Potential Speedup of Computer Vision Applications on
  Embedded Multiprocessors","  Computer vision applications constitute one of the key drivers for embedded
multicore architectures. Although the number of available cores is increasing
in new architectures, designing an application to maximize the utilization of
the platform is still a challenge. In this sense, parallel performance
prediction tools can aid developers in understanding the characteristics of an
application and finding the most adequate parallelization strategy. In this
work, we present a method for early parallel performance estimation on embedded
multiprocessors from sequential application traces. We describe its
implementation in Parana, a fast trace-driven simulator targeting OpenMP
applications on the STMicroelectronics' STxP70 Application-Specific
Multiprocessor (ASMP). Results for the FAST key point detector application show
an error margin of less than 10% compared to the reference cycle-approximate
simulator, with lower modeling effort and up to 20x faster execution time.
"
769,"Disaggregated and optically interconnected memory: when will it be cost
  effective?","  The ""Disaggregated Server"" concept has been proposed for datacenters where
the same type server resources are aggregated in their respective pools, for
example a compute pool, memory pool, network pool, and a storage pool. Each
server is constructed dynamically by allocating the right amount of resources
from these pools according to the workload's requirements. Modularity, higher
packaging and cooling efficiencies, and higher resource utilization are among
the suggested benefits. With the emergence of very large datacenters, ""clouds""
containing tens of thousands of servers, datacenter efficiency has become an
important topic. Few computer chip and systems vendors are working on and
making frequent announcements on silicon photonics and disaggregated memory
systems.
  In this paper we study the trade-off between cost and performance of building
a disaggregated memory system where DRAM modules in the datacenter are pooled,
for example in memory-only chassis and racks. The compute pool and the memory
pool are interconnected by an optical interconnect to overcome the distance and
bandwidth issues of electrical fabrics. We construct a simple cost model that
includes the cost of latency, cost of bandwidth and the savings expected from a
disaggregated memory system. We then identify the level at which a
disaggregated memory system becomes cost competitive with a traditional direct
attached memory system.
  Our analysis shows that a rack-scale disaggregated memory system will have a
non-trivial performance penalty, and at the datacenter scale the penalty is
impractically high, and the optical interconnect costs are at least a factor of
10 more expensive than where they should be when compared to the traditional
direct attached memory systems.
"
770,Stochastic Service Placement,"  Resource allocation for cloud services is a complex task due to the diversity
of the services and the dynamic workloads. One way to address this is by
overprovisioning which results in high cost due to the unutilized resources. A
much more economical approach, relying on the stochastic nature of the demand,
is to allocate just the right amount of resources and use additional more
expensive mechanisms in case of overflow situations where demand exceeds the
capacity. In this paper we study this approach and show both by comprehensive
analysis for independent normal distributed demands and simulation on synthetic
data that it is significantly better than currently deployed methods.
"
771,"An asymptotically optimal policy and state-space collapse for the
  multi-class shared queue","  We consider a multi-class G/G/1 queue with a finite shared buffer. There is
task admission and server scheduling control which aims to minimize the cost
which consists of holding and rejection components. We construct a policy that
is asymptotically optimal in the heavy traffic limit. The policy stems from
solution to Harrison-Taksar (HT) free boundary problem and is expressed by a
single free boundary point. We show that the HT problem solution translated
into the queuelength processes follows a specific {\it triangular} form. This
form implies the queuelength control policy which is different from the known
$c\mu$ priority rule and has a novel structure.
  We exemplify that the probabilistic methods we exploit can be successfully
applied to solving scheduling and admission problems in cloud computing.
"
772,"Modelling Computational Resources for Next Generation Sequencing
  Bioinformatics Analysis of 16S rRNA Samples","  In the rapidly evolving domain of next generation sequencing and
bioinformatics analysis, data generation is one aspect that is increasing at a
concomitant rate. The burden associated with processing large amounts of
sequencing data has emphasised the need to allocate sufficient computing
resources to complete analyses in the shortest possible time with manageable
and predictable costs. A novel method for predicting time to completion for a
popular bioinformatics software (QIIME), was developed using key variables
characteristic of the input data assumed to impact processing time. Multiple
Linear Regression models were developed to determine run time for two denoising
algorithms and a general bioinformatics pipeline. The models were able to
accurately predict clock time for denoising sequences from a naturally
assembled community dataset, but not an artificial community. Speedup and
efficiency tests for AmpliconNoise also highlighted that caution was needed
when allocating resources for parallel processing of data. Accurate modelling
of computational processing time using easily measurable predictors can assist
NGS analysts in determining resource requirements for bioinformatics software
and pipelines. Whilst demonstrated on a specific group of scripts, the
methodology can be extended to encompass other packages running on multiple
architectures, either in parallel or sequentially.
"
773,Faster 64-bit universal hashing using carry-less multiplications,"  Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set
in their x64 processors. We use CLMUL to implement an almost universal 64-bit
hash family (CLHASH). We compare this new family with what might be the fastest
almost universal family on x64 processors (VHASH). We find that CLHASH is at
least 60% faster. We also compare CLHASH with a popular hash function designed
for speed (Google's CityHash). We find that CLHASH is 40% faster than CityHash
on inputs larger than 64 bytes and just as fast otherwise.
"
774,Evaluating kernels on Xeon Phi to accelerate Gysela application,"  This work describes the challenges presented by porting parts ofthe Gysela
code to the Intel Xeon Phi coprocessor, as well as techniques used for
optimization, vectorization and tuning that can be applied to other
applications. We evaluate the performance of somegeneric micro-benchmark on Phi
versus Intel Sandy Bridge. Several interpolation kernels useful for the Gysela
application are analyzed and the performance are shown. Some memory-bound and
compute-bound kernels are accelerated by a factor 2 on the Phi device compared
to Sandy architecture. Nevertheless, it is hard, if not impossible, to reach a
large fraction of the peek performance on the Phi device,especially for
real-life applications as Gysela. A collateral benefit of this optimization and
tuning work is that the execution time of Gysela (using 4D advections) has
decreased on a standard architecture such as Intel Sandy Bridge.
"
775,"Performance Analysis of Random Linear Network Coding in Two-Source
  Single-Relay Networks","  This paper considers the multiple-access relay channel in a setting where two
source nodes transmit packets to a destination node, both directly and via a
relay node, over packet erasure channels. Intra-session network coding is used
at the source nodes and inter-session network coding is employed at the relay
node to combine the recovered source packets of both source nodes. In this
work, we investigate the performance of the network-coded system in terms of
the probability that the destination node will successfully recover the source
packets of the two source nodes. We build our analysis on fundamental
probability expressions for random matrices over finite fields and we derive
upper bounds on the system performance for the case of systematic and
non-systematic network coding. Simulation results show that the upper bounds
are very tight and accurately predict the decoding probability at the
destination node. Our analysis also exposes the clear benefits of systematic
network coding at the source nodes compared to non-systematic transmission.
"
776,Queue Length Behavior in a Switch under the MaxWeight Algorithm,"  We consider a switch operating under the MaxWeight scheduling algorithm,
under any traffic pattern such that all the ports are loaded. This system is
interesting to study since the queue lengths exhibit a multi-dimensional
state-space collapse in the heavy-traffic regime. We use a Lyapunov-type drift
technique to characterize the heavy-traffic behavior of the expectation of the
sum queue lengths in steady-state, under the assumption that all ports are
saturated and all queues receive non-zero traffic. Under these conditions, we
show that the heavy-traffic scaled queue length is given by
$\left(1-\frac{1}{2n}\right)||\sigma||^2$, where $\sigma$ is the vector of the
standard deviations of arrivals to each port in the heavy-traffic limit. In the
special case of uniform Bernoulli arrivals, the corresponding formula is given
by $\left(n-\frac{3}{2}+\frac{1}{2n}\right)$. The result shows that the
heavy-traffic scaled queue length has optimal scaling with respect to $n,$ thus
settling one version of an open conjecture; in fact, it is shown that the
heavy-traffic queue length is at most within a factor of two from the optimal.
We then consider certain asymptotic regimes where the load of the system scales
simultaneously with the number of ports. We show that the MaxWeight algorithm
has optimal queue length scaling behavior provided that the arrival rate
approaches capacity sufficiently fast.
"
777,"Effective Handling of Urgent Jobs - Speed Up Scheduling for Computing
  Applications","  A queue is required when a service provider is not able to handle jobs
arriving over the time. In a highly flexible and dynamic environment, some jobs
might demand for faster execution at run-time especially when the resources are
limited and the jobs are competing for acquiring resources. A user might demand
for speed up (reduced wait time) for some of the jobs present in the queue at
run time. In such cases, it is required to accelerate (directly sending the job
to the server) urgent jobs (requesting for speed up) ahead of other jobs
present in the queue for an earlier completion of urgent jobs. Under the
assumption of no additional resources, such acceleration of jobs would result
in slowing down of other jobs present in the queue. In this paper, we formulate
the problem of Speed Up Scheduling without acquiring any additional resources
for the scheduling of on-line speed up requests posed by a user at run-time and
present algorithms for the same. We apply the idea of Speed Up Scheduling to
two different domains -Web Scheduling and CPU Scheduling. We demonstrate our
results with a simulation based model using trace driven workload and synthetic
datasets to show the usefulness of Speed Up scheduling. Speed Up provides a new
way of addressing urgent jobs, provides a different evaluation criteria for
comparing scheduling algorithms and has practical applications.
"
778,"On the tradeoff of average delay, average service cost, and average
  utility for single server queues with monotone policies","  In this thesis, we study the optimal tradeoff of average delay, average
service cost, and average utility for single server queueing models, with and
without admission control. The continuous time and discrete time queueing
models that we consider are motivated by cross-layer models for noisy
point-to-point links, with random packet arrivals. We study the above tradeoff
problem for a class of admissible policies, which are monotone and stationary
and obtain an asymptotic characterization of the minimum average delay as a
function of the average service cost and average utility constraints.
"
779,"The Feasibility of Using OpenCL Instead of OpenMP for Parallel CPU
  Programming","  OpenCL, along with CUDA, is one of the main tools used to program GPGPUs.
However, it allows running the same code on multi-core CPUs too, making it a
rival for the long-established OpenMP. In this paper we compare OpenCL and
OpenMP when developing and running compute-heavy code on a CPU. Both ease of
programming and performance aspects are considered. Since, unlike a GPU, no
memory copy operation is involved, our comparisons measure the code generation
quality, as well as thread management efficiency of OpenCL and OpenMP. We
evaluate the performance of these development tools under two conditions: a
large number of short-running compute-heavy parallel code executions, when more
thread management is performed, and a small number of long-running parallel
code executions, when less thread management is required. The results show that
OpenCL and OpenMP each win in one of the two conditions. We argue that while
using OpenMP requires less setup, OpenCL can be a viable substitute for OpenMP
from a performance point of view, especially when a high number of thread
invocations is required. We also provide a number of potential pitfalls to
watch for when moving from OpenMP to OpenCL.
"
780,GraphMat: High performance graph analytics made productive,"  Given the growing importance of large-scale graph analytics, there is a need
to improve the performance of graph analysis frameworks without compromising on
productivity. GraphMat is our solution to bridge this gap between a
user-friendly graph analytics framework and native, hand-optimized code.
GraphMat functions by taking vertex programs and mapping them to high
performance sparse matrix operations in the backend. We get the productivity
benefits of a vertex programming framework without sacrificing performance.
GraphMat is in C++, and we have been able to write a diverse set of graph
algorithms in this framework with the same effort compared to other vertex
programming frameworks. GraphMat performs 1.2-7X faster than high performance
frameworks such as GraphLab, CombBLAS and Galois. It achieves better multicore
scalability (13-15X on 24 cores) than other frameworks and is 1.2X off native,
hand-optimized code on a variety of different graph algorithms. Since GraphMat
performance depends mainly on a few scalable and well-understood sparse matrix
operations, GraphMatcan naturally benefit from the trend of increasing
parallelism on future hardware.
"
781,Communication Patterns in Mean Field Models for Wireless Sensor Networks,"  Wireless sensor networks are usually composed of a large number of nodes, and
with the increasing processing power and power consumption efficiency they are
expected to run more complex protocols in the future. These pose problems in
the field of verification and performance evaluation of wireless networks. In
this paper, we tailor the mean-field theory as a modeling technique to analyze
their behavior. We apply this method to the slotted ALOHA protocol, and
establish results on the long term trends of the protocol within a very large
network, specially regarding the stability of ALOHA-type protocols.
"
782,Are Markov Models Effective for Storage Reliability Modelling?,"  Continuous Time Markov Chains (CTMC) have been used extensively to model
reliability of storage systems. While the exponentially distributed sojourn
time of Markov models is widely known to be unrealistic (and it is necessary to
consider Weibull-type models for components such as disks), recent work has
also highlighted some additional infirmities with the CTMC model, such as the
ability to handle repair times. Due to the memoryless property of these models,
any failure or repair of one component resets the ""clock"" to zero with any
partial repair or aging in some other subsystem forgotten. It has therefore
been argued that simulation is the only accurate technique available for
modelling the reliability of a storage system with multiple components.
  We show how both the above problematic aspects can be handled when we
consider a careful set of approximations in a detailed model of the system. A
detailed model has many states, and the transitions between them and the
current state captures the ""memory"" of the various components. We model a
non-exponential distribution using a sum of exponential distributions, along
with the use of a CTMC solver in a probabilistic model checking tool that has
support for reducing large state spaces. Furthermore, it is possible to get
results close to what is obtained through simulation and at much lower cost.
"
783,"Hitting Times in Markov Chains with Restart and their Application to
  Network Centrality","  Motivated by applications in telecommunications, computer scienceand physics,
we consider a discrete-time Markov process withrestart. At each step the
process eitherwith a positive probability restarts from a given distribution,
orwith the complementary probability continues according to a Markovtransition
kernel. The main contribution of the present work is thatwe obtain an explicit
expression for the expectation of the hittingtime (to a given target set) of
the process with restart.The formula is convenient when considering the problem
of optimizationof the expected hitting time with respect to the restart
probability.We illustrate our results with two examplesin uncountable and
countable state spaces andwith an application to network centrality.
"
784,"Separable projection integrals for higher-order correlators of the
  cosmic microwave sky: Acceleration by factors exceeding 100","  We present a case study describing efforts to optimise and modernise ""Modal"",
the simulation and analysis pipeline used by the Planck satellite experiment
for constraining general non-Gaussian models of the early universe via the
bispectrum (or three-point correlator) of the cosmic microwave background
radiation. We focus on one particular element of the code: the projection of
bispectra from the end of inflation to the spherical shell at decoupling, which
defines the CMB we observe today. This code involves a three-dimensional inner
product between two functions, one of which requires an integral, on a
non-rectangular domain containing a sparse grid. We show that by employing
separable methods this calculation can be reduced to a one-dimensional
summation plus two integrations, reducing the overall dimensionality from four
to three. The introduction of separable functions also solves the issue of the
non-rectangular sparse grid. This separable method can become unstable in
certain cases and so the slower non-separable integral must be calculated
instead. We present a discussion of the optimisation of both approaches. We
show significant speed-ups of ~100x, arising from a combination of algorithmic
improvements and architecture-aware optimisations targeted at improving thread
and vectorisation behaviour. The resulting MPI/OpenMP hybrid code is capable of
executing on clusters containing processors and/or coprocessors, with
strong-scaling efficiency of 98.6% on up to 16 nodes. We find that a single
coprocessor outperforms two processor sockets by a factor of 1.3x and that
running the same code across a combination of both microarchitectures improves
performance-per-node by a factor of 3.38x. By making bispectrum calculations
competitive with those for the power spectrum (or two-point correlator) we are
now able to consider joint analysis for cosmological science exploitation of
new data.
"
785,"On data skewness, stragglers, and MapReduce progress indicators","  We tackle the problem of predicting the performance of MapReduce
applications, designing accurate progress indicators that keep programmers
informed on the percentage of completed computation time during the execution
of a job. Through extensive experiments, we show that state-of-the-art progress
indicators (including the one provided by Hadoop) can be seriously harmed by
data skewness, load unbalancing, and straggling tasks. This is mainly due to
their implicit assumption that the running time depends linearly on the input
size. We thus design a novel profile-guided progress indicator, called
NearestFit, that operates without the linear hypothesis assumption and exploits
a careful combination of nearest neighbor regression and statistical curve
fitting techniques. Our theoretical progress model requires fine-grained
profile data, that can be very difficult to manage in practice. To overcome
this issue, we resort to computing accurate approximations for some of the
quantities used in our model through space- and time-efficient data streaming
algorithms. We implemented NearestFit on top of Hadoop 2.6.0. An extensive
empirical assessment over the Amazon EC2 platform on a variety of real-world
benchmarks shows that NearestFit is practical w.r.t. space and time overheads
and that its accuracy is generally very good, even in scenarios where
competitors incur non-negligible errors and wide prediction fluctuations.
Overall, NearestFit significantly improves the current state-of-art on progress
analysis for MapReduce.
"
786,Low-Latency Software Polar Decoders,"  Polar codes are a new class of capacity-achieving error-correcting codes with
low encoding and decoding complexity. Their low-complexity decoding algorithms
rendering them attractive for use in software-defined radio applications where
computational resources are limited. In this work, we present low-latency
software polar decoders that exploit modern processor capabilities. We show how
adapting the algorithm at various levels can lead to significant improvements
in latency and throughput, yielding polar decoders that are suitable for
high-performance software-defined radio applications on modern desktop
processors and embedded-platform processors. These proposed decoders have an
order of magnitude lower latency and memory footprint compared to
state-of-the-art decoders, while maintaining comparable throughput. In
addition, we present strategies and results for implementing polar decoders on
graphical processing units. Finally, we show that the energy efficiency of the
proposed decoders is comparable to state-of-the-art software polar decoders.
"
787,"Run Time Approximation of Non-blocking Service Rates for Streaming
  Systems","  Stream processing is a compute paradigm that promises safe and efficient
parallelism. Modern big-data problems are often well suited for stream
processing's throughput-oriented nature. Realization of efficient stream
processing requires monitoring and optimization of multiple communications
links. Most techniques to optimize these links use queueing network models or
network flow models, which require some idea of the actual execution rate of
each independent compute kernel within the system. What we want to know is how
fast can each kernel process data independent of other communicating kernels.
This is known as the ""service rate"" of the kernel within the queueing
literature. Current approaches to divining service rates are static. Modern
workloads, however, are often dynamic. Shared cloud systems also present
applications with highly dynamic execution environments (multiple users,
hardware migration, etc.). It is therefore desirable to continuously re-tune an
application during run time (online) in response to changing conditions. Our
approach enables online service rate monitoring under most conditions,
obviating the need for reliance on steady state predictions for what are
probably non-steady state phenomena. First, some of the difficulties associated
with online service rate determination are examined. Second, the algorithm to
approximate the online non-blocking service rate is described. Lastly, the
algorithm is implemented within the open source RaftLib framework for
validation using a simple microbenchmark as well as two full streaming
applications.
"
788,ALEA: Fine-grain Energy Profiling with Basic Block Sampling,"  Energy efficiency is an essential requirement for all contemporary computing
systems. We thus need tools to measure the energy consumption of computing
systems and to understand how workloads affect it. Significant recent research
effort has targeted direct power measurements on production computing systems
using on-board sensors or external instruments. These direct methods have in
turn guided studies of software techniques to reduce energy consumption via
workload allocation and scaling. Unfortunately, direct energy measurements are
hampered by the low power sampling frequency of power sensors. The coarse
granularity of power sensing limits our understanding of how power is allocated
in systems and our ability to optimize energy efficiency via workload
allocation.
  We present ALEA, a tool to measure power and energy consumption at the
granularity of basic blocks, using a probabilistic approach. ALEA provides
fine-grained energy profiling via statistical sampling, which overcomes the
limitations of power sensing instruments. Compared to state-of-the-art energy
measurement tools, ALEA provides finer granularity without sacrificing
accuracy. ALEA achieves low overhead energy measurements with mean error rates
between 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both
Intel and ARM platforms. The sampling method caps execution time overhead at
approximately 1%. ALEA is thus suitable for online energy monitoring and
optimization. Finally, ALEA is a user-space tool with a portable,
machine-independent sampling method. We demonstrate two use cases of ALEA,
where we reduce the energy consumption of a k-means computational kernel by 37%
and an ocean modelling code by 33%, compared to high-performance execution
baselines, by varying the power optimization strategy between basic blocks.
"
789,"On the BMAP_1, BMAP_2/PH/g, c retrial queueing system","  In this paper, we analyze a retrial queueing system with Batch Markovian
Arrival Processes and two types of customers. The rate of individual repeated
attempts from the orbit is modulated according to a Markov Modulated Poisson
Process. Using the theory of multi-dimensional asymptotically quasi-Toeplitz
Markov chain, we obtain the stability condition and the algorithm for
calculating the stationary state distribution of the system. Main performance
measures are presented. Furthermore, we investigate some optimization problems.
The algorithm for determining the optimal number of guard servers and total
servers is elaborated. Finally, this queueing system is applied to the cellular
wireless network. Numerical results to illustrate the optimization problems and
the impact of retrial on performance measures are provided. We find that the
performance measures are mainly affected by the two types of customers'
arrivals and service patterns, but the retrial rate plays a less crucial role.
"
790,"Designing Installations for Verification of the Model of Active Queue
  Management Discipline RED in the GNS3","  The problem of RED-module mathematical model results verification, based on
GNS3 experimental stand, is discussed in this article. The experimental stand
consists of virtual Cisco router, traffic generator D-ITG and traffic receiver.
The process of construction of such stand is presented. Also, the interaction
between experimental stand and a computer of investigation in order to obtain
and analyze data from stand is revised. A stochastic model of the traffic
management RED type module was built. Verification of the model was carried out
on the NS-2 basis. However, we would like to conduct verification on a real
router. As a result was the task of designing an experimental stand. It was
decided to verify the clean RED algorithm based on Cisco router. For the
construction of the stand software package GNS3 (Graphical Network Simulator)
was chosen. Thus, the purpose of the study is to build on the GNS3 basis a
virtual stand consisting of a Cisco router, a traffic generator and a receiver.
A traffic generator D-ITG (Distributed Internet Traffic Generator) is used as.
"
791,"RIOT OS Paves the Way for Implementation of High-Performance MAC
  Protocols","  Implementing new, high-performance MAC protocols requires real-time features,
to be able to synchronize correctly between different unrelated devices. Such
features are highly desirable for operating wireless sensor networks (WSN) that
are designed to be part of the Internet of Things (IoT). Unfortunately, the
operating systems commonly used in this domain cannot provide such features. On
the other hand, ""bare-metal"" development sacrifices portability, as well as the
mul-titasking abilities needed to develop the rich applications that are useful
in the domain of the Internet of Things. We describe in this paper how we
helped solving these issues by contributing to the development of a port of
RIOT OS on the MSP430 microcontroller, an architecture widely used in
IoT-enabled motes. RIOT OS offers rich and advanced real-time features,
especially the simultaneous use of as many hardware timers as the underlying
platform (microcontroller) can offer. We then demonstrate the effectiveness of
these features by presenting a new implementation, on RIOT OS, of S-CoSenS, an
efficient MAC protocol that uses very low processing power and energy.
"
792,"Power Aware Wireless File Downloading: A Lyapunov Indexing Approach to A
  Constrained Restless Bandit Problem","  This paper treats power-aware throughput maxi-mization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy. In a special case with no
power-constraint, the multi-user heuristic is shown to be throughput optimal.
Simulations are used to demonstrate effectiveness of the heuristic in the
general case. For simple cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
"
793,Understanding Big Data Analytic Workloads on Modern Processors,"  Big data analytics applications play a significant role in data centers, and
hence it has become increasingly important to understand their behaviors in
order to further improve the performance of data center computer systems, in
which characterizing representative workloads is a key practical problem. In
this paper, after investigating three most impor- tant application domains in
terms of page views and daily visitors, we chose 11 repre- sentative data
analytics workloads and characterized their micro-architectural behaviors by
using hardware performance counters, so as to understand the impacts and
implications of data analytics workloads on the systems equipped with modern
superscalar out-of-order processors. Our study reveals that big data analytics
applications themselves share many inherent characteristics, which place them
in a different class from traditional workloads and scale-out services. To
further understand the characteristics of big data analytics work- loads we
performed a correlation analysis of CPI (cycles per instruction) with other
micro- architecture level characteristics and an investigation of the big data
software stack impacts on application behaviors. Our correlation analysis
showed that even though big data ana- lytics workloads own notable pipeline
front end stalls, the main factors affecting the CPI performance are long
latency data accesses rather than the front end stalls. Our software stack
investigation found that the typical big data software stack significantly
contributes to the front end stalls and incurs bigger working set. Finally we
gave several recommen- dations for architects, programmers and big data system
designers with the knowledge acquired from this paper.
"
794,Optimizing Age-of-Information in a Multi-class Queueing System,"  We consider the age-of-information in a multi-class $M/G/1$ queueing system,
where each class generates packets containing status information. Age of
information is a relatively new metric that measures the amount of time that
elapsed between status updates, thus accounting for both the queueing delay and
the delay between packet generation. This gives rise to a tradeoff between
frequency of status updates, and queueing delay. In this paper, we study this
tradeoff in a system with heterogenous users modeled as a multi-class $M/G/1$
queue. To this end, we derive the exact peak age-of-Information (PAoI) profile
of the system, which measures the ""freshness"" of the status information. We
then seek to optimize the age of information, by formulating the problem using
quasiconvex optimization, and obtain structural properties of the optimal
solution.
"
795,Inhomogeneous CTMC Model of a Call Center with Balking and Abandonment,"  This paper considers a nonstationary multiserver queuing model with
abandonment and balking for inbound call centers. We present a continuous time
Markov chain (CTMC) model which captures the important characteristics of an
inbound call center and obtain a numerical solution for its transient state
probabilities using uniformization method with steady-state detection.
Keywords: call center, transient, Markov processes, numerical methods,
uniformization, abandonment, balking
"
796,"Improved repeatability measures for evaluating performance of feature
  detectors","  The most frequently employed measure for performance characterisation of
local feature detectors is repeatability, but it has been observed that this
does not necessarily mirror actual performance. Presented are improved
repeatability formulations which correlate much better with the true
performance of feature detectors. Comparative results for several
state-of-the-art feature detectors are presented using these measures; it is
found that Hessian-based detectors are generally superior at identifying
features when images are subject to various geometric and photometric
transformations.
"
797,The ELAPS Framework: Experimental Linear Algebra Performance Studies,"  Optimal use of computing resources requires extensive coding, tuning and
benchmarking. To boost developer productivity in these time consuming tasks, we
introduce the Experimental Linear Algebra Performance Studies framework
(ELAPS), a multi-platform open source environment for fast yet powerful
performance experimentation with dense linear algebra kernels, algorithms, and
libraries. ELAPS allows users to construct experiments to investigate how
performance and efficiency vary depending on factors such as caching,
algorithmic parameters, problem size, and parallelism. Experiments are designed
either through Python scripts or a specialized GUI, and run on the whole
spectrum of architectures, ranging from laptops to clusters, accelerators, and
supercomputers. The resulting experiment reports provide various metrics and
statistics that can be analyzed both numerically and visually. We demonstrate
the use of ELAPS in four concrete application scenarios and in as many
computing environments, illustrating its practical value in supporting critical
performance decisions.
"
798,"Reward Processes and Performance Simulation in Supermarket Models with
  Different Servers","  Supermarket models with different servers become a key in modeling resource
management of stochastic networks, such as, computer networks, manufacturing
systems and transportation networks. While these different servers always make
analysis of such a supermarket model more interesting, difficult and
challenging. This paper provides a new novel method for analyzing the
supermarket model with different servers through a multi-dimensional
continuous-time Markov reward processes. Firstly, the utility functions are
constructed for expressing a routine selection mechanism that depends on queue
lengths, on service rates, and on some probabilities of individual preference.
Then applying the continuous-time Markov reward processes, some segmented
stochastic integrals of the random reward function are established by means of
an event-driven technique. Based on this, the mean of the random reward
function in a finite time period is effectively computed by means of the state
jump points of the Markov reward process, and also the mean of the discounted
random reward function in an infinite time period can be calculated through the
same event-driven technique. Finally, some simulation experiments are given to
indicate how the expected queue length of each server depends on the main
parameters of this supermarket model.
"
799,"Triple State QuickSort, A replacement for the C/C++ library qsort","  An industrial grade Quicksort function along with its new algorithm is
presented. Compared to 4 other well known implementations of Quicksort, the new
algorithm reduces both the number of comparisons and swaps in most cases while
staying close to the best of the 4 in worst cases. We trade space for
performance, at the price of n/2 temporary extra spaces in the worst case. Run
time tests reveal an overall improvement of at least 15.8% compared to the
overall best of the other 4 functions. Furthermore, our function scores a 32.7%
run time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our
function is pointer based, which is meant as a replacement for the C/C++
library qsort(). But we also provide an array based function of the same
algorithm for easy porting to different programming languages.
"
800,"Performance analysis of the Kahan-enhanced scalar product on current
  multicore processors","  We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent Intel processors. Using low-level instruction analysis and the
execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. We also investigate the impact of
architectural changes across four generations of Intel Xeon processors.
"
801,"Data dependent energy modelling for worst case energy consumption
  analysis","  Safely meeting Worst Case Energy Consumption (WCEC) criteria requires
accurate energy modeling of software. We investigate the impact of instruction
operand values upon energy consumption in cacheless embedded processors.
Existing instruction-level energy models typically use measurements from random
input data, providing estimates unsuitable for safe WCEC analysis.
  We examine probabilistic energy distributions of instructions and propose a
model for composing instruction sequences using distributions, enabling WCEC
analysis on program basic blocks. The worst case is predicted with statistical
analysis. Further, we verify that the energy of embedded benchmarks can be
characterised as a distribution, and compare our proposed technique with other
methods of estimating energy consumption.
"
802,"Dynamic Allocation Problems in Loss Network Systems with Advanced
  Reservation","  We consider a class of well-known dynamic resource allocation models in loss
network systems with advanced reservation. The most important performance
measure in any loss network system is to compute its blocking probability,
i.e., the probability of an arriving customer in equilibrium finds a fully
utilized system (thereby getting rejected by the system). In this paper, we
derive upper bounds on the asymptotic blocking probabilities for such systems
in high-volume regimes. There have been relatively few results on loss network
systems with advanced reservation due to its inherent complexity. The
theoretical results find applications in a wide class of revenue management
problems in systems with reusable resources and advanced reservation, e.g.,
hotel room, car rental and workforce management. We propose a simple control
policy called the improved class selection policy (ICSP) based on solving a
continuous knapsack problem, similar in spirit to the one proposed in Levi and
Radovanovic (2010). Using our results derived for loss network systems with
advanced reservation, we show the ICSP performs asymptotically near-optimal in
high-volume regimes.
"
803,Critically loaded k-limited polling systems,"  We consider a two-queue polling model with switch-over times and $k$-limited
service (serve at most $k_i$ customers during one visit period to queue $i$) in
each queue. The major benefit of the $k$-limited service discipline is that it
- besides bounding the cycle time - effectuates prioritization by assigning
different service limits to different queues. System performance is studied in
the heavy-traffic regime, in which one of the queues becomes critically loaded
with the other queue remaining stable. By using a singular-perturbation
technique, we rigorously prove heavy-traffic limits for the joint queue-length
distribution. Moreover, it is observed that an interchange exists among the
first two moments in service and switch-over times such that the HT limits
remain unchanged. Not only do the rigorously proven results readily carry over
to $N$($\geq2$) queue polling systems, but one can also easily relax the
distributional assumptions. The results and insights of this note prove their
worth in the performance analysis of Wireless Personal Area Networks (WPAN) and
mobile networks, where different users compete for access to the shared scarce
resources.
"
804,"Network Simulator - Vis\~ao Geral da Ferramenta de Simula\c{c}\~ao de
  Redes","  This paper describes NS - Network Simulator, the computer networks simulation
tool. We offer an overview NS, and also analyze its characteristics and
functions. Finally, we present in detail all steps for preparing a simulation
of a simple model in NS.
"
805,Supercharge me: Boost Router Convergence with SDN,"  Software Defined Networking (SDN) is a promising approach for improving the
performance and manageability of future network architectures. However, little
work has gone into using SDN to improve the performance and manageability of
existing networks without requiring a major overhaul of the existing network
infrastructure.
  In this paper, we show how we can dramatically improve, or supercharge, the
performance of existing IP routers by combining them with SDN-enabled equipment
in a novel way. More particularly, our supercharged solution substantially
reduces the convergence time of an IP router upon link or node failure without
inducing any reconfiguration of the IP router itself. Our key insight is to use
the SDN controller to precompute backup forwarding entries and immediately
activate them upon failure, enabling almost immediate data-plane recovery,
while letting the router converge at its typical slow pace. By boosting
existing equipment's performance, we not only increase their lifetime but also
provide new incentives for network operators to kickstart SDN deployment.
  We implemented a fully functional ""supercharger"" and use it to boost the
convergence performance of a Cisco Nexus 7k router. Using a FPGA-based traffic
generator, we show that our supercharged router systematically converges within
~150ms, a 900x reduction with respect to its normal convergence time under
similar conditions.
"
806,Flexible Queueing Architectures,"  We study a multi-server model with $n$ flexible servers and $n$ queues,
connected through a bipartite graph, where the level of flexibility is captured
by the graph's average degree, $d_n$. Applications in content replication in
data centers, skill-based routing in call centers, and flexible supply chains
are among our main motivations.
  We focus on the scaling regime where the system size $n$ tends to infinity,
while the overall traffic intensity stays fixed. We show that a large capacity
region and an asymptotically vanishing queueing delay are simultaneously
achievable even under limited flexibility ($d_n \ll n$). Our main results
demonstrate that, when $d_n\gg \ln n$, a family of expander-graph-based
flexibility architectures has a capacity region that is within a constant
factor of the maximum possible, while simultaneously ensuring a diminishing
queueing delay for all arrival rate vectors in the capacity region. Our
analysis is centered around a new class of virtual-queue-based scheduling
policies that rely on dynamically constructed job-to-server assignments on the
connectivity graph. For comparison, we also analyze a natural family of modular
architectures, which is simpler but has provably weaker performance.
"
807,"Efficient FFT mapping on GPU for radar processing application: modeling
  and implementation","  General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel
Haswell) increasingly add GPU computing power to the former multicore
architectures. When used for embedded applications (for us, Synthetic aperture
radar) with intensive signal processing requirements, they must constantly
compute convolution algorithms, such as the famous Fast Fourier Transform. Due
to its ""fractal"" nature (the typical butterfly shape, with larger FFTs defined
as combination of smaller ones with auxiliary data array transpose functions),
one can hope to compute analytically the size of the largest FFT that can be
performed locally on an elementary GPU compute block. Then, the full
application must be organized around this given building block size. Now, due
to phenomena involved in the data transfers between various memory levels
across CPUs and GPUs, the optimality of such a scheme is only loosely
predictable (as communications tend to overcome in time the complexity of
computations). Therefore a mix of (theoretical) analytic approach and
(practical) runtime validation is here needed. As we shall illustrate, this
occurs at both stage, first at the level of deciding on a given elementary FFT
block size, then at the full application level.
"
808,"OR-Benchmark: An Open and Reconfigurable Digital Watermarking
  Benchmarking Framework","  Benchmarking digital watermarking algorithms is not an easy task because
different applications of digital watermarking often have very different sets
of requirements and trade-offs between conflicting requirements. While there
have been some general-purpose digital watermarking benchmarking systems
available, they normally do not support complicated benchmarking tasks and
cannot be easily reconfigured to work with different watermarking algorithms
and testing conditions. In this paper, we propose OR-Benchmark, an open and
highly reconfigurable general-purpose digital watermarking benchmarking
framework, which has the following two key features: 1) all the interfaces are
public and general enough to support all watermarking applications and
benchmarking tasks we can think of; 2) end users can easily extend the
functionalities and freely configure what watermarking algorithms are tested,
what system components are used, how the benchmarking process runs, and what
results should be produced. We implemented a prototype of this framework as a
MATLAB software package and used it to benchmark a number of digital
watermarking algorithms involving two types of watermarks for content
authentication and self-restoration purposes. The benchmarking results
demonstrated the advantages of the proposed benchmarking framework, and also
gave us some useful insights about existing image authentication and
self-restoration watermarking algorithms which are an important but less
studied topic in digital watermarking.
"
809,Benchmarking Big Data Systems: State-of-the-Art and Future Directions,"  The great prosperity of big data systems such as Hadoop in recent years makes
the benchmarking of these systems become crucial for both research and industry
communities. The complexity, diversity, and rapid evolution of big data systems
gives rise to various new challenges about how we design generators to produce
data with the 4V properties (i.e. volume, velocity, variety and veracity), as
well as implement application-specific but still comprehensive workloads.
However, most of the existing big data benchmarks can be described as attempts
to solve specific problems in benchmarking systems. This article investigates
the state-of-the-art in benchmarking big data systems along with the future
challenges to be addressed to realize a successful and efficient benchmark.
"
810,"OMP2HMPP: Compiler Framework for Energy Performance Trade-off Analysis
  of Automatically Generated Codes","  We present OMP2HMPP, a tool that, in a first step, automatically translates
OpenMP code into various possible transformations of HMPP. In a second step
OMP2HMPP executes all variants to obtain the performance and power consumption
of each transformation. The resulting trade-off can be used to choose the more
convenient version. After running the tool on a set of codes from the Polybench
benchmark we show that the best automatic transformation is equivalent to a
manual one done by an expert. Compared with original OpenMP code running in 2
quad-core processors we obtain an average speed-up of 31x and 5.86x factor in
operations per watt.
"
811,"On the Feasibility of Wireless Interconnects for High-throughput Data
  Centers","  Data Centers (DCs) are required to be scalable to large data sets so as to
accommodate ever increasing demands of resource-limited embedded and mobile
devices. Thanks to the availability of recent high data rate millimeter-wave
frequency spectrum such as 60GHz and due to the favorable attributes of this
technology, wireless DC (WDC) exhibits the potentials of being a promising
solution especially for small to medium scale DCs. This paper investigates the
problem of throughput scalability of WDCs using the established theory of the
asymptotic throughput of wireless multi-hop networks that are primarily
proposed for homogeneous traffic conditions. The rate-heterogeneous traffic
distribution of a data center however, requires the asymptotic heterogeneous
throughput knowledge of a wireless network in order to study the performance
and feasibility of WDCs for practical purposes. To answer these questions this
paper presents a lower bound for the throughput scalability of a multi-hop
rate-heterogeneous network when traffic generation rates of all nodes are
similar, except one node. We demonstrate that the throughput scalability of
conventional multi-hopping and the spatial reuse of the above bi-rate network
is inefficient and henceforth develop a speculative 2-partitioning scheme that
improves the network throughput scaling potentials. A better lower bound of the
throughput is then obtained. Finally, we obtain the throughput scaling of an
i.i.d. rate-heterogeneous network and obtain its lower bound. Again we propose
a speculative 2-partitioning scheme to achieve a network with higher throughput
in terms of improved lower bound. All of the obtained results have been
verified using simulation experiments.
"
812,"Short Note on Costs of Floating Point Operations on current x86-64
  Architectures: Denormals, Overflow, Underflow, and Division by Zero","  Simple floating point operations like addition or multiplication on
normalized floating point values can be computed by current AMD and Intel
processors in three to five cycles. This is different for denormalized numbers,
which appear when an underflow occurs and the value can no longer be
represented as a normalized floating-point value. Here the costs are about two
magnitudes higher.
"
813,"Mapping Large Scale Research Metadata to Linked Data: A Performance
  Comparison of HBase, CSV and XML","  OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a
database of all EC FP7 and H2020 funded research projects, including metadata
of their results (publications and datasets). These data are stored in an HBase
NoSQL database, post-processed, and exposed as HTML for human consumption, and
as XML through a web service interface. As an intermediate format to facilitate
statistical computations, CSV is generated internally. To interlink the
OpenAIRE data with related data on the Web, we aim at exporting them as Linked
Open Data (LOD). The LOD export is required to integrate into the overall data
processing workflow, where derived data are regenerated from the base data
every day. We thus faced the challenge of identifying the best-performing
conversion approach.We evaluated the performances of creating LOD by a
MapReduce job on top of HBase, by mapping the intermediate CSV files, and by
mapping the XML output.
"
814,Necessity of Future Information in Admission Control,"  We study the necessity of predictive information in a class of queueing
admission control problems, where a system manager is allowed to divert
incoming jobs up to a fixed rate, in order to minimize the queueing delay
experienced by the admitted jobs.
  Spencer et al. (2014) show that the system's delay performance can be
significantly improved by having access to future information in the form of a
lookahead window, during which the times of future arrivals and services are
revealed. They prove that, while delay under an optimal online policy diverges
to infinity in the heavy-traffic regime, it can stay bounded by making use of
future information. However, the diversion polices of Spencer et al. (2014)
require the length of the lookahead window to grow to infinity at a non-trivial
rate in the heavy-traffic regime, and it remained open whether substantial
performance improvement could still be achieved with less future information.
  We resolve this question to a large extent by establishing an asymptotically
tight lower bound on how much future information is necessary to achieve
superior performance, which matches the upper bound of Spencer et al. (2014) up
to a constant multiplicative factor. Our result hence demonstrates that the
system's heavy-traffic delay performance is highly sensitive to the amount of
future information available. Our proof is based on analyzing certain excursion
probabilities of the input sample paths, and exploiting a connection between a
policy's diversion decisions and subsequent server idling, which may be of
independent interest for related dynamic resource allocation problems.
"
815,"A Non-stationary Service Curve Model for Performance Analysis of
  Transient Phases","  Steady-state solutions for a variety of relevant queueing systems are known
today, e.g., from queueing theory, effective bandwidths, and network calculus.
The behavior during transient phases, on the other hand, is understood to a
much lesser extent as its analysis poses significant challenges. Considering
the majority of short-lived flows, transient effects that have diverse causes,
such as TCP slow start, sleep scheduling in wireless networks, or signalling in
cellular networks, are, however, predominant. This paper contributes a general
model of regenerative service processes to characterize the transient behavior
of systems. The model leads to a notion of non-stationary service curves that
can be conveniently integrated into the framework of the stochastic network
calculus. We derive respective models of sleep scheduling and show the
significant impact of transient phases on backlogs and delays. We also consider
measurement methods that estimate the service of an unknown system from
observations of selected probe traffic. We find that the prevailing rate
scanning method does not recover the service during transient phases well. This
limitation is fundamental as it is explained by the non-convexity of
non-stationary service curves. A second key difficulty is proven to be due to
the super-additivity of network service processes. We devise a novel two-phase
probing technique that first determines a minimal pattern of probe traffic.
This probe is used to obtain an accurate estimate of the unknown transient
service.
"
816,A Decentralized Parallelization-in-Time Approach with Parareal,"  With steadily increasing parallelism for high-performance architectures,
simulations requiring a good strong scalability are prone to be limited in
scalability with standard spatial-decomposition strategies at a certain amount
of parallel processors. This can be a show-stopper if the simulation results
have to be computed with wallclock time restrictions (e.g.\,for weather
forecasts) or as fast as possible (e.g. for urgent computing). Here, the
time-dimension is the only one left for parallelization and we focus on
Parareal as one particular parallelization-in-time method.
  We discuss a software approach for making Parareal parallelization
transparent for application developers, hence allowing fast prototyping for
Parareal. Further, we introduce a decentralized Parareal which results in
autonomous simulation instances which only require communicating with the
previous and next simulation instances, hence with strong locality for
communication. This concept is evaluated by a prototypical solver for the
rotational shallow-water equations which we use as a representative black-box
solver.
"
817,"A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with
  Buffering","  The purpose of this paper is to analyze the so-called back-off technique of
the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to
existing models, packets arriving when a station (or node) is in back-off state
are not discarded, but are stored in a buffer of infinite capacity. As in
previous studies, the key point of our analysis hinges on the assumption that
the time on the channel is viewed as a random succession of transmission slots
(whose duration corresponds to the length of a packet) and mini-slots during
which the back-o? of the station is decremented. These events occur
independently, with given probabilities. The state of a node is represented by
a two-dimensional Markov chain in discrete-time, formed by the back-off counter
and the number of packets at the station. Two models are proposed both of which
are shown to cope reasonably well with the physical principles of the protocol.
The stabillity (ergodicity) conditions are obtained and interpreted in terms of
maximum throughput. Several approximations related to these models are also
discussed.
"
818,"Collective Mind, Part II: Towards Performance- and Cost-Aware Software
  Engineering as a Natural Science","  Nowadays, engineers have to develop software often without even knowing which
hardware it will eventually run on in numerous mobile phones, tablets,
desktops, laptops, data centers, supercomputers and cloud services.
Unfortunately, optimizing compilers are not keeping pace with ever increasing
complexity of computer systems anymore and may produce severely underperforming
executable codes while wasting expensive resources and energy.
  We present our practical and collaborative solution to this problem via
light-weight wrappers around any software piece when more than one
implementation or optimization choice available. These wrappers are connected
with a public Collective Mind autotuning infrastructure and repository of
knowledge (c-mind.org/repo) to continuously monitor various important
characteristics of these pieces (computational species) across numerous
existing hardware configurations together with randomly selected optimizations.
Similar to natural sciences, we can now continuously track winning solutions
(optimizations for a given hardware) that minimize all costs of a computation
(execution time, energy spent, code size, failures, memory and storage
footprint, optimization time, faults, contentions, inaccuracy and so on) of a
given species on a Pareto frontier along with any unexpected behavior. The
community can then collaboratively classify solutions, prune redundant ones,
and correlate them with various features of software, its inputs (data sets)
and used hardware either manually or using powerful predictive analytics
techniques. Our approach can then help create a large, realistic, diverse,
representative, and continuously evolving benchmark with related optimization
knowledge while gradually covering all possible software and hardware to be
able to predict best optimizations and improve compilers and hardware depending
on usage scenarios and requirements.
"
819,"Performance Characterization of In-Memory Data Analytics on a Modern
  Cloud Server","  In last decade, data analytics have rapidly progressed from traditional
disk-based processing to modern in-memory processing. However, little effort
has been devoted at enhancing performance at micro-architecture level. This
paper characterizes the performance of in-memory data analytics using Apache
Spark framework. We use a single node NUMA machine and identify the bottlenecks
hampering the scalability of workloads. We also quantify the inefficiencies at
micro-architecture level for various data analysis workloads. Through empirical
evaluation, we show that spark workloads do not scale linearly beyond twelve
threads, due to work time inflation and thread level load imbalance. Further,
at the micro-architecture level, we observe memory bound latency to be the
major cause of work time inflation.
"
820,Characterization and Architectural Implications of Big Data Workloads,"  Big data areas are expanding in a fast way in terms of increasing workloads
and runtime systems, and this situation imposes a serious challenge to workload
characterization, which is the foundation of innovative system and architecture
design. The previous major efforts on big data benchmarking either propose a
comprehensive but a large amount of workloads, or only select a few workloads
according to so-called popularity, which may lead to partial or even biased
observations. In this paper, on the basis of a comprehensive big data benchmark
suite---BigDataBench, we reduced 77 workloads to 17 representative workloads
from a micro-architectural perspective. On a typical state-of-practice
platform---Intel Xeon E5645, we compare the representative big data workloads
with SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive
workload characterization, we have the following observations. First, the big
data workloads are data movement dominated computing with more branch
operations, taking up to 92% percentage in terms of instruction mix, which
places them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC
(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark
based big data workloads have higher front-end stalls. Comparing with the
traditional workloads i. e. PARSEC, the big data workloads have larger
instructions footprint. But we also note that, in addition to varied
instruction-level parallelism, there are significant disparities of front-end
efficiencies among different big data workloads. Third, we found complex
software stacks that fail to use state-of-practise processors efficiently are
one of the main factors leading to high front-end stalls. For the same
workloads, the L1I cache miss rates have one order of magnitude differences
among diverse implementations with different software stacks.
"
821,"Large-scale Optimization-based Non-negative Computational Framework for
  Diffusion Equations: Parallel Implementation and Performance Studies","  It is well-known that the standard Galerkin formulation, which is often the
formulation of choice under the finite element method for solving self-adjoint
diffusion equations, does not meet maximum principles and the non-negative
constraint for anisotropic diffusion equations. Recently, optimization-based
methodologies that satisfy maximum principles and the non-negative constraint
for steady-state and transient diffusion-type equations have been proposed. To
date, these methodologies have been tested only on small-scale academic
problems. The purpose of this paper is to systematically study the performance
of the non-negative methodology in the context of high performance computing
(HPC). PETSc and TAO libraries are, respectively, used for the parallel
environment and optimization solvers. For large-scale problems, it is important
for computational scientists to understand the computational performance of
current algorithms available in these scientific libraries. The numerical
experiments are conducted on the state-of-the-art HPC systems, and a
single-core performance model is used to better characterize the efficiency of
the solvers. Our studies indicate that the proposed non-negative computational
framework for diffusion-type equations exhibits excellent strong scaling for
real-world large-scale problems.
"
822,"Architecture-Aware Configuration and Scheduling of Matrix Multiplication
  on Asymmetric Multicore Processors","  Asymmetric multicore processors (AMPs) have recently emerged as an appealing
technology for severely energy-constrained environments, especially in mobile
appliances where heterogeneity in applications is mainstream. In addition,
given the growing interest for low-power high performance computing, this type
of architectures is also being investigated as a means to improve the
throughput-per-Watt of complex scientific applications.
  In this paper, we design and embed several architecture-aware optimizations
into a multi-threaded general matrix multiplication (gemm), a key operation of
the BLAS, in order to obtain a high performance implementation for ARM
big.LITTLE AMPs. Our solution is based on the reference implementation of gemm
in the BLIS library, and integrates a cache-aware configuration as well as
asymmetric--static and dynamic scheduling strategies that carefully tune and
distribute the operation's micro-kernels among the big and LITTLE cores of the
target processor. The experimental results on a Samsung Exynos 5422, a
system-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the
big.LITTLE model, expose that our cache-aware versions of gemm with asymmetric
scheduling attain important gains in performance with respect to its
architecture-oblivious counterparts while exploiting all the resources of the
AMP to deliver considerable energy efficiency.
"
823,Best bang for your buck: GPU nodes for GROMACS biomolecular simulations,"  The molecular dynamics simulation package GROMACS runs efficiently on a wide
variety of hardware from commodity workstations to high performance computing
clusters. Hardware features are well exploited with a combination of SIMD,
multi-threading, and MPI-based SPMD/MPMD parallelism, while GPUs can be used as
accelerators to compute interactions offloaded from the CPU. Here we evaluate
which hardware produces trajectories with GROMACS 4.6 or 5.0 in the most
economical way. We have assembled and benchmarked compute nodes with various
CPU/GPU combinations to identify optimal compositions in terms of raw
trajectory production rate, performance-to-price ratio, energy efficiency, and
several other criteria. Though hardware prices are naturally subject to trends
and fluctuations, general tendencies are clearly visible. Adding any type of
GPU significantly boosts a node's simulation performance. For inexpensive
consumer-class GPUs this improvement equally reflects in the
performance-to-price ratio. Although memory issues in consumer-class GPUs could
pass unnoticed since these cards do not support ECC memory, unreliable GPUs can
be sorted out with memory checking tools. Apart from the obvious determinants
for cost-efficiency like hardware expenses and raw performance, the energy
consumption of a node is a major cost factor. Over the typical hardware
lifetime until replacement of a few years, the costs for electrical power and
cooling can become larger than the costs of the hardware itself. Taking that
into account, nodes with a well-balanced ratio of CPU and consumer-class GPU
resources produce the maximum amount of GROMACS trajectory over their lifetime.
"
824,Deadline-aware Power Management in Data Centers,"  We study the dynamic power optimization problem in data centers. We formulate
and solve the following offline problem: in which slot which server has to be
assigned to which job; and in which slot which server has to be switched ON or
OFF so that the total power is optimal for some time horizon. We show that the
offline problem is a new version of generalized assignment problem including
new constraints issuing from deadline characteristics of jobs and difference of
activation energy of servers. We propose an online algorithm that solves the
problem heuristically and compare it to randomized routing.
"
825,Window Flow Control Systems with Random Service,"  We present an extension of the window flow control analysis by R. Agrawal
et.al. (Reference [1]), C.-S. Chang (Reference [6]), and C.-S. Chang et. al.
(Reference [8]) to a system with random service time and fixed feedback delay.
We consider two network service models. In the first model, the network service
process itself has no time correlations. The second model addresses a two-state
Markov-modulated service.
"
826,The Anatomy of Large-Scale Distributed Graph Algorithms,"  The increasing complexity of the software/hardware stack of modern
supercomputers results in explosion of parameters. The performance analysis
becomes a truly experimental science, even more challenging in the presence of
massive irregularity and data dependency. We analyze how the existing body of
research handles the experimental aspect in the context of distributed graph
algorithms (DGAs). We distinguish algorithm-level contributions, often
prioritized by authors, from runtime-level concerns that are harder to place.
We show that the runtime is such an integral part of DGAs that experimental
results are difficult to interpret and extrapolate without understanding the
properties of the runtime used. We argue that in order to gain understanding
about the impact of runtimes, more information needs to be gathered. To begin
this process, we provide an initial set of recommendations for describing DGA
results based on our analysis of the current state of the field.
"
827,Coordinated Dynamic Spectrum Management of LTE-U and Wi-Fi Networks,"  This paper investigates the co-existence of Wi-Fi and LTE in emerging
unlicensed frequency bands which are intended to accommodate multiple radio
access technologies. Wi-Fi and LTE are the two most prominent access
technologies being deployed today, motivating further study of the inter-system
interference arising in such shared spectrum scenarios as well as possible
techniques for enabling improved co-existence. An analytical model for
evaluating the baseline performance of co-existing Wi-Fi and LTE is developed
and used to obtain baseline performance measures. The results show that both
Wi-Fi and LTE networks cause significant interference to each other and that
the degradation is dependent on a number of factors such as power levels and
physical topology. The model-based results are partially validated via
experimental evaluations using USRP based SDR platforms on the ORBIT testbed.
Further, inter-network coordination with logically centralized radio resource
management across Wi-Fi and LTE systems is proposed as a possible solution for
improved co-existence. Numerical results are presented showing significant
gains in both Wi-Fi and LTE performance with the proposed inter-network
coordination approach.
"
828,"Dependability Analysis of Control Systems using SystemC and Statistical
  Model Checking","  Stochastic Petri nets are commonly used for modeling distributed systems in
order to study their performance and dependability. This paper proposes a
realization of stochastic Petri nets in SystemC for modeling large embedded
control systems. Then statistical model checking is used to analyze the
dependability of the constructed model. Our verification framework allows users
to express a wide range of useful properties to be verified which is
illustrated through a case study.
"
829,How Data Volume Affects Spark Based Data Analytics on a Scale-up Server,"  Sheer increase in volume of data over the last decade has triggered research
in cluster computing frameworks that enable web enterprises to extract big
insights from big data. While Apache Spark is gaining popularity for exhibiting
superior scale-out performance on the commodity machines, the impact of data
volume on the performance of Spark based data analytics in scale-up
configuration is not well understood. We present a deep-dive analysis of Spark
based applications on a large scale-up server machine. Our analysis reveals
that Spark based data analytics are DRAM bound and do not benefit by using more
than 12 cores for an executor. By enlarging input data size, application
performance degrades significantly due to substantial increase in wait time
during I/O operations and garbage collection, despite 10\% better instruction
retirement rate (due to lower L1 cache misses and higher core utilization). We
match memory behaviour with the garbage collector to improve performance of
applications between 1.6x to 3x.
"
830,Low Delay Random Linear Coding and Scheduling Over Multiple Interfaces,"  Multipath transport protocols like MPTCP transfer data across multiple routes
in parallel and deliver it in order at the receiver. When the delay on one or
more of the paths is variable, as is commonly the case, out of order arrivals
are frequent and head of line blocking leads to high latency. This is
exacerbated when packet loss, which is also common with wireless links, is
tackled using ARQ. This paper introduces Stochastic Earliest Delivery Path
First (S-EDPF), a resilient low delay packet scheduler for multipath transport
protocols. S-EDPF takes explicit account of the stochastic nature of paths and
uses this to minimise in-order delivery delay. S-EDPF also takes account of
FEC, jointly scheduling transmission of information and coded packets and in
this way allows lossy links to reduce delay and improve resiliency, rather than
degrading performance as usually occurs with existing multipath systems. We
implement S-EDPF as a multi-platform application that does not require
administration privileges nor modifications to the operating system and has
negligible impact on energy consumption. We present a thorough experimental
evaluation in both controlled environments and into the wild, revealing
dramatic gains in delay performance compared to existing approaches.
"
831,Scalable Reliability Modelling of RAID Storage Subsystems,"  Reliability modelling of RAID storage systems with its various components
such as RAID controllers, enclosures, expanders, interconnects and disks is
important from a storage system designer's point of view. A model that can
express all the failure characteristics of the whole RAID storage system can be
used to evaluate design choices, perform cost reliability trade-offs and
conduct sensitivity analyses. However, including such details makes the
computational models of reliability quickly infeasible.
  We present a CTMC reliability model for RAID storage systems that scales to
much larger systems than heretofore reported and we try to model all the
components as accurately as possible. We use several state-space reduction
techniques at the user level, such as aggregating all in-series components and
hierarchical decomposition, to reduce the size of our model. To automate
computation of reliability, we use the PRISM model checker as a CTMC solver
where appropriate. Our modelling techniques using PRISM are more practical (in
both time and effort) compared to previously reported Monte-Carlo simulation
techniques.
  Our model for RAID storage systems (that includes, for example, disks,
expanders, enclosures) uses Weibull distributions for disks and, where
appropriate, correlated failure modes for disks, while we use exponential
distributions with independent failure modes for all other components. To use
the CTMC solver, we approximate the Weibull distribution for a disk using sum
of exponentials and we confirm that this model gives results that are in
reasonably good agreement with those from the sequential Monte Carlo simulation
methods for RAID disk subsystems reported in literature earlier. Using a
combination of scalable techniques, we are able to model and compute
reliability for fairly large configurations with upto 600 disks using this
model.
"
832,Rethinking the Intercept Probability of Random Linear Network Coding,"  This letter considers a network comprising a transmitter, which employs
random linear network coding to encode a message, a legitimate receiver, which
can recover the message if it gathers a sufficient number of linearly
independent coded packets, and an eavesdropper. Closed-form expressions for the
probability of the eavesdropper intercepting enough coded packets to recover
the message are derived. Transmission with and without feedback is studied.
Furthermore, an optimization model that minimizes the intercept probability
under delay and reliability constraints is presented. Results validate the
proposed analysis and quantify the secrecy gain offered by a feedback link from
the legitimate receiver.
"
833,On Designing and Testing Distributed Virtual Environments,"  Distributed Real-Time (DRT) systems are among the most complex software
systems to design, test, maintain and evolve. The existence of components
distributed over a network often conflicts with real-time requirements, leading
to design strategies that depend on domain- and even application-specific
knowledge. Distributed Virtual Environment (DVE) systems are DRT systems that
connect multiple users instantly with each other and with a shared virtual
space over a network. DVE systems deviate from traditional DRT systems in the
importance of the quality of the end user experience. We present an analysis of
important, but challenging, issues in the design, testing and evaluation of DVE
systems through the lens of experiments with a concrete DVE, OpenSimulator. We
frame our observations within six dimensions of well-known design concerns:
correctness, fault tolerance/prevention, scalability, time sensitivity,
consistency, and overhead of distribution. Furthermore, we place our
experimental work in a broader historical context, showing that these
challenges are intrinsic to DVEs and suggesting lines of future research.
"
834,"Personalized QoS Prediction of Cloud Services via Learning
  Neighborhood-based Model","  The explosion of cloud services on the Internet brings new challenges in
service discovery and selection. Particularly, the demand for efficient
quality-of-service (QoS) evaluation is becoming urgently strong. To address
this issue, this paper proposes neighborhood-based approach for QoS prediction
of cloud services by taking advantages of collaborative intelligence. Different
from heuristic collaborative filtering and matrix factorization, we define a
formal neighborhood-based prediction framework which allows an efficient global
optimization scheme, and then exploit different baseline estimate component to
improve predictive performance. To validate the proposed methods, a large-scale
QoS-specific dataset which consists of invocation records from 339 service
users on 5,825 web services on a world-scale distributed network is used.
Experimental results demonstrate that the learned neighborhood-based models can
overcome existing difficulties of heuristic collaborative filtering methods and
achieve superior performance than state-of-the-art prediction methods.
"
835,Performance-oriented DevOps: A Research Agenda,"  DevOps is a trend towards a tighter integration between development (Dev) and
operations (Ops) teams. The need for such an integration is driven by the
requirement to continuously adapt enterprise applications (EAs) to changes in
the business environment. As of today, DevOps concepts have been primarily
introduced to ensure a constant flow of features and bug fixes into new
releases from a functional perspective. In order to integrate a non-functional
perspective into these DevOps concepts this report focuses on tools,
activities, and processes to ensure one of the most important quality
attributes of a software system, namely performance.
  Performance describes system properties concerning its timeliness and use of
resources. Common metrics are response time, throughput, and resource
utilization. Performance goals for EAs are typically defined by setting upper
and/or lower bounds for these metrics and specific business transactions. In
order to ensure that such performance goals can be met, several activities are
required during development and operation of these systems as well as during
the transition from Dev to Ops. Activities during development are typically
summarized by the term Software Performance Engineering (SPE), whereas
activities during operations are called Application Performance Management
(APM). SPE and APM were historically tackled independently from each other, but
the newly emerging DevOps concepts require and enable a tighter integration
between both activity streams. This report presents existing solutions to
support this integration as well as open research challenges in this area.
"
836,Using Genetic Algorithms to Benchmark the Cloud,"  This paper presents a novel application of Genetic Algorithms(GAs) to
quantify the performance of Platform as a Service (PaaS), a cloud service model
that plays a critical role in both industry and academia. While Cloud
benchmarks are not new, in this novel concept, the authors use a GA to take
advantage of the elasticity in Cloud services in a graceful manner that was not
previously possible. Using Google App Engine, Heroku, and Python Anywhere with
three distinct classes of client computers running our GA codebase, we
quantified the completion time for application of the GA to search for the
parameters of controllers for dynamical systems. Our results show statistically
significant differences in PaaS performance by vendor, and also that the
performance of the PaaS performance is dependent upon the client that uses it.
Results also show the effectiveness of our GA in determining the level of
service of PaaS providers, and for determining if the level of service of one
PaaS vendor is repeatable with another. Such a concept could then increase the
appeal of PaaS Cloud services by making them more financially appealing.
"
837,"Coarse-Grain Performance Estimator for Heterogeneous Parallel Computing
  Architectures like Zynq All-Programmable SoC","  Heterogeneous computing is emerging as a mandatory requirement for
power-efficient system design. With this aim, modern heterogeneous platforms
like Zynq All-Programmable SoC, that integrates ARM-based SMP and programmable
logic, have been designed. However, those platforms introduce large design
cycles consisting on hardware/software partitioning, decisions on granularity
and number of hardware accelerators, hardware/software integration, bitstream
generation, etc.
  This paper presents a performance parallel heterogeneous estimation for
systems where hardware/software co-design and run-time heterogeneous task
scheduling are key. The results show that the programmer can quickly decide,
based only on her/his OmpSs (OpenMP + extensions) application, which is the
co-design that achieves nearly optimal heterogeneous parallel performance,
based on the methodology presented and considering only synthesis estimation
results. The methodology presented reduces the programmer co-design decision
from hours to minutes and shows high potential on hardware/software
heterogeneous parallel performance estimation on the Zynq All-Programmable SoC.
"
838,Performance monitoring for multicore embedded computing systems on FPGAs,"  When designing modern embedded computing systems, most software programmers
choose to use multicore processors, possibly in combination with
general-purpose graphics processing units (GPGPUs) and/or hardware
accelerators. They also often use an embedded Linux O/S and run
multi-application workloads that may even be multi-threaded. Modern FPGAs are
large enough to combine multicore hard/soft processors with multiple hardware
accelerators as custom compute units, enabling entire embedded compute systems
to be implemented on a single FPGA. Furthermore, the large FPGA vendors also
support embedded Linux kernels for both their soft and embedded processors.
When combined with high-level synthesis to generate hardware accelerators using
a C-to-gates flows, the necessary primitives for a framework that can enable
software designers to use FPGAs as their custom compute platform now exist.
However, in order to ensure that computing resources are integrated and shared
effectively, software developers need to be able to monitor and debug the
runtime performance of the applications in their workload. This paper describes
ABACUS, a performance-monitoring framework that can be used to debug the
execution behaviours and interactions of multi-application workloads on
multicore systems. We also discuss how this framework is extensible for use
with hardware accelerators in heterogeneous systems.
"
839,"Parameter Sensitivity Analysis of the Energy/Frequency Convexity Rule
  for Nanometer-scale Application Processors","  Both theoretical and experimental evidence are presented in this work in
order to validate the existence of an Energy/Frequency Convexity Rule, which
relates energy consumption and microprocessor frequency for nanometer-scale
microprocessors. Data gathered during several month-long experimental
acquisition campaigns, supported by several independent publications, suggest
that energy consumed is indeed depending on the microprocessor's clock
frequency, and, more interestingly, the curve exhibits a clear minimum over the
processor's frequency range. An analytical model for this behavior is presented
and motivated, which fits well with the experimental data. A parameter
sensitivity analysis shows how parameters affect the energy minimum in the
clock frequency space. The conditions are discussed under which this convexity
rule can be exploited, and when other methods are more effective, with the aim
of improving the computer system's energy management efficiency. We show that
the power requirements of the computer system, besides the microprocessor, and
the overhead affect the location of the energy minimum the most. The
sensitivity analysis of the Energy/Frequency Convexity Rule puts forward a
number of simple guidelines especially for by low-power systems, such as
battery-powered and embedded systems, and less likely by high-performance
computer systems.
"
840,Brewing Analytics Quality for Cloud Performance,"  Cloud computing has become increasingly popular. Many options of cloud
deployments are available. Testing cloud performance would enable us to choose
a cloud deployment based on the requirements. In this paper, we present an
innovative process, implemented in software, to allow us to assess the quality
of the cloud performance data. The process combines performance data from
multiple machines, spanning across user experience data, workload performance
metrics, and readily available system performance data. Furthermore, we discuss
the major challenges of bringing raw data into tidy data formats in order to
enable subsequent analysis, and describe how our process has several layers of
assessment to validate the quality of the data processing procedure. We present
a case study to demonstrate the effectiveness of our proposed process, and
conclude our paper with several future research directions worth investigating.
"
841,Time parallel gravitational collapse simulation,"  This article demonstrates the applicability of the parallel-in-time method
Parareal to the numerical solution of the Einstein gravity equations for the
spherical collapse of a massless scalar field. To account for the shrinking of
the spatial domain in time, a tailored load balancing scheme is proposed and
compared to load balancing based on number of time steps alone. The performance
of Parareal is studied for both the sub-critical and black hole case; our
experiments show that Parareal generates substantial speedup and, in the
super-critical regime, can reproduce Choptuik's black hole mass scaling law.
"
842,"On the energy efficiency of client-centric data consistency management
  under random read/write access to Big Data with Apache HBase","  The total estimated energy bill for data centers in 2010 was \$11.5 billion,
and experts estimate that the energy cost of a typical data center doubles
every five years. On the other hand, computational developments have started to
lag behind storage advancements, therein becoming a future bottleneck for the
ongoing data growth which already approaches Exascale levels. We investigate
the relationship among data throughput and energy footprint on a large storage
cluster, with the goal of formalizing it as a metric that reflects the trading
among consistency and energy. Employing a client-centric consistency approach,
and while honouring ACID properties of the chosen columnar store for the case
study (Apache HBase), we present the factors involved in the energy consumption
of the system as well as lessons learned to underpin further design of
energy-efficient cluster scale storage systems.
"
843,Exposing Provenance Metadata Using Different RDF Models,"  A standard model for exposing structured provenance metadata of scientific
assertions on the Semantic Web would increase interoperability,
discoverability, reliability, as well as reproducibility for scientific
discourse and evidence-based knowledge discovery. Several Resource Description
Framework (RDF) models have been proposed to track provenance. However,
provenance metadata may not only be verbose, but also significantly redundant.
Therefore, an appropriate RDF provenance model should be efficient for
publishing, querying, and reasoning over Linked Data. In the present work, we
have collected millions of pairwise relations between chemicals, genes, and
diseases from multiple data sources, and demonstrated the extent of redundancy
of provenance information in the life science domain. We also evaluated the
suitability of several RDF provenance models for this crowdsourced data set,
including the N-ary model, the Singleton Property model, and the
Nanopublication model. We examined query performance against three commonly
used large RDF stores, including Virtuoso, Stardog, and Blazegraph. Our
experiments demonstrate that query performance depends on both RDF store as
well as the RDF provenance model.
"
844,Federating OMNeT++ Simulations with Testbed Environments,"  We are in the process of developing a system architecture for opportunistic
and information centric communications. This architecture (called Keetchi),
meant for the Internet of Things (IoT) is focussed on enabling applications to
perform distributed and decentralised communications among smart devices. To
realise and evaluate this architecture, we follow a 3-step approach. Our first
approach of evaluation is the development of a testbed with smart devices
(mainly smart phones and tablets) deployed with this architecture including the
applications. The second step is where the architecture is evaluated in large
scale scenarios with the OMNeT++ simulation environment. The third step is
where the OMNeT++ simulation environment is fed with traces of data collected
from experiments done using the testbed. In realising these environments, we
develop the functionality of this architecture as a common code base that is
able to operate in the OMNeT++ environment as well as in the smart devices of
the testbed (e.g., Android, iOS, Contiki, etc.). This paper presents the
details of the ""Write once, compile anywhere"" (WOCA) code base architecture of
Keetchi.
"
845,uIP Support for the Network Simulation Cradle,"  This paper introduces the ongoing integration of Contiki's uIP stack into the
OMNeT++ port of the Network Simulation Cradle (NSC). The NSC utilizes code from
real world stack implementations and allows for an accurate simulation and
comparison of different TCP/IP stacks. uIP(v6) provides resource-constrained
devices with an RFC-compliant TCP/IP stack and promotes the use of IPv6 in the
vastly growing field of Internet of Things scenarios. This work-in-progress
report discusses our motivation to integrate uIP into the NSC, our chosen
approach and possible use cases for the simulation of uIP in OMNeT++.
"
846,Dynamic Index NAT as a Mobility Solution in OMNeT++,"  Mobility in wireless networks causes a major issue from the IP-addressing
perspective. When a Mobile Node (MN) moves to another subnet, it will probably
get assigned a new IP address. This causes a routing problem since the MN will
not be reachable with its previous IP address known to the other communication
party. Real time applications might suffer from connection drops, which is
recognized as inconvenience in the currently used service, unless some solution
is provided. An approach to maintain session continuity while traversing
heterogeneous networks of different subnet addresses is proposed. Here, a
cross-layer module is implemented in OMNeT++ with NAT functionality to provide
a seamless handover. A proof of concept is also shown with analogy to the
Mobile IPv6 protocol provided in INET.
"
847,Integration of RTMFP in the OMNeT++ Simulation Environment,"  This paper introduces the new Real-Time Media Flow Protocol (RTMFP)
simulation model for the INET framework for the OMNeT++ simulation environment.
RTMFP is a message orientated protocol with a focus on real time peer-to-peer
communication. After Adobe Inc. released the specifications, we were able to
implement the protocol in INET and compare its performance to the similar
Stream Control Transmission Protocol (SCTP) with a focus on congestion control
and flow control mechanisms.
"
848,Execution-Cache-Memory Performance Model: Introduction and Validation,"  This report serves two purposes: To introduce and validate the
Execution-Cache-Memory (ECM) performance model and to provide a thorough
analysis of current Intel processor architectures with a special emphasis on
Intel Xeon Haswell-EP. The ECM model is a simple analytical performance model
which focuses on basic architectural resources. The architectural analysis and
model predictions are showcased and validated using a set of elementary
microbenchmarks.
"
849,Integration of the Packetdrill Testing Tool in INET,"  Google released in 2013 a script-based tool called packetdrill, which allows
to test transport protocols like UDP and TCP on Linux and BSD-based operating
systems. The scripts defining a test-case allow to inject packets to the
implementation under test, perform operations at the API controlling the
transport protocol and verify the sending of packets, all at specified times.
This paper describes a port of packetdrill to the INET framework for the
OMNeT++ simulation environment providing a simple and powerful method of
testing the transport protocols implemented in INET.
"
850,"Realistic, Extensible DNS and mDNS Models for INET/OMNeT++","  The domain name system (DNS) is one of the core services in today's network
structures. In local and ad-hoc networks DNS is often enhanced or replaced by
mDNS. As of yet, no simulation models for DNS and mDNS have been developed for
INET/OMNeT++. We introduce DNS and mDNS simulation models for OMNeT++, which
allow researchers to easily prototype and evaluate extensions for these
protocols. In addition, we present models for our own experimental extensions,
namely Stateless DNS and Privacy-Enhanced mDNS, that are based on the
aforementioned models. Using our models we were able to further improve the
efficiency of our protocol extensions.
"
851,ptp++: A Precision Time Protocol Simulation Model for OMNeT++ / INET,"  Precise time synchronization is expected to play a key role in emerging
distributed and real-time applications such as the smart grid and Internet of
Things (IoT) based applications. The Precision Time Protocol (PTP) is currently
viewed as one of the main synchronization solutions over a packet-switched
network, which supports microsecond synchronization accuracy. In this paper, we
present a PTP simulation model for OMNeT++ INET, which allows to investigate
the synchronization accuracy under different network configurations and
conditions. To show some illustrative simulation results using the developed
module, we investigate on the network load fluctuations and their impacts on
the PTP performance by considering a network with class-based
quality-of-service (QoS) support. The simulation results show that the network
load significantly affects the network delay symmetry, and investigate a new
technique called class probing to improve the PTP accuracy and mitigate the
load fluctuation effects.
"
852,High Frequency Radio Network Simulation Using OMNeT++,"  Harris Corporation has an interest in making HF radios a suitable medium for
wireless information networks using standard Internet protocols. Although HF
radio links have many unique characteristics, HF wireless subnets can be
subject to many of the same traffic flow characteristics and topologies as
existing line-of-sight (LOS) radio networks, giving rise to similar issues
(media access, connectivity, routing) which lend themselves to investigation
through simulation. Accordingly, we have undertaken to develop efficient,
high-fidelity simulations of various aspects of HF radio communications and
networking using the OMNeT++ framework. Essential aspects of these simulations
include HF channel models simulating relevant channel attributes such as Signal
to Noise Ratio, multipath, and Doppler spread; a calibrated physical layer
model reproducing the error statistics (including burst error distributions) of
the MIL-STD-188-110B/C HF modem waveforms, both narrowband (3 kHz) and wideband
(up to 24 kHz) on the simulated HF channels; a model of the NATO STANAG 5066
data link protocol; and integration of these models with the OMNeT++ network
simulation framework and its INET library of Internet protocol models. This
simulation is used to evaluate the impacts of different STANAG 5066
configuration settings on TCP network performance, and to evaluate strategies
for optimizing throughput over HF links using TCP Performance Enhancing Proxy
(PEP) techniques.
"
853,"Proceedings of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015","  This is the Proceedings of the 2nd OMNeT++ Community Summit, which was held
at IBM Research - Zurich, Switzerland on September 3-4, 2015.
"
854,"MiXiM, PAWiS, and STEAM-Sim Integration - Combining Channel Models,
  Energy Awareness, and Real-life Application Code","  After a decade of research in the field of wireless sensor networks (WSNs)
there are still open issues. WSNs impose several severe requirements regarding
energy consumption, processing capabilities, mobility, and robustness of
wireless transmissions. Simulation has shown to be the most cost-efficient
approach for evaluation of WSNs, thus a number of simulators are available.
Unfortunately, these simulation environments typically consider WSNs from a
special point of view. In this work we present the integration of three such
specialized frameworks, namely MiXiM, PAWiS, and STEAM-Sim. This integration
combines the strengths of the single frameworks such as realistic channel
models, mobility patterns, accurate energy models, and inclusion of real-life
application code. The result is a new simulation environment which enables a
more general consideration of WSNs. We implemented and verified our proposed
concept by means of static and mobile scenarios. As the presented results show,
the combined framework gives the same results regarding the functionality and
energy consumption as our ""golden model"". Therefore the system integration was
successful and the framework is ready to be used by the community.
"
855,"Skip This Paper - RINASim: Your Recursive InterNetwork Architecture
  Simulator","  Recursive InterNetwork Architecture is a clean-slate approach to how to deal
with the current issues of the Internet based on the traditional TCP/IP
networking stack. Instead of using a fixed number of layers with dedicated
functionality, RINA proposes a single generic layer with programmable
functionality that may be recursively stacked. We introduce a brand new
framework for modeling and simulation of RINA that is intended for OMNeT++.
"
856,"Implementation of a Wake-up Radio Cross-Layer Protocol in OMNeT++ /
  MiXiM","  This paper presents the DoRa protocol, which is a new cross-layer protocol
for handling the double radio of nodes in wake-up radio scenario. The
implementation details in OMNET++/MiXiM are also given, with a focus on the
implemented MAC layers. The main goal of the DoRa protocol is to reduce energy
consumption in wireless sensor network, by taking full advantage of the passive
wake-up scheme. The performance of the DoRa protocol is then evaluated and
results are compared with B-MAC and IEEE 802.15.4 protocols.
"
857,Looking into Hardware-in-the-Loop Coupling of OMNeT++ and RoSeNet,"  Network emulation using real sensor node hardware is used to increase the
accuracy of pure network simulations. Coupling OMNeT++ with network emulation
platforms and tools introduces new application possibilities for both sides.
This work-in-progress report covers our experiences of using OMNeT++ as a test
driver for RoSeNet, a network emulation and test platform for low-power
wireless technologies like IEEE 802.15.4. OMNeT++ and RoSeNet were
interconnected to enable a co-simulation of real sensor networks with a MAC
layer simulation model. Experiences and insights on this Hardware-in-the-Loop
(HIL) simulation together with ideas to extend OMNeT++ and to provide a generic
interconnection API complete the report.
"
858,Implementation of PFC and RCM for RoCEv2 Simulation in OMNeT++,"  As traffic patterns and network topologies become more and more complicated
in current enterprise data centers and TOP500 supercomputers, the probability
of network congestion increases. If no countermeasures are taken, network
congestion causes long communication delays and degrades network performance. A
congestion control mechanism is often provided to reduce the consequences of
congestion. However, it is usually difficult to configure and activate a
congestion control mechanism in production clusters and supercomputers due to
concerns that it may negatively impact jobs if the mechanism is not
appropriately configured. Therefore, simulations for these situations are
necessary to identify congestion points and sources, and more importantly, to
determine optimal settings that can be utilized to reduce congestion in those
complicated networks. In this paper, we use OMNeT++ to implement the IEEE
802.1Qbb Priority-based Flow Control (PFC) and RoCEv2 Congestion Management
(RCM) in order to simulate clusters with RoCEv2 interconnects.
"
859,"A Tutorial of the Mobile Multimedia Wireless Sensor Network OMNeT++
  Framework","  In this work, we will give a detailed tutorial instruction about how to use
the Mobile Multi-Media Wireless Sensor Networks (M3WSN) simulation framework.
The M3WSN framework has been published as a scientific paper in the 6th
International Workshop on OMNeT++ (2013). M3WSN framework enables the
multimedia transmission of real video sequence. Therefore, a set of multimedia
algorithms, protocols, and services can be evaluated by using QoE metrics.
Moreover, key video-related information, such as frame types, GoP length and
intra-frame dependency can be used for creating new assessment and optimization
solutions. To support mobility, M3WSN utilizes different mobility traces to
enable the understanding of how the network behaves under mobile situations.
This tutorial will cover how to install and configure the M3WSN framework,
setting and running the experiments, creating mobility and video traces, and
how to evaluate the performance of different protocols. The tutorial will be
given in an environment of Ubuntu 12.04 LTS and OMNeT++ 4.2.
"
860,"Invited Abstract: A Simulation Package for Energy Consumption of Content
  Delivery Networks (CDNs)","  Content Delivery Networks (CDNs) are becoming an integral part of the future
generation Internet. Traditionally, these networks have been designed with the
goals of traffic offload and the improvement of users' quality of experience
(QoE), but the energy consumption is also becoming an indispensable design
factor for CDNs to be a sustainable solution. To study and improve the CDN
architectures using this new design metric, we are planning to develop a
generic and flexible simulation package in OMNet++. This package is aimed to
render a holistic view about the CDN energy consumption behaviour by
incorporating the state-of-the-art energy consumption models proposed for the
individual elements of CDNs (e.g. servers, routers, wired and wireless links,
wireless devices, etc.) and for the various Internet contents (web pages,
files, streaming video, etc.).
"
861,Automatic Loop Kernel Analysis and Performance Modeling With Kerncraft,"  Analytic performance models are essential for understanding the performance
characteristics of loop kernels, which consume a major part of CPU cycles in
computational science. Starting from a validated performance model one can
infer the relevant hardware bottlenecks and promising optimization
opportunities. Unfortunately, analytic performance modeling is often tedious
even for experienced developers since it requires in-depth knowledge about the
hardware and how it interacts with the software. We present the ""Kerncraft""
tool, which eases the construction of analytic performance models for streaming
kernels and stencil loop nests. Starting from the loop source code, the problem
size, and a description of the underlying hardware, Kerncraft can ideally
predict the single-core performance and scaling behavior of loops on multicore
processors using the Roofline or the Execution-Cache-Memory (ECM) model. We
describe the operating principles of Kerncraft with its capabilities and
limitations, and we show how it may be used to quickly gain insights by
accelerated analytic modeling.
"
862,"Joint Channel and Queue Aware Scheduling for Wireless Links with
  Multiple Fading States","  In this work, we address the delay optimal scheduling problem for wireless
transmission with fixed modulation over multi-state fading channels. We propose
a stochastic scheduling policy which schedules the source to transmit with
probability jointly based on the buffer and channel states, with an average
power constraint at the transmitter. Our objective is to minimize the average
queueing delay by choosing the optimal transmission probabilities. Using Markov
chain modeling, we formulate a power-constrained delay minimization problem,
and then transform it into a Linear Programming (LP) one. By analyzing its
property, we can derive the optimal threshold-based scheduling policy together
with the corresponding transmission probabilities. Our theoretical analysis is
corroborated by simulation results.
"
863,"Network Capability in Localizing Node Failures via End-to-end Path
  Measurements","  We investigate the capability of localizing node failures in communication
networks from binary states (normal/failed) of end-to-end paths. Given a set of
nodes of interest, uniquely localizing failures within this set requires that
different observable path states associate with different node failure events.
However, this condition is difficult to test on large networks due to the need
to enumerate all possible node failures. Our first contribution is a set of
sufficient/necessary conditions for identifying a bounded number of failures
within an arbitrary node set that can be tested in polynomial time. In addition
to network topology and locations of monitors, our conditions also incorporate
constraints imposed by the probing mechanism used. We consider three probing
mechanisms that differ according to whether measurement paths are (i)
arbitrarily controllable, (ii) controllable but cycle-free, or (iii)
uncontrollable (determined by the default routing protocol). Our second
contribution is to quantify the capability of failure localization through (1)
the maximum number of failures (anywhere in the network) such that failures
within a given node set can be uniquely localized, and (2) the largest node set
within which failures can be uniquely localized under a given bound on the
total number of failures. Both measures in (1-2) can be converted into
functions of a per-node property, which can be computed efficiently based on
the above sufficient/necessary conditions. We demonstrate how measures (1-2)
proposed for quantifying failure localization capability can be used to
evaluate the impact of various parameters, including topology, number of
monitors, and probing mechanisms.
"
864,"Latency Analysis of an Aerial Video Tracking System Using Fiacre and
  Tina","  We describe our experience with modeling a video tracking system used to
detect and follow moving targets from an airplane. We provide a formal model
that takes into account the real-time properties of the system and use it to
compute the worst and best-case end to end latency. We also compute a lower
bound on the delay between the loss of two frames. Our approach is based on the
model-checking tool Tina, that provides state-space generation and
model-checking algorithms for an extension of Time Petri Nets with data and
priorities. We propose several models divided in two main categories: first
Time Petri Net models, which are used to study the behavior of the system in
the most basic way; then models based on the Fiacre specification language,
where we take benefit of richer data structures to directly model the buffering
of video information and the use of an unbounded number of frame identifiers.
"
865,Throughput capacity of two-hop relay MANETs under finite buffers,"  Since the seminal work of Grossglauser and Tse [1], the two-hop relay
algorithm and its variants have been attractive for mobile ad hoc networks
(MANETs) due to their simplicity and efficiency. However, most literature
assumed an infinite buffer size for each node, which is obviously not
applicable to a realistic MANET. In this paper, we focus on the exact
throughput capacity study of two-hop relay MANETs under the practical finite
relay buffer scenario. The arrival process and departure process of the relay
queue are fully characterized, and an ergodic Markov chain-based framework is
also provided. With this framework, we obtain the limiting distribution of the
relay queue and derive the throughput capacity under any relay buffer size.
Extensive simulation results are provided to validate our theoretical framework
and explore the relationship among the throughput capacity, the relay buffer
size and the number of nodes.
"
866,On throughput capacity for a class of buffer-limited MANETs,"  Available throughput performance studies for mobile ad hoc networks (MANETs)
suffer from two major limitations: they mainly focus on the scaling law study
of throughput, while the exact throughput of such networks remains largely
unknown; they usually consider the infinite buffer scenarios, which are not
applicable to the practical networks with limited buffer. As a step to address
these limitations, this paper develops a general framework for the exact
throughput capacity study of a class of buffer-limited MANETs with the two-hop
relay. We first provide analysis to reveal how the throughput capacity of such
a MANET is determined by its relay-buffer blocking probability (RBP). Based on
the Embedded Markov Chain Theory and Queuing Theory, a novel theoretical
framework is then developed to enable the RBP and closed-form expression for
exact throughput capacity to be derived. We further conduct case studies under
two typical transmission scheduling schemes to illustrate the applicability of
our framework and to explore the corresponding capacity optimization as well as
capacity scaling law. Finally, extensive simulation and numerical results are
provided to validate the efficiency of our framework and to show the impacts
brought by the buffer constraint.
"
867,End-to-end delay in two hop relay MANETs with limited buffer,"  Despite lots of literature has been dedicated to researching the delay
performance in two-hop relay (2HR) mobile ad hoc networks (MANETs), however,
they usually assume the buffer size of each node is infinite, so these studies
are not applicable to and thus may not reflect the real delay performance of a
practical MANET with limited buffer. To address this issue, in this paper we
explore the packet end-to-end delay in a 2HR MANET, where each node is equipped
with a bounded and shared relay-buffer for storing and forwarding packets of
all other flows. The transmission range of each node can be adjusted and a
group-based scheduling scheme is adopted to avoid interference between
simultaneous transmissions, meanwhile a handshake mechanism is added to the 2HR
routing algorithm to avoid packet loss. With the help of Markov Chain Theory
and Queuing Theory, we develop a new framework to fully characterize the packet
delivery processes, and obtain the relay-buffer blocking probability (RBP)
under any given exogenous packet input rate. Based on the RBP, we can compute
the packet queuing delay in its source node and delivery delay respectively,
and further derive the end-to-end delay in such a MANET with limited buffer.
"
868,"End-to-end delay modeling in buffer-limited MANETs: a general
  theoretical framework","  This paper focuses on a class of important two-hop relay mobile ad hoc
networks (MANETs) with limited-buffer constraint and any mobility model that
leads to the uniform distribution of the locations of nodes in steady state,
and develops a general theoretical framework for the end-to-end (E2E) delay
modeling there. We first combine the theories of Fixed-Point,
Quasi-Birth-and-Death process and embedded Markov chain to model the limiting
distribution of the occupancy states of a relay buffer, and then apply the
absorbing Markov chain theory to characterize the packet delivery process, such
that a complete theoretical framework is developed for the E2E delay analysis.
With the help of this framework, we derive a general and exact expression for
the E2E delay based on the modeling of both packet queuing delay and delivery
delay. To demonstrate the application of our framework, case studies are
further provided under two network scenarios with different MAC protocols to
show how the E2E delay can be analytically determined for a given network
scenario. Finally, we present extensive simulation and numerical results to
illustrate the efficiency of our delay analysis as well as the impacts of
network parameters on delay performance.
"
869,"Delay Optimal Power Aware Opportunistic Scheduling with Mutual
  Information Accumulation","  This paper considers optimization of power and delay in a time-varying
wireless link using rateless codes. The link serves a sequence of
variable-length packets. Each packet is coded and transmitted over multiple
slots. Channel conditions can change from slot to slot and are unknown to the
transmitter. The amount of mutual information accumulated on each slot depends
on the random channel realization and the power used. The goal is to minimize
average service delay subject to an average power constraint. We formulate this
problem as a frame-based stochastic optimization problem and solve it via an
online algorithm. We show that the subproblem within each frame is a simple
integer program which can be effectively solved using a dynamic program. The
optimality of this online algorithm is proved using the frame-based Lyapunov
drift analysis.
"
870,"Proceedings Thirteenth Workshop on Quantitative Aspects of Programming
  Languages and Systems","  This volume contains the proceedings of the Thirteenth Workshop on
Quantitative Aspects of Programming Languages and Systems (QAPL 2015), held in
London, UK, on 11 and 12 April, 2015. QAPL 2015 was a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS) focussing
on quantitative aspects of computation. The Program Committee of QAPL 2015
selected 8 regular papers and 2 presentation-only papers. The workshop
programme included two QAPL keynote presentations by Catuscia Palamidessi
(Inria/LIX, France) on ""Quantitative Aspects of Privacy and Information Flow,""
and Holger Hermanns (Saarland University, Germany) on ""Optimal Continuous Time
Markov Decisions.""
"
871,CARMA: Collective Adaptive Resource-sharing Markovian Agents,"  In this paper we present CARMA, a language recently defined to support
specification and analysis of collective adaptive systems. CARMA is a
stochastic process algebra equipped with linguistic constructs specifically
developed for modelling and programming systems that can operate in open-ended
and unpredictable environments. This class of systems is typically composed of
a huge number of interacting agents that dynamically adjust and combine their
behaviour to achieve specific goals. A CARMA model, termed a collective,
consists of a set of components, each of which exhibits a set of attributes. To
model dynamic aggregations, which are sometimes referred to as ensembles, CARMA
provides communication primitives that are based on predicates over the
exhibited attributes. These predicates are used to select the participants in a
communication. Two communication mechanisms are provided in the CARMA language:
multicast-based and unicast-based. In this paper, we first introduce the basic
principles of CARMA and then we show how our language can be used to support
specification with a simple but illustrative example of a socio-technical
collective adaptive system.
"
872,"Efficient Checking of Individual Rewards Properties in Markov Population
  Models","  In recent years fluid approaches to the analysis of Markov populations models
have been demonstrated to have great pragmatic value. Initially developed to
estimate the behaviour of the system in terms of the expected values of
population counts, the fluid approach has subsequently been extended to more
sophisticated interrogations of models through its embedding within model
checking procedures. In this paper we extend recent work on checking CSL
properties of individual agents within a Markovian population model, to
consider the checking of properties which incorporate rewards.
"
873,iotools: High-Performance I/O Tools for R,"  The iotools package provides a set of tools for Input/Output (I/O) intensive
datasets processing in R (R Core Team, 2014). Efficent parsing methods are
included which minimize copying and avoid the use of intermediate string
representations whenever possible. Functions for applying chunk-wise operations
allow for computing on streaming input as well as arbitrarily large files. We
present a set of example use cases for iotools, as well as extensive benchmarks
comparing comparable functions provided in both core-R as well as other
contributed packages.
"
874,"On the Rate of Convergence of Mean-Field Models: Stein's Method Meets
  the Perturbation Theory","  This paper studies the rate of convergence of a family of continuous-time
Markov chains (CTMC) to a mean-field model. When the mean-field model is a
finite-dimensional dynamical system with a unique equilibrium point, an
analysis based on Stein's method and the perturbation theory shows that under
some mild conditions, the stationary distributions of CTMCs converge (in the
mean-square sense) to the equilibrium point of the mean-field model if the
mean-field model is globally asymptotically stable and locally exponentially
stable. In particular, the mean square difference between the $M$th CTMC in the
steady state and the equilibrium point of the mean-field system is $O(1/M),$
where $M$ is the size of the $M$th CTMC. This approach based on Stein's method
provides a new framework for studying the convergence of CTMCs to their
mean-field limit by mainly looking into the stability of the mean-field model,
which is a deterministic system and is often easier to analyze than the CTMCs.
More importantly, this approach quantifies the rate of convergence, which
reveals the approximation error of using mean-field models for approximating
finite-size systems.
"
875,On the Maximal Shortest Path in a Connected Component in V2V,"  In this work, a VANET (Vehicular Ad-hoc NETwork) is considered to operate on
a simple lane, without infrastructure. The arrivals of vehicles are assumed to
be general with any traffic and speed assumptions. The vehicles communicate
through the shortest path. In this paper, we study the probability distribution
of the number of hops on the maximal shortest path in a connected component of
vehicles. The general formulation is given for any assumption of road traffic.
Then, it is applied to calculate the z-transform of this distribution for
medium and dense networks in the Poisson case. Our model is validated with the
Madrid road traces of the Universitat Polit\`ecnica de Catalunya. These results
may be useful for example when evaluating diffusion protocols through the
shortest path in a VANET, where not only the mean but also the other moments
are needed to derive accurate results.
"
876,"Performance Analysis of a Heterogeneous Traffic Scheduler using Large
  Deviation Principle","  In this paper, we study the stability of light traffic achieved by a
scheduling algorithm which is suitable for heterogeneous traffic networks.
Since analyzing a scheduling algorithm is intractable using the conventional
mathematical tool, our goal is to minimize the largest queue-overflow
probability achieved by the algorithm. In the large deviation setting, this
problem is equivalent to maximizing the asymptotic decay rate of the largest
queue-overflow probability. We first derive an upper bound on the decay rate of
the queue overflow probability as the queue overflow threshold approaches
infinity. Then, we study several structural properties of the minimum-cost-path
to overflow of the queue with the largest length, which is basically equivalent
to the decay rate of the largest queue-overflow probability. Given these
properties, we prove that the queue with the largest length follows a sample
path with linear increment. For certain parameter value, the scheduling
algorithm is asymptotically optimal in reducing the largest queue length.
Through numerical results, we have shown the large deviation properties of the
queue length typically used in practice while varying one parameter of the
algorithm.
"
877,"Efficient Replication of Queued Tasks for Latency Reduction in Cloud
  Systems","  In cloud computing systems, assigning a job to multiple servers and waiting
for the earliest copy to finish is an effective method to combat the
variability in response time of individual servers. Although adding redundant
replicas always reduces service time, the total computing time spent per job
may be higher, thus increasing waiting time in queue. The total time spent per
job is also proportional to the cost of computing resources. We analyze how
different redundancy strategies, for eg. number of replicas, and the time when
they are issued and canceled, affect the latency and computing cost. We get the
insight that the log-concavity of the service time distribution is a key factor
in determining whether adding redundancy reduces latency and cost. If the
service distribution is log-convex, then adding maximum redundancy reduces both
latency and cost. And if it is log-concave, then having fewer replicas and
canceling the redundant requests early is more effective.
"
878,"Multi-dimensional intra-tile parallelization for memory-starved stencil
  computations","  Optimizing the performance of stencil algorithms has been the subject of
intense research over the last two decades. Since many stencil schemes have low
arithmetic intensity, most optimizations focus on increasing the temporal data
access locality, thus reducing the data traffic through the main memory
interface with the ultimate goal of decoupling from this bottleneck. There are,
however, only few approaches that explicitly leverage the shared cache feature
of modern multicore chips. If every thread works on its private, separate cache
block, the available cache space can become too small, and sufficient temporal
locality may not be achieved.
  We propose a flexible multi-dimensional intra-tile parallelization method for
stencil algorithms on multicore CPUs with a shared outer-level cache. This
method leads to a significant reduction in the required cache space without
adverse effects from hardware prefetching or TLB shortage. Our \emph{Girih}
framework includes an auto-tuner to select optimal parameter configurations on
the target hardware. We conduct performance experiments on two contemporary
Intel processors and compare with the state-of-the-art stencil frameworks PLUTO
and Pochoir, using four corner-case stencil schemes and a wide range of problem
sizes. \emph{Girih} shows substantial performance advantages and best
arithmetic intensity at almost all problem sizes, especially on low-intensity
stencils with variable coefficients. We study in detail the performance
behavior at varying grid size using phenomenological performance modeling. Our
analysis of energy consumption reveals that our method can save energy by
reduced DRAM bandwidth usage even at marginal performance gain. It is thus well
suited for future architectures that will be strongly challenged by the cost of
data movement, be it in terms of performance or energy consumption.
"
879,"Optimization of an electromagnetics code with multicore wavefront
  diamond blocking and multi-dimensional intra-tile parallelization","  Understanding and optimizing the properties of solar cells is becoming a key
issue in the search for alternatives to nuclear and fossil energy sources. A
theoretical analysis via numerical simulations involves solving Maxwell's
Equations in discretized form and typically requires substantial computing
effort. We start from a hybrid-parallel (MPI+OpenMP) production code that
implements the Time Harmonic Inverse Iteration Method (THIIM) with
Finite-Difference Frequency Domain (FDFD) discretization. Although this
algorithm has the characteristics of a strongly bandwidth-bound stencil update
scheme, it is significantly different from the popular stencil types that have
been exhaustively studied in the high performance computing literature to date.
We apply a recently developed stencil optimization technique, multicore
wavefront diamond tiling with multi-dimensional cache block sharing, and
describe in detail the peculiarities that need to be considered due to the
special stencil structure. Concurrency in updating the components of the
electric and magnetic fields provides an additional level of parallelism. The
dependence of the cache size requirement of the optimized code on the blocking
parameters is modeled accurately, and an auto-tuner searches for optimal
configurations in the remaining parameter space. We were able to completely
decouple the execution from the memory bandwidth bottleneck, accelerating the
implementation by a factor of three to four compared to an optimal
implementation with pure spatial blocking on an 18-core Intel Haswell CPU.
"
880,A Novel Offloading Partitioning Algorithm in Mobile Cloud Computing,"  This paper has been withdrawn by the author
"
881,"Analysis of the Energy-Performance Tradeoff for Delayed Mobile
  Offloading","  This paper has been withdrawn by the author
"
882,"There is no fast lunch: an examination of the running speed of
  evolutionary algorithms in several languages","  It is quite usual when an evolutionary algorithm tool or library uses a
language other than C, C++, Java or Matlab that a reviewer or the audience
questions its usefulness based on the speed of those other languages,
purportedly slower than the aforementioned ones. Despite speed being not
everything needed to design a useful evolutionary algorithm application, in
this paper we will measure the speed for several very basic evolutionary
algorithm operations in several languages which use different virtual machines
and approaches, and prove that, in fact, there is no big difference in speed
between interpreted and compiled languages, and that in some cases, interpreted
languages such as JavaScript or Python can be faster than compiled languages
such as Scala, making them worthy of use for evolutionary algorithm
experimentation.
"
883,"Power Consumption of Virtualization Technologies: an Empirical
  Investigation","  Virtualization is growing rapidly as a result of the increasing number of
alternative solutions in this area, and of the wide range of application field.
Until now, hypervisor-based virtualization has been the de facto solution to
perform server virtualization. Recently, container-based virtualization - an
alternative to hypervisors - has gained more attention because of lightweight
characteristics, attracting cloud providers that have already made use of it to
deliver their services. However, a gap in the existing research on containers
exists in the area of power consumption. This paper presents the results of a
performance comparison in terms of power consumption of four different
virtualization technologies: KVM and Xen, which are based on hypervisor
virtualization, Docker and LXC which are based on container virtualization. The
aim of this empirical investigation, carried out by means of a testbed, is to
understand how these technologies react to particular workloads. Our initial
results show how, despite of the number of virtual entities running, both kinds
of virtualization alternatives behave similarly in idle state and in CPU/Memory
stress test. Contrarily, the results on network performance show differences
between the two technologies.
"
884,"Queueing Analysis of Unicast IPTV With User Mobility and Adaptive
  Modulation and Coding in Wireless Cellular Networks","  Unicast IPTV services that can support live TV, video-on-demand (VoD), video
conferencing, and online gaming applications over broadband wireless cellular
networks have been becoming popular in recent years. However, video streaming
services significantly impact the performance of wireless cellular networks
because they are bandwidth hogs. To maintain the system performance, effective
admission control and resource allocation mechanisms based on an accurate
mathematical analysis are required. On the other hand, the quality of a
wireless link usually changes with time due to the user mobility or
time-varying channel characteristics. To counteract such time-varying channels
and improve the spectral efficiency, adaptive modulation and coding (AMC)
scheme can be adopted in offering unicast IPTV services for mobile users. In
this paper, closed-form solutions for the bandwidth usage, blocking rate, and
dropping rate of unicast IPTV services over wireless cellular networks were
derived based on the novel queueing model that considers both user mobility and
AMC. Simulations were also conducted to validate the accuracy of analytical
results. Numerical results demonstrate that the presented analytical results
are accurate. Based on the accurate closed-form solutions, network providers
can implement precise admission control and resource allocation for their
networks to enhance the system performance.
"
885,"A lightweight optimization selection method for Sparse Matrix-Vector
  Multiplication","  In this paper, we propose an optimization selection methodology for the
ubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two
models that attempt to identify the major performance bottleneck of the kernel
for every instance of the problem and then select an appropriate optimization
to tackle it. Our first model requires online profiling of the input matrix in
order to detect its most prevailing performance issue, while our second model
only uses comprehensive structural features of the sparse matrix. Our method
delivers high performance stability for SpMV across different platforms and
sparse matrices, due to its application and architecture awareness. Our
experimental results demonstrate that a) our approach is able to distinguish
and appropriately optimize special matrices in multicore platforms that fall
out of the standard class of memory bandwidth bound matrices, and b) lead to a
significant performance gain of 29% in a manycore platform compared to an
architecture-centric optimization, as a result of the successful selection of
the appropriate optimization for the great majority of the matrices. With a
runtime overhead equivalent to a couple dozen SpMV operations, our approach is
practical for use in iterative numerical solvers of real-life applications.
"
886,"ELDA: Towards Efficient and Lightweight Detection of Cache Pollution
  Attacks in NDN","  As a promising architectural design for future Internet, named data
networking (NDN) relies on in-network caching to efficiently deliver name-based
content. However, the in-network caching is vulnerable to cache pollution
attacks (CPA), which can reduce cache hits by violating cache locality and
significantly degrade the overall performance of NDN. To defend against CPA
attacks, the most effective way is to first detect the attacks and then
throttle them. Since the CPA attack itself has already imposed a huge burden on
victims, to avoid exhausting the remaining resources on the victims for
detection purpose, we expect a lightweight detection solution. We thus propose
ELDA, an Efficient and Lightweight Detection scheme against cache pollution
Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to
monitor the interest traffic. Our analysis and simulations demonstrate that, by
consuming a few computation and memory resources, ELDA can effectively and
efficiently detect CPA attacks.
"
887,"GEMMbench: a framework for reproducible and collaborative benchmarking
  of matrix multiplication","  The generic matrix-matrix multiplication (GEMM) is arguably the most popular
computational kernel of the 20th century. Yet, surprisingly, no common
methodology for evaluating GEMM performance has been established over the many
decades of using GEMM for comparing architectures, compilers and ninja-class
programmers.
  We introduce GEMMbench, a framework and methodology for evaluating
performance of GEMM implementations. GEMMbench is implemented on top of
Collective Knowledge (CK), a lightweight framework for reproducible and
collaborative R&D in computer systems. Using CK allows the R&D community to
crowdsource hand-written and compiler-generated GEMM implementations and to
study their performance across multiple platforms, data sizes and data types.
  Our initial implementation supports hand-written OpenCL kernels operating on
matrices consisting of single- and double-precision floating-point values, and
producing single or multiple output elements per work-item (via thread
coarsening and vectorization).
"
888,Model-Driven Automatic Tiling with Cache Associativity Lattices,"  Traditional compiler optimization theory distinguishes three separate classes
of cache miss -- Cold, Conflict and Capacity. Tiling for cache is typically
guided by capacity miss counts. Models of cache function have not been
effectively used to guide cache tiling optimizations due to model error and
expense. Instead, heuristic or empirical approaches are used to select tilings.
We argue that conflict misses, traditionally neglected or seen as a small
constant effect, are the only fundamentally important cache miss category, that
they form a solid basis by which caches can become modellable, and that models
leaning on cache associatvity analysis can be used to generate cache performant
tilings. We develop a mathematical framework that expresses potential and
actual cache misses in associative caches using Associativity Lattices. We show
these lattices to possess two theoretical advantages over rectangular tiles --
volume maximization and miss regularity. We also show that to generate such
lattice tiles requires, unlike rectangular tiling, no explicit, expensive
lattice point counting. We also describe an implementation of our lattice
tiling approach, show that it can be used to give speedups of over 10x versus
unoptimized code, and despite currently only tiling for one level of cache, can
already be competitive with the aggressive compiler optimizations used in
general purposes compares such as GCC and Intel's ICC. We also show that the
tiling approach can lead to reasonable automatic parallelism when compared to
existing auto-threading compilers.
"
889,Toward Transparent Heterogeneous Systems,"  Heterogeneous parallel systems are widely spread nowadays. Despite their
availability, their usage and adoption are still limited, and even more rarely
they are used to full power. Indeed, compelling new technologies are constantly
developed and keep changing the technological landscape, but each of them
targets a limited sub-set of supported devices, and nearly all of them require
new programming paradigms and specific toolsets. Software, however, can hardly
keep the pace with the growing number of computational capabilities, and
developers are less and less motivated in learning skills that could quickly
become obsolete. In this paper we present our effort in the direction of a
transparent system optimization based on automatic code profiling and
Just-In-Time compilation, that resulted in a fully-working embedded prototype
capable of dynamically detect computing-intensive code blocks and automatically
dispatch them to different computation units. Experimental results show that
our system allows gains up to 32x in performance --- after an initial warm-up
phase --- without requiring any human intervention.
"
890,"Analysis and Optimization of Sparse Random Linear Network Coding for
  Reliable Multicast Services","  Point-to-multipoint communications are expected to play a pivotal role in
next-generation networks. This paper refers to a cellular system transmitting
layered multicast services to a multicast group of users. Reliability of
communications is ensured via different Random Linear Network Coding (RLNC)
techniques. We deal with a fundamental problem: the computational complexity of
the RLNC decoder. The higher the number of decoding operations is, the more the
user's computational overhead grows and, consequently, the faster the battery
of mobile devices drains. By referring to several sparse RLNC techniques, and
without any assumption on the implementation of the RLNC decoder in use, we
provide an efficient way to characterize the performance of users targeted by
ultra-reliable layered multicast services. The proposed modeling allows to
efficiently derive the average number of coded packet transmissions needed to
recover one or more service layers. We design a convex resource allocation
framework that allows to minimize the complexity of the RLNC decoder by jointly
optimizing the transmission parameters and the sparsity of the code. The
designed optimization framework also ensures service guarantees to
predetermined fractions of users. The performance of the proposed optimization
framework is then investigated in a LTE-A eMBMS network multicasting H.264/SVC
video services.
"
891,Cache Miss Estimation for Non-Stationary Request Processes,"  The aim of the paper is to evaluate the miss probability of a Least Recently
Used (LRU) cache, when it is offered a non-stationary request process given by
a Poisson cluster point process. First, we construct a probability space using
Palm theory, describing how to consider a tagged document with respect to the
rest of the request process. This framework allows us to derive a general
integral formula for the expected number of misses of the tagged document.
Then, we consider the limit when the cache size and the arrival rate go to
infinity proportionally, and use the integral formula to derive an asymptotic
expansion of the miss probability in powers of the inverse of the cache size.
This enables us to quantify and improve the accuracy of the so-called Che
approximation.
"
892,"Minimizing Total Busy Time for Energy-Aware Virtual Machine Allocation
  Problems","  This paper investigates the energy-aware virtual machine (VM) allocation
problems in clouds along characteristics: multiple resources, fixed interval
time and non-preemption of virtual machines. Many previous works have been
proposed to use a minimum number of physical machines, however, this is not
necessarily a good solution to minimize total energy consumption in the VM
placement with multiple resources, fixed interval time and non-preemption. We
observed that minimizing the sum of total busy time of all physical machines
implies minimizing total energy consumption of physical machines. In addition
to, if mapping of a VM onto physical machines have the same total busy time
then the best mapping has physical machine's remaining available resource
minimizing. Based on these observations, we proposed heuristic-based EM
algorithm to solve the energy-aware VM allocation with fixed starting time and
duration time. In addition, this work studies some heuristics for sorting the
list of virtual machines (e.g., sorting by the earliest starting time, or
latest finishing time, or the longest duration time first, etc.) to allocate
VM. We evaluate the EM using CloudSim toolkit and jobs log-traces in the
Feitelson's Parallel Workloads Archive. Simulation's results show that all of
EM-ST, EM-LFT and EM-LDTF algorithms could reduce total energy consumption
compared to state-of-the-art of power-aware VM allocation algorithms. (e.g.
Power-Aware Best-Fit Decreasing (PABFD) [7])).
"
893,"Efficient Resource Sharing Through GPU Virtualization on Accelerated
  High Performance Computing Systems","  The High Performance Computing (HPC) field is witnessing a widespread
adoption of Graphics Processing Units (GPUs) as co-processors for conventional
homogeneous clusters. The adoption of prevalent Single- Program Multiple-Data
(SPMD) programming paradigm for GPU-based parallel processing brings in the
challenge of resource underutilization, with the asymmetrical
processor/co-processor distribution. In other words, under SPMD, balanced
CPU/GPU distribution is required to ensure full resource utilization. In this
paper, we propose a GPU resource virtualization approach to allow underutilized
microprocessors to effi- ciently share the GPUs. We propose an efficient GPU
sharing scenario achieved through GPU virtualization and analyze the
performance potentials through execution models. We further present the
implementation details of the virtualization infrastructure, followed by the
experimental analyses. The results demonstrate considerable performance gains
with GPU virtualization. Furthermore, the proposed solution enables full
utilization of asymmetrical resources, through efficient GPU sharing among
microprocessors, while incurring low overhead due to the added virtualization
layer.
"
894,Deseeding Energy Consumption of Network Stacks,"  Regular works on energy efficiency strategies for wireless communications are
based on classical energy models that account for the wireless card only.
Nevertheless, there is a non-negligible energy toll called ""cross-factor"" that
encompasses the energy drained while a frame crosses the network stack of an
OS.
  This paper addresses the challenge of deepen into the roots of the
cross-factor, deseed its components and analyse its causes. Energy issues are
critical for IoT devices. Thus, this paper conceives and validates a new
comprehensive framework that enables us to measure a wide range of wireless
devices, as well as multiple devices synchronously. We also present a rigorous
methodology to perform whole-device energy measurements in laptops, a more
generic and suitable device to perform energy debugging. Finally, and using
this framework, we provide a collection of measurements and insights that
deepens our understanding of the cross-factor.
"
895,"Cross-layer Chase Combining with Selective Retransmission, Analysis and
  Throughput Optimization for OFDM Systems","  In this paper, we present bandwidth efficient retransmission method employong
selective retransmission approach at modulation layer under orthogonal
frequency division multiplexing (OFDM) signaling. Our proposed cross-layer
design embeds a selective retransmission sublayer in physical layer (PHY) that
targets retransmission of information symbols transmitted over poor quality
OFDM sub-carriers. Most of the times, few errors in decoded bit stream result
in packet failure at medium access control (MAC) layer. The unnecessary
retransmission of good quality information symbols of a failed packet has
detrimental effect on overall throughput of transceiver. We propose a
cross-layer Chase combining with selective retransmission (CCSR) method by
blending Chase combining at MAC layer and selective retransmission in PHY. The
selective retransmission in PHY targets the poor quality information symbols
prior to decoding, which results into lower hybrid automatic repeat reQuest
(HARQ) retransmissions at MAC layer. We also present tight bit-error rate (BER)
upper bound and tight throughput lower bound for CCSR method. In order to
maximize throughput of the proposed method, we formulate optimization problem
with respect to the amount of information to be retransmitted in selective
retransmission. The simulation results demonstrate significant throughput gain
of the proposed CCSR method as compared to conventional Chase combining method.
"
896,HPA: An Opportunistic Approach to Embedded Energy Efficiency,"  Reducing energy consumption is a challenge that is faced on a daily basis by
teams from the High-Performance Computing as well as the Embedded domain. This
issue is mostly attacked from an hardware perspective, by devising
architectures that put energy efficiency as a primary target, often at the cost
of processing power. Lately, computing platforms have become more and more
heterogeneous, but the exploitation of these additional capabilities is so
complex from the application developer's perspective that they are left unused
most of the time, resulting therefore in a supplemental waste of energy rather
than in faster processing times.
  In this paper we present a transparent, on-the-fly optimization scheme that
allows a generic application to automatically exploit the available computing
units to partition its computational load. We have called our approach
Heterogeneous Platform Accelerator (HPA). The idea is to use profiling to
automatically select a computing-intensive candidate for acceleration, and then
distribute the computations to the different units by off-loading blocks of
code to them.
  Using an NVIDIA Jetson TK1 board, we demonstrate that not only HPA results in
faster processing speed, but also in a considerable reduction in the total
energy absorbed.
"
897,Technical Privacy Metrics: a Systematic Survey,"  The goal of privacy metrics is to measure the degree of privacy enjoyed by
users in a system and the amount of protection offered by privacy-enhancing
technologies. In this way, privacy metrics contribute to improving user privacy
in the digital world. The diversity and complexity of privacy metrics in the
literature makes an informed choice of metrics challenging. As a result,
instead of using existing metrics, new metrics are proposed frequently, and
privacy studies are often incomparable. In this survey we alleviate these
problems by structuring the landscape of privacy metrics. To this end, we
explain and discuss a selection of over eighty privacy metrics and introduce
categorizations based on the aspect of privacy they measure, their required
inputs, and the type of data that needs protection. In addition, we present a
method on how to choose privacy metrics based on nine questions that help
identify the right privacy metrics for a given scenario, and highlight topics
where additional work on privacy metrics is needed. Our survey spans multiple
privacy domains and can be understood as a general framework for privacy
measurement.
"
898,"Data Center Server Provision: Distributed Asynchronous Control for
  Coupled Renewal Systems","  This paper considers a cost minimization problem for data centers with N
servers and randomly arriving service requests. A central router decides which
server to use for each new request. Each server has three types of states
(active, idle, setup) with different costs and time durations. The servers
operate asynchronously over their own states and can choose one of multiple
sleep modes when idle. We develop an online distributed control algorithm so
that each server makes its own decisions, the request queues are bounded and
the overall time average cost is near optimal with probability 1. The algorithm
does not need probability information for the arrival rate or job sizes. Next,
an improved algorithm that uses a single queue is developed via a
""virtualization"" technique which is shown to provide the same (near optimal)
costs. Simulation experiments on a real data center traffic trace demonstrate
the efficiency of our algorithm compared to other existing algorithms.
"
899,"On the joint impact of bias and power control on downlink spectral
  efficiency in cellular networks","  Cell biasing and downlink transmit power are two controls that may be used to
improve the spectral efficiency of cellular networks. With cell biasing, each
mobile user associates with the base station offering, say, the highest biased
signal to interference plus noise ratio. Biasing affects the cell association
decisions of mobile users, but not the received instantaneous downlink
transmission rates. Adjusting the collection of downlink transmission powers
can likewise affect the cell associations, but in contrast with biasing, it
also directly affects the instantaneous rates. This paper investigates the
joint use of both cell biasing and transmission power control and their
(individual and joint) effects on the statistical properties of the collection
of per-user spectral efficiencies. Our analytical results and numerical
investigations demonstrate in some cases a significant performance improvement
in the Pareto efficient frontiers of both a mean-variance and
throughput-fairness tradeoff from using both bias and power controls over using
either control alone.
"
900,Speeding Up Distributed Machine Learning Using Codes,"  Codes are widely used in many engineering applications to offer robustness
against noise. In large-scale systems there are several types of noise that can
affect the performance of distributed machine learning algorithms -- straggler
nodes, system failures, or communication bottlenecks -- but there has been
little interaction cutting across codes, machine learning, and distributed
systems. In this work, we provide theoretical insights on how coded solutions
can achieve significant gains compared to uncoded ones. We focus on two of the
most basic building blocks of distributed learning algorithms: matrix
multiplication and data shuffling. For matrix multiplication, we use codes to
alleviate the effect of stragglers, and show that if the number of homogeneous
workers is $n$, and the runtime of each subtask has an exponential tail, coded
computation can speed up distributed matrix multiplication by a factor of $\log
n$. For data shuffling, we use codes to reduce communication bottlenecks,
exploiting the excess in storage. We show that when a constant fraction
$\alpha$ of the data matrix can be cached at each worker, and $n$ is the number
of workers, \emph{coded shuffling} reduces the communication cost by a factor
of $(\alpha + \frac{1}{n})\gamma(n)$ compared to uncoded shuffling, where
$\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$ users to
multicasting a common message (of the same size) to $n$ users. For instance,
$\gamma(n) \simeq n$ if multicasting a message to $n$ users is as cheap as
unicasting a message to one user. We also provide experiment results,
corroborating our theoretical gains of the coded algorithms.
"
901,Get More With Less: Near Real-Time Image Clustering on Mobile Phones,"  Machine learning algorithms, in conjunction with user data, hold the promise
of revolutionizing the way we interact with our phones, and indeed their
widespread adoption in the design of apps bear testimony to this promise.
However, currently, the computationally expensive segments of the learning
pipeline, such as feature extraction and model training, are offloaded to the
cloud, resulting in an over-reliance on the network and under-utilization of
computing resources available on mobile platforms. In this paper, we show that
by combining the computing power distributed over a number of phones, judicious
optimization choices, and contextual information it is possible to execute the
end-to-end pipeline entirely on the phones at the edge of the network,
efficiently. We also show that by harnessing the power of this combination, it
is possible to execute a computationally expensive pipeline at near real-time.
  To demonstrate our approach, we implement an end-to-end image-processing
pipeline -- that includes feature extraction, vocabulary learning,
vectorization, and image clustering -- on a set of mobile phones. Our results
show a 75% improvement over the standard, full pipeline implementation running
on the phones without modification -- reducing the time to one minute under
certain conditions. We believe that this result is a promising indication that
fully distributed, infrastructure-less computing is possible on networks of
mobile phones; enabling a new class of mobile applications that are less
reliant on the cloud.
"
902,"A Design of Endurance Queue for Co-Existing Systems in Multi-Programmed
  Environments","  These days enterprise applications try to integrate online processing and
batch jobs into a common software stack for seamless monitoring and driverless
operations. Continuous integration of these systems results in choking of the
poorly performing sub-systems, when the service demand and throughput are not
synchronized. A poorly performing sub-system may become a serious performance
bottleneck for the entire system if its serviceability and the capacity is over
utilized by increased service demand from upstream systems. From all the
integrated sub-systems, queuing systems are majorly categorized as choking
elements due to their limited service length and lack of processing details.
The situation becomes more pronounced in multiprogramming environments where
the queue performance exponentially degrades with increased degree of
multiprogramming at upstream levels. This paper presents an approach to compute
the queue length and devise a distribution model such that the queue length is
dynamically adjusted depending on the sudden growth or decline of transmission
packets. The idea is to design a heat map of the memory and correlate it with
the queue length distribution. With each degree of multi-programmability, the
data processing logic is adjusted by the distribution model to arrive at an
endurance level queue for long term service under variable load conditions. It
will take away the current implementation of using delayed processing logic
and/or batch processing of data at downstream systems.
"
903,"The Transitional Behavior of Interference in Millimeter Wave Networks
  and Its Impact on Medium Access Control","  Millimeter wave (mmWave) communication systems use large number of antenna
elements that can potentially overcome severe channel attenuation by narrow
beamforming. Narrow-beam operation in mmWave networks also reduces multiuser
interference, introducing the concept of noise-limited wireless networks as
opposed to interference-limited ones. The noise-limited or interference-limited
regime heavily reflects on the medium access control (MAC) layer throughput and
on proper resource allocation and interference management strategies. Yet,
these regimes are ignored in current approaches to mmWave MAC layer design,
with the potential disastrous consequences on the communication performance. In
this paper, we investigate these regimes in terms of collision probability and
throughput. We derive tractable closed-form expressions for the collision
probability and MAC layer throughput of mmWave ad hoc networks, operating under
slotted ALOHA. The new analysis reveals that mmWave networks may exhibit a
non-negligible transitional behavior from a noise-limited regime to an
interference-limited one, depending on the density of the transmitters, density
and size of obstacles, transmission probability, operating beamwidth, and
transmission power. Such transitional behavior necessitates a new framework of
adaptive hybrid resource allocation procedure, containing both contention-based
and contention-free phases with on-demand realization of the contention-free
phase. Moreover, the conventional collision avoidance procedure in the
contention-based phase should be revisited, due to the transitional behavior of
interference, to maximize throughput/delay performance of mmWave networks. We
conclude that, unless proper hybrid schemes are investigated, the severity of
the transitional behavior may significantly reduce throughput/delay performance
of mmWave networks.
"
904,"On the performance overhead tradeoff of distributed principal component
  analysis via data partitioning","  Principal component analysis (PCA) is not only a fundamental dimension
reduction method, but is also a widely used network anomaly detection
technique. Traditionally, PCA is performed in a centralized manner, which has
poor scalability for large distributed systems, on account of the large network
bandwidth cost required to gather the distributed state at a fusion center.
Consequently, several recent works have proposed various distributed PCA
algorithms aiming to reduce the communication overhead incurred by PCA without
losing its inferential power. This paper evaluates the tradeoff between
communication cost and solution quality of two distributed PCA algorithms on a
real domain name system (DNS) query dataset from a large network. We also apply
the distributed PCA algorithm in the area of network anomaly detection and
demonstrate that the detection accuracy of both distributed PCA-based methods
has little degradation in quality, yet achieves significant savings in
communication bandwidth.
"
905,Tandem Queueing Systems Maximum Throughput Problem,"  In this paper we consider the problem of maximum throughput for tandem
queueing system. We modeled this system as a Quasi-Birth-Death process. In
order to do this we named level the number of customers waiting in the first
buffer (including the customer in service) and we called phase the state of the
remining servers. Using this model we studied the problem of maximum throughput
of the system: the maximum arrival rate that a given system could support
before becoming saturated, or unstable. We considered different particular
cases of such systems, which were obtained by modifying the capacity of the
intermediary buffers, the arrival rate and the service rates. The results of
the simulations are presented in our paper and can be summed up in the
following conclusions: 1. The analytic formula for the maximum throughput of
the system tends to become rather complicated when the number of servers
increase 2. The maximum throughput of the system converges as the number of
servers increases 3. The homogeneous case reveals an interesting
characteristic: if we reverse the order of the servers, maximum thruoughput of
the system remains unchanged The QBD process used for the case of Poisson
arrivals can be extended to model more general arrival processes.
"
906,"Evaluation of Time-Critical Communications for IEC 61850-Substation
  Network Architecture","  Present-day developments, in electrical power transmission and distribution,
require considerations of the status quo. In other meaning, international
regulations enforce increasing of reliability and reducing of environment
impact, correspondingly they motivate developing of dependable systems. Power
grids especially intelligent (smart grids) ones become industrial solutions
that follow standardized development. The International standardization, in the
field of power transmission and distribution, improve technology influences.
The rise of dedicated standards for SAS (Substation Automation Systems)
communications, such as the leading International Electro-technical Commission
standard IEC 61850, enforces modern technological trends in this field. Within
this standard, a constraint of low ETE (End-to-End) latency should be
respected, and time-critical status transmission must be achieved. This
experimental study emphasis on IEC 61850 SAS communication standard, e.g. IEC
61850 GOOSE (Generic Object Oriented Substation Events), to implement an
investigational method to determine the protection communication delay. This
method observes GOOSE behaviour by adopting monitoring and analysis
capabilities. It is observed by using network test equipment, i.e. SPAN (Switch
Port Analyser) and TAP (Test Access Point) devices, with on-the-shelf available
hardware and software solutions.
"
907,"Non-Asymptotic Delay Bounds for (k,l) Fork-Join Systems and Multi-Stage
  Fork-Join Networks","  Parallel systems have received increasing attention with numerous recent
applications such as fork-join systems, load-balancing, and l-out-of-k
redundancy. Common to these systems is a join or resequencing stage, where
tasks that have finished service may have to wait for the completion of other
tasks so that they leave the system in a predefined order. These
synchronization constraints make the analysis of parallel systems challenging
and few explicit results are known. In this work, we model parallel systems
using a max-plus approach that enables us to derive statistical bounds of
waiting and sojourn times. Taking advantage of max-plus system theory, we also
show end-to-end delay bounds for multi-stage fork-join networks. We contribute
solutions for basic G|G|1 fork-join systems, parallel systems with
load-balancing, as well as general (k,l) fork-join systems with redundancy. Our
results provide insights into the respective advantages of l-out-of-k
redundancy vs. load-balancing.
"
908,"Performance Analysis of an Unreliable $M/G/1$ Retrial Queue with Two-way
  Communication","  Efficient use of call center operators through technological innovations more
often come at the expense of added operation management issues. In this paper,
the stationary characteristics of an $M/G/1$ retrial queue is investigated
where the single server, subject to active failures, primarily attends incoming
calls and directs outgoing calls only when idle. The incoming calls arriving at
the server follow a Poisson arrival process, while outgoing calls are made in
an exponentially distributed time. On finding the server unavailable (either
busy or temporarily broken down), incoming calls intrinsically join the virtual
orbit from which they re-attempt for service at exponentially distributed time
intervals. The system stability condition along with probability generating
functions for the joint queue length distribution of the number of calls in the
orbit and the state of the server are derived and evaluated numerically in the
context of mean system size, server availability, failure frequency and orbit
waiting time.
"
909,"An Online Delay Efficient Packet Scheduler for M2M Traffic in Industrial
  Automation","  Some Machine-to-Machine (M2M) communication links particularly those in a
industrial automation plant have stringent latency requirements. In this paper,
we study the delay-performance for the M2M uplink from the sensors to a
Programmable Logic Controller (PLC) in a industrial automation scenario. The
uplink traffic can be broadly classified as either Periodic Update (PU) and
Event Driven (ED). The PU arrivals from different sensors are periodic,
synchronized by the PLC and need to be processed by a prespecified firm latency
deadline. On the other hand, the ED arrivals are random, have low-arrival rate,
but may need to be processed quickly depending upon the criticality of the
application. To accommodate these contrasting Quality-of-Service (QoS)
requirements, we model the utility of PU and ED packets using step function and
sigmoidal functions of latency respectively. Our goal is to maximize the
overall system utility while being proportionally fair to both PU and ED data.
To this end, we propose a novel online QoS-aware packet scheduler that gives
priority to ED data as long as that results the latency deadline is met for PU
data. However as the size of networks increases, we drop the PU packets that
fail to meet latency deadline which reduces congestion and improves overall
system utility. Using extensive simulations, we compare the performance of our
scheme with various scheduling policies such as First-Come-First-Serve (FCFS),
Earliest-Due-Date (EDD) and (preemptive) priority. We show that our scheme
outperforms the existing schemes for various simulation scenarios.
"
910,Network Coding as a Service,"  Network Coding (NC) shows great potential in various communication scenarios
through changing the packet forwarding principles of current networks. It can
improve not only throughput, latency, reliability and security but also
alleviates the need of coordination in many cases. However, it is still
controversial due to widespread misunderstandings on how to exploit the
advantages of it. The aim of the paper is to facilitate the usage of NC by
$(i)$ explaining how it can improve the performance of the network (regardless
the existence of any butterfly in the network), $(ii)$ showing how Software
Defined Networking (SDN) can resolve the crucial problems of deployment and
orchestration of NC elements, and $(iii)$ providing a prototype architecture
with measurement results on the performance of our network coding capable
software router implementation compared by fountain codes.
"
911,Constrained Multi-user Multi-server Max-Min Fair Queuing,"  In this paper, a multi-user multi-server queuing system is studied in which
each user is constrained to get service from a subset of servers. In the
studied system, rate allocation in the sense of max-min fairness results in
multi-level fair rates. To achieve such fair rates, we propose $CM^4FQ$
algorithm. In this algorithm users are chosen for service on a packet by packet
basis. The priority of each user $i$ to be chosen at time $t$ is determined
based on a parameter known as service tag (representing the amount of work
counted for user $i$ till time $t$). Hence, a free server will choose to serve
an eligible user with the minimum service tag. Based on such simple selection
criterion, $CM^4FQ$ aims at guaranteed fair throughput for each demanding user
without explicit knowledge of each server service rate. We argue that $CM^4FQ$
can be applied in a variety of practical queuing systems specially in mobile
cloud computing architecture.
"
912,"M2M Massive Access in LTE: RACH Performance Evaluation in a Smart City
  Scenario","  Several studies assert that the random access procedure of the Long Term
Evolution (LTE) cellular standard may not be effective whenever a massive
number of simultaneous connection attempts are performed by terminals, as may
happen in a typical Internet of Things or Smart City scenario. Nevertheless,
simulation studies in real deployment scenarios are missing because many
system-level simulators do not implement the LTE random access procedure in
detail. In this paper, we propose a patch for the LTE module of ns-3, one of
the most prominent open-source network simulators, to improve the accuracy of
the routine that simulates the LTE Random Access Channel (RACH). The patched
version of the random access procedure is compared with the default one and the
issues arising from massive simultaneous access from mobile terminals in LTE
are assessed via a simulation campaign.
"
913,"Architecture-Aware Optimization of an HEVC decoder on Asymmetric
  Multicore Processors","  Low-power asymmetric multicore processors (AMPs) attract considerable
attention due to their appealing performance-power ratio for energy-constrained
environments. However, these processors pose a significant programming
challenge due to the integration of cores with different performance
capabilities, asking for an asymmetry-aware scheduling solution that carefully
distributes the workload.
  The recent HEVC standard, which offers several high-level parallelization
strategies, is an important application that can benefit from an implementation
tailored for the low-power AMPs present in many current mobile or hand-held
devices. In this scenario, we present an architecture-aware implementation of
an HEVC decoder that embeds a criticality-aware scheduling strategy tuned for a
Samsung Exynos 5422 system-on-chip furnished with an ARM big.LITTLE AMP. The
performance and energy efficiency of our solution is further enhanced by
exploiting the NEON vector engine available in the ARM big.LITTLE architecture.
Experimental results expose a 1080p real-time HEVC decoding at 24 frames/sec,
and a reduction of energy consumption over 20%.
"
914,Convex Optimization of Real Time SoC,"  Convex optimization methods are employed to optimize a real-time (RT)
system-on-chip (SoC) under a variety of physical resource-driven constraints,
demonstrated on an industry MPEG2 encoder SoC. The power optimization is
compared to conventional performance-optimization framework, showing a factor
of two and a half saving in power. Convex optimization is shown to be very
efficient in a high-level early stage design exploration, guiding computer
architects as to the choice of area, voltage, and frequency of the individual
components of the Chip Multiprocessor (CMP).
"
915,"A Gain Function for Architectural Decision-Making in Scientific
  Computing","  Scientific Computing typically requires large computational needs which have
been addressed with High Performance Distributed Computing. It is essential to
efficiently deploy a number of complex scientific applications, which have
different characteristics, and so require distinct computational resources too.
However, in many research laboratories, this high performance architecture is
not dedicated. So, the architecture must be shared to execute a set of
scientific applications, with so many different execution times and relative
importance to research. Also, the high performance architectures have different
characteristics and costs. When a new infrastructure has to be acquired to meet
the needs of this scenario, the decision-making is hard and complex. In this
work, we present a Gain Function as a model of an utility function, with which
it is possible a decision-making with confidence. With the function is possible
to evaluate the best architectural option taking into account aspects of
applications and architectures, including the executions time, cost of
architecture, the relative importance of each application and also the relative
importance of performance and cost on the final evaluation. This paper presents
the Gain Function, examples, and a real case showing their applicabilities.
"
916,Buffer-aware Worst Case Timing Analysis of Wormhole Network On Chip,"  A buffer-aware worst-case timing analysis of wormhole NoC is proposed in this
paper to integrate the impact of buffer size on the different dependencies
relationship between flows, i.e. direct and indirect blocking flows, and
consequently the timing performance. First, more accurate definitions of direct
and indirect blocking flows sets have been introduced to take into account the
buffer size impact. Then, the modeling and worst-case timing analysis of
wormhole NoC have been detailed, based on Network Calculus formalism and the
newly defined blocking flows sets. This introduced approach has been
illustrated in the case of a realistic NoC case study to show the trade off
between latency and buffer size. The comparative analysis of our proposed
Buffer-aware timing analysis with conventional approaches is conducted and
noticeable enhancements in terms of maximum latency have been proved.
"
917,Daleel: Simplifying Cloud Instance Selection Using Machine Learning,"  Decision making in cloud environments is quite challenging due to the
diversity in service offerings and pricing models, especially considering that
the cloud market is an incredibly fast moving one. In addition, there are no
hard and fast rules, each customer has a specific set of constraints (e.g.
budget) and application requirements (e.g. minimum computational resources).
Machine learning can help address some of the complicated decisions by carrying
out customer-specific analytics to determine the most suitable instance type(s)
and the most opportune time for starting or migrating instances. We employ
machine learning techniques to develop an adaptive deployment policy, providing
an optimal match between the customer demands and the available cloud service
offerings. We provide an experimental study based on extensive set of job
executions over a major public cloud infrastructure.
"
918,Energy Efficient Video Fusion with Heterogeneous CPU-FPGA Devices,"  This paper presents a complete video fusion system with hardware acceleration
and investigates the energy trade-offs between computing in the CPU or the FPGA
device. The video fusion application is based on the Dual-Tree Complex Wavelet
Transforms (DT-CWT). In this work the transforms are mapped to a hardware
accelerator using high-level synthesis tools for the FPGA and also vectorized
code for the single instruction multiple data (SIMD) engine available in the
CPU. The accelerated system reduces computation time and energy by a factor of
2. Moreover, the results show a key finding that the FPGA is not always the
best choice for acceleration, and the SIMD engine should be selected when the
wavelet decomposition reduces the frame size below a certain threshold. This
dependency on workload size means that an adaptive system that intelligently
selects between the SIMD engine and the FPGA achieves the most energy and
performance efficiency point.
"
919,"Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon
  Era","  The key challenge to improving performance in the age of Dark Silicon is how
to leverage transistors when they cannot all be used at the same time. In
modern SOCs, these transistors are often used to create specialized
accelerators which improve energy efficiency for some applications by 10-1000X.
While this might seem like the magic bullet we need, for most CPU applications
more energy is dissipated in the memory system than in the processor: these
large gains in efficiency are only possible if the DRAM and memory hierarchy
are mostly idle. We refer to this desirable state as Dark Memory, and it only
occurs for applications with an extreme form of locality.
  To show our findings, we introduce Pareto curves in the energy/op and
mm$^2$/(ops/s) metric space for compute units, accelerators, and on-chip
memory/interconnect. These Pareto curves allow us to solve the power,
performance, area constrained optimization problem to determine which
accelerators should be used, and how to set their design parameters to optimize
the system. This analysis shows that memory accesses create a floor to the
achievable energy-per-op. Thus high performance requires Dark Memory, which in
turn requires co-design of the algorithm for parallelism and locality, with the
hardware.
"
920,A Stochastic Performance Model for Pipelined Krylov Methods,"  Pipelined Krylov methods seek to ameliorate the latency due to inner products
necessary for projection by overlapping it with the computation associated with
sparse matrix-vector multiplication. We clarify a folk theorem that this can
only result in a speedup of $2\times$ over the naive implementation. Examining
many repeated runs, we show that stochastic noise also contributes to the
latency, and we model this using an analytical probability distribution. Our
analysis shows that speedups greater than $2\times$ are possible with these
algorithms.
"
921,"High-performance generation of the Hamiltonian and Overlap matrices in
  FLAPW methods","  One of the greatest efforts of computational scientists is to translate the
mathematical model describing a class of physical phenomena into large and
complex codes. Many of these codes face the difficulty of implementing the
mathematical operations in the model in terms of low level optimized kernels
offering both performance and portability. Legacy codes suffer from the
additional curse of rigid design choices based on outdated performance metrics
(e.g. minimization of memory footprint). Using a representative code from the
Materials Science community, we propose a methodology to restructure the most
expensive operations in terms of an optimized combination of dense linear
algebra kernels. The resulting algorithm guarantees an increased performance
and an extended life span of this code enabling larger scale simulations.
"
922,Recursive Algorithms for Dense Linear Algebra: The ReLAPACK Collection,"  To exploit both memory locality and the full performance potential of highly
tuned kernels, dense linear algebra libraries such as LAPACK commonly implement
operations as blocked algorithms. However, to achieve next-to-optimal
performance with such algorithms, significant tuning is required. On the other
hand, recursive algorithms are virtually tuning free, and yet attain similar
performance. In this paper, we first analyze and compare blocked and recursive
algorithms in terms of performance, and then introduce ReLAPACK, an open-source
library of recursive algorithms to seamlessly replace most of LAPACK's blocked
algorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,
and even improves upon the performance of optimizes libraries.
"
923,Resource Management for OFDMA based Next Generation 802.11ax WLANs,"  Recently, IEEE 802.11ax Task Group has adapted OFDMA as a new technique for
enabling multi-user transmission. It has been also decided that the scheduling
duration should be same for all the users in a multi-user OFDMA so that the
transmission of the users should end at the same time. In order to realize that
condition, the users with insufficient data should transmit null data (i.e.
padding) to fill the duration. While this scheme offers strong features such as
resilience to Overlapping Basic Service Set (OBSS) interference and ease of
synchronization, it also poses major side issues of degraded throughput
performance and waste of devices' energy. In this work, for OFDMA based 802.11
WLANs we first propose practical algorithm in which the scheduling duration is
fixed and does not change from time to time. In the second algorithm the
scheduling duration is dynamically determined in a resource allocation
framework by taking into account the padding overhead, airtime fairness and
energy consumption of the users. We analytically investigate our resource
allocation problems through Lyapunov optimization techniques and show that our
algorithms are arbitrarily close to the optimal performance at the price of
reduced convergence rate. We also calculate the overhead of our algorithms in a
realistic set-up and propose solutions for the implementation issues.
"
924,Spatial multi-LRU Caching for Wireless Networks with Coverage Overlaps,"  This article introduces a novel family of decentralised caching policies,
applicable to wireless networks with finite storage at the edge-nodes
(stations). These policies are based on the Least-Recently-Used replacement
principle, and are, here, referred to as spatial multi-LRU. Based on these,
cache inventories are updated in a way that provides content diversity to users
who are covered by, and thus have access to, more than one station. Two
variations are proposed, namely the multi-LRU-One and -All, which differ in the
number of replicas inserted in the involved caches. By introducing spatial
approximations, we propose a Che-like method to predict the hit probability,
which gives very accurate results under the Independent Reference Model (IRM).
It is shown that the performance of multi-LRU increases the more the
multi-coverage areas increase, and it approaches the performance of other
proposed centralised policies, when multi-coverage is sufficient. For IRM
traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic
exhibits temporal locality the -All variation can perform better.
"
925,"Contrasting Effects of Replication in Parallel Systems: From Overload to
  Underload and Back","  Task replication has recently been advocated as a practical solution to
reduce latencies in parallel systems. In addition to several convincing
empirical studies, some others provide analytical results, yet under some
strong assumptions such as Poisson arrivals, exponential service times, or
independent service times of the replicas themselves, which may lend themselves
to some contrasting and perhaps contriving behavior. For instance, under the
second assumption, an overloaded system can be stabilized by a replication
factor, but can be sent back in overload through further replication. In turn,
under the third assumption, strictly larger stability regions of replication
systems do not necessarily imply smaller delays.
  Motivated by the need to dispense with such common and restricting
assumptions, which may additionally cause unexpected behavior, we develop a
unified and general theoretical framework to compute tight bounds on the
distribution of response times in general replication systems. These results
immediately lend themselves to the optimal number of replicas minimizing
response time quantiles, depending on the parameters of the system (e.g., the
degree of correlation amongst replicas). As a concrete application of our
framework, we design a novel replication policy which can improve the stability
region of classical fork-join queueing systems by $\mathcal{O}(\ln K)$, in the
number of servers $K$.
"
926,Wanted: Floating-Point Add Round-off Error instruction,"  We propose a new instruction (FPADDRE) that computes the round-off error in
floating-point addition. We explain how this instruction benefits
high-precision arithmetic operations in applications where double precision is
not sufficient. Performance estimates on Intel Haswell, Intel Skylake, and AMD
Steamroller processors, as well as Intel Knights Corner co-processor,
demonstrate that such an instruction would improve the latency of double-double
addition by up to 55% and increase double-double addition throughput by up to
103%, with smaller, but non-negligible benefits for double-double
multiplication. The new instruction delivers up to 2x speedups on three
benchmarks that use high-precision floating-point arithmetic: double-double
matrix-matrix multiplication, compensated dot product, and polynomial
evaluation via the compensated Horner scheme.
"
927,"Analysis of the Packet Loss Probability in Energy Harvesting Cognitive
  Radio Networks","  A Markovian battery model is proposed to provide the variation of energy
states for energy harvesting (EH) secondary users (SUs) in the EH cognitive
radio networks (CRN). Based on the proposed battery model, we derive the packet
loss probability in the EH SUs due to sensing inaccuracy and energy outage.
With the proposed analysis, the packet loss probability can easily be predicted
and utilized to optimize the transmission policy (i.e., opportunities for
successful transmission and EH) of EH SUs to improve their throughput.
Especially, the proposed method can be applied to upper layer (scheduling and
routing) optimization. To this end, we validate the proposed analysis through
Monte-Carlo simulation and show an agreement between the analysis and
simulations results.
"
928,"FastCap: An Efficient and Fair Algorithm for Power Capping in Many-Core
  Systems","  Future servers will incorporate many active lowpower modes for different
system components, such as cores and memory. Though these modes provide
flexibility for power management via Dynamic Voltage and Frequency Scaling
(DVFS), they must be operated in a coordinated manner. Such coordinated control
creates a combinatorial space of possible power mode configurations. Given the
rapid growth of the number of cores, it is becoming increasingly challenging to
quickly select the configuration that maximizes the performance under a given
power budget. Prior power capping techniques do not scale well to large numbers
of cores, and none of those works has considered memory DVFS. In this paper, we
present FastCap, our optimization approach for system-wide power capping, using
both CPU and memory DVFS. Based on a queuing model, FastCap formulates power
capping as a non-linear optimization problem where we seek to maximize the
system performance under a power budget, while promoting fairness across
applications. Our FastCap algorithm solves the optimization online and
efficiently (low complexity on the number of cores), using a small set of
performance counters as input. To evaluate FastCap, we simulate it for a
many-core server running different types of workloads. Our results show that
FastCap caps power draw accurately, while producing better application
performance and fairness than many existing CPU power capping methods (even
after they are extended to use of memory DVFS as well).
"
929,"Performance Assessment of WhatsApp and IMO on Android Operating System
  (Lollipop and KitKat) during VoIP calls using 3G or WiFi","  This paper assesses the performance of mobile messaging and VoIP connections.
We investigate the CPU usage of WhatsApp and IMO under different scenarios.
This analysis also enabled a comparison of the performance of these
applications on two Android operating system (OS) versions: KitKat or Lollipop.
Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4.
The applications behavior was statistically investigated for both sending and
receiving VoIP calls. Connections have been examined over 3G and WiFi. The
handset model plays a decisive role in CPU usage of the application. t-tests
showed that IMO has a better performance that WhatsApp whatever be the Android
at a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires
less CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy
Note 4 using WiFi always outperformed S4 in terms of processing efficiency.
"
930,Design Heuristic for Parallel Many Server Systems under FCFS-ALIS,"  We study a parallel queueing system with multiple types of servers and
customers. A bipartite graph describes which pairs of customer-server types are
compatible. We consider the service policy that always assigns servers to the
first, longest waiting compatible customer, and that always assigns customers
to the longest idle compatible server if on arrival, multiple compatible
servers are available. For a general renewal stream of arriving customers and
general service time distributions, the behavior of such systems is very
complicated. In particular, the calculation of matching rates, the fraction of
services of customer-server type, is intractable. We suggest through a
heuristic argument that if the number of servers becomes large, the matching
rates are well approximated by matching rates calculated from the tractable
bipartite infinite matching model. We present simulation evidence to support
this heuristic argument, and show how this can be used to design systems with
desired performance requirements.
"
931,Performance Localisation,"  Performance becomes an issue particularly when execution cost hinders the
functionality of a program. Typically a profiler can be used to find program
code execution which represents a large portion of the overall execution cost
of a program. Pinpointing where a performance issue exists provides a starting
point for tracing cause back through a program.
  While profiling shows where a performance issue manifests, we use mutation
analysis to show where a performance improvement is likely to exist. We find
that mutation analysis can indicate locations within a program which are highly
impactful to the overall execution cost of a program yet are executed
relatively infrequently. By better locating potential performance improvements
in programs we hope to make performance improvement more amenable to
automation.
"
932,"PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark
  for Big-Data Platforms","  The rise of big data systems has created a need for benchmarks to measure and
compare the capabilities of these systems. Big data benchmarks present unique
scalability challenges. The supercomputing community has wrestled with these
challenges for decades and developed methodologies for creating rigorous
scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline
benchmark employs supercomputing benchmarking methodologies to create a
scalable benchmark that is reflective of many real-world big data processing
systems. The PageRank pipeline benchmark builds on existing prior scalable
benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with
multiple integrated kernels that can be run together or independently. Each
kernel is well defined mathematically and can be implemented in any programming
environment. The linear algebraic nature of PageRank makes it well suited to
being implemented using the GraphBLAS standard. The computations are simple
enough that performance predictions can be made based on simple computing
hardware models. The surrounding kernels provide the context for each kernel
that allows rigorous definition of both the input and the output for each
kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable
in both problem size and hardware, it can be used to measure and quantitatively
compare a wide range of present day and future systems. Serial implementations
in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been
implemented and their single threaded performance has been measured.
"
933,"Quality and Cost of Deterministic Network Calculus - Design and
  Evaluation of an Accurate and Fast Analysis","  Networks are integral parts of modern safety-critical systems and
certification demands the provision of guarantees for data transmissions.
Deterministic Network Calculus (DNC) can compute a worst-case bound on a data
flow's end-to-end delay. Accuracy of DNC results has been improved steadily,
resulting in two DNC branches: the classical algebraic analysis and the more
recent optimization-based analysis. The optimization-based branch provides a
theoretical solution for tight bounds. Its computational cost grows, however,
(possibly super-)exponentially with the network size. Consequently, a heuristic
optimization formulation trading accuracy against computational costs was
proposed. In this paper, we challenge optimization-based DNC with a new
algebraic DNC algorithm.
  We show that: (i) no current optimization formulation scales well with the
network size and (ii) algebraic DNC can be considerably improved in both
aspects, accuracy and computational cost. To that end, we contribute a novel
DNC algorithm that transfers the optimization's search for best attainable
delay bounds to algebraic DNC. It achieves a high degree of accuracy and our
novel efficiency improvements reduce the cost of the analysis dramatically. In
extensive numerical experiments, we observe that our delay bounds deviate from
the optimization-based ones by only 1.142% on average while computation times
simultaneously decrease by several orders of magnitude.
"
934,What slows you down? Your network or your device?,"  This study takes a close look at mobile web performance. The two main
parameters determining web page load time are the network speed and the
computing power of the end-user device. Based on data from real users, this
paper quantifies the relative importance of network and device. The findings
suggest that increased processing power of latest generation smart phones and
optimized browsers have a significant impact on web performance; up to 56%
reduction in median page load time from one generation to the following. The
cellular networks, on the other hand, have become so mature that the median
page load time on one fiber-to-the-home network (using wifi for the last meter)
is only 18-28% faster than cellular and the median page load time on one DSL
network is 19% slower compared to a well-deployed cellular network.
"
935,TTC: A high-performance Compiler for Tensor Transpositions,"  We present TTC, an open-source parallel compiler for multidimensional tensor
transpositions. In order to generate high-performance C++ code, TTC explores a
number of optimizations, including software prefetching, blocking,
loop-reordering, and explicit vectorization. To evaluate the performance of
multidimensional transpositions across a range of possible use-cases, we also
release a benchmark covering arbitrary transpositions of up to six dimensions.
Performance results show that the routines generated by TTC achieve close to
peak memory bandwidth on both the Intel Haswell and the AMD Steamroller
architectures, and yield significant performance gains over modern compilers.
By implementing a set of pruning heuristics, TTC allows users to limit the
number of potential solutions; this option is especially useful when dealing
with high-dimensional tensors, as the search space might become prohibitively
large. Experiments indicate that when only 100 potential solutions are
considered, the resulting performance is about 99% of that achieved with
exhaustive search.
"
936,"Study and evaluation of an Irregular Graph Algorithm on Multicore and
  GPU Processor Architectures","  One area of Computing applications which poses significant challenge of
performance scalability on Chip Multiprocessors(CMP's) are Irregular
applications. Such applications have very little computation and unpredictable
memory access patterns making them memory-bound in contrast to compute-bound
applications. Since the gap between processor and memory performance continues
to exist, difficulty to hide and decrease this gap is one of the important
factors which results in poor performance of these applications on CMP's.
  The goal of this thesis is to overcome many such challenges posed during
performance acceleration of an irregular graph algorithm called Triad Census.
We accelerated the Triad Census algorithm on two significantly different Chip
Multiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads per
socket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads
per core).
  The experimental results obtained on Intel Multicore Xeon system shows
performance speedups (w.r.t baseline sequential) of maximum 56x , average 33x
and minimum 8.3x for real world graph data sets. On NVIDIA Tesla C1060 GPGPU,
we were able to match almost equally the Multicore results - 58.4x maximum,
32.8x average and 4.2x minimum speedups w.r.t baseline sequential. In terms of
raw performance, for the graph data set called Patents network, our results on
Intel Xeon Multicore(16 hw threads) were 1.27x times faster than previous
results on Cray XMT(16 hw threads) while results achieved on GPGPU were
comparatively slower(0.72x). To the best of our knowledge, this algorithm has
only been accelerated on supercomputer class computer named Cray XMT and no
work exists that demonstrates performance evaluation and comparison of this
algorithm on relatively lower-cost Multicore and GPGPU based platforms.
"
937,"A Performance Evaluation of Container Technologies on Internet of Things
  Devices","  The use of virtualization technologies in different contexts - such as Cloud
Environments, Internet of Things (IoT), Software Defined Networking (SDN) - has
rapidly increased during the last years. Among these technologies,
container-based solutions own characteristics for deploying distributed and
lightweight applications. This paper presents a performance evaluation of
container technologies on constrained devices, in this case, on Raspberry Pi.
The study shows that, overall, the overhead added by containers is negligible.
"
938,Extension of PRISM by Synthesis of Optimal Timeouts in Fixed-Delay CTMC,"  We present a practically appealing extension of the probabilistic model
checker PRISM rendering it to handle fixed-delay continuous-time Markov chains
(fdCTMCs) with rewards, the equivalent formalism to the deterministic and
stochastic Petri nets (DSPNs). fdCTMCs allow transitions with fixed-delays (or
timeouts) on top of the traditional transitions with exponential rates. Our
extension supports an evaluation of expected reward until reaching a given set
of target states. The main contribution is that, considering the fixed-delays
as parameters, we implemented a synthesis algorithm that computes the
epsilon-optimal values of the fixed-delays minimizing the expected reward. We
provide a performance evaluation of the synthesis on practical examples.
"
939,"Faster and Cheaper: Parallelizing Large-Scale Matrix Factorization on
  GPUs","  Matrix factorization (MF) is employed by many popular algorithms, e.g.,
collaborative filtering. The emerging GPU technology, with massively multicore
and high intra-chip memory bandwidth but limited memory capacity, presents an
opportunity for accelerating MF much further when appropriately exploiting the
GPU architectural characteristics.
  This paper presents cuMF, a CUDA-based matrix factorization library that
implements memory-optimized alternate least square (ALS) method to solve very
large-scale MF. CuMF uses a variety set of techniques to maximize the
performance on either single or multiple GPUs. These techniques include smart
access of sparse data leveraging GPU memory hierarchy, using data parallelism
in conjunction with model parallelism, minimizing the communication overhead
between computing units, and utilizing a novel topology-aware parallel
reduction scheme.
  With only a single machine with four Nvidia GPU cards, cuMF can be 6-10 times
as fast, and 33-100 times as cost-efficient, compared with the state-of-art
distributed CPU solutions. Moreover, this cuMF can solve the largest matrix
factorization problem ever reported yet in current literature, while
maintaining impressively good performance.
"
940,On Delay-Optimal Scheduling in Queueing Systems with Replications,"  In modern computer systems, jobs are divided into short tasks and executed in
parallel. Empirical observations in practical systems suggest that the task
service times are highly random and the job service time is bottlenecked by the
slowest straggling task. One common solution for straggler mitigation is to
replicate a task on multiple servers and wait for one replica of the task to
finish early. The delay performance of replications depends heavily on the
scheduling decisions of when to replicate, which servers to replicate on, and
which job to serve first. So far, little is understood on how to optimize these
scheduling decisions for minimizing the delay to complete the jobs. In this
paper, we present a comprehensive study on delay-optimal scheduling of
replications in both centralized and distributed multi-server systems.
Low-complexity scheduling policies are designed and are proven to be
delay-optimal or near delay-optimal in stochastic ordering among all causal and
non-preemptive policies. These theoretical results are established for general
system settings and delay metrics that allow for arbitrary arrival processes,
arbitrary job sizes, arbitrary due times, and heterogeneous servers with data
locality constraints. Novel sample-path tools are developed to prove these
results.
"
941,Helenos: A Realistic Benchmark for Distributed Transactional Memory,"  Transactional Memory (TM) is an approach to concurrency control that aims to
make writing parallel programs both effective and simple. The approach is
started in non-distributed multiprocessor systems, but is gaining popularity in
distributed systems to synchronize tasks at large scales. Efficiency and
scalability are often the key issues in TM research, so performance benchmarks
are an important part of it. However, while standard TM benchmarks like the
STAMP suite and STMBench7 are available and widely accepted, they do not
translate well into distributed systems. Hence, the set of benchmarks usable
with distributed TM systems is very limited, and must be padded with
microbenchmarks, whose simplicity and artificial nature often makes them
uninformative or misleading. Therefore, this paper introduces Helenos, a
realistic, complex, and comprehensive distributed TM benchmark based on the
problem of the Facebook inbox, an application of the Cassandra distributed
store.
"
942,"Queueing Analysis of a Large-Scale Bike Sharing System through
  Mean-Field Theory","  The bike sharing systems are fast increasing as a public transport mode in
urban short trips, and have been developed in many major cities around the
world. A major challenge in the study of bike sharing systems is that
large-scale and complex queueing networks have to be applied through
multi-dimensional Markov processes, while their discussion always suffers a
common difficulty: State space explosion. For this reason, this paper provides
a mean-field computational method to study such a large-scale bike sharing
steps: Firstly, a multi-dimensional Markov process is set up for expressing the
states of the bike sharing system, and the empirical process of the
multi-dimensional Markov process is given to partly overcome the difficulty of
state space explosion. Based on this, the mean-field equations are derived by
means of a virtual time-inhomogeneous M(t)/M(t)/1/K queue whose arrival and
service rates are determined by the mean-field computation. Secondly, the
martingale limit is employed to investigate the limiting behavior of the
empirical process, the fixed point is proved to be unique so that it can be
computed by means of a nonlinear birth-death process, the asymptotic
independence of this system is discussed simply, and specifically, these lead
to numerical computation of the steady-state probability of the problematic
(empty or full) stations. Finally, some numerical examples are given for
valuable observation on how the steady-state probability of the problematic
stations depends on some crucial parameters of the bike sharing system.
"
943,Effect of Bitcoin fee on transaction-confirmation process,"  In Bitcoin system, transactions are prioritized according to transaction
fees. Transactions without fees are given low priority and likely to wait for
confirmation. Because the demand of micro payment in Bitcoin is expected to
increase due to low remittance cost, it is important to quantitatively
investigate how transactions with small fees of Bitcoin affect the
transaction-confirmation time. In this paper, we analyze the
transaction-confirmation time by queueing theory. We model the
transaction-confirmation process of Bitcoin as a priority queueing system with
batch service, deriving the mean transaction-confirmation time. Numerical
examples show how the demand of transactions with low fees affects the
transaction-confirmation time. We also consider the effect of the maximum block
size on the transaction-confirmation time.
"
944,"C3PO: Computation Congestion Control (PrOactive) - an algorithm for
  dynamic diffusion of ephemeral in-network services","  There is an obvious trend that more and more data and computation are
migrating into networks nowadays. Combining mature virtualization technologies
with service-centric net- working, we are entering into an era where countless
services reside in an ISP network to provide low-latency access. Such services
are often computation intensive and are dynamically created and destroyed on
demands everywhere in the network to perform various tasks. Consequently, these
ephemeral in-network services introduce a new type of congestion in the network
which we refer to as ""computation congestion"". The service load need to be
effectively distributed on different nodes in order to maintain the
funtionality and responsiveness of the network, which calls for a new design
rather than reusing the centralised scheduler designed for cloud-based
services. In this paper, we study both passive and proactive control
strategies, based on the proactive control we further propose a fully
distributed solution which is low complexity, adaptive, and responsive to
network dynamics.
"
945,"Performance analysis of the Kahan-enhanced scalar product on current
  multi- and manycore processors","  We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent multi- and manycore processors. Using low-level instruction analysis and
the execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. The ECM model is extended
appropriately to accommodate not only modern Intel multicore chips but also the
Intel Xeon Phi ""Knights Corner"" coprocessor and an IBM POWER8 CPU. This allows
us to discuss the impact of processor features on the performance across four
modern architectures that are relevant for high performance computing.
"
946,"On the Duration and Intensity of Competitions in Nonlinear P\'olya Urn
  Processes with Fitness","  Cumulative advantage (CA) refers to the notion that accumulated resources
foster the accumulation of further resources in competitions, a phenomenon that
has been empirically observed in various contexts. The oldest and arguably
simplest mathematical model that embodies this general principle is the P\'olya
urn process, which finds applications in a myriad of problems. The original
model captures the dynamics of competitions between two equally fit agents
under linear CA effects, which can be readily generalized to incorporate
different fitnesses and nonlinear CA effects. We study two statistics of
competitions under the generalized model, namely duration (i.e., time of the
last tie) and intensity (i.e., number of ties). We give rigorous mathematical
characterizations of the tail distributions of both duration and intensity
under the various regimes for fitness and nonlinearity, which reveal very
interesting behaviors. For example, fitness superiority induces much shorter
competitions in the sublinear regime while much longer competitions in the
superlinear regime. Our findings can shed light on the application of P\'olya
urn processes in more general contexts where fitness and nonlinearity may be
present.
"
947,"Incentivizing Sharing in Realtime D2D Streaming Networks: A Mean Field
  Game Perspective","  We consider the problem of streaming live content to a cluster of co-located
wireless devices that have both an expensive unicast base-station-to-device
(B2D) interface, as well as an inexpensive broadcast device-to-device (D2D)
interface, which can be used simultaneously. Our setting is a streaming system
that uses a block-by-block random linear coding approach to achieve a target
percentage of on-time deliveries with minimal B2D usage. Our goal is to design
an incentive framework that would promote such cooperation across devices,
while ensuring good quality of service. Based on ideas drawn from truth-telling
auctions, we design a mechanism that achieves this goal via appropriate
transfers (monetary payments or rebates) in a setting with a large number of
devices, and with peer arrivals and departures. Here, we show that a Mean Field
Game can be used to accurately approximate our system. Furthermore, the
complexity of calculating the best responses under this regime is low. We
implement the proposed system on an Android testbed, and illustrate its
efficient performance using real world experiments.
"
948,"IPA in the Loop: Control Design for Throughput Regulation in Computer
  Processors","  A new technique for performance regulation in event-driven systems, recently
proposed by the authors, consists of an adaptive-gain integral control. The
gain is adjusted in the control loop by a real-time estimation of the
derivative of the plant-function with respect to the control input. This
estimation is carried out by Infinitesimal Perturbation Analysis (IPA). The
main motivation comes from applications to throughput regulation in computer
processors, where to-date, testing and assessment of the proposed control
technique has been assessed by simulation. The purpose of this paper is to
report on its implementation on a machine, namely an Intel Haswell
microprocessor, and compare its performance to that obtained from cycle-level,
full system simulation environment. The intrinsic contribution of the paper to
the Workshop on Discrete Event System is in describing the process of taking an
IPA-based design and simulation to a concrete implementation, thereby providing
a bridge between theory and applications.
"
949,"Modeling and predicting measured response time of cloud-based web
  services using long-memory time series","  Predicting cloud performance from user's perspective is a complex task,
because of several factors involved in providing the service to the consumer.
In this work, the response time of 10 real-world services is analyzed. We have
observed long memory in terms of the measured response time of the
CPU-intensive services and statistically verified this observation using
estimators of the Hurst exponent. Then, na\""ive, mean, autoregressive
integrated moving average (ARIMA) and autoregressive fractionally integrated
moving average (ARFIMA) methods are used to forecast the future values of
quality of service (QoS) at runtime. Results of the cross-validation over the
10 datasets show that the long-memory ARFIMA model provides the mean of 37.5 %
and the maximum of 57.8 % reduction in the forecast error when compared to the
short-memory ARIMA model according to the standard error measure of mean
absolute percentage error. Our work implies that consideration of the
long-range dependence in QoS data can help to improve the selection of services
according to their possible future QoS values.
"
950,Ready for Rain? A View from SPEC Research on the Future of Cloud Metrics,"  In the past decade, cloud computing has emerged from a pursuit for a
service-driven information and communication technology (ICT), into a
signifcant fraction of the ICT market. Responding to the growth of the market,
many alternative cloud services and their underlying systems are currently
vying for the attention of cloud users and providers. Thus, benchmarking them
is needed, to enable cloud users to make an informed choice, and to enable
system DevOps to tune, design, and evaluate their systems. This requires
focusing on old and new system properties, possibly leading to the re-design of
classic benchmarking metrics, such as expressing performance as throughput and
latency (response time), and the design of new, cloud-specififc metrics.
Addressing this requirement, in this work we focus on four system properties:
(i) elasticity of the cloud service, to accommodate large variations in the
amount of service requested, (ii) performance isolation between the tenants of
shared cloud systems, (iii) availability of cloud services and systems, and the
(iv) operational risk of running a production system in a cloud
environment.Focusing on key metrics, for each of these properties we review the
state-of-the-art, then select or propose new metrics together with measurement
approaches. We see the presented metrics as a foundation towards upcoming,
industry-standard, cloud benchmarks.
  Keywords: Cloud Computing; Metrics; Measurement; Benchmarking; Elasticity;
Isolation; Performance; Service Level Objective; Availability; Operational
Risk.
"
951,Analysis of a trunk reservation policy in the framework of fog computing,"  We analyze in this paper a system composed of two data centers with limited
capacity in terms of servers. When one request for a single server is blocked
at the first data center, this request is forwarded to the second one. To
protect the single server requests originally assigned to the second data
center, a trunk reservation policy is introduced (i.e., a redirected request is
accepted only if there is a sufficient number of free servers at the second
data center). After rescaling the system by assuming that there are many
servers in both data centers and high request arrival rates, we are led to
analyze a random walk in the quarter plane, which has the particularity of
having non constant reflecting conditions on one boundary of the quarter plane.
Contrary to usual reflected random walks, to compute the stationary
distribution of the presented random walk, we have to determine three unknown
functions, one polynomial and two infinite generating functions. We show that
the coefficients of the polynomial are solutions to a linear system. After
solving this linear system, we are able to compute the two other unknown
functions and the blocking probabilities at both data centers. Numerical
experiments are eventually performed to estimate the gain achieved by the trunk
reservation policy.
"
952,Ergodic Theory for Controlled Markov Chains with Stationary Inputs,"  Consider a stochastic process $\{X(t)\}$ on a finite state space $ {\sf
X}=\{1,\dots, d\}$. It is conditionally Markov, given a real-valued `input
process' $\{\zeta(t)\}$. This is assumed to be small, which is modeled through
the scaling, \[ \zeta_t = \varepsilon \zeta^1_t, \qquad 0\le \varepsilon \le
1\,, \] where $\{\zeta^1(t)\}$ is a bounded stationary process. The following
conclusions are obtained, subject to smoothness assumptions on the controlled
transition matrix and a mixing condition on $\{\zeta(t)\}$:
  (i) A stationary version of the process is constructed, that is coupled with
a stationary version of the Markov chain $\{X^\bullet$(t)\}obtained with
$\{\zeta(t)\}\equiv 0$. The triple $(\{X(t)\}, \{X^\bullet(t)\},\{\zeta(t)\})$
is a jointly stationary process satisfying \[ {\sf P}\{X(t) \neq X^\bullet(t)\}
= O(\varepsilon) \] Moreover, a second-order Taylor-series approximation is
obtained: \[ {\sf P}\{X(t) =i \} ={\sf P}\{X^\bullet(t) =i \} + \varepsilon^2
\varrho(i) + o(\varepsilon^2),\quad 1\le i\le d, \] with an explicit formula
for the vector $\varrho\in\mathbb{R}^d$.
  (ii) For any $m\ge 1$ and any function $f\colon \{1,\dots,d\}\times
\mathbb{R}\to\mathbb{R}^m$, the stationary stochastic process $Y(t) =
f(X(t),\zeta(t))$ has a power spectral density $\text{S}_f$ that admits a
second order Taylor series expansion: A function $\text{S}^{(2)}_f\colon
[-\pi,\pi] \to \mathbb{C}^{ m\times m}$ is constructed such that \[
\text{S}_f(\theta) = \text{S}^\bullet_f(\theta) + \varepsilon^2
\text{S}_f^{(2)}(\theta) + o(\varepsilon^2),\quad \theta\in [-\pi,\pi] . \] An
explicit formula for the function $\text{S}_f^{(2)}$ is obtained, based in part
on the bounds in (i).
  The results are illustrated using a version of the timing channel of
Anantharam and Verdu.
"
953,"A Hybrid Performance Analysis Technique for Distributed Real-Time
  Embedded Systems","  It remains a challenging problem to tightly estimate the worst case response
time of an application in a distributed embedded system, especially when there
are dependencies between tasks. We discovered that the state-of-the art
techniques considering task dependencies either fail to obtain a conservative
bound or produce a loose upper bound. We propose a novel conservative
performance analysis, called hybrid performance analysis, combining the
response time analysis technique and the scheduling time bound analysis
technique to compute a tighter bound fast. Through extensive experiments with
randomly generated graphs, superior performance of our proposed approach
compared with previous methods is confirmed.
"
954,"A Unified, Hardware-Fitted, Cross-GPU Performance Model","  We present a mechanism to symbolically gather performance-relevant operation
counts from numerically-oriented subprograms (`kernels') expressed in the Loopy
programming system, and apply these counts in a simple, linear model of kernel
run time. We use a series of `performance-instructive' kernels to fit the
parameters of a unified model to the performance characteristics of GPU
hardware from multiple hardware generations and vendors. We evaluate the
predictive power of the model on a broad array of computational kernels
relevant to scientific computing. In terms of the geometric mean, our simple,
vendor- and GPU-type-independent model achieves relative accuracy comparable to
that of previously published work using hardware specific models.
"
955,"Right buffer sizing matters: some dynamical and statistical studies on
  Compound TCP","  Motivated by recent concerns that queuing delays in the Internet are on the
rise, we conduct a performance evaluation of Compound TCP (C-TCP) in two
topologies: a single bottleneck and a multi-bottleneck topology, under
different traffic scenarios. The first topology consists of a single bottleneck
router, and the second consists of two distinct sets of TCP flows, regulated by
two edge routers, feeding into a common core router. We focus on some dynamical
and statistical properties of the underlying system. From a dynamical
perspective, we develop fluid models in a regime wherein the number of flows is
large, bandwidth-delay product is high, buffers are dimensioned small
(independent of the bandwidth-delay product) and routers deploy a Drop-Tail
queue policy. A detailed local stability analysis for these models yields the
following key insight: smaller buffers favour stability. Additionally, we
highlight that larger buffers, in addition to increasing latency, are prone to
inducing limit cycles in the system dynamics, via a Hopf bifurcation. These
limit cycles in turn cause synchronisation among the TCP flows, and also result
in a loss of link utilisation. For the topologies considered, we also
empirically analyse some statistical properties of the bottleneck queues. These
statistical analyses serve to validate an important modelling assumption: that
in the regime considered, each bottleneck queue may be approximated as either
an $M/M/1/B$ or an $M/D/1/B$ queue. This immediately makes the modelling
perspective attractive and the analysis tractable. Finally, we show that
smaller buffers, in addition to ensuring stability and low latency, would also
yield fairly good system performance, in terms of throughput and flow
completion times.
"
956,Balanced Fair Resource Sharing in Computer Clusters,"  We represent a computer cluster as a multi-server queue with some arbitrary
bipartite graph of compatibilities between jobs and servers. Each server
processes its jobs sequentially in FCFS order. The service rate of a job at any
given time is the sum of the service rates of all servers processing this job.
We show that the corresponding queue is quasi-reversible and use this property
to design a scheduling algorithm achieving balanced fair sharing of the service
capacity.
"
957,Smart Shires: The Revenge of Countrysides (Extended Version),"  This paper discusses the need to devise novel strategies to create smart
services specifically designed to non-metropolitan areas, i.e. countrysides.
These solutions must be viable, cheap an should take into consideration the
different nature of countrysides, that cannot afford the deployment of services
designed for smart cities. These solutions would have an important social
impact for people leaving in these countrysides, and might slow down the
constant migration of citizens towards metropolis. In this work, we focus on
communication technologies and practical technological/software distributed
architectures. An important aspect for the real deployment of these smart
shires is their simulation. We show that priority-based broadcast schemes over
ad-hoc networks can represent an effective communication substrate to be used
in a software middleware promoting the creation of applications for smart shire
scenarios.
"
958,"Do the Hard Stuff First: Scheduling Dependent Computations in
  Data-Analytics Clusters","  We present a scheduler that improves cluster utilization and job completion
times by packing tasks having multi-resource requirements and
inter-dependencies. While the problem is algorithmically very hard, we achieve
near-optimality on the job DAGs that appear in production clusters at a large
enterprise and in benchmarks such as TPC-DS. A key insight is that carefully
handling the long-running tasks and those with tough-to-pack resource needs
will produce good-enough schedules. However, which subset of tasks to treat
carefully is not clear (and intractable to discover). Hence, we offer a search
procedure that evaluates various possibilities and outputs a preferred schedule
order over tasks. An online component enforces the schedule orders desired by
the various jobs running on the cluster. In addition, it packs tasks, overbooks
the fungible resources and guarantees bounded unfairness for a variety of
desirable fairness schemes. Relative to the state-of-the art schedulers, we
speed up 50% of the jobs by over 30% each.
"
959,"An Analytical Solution for Probabilistic Guarantees of Reservation Based
  Soft Real-Time Systems","  We show a methodology for the computation of the probability of deadline miss
for a periodic real-time task scheduled by a resource reservation algorithm. We
propose a modelling technique for the system that reduces the computation of
such a probability to that of the steady state probability of an infinite state
Discrete Time Markov Chain with a periodic structure. This structure is
exploited to develop an efficient numeric solution where different
accuracy/computation time trade-offs can be obtained by operating on the
granularity of the model. More importantly we offer a closed form conservative
bound for the probability of a deadline miss. Our experiments reveal that the
bound remains reasonably close to the experimental probability in one real-time
application of practical interest. When this bound is used for the optimisation
of the overall Quality of Service for a set of tasks sharing the CPU, it
produces a good sub-optimal solution in a small amount of time.
"
960,"Architectural Impact on Performance of In-memory Data Analytics: Apache
  Spark Case Study","  While cluster computing frameworks are continuously evolving to provide
real-time data analysis capabilities, Apache Spark has managed to be at the
forefront of big data analytics for being a unified framework for both, batch
and stream data processing. However, recent studies on micro-architectural
characterization of in-memory data analytics are limited to only batch
processing workloads. We compare micro-architectural performance of batch
processing and stream processing workloads in Apache Spark using hardware
performance counters on a dual socket server. In our evaluation experiments, we
have found that batch processing are stream processing workloads have similar
micro-architectural characteristics and are bounded by the latency of frequent
data access to DRAM. For data accesses we have found that simultaneous
multi-threading is effective in hiding the data latencies. We have also
observed that (i) data locality on NUMA nodes can improve the performance by
10% on average and(ii) disabling next-line L1-D prefetchers can reduce the
execution time by up-to 14\% and (iii) multiple small executors can provide
up-to 36\% speedup over single large executor.
"
961,"Array Program Transformation with Loo.py by Example: High-Order Finite
  Elements","  To concisely and effectively demonstrate the capabilities of our program
transformation system Loo.py, we examine a transformation path from two
real-world Fortran subroutines as found in a weather model to a single
high-performance computational kernel suitable for execution on modern GPU
hardware. Along the transformation path, we encounter kernel fusion,
vectorization, prefetch- ing, parallelization, and algorithmic changes achieved
by mechanized conversion between imperative and functional/substitution- based
code, among a number more. We conclude with performance results that
demonstrate the effects and support the effectiveness of the applied
transformations.
"
962,System Level Performance Evaluation of LTE-V2X Network,"  Vehicles are among the fastest growing type of connected devices. Therefore,
there is a need for Vehicle-to-Everything (V2X) communication i.e. passing of
information from a Vehicle-to-Vehicle (V2V) or Vehicle-to-Infrastructure (V2I)
and vice versa. In this paper, the main focus is on the communication between
vehicles and road side units (RSUs) commonly referred to as V2I communication
in a multi-lane freeway scenario. Moreover, we analyze network related
bottlenecks such as the maximum number of vehicles that can be supported when
coverage is provided by the Long Term Evolution Advanced (LTE-A) network. The
performance evaluation is assessed through extensive system-level simulations.
Results show that new resource allocation and interference mitigation
techniques are needed in order to achieve the required high reliability
requirements, especially when network load is high.
"
963,Age-of-Information in the Presence of Error,"  We consider the peak age-of-information (PAoI) in an M/M/1 queueing system
with packet delivery error, i.e., update packets can get lost during
transmissions to their destination. We focus on two types of policies, one is
to adopt Last-Come-First-Served (LCFS) scheduling, and the other is to utilize
retransmissions, i.e., keep transmitting the most recent packet. Both policies
can effectively avoid the queueing delay of a busy channel and ensure a small
PAoI. Exact PAoI expressions under both policies with different error
probabilities are derived, including First-Come-First-Served (FCFS), LCFS with
preemptive priority, LCFS with non-preemptive priority, Retransmission with
preemptive priority, and Retransmission with non-preemptive priority. Numerical
results obtained from analysis and simulation are presented to validate our
results.
"
964,On the Aloha throughput-fairness tradeoff,"  A well-known inner bound of the stability region of the slotted Aloha
protocol on the collision channel with n users assumes worst-case service rates
(all user queues non-empty). Using this inner bound as a feasible set of
achievable rates, a characterization of the throughput--fairness tradeoff over
this set is obtained, where throughput is defined as the sum of the individual
user rates, and two definitions of fairness are considered: the Jain-Chiu-Hawe
function and the sum-user alpha-fair (isoelastic) utility function. This
characterization is obtained using both an equality constraint and an
inequality constraint on the throughput, and properties of the optimal
controls, the optimal rates, and the fairness as a function of the target
throughput are established. A key fact used in all theorems is the observation
that all contention probability vectors that extremize the fairness functions
take at most two non-zero values.
"
965,Energy Efficiency in Wireless Sensor Networks,"  Unlike most of the current research that focuses on a single aspect of WSNs,
we present an Energy Driven Architecture (EDA) as a new architecture for
minimising the total energy consumption of WSNs. EDA as a constituent-based
architecture is used to deploy WSNs according to energy dissipation through
their constituents. This view of overall energy consumption in WSNs can be
applied to optimising and balancing energy consumption and increasing the
network lifetime. Refer back to the architecture, we introduce a single overall
model and propose a feasible formulation to express the overall energy
consumption of a generic wireless sensor network application in terms of its
energy constituents. The formulation offers a concrete expression for
evaluating the performance of WSN application, optimising its constituents
operations, and designing more energy-efficient applications. The ultimate aim
is to produce an energy map architecture of a generic WSN application that
comprises essential and definable energy constituents and the relationships
between these constituents to explore strategies for minimising the overall
energy consumption of the application. Later, parameters affecting energy in
WSNs are extracted. The dependency between these parameters and the average
energy consumption of an application is then investigated. A few statistical
tools are applied for parameter reduction followed by random forest regression
to model energy consumption per delivered packet with and without parameter
reduction to determine the reduction in accuracy due to reduction. Finally, an
energy-efficient dynamic topology management algorithm is proposed based on the
EDA model and the prevalent parameters. The performance of the new topology
management algorithm, which employs Dijkstra to find energy-efficient lowest
cost paths among nodes, is compared to similar topology management algorithms.
"
966,Asymptotics of Insensitive Load Balancing and Blocking Phases,"  We address the problem of giving robust performance bounds based on the study
of the asymptotic behavior of the insensitive load balancing schemes when the
number of servers and the load scales jointly. These schemes have the desirable
property that the stationary distribution of the resulting stochastic network
depends on the distribution of job sizes only through its mean. It was shown
that they give good estimates of performance indicators for systems with finite
buffers, generalizing henceforth Erlang's formula whereas optimal policies are
already theoretically and computationally out of reach for networks of moderate
size. We study a single class of traffic acting on a symmetric set of processor
sharing queues with finite buffers and we consider the case where the load
scales with the number of servers. We characterize central limit theorems and
large deviations, the response of symmetric systems under those schemes at
different scales and show that three amplitudes of deviations can be
identified. A central limit scaling takes place for a sub-critical load; for
$\rho=1$, the number of free servers scales like $n^{ {\theta \over \theta+1}}$
($\theta$ being the buffer depth and $n$ being the number of servers) and is of
order 1 for super-critical loads. This further implies the existence of
different phases for the blocking probability, Before a (refined) critical load
$\rho_c(n)=1-a n^{- {\theta \over \theta+1}}$, the blocking is exponentially
small and becomes of order $ n^{- {\theta \over \theta+1}}$ at $\rho_c(n)$.
This generalizes the well-known Quality and Efficiency Driven (QED) regime or
Halfin-Whitt regime for a one-dimensional queue, and leads to a generalized
staffing rule for a given target blocking probability.
"
967,On Stability and Sojourn Time of Peer-to-Peer Queuing Systems,"  Recent development of peer-to-peer (P2P) services (e.g. streaming, file
sharing, and storage) systems introduces a new type of queue systems that
receive little attention before, where both job and server arrive and depart
randomly. Current study on these models focuses on the stability condition,
under exponential workload assumption. This paper extends existing result in
two aspects. In the first part of the paper we relax the exponential workload
assumption, and study the stability of systems with general workload
distribution. The second part of the paper focuses on the job sojourn time. An
upper bound and a lower bound for job sojourn time are investigated. We
evaluate tightness of the bounds by numerical analysis.
"
968,Simulation of the Internet of Things,"  This paper presents main concepts and issues concerned with the simulation of
Internet of Things (IoT). The heterogeneity of possible scenarios, arising from
the massive deployment of an enormous amount of sensors and devices, imposes
the use of sophisticated modeling and simulation techniques. In fact, the
simulation of IoT introduces several issues from both quantitative and
qualitative aspects. We discuss novel simulation techniques to enhance
scalability and to permit the real-time execution of massively populated IoT
environments (e.g., large-scale smart cities). In particular, we claim that
agent-based, adaptive Parallel and Distributed Simulation (PADS) approaches are
needed, together with multi-level simulation, which provide means to perform
highly detailed simulations, on demand. We present a use case concerned with
the simulation of smart territories.
"
969,Delay Bounds for Multiclass FIFO,"  FIFO is perhaps the simplest scheduling discipline. For single-class FIFO,
its delay guarantee performance has been extensively studied: The well-known
results include a stochastic delay bound for $GI/GI/1$ by Kingman and a
deterministic delay bound for $D/D/1$ by Cruz. However, for multiclass FIFO,
few such results are available. To fill the gap, we prove delay bounds for
multiclass FIFO in this work, considering both deterministic and stochastic
cases. Specifically, delay bounds are presented for multiclass D/D/1, GI/GI/1
and G/G/1. In addition, examples are provided for several basic settings to
demonstrate the obtained bounds in more explicit forms, which are also compared
with simulation results.
"
970,D-SPACE4Cloud: A Design Tool for Big Data Applications,"  The last years have seen a steep rise in data generation worldwide, with the
development and widespread adoption of several software projects targeting the
Big Data paradigm. Many companies currently engage in Big Data analytics as
part of their core business activities, nonetheless there are no tools and
techniques to support the design of the underlying hardware configuration
backing such systems. In particular, the focus in this report is set on Cloud
deployed clusters, which represent a cost-effective alternative to on premises
installations. We propose a novel tool implementing a battery of optimization
and prediction techniques integrated so as to efficiently assess several
alternative resource configurations, in order to determine the minimum cost
cluster deployment satisfying QoS constraints. Further, the experimental
campaign conducted on real systems shows the validity and relevance of the
proposed method.
"
971,"Enhancing Performance Bounds of Multiple-Ring Networks with Cyclic
  Dependencies based on Network Calculus","  Tightening performance bounds of ring networks with cyclic dependencies is
still an open problem in the literature. In this paper, we tackle such a
challenging issue based on Network Calculus. First, we review the conventional
timing approaches in the area and identify their main limitations, in terms of
delay bounds pessimism. Afterwards, we have introduced a new concept called Pay
Multiplexing Only at Convergence points (PMOC) to overcome such limitations.
PMOC considers the flow serialization phenomena along the flow path, by paying
the bursts of interfering flows only at the convergence points. The guaranteed
endto- end service curves under such a concept have been defined and proved for
mono-ring and multiple-ring networks, as well as under Arbitrary and Fixed
Priority multiplexing. A sensitivity analysis of the computed delay bounds for
mono and multiple-ring networks is conducted with respect to various flow and
network parameters, and their tightness is assessed in comparison with an
achievable worst-case delay. A noticeable enhancement of the delay bounds, thus
network resource efficiency and scalability, is highlighted under our proposal
with reference to conventional approaches. Finally, the efficiency of the PMOC
approach to provide timing guarantees is confirmed in the case of a realistic
avionics application.
"
972,"A Tighter Real-Time Communication Analysis for Wormhole-Switched
  Priority-Preemptive NoCs","  Simulations and runtime measurements are some of the methods which can be
used to evaluate whether a given NoC-based platform can accommodate application
workload and fulfil its timing requirements. Yet, these techniques are often
time-consuming, and hence can evaluate only a limited set of scenarios.
Therefore, these approaches are not suitable for safety-critical and hard
real-time systems, where one of the fundamental requirements is to provide
strong guarantees that all timing requirements will always be met, even in the
worst-case conditions. For such systems the analytic-based real-time analysis
is the only viable approach.
  In this paper the focus is on the real-time communication analysis for
wormhole-switched priority-preemptive NoCs. First, we elaborate on the existing
analysis and identify one source of pessimism. Then, we propose an extension to
the analysis, which efficiently overcomes this limitation, and allows for a
less pessimistic analysis. Finally, through a comprehensive experimental
evaluation, we compare the newly proposed approach against the existing one,
and also observe how the trends change with different traffic parameters.
"
973,On Pollaczek-Khinchine Formula for Peer-to-Peer Networks,"  The performance analysis of peer-to-peer (P2P) networks calls for a new kind
of queueing model, in which jobs and service stations arrive randomly. Except
in some simple special cases, in general, the queueing model with varying
service rate is mathematically intractable. Motivated by the P-K formula for
M/G/1 queue, we developed a limiting analysis approach based on the connection
between the fluctuation of service rate and the mean queue length. Considering
the two extreme service rates, we proved the conjecture on the lower bound and
upper bound of mean queue length previously postulated. Furthermore, an
approximate P-K formula to estimate the mean queue length is derived from the
convex combination of these two bounds and the conditional mean queue length
under the overload condition. We confirmed the accuracy of our approximation by
extensive simulation studies with different system parameters. We also verified
that all limiting cases of the system behavior are consistent with the
predictions of our formula.
"
974,User Performance in Small Cells Networks with Inter-Cell Mobility,"  We analyze the impact of intra-cell mobility on user performance in dense
networks such as that enabled by LTE-A and 5G. To this end, we consider a
homogeneous network of small cells and first show how to reduce the evaluation
of user performance to the case of a single representative cell. We then
propose simple analytical models that capture mobility through the distribution
of the residual sojourn time of mobile users in the cell. An approximate model,
based on Quasi-Stationary (QS) assumptions, is developed in order to speed up
computation in the Markovian framework. We use these models to derive the
average throughput of both mobile and static users, along with the probability
of handover for mobile users. Numerical evaluation and simulation results are
provided to assess the accuracy of the proposed models. We show, in particular,
that both classes of users benefit from a throughput gain induced by the
""opportunistic"" displacement of mobile users among cells.
"
975,"Outage Probability and Capacity for Two-Tier Femtocell Networks by
  Approximating Ratio of Rayleigh and Log Normal Random Variables","  This paper presents the derivation for per-tier outage probability of a
randomly deployed femtocell network over an existing macrocell network. The
channel characteristics of macro user and femto user are addressed by
considering different propagation modeling for outdoor and indoor links.
Location based outage probability analysis and capacity of the system with
outage constraints are used to analyze the system performance. To obtain the
simplified expressions, approximations of ratios of Rayleigh random variables
(RVs), Rayleigh to log normal RVs and their weighted summations, are derived
with the verifications using simulations.
"
976,Breaking Through the Full-Duplex Wi-Fi Capacity Gain,"  In this work we identify a seminal design guideline that prevents current
Full-Duplex (FD) MAC protocols to scale the FD capacity gain (i.e. 2x the
half-duplex throughput) in single-cell Wi-Fi networks. Under such guideline
(referred to as 1:1), a MAC protocol attempts to initiate up to two
simultaneous transmissions in the FD bandwidth. Since in single-cell Wi-Fi
networks MAC performance is bounded by the PHY layer capacity, this implies
gains strictly less than 2x over half-duplex at the MAC layer. To face this
limitation, we argue for the 1:N design guideline. Under 1:N, FD MAC protocols
'see' the FD bandwidth through N>1 orthogonal narrow-channel PHY layers. Based
on theoretical results and software defined radio experiments, we show the 1:N
design can leverage the Wi-Fi capacity gain more than 2x at and below the MAC
layer. This translates the denser modulation scheme incurred by channel
narrowing and the increase in the spatial reuse offer enabled by channel
orthogonality. With these results, we believe our design guideline can inspire
a new generation of Wi-Fi MAC protocols that fully embody and scale the FD
capacity gain.
"
977,OWL: a Reliable Online Watcher for LTE Control Channel Measurements,"  Reliable network measurements are a fundamental component of networking
research as they enable network analysis, system debugging, performance
evaluation and optimization. In particular, decoding the LTE control channel
would give access to the full base station traffic at a 1 ms granularity, thus
allowing for traffic profiling and accurate measurements. Although a few
open-source implementations of LTE are available, they do not provide tools to
reliably decoding the LTE control channel and, thus, accessing the scheduling
information. In this paper, we present OWL, an Online Watcher for LTE that is
able to decode all the resource blocks in more than 99% of the system frames,
significantly outperforming existing non-commercial prior decoders. Compared to
previous attempts, OWL grounds the decoding procedure on information obtained
from the LTE random access mechanism. This makes it possible to run our
software on inexpensive hardware coupled with almost any software defined radio
capable of sampling the LTE signal with sufficient accuracy.
"
978,DINAMITE: A modern approach to memory performance profiling,"  Diagnosing and fixing performance problems on multicore machines with deep
memory hierarchies is extremely challenging. Certain problems are best
addressed when we can analyze the entire trace of program execution, e.g.,
every memory access. Unfortunately such detailed execution logs are very large
and cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for
Dynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is
a collection of tools for end-to-end performance analysis: from the LLVM
compiler pass that instruments the program to plug-and-play tools that use a
modern data analytics engine Spark Streaming for trace introspection. Using
DINAMITE we found opportunities to improve data layout in several applications
that resulted in 15-20% performance improvements and found a shared-variable
bottleneck in a popular key-value store, whose elimination improved performance
by 20x.
"
979,"A partitioned shift-without-invert algorithm to improve parallel
  eigensolution efficiency in real-space electronic transport","  We present an eigenspectrum partitioning scheme without inversion for the
recently described real-space electronic transport code, TRANSEC. The primary
advantage of TRANSEC is its highly parallel algorithm, which enables studying
conductance in large systems. The present scheme adds a new source of
parallelization, significantly enhancing TRANSEC's parallel scalability,
especially for systems with many electrons. In principle, partitioning could
enable super-linear parallel speedup, as we demonstrate in calculations within
TRANSEC. In practical cases, we report better than five-fold improvement in CPU
time and similar improvements in wall time, compared to previously-published
large calculations. Importantly, the suggested scheme is relatively simple to
implement. It can be useful for general large Hermitian or weakly non-Hermitian
eigenvalue problems, whenever relatively accurate inversion via direct or
iterative linear solvers is impractical.
"
980,CG-OoO: Energy-Efficient Coarse-Grain Out-of-Order Execution,"  We introduce the Coarse-Grain Out-of-Order (CG- OoO) general purpose
processor designed to achieve close to In-Order processor energy while
maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance
proportional general purpose architecture that scales according to the program
load. Block-level code processing is at the heart of the this architecture;
CG-OoO speculates, fetches, schedules, and commits code at block-level
granularity. It eliminates unnecessary accesses to energy consuming tables, and
turns large tables into smaller and distributed tables that are cheaper to
access. CG-OoO leverages compiler-level code optimizations to deliver efficient
static code, and exploits dynamic instruction-level parallelism and block-level
parallelism. CG-OoO introduces Skipahead issue, a complexity effective, limited
out-of-order instruction scheduling model. Through the energy efficiency
techniques applied to the compiler and processor pipeline stages, CG-OoO closes
64% of the average energy gap between the In-Order and Out-of-Order baseline
processors at the performance of the OoO baseline. This makes CG-OoO 1.9x more
efficient than the OoO on the energy-delay product inverse metric.
"
981,Analyzing Distributed Join-Idle-Queue: A Fluid Limit Approach,"  In the context of load balancing, Lu et al. introduced the distributed
Join-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a
cluster of parallel servers. Each dispatcher maintains a queue of idle servers;
when a job arrives to a dispatcher, it sends it to a server on its queue, or to
a random server if the queue is empty. In turn, when a server has no jobs, it
requests to be placed on the idle queue of a randomly chosen dispatcher.
  Although this algorithm was shown to be quite effective, the original
asymptotic analysis makes simplifying assumptions that become increasingly
inaccurate as the system load increases. Further, the analysis does not
naturally generalize to interesting variations, such as having a server request
to be placed on the idle queue of a dispatcher before it has completed all
jobs, which can be beneficial under high loads.
  We provide a new asymptotic analysis of Join-Idle-Queue systems based on mean
field fluid limit methods, deriving families of differential equations that
describe these systems. Our analysis avoids previous simplifying assumptions,
is empirically more accurate, and generalizes naturally to the variation
described above, as well as other simple variations. Our theoretical and
empirical analyses shed further light on the performance of Join-Idle-Queue,
including potential performance pitfalls under high load.
"
982,"How is a data-driven approach better than random choice in label space
  division for multi-label classification?","  We propose using five data-driven community detection approaches from social
networks to partition the label space for the task of multi-label
classification as an alternative to random partitioning into equal subsets as
performed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector,
infomap, walktrap and label propagation algorithms. We construct a label
co-occurence graph (both weighted an unweighted versions) based on training
data and perform community detection to partition the label set. We include
Binary Relevance and Label Powerset classification methods for comparison. We
use gini-index based Decision Trees as the base classifier. We compare educated
approaches to label space divisions against random baselines on 12 benchmark
data sets over five evaluation measures. We show that in almost all cases seven
educated guess approaches are more likely to outperform RAkELd than otherwise
in all measures, but Hamming Loss. We show that fastgreedy and walktrap
community detection methods on weighted label co-occurence graphs are 85-92%
more likely to yield better F1 scores than random partitioning. Infomap on the
unweighted label co-occurence graphs is on average 90% of the times better than
random paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard
similarity. Weighted fastgreedy is better on average than RAkELd when it comes
to Hamming Loss.
"
983,"A figure of merit for describing the performance of scaling of
  parallelization","  With the spread of multi- and many-core processors more and more typical task
is to re-implement some source code written originally for a single processor
to run on more than one cores. Since it is a serious investment, it is
important to decide how much efforts pays off, and whether the resulting
implementation has as good performability as it could be. The Amdahl's law
provides some theoretical upper limits for the performance gain reachable
through parallelizing the code, but it needs the detailed architectural
knowledge of the program code, does not consider the housekeeping activity
needed for parallelization and cannot tell how the actual stage of
parallelization implementation performs. The present paper suggests a
quantitative measure for that goal. This figure of merit is derived
experimentally, from measured running time, and number of threads/cores. It can
be used to quantify the used parallelization technology, the connection between
the computing units, the acceleration technology under the given conditions,
communication method within SoC, or the performance of the software
team/compiler.
"
984,On Continuous-space Embedding of Discrete-parameter Queueing Systems,"  Motivated by the problem of discrete-parameter simulation optimization (DPSO)
of queueing systems, we consider the problem of embedding the discrete
parameter space into a continuous one so that descent-based continuous-space
methods could be directly applied for efficient optimization. We show that a
randomization of the simulation model itself can be used to achieve such an
embedding when the objective function is a long-run average measure. Unlike
spatial interpolation, the computational cost of this embedding is independent
of the number of parameters in the system, making the approach ideally suited
to high-dimensional problems. We describe in detail the application of this
technique to discrete-time queues for embedding queue capacities, number of
servers and server-delay parameters into continuous space and empirically show
that the technique can produce smooth interpolations of the objective function.
Through an optimization case-study of a queueing network with $10^7$ design
points, we demonstrate that existing continuous optimizers can be effectively
applied over such an embedding to find good solutions.
"
985,"Analysis of buffering effects on hard real-time priority-preemptive
  wormhole networks","  There are several approaches to analyse the worst-case response times of
sporadic packets transmitted over priority-preemptive wormhole networks. In
this paper, we provide an overview of the different approaches, discuss their
strengths and weaknesses, and propose an approach that captures all effects
considered by previous approaches while providing tight yet safe upper bounds
for packet response times. We specifically address the problems created by
buffering and backpressure in wormhole networks, which amplifies the problem of
indirect interference in a way that has not been considered by the early
analysis approaches. Didactic examples and large-scale experiments with
synthetically generated packet flow sets provide evidence of the strength of
the proposed approach.
"
986,Universal Routing in Multi-hop Radio Networks,"  In this article we introduce a new model to study stability in multi-hop
wireless networks in the framework of adversarial queueing. In such a model, a
routing protocol consists of three components: a transmission policy, a
scheduling policy to select the packet to transmit form a set of packets parked
at a node, and a hearing control mechanism to coordinate transmissions with
scheduling. For such a setting, we propose a definition of universal stability
that takes into account not only the scheduling policies (as in the standard
wireline adversarial model), but also the transmission policies. First, we show
that any scheduling policy that is unstable in the classical wireline
adversarial model remains unstable in the multi-hop radio network model, even
in scenarios free of inter- ferences. Then, we show that both SIS and LIS (two
well-known universally stable scheduling policies in the wireline adversarial
model) remain stable in the multi-hop radio network model, provided a proactive
hearing control is used. In contrast, such scheduling policies turn out to be
unstable when using a reactive hearing control. However, the scheduling policy
LIS can be enforced to be universally stable provided ties are resolved in a
permanent manner. Such a situation doesn't hold in the case of SIS, which
remains unstable regardless of how ties are resolved. Furthermore, for some
transmission policies which we call regular, we also show that all scheduling
policies that are universally stable when using proactive hearing control
(which include SIS and LIS) remain universally stable when using reactive
hearing control.
"
987,"A Unified Framework for Analyzing Closed Queueing Networks in Bike
  Sharing Systems","  During the last decade bike sharing systems have emerged as a public
transport mode in urban short trips in more than 500 major cities around the
world. For the mobility service mode, many challenges from its operations are
not well addressed yet, for example, how to develop the bike sharing systems to
be able to effectively satisfy the fluctuating demands both for bikes and for
vacant lockers. To this end, it is a key to give performance analysis of the
bike sharing systems. This paper first describes a large-scale bike sharing
system. Then the bike sharing system is abstracted as a closed queueing network
with multi-class customers, where the virtual customers and the virtual nodes
are set up, and the service rates as well as the relative arrival rates are
established. Finally, this paper gives a product-form solution to the steady
state joint probabilities of queue lengths, and gives performance analysis of
the bike sharing system. Therefore, this paper provides a unified framework for
analyzing closed queueing networks in the study of bike sharing systems. We
hope the methodology and results of this paper can be applicable in the study
of more general bike sharing systems.
"
988,"A Distributed Algorithm for Training Augmented Complex Adaptive IIR
  Filters","  In this paper we consider the problem of decentralized (distributed) adaptive
learning, where the aim of the network is to train the coefficients of a widely
linear autoregressive moving average (ARMA) model by measurements collected by
the nodes. Such a problem arises in many sensor network-based applications such
as target tracking, fast rerouting, data reduction and data aggregation. We
assume that each node of the network uses the augmented complex adaptive
infinite impulse response (ACAIIR) filter as the learning rule, and nodes
interact with each other under an incremental mode of cooperation. Since the
proposed algorithm (incremental augmented complex IIR (IACA-IIR) algorithm)
relies on the augmented complex statistics, it can be used to model both types
of complex-valued signals (proper and improper signals). To evaluate the
performance of the proposed algorithm, we use both synthetic and real-world
complex signals in our simulations. The results exhibit superior performance of
the proposed algorithm over the non-cooperative ACAIIR algorithm.
"
989,Program Execution on Reconfigurable Multicore Architectures,"  Based on the two observations that diverse applications perform better on
different multicore architectures, and that different phases of an application
may have vastly different resource requirements, Pal et al. proposed a novel
reconfigurable hardware approach for executing multithreaded programs. Instead
of mapping a concurrent program to a fixed architecture, the architecture
adaptively reconfigures itself to meet the application's concurrency and
communication requirements, yielding significant improvements in performance.
Based on our earlier abstract operational framework for multicore execution
with hierarchical memory structures, we describe execution of multithreaded
programs on reconfigurable architectures that support a variety of clustered
configurations. Such reconfiguration may not preserve the semantics of programs
due to the possible introduction of race conditions arising from concurrent
accesses to shared memory by threads running on the different cores. We present
an intuitive partial ordering notion on the cluster configurations, and show
that the semantics of multithreaded programs is always preserved for
reconfigurations ""upward"" in that ordering, whereas semantics preservation for
arbitrary reconfigurations can be guaranteed for well-synchronised programs. We
further show that a simple approximate notion of efficiency of execution on the
different configurations can be obtained using the notion of amortised
bisimulations, and extend it to dynamic reconfiguration.
"
990,A New Parallel Algorithm for Two-Pass Connected Component Labeling,"  Connected Component Labeling (CCL) is an important step in pattern
recognition and image processing. It assigns labels to the pixels such that
adjacent pixels sharing the same features are assigned the same label.
Typically, CCL requires several passes over the data. We focus on two-pass
technique where each pixel is given a provisional label in the first pass
whereas an actual label is assigned in the second pass.
  We present a scalable parallel two-pass CCL algorithm, called PAREMSP, which
employs a scan strategy and the best union-find technique called REMSP, which
uses REM's algorithm for storing label equivalence information of pixels in a
2-D image. In the first pass, we divide the image among threads and each thread
runs the scan phase along with REMSP simultaneously. In the second phase, we
assign the final labels to the pixels. As REMSP is easily parallelizable, we
use the parallel version of REMSP for merging the pixels on the boundary. Our
experiments show the scalability of PAREMSP achieving speedups up to $20.1$
using $24$ cores on shared memory architecture using OpenMP for an image of
size $465.20$ MB. We find that our proposed parallel algorithm achieves linear
scaling for a large resolution fixed problem size as the number of processing
elements are increased. Additionally, the parallel algorithm does not make use
of any hardware specific routines, and thus is highly portable.
"
991,A Study of Energy and Locality Effects using Space-filling Curves,"  The cost of energy is becoming an increasingly important driver for the
operating cost of HPC systems, adding yet another facet to the challenge of
producing efficient code. In this paper, we investigate the energy implications
of trading computation for locality using Hilbert and Morton space-filling
curves with dense matrix-matrix multiplication. The advantage of these curves
is that they exhibit an inherent tiling effect without requiring specific
architecture tuning. By accessing the matrices in the order determined by the
space-filling curves, we can trade computation for locality. The index
computation overhead of the Morton curve is found to be balanced against its
locality and energy efficiency, while the overhead of the Hilbert curve
outweighs its improvements on our test system.
"
992,Fault-Tolerant Adaptive Parallel and Distributed Simulation,"  Discrete Event Simulation is a widely used technique that is used to model
and analyze complex systems in many fields of science and engineering. The
increasingly large size of simulation models poses a serious computational
challenge, since the time needed to run a simulation can be prohibitively
large. For this reason, Parallel and Distributes Simulation techniques have
been proposed to take advantage of multiple execution units which are found in
multicore processors, cluster of workstations or HPC systems. The current
generation of HPC systems includes hundreds of thousands of computing nodes and
a vast amount of ancillary components. Despite improvements in manufacturing
processes, failures of some components are frequent, and the situation will get
worse as larger systems are built. In this paper we describe FT-GAIA, a
software-based fault-tolerant extension of the GAIA/ART\`IS parallel simulation
middleware. FT-GAIA transparently replicates simulation entities and
distributes them on multiple execution nodes. This allows the simulation to
tolerate crash-failures of computing nodes; furthermore, FT-GAIA offers some
protection against byzantine failures since synchronization messages are
replicated as well, so that the receiving entity can identify and discard
corrupted messages. We provide an experimental evaluation of FT-GAIA on a
running prototype. Results show that a high degree of fault tolerance can be
achieved, at the cost of a moderate increase in the computational load of the
execution units.
"
993,"Performance of spatial Multi-LRU caching under traffic with temporal
  locality","  In this work a novel family of decentralised caching policies for wireless
networks is introduced, referred to as spatial multi-LRU. These improve
cache-hit probability by exploiting multi-coverage. Two variations are
proposed, the multi-LRU-One and -All, which differ in the number of replicas
inserted in the covering edge-caches. The evaluation is done under spatial
traffic that exhibits temporal locality, with varying content catalogue and
dependent demands. The performance metric is hit probability and the policies
are compared to (1) the single-LRU and (2) an upper bound for all centralised
policies with periodic popularity updates. Numerical results show the multi-LRU
policies outperform both comparison policies. The reason is their passive
adaptability to popularity changes. Between the -One and -All variation, which
one is preferable strongly depends on the available storage space and on
traffic characteristics. The performance also depends on the popularity shape.
"
994,Modeling and Predicting DNS Server Load,"  The DNS relies on caching to ensure high scalability and good performance. In
optimizing caching, TTL adjustment provides a means of balancing between query
load and TTL-dependent performances such as data consistency, load balancing,
migration time, etc. To gain the desired balance, TTL adjustment depends on
predictions of query loads under alternative TTLs. This paper proposes a model
of DNS server load, which employs the uniform aggregate caching model to
simplify the complexity of modeling clients' requests and their caching. A
method of predicting DNS server load is developed using that model. The
prediction method is solely based on the unilateral measurements or
observations at authoritative servers. Without reliance on lots of multi-point
measurements nor distributed measuring facilities, the method is best suited
for DNS authoritative operators. The proposed model and prediction method are
validated through extensive simulations. Finally, global sensibility analysis
is conducted to evaluate the impacts of measurement uncertainties or errors on
the predictions.
"
995,Design of a high-performance GEMM-like Tensor-Tensor Multiplication,"  We present ""GEMM-like Tensor-Tensor multiplication"" (GETT), a novel approach
to tensor contractions that mirrors the design of a high-performance general
matrix-matrix multiplication (GEMM). The critical insight behind GETT is the
identification of three index sets, involved in the tensor contraction, which
enable us to systematically reduce an arbitrary tensor contraction to loops
around a highly tuned ""macro-kernel"". This macro-kernel operates on suitably
prepared (""packed"") sub-tensors that reside in a specified level of the cache
hierarchy. In contrast to previous approaches to tensor contractions, GETT
exhibits desirable features such as unit-stride memory accesses,
cache-awareness, as well as full vectorization, without requiring auxiliary
memory. To compare our technique with other modern tensor contractions, we
integrate GETT alongside the so called Transpose-Transpose-GEMM-Transpose and
Loops-over-GEMM approaches into an open source ""Tensor Contraction Code
Generator"" (TCCG). The performance results for a wide range of tensor
contractions suggest that GETT has the potential of becoming the method of
choice: While GETT exhibits excellent performance across the board, its
effectiveness for bandwidth-bound tensor contractions is especially impressive,
outperforming existing approaches by up to $12.4\times$. More precisely, GETT
achieves speedups of up to $1.41\times$ over an equivalent-sized GEMM for
bandwidth-bound tensor contractions while attaining up to $91.3\%$ of peak
floating-point performance for compute-bound tensor contractions.
"
996,High-Performance Tensor Contraction without Transposition,"  Tensor computations--in particular tensor contraction (TC)--are important
kernels in many scientific computing applications. Due to the fundamental
similarity of TC to matrix multiplication (MM) and to the availability of
optimized implementations such as the BLAS, tensor operations have
traditionally been implemented in terms of BLAS operations, incurring both a
performance and a storage overhead. Instead, we implement TC using the flexible
BLIS framework, which allows for transposition (reshaping) of the tensor to be
fused with internal partitioning and packing operations, requiring no explicit
transposition operations or additional workspace. This implementation, TBLIS,
achieves performance approaching that of MM, and in some cases considerably
higher than that of traditional TC. Our implementation supports multithreading
using an approach identical to that used for MM in BLIS, with similar
performance characteristics. The complexity of managing tensor-to-matrix
transformations is also handled automatically in our approach, greatly
simplifying its use in scientific applications.
"
997,Efficient Timeout Synthesis in Fixed-Delay CTMC Using Policy Iteration,"  We consider the fixed-delay synthesis problem for continuous-time Markov
chains extended with fixed-delay transitions (fdCTMC). The goal is to
synthesize concrete values of the fixed-delays (timeouts) that minimize the
expected total cost incurred before reaching a given set of target states. The
same problem has been considered and solved in previous works by computing an
optimal policy in a certain discrete-time Markov decision process (MDP) with a
huge number of actions that correspond to suitably discretized values of the
timeouts.
  In this paper, we design a symbolic fixed-delay synthesis algorithm which
avoids the explicit construction of large action spaces. Instead, the algorithm
computes a small sets of ""promising"" candidate actions on demand. The candidate
actions are selected by minimizing a certain objective function by computing
its symbolic derivative and extracting a univariate polynomial whose roots are
precisely the points where the derivative takes zero value. Since roots of high
degree univariate polynomials can be isolated very efficiently using modern
mathematical software, we achieve not only drastic memory savings but also
speedup by three orders of magnitude compared to the previous methods.
"
998,Stochastic Modeling of Hybrid Cache Systems,"  In recent years, there is an increasing demand of big memory systems so to
perform large scale data analytics. Since DRAM memories are expensive, some
researchers are suggesting to use other memory systems such as non-volatile
memory (NVM) technology to build large-memory computing systems. However,
whether the NVM technology can be a viable alternative (either economically and
technically) to DRAM remains an open question. To answer this question, it is
important to consider how to design a memory system from a ""system
perspective"", that is, incorporating different performance characteristics and
price ratios from hybrid memory devices.
  This paper presents an analytical model of a ""hybrid page cache system"" so to
understand the diverse design space and performance impact of a hybrid cache
system. We consider (1) various architectural choices, (2) design strategies,
and (3) configuration of different memory devices. Using this model, we provide
guidelines on how to design hybrid page cache to reach a good trade-off between
high system throughput (in I/O per sec or IOPS) and fast cache reactivity which
is defined by the time to fill the cache. We also show how one can configure
the DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an
example for NVM and conduct numerical analysis. Our analysis indicates that
incorporating PCM in a page cache system significantly improves the system
performance, and it also shows larger benefit to allocate more PCM in page
cache in some cases. Besides, for the common setting of performance-price ratio
of PCM, ""flat architecture"" offers as a better choice, but ""layered
architecture"" outperforms if PCM write performance can be significantly
improved in the future.
"
999,TTC: A Tensor Transposition Compiler for Multiple Architectures,"  We consider the problem of transposing tensors of arbitrary dimension and
describe TTC, an open source domain-specific parallel compiler. TTC generates
optimized parallel C++/CUDA C code that achieves a significant fraction of the
system's peak memory bandwidth. TTC exhibits high performance across multiple
architectures, including modern AVX-based systems (e.g.,~Intel Haswell, AMD
Steamroller), Intel's Knights Corner as well as different CUDA-based GPUs such
as NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a
meaningful baseline implementation generated by external C++ compilers; the
results suggest that a domain-specific compiler can outperform its general
purpose counterpart significantly: For instance, comparing with Intel's latest
C++ compiler on the Haswell and Knights Corner architecture, TTC yields
speedups of up to $8\times$ and $32\times$, respectively. We also showcase
TTC's support for multiple leading dimensions, making it a suitable candidate
for the generation of performance-critical packing functions that are at the
core of the ubiquitous BLAS 3 routines.
"
1000,"Novel Performance Analysis of Network Coded Communications in
  Single-Relay Networks","  In this paper, we analyze the performance of a single-relay network in which
the reliability is provided by means of Random Linear Network Coding (RLNC). We
consider a scenario when both source and relay nodes can encode packets. Unlike
the traditional approach to relay networks, we introduce a passive relay mode,
in which the relay node simply retransmits collected packets in case it cannot
decode them. In contrast with the previous studies, we derive a novel
theoretical framework for the performance characterization of the considered
relay network. We extend our analysis to a more general scenario, in which
coding coefficients are generated from non-binary fields. The theoretical
results are verified using simulation, for both binary and non-binary fields.
It is also shown that the passive relay mode significantly improves the
performance compared with the active-only case, offering an up to two-fold gain
in terms of the decoding probability. The proposed framework can be used as a
building block for the analysis of more complex network topologies.
"
1001,"Proceedings of the Workshop on FORmal methods for the quantitative
  Evaluation of Collective Adaptive SysTems","  Collective Adaptive Systems (CAS) consist of a large number of spatially
distributed heterogeneous entities with decentralised control and varying
degrees of complex autonomous behaviour that may be competing for shared
resources even when collaborating to reach common goals. It is important to
carry out thorough quantitative modelling and analysis and verification of
their design to investigate all aspects of their behaviour before they are put
into operation. This requires combinations of formal methods and applied
mathematics which moreover scale to large-scale CAS. The primary goal of
FORECAST is to raise awareness in the software engineering and formal methods
communities of the particularities of CAS and the design and control problems
which they bring.
"
1002,"On Formal Methods for Collective Adaptive System Engineering. {Scalable
  Approximated, Spatial} Analysis Techniques. Extended Abstract","  In this extended abstract a view on the role of Formal Methods in System
Engineering is briefly presented. Then two examples of useful analysis
techniques based on solid mathematical theories are discussed as well as the
software tools which have been built for supporting such techniques. The first
technique is Scalable Approximated Population DTMC Model-checking. The second
one is Spatial Model-checking for Closure Spaces. Both techniques have been
developed in the context of the EU funded project QUANTICOL.
"
1003,Database-Backed Web Applications in the Wild: How Well Do They Work?,"  Most modern database-backed web applications are built upon Object Relational
Mapping (ORM) frameworks. While ORM frameworks ease application development by
abstracting persistent data as objects, such convenience often comes with a
performance cost. In this paper, we present CADO, a tool that analyzes the
application logic and its interaction with databases using the Ruby on Rails
ORM framework. CADO includes a static program analyzer, a profiler and a
synthetic data generator to extract and understand application's performance
characteristics. We used CADO to analyze the performance problems of 27
real-world open-source Rails applications, covering domains such as online
forums, e-commerce, project management, blogs, etc. Based on the results, we
uncovered a number of issues that lead to sub-optimal application performance,
ranging from issuing queries, how result sets are used, and physical design. We
suggest possible remedies for each issue, and highlight new research
opportunities that arise from them.
"
1004,"The Vectorization of the Tersoff Multi-Body Potential: An Exercise in
  Performance Portability","  Molecular dynamics simulations, an indispensable research tool in
computational chemistry and materials science, consume a significant portion of
the supercomputing cycles around the world. We focus on multi-body potentials
and aim at achieving performance portability. Compared with well-studied pair
potentials, multibody potentials deliver increased simulation accuracy but are
too complex for effective compiler optimization. Because of this, achieving
cross-platform performance remains an open question. By abstracting from target
architecture and computing precision, we develop a vectorization scheme
applicable to both CPUs and accelerators. We present results for the Tersoff
potential within the molecular dynamics code LAMMPS on several architectures,
demonstrating efficiency gains not only for computational kernels, but also for
large-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver
is between 3 and 5 times faster than the pure MPI reference.
"
1005,Challenges in Quantitative Abstractions for Collective Adaptive Systems,"  Like with most large-scale systems, the evaluation of quantitative properties
of collective adaptive systems is an important issue that crosscuts all its
development stages, from design (in the case of engineered systems) to runtime
monitoring and control. Unfortunately it is a difficult problem to tackle in
general, due to the typically high computational cost involved in the analysis.
This calls for the development of appropriate quantitative abstraction
techniques that preserve most of the system's dynamical behaviour using a more
compact representation. This paper focuses on models based on ordinary
differential equations and reviews recent results where abstraction is achieved
by aggregation of variables, reflecting on the shortcomings in the state of the
art and setting out challenges for future research.
"
1006,Designing a High Performance Parallel Personal Cluster,"  Today, many scientific and engineering areas require high performance
computing to perform computationally intensive experiments. For example, many
advances in transport phenomena, thermodynamics, material properties,
computational chemistry and physics are possible only because of the
availability of such large scale computing infrastructures. Yet many challenges
are still open. The cost of energy consumption, cooling, competition for
resources have been some of the reasons why the scientific and engineering
communities are turning their interests to the possibility of implementing
energy-efficient servers utilizing low-power CPUs for computing-intensive
tasks. In this paper we introduce a novel approach, which was recently
presented at Linux Conference Europe 2015, based on the Beowulf concept and
utilizing single board computers (SBC). We present a low-energy consumption
architecture capable to tackle heavily demanding scientific computational
problems. Additionally, our goal is to provide a low cost personal solution for
scientists and engineers. In order to evaluate the performance of the proposed
architecture we ran several standard benchmarking tests. Furthermore, we assess
the reliability of the machine in real life situations by performing two
benchmark tools involving practical TCAD for physicist and engineers in the
semiconductor industry.
"
1007,"Optimal Placement of Cores, Caches and Memory Controllers in Network
  On-Chip","  Parallel programming is emerging fast and intensive applications need more
resources, so there is a huge demand for on-chip multiprocessors. Accessing L1
caches beside the cores are the fastest after registers but the size of private
caches cannot increase because of design, cost and technology limits. Then
split I-cache and D-cache are used with shared LLC (last level cache). For a
unified shared LLC, bus interface is not scalable, and it seems that
distributed shared LLC (DSLLC) is a better choice. Most of papers assume a
distributed shared LLC beside each core in on-chip network. Many works assume
that DSLLCs are placed in all cores; however, we will show that this design
ignores the effect of traffic congestion in on-chip network. In fact, our work
focuses on optimal placement of cores, DSLLCs and even memory controllers to
minimize the expected latency based on traffic load in a mesh on-chip network
with fixed number of cores and total cache capacity. We try to do some
analytical modeling deriving intended cost function and then optimize the mean
delay of the on-chip network communication. This work is supposed to be
verified using some traffic patterns that are run on CSIM simulator.
"
1008,How to Emulate Web Traffic Using Standard Load Testing Tools,"  Conventional load-testing tools are based on a fifty-year old time-share
computer paradigm where a finite number of users submit requests and respond in
a synchronized fashion. Conversely, modern web traffic is essentially
asynchronous and driven by an unknown number of users. This difference presents
a conundrum for testing the performance of modern web applications. Even when
the difference is recognized, performance engineers often introduce
modifications to their test scripts based on folklore or hearsay published in
various Internet fora, much of which can lead to wrong results. We present a
coherent methodology, based on two fundamental principles, for emulating web
traffic using a standard load-test environment.
"
1009,Scientific Computing Using Consumer Video-Gaming Hardware Devices,"  Commodity video-gaming hardware (consoles, graphics cards, tablets, etc.)
performance has been advancing at a rapid pace owing to strong consumer demand
and stiff market competition. Gaming hardware devices are currently amongst the
most powerful and cost-effective computational technologies available in
quantity. In this article, we evaluate a sample of current generation
video-gaming hardware devices for scientific computing and compare their
performance with specialized supercomputing general purpose graphics processing
units (GPGPUs). We use the OpenCL SHOC benchmark suite, which is a measure of
the performance of compute hardware on various different scientific application
kernels, and also a popular public distributed computing application,
Einstein@Home in the field of gravitational physics for the purposes of this
evaluation.
"
1010,Evaluating the Strength of Genomic Privacy Metrics,"  The genome is a unique identifier for human individuals. The genome also
contains highly sensitive information, creating a high potential for misuse of
genomic data (for example, genetic discrimination). In this paper, I
investigated how genomic privacy can be measured in scenarios where an
adversary aims to infer a person's genomic markers by constructing probability
distributions on the values of genetic variations. I measured the strength of
privacy metrics by requiring that metrics are monotonic with increasing
adversary strength and uncovered serious problems with several existing metrics
currently used to measure genomic privacy. I provide suggestions on metric
selection, interpretation, and visualization, and illustrate the work flow
using a case study on Alzheimer's disease.
"
1011,Statistical Delay Bound for WirelessHART Networks,"  In this paper we provide a performance analysis framework for wireless
industrial networks by deriving a service curve and a bound on the delay
violation probability. For this purpose we use the (min,x) stochastic network
calculus as well as a recently presented recursive formula for an end-to-end
delay bound of wireless heterogeneous networks. The derived results are mapped
to WirelessHART networks used in process automation and were validated via
simulations. In addition to WirelessHART, our results can be applied to any
wireless network whose physical layer conforms the IEEE 802.15.4 standard,
while its MAC protocol incorporates TDMA and channel hopping, like e.g.
ISA100.11a or TSCH-based networks. The provided delay analysis is especially
useful during the network design phase, offering further research potential
towards optimal routing and power management in QoS-constrained wireless
industrial networks.
"
1012,"AUGURY: A time-series based application for the analysis and forecasting
  of system and network performance metrics","  This paper presents AUGURY, an application for the analysis of monitoring
data from computers, servers or cloud infrastructures. The analysis is based on
the extraction of patterns and trends from historical data, using elements of
time-series analysis. The purpose of AUGURY is to aid a server administrator by
forecasting the behaviour and resource usage of specific applications and in
presenting a status report in a concise manner. AUGURY provides tools for
identifying network traffic congestion and peak usage times, and for making
memory usage projections. The application data processing specialises in two
tasks: the parametrisation of the memory usage of individual applications and
the extraction of the seasonal component from network traffic data. AUGURY uses
a different underlying assumption for each of these two tasks. With respect to
the memory usage, a limited number of single-valued parameters are assumed to
be sufficient to parameterize any application being hosted on the server.
Regarding the network traffic data, long-term patterns, such as hourly or daily
exist and are being induced by work-time schedules and automatised
administrative jobs. In this paper, the implementation of each of the two tasks
is presented, tested using locally-generated data, and applied to data from
weather forecasting applications hosted on a web server. This data is used to
demonstrate the insight that AUGURY can add to the monitoring of server and
cloud infrastructures.
"
1013,"Overlay Secondary Spectrum Sharing with Independent Re-attempts in
  Cognitive Radios","  Opportunistic spectrum access (OSA) is a promising reform paradigm envisioned
to address the issue of spectrum scarcity in cognitive radio networks (CRNs).
While current models consider various aspects of the OSA scheme, the impact of
retrial phenomenon in multi-channel CRNs has not yet been analyzed. In this
work, we present a continuous-time Markov chain (CTMC) model in which the
blocked/preempted secondary users (SUs) enter a finite retrial group (or orbit)
and re-attempt independently for service in an exponentially distributed random
manner. Taking into account the inherent retrial tendency of SUs, we
numerically assess the performance of the proposed scheme in terms of dropping
probability and throughput of SUs.
"
1014,Delay and Backlog Analysis for 60 GHz Wireless Networks,"  To meet the ever-increasing demands on higher throughput and better network
delay performance, 60 GHZ networking is proposed as a promising solution for
the next generation of wireless communications. To successfully deploy such
networks, its important to understand their performance first. However, due to
the unique fading characteristic of the 60 GHz channel, the characterization of
the corresponding service process, offered by the channel, using the
conventional methodologies may not be tractable. In this work, we provide an
alternative approach to derive a closed-form expression that characterizes the
cumulative service process of the 60 GHz channel in terms of the moment
generating function (MGF) of its instantaneous channel capacity. We then use
this expression to derive probabilistic upper bounds on the backlog and delay
that are experienced by a flow traversing this network, using results from the
MGF-based network calculus. The computed bounds are validated using simulation.
We provide numerical results for different networking scenarios and for
different traffic and channel parameters and we show that the 60 GHz wireless
network is capable of satisfying stringent quality-of-Service (QoS)
requirements, in terms of network delay and reliability. With this analysis
approach at hand, a larger scale 60 GHz network design and optimization is
possible.
"
1015,"A survey of sparse matrix-vector multiplication performance on large
  matrices","  We contribute a third-party survey of sparse matrix-vector (SpMV) product
performance on industrial-strength, large matrices using: (1) The SpMV
implementations in Intel MKL, the Trilinos project (Tpetra subpackage), the
CUSPARSE library, and the CUSP library, each running on modern architectures.
(2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package).
(3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software
package).
"
1016,"Rate Reduction for State-labelled Markov Chains with Upper Time-bounded
  CSL Requirements","  This paper presents algorithms for identifying and reducing a dedicated set
of controllable transition rates of a state-labelled continuous-time Markov
chain model. The purpose of the reduction is to make states to satisfy a given
requirement, specified as a CSL upper time-bounded Until formula. We
distinguish two different cases, depending on the type of probability bound. A
natural partitioning of the state space allows us to develop possible
solutions, leading to simple algorithms for both cases.
"
1017,"Improving Zero-Day Malware Testing Methodology Using Statistically
  Significant Time-Lagged Test Samples","  Enterprise networks are in constant danger of being breached by
cyber-attackers, but making the decision about what security tools to deploy to
mitigate this risk requires carefully designed evaluation of security products.
One of the most important metrics for a protection product is how well it is
able to stop malware, specifically on ""zero""-day malware that has not been seen
by the security community before. However, evaluating zero-day performance is
difficult, because of larger number of previously unseen samples that are
needed to properly measure the true and false positive rate, and the challenges
involved in accurately labeling these samples. This paper addresses these
issues from a statistical and practical perspective. Our contributions include
first showing that the number of benign files needed for proper evaluation is
on the order of a millions, and the number of malware samples needed is on the
order of tens of thousands. We then propose and justify a time-delay method for
easily collecting large number of previously unseen, but labeled, samples. This
enables cheap and accurate evaluation of zero-day true and false positive
rates. Finally, we propose a more fine-grain labeling of the malware/benignware
in order to better model the heterogeneous distribution of files on various
networks.
"
1018,Infinite Unlimited Churn,"  We study unlimited infinite churn in peer-to-peer overlay networks. Under
this churn, arbitrary many peers may concurrently request to join or leave the
overlay network; moreover these requests may never stop coming. We prove that
unlimited adversarial churn, where processes may just exit the overlay network,
is unsolvable. We focus on cooperative churn where exiting processes
participate in the churn handling algorithm. We define the problem of unlimited
infinite churn in this setting. We distinguish the fair version of the problem,
where each request is eventually satisfied, from the unfair version that just
guarantees progress. We focus on local solutions to the problem, and prove that
a local solution to the Fair Infinite Unlimited Churn is impossible. We then
present and prove correct an algorithm UIUC that solves the Unfair Infinite
Unlimited Churn Problem for a linearized peer-to-peer overlay network. We
extend this solution to skip lists and skip graphs.
"
1019,An Enhanced Buffer Management Scheme for Multimedia Traffic in HSDPA,"  High Speed Downlink Packet Access (HSDPA) was introduced to UMTS radio access
segment to provide higher capacity for new packet switched services. As a
result, packet switched sessions with multiple diverse traffic flows such as
concurrent voice and data, or video and data being transmitted to the same user
are a likely commonplace cellular packet data scenario. In HSDPA, Radio Access
Network (RAN) buffer management schemes are essential to support the end-to-end
QoS of such sessions. Hence in this paper we present the end-to-end performance
study of a proposed RAN buffer management scheme for multi-flow sessions via
dynamic system-level HSDPA simulations. The scheme is an enhancement of a
Time-Space Priority (TSP)queuing strategy applied to the Node B MAC-hs buffer
allocated to an end user with concurrent real-time (RT) and non-real-time (NRT)
flows during a multi-flow session. The experimental multiflow scenario is a
packet voice call with concurrent TCP-based file download to the same user.
Results show that with the proposed enhancements to the TSP-based RAN buffer
management,end-to-end QoS performance gains accrue to the NRT flow without
compromising RT flow QoS of the same end user session.
"
1020,"Analysis of M2/M2/1/R,N Queuing Model for Multimedia over 3.5G Wireless
  Network Downlink","  Analysis of an M2/M2/1/R, N queuing model for the multimedia transmission
over HSDPA/3.5G downlink is presented. The queue models the downlink buffer
with source multimedia traffic streams comprising two classes of flows:
realtime and non real-time. Time priority is accorded to the real-time flows
while the non real-time flows are given buffer space priority. An analytic
evaluation of the impact of varying the buffer partition threshold on the QoS
performance of both classes of customers is undertaken. The results are
validated with a discrete event simulation model developed in C language.
Finally, a cost function for the joint optimization of the traffic QoS
parameters is derived.
"
1021,"End-to-End QoS Improvement of HSDPA End-User Multi-flow Traffic Using
  RAN Buffer Management","  High Speed Downlink Packet Access (HSDPA) was introduced to UMTS radio access
segment to provide higher capacity for new packet switched services. As a
result, packet switched sessions with multiple diverse traffic flows such as
concurrentvoice and data, or video and data being transmitted to the same user
are a likely commonplace cellular packet data scenario. In HSDPA, Radio Access
Network (RAN) buffer management schemes are essential to support the end-to-end
QoS of such sessions. Hence in this paper we present the end-to-end performance
study of a proposed RAN buffer management scheme for multi-flow sessions via
dynamic system-level HSDPA simulations. The scheme is an enhancement of a
Time-Space Priority (TSP)queuing strategy applied to the Node B MAC-hs buffer
allocated to an end user with concurrent real-time (RT) and non-real-time (NRT)
flows during a multi-flow session. The experimental multiflow scenario is a
packet voice call with concurrent TCP-based file download to the same user.
Results show that with the proposed enhancements to the TSP-based RAN buffer
management, end-to-end QoS performance gains accrue to the NRT flow without
compromising RT flow QoS of the same end user session.
"
1022,"Bound-Based Power Optimization for Multi-Hop Heterogeneous Wireless
  Industrial Networks Under Statistical Delay Constraints","  The noticeably increased deployment of wireless networks for battery-limited
industrial applications in recent years highlights the need for tractable
performance analysis methodologies as well as efficient QoS-aware transmit
power management schemes. In this work, we seek to combine several important
aspects of such networks, i.e., multi-hop connectivity, channel heterogeneity
and the queuing effect, in order to address these needs. We design
delay-bound-based algorithms for transmit power minimization and network
lifetime maximization of multi-hop heterogeneous wireless networks using our
previously developed stochastic network calculus approach for performance
analysis of a cascade of buffered wireless fading channels. Our analysis shows
an overall transmit power saving of up to 95% compared to a fixed power
allocation scheme when using a service model in terms of the Shannon capacity
limit. For a more realistic set-up, we evaluate the performance of the
suggested algorithm in a WirelessHART network, which is a widely used
communication standard for process automation and other industrial
applications. We find that link heterogeneity can significantly reduce network
lifetime when no efficient power management is applied. Moreover, we show,
using extensive simulation study, that the proposed bound-based power
allocation performs reasonably well compared to the real optimum, especially in
the case of WirelessHART networks.
"
1023,"LITMUS: An Open Extensible Framework for Benchmarking RDF Data
  Management Solutions","  Developments in the context of Open, Big, and Linked Data have led to an
enormous growth of structured data on the Web. To keep up with the pace of
efficient consumption and management of the data at this rate, many data
Management solutions have been developed for specific tasks and applications.
We present LITMUS, a framework for benchmarking data management solutions.
LITMUS goes beyond classical storage benchmarking frameworks by allowing for
analysing the performance of frameworks across query languages. In this
position paper we present the conceptual architecture of LITMUS as well as the
considerations that led to this architecture.
"
1024,Optimal routing in two-queue polling systems,"  We consider a polling system with two queues, exhaustive service, no
switch-over times and exponential service times. The waiting cost depends on
the position of the queue relative to the server: It costs a customer c per
time unit to wait in the busy queue (where the server is) and d per time unit
in the idle queue (where no server is). Customers arrive according to a Poisson
process. We study the control problem of how arrivals should be routed to the
two queues in order to minimize expected waiting costs and characterize
individually and socially optimal routing policies under three scenarios of
available information at decision epochs: no, partial and complete information.
In the complete information case, we develop a new iterative algorithm to
determine individually optimal policies, and show that such policies can be
described by a switching curve. We conjecture that a linear switching curve is
socially optimal, and prove that this policy is indeed optimal for the fluid
version of the two-queue polling system.
"
1025,Coz: Finding Code that Counts with Causal Profiling,"  Improving performance is a central concern for software developers. To locate
optimization opportunities, developers rely on software profilers. However,
these profilers only report where programs spent their time: optimizing that
code may have no impact on performance. Past profilers thus both waste
developer time and make it difficult for them to uncover significant
optimization opportunities.
  This paper introduces causal profiling. Unlike past profiling approaches,
causal profiling indicates exactly where programmers should focus their
optimization efforts, and quantifies their potential impact. Causal profiling
works by running performance experiments during program execution. Each
experiment calculates the impact of any potential optimization by virtually
speeding up code: inserting pauses that slow down all other code running
concurrently. The key insight is that this slowdown has the same relative
effect as running that line faster, thus ""virtually"" speeding it up.
  We present Coz, a causal profiler, which we evaluate on a range of
highly-tuned applications: Memcached, SQLite, and the PARSEC benchmark suite.
Coz identifies previously unknown optimization opportunities that are both
significant and targeted. Guided by Coz, we improve the performance of
Memcached by 9%, SQLite by 25%, and accelerate six PARSEC applications by as
much as 68%; in most cases, these optimizations involve modifying under 10
lines of code.
"
1026,"Performance prediction of finite-difference solvers for different
  computer architectures","  The life-cycle of a partial differential equation (PDE) solver is often
characterized by three development phases: the development of a stable
numerical discretization, development of a correct (verified) implementation,
and the optimization of the implementation for different computer
architectures. Often it is only after significant time and effort has been
invested that the performance bottlenecks of a PDE solver are fully understood,
and the precise details varies between different computer architectures. One
way to mitigate this issue is to establish a reliable performance model that
allows a numerical analyst to make reliable predictions of how well a numerical
method would perform on a given computer architecture, before embarking upon
potentially long and expensive implementation and optimization phases. The
availability of a reliable performance model also saves developer effort as it
both informs the developer on what kind of optimisations are beneficial, and
when the maximum expected performance has been reached and optimisation work
should stop. We show how discretization of a wave equation can be theoretically
studied to understand the performance limitations of the method on modern
computer architectures. We focus on the roofline model, now broadly used in the
high-performance computing community, which considers the achievable
performance in terms of the peak memory bandwidth and peak floating point
performance of a computer with respect to algorithmic choices. A first
principles analysis of operational intensity for key time-stepping
finite-difference algorithms is presented. With this information available at
the time of algorithm design, the expected performance on target computer
systems can be used as a driver for algorithm design.
"
1027,"A Non-stationary Service Curve Model for Estimation of Cellular Sleep
  Scheduling","  While steady-state solutions of backlog and delay have been derived for
essential wireless systems, the analysis of transient phases still poses
significant challenges. Considering the majority of short-lived and interactive
flows, transient startup effects, as caused by sleep scheduling in cellular
networks, have, however, a substantial impact on the performance. To facilitate
reasoning about the transient behavior of systems, this paper contributes a
notion of non-stationary service curves. Models of systems with sleep
scheduling are derived and transient backlogs and delays are analyzed. Further,
measurement methods that estimate the service of an unknown system from
observations of selected probe traffic are developed. Fundamental limitations
of existing measurement methods are explained by the non-convexity of the
transient service and further difficulties are shown to be due to the
super-additivity of network service processes. A novel two-phase probing
technique is devised that first determines the shape of a minimal probe and
subsequently obtains an accurate estimate of the unknown service. In a
comprehensive measurement campaign, the method is used to evaluate the service
of cellular networks with sleep scheduling (2G, 3G, and 4G), revealing
considerable transient backlog and delay overshoots that persist for long
relaxation times.
"
1028,Julia Implementation of the Dynamic Distributed Dimensional Data Model,"  Julia is a new language for writing data analysis programs that are easy to
implement and run at high performance. Similarly, the Dynamic Distributed
Dimensional Data Model (D4M) aims to clarify data analysis operations while
retaining strong performance. D4M accomplishes these goals through a
composable, unified data model on associative arrays. In this work, we present
an implementation of D4M in Julia and describe how it enables and facilitates
data analysis. Several experiments showcase scalable performance in our new
Julia version as compared to the original Matlab implementation.
"
1029,"Security and Performance Comparison of Different Secure Channel
  Protocols for Avionics Wireless Networks","  The notion of Integrated Modular Avionics (IMA) refers to inter-connected
pieces of avionics equipment supported by a wired technology, with stringent
reliability and safety requirements. If the inter-connecting wires are
physically secured so that a malicious user cannot access them directly, then
this enforces (at least partially) the security of the network. However,
substituting the wired network with a wireless network - which in this context
is referred to as an Avionics Wireless Network (AWN) - brings a number of new
challenges related to assurance, reliability, and security. The AWN thus has to
ensure that it provides at least the required security and safety levels
offered by the equivalent wired network. Providing a wired-equivalent security
for a communication channel requires the setting up of a strong, secure
(encrypted) channel between the entities that are connected to the AWN. In this
paper, we propose three approaches to establish such a secure channel based on
(i) pre-shared keys, (ii) trusted key distribution, and (iii) key-sharing
protocols. For each of these approaches, we present two representative protocol
variants. These protocols are then implemented as part of a demo AWN and they
are then compared based on performance measurements. Most importantly, we have
evaluated these protocols based on security and operational requirements that
we define in this paper for an AWN.
"
1030,"An Efficient, Secure and Trusted Channel Protocol for Avionics Wireless
  Networks","  Avionics networks rely on a set of stringent reliability and safety
requirements. In existing deployments, these networks are based on a wired
technology, which supports these requirements. Furthermore, this technology
simplifies the security management of the network since certain assumptions can
be safely made, including the inability of an attacker to access the network,
and the fact that it is almost impossible for an attacker to introduce a node
into the network. The proposal for Avionics Wireless Networks (AWNs), currently
under development by multiple aerospace working groups, promises a reduction in
the complexity of electrical wiring harness design and fabrication, a reduction
in the total weight of wires, increased customization possibilities, and the
capacity to monitor otherwise inaccessible moving or rotating aircraft parts
such as landing gear and some sections of the aircraft engines. While providing
these benefits, the AWN must ensure that it provides levels of safety that are
at minimum equivalent to those offered by the wired equivalent. In this paper,
we propose a secure and trusted channel protocol that satisfies the stated
security and operational requirements for an AWN protocol. There are three main
objectives for this protocol. First, the protocol has to provide the assurance
that all communicating entities can trust each other, and can trust their
internal (secure) software and hardware states. Second, the protocol has to
establish a fair key exchange between all communicating entities so as to
provide a secure channel. Finally, the third objective is to be efficient for
both the initial start-up of the network and when resuming a session after a
cold and/or warm restart of a node. The proposed protocol is implemented and
performance measurements are presented based on this implementation. In
addition, we formally verify our proposed protocol using CasperFDR.
"
1031,Robust benchmarking in noisy environments,"  We propose a benchmarking strategy that is robust in the presence of timer
error, OS jitter and other environmental fluctuations, and is insensitive to
the highly nonideal statistics produced by timing measurements. We construct a
model that explains how these strongly nonideal statistics can arise from
environmental fluctuations, and also justifies our proposed strategy. We
implement this strategy in the BenchmarkTools Julia package, where it is used
in production continuous integration (CI) pipelines for developing the Julia
language and its ecosystem.
"
1032,Delay Evaluation of OpenFlow Network Based on Queueing Model,"  As one of the most popular south-bound protocol of software-defined
networking(SDN), OpenFlow decouples the network control from forwarding
devices. It offers flexible and scalable functionality for networks. These
advantages may cause performance issues since there are performance penalties
in terms of packet processing speed. It is important to understand the
performance of OpenFlow switches and controllers for its deployments. In this
paper we model the packet processing time of OpenFlow switches and controllers.
We mainly analyze how the probability of packet-in messages impacts the
performance of switches and controllers. Our results show that there is a
performance penalty in OpenFlow networks. However, the penalty is not much when
probability of packet-in messages is low. This model can be used for a network
designer to approximate the performance of her deployments.
"
1033,Transient performance analysis of zero-attracting LMS,"  Zero-attracting least-mean-square (ZA-LMS) algorithm has been widely used for
online sparse system identification. It combines the LMS framework and
$\ell_1$-norm regularization to promote sparsity, and relies on subgradient
iterations. Despite the significant interest in ZA-LMS, few works analyzed its
transient behavior. The main difficulty lies in the nonlinearity of the update
rule. In this work, a detailed analysis in the mean and mean-square sense is
carried out in order to examine the behavior of the algorithm. Simulation
results illustrate the accuracy of the model and highlight its performance
through comparisons with an existing model.
"
1034,"Effect of Human Learning on the Transient Performance of Cloud-based
  Tiered Applications","  Cloud based tiered applications are increasingly becoming popular, be it on
phones or on desktops. End users of these applications range from novice to
expert depending on how experienced they are in using them. With repeated usage
(practice) of an application, a user's think time gradually decreases, known as
learning phenomenon. In contrast to the popular notion of constant mean think
time of users across all practice sessions, decrease in mean think time over
practice sessions does occur due to learning. This decrease gives rise to a
different system workload thereby affecting the application's short-term
performance. However, such impact of learning on performance has never been
accounted for. In this work we propose a model that accounts for human learning
behavior in analyzing the transient (short-term) performance of a 3-tier cloud
based application. Our approach is based on a closed queueing network model. We
solve the model using discrete event simulation. In addition to the overall
mean System Response Time (SRT), our model solution also generates the mean
SRTs for various types (novice, intermediate, expert) of requests submitted by
users at various levels of their expertise. We demonstrate that our model can
be used to evaluate various what-if scenarios to decide the number of VMs we
need for each tier-a VM configuration-that would meet the response time SLA.
The results show that the lack of accountability of learning may lead to a
selection of an inappropriate VM configuration. The results further show that
the mean SRTs for various types of requests are better measures to consider in
VM allocation process in comparison to the overall mean SRT.
"
1035,Devito: automated fast finite difference computation,"  Domain specific languages have successfully been used in a variety of fields
to cleanly express scientific problems as well as to simplify implementation
and performance opti- mization on different computer architectures. Although a
large number of stencil languages are available, finite differ- ence domain
specific languages have proved challenging to design because most practical use
cases require additional features that fall outside the finite difference
abstraction. Inspired by the complexity of real-world seismic imaging problems,
we introduce Devito, a domain specific language in which high level equations
are expressed using symbolic expressions from the SymPy package. Complex
equations are automatically manipulated, optimized, and translated into highly
optimized C code that aims to perform compa- rably or better than hand-tuned
code. All this is transpar- ent to users, who only see concise symbolic
mathematical expressions.
"
1036,"On Performance Modeling for MANETs under General Limited Buffer
  Constraint","  Understanding the real achievable performance of mobile ad hoc networks
(MANETs) under practical network constraints is of great importance for their
applications in future highly heterogeneous wireless network environments. This
paper explores, for the first time, the performance modeling for MANETs under a
general limited buffer constraint, where each network node maintains a limited
source buffer of size $B_s$ to store its locally generated packets and also a
limited shared relay buffer of size $B_r$ to store relay packets for other
nodes. Based on the Queuing theory and birth-death chain theory, we first
develop a general theoretical framework to fully depict the source/relay buffer
occupancy process in such a MANET, which applies to any distributed MAC
protocol and any mobility model that leads to the uniform distribution of
nodes' locations in steady state. With the help of this framework, we then
derive the exact expressions of several key network performance metrics,
including achievable throughput, throughput capacity, and expected end-to-end
delay. We further conduct case studies under two network scenarios and provide
the corresponding theoretical/simulation results to demonstrate the application
as well as the efficiency of our theoretical framework. Finally, we present
extensive numerical results to illustrate the impacts of buffer constraint on
the performance of a buffer-limited MANET.
"
1037,"Physical Layer Security-Aware Routing and Performance Tradeoffs in Ad
  Hoc Networks","  The application of physical layer security in ad hoc networks has attracted
considerable academic attention recently. However, the available studies mainly
focus on the single-hop and two-hop network scenarios, and the price in terms
of degradation of communication quality of service (QoS) caused by improving
security is largely uninvestigated. As a step to address these issues, this
paper explores the physical layer security-aware routing and performance
tradeoffs in a multi-hop ad hoc network. Specifically, for any given end-to-end
path we first derive its connection outage probability (COP) and secrecy outage
probability (SOP) in closed-form, which serve as the performance metrics of
communication QoS and transmission security, respectively. Based on the
closed-form expressions, we then study the security-QoS tradeoffs to minimize
COP (resp. SOP) conditioned on that SOP (resp. COP) is guaranteed. With the
help of analysis of a given path, we further propose the routing algorithms
which can achieve the optimal performance tradeoffs for any pair of source and
destination nodes in a distributed manner. Finally, simulation and numerical
results are presented to validate the efficiency of our theoretical analysis,
as well as to illustrate the security-QoS tradeoffs and the routing
performance.
"
1038,"An ECM-based energy-efficiency optimization approach for
  bandwidth-limited streaming kernels on recent Intel Xeon processors","  We investigate an approach that uses low-level analysis and the
execution-cache-memory (ECM) performance model in combination with tuning of
hardware parameters to lower energy requirements of memory-bound applications.
The ECM model is extended appropriately to deal with software optimizations
such as non-temporal stores. Using incremental steps and the ECM model, we
analytically quantify the impact of various single-core optimizations and
pinpoint microarchitectural improvements that are relevant to energy
consumption. Using a 2D Jacobi solver as example that can serve as a blueprint
for other memory-bound applications, we evaluate our approach on the four most
recent Intel Xeon E5 processors (Sandy Bridge-EP, Ivy Bridge-EP, Haswell-EP,
and Broadwell-EP). We find that chip energy consumption can be reduced in the
range of 2.0-2.4$\times$ on the examined processors.
"
1039,Devito: Towards a generic Finite Difference DSL using Symbolic Python,"  Domain specific languages (DSL) have been used in a variety of fields to
express complex scientific problems in a concise manner and provide automated
performance optimization for a range of computational architectures. As such
DSLs provide a powerful mechanism to speed up scientific Python computation
that goes beyond traditional vectorization and pre-compilation approaches,
while allowing domain scientists to build applications within the comforts of
the Python software ecosystem. In this paper we present Devito, a new finite
difference DSL that provides optimized stencil computation from high-level
problem specifications based on symbolic Python expressions. We demonstrate
Devito's symbolic API and performance advantages over traditional Python
acceleration methods before highlighting its use in the scientific context of
seismic inversion problems.
"
1040,ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data,"  Causal inference from observational data is a subject of active research and
development in statistics and computer science. Many toolkits have been
developed for this purpose that depends on statistical software. However, these
toolkits do not scale to large datasets. In this paper we describe a suite of
techniques for expressing causal inference tasks from observational data in
SQL. This suite supports the state-of-the-art methods for causal inference and
run at scale within a database engine. In addition, we introduce several
optimization techniques that significantly speedup causal inference, both in
the online and offline setting. We evaluate the quality and performance of our
techniques by experiments of real datasets.
"
1041,"Automating Large-Scale Simulation and Data Analysis with OMNeT++:
  Lession Learned and Future Perspectives","  Simulation is widely adopted in the study of modern computer networks. In
this context, OMNeT++ provides a set of very effective tools that span from the
definition of the network, to the automation of simulation execution and quick
result representation. However, as network models become more and more complex
to cope with the evolution of network systems, the amount of simulation
factors, the number of simulated nodes and the size of results grow
consequently, leading to simulations with larger scale. In this work, we
perform a critical analysis of the tools provided by OMNeT++ in case of such
large-scale simulations. We then propose a unified and flexible software
architecture to support simulation automation.
"
1042,"Stacked-VLAN-Based Modeling of Hybrid ISP Traffic Control Schemes and
  Service Plans Exploiting Excess Bandwidth in Shared Access Networks","  The current practice of shaping subscriber traffic using a token bucket
filter by Internet service providers may result in a severe waste of network
resources in shared access networks; except for a short period of time
proportional to the size of a token bucket, it cannot allocate excess bandwidth
among active subscribers even when there are only a few active subscribers. To
better utilize the network resources in shared access networks, therefore, we
recently proposed and analyzed the performance of access traffic control
schemes, which can allocate excess bandwidth among active subscribers
proportional to their token generation rates. Also, to exploit the excess
bandwidth allocation enabled by the proposed traffic control schemes, we have
been studying flexible yet practical service plans under a hybrid traffic
control architecture, which are attractive to both an Internet service provider
and its subscribers in terms of revenue and quality of service. In this paper
we report the current status of our modeling of the hybrid traffic control
schemes and service plans with OMNeT++/INET-HNRL based on IEEE standard 802.1Q
stacked VLANs.
"
1043,"On protocol and physical interference models in Poisson wireless
  networks","  This paper analyzes the connection between the protocol and physical
interference models in the setting of Poisson wireless networks. A transmission
is successful under the protocol model if there are no interferers within a
parameterized guard zone around the receiver, while a transmission is
successful under the physical model if the signal to interference plus noise
ratio (SINR) at the receiver is above a threshold. The parameterized protocol
model forms a family of decision rules for predicting the success or failure of
the same transmission attempt under the physical model. For Poisson wireless
networks, we employ stochastic geometry to determine the prior, evidence, and
posterior distributions associated with this estimation problem. With this in
hand, we proceed to develop five sets of results: i) the maximum correlation of
protocol and physical model success indicators, ii) the minimum Bayes risk in
estimating physical success from a protocol observation, iii) the receiver
operating characteristic (ROC) of false rejection (Type I) and false acceptance
(Type II) probabilities, iv) the impact of Rayleigh fading vs. no fading on the
correlation and ROC, and v) the impact of multiple prior protocol model
observations in the setting of a wireless network with a fixed set of nodes in
which the nodes employ the slotted Aloha protocol in each time slot.
"
1044,Infinite Server Queueing Networks with Deadline Based Routing,"  Motivated by timeouts in Internet services, we consider networks of infinite
server queues in which routing decisions are based on deadlines. Specifically,
at each node in the network, the total service time equals the minimum of
several independent service times (e.g. the minimum of the amount of time
required to complete a transaction and a deadline). Furthermore, routing
decisions depend on which of the independent service times achieves the minimum
(e.g. exceeding a deadline will require the customer to be routed so they can
re-attempt the transaction). Because current routing decisions are dependent on
past service times, much of the existing theory on product-form queueing
networks does not apply. In spite of this, we are able to show that such
networks have product-form equilibrium distributions. We verify our analytic
characterization with a simulation of a simple network. We also discuss
extensions of this work to more general settings.
"
1045,FPGA implementation of the procedures for video quality assessment,"  Video resolutions used in variety of media are constantly rising. While
manufacturers struggle to perfect their screens it is also important to ensure
high quality of displayed image. Overall quality can be measured using Mean
Opinion Score (MOS). Video quality can be affected by miscellaneous artifacts,
appearing at every stage of video creation and transmission. In this paper, we
present a solution to calculate four distinct video quality metrics that can be
applied to a real time video quality assessment system. Our assessment module
is capable of processing 8K resolution in real time set at the level of 30
frames per second. Throughput of 2.19 GB/s surpasses performance of pure
software solutions. To concentrate on architectural optimization, the module
was created using high level language.
"
1046,Benchmarking SciDB Data Import on HPC Systems,"  SciDB is a scalable, computational database management system that uses an
array model for data storage. The array data model of SciDB makes it ideally
suited for storing and managing large amounts of imaging data. SciDB is
designed to support advanced analytics in database, thus reducing the need for
extracting data for analysis. It is designed to be massively parallel and can
run on commodity hardware in a high performance computing (HPC) environment. In
this paper, we present the performance of SciDB using simulated image data. The
Dynamic Distributed Dimensional Data Model (D4M) software is used to implement
the benchmark on a cluster running the MIT SuperCloud software stack. A peak
performance of 2.2M database inserts per second was achieved on a single node
of this system. We also show that SciDB and the D4M toolbox provide more
efficient ways to access random sub-volumes of massive datasets compared to the
traditional approaches of reading volumetric data from individual files. This
work describes the D4M and SciDB tools we developed and presents the initial
performance results. This performance was achieved by using parallel inserts, a
in-database merging of arrays as well as supercomputing techniques, such as
distributed arrays and single-program-multiple-data programming.
"
1047,"Proceedings of the 3rd OMNeT++ Community Summit, Brno University of
  Technology - Czech Republic, September 15-16, 2016","  These are the Proceedings of the 3rd OMNeT++ Community Summit, which was held
at the University of Technology in Brno - Czech Republic - on September 15-16,
2016.
"
1048,An Infinite Dimensional Model for A Single Server Priority Queue,"  We consider a Markovian single server queue in which customers are
preemptively scheduled by exogenously assigned priority levels. The novelty in
our model is that the priority levels are randomly assigned from a continuous
probability measure rather than a discrete one. Because the priority levels are
drawn from a continuum, the queue is modeled by a measure-valued stochastic
process. We analyze the steady state behavior of this process and provide
several results. We derive a measure that describes the average distribution of
customer priority levels in the system; we provide a formula for the expected
sojourn time of a customer as a function of his priority level; and we provide
a formula for the expected waiting time of a customer as a function of his
priority level. We interpret these quantitative results and give a qualitative
understanding of how the priority levels affect individual customers as well as
how they affect the system as a whole. The theoretical analysis is verified by
simulation. We also discuss some directions of future work.
"
1049,Benchmarking the Graphulo Processing Framework,"  Graph algorithms have wide applicablity to a variety of domains and are often
used on massive datasets. Recent standardization efforts such as the GraphBLAS
specify a set of key computational kernels that hardware and software
developers can adhere to. Graphulo is a processing framework that enables
GraphBLAS kernels in the Apache Accumulo database. In our previous work, we
have demonstrated a core Graphulo operation called \textit{TableMult} that
performs large-scale multiplication operations of database tables. In this
article, we present the results of scaling the Graphulo engine to larger
problems and scalablity when a greater number of resources is used.
Specifically, we present two experiments that demonstrate Graphulo scaling
performance is linear with the number of available resources. The first
experiment demonstrates cluster processing rates through Graphulo's TableMult
operator on two large graphs, scaled between $2^{17}$ and $2^{19}$ vertices.
The second experiment uses TableMult to extract a random set of rows from a
large graph ($2^{19}$ nodes) to simulate a cued graph analytic. These
benchmarking results are of relevance to Graphulo users who wish to apply
Graphulo to their graph problems.
"
1050,DynIMS: A Dynamic Memory Controller for In-memory Storage on HPC Systems,"  In order to boost the performance of data-intensive computing on HPC systems,
in-memory computing frameworks, such as Apache Spark and Flink, use local DRAM
for data storage. Optimizing the memory allocation to data storage is critical
to delivering performance to traditional HPC compute jobs and throughput to
data-intensive applications sharing the HPC resources. Current practices that
statically configure in-memory storage may leave inadequate space for compute
jobs or lose the opportunity to utilize more available space for data-intensive
applications. In this paper, we explore techniques to dynamically adjust
in-memory storage and make the right amount of space for compute jobs. We have
developed a dynamic memory controller, DynIMS, which infers memory demands of
compute tasks online and employs a feedback-based control model to adapt the
capacity of in-memory storage. We test DynIMS using mixed HPCC and Spark
workloads on a HPC cluster. Experimental results show that DynIMS can achieve
up to 5X performance improvement compared to systems with static memory
allocations.
"
1051,"Soft Cache Hits and the Impact of Alternative Content Recommendations on
  Mobile Edge Caching","  Caching popular content at the edge of future mobile networks has been widely
considered in order to alleviate the impact of the data tsunami on both the
access and backhaul networks. A number of interesting techniques have been
proposed, including femto-caching and ""delayed"" or opportunistic cache access.
Nevertheless, the majority of these approaches suffer from the rather limited
storage capacity of the edge caches, compared to the tremendous and rapidly
increasing size of the Internet content catalog. We propose to depart from the
assumption of hard cache misses, common in most existing works, and consider
""soft"" cache misses, where if the original content is not available, an
alternative content that is locally cached can be recommended. Given that
Internet content consumption is increasingly entertainment-oriented, we believe
that a related content could often lead to complete or at least partial user
satisfaction, without the need to retrieve the original content over expensive
links. In this paper, we formulate the problem of optimal edge caching with
soft cache hits, in the context of delayed access, and analyze the expected
gains. We then show using synthetic and real datasets of related video contents
that promising caching gains could be achieved in practice.
"
1052,Revisiting 802.11 Rate Adaptation from Energy Consumption's Perspective,"  Rate adaptation in 802.11 WLANs has received a lot of attention from the
research community, with most of the proposals aiming at maximising throughput
based on network conditions. Considering energy consumption, an implicit
assumption is that optimality in throughput implies optimality in energy
efficiency, but this assumption has been recently put into question. In this
paper, we address via analysis and experimentation the relation between
throughput performance and energy efficiency in multi-rate 802.11 scenarios. We
demonstrate the trade-off between these performance figures, confirming that
they may not be simultaneously optimised, and analyse their sensitivity towards
the energy consumption parameters of the device. Our results provide the means
to design novel rate adaptation schemes that takes energy consumption into
account.
"
1053,"Data-Unit-Size Distribution Model with Retransmitted Packet Size
  Preservation Property and Its Application to Goodput Analysis for
  Stop-and-Wait Protocol: Case of Independent Packet Losses","  This paper proposes a data-unit-size distribution model to represent the
retransmitted packet size preservation (RPSP) property in a scenario where
independently lost packets are retransmitted by a stop-and-wait protocol. RPSP
means that retransmitted packets with the same sequence number are equal in
size to the packet of the original transmission, which is identical to the
packet generated from a message through the segmentation function, namely,
generated packet. Furthermore, we derive goodput formula using an approach to
derive the data-unit-size distribution. We investigate the effect of RPSP on
frame size distributions and goodput in a simple case when no collision happens
over the bit-error prone wireless network equipped with IEEE 802.11 Distributed
Coordination Function, which is a typical example of the stop-and-wait
protocol. Numerical results show that the effect gets stronger as bit error
rate increases and the maximum size of the generated packets is larger than the
mean size for large enough packet retry limits because longer packets will be
repeatedly corrupted and retransmitted more times as a result of RPSP.
"
1054,Poly-Symmetry in Processor-Sharing Systems,"  We consider a system of processor-sharing queues with state-dependent service
rates. These are allocated according to balanced fairness within a polymatroid
capacity set. Balanced fairness is known to be both insensitive and
Pareto-efficient in such systems, which ensures that the performance metrics,
when computable, will provide robust insights into the real performance of the
system considered. We first show that these performance metrics can be
evaluated with a complexity that is polynomial in the system size if the system
is partitioned into a finite number of parts, so that queues are exchangeable
within each part and asymmetric across different parts. This in turn allows us
to derive stochastic bounds for a larger class of systems which satisfy less
restrictive symmetry assumptions. These results are applied to practical
examples of tree data networks, such as backhaul networks of Internet service
providers, and computer clusters.
"
1055,"Wireless Vehicular Networks in Emergencies: A Single Frequency Network
  Approach","  Obtaining high quality sensor information is critical in vehicular
emergencies. However, existing standards such as IEEE 802.11p/DSRC and LTE-A
cannot support either the required data rates or the latency requirements. One
solution to this problem is for municipalities to invest in dedicated base
stations to ensure that drivers have the information they need to make safe
decisions in or near accidents. In this paper we further propose that these
municipality-owned base stations form a Single Frequency Network (SFN). In
order to ensure that transmissions are reliable, we derive tight bounds on the
outage probability when the SFN is overlaid on an existing cellular network.
Using our bounds, we propose a transmission power allocation algorithm. We show
that our power allocation model can reduce the total instantaneous SFN
transmission power up to $20$ times compared to a static uniform power
allocation solution, for the considered scenarios. The result is particularly
important when base stations rely on an off-grid power source (i.e.,
batteries).
"
1056,10-millisecond Computing,"  Despite computation becomes much complex on data with an unprecedented scale,
we argue computers or smart devices should and will consistently provide
information and knowledge to human being in the order of a few tens
milliseconds. We coin a new term 10-millisecond computing to call attention to
this class of workloads. 10-millisecond computing raises many challenges for
both software and hardware stacks. In this paper, using a typical
workload-memcached on a 40-core server (a main-stream server in near future),
we quantitatively measure 10-ms computing's challenges to conventional
operating systems. For better communication, we propose a simple metric-outlier
proportion to measure quality of service: for N completed requests or jobs, if
M jobs or requests' latencies exceed the outlier threshold t, the outlier
proportion is M/N . For a 1K-scale system running Linux (version 2.6.32), LXC
(version 0.7.5) or XEN (version 4.0.0), respectively, we surprisingly find that
so as to reduce the service outlier proportion to 10% (10% users will feel QoS
degradation), the outlier proportion of a single server has to be reduced by
871X, 2372X, 2372X accordingly. Also, we discuss the possible design spaces of
10-ms computing systems from perspectives of datacenter architectures,
networking, OS and scheduling, and benchmarking.
"
1057,"The Simulation Model Partitioning Problem: an Adaptive Solution Based on
  Self-Clustering (Extended Version)","  This paper is about partitioning in parallel and distributed simulation. That
means decomposing the simulation model into a numberof components and to
properly allocate them on the execution units. An adaptive solution based on
self-clustering, that considers both communication reduction and computational
load-balancing, is proposed. The implementation of the proposed mechanism is
tested using a simulation model that is challenging both in terms of structure
and dynamicity. Various configurations of the simulation model and the
execution environment have been considered. The obtained performance results
are analyzed using a reference cost model. The results demonstrate that the
proposed approach is promising and that it can reduce the simulation execution
time in both parallel and distributed architectures.
"
1058,"A Mean-Field Matrix-Analytic Method for Bike Sharing Systems under
  Markovian Environment","  To reduce automobile exhaust pollution, traffic congestion and parking
difficulties, bike-sharing systems are rapidly developed in many countries and
more than 500 major cities in the world over the past decade. In this paper, we
discuss a large-scale bike-sharing system under Markovian environment, and
propose a mean-field matrix-analytic method in the study of bike-sharing
systems through combining the mean-field theory with the time-inhomogeneous
queues as well as the nonlinear QBD processes. Firstly, we establish an
empirical measure process to express the states of this bike-sharing system.
Secondly, we apply the mean-field theory to establishing a time-inhomogeneous
MAP(t)/MAP(t)/1/K+2L+1 queue, and then to setting up a system of mean-field
equations. Thirdly, we use the martingale limit theory to show the asymptotic
independence of this bike-sharing system, and further analyze the limiting
interchangeability as N goes to infinity and t goes to infinity. Based on this,
we discuss and compute the fixed point in terms of a nonlinear QBD process.
Finally, we analyze performance measures of this bike-sharing system, such as,
the mean of stationary bike number at any station and the stationary
probability of problematic stations. Furthermore, we use numerical examples to
show how the performance measures depend on the key parameters of this
bike-sharing system. We hope the methodology and results of this paper are
applicable in the study of more general large-scale bike-sharing systems.
"
1059,"Optimizing Memory Efficiency for Deep Convolutional Neural Networks on
  GPUs","  Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve
state-of-the-art recognition accuracy. Due to the substantial compute and
memory operations, however, they require significant execution time. The
massive parallel computing capability of GPUs make them as one of the ideal
platforms to accelerate CNNs and a number of GPU-based CNN libraries have been
developed. While existing works mainly focus on the computational efficiency of
CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have
intricate data structures and their memory behavior can have significant impact
on the performance. In this work, we study the memory efficiency of various CNN
layers and reveal the performance implication from both data layouts and memory
access patterns. Experiments show the universal effect of our proposed
optimizations on both single layers and various networks, with up to 27.9x for
a single layer and up to 5.6x on the whole networks.
"
1060,"Cost minimization of network services with buffer and end-to-end
  deadline constraints","  Cloud computing technology provides the means to share physical resources
among multiple users and data center tenants by exposing them as virtual
resources. There is a strong industrial drive to use similar technology and
concepts to provide timing sensitive services. One such is virtual networking
services, so called services chains, which consist of several interconnected
virtual network functions. This allows for the capacity to be scaled up and
down by adding or removing virtual resources. In this work, we develop a model
of a service chain and pose the dynamic allocation of resources as an
optimization problem. We design and present a set of strategies to allot
virtual network nodes in an optimal fashion subject to latency and buffer
constraints.
"
1061,Tuning Crowdsourced Human Computation,"  As the use of crowdsourcing increases, it is important to think about
performance optimization. For this purpose, it is possible to think about each
worker as a HPU(Human Processing Unit), and to draw inspiration from
performance optimization on traditional computers or cloud nodes with CPUs.
However, as we characterize HPUs in detail for this purpose, we find that there
are important differences between CPUs and HPUs, leading to the need for
completely new optimization algorithms.
  In this paper, we study the specific optimization problem of obtaining
results fastest for a crowd sourced job with a fixed total budget. In
crowdsourcing, jobs are usually broken down into sets of small tasks, which are
assigned to workers one at a time. We consider three scenarios of increasing
complexity: Identical Round Homogeneous tasks, Multiplex Round Homogeneous
tasks, and Multiple Round Heterogeneous tasks. For each scenario, we analyze
the stochastic behavior of the HPU clock-rate as a function of the remuneration
offered. After that, we develop an optimum Budget Allocation strategy to
minimize the latency for job completion. We validate our results through
extensive simulations and experiments on Amazon Mechanical Turk.
"
1062,Power Control for Packet Streaming with Head-of-Line Deadlines,"  We consider a mathematical model for streaming media packets (as the
motivating key example) from a transmitter buffer to a receiver over a wireless
link while controlling the transmitter power (hence, the packet/job processing
rate). When each packet comes to the head-of-line (HOL) in the buffer, it is
given a deadline $D$ which is the maximum number of times the transmitter can
attempt retransmission in order to successfully transmit the packet. If this
number of transmission attempts is exhausted, the packet is ejected from the
buffer and the next packet comes to the HOL. Costs are incurred in each time
slot for holding packets in the buffer, expending transmitter power, and
ejecting packets which exceed their deadlines. We investigate how transmission
power should be chosen so as to minimize the total cost of transmitting the
items in the buffer. We formulate the optimal power control problem in a
dynamic programming framework and then hone in on the special case of fixed
interference. For this special case, we are able to provide a precise analytic
characterization of how the power control should vary with the backlog and how
the power control should react to approaching deadlines. In particular, we show
monotonicity results for how the transmitter should adapt power levels to the
backlog and approaching deadlines. We leverage these analytic results from the
special case to build a power control scheme for the general case. Monte Carlo
simulations are used to evaluate the performance of the resulting power control
scheme as compared to the optimal scheme. The resulting power control scheme is
sub-optimal but it provides a low-complexity approximation of the optimal power
control. Simulations show that our proposed schemes outperform benchmark
algorithms. We also discuss applications of the model to other practical
operational scenarios.
"
1063,"D2D-U: Device-to-Device Communications in Unlicensed Bands for 5G and
  Beyond","  Device-to-Device (D2D) communication, which enables direct communication
between nearby mobile devices, is an attractive add-on component to improve
spectrum efficiency and user experience by reusing licensed cellular spectrum
in 5G system. In this paper, we propose to enable D2D communication in
unlicensed spectrum (D2D-U) as an underlay of the uplink LTE network for
further booming the network capacity. A sensing-based protocol is designed to
support the unlicensed channel access for both LTE and D2D users. We further
investigate the subchannel allocation problem to maximize the sum rate of LTE
and D2D users while taking into account their interference to the existing
Wi-Fi systems. Specifically, we formulate the subchannel allocation as a
many-to-many matching problem with externalities, and develop an iterative
user-subchannel swap algorithm. Analytical and simulation results show that the
proposed D2D-U scheme can significantly improve the system sum-rate.
"
1064,"Breakdown of a Benchmark Score Without Internal Analysis of Benchmarking
  Program","  A breakdown of a benchmark score is how much each aspect of the system
performance affects the score. Existing methods require internal analysis on
the benchmarking program and then involve the following problems: (1) require a
certain amount of labor for code analysis, profiling, simulation, and so on and
(2) require the benchmarking program itself. In this paper, we present a method
for breaking down a benchmark score without internal analysis of the
benchmarking program. The method utilizes regression analysis of benchmark
scores on a number of systems. Experimental results with 3 benchmarks on 15
Android smartphones showed that our method could break down those benchmark
scores even though there is room for improvement in accuracy.
"
1065,"Non-Asymptotic Delay Bounds for Multi-Server Systems with
  Synchronization Constraints","  Multi-server systems have received increasing attention with important
implementations such as Google MapReduce, Hadoop, and Spark. Common to these
systems are a fork operation, where jobs are first divided into tasks that are
processed in parallel, and a later join operation, where completed tasks wait
until the results of all tasks of a job can be combined and the job leaves the
system. The synchronization constraint of the join operation makes the analysis
of fork-join systems challenging and few explicit results are known. In this
work, we model fork-join systems using a max-plus server model that enables us
to derive statistical bounds on waiting and sojourn times for general arrival
and service time processes. We contribute end-to-end delay bounds for
multi-stage fork-join networks that grow in $\mathcal{O}(h \ln k)$ for $h$
fork-join stages, each with $k$ parallel servers. We perform a detailed
comparison of different multi-server configurations and highlight their pros
and cons. We also include an analysis of single-queue fork-join systems that
are non-idling and achieve a fundamental performance gain, and compare these
results to both simulation and a live Spark system.
"
1066,Hybrid Static/Dynamic Schedules for Tiled Polyhedral Programs,"  Polyhedral compilers perform optimizations such as tiling and
parallelization; when doing both, they usually generate code that executes
""barrier-synchronized wavefronts"" of tiles. We present a system to express and
generate code for hybrid schedules, where some constraints are automatically
satisfied through the structure of the code, and the remainder are dynamically
enforced at run-time with data flow mechanisms. We prove bounds on the added
overheads that are better, by at least one polynomial degree, than those of
previous techniques.
  We propose a generic mechanism to implement the needed synchronization, and
show it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU
(CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow
platforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete
implementation that works without the need of any sophisticated run-time
mechanism.
  Our experiments show our simple implementation to be competitive or better
than the wavefront-synchronized code generated by other systems. We also show
how the proposed mechanism can achieve 24% to 70% reduction in energy.
"
1067,Location Aggregation of Spatial Population CTMC Models,"  In this paper we focus on spatial Markov population models, describing the
stochastic evolution of populations of agents, explicitly modelling their
spatial distribution, representing space as a discrete, finite graph. More
specifically, we present a heuristic approach to aggregating spatial locations,
which is designed to preserve the dynamical behaviour of the model whilst
reducing the computational cost of analysis. Our approach combines stochastic
approximation ideas (moment closure, linear noise), with computational
statistics (spectral clustering) to obtain an efficient aggregation, which is
experimentally shown to be reasonably accurate on two case studies: an instance
of epidemic spreading and a London bike sharing scenario.
"
1068,Evaluating load balancing policies for performance and energy-efficiency,"  Nowadays, more and more increasingly hard computations are performed in
challenging fields like weather forecasting, oil and gas exploration, and
cryptanalysis. Many of such computations can be implemented using a computer
cluster with a large number of servers. Incoming computation requests are then,
via a so-called load balancing policy, distributed over the servers to ensure
optimal performance. Additionally, being able to switch-off some servers during
low period of workload, gives potential to reduced energy consumption.
Therefore, load balancing forms, albeit indirectly, a trade-off between
performance and energy consumption. In this paper, we introduce a syntax for
load-balancing policies to dynamically select a server for each request based
on relevant criteria, including the number of jobs queued in servers, power
states of servers, and transition delays between power states of servers. To
evaluate many policies, we implement two load balancers in: (i) iDSL, a
language and tool-chain for evaluating service-oriented systems, and (ii) a
simulation framework in AnyLogic. Both implementations are successfully
validated by comparison of the results.
"
1069,"Hybrid CPU-GPU generation of the Hamiltonian and Overlap matrices in
  FLAPW methods","  In this paper we focus on the integration of high-performance numerical
libraries in ab initio codes and the portability of performance and
scalability. The target of our work is FLEUR, a software for electronic
structure calculations developed in the Forschungszentrum J\""ulich over the
course of two decades. The presented work follows up on a previous effort to
modernize legacy code by re-engineering and rewriting it in terms of highly
optimized libraries. We illustrate how this initial effort to get efficient and
portable shared-memory code enables fast porting of the code to emerging
heterogeneous architectures. More specifically, we port the code to nodes
equipped with multiple GPUs. We divide our study in two parts. First, we show
considerable speedups attained by minor and relatively straightforward code
changes to off-load parts of the computation to the GPUs. Then, we identify
further possible improvements to achieve even higher performance and
scalability. On a system consisting of 16-cores and 2 GPUs, we observe speedups
of up to 5x with respect to our optimized shared-memory code, which in turn
means between 7.5x and 12.5x speedup with respect to the original FLEUR code.
"
1070,"Optimal Heavy-Traffic Queue Length Scaling in an Incompletely Saturated
  Switch","  We consider an input queued switch operating under the MaxWeight scheduling
algorithm. This system is interesting to study because it is a model for
Internet routers and data center networks. Recently, it was shown that the
MaxWeight algorithm has optimal heavy-traffic queue length scaling when all
ports are uniformly saturated. Here we consider the case when an arbitrary
number of ports are saturated (which we call the incompletely saturated case),
and each port is allowed to saturate at a different rate. We use a recently
developed drift technique to show that the heavy-traffic queue length under the
MaxWeight scheduling algorithm has optimal scaling with respect to the switch
size even in these cases.
"
1071,Multi-level Simulation of Internet of Things on Smart Territories,"  In this paper, a methodology is presented and employed for simulating the
Internet of Things (IoT). The requirement for scalability, due to the possibly
huge amount of involved sensors and devices, and the heterogeneous scenarios
that might occur, impose resorting to sophisticated modeling and simulation
techniques. In particular, multi-level simulation is regarded as a main
framework that allows simulating large-scale IoT environments while keeping
high levels of detail, when it is needed. We consider a use case based on the
deployment of smart services in decentralized territories. A two level
simulator is employed, which is based on a coarse agent-based, adaptive
parallel and distributed simulation approach to model the general life of
simulated entities. However, when needed a finer grained simulator (based on
OMNeT++) is triggered on a restricted portion of the simulated area, which
allows considering all issues concerned with wireless communications. Based on
this use case, it is confirmed that the ad-hoc wireless networking technologies
do represent a principle tool to deploy smart services over decentralized
countrysides. Moreover, the performance evaluation confirms the viability of
utilizing multi-level simulation for simulating large scale IoT environments.
"
1072,"A new GPU implementation for lattice-Boltzmann simulations on sparse
  geometries","  We describe a high-performance implementation of the lattice Boltzmann method
(LBM) for sparse 3D geometries on graphic processors (GPU). The main
contribution of this work is a data layout that allows to minimise the number
of redundant memory transactions during the propagation step of LBM. We show
that by using a uniform mesh of small three-dimensional tiles and a careful
data placement it is possible to utilise more than 70% of maximum theoretical
GPU memory bandwidth for D3Q19 lattice and double precision numbers. The
performance of our implementation is thoroughly examined and compared with
other GPU implementations of LBM. The proposed method performs the best for
sparse geometries with good spatial locality.
"
1073,"Practical Interpolation for Spectrum Cartography through Local Path Loss
  Modeling","  A fundamental building block for supporting better utilization of radio
spectrum involves predicting the impact that an emitter will have at different
geographic locations. To this end, fixed sensors can be deployed to spatially
sample the RF environment over an area of interest, with interpolation methods
used to infer received power at locations between sensors. This paper describes
a radio map interpolation method that exploits the known properties of most
path loss models, with the aim of minimizing the RMS errors in predicted
dB-power. We show that the results come very close to those for ideal Simple
Kriging. Moreover, the method is simpler in terms of real-time computation by
the network and it requires no knowledge of the spatial correlation of shadow
fading. Our analysis of the method is general, but we exemplify it for a
specific network geometry, comprising a grid-like pattern of sensors. We also
provide comparisons to other widely used interpolation methods.
"
1074,"A Case for Malleable Thread-Level Linear Algebra Libraries: The LU
  Factorization with Partial Pivoting","  We propose two novel techniques for overcoming load-imbalance encountered
when implementing so-called look-ahead mechanisms in relevant dense matrix
factorizations for the solution of linear systems. Both techniques target the
scenario where two thread teams are created/activated during the factorization,
with each team in charge of performing an independent task/branch of execution.
The first technique promotes worker sharing (WS) between the two tasks,
allowing the threads of the task that completes first to be reallocated for use
by the costlier task. The second technique allows a fast task to alert the
slower task of completion, enforcing the early termination (ET) of the second
task, and a smooth transition of the factorization procedure into the next
iteration.
  The two mechanisms are instantiated via a new malleable thread-level
implementation of the Basic Linear Algebra Subprograms (BLAS), and their
benefits are illustrated via an implementation of the LU factorization with
partial pivoting enhanced with look-ahead. Concretely, our experimental results
on a six core Intel-Xeon processor show the benefits of combining WS+ET,
reporting competitive performance in comparison with a task-parallel
runtime-based solution.
"
1075,A Metric for Performance Portability,"  The term ""performance portability"" has been informally used in computing to
refer to a variety of notions which generally include: 1) the ability to run
one application across multiple hardware platforms; and 2) achieving some
notional level of performance on these platforms. However, there has been a
noticeable lack of consensus on the precise meaning of the term, and authors'
conclusions regarding their success (or failure) to achieve performance
portability have thus been subjective. Comparing one approach to performance
portability with another has generally been marked with vague claims and
verbose, qualitative explanation of the comparison. This paper presents a
concise definition for performance portability, along with a simple metric that
accurately captures the performance and portability of an application across
different platforms. The utility of this metric is then demonstrated with a
retroactive application to previous work.
"
1076,Cultivating Software Performance in Cloud Computing,"  There exist multitudes of cloud performance metrics, including workload
performance, application placement, software/hardware optimization,
scalability, capacity, reliability, agility and so on. In this paper, we
consider jointly optimizing the performance of the software applications in the
cloud. The challenges lie in bringing a diversity of raw data into tidy data
format, unifying performance data from multiple systems based on timestamps,
and assessing the quality of the processed performance data. Even after
verifying the quality of cloud performance data, additional challenges block
optimizing cloud computing. In this paper, we identify the challenges of cloud
computing from the perspectives of computing environment, data collection,
performance analytics and production environment.
"
1077,"Analysis on 60 GHz Wireless Communications with Beamwidth-Dependent
  Misalignment","  High speed wireless access on 60 GHz spectrum relies on high-gain directional
antennas to overcome the severe signal attenuation. However, perfect alignment
between transmitting and receiving antenna beams is rare in practice and
overheard signals from concurrent transmissions may cause significant
interference. In this paper we analyze the impact of antenna beam misalignment
on the system performance of 60 GHz wireless access. We quantify the signal
power loss caused by beam misalignment and the interference power accumulated
from neighboring concurrent transmissions whose signals are leaked either via
the main-beam pointing in the similar direction or via side-lobe emission, and
derive the probability distribution of the signal to interference plus noise
power ratio (SINR). For scenarios where interfering transmitters are
distributed uniformly at random, we derive upper and lower bounds on the
cumulative distribution function (abbreviated as CDF or c.d.f.) of SINR, which
can be easily applied to evaluate system performance. We validate our
analytical results by simulations where random nodes are uniformly distributed
within a circular hall, and evaluate the sensitivity of average throughput and
outage probability against two parameters: the half-power (3 dB) beamwidth to
main-lobe beamwidth ratio and the beam misalignment deviation to main-lobe
beamwidth ratio. Our results indicate that the derived lower bound performs
well when the half-power beamwidth to main-lobe beamwidth ratio or the number
of concurrent transmission links is small. When the number of active links is
high, it is desirable in antenna design to balance the degradation caused by
beam misalignment (wider beam is better) and the interference from concurrent
transmission (narrower beam is better).
"
1078,DESP-C++: A Discrete-Event Simulation Package for C++,"  DESP-C++ is a C++ discrete-event random simulation engine that has been
designed to be fast, very easy to use and expand, and valid. DESP-C++ is based
on the resource view. Its complete architecture is presented in detail, as well
as a short "" user manual "". The validity of DESP-C++ is demonstrated by the
simulation of three significant models. In each case, the simulation results
obtained with DESP-C++ match those obtained with a validated simulation
software: QNAP2. The versatility of DESP-C++ is also illustrated this way,
since the modelled systems are very different from each other: a simple
production system, the dining philosopher classical deadlock problem, and a
complex object-oriented database management system.
"
1079,"Serving the Grid: an Experimental Study of Server Clusters as Real-Time
  Demand Response Resources","  Demand response is a crucial technology to allow large-scale penetration of
intermittent renewable energy sources in the electric grid. This paper is based
on the thesis that datacenters represent especially attractive candidates for
providing flexible, real-time demand response services to the grid; they are
capable of finely-controllable power consumption, fast power ramp-rates, and
large dynamic range. This paper makes two main contributions: (a) it provides
detailed experimental evidence justifying this thesis, and (b) it presents a
comparative investigation of three candidate software interfaces for power
control within the servers. All of these results are based on a series of
experiments involving real-time power measurements on a lab-scale server
cluster. This cluster was specially instrumented for accurate and fast power
measurements on a time-scale of 100 ms or less. Our results provide preliminary
evidence for the feasibility of large scale demand response using datacenters,
and motivates future work on exploiting this capability.
"
1080,Comparison Between IPv4 to IPv6 Transition Techniques,"  The IPv4 addresses exhaustion demands a protocol transition from IPv4 to
IPv6. The original transition technique, the dual stack, is not widely deployed
yet and it demanded the creation of new transition techniques to extend the
transition period. This work makes an experimental comparison of techniques
that use dual stack with a limited IPv4 address. This limited address might be
a RFC 1918 address with a NAT at the Internet Service Provider (ISP) gateway,
also known as Carrier Grade NAT (CGN), or an Address Plus Port (A+P) shared
IPv4 address. The chosen techniques also consider an IPv6 only ISP network. The
transport of the IPv4 packets through the IPv6 only networks may use IPv4
packets encapsulated on IPv6 packets or a double translation, by making one
IPv4 to IPv6 translation to enter the IPv6 only network and one IPv6 to IPv4
translation to return to the IPv4 network. The chosen techniques were DS-Lite,
464XLAT, MAP-E and MAP-T. The first part of the test is to check some of the
most common usages of the Internet by a home user and the impacts of the
transition techniques on the user experience. The second part is a measured
comparison considering bandwidth, jitter and latency introduced by the
techniques and processor usage on the network equipment.
"
1081,"Characterising radio telescope software with the Workload
  Characterisation Framework","  We present a modular framework, the Workload Characterisation Framework
(WCF), that is developed to reproducibly obtain, store and compare key
characteristics of radio astronomy processing software. As a demonstration, we
discuss the experiences using the framework to characterise a LOFAR calibration
and imaging pipeline.
"
1082,Support vector regression model for BigData systems,"  Nowadays Big Data are becoming more and more important. Many sectors of our
economy are now guided by data-driven decision processes. Big Data and business
intelligence applications are facilitated by the MapReduce programming model
while, at infrastructural layer, cloud computing provides flexible and cost
effective solutions for allocating on demand large clusters. In such systems,
capacity allocation, which is the ability to optimally size minimal resources
for achieve a certain level of performance, is a key challenge to enhance
performance for MapReduce jobs and minimize cloud resource costs. In order to
do so, one of the biggest challenge is to build an accurate performance model
to estimate job execution time of MapReduce systems. Previous works applied
simulation based models for modeling such systems. Although this approach can
accurately describe the behavior of Big Data clusters, it is too
computationally expensive and does not scale to large system. We try to
overcome these issues by applying machine learning techniques. More precisely
we focus on Support Vector Regression (SVR) which is intrinsically more robust
w.r.t other techniques, like, e.g., neural networks, and less sensitive to
outliers in the training set. To better investigate these benefits, we compare
SVR to linear regression.
"
1083,"Broadcast Strategies and Performance Evaluation of IEEE 802.15.4 in
  Wireless Body Area Networks WBAN","  The rapid advances in sensors and ultra-low power wireless communication has
enabled a new generation of wireless sensor networks: Wireless Body Area
Networks (WBAN). To the best of our knowledge the current paper is the first to
address broadcast in WBAN. We first analyze several broadcast strategies
inspired from the area of Delay Tolerant Networks (DTN). The proposed
strategies are evaluated via the OMNET++ simulator that we enriched with
realistic human body mobility models and channel models issued from the recent
research on biomedical and health informatics. Contrary to the common
expectation, our results show that existing research in DTN cannot be
transposed without significant modifications in WBANs area. That is, existing
broadcast strategies for DTNs do not perform well with human body mobility.
However, our extensive simulations give valuable insights and directions for
designing efficient broadcast in WBAN. Furthermore, we propose a novel
broadcast strategy that outperforms the existing ones in terms of end-to-end
delay, network coverage and energy consumption. Additionally, we performed
investigations of independent interest related to the ability of all the
studied strategies to ensure the total order delivery property when stressed
with various packet rates. These investigations open new and challenging
research directions.
"
1084,"Sorting Data on Ultra-Large Scale with RADULS. New Incarnation of Radix
  Sort","  The paper introduces RADULS, a new parallel sorter based on radix sort
algorithm, intended to organize ultra-large data sets efficiently. For example
4G 16-byte records can be sorted with 16 threads in less than 15 seconds on
Intel Xeon-based workstation. The implementation of RADULS is not only highly
optimized to gain such an excellent performance, but also parallelized in a
cache friendly manner to make the most of modern multicore architectures.
Besides, our parallel scheduler launches a few different procedures at runtime,
according to the current parameters of the execution, for proper workload
management. All experiments show RADULS to be superior to competing algorithms.
"
1085,Geographical Load Balancing across Green Datacenters,"  ""Geographic Load Balancing"" is a strategy for reducing the energy cost of
data centers spreading across different terrestrial locations. In this paper,
we focus on load balancing among micro-datacenters powered by renewable energy
sources. We model via a Markov Chain the problem of scheduling jobs by
prioritizing datacenters where renewable energy is currently available. Not
finding a convenient closed form solution for the resulting chain, we use mean
field techniques to derive an asymptotic approximate model which instead is
shown to have an extremely simple and intuitive steady state solution. After
proving, using both theoretical and discrete event simulation results, that the
system performance converges to the asymptotic model for an increasing number
of datacenters, we exploit the simple closed form model's solution to
investigate relationships and trade-offs among the various system parameters.
"
1086,"Spatial multi-LRU: Distributed Caching for Wireless Networks with
  Coverage Overlaps","  This article introduces a novel family of decentralised caching policies,
applicable to wireless networks with finite storage at the edge-nodes
(stations). These policies, that are based on the Least-Recently-Used
replacement principle, are here referred to as spatial multi-LRU. They update
cache inventories in a way that provides content diversity to users who are
covered by, and thus have access to, more than one station. Two variations are
proposed, the multi-LRU-One and -All, which differ in the number of replicas
inserted in the involved caches. We analyse their performance under two types
of traffic demand, the Independent Reference Model (IRM) and a model that
exhibits temporal locality. For IRM, we propose a Che-like approximation to
predict the hit probability, which gives very accurate results. Numerical
evaluations show that the performance of multi-LRU increases the more the
multi-coverage areas increase, and it is close to the performance of
centralised policies, when multi-coverage is sufficient. For IRM traffic,
multi-LRU-One is preferable to multi-LRU-All, whereas when the traffic exhibits
temporal locality the -All variation can perform better. Both variations
outperform the simple LRU. When popularity knowledge is not accurate, the new
policies can perform better than centralised ones.
"
1087,"Efficient Realization of Householder Transform through
  Algorithm-Architecture Co-design for Acceleration of QR Factorization","  We present efficient realization of Householder Transform (HT) based QR
factorization through algorithm-architecture co-design where we achieve
performance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art
multicore, General Purpose Graphics Processing Units (GPGPUs), Field
Programmable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and
experimental analysis of classical HT is performed for opportunities to exhibit
higher degree of parallelism where parallelism is quantified as a number of
parallel operations per level in the Directed Acyclic Graph (DAG) of the
transform. Based on theoretical analysis of classical HT, an opportunity
re-arrange computations in the classical HT is identified that results in
Modified HT (MHT) where it is shown that MHT exhibits 1.33x times higher
parallelism than classical HT. Experiments in off-the-shelf multicore and
General Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that
MHT is capable of achieving slightly better or equal performance compared to
classical HT based QR factorization realizations in the optimized software
packages for Dense Linear Algebra (DLA). We implement MHT on a customized
platform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better
performance than native implementation of classical HT on the same accelerator.
For custom realization of HT and MHT based QR factorization, we also identify
macro operations in the DAGs of HT and MHT that are realized on a
Reconfigurable Data-path (RDP). We also observe that due to re-arrangement in
the computations in MHT, custom realization of MHT is capable of achieving 12%
better performance improvement over multicore and GPGPUs than the performance
improvement reported by General Matrix Multiplication (GEMM) over highly tuned
DLA software packages for multicore and GPGPUs which is counter-intuitive.
"
1088,"Dissecting demand response mechanisms: the role of consumption forecasts
  and personalized offers","  Demand-Response (DR) programs, whereby users of an electricity network are
encouraged by economic incentives to rearrange their consumption in order to
reduce production costs, are envisioned to be a key feature of the smart grid
paradigm. Several recent works proposed DR mechanisms and used analytical
models to derive optimal incentives. Most of these works, however, rely on a
macroscopic description of the population that does not model individual
choices of users. In this paper, we conduct a detailed analysis of those models
and we argue that the macroscopic descriptions hide important assumptions that
can jeopardize the mechanisms' implementation (such as the ability to make
personalized offers and to perfectly estimate the demand that is moved from a
timeslot to another). Then, we start from a microscopic description that
explicitly models each user's decision. We introduce four DR mechanisms with
various assumptions on the provider's capabilities. Contrarily to previous
studies, we find that the optimization problems that result from our mechanisms
are complex and can be solved numerically only through a heuristic. We present
numerical simulations that compare the different mechanisms and their
sensitivity to forecast errors. At a high level, our results show that the
performance of DR mechanisms under reasonable assumptions on the provider's
capabilities are significantly lower than
"
1089,"Optimizing Stochastic Scheduling in Fork-Join Queueing Models: Bounds
  and Applications","  Fork-Join (FJ) queueing models capture the dynamics of system parallelization
under synchronization constraints, for example, for applications such as
MapReduce, multipath transmission and RAID systems. Arriving jobs are first
split into tasks and mapped to servers for execution, such that a job can only
leave the system when all of its tasks are executed.
  In this paper, we provide computable stochastic bounds for the waiting and
response time distributions for heterogeneous FJ systems under general
parallelization benefit. Our main contribution is a generalized mathematical
framework for probabilistic server scheduling strategies that are essentially
characterized by a probability distribution over the number of utilized
servers, and the optimization thereof. We highlight the trade-off between the
scaling benefit due to parallelization and the FJ inherent synchronization
penalty. Further, we provide optimal scheduling strategies for arbitrary
scaling regimes that map to different levels of parallelization benefit. One
notable insight obtained from our results is that different applications with
varying parallelization benefits result in different optimal strategies.
Finally, we complement our analytical results by applying them to various
applications showing the optimality of the proposed scheduling strategies.
"
1090,"A Generalized Performance Evaluation Framework for Parallel Systems with
  Output Synchronization","  Frameworks, such as MapReduce and Hadoop are abundant nowadays. They seek to
reap benefits of parallelization, albeit subject to a synchronization
constraint at the output. Fork-Join (FJ) queuing models are used to analyze
such systems. Arriving jobs are split into tasks each of which is mapped to
exactly one server. A job leaves the system when all of its tasks are executed.
  As a metric of performance, we consider waiting times for both
work-conserving and non-work conserving server systems under a mathematical
set-up general enough to take into account possible phase-type behavior of the
servers, and as suggested by recent evidences, bursty arrivals.
  To this end, we present a Markov-additive process framework for an FJ system
and provide computable bounds on tail probabilities of steady-state waiting
times, for both types of servers separately. We apply our results to three
scenarios, namely, non-renewal (Markov-modulated) arrivals, servers showing
phase-type behavior, and Markov-modulated arrivals and services. We compare our
bounds against estimates obtained through simulations and also provide a
theoretical conceptualization of provisions in FJ systems. Finally, we
calibrate our model with real data traces, and illustrate how our bounds can be
used to devise provisions.
"
1091,"Application-aware Retiming of Accelerators: A High-level Data-driven
  Approach","  Flexibility at hardware level is the main driving force behind adaptive
systems whose aim is to realise microarhitecture deconfiguration 'online'. This
feature allows the software/hardware stack to tolerate drastic changes of the
workload in data centres. With emerge of FPGA reconfigurablity this technology
is becoming a mainstream computing paradigm. Adaptivity is usually accompanied
by the high-level tools to facilitate multi-dimensional space exploration. An
essential aspect in this space is memory orchestration where on-chip and
off-chip memory distribution significantly influences the architecture in
coping with the critical spatial and timing constraints, e.g. Place and Route.
This paper proposes a memory smart technique for a particular class of adaptive
systems: Elastic Circuits which enjoy slack elasticity at fine level of
granularity. We explore retiming of a set of popular benchmarks via
investigating the memory distribution within and among accelerators. The area,
performance and power patterns are adopted by our high-level synthesis
framework, with respect to the behaviour of the input descriptions, to improve
the quality of the synthesised elastic circuits.
"
1092,"Accelerated Convolutions for Efficient Multi-Scale Time to Contact
  Computation in Julia","  Convolutions have long been regarded as fundamental to applied mathematics,
physics and engineering. Their mathematical elegance allows for common tasks
such as numerical differentiation to be computed efficiently on large data
sets. Efficient computation of convolutions is critical to artificial
intelligence in real-time applications, like machine vision, where convolutions
must be continuously and efficiently computed on tens to hundreds of kilobytes
per second. In this paper, we explore how convolutions are used in fundamental
machine vision applications. We present an accelerated n-dimensional
convolution package in the high performance computing language, Julia, and
demonstrate its efficacy in solving the time to contact problem for machine
vision. Results are measured against synthetically generated videos and
quantitatively assessed according to their mean squared error from the ground
truth. We achieve over an order of magnitude decrease in compute time and
allocated memory for comparable machine vision applications. All code is
packaged and integrated into the official Julia Package Manager to be used in
various other scenarios.
"
1093,M/g/c/c state dependent queueing model for road traffic simulation,"  In this paper, we present a stochastic queuing model for the road traffic,
which captures the stationary density-flow relationships in both uncongested
and congestion conditions. The proposed model is based on the $M/g/c/c$ state
dependent queuing model of Jain and Smith, and is inspired from the
deterministic Godunov scheme for the road traffic simulation. We first propose
a reformulation of the $M/g/c/c$ state dependent model that works with
density-flow fundamental diagrams rather than density-speed relationships. We
then extend this model in order to consider upstream traffic demand as well as
downstream traffic supply. Finally, we calculate the speed and travel time
distributions for the $M/g/c/c$ state dependent queuing model and for the
proposed model, and derive stationary performance measures (expected number of
cars, blocking probability, expected travel time, and throughput). A comparison
with results predicted by the $M/g/c/c$ state dependent queuing model shows
that the proposed model correctly represents the dynamics of traffic and gives
good performances measures. The results illustrate the good accuracy of the
proposed model.
"
1094,"Experiences with Some Benchmarks for Deductive Databases and
  Implementations of Bottom-Up Evaluation","  OpenRuleBench is a large benchmark suite for rule engines, which includes
deductive databases. We previously proposed a translation of Datalog to C++
based on a method that ""pushes"" derived tuples immediately to places where they
are used. In this paper, we report performance results of various
implementation variants of this method compared to XSB, YAP and DLV. We study
only a fraction of the OpenRuleBench problems, but we give a quite detailed
analysis of each such task and the factors which influence performance. The
results not only show the potential of our method and implementation approach,
but could be valuable for anybody implementing systems which should be able to
execute tasks of the discussed types.
"
1095,An Infinite Dimensional Model for a Many Server Priority Queue,"  We consider a Markovian many server queueing system in which customers are
preemptively scheduled according to exogenously assigned priority levels. The
priority levels are randomly assigned from a continuous probability measure
rather than a discrete one and hence, the queue is modeled by an infinite
dimensional stochastic process. We analyze the equilibrium behavior of the
system and provide several results. We derive the Radon-Nikodym derivative
(with respect to Lebesgue measure) of the measure that describes the average
distribution of customer priority levels in the system; we provide a formula
for the expected sojourn time of a customer as a function of his priority
level; and we provide a formula for the expected waiting time of a customer as
a function of his priority level. We verify our theoretical analysis with
discrete-event simulations. We discuss how each of our results generalizes
previous work on infinite dimensional models for single server priority queues.
"
1096,Delay-Optimal Scheduling for Queueing Systems with Switching Overhead,"  We study the scheduling polices for asymptotically optimal delay in queueing
systems with switching overhead. Such systems consist of a single server that
serves multiple queues, and some capacity is lost whenever the server switches
to serve a different set of queues. The capacity loss due to this switching
overhead can be significant in many emerging applications, and needs to be
explicitly addressed in the design of scheduling policies. For example, in
60GHz wireless networks with directional antennas, base stations need to train
and reconfigure their beam patterns whenever they switch from one client to
another. Considerable switching overhead can also be observed in many other
queueing systems such as transportation networks and manufacturing systems.
While the celebrated Max-Weight policy achieves asymptotically optimal average
delay for systems without switching overhead, it fails to preserve
throughput-optimality, let alone delay-optimality, when switching overhead is
taken into account. We propose a class of Biased Max-Weight scheduling policies
that explicitly takes switching overhead into account. The Biased Max-Weight
policy can use either queue length or head-of-line waiting time as an indicator
of the system status. We prove that our policies not only are
throughput-optimal, but also can be made arbitrarily close to the asymptotic
lower bound on average delay. To validate the performance of the proposed
policies, we provide extensive simulation with various system topologies and
different traffic patterns. We show that the proposed policies indeed achieve
much better delay performance than that of the state-of-the-art policy.
"
1097,"Formal Analysis of SEU Mitigation for Early Dependability and
  Performability Analysis of FPGA-based Space Applications","  SRAM-based FPGAs are increasingly popular in the aerospace industry due to
their field programmability and low cost. However, they suffer from cosmic
radiation induced Single Event Upsets (SEUs). In safety-critical applications,
the dependability of the design is a prime concern since failures may have
catastrophic consequences. An early analysis of the relationship between
dependability metrics, performability-area trade-off, and different mitigation
techniques for such applications can reduce the design effort while increasing
the design confidence. This paper introduces a novel methodology based on
probabilistic model checking, for the analysis of the reliability,
availability, safety and performance-area tradeoffs of safety-critical systems
for early design decisions. Starting from the high-level description of a
system, a Markov reward model is constructed from the Control Data Flow Graph
(CDFG) and a component characterization library targeting FPGAs. The proposed
model and exhaustive analysis capture all the failure states (based on the
fault detection coverage) and repairs possible in the system. We present
quantitative results based on an FIR filter circuit to illustrate the
applicability of the proposed approach and to demonstrate that a wide range of
useful dependability and performability properties can be analyzed using the
proposed methodology. The modeling results show the relationship between
different mitigation techniques and fault detection coverage, exposing their
direct impact on the design for early decisions.
"
1098,"On the Performance of Network Parallel Training in Artificial Neural
  Networks","  Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.
"
1099,GPGPU Performance Estimation with Core and Memory Frequency Scaling,"  Graphics Processing Units (GPUs) support dynamic voltage and frequency
scaling (DVFS) in order to balance computational performance and energy
consumption. However, there still lacks simple and accurate performance
estimation of a given GPU kernel under different frequency settings on real
hardware, which is important to decide best frequency configuration for energy
saving. This paper reveals a fine-grained model to estimate the execution time
of GPU kernels with both core and memory frequency scaling. Over a 2.5x range
of both core and memory frequencies among 12 GPU kernels, our model achieves
accurate results (within 3.5\%) on real hardware. Compared with the cycle-level
simulators, our model only needs some simple micro-benchmark to extract a set
of hardware parameters and performance counters of the kernels to produce this
high accuracy.
"
1100,"Light traffic behavior under the power-of-two load balancing strategy:
  The case of heterogeneous servers","  We consider a multi-server queueing system under the power-of-two policy with
Poisson job arrivals, heterogeneous servers and a general job requirement
distribution; each server operates under the first-come first-serve policy and
there are no buffer constraints. We analyze the performance of this system in
light traffic by evaluating the first two light traffic derivatives of the
average job response time. These expressions point to several interesting
structural features associated with server heterogeneity in light traffic: For
unequal capacities, the average job response time is seen to decrease for small
values of the arrival rate, and the more diverse the server speeds, the greater
the gain in performance. These theoretical findings are assessed through
limited simulations.
"
1101,"Steady state availability general equations of decision and sequential
  processes in Continuous Time Markov Chain models","  Continuous Time Markov Chain (CMTC) is widely used to describe and analyze
systems in several knowledge areas. Steady state availability is one important
analysis that can be made through Markov chain formalism that allows
researchers generate equations for several purposes, such as channel capacity
estimation in wireless networks as well as system performance estimations. The
problem with this kind of analysis is the complex process to generating these
equations. In this letter, we have developed general equations for decision and
sequential processes of CMTC Models, aiming to help researchers to develop
steady state availability equations. We also have developed the general
equation here termed as Closed Decision Process.
"
1102,"Parameter and State Estimation in Queues and Related Stochastic Models:
  A Bibliography","  This is an annotated bibliography on estimation and inference results for
queues and related stochastic models. The purpose of this document is to
collect and categorise works in the field, allowing for researchers and
practitioners to explore the various types of results that exist. This
bibliography attempts to include all known works that satisfy both of these
requirements: -Works that deal with queueing models. -Works that contain
contributions related to the methodology of parameter estimation, state
estimation, hypothesis testing, confidence interval and/or actual datasets of
application areas. Our attempt is to make this bibliography exhaustive, yet
there are possibly some papers that we have missed. As it is updated
continuously, additions and comments are welcomed. The sections below
categorise the works based on several categories. A single paper may appear in
several categories simultaneously. The final section lists all works in
chronological order along with short descriptions of the contributions. This
bibliography is maintained at
http://www.maths.uq.edu.au/~pkp/papers/Qest/Qest.html and may be cited as such.
We welcome additions and corrections.
"
1103,Autotuning GPU Kernels via Static and Predictive Analysis,"  Optimizing the performance of GPU kernels is challenging for both human
programmers and code generators. For example, CUDA programmers must set thread
and block parameters for a kernel, but might not have the intuition to make a
good choice. Similarly, compilers can generate working code, but may miss
tuning opportunities by not targeting GPU models or performing code
transformations. Although empirical autotuning addresses some of these
challenges, it requires extensive experimentation and search for optimal code
variants. This research presents an approach for tuning CUDA kernels based on
static analysis that considers fine-grained code structure and the specific GPU
architecture features. Notably, our approach does not require any program runs
in order to discover near-optimal parameter settings. We demonstrate the
applicability of our approach in enabling code autotuners such as Orio to
produce competitive code variants comparable with empirical-based methods,
without the high cost of experiments.
"
1104,"Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications
  Using HyperMapper","  In this paper we investigate an emerging application, 3D scene understanding,
likely to be significant in the mobile space in the near future. The goal of
this exploration is to reduce execution time while meeting our quality of
result objectives. In previous work we showed for the first time that it is
possible to map this application to power constrained embedded systems,
highlighting that decision choices made at the algorithmic design-level have
the most impact.
  As the algorithmic design space is too large to be exhaustively evaluated, we
use a previously introduced multi-objective Random Forest Active Learning
prediction framework dubbed HyperMapper, to find good algorithmic designs. We
show that HyperMapper generalizes on a recent cutting edge 3D scene
understanding algorithm and on a modern GPU-based computer architecture.
HyperMapper is able to beat an expert human hand-tuning the algorithmic
parameters of the class of Computer Vision applications taken under
consideration in this paper automatically. In addition, we use crowd-sourcing
using a 3D scene understanding Android app to show that the Pareto front
obtained on an embedded system can be used to accelerate the same application
on all the 83 smart-phones and tablets crowd-sourced with speedups ranging from
2 to over 12.
"
1105,gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms,"  Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields
ranging from computer science to natural sciences and engineering. With the
rising data production bandwidths of modern FFT applications, judging best
which algorithmic tool to apply, can be vital to any scientific endeavor. As
tailored FFT implementations exist for an ever increasing variety of high
performance computer hardware, choosing the best performing FFT implementation
has strong implications for future hardware purchase decisions, for resources
FFTs consume and for possibly decisive financial and time savings ahead of the
competition. This paper therefor presents gearshifft, which is an open-source
and vendor agnostic benchmark suite to process a wide variety of problem sizes
and types with state-of-the-art FFT implementations (fftw, clfft and cufft).
gearshifft provides a reproducible, unbiased and fair comparison on a wide
variety of hardware to explore which FFT variant is best for a given problem
size.
"
1106,"A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse
  Gaussian Graphical Models","  Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many
related tasks (large $K$) under a high-dimensional (large $p$) situation is an
important task. Most previous studies for the joint estimation of multiple
sGGMs rely on penalized log-likelihood estimators that involve expensive and
difficult non-smooth optimizations. We propose a novel approach, FASJEM for
\underline{fa}st and \underline{s}calable \underline{j}oint
structure-\underline{e}stimation of \underline{m}ultiple sGGMs at a large
scale. As the first study of joint sGGM using the Elementary Estimator
framework, our work has three major contributions: (1) We solve FASJEM through
an entry-wise manner which is parallelizable. (2) We choose a proximal
algorithm to optimize FASJEM. This improves the computational efficiency from
$O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to
$O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation
with a convergence rate of $O(\log(Kp)/n_{tot})$. On several synthetic and four
real-world datasets, FASJEM shows significant improvements over baselines on
accuracy, computational complexity, and memory costs.
"
1107,"Comparative benchmarking of cloud computing vendors with High
  Performance Linpack","  We present a comparative analysis of the maximum performance achieved by the
Linpack benchmark on compute intensive hardware publicly available from
multiple cloud providers. We study both performance within a single compute
node, and speedup for distributed memory calculations with up to 32 nodes or at
least 512 computing cores. We distinguish between hyper-threaded and
non-hyper-threaded scenarios and estimate the performance per single computing
core. We also compare results with a traditional supercomputing system for
reference. Our findings provide a way to rank the cloud providers and
demonstrate the viability of the cloud for high performance computing
applications.
"
1108,"Steady-state analysis of single exponential vacation in a
  $PH/MSP/1/\infty$ queue using roots","  We consider an infinite-buffer single-server queue where inter-arrival times
are phase-type ($PH$), the service is provided according to Markovian service
process $(MSP)$, and the server may take single, exponentially distributed
vacations when the queue is empty. The proposed analysis is based on roots of
the associated characteristic equation of the vector-generating function (VGF)
of system-length distribution at a pre-arrival epoch. Also, we obtain the
steady-state system-length distribution at an arbitrary epoch along with some
important performance measures such as the mean number of customers in the
system and the mean system sojourn time of a customer. Later, we have
established heavy- and light-traffic approximations as well as an approximation
for the tail probabilities at pre-arrival epoch based on one root of the
characteristic equation. At the end, we present numerical results in the form
of tables to show the effect of model parameters on the performance measures.
"
1109,LAMMPS' PPPM Long-Range Solver for the Second Generation Xeon Phi,"  Molecular Dynamics is an important tool for computational biologists,
chemists, and materials scientists, consuming a sizable amount of
supercomputing resources. Many of the investigated systems contain charged
particles, which can only be simulated accurately using a long-range solver,
such as PPPM. We extend the popular LAMMPS molecular dynamics code with an
implementation of PPPM particularly suitable for the second generation Intel
Xeon Phi. Our main target is the optimization of computational kernels by means
of vectorization, and we observe speedups in these kernels of up to 12x. These
improvements carry over to LAMMPS users, with overall speedups ranging between
2-3x, without requiring users to retune input parameters. Furthermore, our
optimizations make it easier for users to determine optimal input parameters
for attaining top performance.
"
1110,Kerncraft: A Tool for Analytic Performance Modeling of Loop Kernels,"  Achieving optimal program performance requires deep insight into the
interaction between hardware and software. For software developers without an
in-depth background in computer architecture, understanding and fully utilizing
modern architectures is close to impossible. Analytic loop performance modeling
is a useful way to understand the relevant bottlenecks of code execution based
on simple machine models. The Roofline Model and the Execution-Cache-Memory
(ECM) model are proven approaches to performance modeling of loop nests. In
comparison to the Roofline model, the ECM model can also describes the
single-core performance and saturation behavior on a multicore chip. We give an
introduction to the Roofline and ECM models, and to stencil performance
modeling using layer conditions (LC). We then present Kerncraft, a tool that
can automatically construct Roofline and ECM models for loop nests by
performing the required code, data transfer, and LC analysis. The layer
condition analysis allows to predict optimal spatial blocking factors for loop
nests. Together with the models it enables an ab-initio estimate of the
potential benefits of loop blocking optimizations and of useful block sizes. In
cases where LC analysis is not easily possible, Kerncraft supports a cache
simulator as a fallback option. Using a 25-point long-range stencil we
demonstrate the usefulness and predictive power of the Kerncraft tool.
"
1111,"Benchmarking the computing resources at the Instituto de Astrof\'isica
  de Canarias","  The aim of this study is the characterization of the computing resources used
by researchers at the ""Instituto de Astrof\'isica de Canarias"" (IAC). Since
there is a huge demand of computing time and we use tools such as HTCondor to
implement High Throughput Computing (HTC) across all available PCs, it is
essential for us to assess in a quantitative way, using objective parameters,
the performances of our computing nodes. In order to achieve that, we have run
a set of benchmark tests on a number of different desktop and laptop PC models
among those used in our institution. In particular, we run the ""Polyhedron
Fortran Benchmarks"" suite, using three different compilers: GNU Fortran
Compiler, Intel Fortran Compiler and the PGI Fortran Compiler; execution times
are then normalized to the reference values published by Polyhedron. The same
tests were run multiple times on a same PCs, and on 3 to 5 PCs of the same
model (whenever possible) to check for repeatability and consistency of the
results. We found that in general execution times, for a given PC model, are
consistent within an uncertainty of about 10%, and show a gain in CPU speed of
a factor of about 3 between the oldest PCs used at the IAC (7-8 years old) and
the newest ones.
"
1112,"First Experiences Optimizing Smith-Waterman on Intel's Knights Landing
  Processor","  The well-known Smith-Waterman (SW) algorithm is the most commonly used method
for local sequence alignments. However, SW is very computationally demanding
for large protein databases. There exist several implementations that take
advantage of computing parallelization on many-cores, FPGAs or GPUs, in order
to increase the alignment throughtput. In this paper, we have explored SW
acceleration on Intel KNL processor. The novelty of this architecture requires
the revision of previous programming and optimization techniques on many-core
architectures. To the best of authors knowledge, this is the first KNL
architecture assessment for SW algorithm. Our evaluation, using the renowned
Environmental NR database as benchmark, has shown that multi-threading and SIMD
exploitation reports competitive performance (351 GCUPS) in comparison with
other implementations.
"
1113,"An analysis of core- and chip-level architectural features in four
  generations of Intel server processors","  This paper presents a survey of architectural features among four generations
of Intel server processors (Sandy Bridge, Ivy Bridge, Haswell, and Broad- well)
with a focus on performance with floating point workloads. Starting on the core
level and going down the memory hierarchy we cover instruction throughput for
floating-point instructions, L1 cache, address generation capabilities, core
clock speed and its limitations, L2 and L3 cache bandwidth and latency, the
impact of Cluster on Die (CoD) and cache snoop modes, and the Uncore clock
speed. Using microbenchmarks we study the influence of these factors on code
performance. This insight can then serve as input for analytic performance
models. We show that the energy efficiency of the LINPACK and HPCG benchmarks
can be improved considerably by tuning the Uncore clock speed without
sacrificing performance, and that the Graph500 benchmark performance may profit
from a suitable choice of cache snoop mode settings.
"
1114,Wireless Node Cooperation with Resource Availability Constraints,"  Base station cooperation is a promising scheme to improve network performance
for next generation cellular networks. Up to this point research has focused on
station grouping criteria based solely on geographic proximity. However, for
the cooperation to be meaningful, each station participating in a group should
have sufficient available resources to share with others. In this work we
consider an alternative grouping criterion based on a distance that considers
both geographic proximity and available resources of the stations. When the
network is modelled by a Poisson Point Process, we derive analytical formulas
on the proportion of cooperative pairs or single stations, and the expected sum
interference from each of the groups. The results illustrate that cooperation
gains strongly depend on the distribution of available resources over the
network.
"
1115,"Evaluation of DVFS techniques on modern HPC processors and accelerators
  for energy-aware applications","  Energy efficiency is becoming increasingly important for computing systems,
in particular for large scale HPC facilities. In this work we evaluate, from an
user perspective, the use of Dynamic Voltage and Frequency Scaling (DVFS)
techniques, assisted by the power and energy monitoring capabilities of modern
processors in order to tune applications for energy efficiency. We run selected
kernels and a full HPC application on two high-end processors widely used in
the HPC context, namely an NVIDIA K80 GPU and an Intel Haswell CPU. We evaluate
the available trade-offs between energy-to-solution and time-to-solution,
attempting a function-by-function frequency tuning. We finally estimate the
benefits obtainable running the full code on a HPC multi-GPU node, with respect
to default clock frequency governors. We instrument our code to accurately
monitor power consumption and execution time without the need of any additional
hardware, and we enable it to change CPUs and GPUs clock frequencies while
running. We analyze our results on the different architectures using a simple
energy-performance model, and derive a number of energy saving strategies which
can be easily adopted on recent high-end HPC systems for generic applications.
"
1116,Redundancy Suppression In Time-Aware Dynamic Binary Instrumentation,"  Software tracing techniques are well-established and used by instrumentation
tools to extract run-time information for program analysis and debugging.
Dynamic binary instrumentation as one tool instruments program binaries to
extract information. Unfortunately, instrumentation causes perturbation that is
unacceptable for time-sensitive applications. Consequently we developed DIME*,
a tool for dynamic binary instrumentation that considers timing constraints.
DIME* uses Pin and a rate-based server approach to extract information only as
long as user-specified constraints are maintained. Due to the large amount of
redundancies in program traces, DIME* reduces the instrumentation overhead by
one to three orders of magnitude compared to native Pin while extracting up to
99% of the information. We instrument VLC and PostgreSQL to demonstrate the
usability of DIME*.
"
1117,A Visual Web Tool to Perform What-If Analysis of Optimization Approaches,"  In Operation Research, practical evaluation is essential to validate the
efficacy of optimization approaches. This paper promotes the usage of
performance profiles as a standard practice to visualize and analyze
experimental results. It introduces a Web tool to construct and export
performance profiles as SVG or HTML files. In addition, the application relies
on a methodology to estimate the benefit of hypothetical solver improvements.
Therefore, the tool allows one to employ what-if analysis to screen possible
research directions, and identify those having the best potential. The approach
is showcased on two Operation Research technologies: Constraint Programming and
Mixed Integer Linear Programming.
"
1118,CLTune: A Generic Auto-Tuner for OpenCL Kernels,"  This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and
tunes kernel performance of a generic, user-defined search space of possible
parameter-value combinations. Example parameters include the OpenCL workgroup
size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be
used in the following scenarios: 1) when there are too many tunable parameters
to explore manually, 2) when performance portability across OpenCL devices is
desired, or 3) when the optimal parameters change based on input argument
values (e.g. matrix dimensions). The auto-tuner is generic, easy to use,
open-source, and supports multiple search strategies including simulated
annealing and particle swarm optimisation. CLTune is evaluated on two GPU
case-studies inspired by the recent successes in deep learning: 2D convolution
and matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need
for auto-tuning by optimizing for different filter sizes, achieving performance
on-par or better than the state-of-the-art. For matrix-multiplication, we use
CLTune to explore a parameter space of more than two-hundred thousand
configurations, we show the need for device-specific tuning, and outperform the
clBLAS library on NVIDIA, AMD and Intel GPUs.
"
1119,Flare: Native Compilation for Heterogeneous Workloads in Apache Spark,"  The need for modern data analytics to combine relational, procedural, and
map-reduce-style functional processing is widely recognized. State-of-the-art
systems like Spark have added SQL front-ends and relational query optimization,
which promise an increase in expressiveness and performance. But how good are
these extensions at extracting high performance from modern hardware platforms?
  While Spark has made impressive progress, we show that for relational
workloads, there is still a significant gap compared with best-of-breed query
engines. And when stepping outside of the relational world, query optimization
techniques are ineffective if large parts of a computation have to be treated
as user-defined functions (UDFs).
  We present Flare: a new back-end for Spark that brings performance closer to
the best SQL engines, without giving up the added expressiveness of Spark. We
demonstrate order of magnitude speedups both for relational workloads such as
TPC-H, as well as for a range of machine learning kernels that combine
relational and iterative functional processing.
  Flare achieves these results through (1) compilation to native code, (2)
replacing parts of the Spark runtime system, and (3) extending the scope of
optimization and code generation to large classes of UDFs.
"
1120,"Automatically Tuning the GCC Compiler to Optimize the Performance of
  Applications Running on Embedded Systems","  This paper introduces a novel method for automatically tuning the selection
of compiler flags to optimize the performance of software intended to run on
embedded hardware platforms. We begin by developing our approach on code
compiled by the GNU C Compiler (GCC) for the ARM Cortex-M3 (CM3) processor; and
we show how our method outperforms the industry standard -O3 optimization level
across a diverse embedded benchmark suite. First we quantify the potential
gains by using existing iterative compilation approaches that time-intensively
search for optimal configurations for each benchmark. Then we adapt iterative
compilation to output a single configuration that optimizes performance across
the entire benchmark suite. Although this is a time-consuming process, our
approach constructs an optimized variation of -O3, which we call -Ocm3, that
realizes nearly two thirds of known available gains on the CM3 and
significantly outperforms a more complex state-of-the-art predictive method in
cross-validation experiments. Finally, we demonstrate our method on additional
platforms by constructing two more optimization levels that find even more
significant speed-ups on the ARM Cortex-A8 and 8-bit AVR processors.
"
1121,Optimal Service Elasticity in Large-Scale Distributed Systems,"  A fundamental challenge in large-scale cloud networks and data centers is to
achieve highly efficient server utilization and limit energy consumption, while
providing excellent user-perceived performance in the presence of uncertain and
time-varying demand patterns. Auto-scaling provides a popular paradigm for
automatically adjusting service capacity in response to demand while meeting
performance targets, and queue-driven auto-scaling techniques have been widely
investigated in the literature. In typical data center architectures and cloud
environments however, no centralized queue is maintained, and load balancing
algorithms immediately distribute incoming tasks among parallel queues. In
these distributed settings with vast numbers of servers, centralized
queue-driven auto-scaling techniques involve a substantial communication
overhead and major implementation burden, or may not even be viable at all.
  Motivated by the above issues, we propose a joint auto-scaling and load
balancing scheme which does not require any global queue length information or
explicit knowledge of system parameters, and yet provides provably near-optimal
service elasticity. We establish the fluid-level dynamics for the proposed
scheme in a regime where the total traffic volume and nominal service capacity
grow large in proportion. The fluid-limit results show that the proposed scheme
achieves asymptotic optimality in terms of user-perceived delay performance as
well as energy consumption. Specifically, we prove that both the waiting time
of tasks and the relative energy portion consumed by idle servers vanish in the
limit. At the same time, the proposed scheme operates in a distributed fashion
and involves only constant communication overhead per task, thus ensuring
scalability in massive data center operations.
"
1122,"Groups of Repairmen and Repair-based Load Balancing in Supermarket
  Models with Repairable Servers","  Supermarket models are a class of interesting parallel queueing networks with
dynamic randomized load balancing and real-time resource management. When the
parallel servers are subject to breakdowns and repairs, analysis of such a
supermarket model becomes more difficult and challenging. In this paper, we
apply the mean-field theory to studying four interrelated supermarket models
with repairable servers, and numerically indicate impact of the different
repairman groups on performance of the systems. First, we set up the systems of
mean-field equations for the supermarket models with repairable servers. Then
we prove the asymptotic independence of the supermarket models through the
operator semi-group and the mean-field limit. Furthermore, we show that the
fixed points of the supermarket models satisfy the systems of nonlinear
equations. Finally, we use the fixed points to give numerical computation for
performer analysis, and provide valuable observations on model improvement.
Therefore, this paper provides a new and effective method in the study of
complex supermarket models.
"
1123,"Optimizing Communication by Compression for Multi-GPU Scalable
  Breadth-First Searches","  The Breadth First Search (BFS) algorithm is the foundation and building block
of many higher graph-based operations such as spanning trees, shortest paths
and betweenness centrality. The importance of this algorithm increases each day
due to it is a key requirement for many data structures which are becoming
popular nowadays. When the BFS algorithm is parallelized by distributing the
graph between several processors the interconnection network limits the
performance. Hence, improvements on this area may benefit the overall
performance of the algorithm.
  This work presents an alternative compression scheme for communications in
distributed BFS processing. It focuses on BFS processors using General-Purpose
Graphics Processing Units.
"
1124,Faster Base64 Encoding and Decoding Using AVX2 Instructions,"  Web developers use base64 formats to include images, fonts, sounds and other
resources directly inside HTML, JavaScript, JSON and XML files. We estimate
that billions of base64 messages are decoded every day. We are motivated to
improve the efficiency of base64 encoding and decoding. Compared to
state-of-the-art implementations, we multiply the speeds of both the encoding
(~10x) and the decoding (~7x). We achieve these good results by using the
single-instruction-multiple-data (SIMD) instructions available on recent Intel
processors (AVX2). Our accelerated software abides by the specification and
reports errors when encountering characters outside of the base64 set. It is
available online as free software under a liberal license.
"
1125,Towards a property graph generator for benchmarking,"  The use of synthetic graph generators is a common practice among
graph-oriented benchmark designers, as it allows obtaining graphs with the
required scale and characteristics. However, finding a graph generator that
accurately fits the needs of a given benchmark is very difficult, thus
practitioners end up creating ad-hoc ones. Such a task is usually
time-consuming, and often leads to reinventing the wheel. In this paper, we
introduce the conceptual design of DataSynth, a framework for property graphs
generation with customizable schemas and characteristics. The goal of DataSynth
is to assist benchmark designers in generating graphs efficiently and at scale,
saving from implementing their own generators. Additionally, DataSynth
introduces novel features barely explored so far, such as modeling the
correlation between properties and the structure of the graph. This is achieved
by a novel property-to-node matching algorithm for which we present preliminary
promising results.
"
1126,Loop Tiling in Large-Scale Stencil Codes at Run-time with OPS,"  The key common bottleneck in most stencil codes is data movement, and prior
research has shown that improving data locality through optimisations that
schedule across loops do particularly well. However, in many large PDE
applications it is not possible to apply such optimisations through compilers
because there are many options, execution paths and data per grid point, many
dependent on run-time parameters, and the code is distributed across different
compilation units. In this paper, we adapt the data locality improving
optimisation called iteration space slicing for use in large OPS applications
both in shared-memory and distributed-memory systems, relying on run-time
analysis and delayed execution. We evaluate our approach on a number of
applications, observing speedups of 2$\times$ on the Cloverleaf 2D/3D proxy
application, which contain 83/141 loops respectively, $3.5\times$ on the linear
solver TeaLeaf, and $1.7\times$ on the compressible Navier-Stokes solver
OpenSBLI. We demonstrate strong and weak scalability up to 4608 cores of
CINECA's Marconi supercomputer. We also evaluate our algorithms on Intel's
Knights Landing, demonstrating maintained throughput as the problem size grows
beyond 16GB, and we do scaling studies up to 8704 cores. The approach is
generally applicable to any stencil DSL that provides per loop data access
information.
"
1127,"On the equivalence between multiclass processor sharing and random order
  scheduling policies","  Consider a single server system serving a multiclass population. Some popular
scheduling policies for such system are the discriminatory processor sharing
(DPS), discriminatory random order service (DROS), generalized processor
sharing (GPS) and weighted fair queueing (WFQ). In this paper, we propose two
classes of policies, namely MPS (multiclass processor sharing) and MROS
(multiclass random order service), that generalize the four policies mentioned
above. For the special case when the multiclass population arrive according to
Poisson processes and have independent and exponential service requirement with
parameter $\mu$, we show that the tail of the sojourn time distribution for a
class $i$ customer in a system with the MPS policy is a constant multiple of
the tail of the waiting time distribution of a class $i$ customer in a system
with the MROS policy. This result implies that for a class $i$ customer, the
tail of the sojourn time distribution in a system with the DPS (GPS) scheduling
policy is a constant multiple of the tail of the waiting time distribution in a
system with the DROS (respectively WFQ) policy.
"
1128,A Comparison of Parallel Graph Processing Implementations,"  The rapidly growing number of large network analysis problems has led to the
emergence of many parallel and distributed graph processing systems---one
survey in 2014 identified over 80. Since then, the landscape has evolved; some
packages have become inactive while more are being developed. Determining the
best approach for a given problem is infeasible for most developers. To enable
easy, rigorous, and repeatable comparison of the capabilities of such systems,
we present an approach and associated software for analyzing the performance
and scalability of parallel, open-source graph libraries. We demonstrate our
approach on five graph processing packages: GraphMat, the Graph500, the Graph
Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic
and real-world datasets. We examine previously overlooked aspects of parallel
graph processing performance such as phases of execution and energy usage for
three algorithms: breadth first search, single source shortest paths, and
PageRank and compare our results to Graphalytics.
"
1129,"Performance Analysis of Reliable Video Streaming with Strict Playout
  Deadline in Multi-Hop Wireless Networks","  Motivated by emerging vision-based intelligent services, we consider the
problem of rate adaptation for high quality and low delay visual information
delivery over wireless networks using scalable video coding. Rate adaptation in
this setting is inherently challenging due to the interplay between the
variability of the wireless channels, the queuing at the network nodes and the
frame-based decoding and playback of the video content at the receiver at very
short time scales. To address the problem, we propose a low-complexity,
model-based rate adaptation algorithm for scalable video streaming systems,
building on a novel performance model based on stochastic network calculus. We
validate the model using extensive simulations. We show that it allows fast,
near optimal rate adaptation for fixed transmission paths, as well as
cross-layer optimized routing and video rate adaptation in mesh networks, with
less than $10$\% quality degradation compared to the best achievable
performance.
"
1130,"A Decision Tree Based Approach Towards Adaptive Profiling of Distributed
  Applications","  The adoption of the distributed paradigm has allowed applications to increase
their scalability, robustness and fault tolerance, but it has also complicated
their structure, leading to an exponential growth of the applications'
configuration space and increased difficulty in predicting their performance.
In this work, we describe a novel, automated profiling methodology that makes
no assumptions on application structure. Our approach utilizes oblique Decision
Trees in order to recursively partition an application's configuration space in
disjoint regions, choose a set of representative samples from each subregion
according to a defined policy and return a model for the entire space as a
composition of linear models over each subregion. An extensive evaluation over
real-life applications and synthetic performance functions showcases that our
scheme outperforms other state-of-the-art profiling methodologies. It
particularly excels at reflecting abnormalities and discontinuities of the
performance function, allowing the user to influence the sampling policy based
on the modeling accuracy and the space coverage.
"
1131,ROSA: R Optimizations with Static Analysis,"  R is a popular language and programming environment for data scientists. It
is increasingly co-packaged with both relational and Hadoop-based data
platforms and can often be the most dominant computational component in data
analytics pipelines. Recent work has highlighted inefficiencies in executing R
programs, both in terms of execution time and memory requirements, which in
practice limit the size of data that can be analyzed by R. This paper presents
ROSA, a static analysis framework to improve the performance and space
efficiency of R programs. ROSA analyzes input programs to determine program
properties such as reaching definitions, live variables, aliased variables, and
types of variables. These inferred properties enable program transformations
such as C++ code translation, strength reduction, vectorization, code motion,
in addition to interpretive optimizations such as avoiding redundant object
copies and performing in-place evaluations. An empirical evaluation shows
substantial reductions by ROSA in execution time and memory consumption over
both CRAN R and Microsoft R Open.
"
1132,"Best-by-Simulations: A Framework for Comparing Efficiency of
  Reconfigurable Multicore Architectures on Workloads with Deadlines","  Energy consumption is a major concern in multicore systems. Perhaps the
simplest strategy for reducing energy costs is to use only as many cores as
necessary while still being able to deliver a desired quality of service.
Motivated by earlier work on a dynamic (heterogeneous) core allocation scheme
for H.264 video decoding that reduces energy costs while delivering desired
frame rates, we formulate operationally the general problem of executing a
sequence of actions on a reconfigurable machine while meeting a corresponding
sequence of absolute deadlines, with the objective of reducing cost. Using a
transition system framework that associates costs (e.g., time, energy) with
executing an action on a particular resource configuration, we use the notion
of amortised cost to formulate in terms of simulation relations appropriate
notions for comparing deadline-conformant executions. We believe these notions
can provide the basis for an operational theory of optimal cost executions and
performance guarantees for approximate solutions, in particular relating the
notion of simulation from transition systems to that of competitive analysis
used for, e.g., online algorithms.
"
1133,"Persistent Spread Measurement for Big Network Data Based on Register
  Intersection","  Persistent spread measurement is to count the number of distinct elements
that persist in each network flow for predefined time periods. It has many
practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS
attack, stealthy network scan, or faked network trend, which cannot be detected
by traditional flow cardinality measurement. With big network data, one
challenge is to measure the persistent spreads of a massive number of flows
without incurring too much memory overhead as such measurement may be performed
at the line speed by network processors with fast but small on-chip memory. We
propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture
for this purpose. It achieves far better memory efficiency than the best prior
work of V-Bitmap, and in the meantime drastically extends the measurement
range. Theoretical analysis and extensive experiments demonstrate that VI-HLL
provides good measurement accuracy even in very tight memory space of less than
1 bit per flow.
"
1134,"Fully Distributed and Asynchronized Stochastic Gradient Descent for
  Networked Systems","  This paper considers a general data-fitting problem over a networked system,
in which many computing nodes are connected by an undirected graph. This kind
of problem can find many real-world applications and has been studied
extensively in the literature. However, existing solutions either need a
central controller for information sharing or requires slot synchronization
among different nodes, which increases the difficulty of practical
implementations, especially for a very large and heterogeneous system.
  As a contrast, in this paper, we treat the data-fitting problem over the
network as a stochastic programming problem with many constraints. By adapting
the results in a recent paper, we design a fully distributed and asynchronized
stochastic gradient descent (SGD) algorithm. We show that our algorithm can
achieve global optimality and consensus asymptotically by only local
computations and communications. Additionally, we provide a sharp lower bound
for the convergence speed in the regular graph case. This result fits the
intuition and provides guidance to design a `good' network topology to speed up
the convergence. Also, the merit of our design is validated by experiments on
both synthetic and real-world datasets.
"
1135,"CBinfer: Change-Based Inference for Convolutional Neural Networks on
  Video Data","  Extracting per-frame features using convolutional neural networks for
real-time processing of video data is currently mainly performed on powerful
GPU-accelerated workstations and compute clusters. However, there are many
applications such as smart surveillance cameras that require or would benefit
from on-site processing. To this end, we propose and evaluate a novel algorithm
for change-based evaluation of CNNs for video data recorded with a static
camera setting, exploiting the spatio-temporal sparsity of pixel changes. We
achieve an average speed-up of 8.6x over a cuDNN baseline on a realistic
benchmark with a negligible accuracy loss of less than 0.1% and no retraining
of the network. The resulting energy efficiency is 10x higher than that of
per-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1
platform.
"
1136,HPTT: A High-Performance Tensor Transposition C++ Library,"  Recently we presented TTC, a domain-specific compiler for tensor
transpositions. Despite the fact that the performance of the generated code is
nearly optimal, due to its offline nature, TTC cannot be utilized in all the
application codes in which the tensor sizes and the necessary tensor
permutations are determined at runtime. To overcome this limitation, we
introduce the open-source C++ library High-Performance Tensor Transposition
(HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking,
multi-threading, and explicit vectorization; furthermore it decomposes any
transposition into multiple loops around a so called micro-kernel. This modular
design---inspired by BLIS---makes HPTT easy to port to different architectures,
by only replacing the hand-vectorized micro-kernel (e.g., a 4x4 transpose).
HPTT also offers an optional autotuning framework---guided by a performance
model---that explores a vast search space of implementations at runtime
(similar to FFTW). Across a wide range of different tensor transpositions and
architectures (e.g., Intel Ivy Bridge, Intel Knights Landing, ARMv7, IBM
Power7), HPTT attains a bandwidth comparable to that of SAXPY, and yields
remarkable speedups over Eigen's tensor transposition implementation. Most
importantly, the integration of HPTT into the Cyclops Tensor Framework (CTF)
improves the overall performance of tensor contractions by up to 3.1x.
"
1137,Parallel Multi Channel Convolution using General Matrix Multiplication,"  Convolutional neural networks (CNNs) have emerged as one of the most
successful machine learning technologies for image and video processing. The
most computationally intensive parts of CNNs are the convolutional layers,
which convolve multi-channel images with multiple kernels. A common approach to
implementing convolutional layers is to expand the image into a column matrix
(im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using
an existing parallel General Matrix Multiplication (GEMM) library. This im2col
conversion greatly increases the memory footprint of the input matrix and
reduces data locality.
  In this paper we propose a new approach to MCMK convolution that is based on
General Matrix Multiplication (GEMM), but not on im2col. Our algorithm
eliminates the need for data replication on the input thereby enabling us to
apply the convolution kernels on the input images directly. We have implemented
several variants of our algorithm on a CPU processor and an embedded ARM
processor. On the CPU, our algorithm is faster than im2col in most cases.
"
1138,Stationary Distribution of a Generalized LRU-MRU Content Cache,"  Many different caching mechanisms have been previously proposed, exploring
different insertion and eviction policies and their performance individually
and as part of caching networks. We obtain a novel closed-form stationary
invariant distribution for a generalization of LRU and MRU caching nodes under
a reference Markov model. Numerical comparisons are made with an ""Incremental
Rank Progress"" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random
replacement) methods under a steady-state Zipf popularity distribution. The
range of cache hit probabilities is smaller under MRU and larger under IRP
compared to LRU. We conclude with the invariant distribution for a special case
of a random-eviction caching tree-network and associated discussion.
"
1139,"On the Capacity Requirement for Arbitrary End-to-End Deadline and
  Reliability Guarantees in Multi-hop Networks","  It has been shown that it is impossible to achieve both stringent end-to-end
deadline and reliability guarantees in a large network without having complete
information of all future packet arrivals. In order to maintain desirable
performance in the presence of uncertainty of future packet arrivals, common
practice is to add redundancy by increasing link capacities. This paper studies
the amount of capacity needed to provide stringent performance guarantees. We
propose a low-complexity online algorithm and prove that it only requires a
small amount of redundancy to guarantee both end-to-end deadline and
reliability. Further, we show that in large networks with very high reliability
requirements, the redundancy needed by our policy is at most twice as large as
a theoretical lower bound. Also, for practical implementation, we propose a
fully distributed protocol based on the previous centralized policy. Without
adding redundancy, we further propose a low-complexity order-optimal online
policy for the network. Simulation results also show that our policy achieves
much better performance than other state-of-the-art policies.
"
1140,"Benchmarking OpenCL, OpenACC, OpenMP, and CUDA: programming
  productivity, performance, and energy consumption","  Many modern parallel computing systems are heterogeneous at their node level.
Such nodes may comprise general purpose CPUs and accelerators (such as, GPU, or
Intel Xeon Phi) that provide high performance with suitable energy-consumption
characteristics. However, exploiting the available performance of heterogeneous
architectures may be challenging. There are various parallel programming
frameworks (such as, OpenMP, OpenCL, OpenACC, CUDA) and selecting the one that
is suitable for a target context is not straightforward.
  In this paper, we study empirically the characteristics of OpenMP, OpenACC,
OpenCL, and CUDA with respect to programming productivity, performance, and
energy. To evaluate the programming productivity we use our homegrown tool
CodeStat, which enables us to determine the percentage of code lines that was
required to parallelize the code using a specific framework. We use our tool
x-MeterPU to evaluate the energy consumption and the performance. Experiments
are conducted using the industry-standard SPEC benchmark suite and the Rodinia
benchmark suite for accelerated computing on heterogeneous systems that combine
Intel Xeon E5 Processors with a GPU accelerator or an Intel Xeon Phi
co-processor.
"
1141,Testing Docker Performance for HPC Applications,"  The main goal for this article is to compare performance penalties when using
KVM virtualization and Docker containers for creating isolated environments for
HPC applications. The article provides both data obtained using commonly
accepted synthetic tests (High Performance Linpack) and real life applications
(OpenFOAM). The article highlights the influence on resulting application
performance of major infrastructure configuration options: CPU type presented
to VM, networking connection type used.
"
1142,A note on integrating products of linear forms over the unit simplex,"  Integrating a product of linear forms over the unit simplex can be done in
polynomial time if the number of variables n is fixed (V. Baldoni et al.,
2011). In this note, we highlight that this problem is equivalent to obtaining
the normalizing constant of state probabilities for a popular class of Markov
processes used in queueing network theory. In light of this equivalence, we
survey existing computational algorithms developed in queueing theory that can
be used for exact integration. For example, under some regularity conditions,
queueing theory algorithms can exactly integrate a product of linear forms of
total degree N by solving N systems of linear equations.
"
1143,Optimizing Mission Critical Data Dissemination in Massive IoT Networks,"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
"
1144,Accelerated Nearest Neighbor Search with Quick ADC,"  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a
foundation of many multimedia retrieval systems. Because it offers low
responses times, Product Quantization (PQ) is a popular solution. PQ compresses
high-dimensional vectors into short codes using several sub-quantizers, which
enables in-RAM storage of large databases. This allows fast answers to NN
queries, without accessing the SSD or HDD. The key feature of PQ is that it can
compute distances between short codes and high-dimensional vectors using
cache-resident lookup tables. The efficiency of this technique, named
Asymmetric Distance Computation (ADC), remains limited because it performs many
cache accesses.
  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to
6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)
units available in current CPUs. Efficiently exploiting SIMD requires
algorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key
modifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard
8-bit sub-quantizers and (ii) the quantization of floating-point distances.
This allows Quick ADC to exceed the performance of state-of-the-art systems,
e.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors
(128-bit codes).
"
1145,Accelerating Discrete Wavelet Transforms on Parallel Architectures,"  The 2-D discrete wavelet transform (DWT) can be found in the heart of many
image-processing algorithms. Until recently, several studies have compared the
performance of such transform on various shared-memory parallel architectures,
especially on graphics processing units (GPUs). All these studies, however,
considered only separable calculation schemes. We show that corresponding
separable parts can be merged into non-separable units, which halves the number
of steps. In addition, we introduce an optional optimization approach leading
to a reduction in the number of arithmetic operations. The discussed schemes
were adapted on the OpenCL framework and pixel shaders, and then evaluated
using GPUs of two biggest vendors. We demonstrate the performance of the
proposed non-separable methods by comparison with existing separable schemes.
The non-separable schemes outperform their separable counterparts on numerous
setups, especially considering the pixel shaders.
"
1146,"Correcting for Non-Markovian Asymptotic Effects using Markovian
  Representation","  Asymptotic properties of Markov Processes, such as steady state probabilities
or hazard rate for absorbing states can be efficiently calculated by means of
linear algebra even for large-scale problems. This paper discusses the methods
for adjusting parameters of the Markov models to account for non-constant
transition rates. In particular, transitions with fixed delays are considered
along with the transitions that follow Weibull and lognormal distributions.
Procedures for both steady-state solutions in the absence of an absorbing
state, and for hazard rates to an absorbing state are provided and demonstrated
on several examples.
"
1147,"How does Docker affect energy consumption? Evaluating workloads in and
  out of Docker containers","  Context: Virtual machines provide isolation of services at the cost of
hypervisors and more resource usage. This spurred the growth of systems like
Docker that enable single hosts to isolate several applications, similar to
VMs, within a low-overhead abstraction called containers.
  Motivation: Although containers tout low overhead performance, do they still
have low energy consumption?
  Methodology: This work statistically compares ($t$-test, Wilcoxon) the energy
consumption of three application workloads in Docker and on bare-metal Linux.
  Results: In all cases, there was a statistically significant ($t$-test and
Wilcoxon $p < 0.05$) increase in energy consumption when running tests in
Docker, mostly due to the performance of I/O system calls.
"
1148,"Affinity Scheduling and the Applications on Data Center Scheduling with
  Data Locality","  MapReduce framework is the de facto standard in Hadoop. Considering the data
locality in data centers, the load balancing problem of map tasks is a special
case of affinity scheduling problem. There is a huge body of work on affinity
scheduling, proposing heuristic algorithms which try to increase data locality
in data centers like Delay Scheduling and Quincy. However, not enough attention
has been put on theoretical guarantees on throughput and delay optimality of
such algorithms. In this work, we present and compare different algorithms and
discuss their shortcoming and strengths. To the best of our knowledge, most
data centers are using static load balancing algorithms which are not efficient
in any ways and results in wasting the resources and causing unnecessary delays
for users.
"
1149,Scheduling Distributed Resources in Heterogeneous Private Clouds,"  We first consider the static problem of allocating resources to ( i.e. ,
scheduling) multiple distributed application framework s, possibly with
different priorities and server preferences , in a private cloud with
heterogeneous servers. Several fai r scheduling mechanisms have been proposed
for this purpose. We extend pr ior results on max-min and proportional fair
scheduling to t his constrained multiresource and multiserver case for generi c
fair scheduling criteria. The task efficiencies (a metric r elated to
proportional fairness) of max-min fair allocations found b y progressive
filling are compared by illustrative examples . They show that ""server
specific"" fairness criteria and those that are b ased on residual (unreserved)
resources are more efficient.
"
1150,Spin Summations: A High-Performance Perspective,"  Besides tensor contractions, one of the most pronounced computational
bottlenecks in the non-orthogonally spin-adapted forms of the quantum chemistry
methods CCSDT and CCSDTQ, and their approximate forms---including CCSD(T) and
CCSDT(Q)---are spin summations. At a first sight, spin summations are
operations similar to tensor transpositions; a closer look instead reveals
additional challenges to high-performance calculations, including temporal
locality as well as scattered memory accesses. This publication explores a
sequence of algorithmic solutions for spin summations, each exploiting
individual properties of either the underlying hardware (e.g. caches,
vectorization), or the problem itself (e.g. factorizability). The final
algorithm combines the advantages of all the solutions, while avoiding their
drawbacks; this algorithm, achieves high-performance through parallelization,
vectorization, and by exploiting the temporal locality inherent to spin
summations. Combined, these optimizations result in speedups between 2.4x and
5.5x over the NCC quantum chemistry software package. In addition to such a
performance boost, our algorithm can perform the spin summations in-place, thus
reducing the memory footprint by 2x over an out-of-place variant.
"
1151,MITHRIL: Mining Sporadic Associations for Cache Prefetching,"  The growing pressure on cloud application scalability has accentuated storage
performance as a critical bottle- neck. Although cache replacement algorithms
have been extensively studied, cache prefetching - reducing latency by
retrieving items before they are actually requested remains an underexplored
area. Existing approaches to history-based prefetching, in particular, provide
too few benefits for real systems for the resources they cost. We propose
MITHRIL, a prefetching layer that efficiently exploits historical patterns in
cache request associations. MITHRIL is inspired by sporadic association rule
mining and only relies on the timestamps of requests. Through evaluation of 135
block-storage traces, we show that MITHRIL is effective, giving an average of a
55% hit ratio increase over LRU and PROBABILITY GRAPH, a 36% hit ratio gain
over AMP at reasonable cost. We further show that MITHRIL can supplement any
cache replacement algorithm and be readily integrated into existing systems.
Furthermore, we demonstrate the improvement comes from MITHRIL being able to
capture mid-frequency blocks.
"
1152,Mira: A Framework for Static Performance Analysis,"  The performance model of an application can pro- vide understanding about its
runtime behavior on particular hardware. Such information can be analyzed by
developers for performance tuning. However, model building and analyzing is
frequently ignored during software development until perfor- mance problems
arise because they require significant expertise and can involve many
time-consuming application runs. In this paper, we propose a fast, accurate,
flexible and user-friendly tool, Mira, for generating performance models by
applying static program analysis, targeting scientific applications running on
supercomputers. We parse both the source code and binary to estimate
performance attributes with better accuracy than considering just source or
just binary code. Because our analysis is static, the target program does not
need to be executed on the target architecture, which enables users to perform
analysis on available machines instead of conducting expensive exper- iments on
potentially expensive resources. Moreover, statically generated models enable
performance prediction on non-existent or unavailable architectures. In
addition to flexibility, because model generation time is significantly reduced
compared to dynamic analysis approaches, our method is suitable for rapid
application performance analysis and improvement. We present several scientific
application validation results to demonstrate the current capabilities of our
approach on small benchmarks and a mini application.
"
1153,"Application of the Computer Capacity to the Analysis of Processors
  Evolution","  The notion of computer capacity was proposed in 2012, and this quantity has
been estimated for computers of different kinds.
  In this paper we show that, when designing new processors, the manufacturers
change the parameters that affect the computer capacity. This allows us to
predict the values of parameters of future processors. As the main example we
use Intel processors, due to the accessibility of detailed description of all
their technical characteristics.
"
1154,Cost-Performance Tradeoffs in Fusing Unreliable Computational Units,"  We investigate fusing several unreliable computational units that perform the
same task. We model an unreliable computational outcome as an additive
perturbation to its error-free result in terms of its fidelity and cost. We
analyze performance of repetition-based strategies that distribute cost across
several unreliable units and fuse their outcomes. When the cost is a convex
function of fidelity, the optimal repetition-based strategy in terms of
incurred cost while achieving a target mean-square error (MSE) performance may
fuse several computational units. For concave and linear costs, a single more
reliable unit incurs lower cost compared to fusion of several lower cost and
less reliable units while achieving the same MSE performance. We show how our
results give insight into problems from theoretical neuroscience, circuits, and
crowdsourcing.
"
1155,Liquid Cloud Storage,"  A liquid system provides durable object storage based on spreading
redundantly generated data across a network of hundreds to thousands of
potentially unreliable storage nodes. A liquid system uses a combination of a
large code, lazy repair, and a flow storage organization. We show that a liquid
system can be operated to enable flexible and essentially optimal combinations
of storage durability, storage overhead, repair bandwidth usage, and access
performance.
"
1156,"Parallel Accelerated Vector Similarity Calculations for Genomics
  Applications","  The surge in availability of genomic data holds promise for enabling
determination of genetic causes of observed individual traits, with
applications to problems such as discovery of the genetic roots of phenotypes,
be they molecular phenotypes such as gene expression or metabolite
concentrations, or complex phenotypes such as diseases. However, the growing
sizes of these datasets and the quadratic, cubic or higher scaling
characteristics of the relevant algorithms pose a serious computational
challenge necessitating use of leadership scale computing. In this paper we
describe a new approach to performing vector similarity metrics calculations,
suitable for parallel systems equipped with graphics processing units (GPUs) or
Intel Xeon Phi processors. Our primary focus is the Proportional Similarity
metric applied to Genome Wide Association Studies (GWAS) and Phenome Wide
Association Studies (PheWAS). We describe the implementation of the algorithms
on accelerated processors, methods used for eliminating redundant calculations
due to symmetries, and techniques for efficient mapping of the calculations to
many-node parallel systems. Results are presented demonstrating high per-node
performance and parallel scalability with rates of more than five quadrillion
elementwise comparisons achieved per second on the ORNL Titan system. In a
companion paper we describe corresponding techniques applied to calculations of
the Custom Correlation Coefficient for comparative genomics applications.
"
1157,"Parallel Accelerated Custom Correlation Coefficient Calculations for
  Genomics Applications","  The massive quantities of genomic data being made available through gene
sequencing techniques are enabling breakthroughs in genomic science in many
areas such as medical advances in the diagnosis and treatment of diseases.
Analyzing this data, however, is a computational challenge insofar as the
computational costs of the relevant algorithms can grow with quadratic, cubic
or higher complexity-leading to the need for leadership scale computing. In
this paper we describe a new approach to calculations of the Custom Correlation
Coefficient (CCC) between Single Nucleotide Polymorphisms (SNPs) across a
population, suitable for parallel systems equipped with graphics processing
units (GPUs) or Intel Xeon Phi processors. We describe the mapping of the
algorithms to accelerated processors, techniques used for eliminating redundant
calculations due to symmetries, and strategies for efficient mapping of the
calculations to many-node parallel systems. Results are presented demonstrating
high per-node performance and near-ideal parallel scalability with rates of
more than nine quadrillion elementwise comparisons achieved per second with the
latest optimized code on the ORNL Titan system, this being orders of magnitude
faster than rates achieved using other codes and platforms as reported in the
literature. Also it is estimated that as many as 90 quadrillion comparisons per
second may be achievable on the upcoming ORNL Summit system, an additional 10X
performance increase. In a companion paper we describe corresponding techniques
applied to calculations of the Proportional Similarity metric for comparative
genomics applications.
"
1158,Optimum Transmission Window for EPONs with Gated-Limited Service,"  This paper studies the Ethernet Passive Optical Network (EPON) with
gated-limited service. The transmission window (TW) is limited in this system
to guaranteeing a bounded delay experienced by disciplined users, and to
constrain malicious users from monopolizing the transmission channel. Thus,
selecting an appropriate TW size is critical to the performance of EPON with
gated-limited service discipline. To investigate the impact of TW size on
packet delay, we derive a generalized mean waiting time formula for M/G/1 queue
with vacation times and gated-limited service discipline. A distinguished
feature of this model is that there are two queues in the buffer of each
optical network unit (ONU): one queue is inside the gate and the other one is
outside the gate. Furthermore, based on the Chernoff bound of queue length, we
provide a simple rule to determine an optimum TW size for gated-limited service
EPONs. Analytic results reported in this paper are all verified by simulations.
"
1159,"Reliability of Broadcast Communications Under Sparse Random Linear
  Network Coding","  Ultra-reliable Point-to-Multipoint (PtM) communications are expected to
become pivotal in networks offering future dependable services for smart
cities. In this regard, sparse Random Linear Network Coding (RLNC) techniques
have been widely employed to provide an efficient way to improve the
reliability of broadcast and multicast data streams. This paper addresses the
pressing concern of providing a tight approximation to the probability of a
user recovering a data stream protected by this kind of coding technique. In
particular, by exploiting the Stein--Chen method, we provide a novel and
general performance framework applicable to any combination of system and
service parameters, such as finite field sizes, lengths of the data stream and
level of sparsity. The deviation of the proposed approximation from Monte Carlo
simulations is negligible, improving significantly on the state of the art
performance bounds.
"
1160,"A refined and asymptotic analysis of optimal stopping problems of Bruss
  and Weber","  The classical secretary problem has been generalized over the years into
several directions. In this paper we confine our interest to those
generalizations which have to do with the more general problem of stopping on a
last observation of a specific kind. We follow Dendievel, (where a bibliography
can be found) who studies several types of such problems, mainly initiated by
Bruss and Weber. Whether in discrete time or continuous time, whether all
parameters are known or must be sequentially estimated, we shall call such
problems simply ""Bruss-Weber problems"". Our contribution in the present paper
is a refined analysis of several problems in this class and a study of the
asymptotic behaviour of solutions.
  The problems we consider center around the following model. Let
$X_1,X_2,\ldots,X_n$ be a sequence of independent random variables which can
take three values: $\{+1,-1,0\}.$ Let $p:=\P(X_i=1), p':=\P(X_i=-1),
\qt:=\P(X_i=0), p\geq p'$, where $p+p'+\qt=1$. The goal is to maximize the
probability of stopping on a value $+1$ or $-1$ appearing for the last time in
the sequence. Following a suggestion by Bruss, we have also analyzed an
x-strategy with incomplete information: the cases $p$ known, $n$ unknown, then
$n$ known, $p$ unknown and finally $n,p$ unknown are considered. We also
present simulations of the corresponding complete selection algorithm.
"
1161,"Approximation of LRU Caches Miss Rate: Application to Power-law
  Popularities","  Building on the 1977 pioneering work of R. Fagin, we give a closed-form
expression for the approximated Miss Rate (MR) of LRU Caches assuming a
power-law popularity. Asymptotic behavior of this expression is an already
known result when power-law parameter is above 1. It is extended to any value
of the parameter. In addition, we bring a new analysis of the conditions (cache
relative size, popularity parameter) under which the ratio of LRU MR to Static
MR is worst-case.
"
1162,Tracking System Behaviour from Resource Usage Data,"  Resource usage data, collected using tools such as TACC Stats, capture the
resource utilization by nodes within a high performance computing system. We
present methods to analyze the resource usage data to understand the system
performance and identify performance anomalies. The core idea is to model the
data as a three-way tensor corresponding to the compute nodes, usage metrics,
and time. Using the reconstruction error between the original tensor and the
tensor reconstructed from a low rank tensor decomposition, as a scalar
performance metric, enables us to monitor the performance of the system in an
online fashion. This error statistic is then used for anomaly detection that
relies on the assumption that the normal/routine behavior of the system can be
captured using a low rank approx- imation of the original tensor. We evaluate
the performance of the algorithm using information gathered from system logs
and show that the performance anomalies identified by the proposed method
correlates with critical errors reported in the system logs. Results are shown
for data collected for 2013 from the Lonestar4 system at the Texas Advanced
Computing Center (TACC)
"
1163,"Modeling and Design of Millimeter-Wave Networks for Highway Vehicular
  Communication","  Connected and autonomous vehicles will play a pivotal role in future
Intelligent Transportation Systems (ITSs) and smart cities, in general.
High-speed and low-latency wireless communication links will allow
municipalities to warn vehicles against safety hazards, as well as support
cloud-driving solutions to drastically reduce traffic jams and air pollution.
To achieve these goals, vehicles need to be equipped with a wide range of
sensors generating and exchanging high rate data streams. Recently, millimeter
wave (mmWave) techniques have been introduced as a means of fulfilling such
high data rate requirements. In this paper, we model a highway communication
network and characterize its fundamental link budget metrics. In particular, we
specifically consider a network where vehicles are served by mmWave Base
Stations (BSs) deployed alongside the road. To evaluate our highway network, we
develop a new theoretical model that accounts for a typical scenario where
heavy vehicles (such as buses and lorries) in slow lanes obstruct Line-of-Sight
(LOS) paths of vehicles in fast lanes and, hence, act as blockages. Using tools
from stochastic geometry, we derive approximations for the
Signal-to-Interference-plus-Noise Ratio (SINR) outage probability, as well as
the probability that a user achieves a target communication rate (rate coverage
probability). Our analysis provides new design insights for mmWave highway
communication networks. In considered highway scenarios, we show that reducing
the horizontal beamwidth from $90^\circ$ to $30^\circ$ determines a minimal
reduction in the SINR outage probability (namely, $4 \cdot 10^{-2}$ at
maximum). Also, unlike bi-dimensional mmWave cellular networks, for small BS
densities (namely, one BS every $500$ m) it is still possible to achieve an
SINR outage probability smaller than $0.2$.
"
1164,"On the Scalability of Data Reduction Techniques in Current and Upcoming
  HPC Systems from an Application Perspective","  We implement and benchmark parallel I/O methods for the fully-manycore driven
particle-in-cell code PIConGPU. Identifying throughput and overall I/O size as
a major challenge for applications on today's and future HPC systems, we
present a scaling law characterizing performance bottlenecks in
state-of-the-art approaches for data reduction. Consequently, we propose,
implement and verify multi-threaded data-transformations for the I/O library
ADIOS as a feasible way to trade underutilized host-side compute potential on
heterogeneous systems for reduced I/O latency.
"
1165,"M/G/c/c state dependent queuing model for a road traffic system of two
  sections in tandem","  We propose in this article a M/G/c/c state dependent queuing model for road
traffic flow. The model is based on finite capacity queuing theory which
captures the stationary density-flow relationships. It is also inspired from
the deterministic Godunov scheme for the road traffic simulation. We first
present a reformulation of the existing linear case of M/G/c/c state dependent
model, in order to use flow rather than speed variables. We then extend this
model in order to consider upstream traffic demand and downstream traffic
supply. After that, we propose the model for two road sections in tandem where
both sections influence each other. In order to deal with this mutual
dependence, we solve an implicit system given by an algebraic equation.
Finally, we derive some performance measures (throughput and expected travel
time). A comparison with results predicted by the M/G/c/c state dependent
queuing networks shows that the model we propose here captures really the
dynamics of the road traffic.
"
1166,Load Balancing in Large-Scale Systems with Multiple Dispatchers,"  Load balancing algorithms play a crucial role in delivering robust
application performance in data centers and cloud networks. Recently, strong
interest has emerged in Join-the-Idle-Queue (JIQ) algorithms, which rely on
tokens issued by idle servers in dispatching tasks and outperform power-of-$d$
policies. Specifically, JIQ strategies involve minimal information exchange,
and yet achieve zero blocking and wait in the many-server limit. The latter
property prevails in a multiple-dispatcher scenario when the loads are strictly
equal among dispatchers. For various reasons it is not uncommon however for
skewed load patterns to occur. We leverage product-form representations and
fluid limits to establish that the blocking and wait then no longer vanish,
even for arbitrarily low overall load. Remarkably, it is the least-loaded
dispatcher that throttles tokens and leaves idle servers stranded, thus acting
as bottleneck.
  Motivated by the above issues, we introduce two enhancements of the ordinary
JIQ scheme where tokens are either distributed non-uniformly or occasionally
exchanged among the various dispatchers. We prove that these extensions can
achieve zero blocking and wait in the many-server limit, for any subcritical
overall load and arbitrarily skewed load profiles. Extensive simulation
experiments demonstrate that the asymptotic results are highly accurate, even
for moderately sized systems.
"
1167,Performance Modeling and Prediction for Dense Linear Algebra,"  This dissertation introduces measurement-based performance modeling and
prediction techniques for dense linear algebra algorithms. As a core principle,
these techniques avoid executions of such algorithms entirely, and instead
predict their performance through runtime estimates for the underlying compute
kernels. For a variety of operations, these predictions allow to quickly select
the fastest algorithm configurations from available alternatives. We consider
two scenarios that cover a wide range of computations:
  To predict the performance of blocked algorithms, we design
algorithm-independent performance models for kernel operations that are
generated automatically once per platform. For various matrix operations,
instantaneous predictions based on such models both accurately identify the
fastest algorithm, and select a near-optimal block size.
  For performance predictions of BLAS-based tensor contractions, we propose
cache-aware micro-benchmarks that take advantage of the highly regular
structure inherent to contraction algorithms. At merely a fraction of a
contraction's runtime, predictions based on such micro-benchmarks identify the
fastest combination of tensor traversal and compute kernel.
"
1168,To Index or Not to Index: Optimizing Exact Maximum Inner Product Search,"  Exact Maximum Inner Product Search (MIPS) is an important task that is widely
pertinent to recommender systems and high-dimensional similarity search. The
brute-force approach to solving exact MIPS is computationally expensive, thus
spurring recent development of novel indexes and pruning techniques for this
task. In this paper, we show that a hardware-efficient brute-force approach,
blocked matrix multiply (BMM), can outperform the state-of-the-art MIPS solvers
by over an order of magnitude, for some -- but not all -- inputs.
  In this paper, we also present a novel MIPS solution, MAXIMUS, that takes
advantage of hardware efficiency and pruning of the search space. Like BMM,
MAXIMUS is faster than other solvers by up to an order of magnitude, but again
only for some inputs. Since no single solution offers the best runtime
performance for all inputs, we introduce a new data-dependent optimizer,
OPTIMUS, that selects online with minimal overhead the best MIPS solver for a
given input. Together, OPTIMUS and MAXIMUS outperform state-of-the-art MIPS
solvers by 3.2$\times$ on average, and up to 10.9$\times$, on widely studied
MIPS datasets.
"
1169,Delay Comparison of Delivery and Coding Policies in Data Clusters,"  A key function of cloud infrastructure is to store and deliver diverse files,
e.g., scientific datasets, social network information, videos, etc. In such
systems, for the purpose of fast and reliable delivery, files are divided into
chunks, replicated or erasure-coded, and disseminated across servers. It is
neither known in general how delays scale with the size of a request nor how
delays compare under different policies for coding, data dissemination, and
delivery.
  Motivated by these questions, we develop and explore a set of evolution
equations as a unified model which captures the above features. These equations
allow for both efficient simulation and mathematical analysis of several
delivery policies under general statistical assumptions. In particular, we
quantify in what sense a workload aware delivery policy performs better than a
workload agnostic policy. Under a dynamic or stochastic setting, the sample
path comparison of these policies does not hold in general. The comparison is
shown to hold under the weaker increasing convex stochastic ordering, still
stronger than the comparison of averages.
  This result further allows us to obtain insightful computable performance
bounds. For example, we show that in a system where files are divided into
chunks of equal size, replicated or erasure-coded, and disseminated across
servers at random, the job delays increase sub-logarithmically in the request
size for small and medium-sized files but linearly for large files.
"
1170,Link Adaptation for Wireless Video Communication Systems,"  This PhD thesis considers the performance evaluation and enhancement of video
communication over wireless channels. The system model considers hybrid
automatic repeat request (HARQ) with Chase combining and turbo product codes
(TPC). The thesis proposes algorithms and techniques to optimize the
throughput, transmission power and complexity of HARQ-based wireless video
communication. A semi-analytical solution is developed to model the performance
of delay-constrained HARQ systems. The semi-analytical and Monte Carlo
simulation results reveal that significant complexity reduction can be achieved
by noting that the coding gain advantage of the soft over hard decoding is
reduced when Chase combining is used, and it actually vanishes completely for
particular codes. Moreover, the thesis proposes a novel power optimization
algorithm that achieves a significant power saving of up to 80%. Joint
throughput maximization and complexity reduction is considered as well. A CRC
(cyclic redundancy check)-free HARQ is proposed to improve the system
throughput when short packets are transmitted. In addition, the computational
complexity/delay is reduced when the packets transmitted are long. Finally, a
content-aware and occupancy-based HARQ scheme is proposed to ensure minimum
video quality distortion with continuous playback.
"
1171,Group-Server Queues,"  By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
"
1172,Towards Adaptive Resilience in High Performance Computing,"  Failure rates in high performance computers rapidly increase due to the
growth in system size and complexity. Hence, failures became the norm rather
than the exception. Different approaches on high performance computing (HPC)
systems have been introduced, to prevent failures (e. g., redundancy) or at
least minimize their impacts (e. g., checkpoint and restart). In most cases,
when these approaches are employed to increase the resilience of certain parts
of a system, energy consumption rapidly increases, or performance significantly
degrades. To address this challenge, we propose on-demand resilience as an
approach to achieve adaptive resilience in HPC systems. In this work, the HPC
system is considered in its entirety and resilience mechanisms such as
checkpointing, isolation, and migration, are activated on-demand. Using the
proposed approach, the unavoidable increase in total energy consumption and
system performance degradation is decreased compared to the typical
checkpoint/restart and redundant resilience mechanisms. Our work aims to
mitigate a large number of failures occurring at various layers in the system,
to prevent their propagation, and to minimize their impact, all of this in an
energy-saving manner. In the case of failures that are estimated to occur but
cannot be mitigated using the proposed on-demand resilience approach, the
system administrators will be notified in view of performing further
investigations into the causes of these failures and their impacts.
"
1173,"Mean-Payoff Optimization in Continuous-Time Markov Chains with
  Parametric Alarms","  Continuous-time Markov chains with alarms (ACTMCs) allow for alarm events
that can be non-exponentially distributed. Within parametric ACTMCs, the
parameters of alarm-event distributions are not given explicitly and can be
subject of parameter synthesis. An algorithm solving the $\varepsilon$-optimal
parameter synthesis problem for parametric ACTMCs with long-run average
optimization objectives is presented. Our approach is based on reduction of the
problem to finding long-run average optimal strategies in semi-Markov decision
processes (semi-MDPs) and sufficient discretization of parameter (i.e., action)
space. Since the set of actions in the discretized semi-MDP can be very large,
a straightforward approach based on explicit action-space construction fails to
solve even simple instances of the problem. The presented algorithm uses an
enhanced policy iteration on symbolic representations of the action space. The
soundness of the algorithm is established for parametric ACTMCs with
alarm-event distributions satisfying four mild assumptions that are shown to
hold for uniform, Dirac and Weibull distributions in particular, but are
satisfied for many other distributions as well. An experimental implementation
shows that the symbolic technique substantially improves the efficiency of the
synthesis algorithm and allows to solve instances of realistic size.
"
1174,"Optimizing the Performance of Reactive Molecular Dynamics Simulations
  for Multi-Core Architectures","  Reactive molecular dynamics simulations are computationally demanding.
Reaching spatial and temporal scales where interesting scientific phenomena can
be observed requires efficient and scalable implementations on modern hardware.
In this paper, we focus on optimizing the performance of the widely used
LAMMPS/ReaxC package for multi-core architectures. As hybrid parallelism allows
better leverage of the increasing on-node parallelism, we adopt thread
parallelism in the construction of bonded and nonbonded lists, and in the
computation of complex ReaxFF interactions. To mitigate the I/O overheads due
to large volumes of trajectory data produced and to save users the burden of
post-processing, we also develop a novel in-situ tool for molecular species
analysis. We analyze the performance of the resulting ReaxC-OMP package on
Mira, an IBM Blue Gene/Q supercomputer. For PETN systems of sizes ranging from
32 thousand to 16.6 million particles, we observe speedups in the range of
1.5-4.5x. We observe sustained performance improvements for up to 262,144 cores
(1,048,576 processes) of Mira and a weak scaling efficiency of 91.5% in large
simulations containing 16.6 million particles. The in-situ molecular species
analysis tool incurs only insignificant overheads across various system sizes
and run configurations.
"
1175,$\mu$Nap: Practical Micro-Sleeps for 802.11 WLANs,"  In this paper, we revisit the idea of putting interfaces to sleep during
'packet overhearing' (i.e., when there are ongoing transmissions addressed to
other stations) from a practical standpoint. To this aim, we perform a robust
experimental characterisation of the timing and consumption behaviour of a
commercial 802.11 card. We design $\mu$Nap, a local standard-compliant
energy-saving mechanism that leverages micro-sleep opportunities inherent to
the CSMA operation of 802.11 WLANs. This mechanism is backwards compatible and
incrementally deployable, and takes into account the timing limitations of
existing hardware, as well as practical CSMA-related issues (e.g., capture
effect). According to the performance assessment carried out through
trace-based simulation, the use of our scheme would result in a 57% reduction
in the time spent in overhearing, thus leading to an energy saving of 15.8% of
the activity time.
"
1176,"On the Energy Efficiency of Rate and Transmission Power Control in
  802.11","  Rate adaptation and transmission power control in 802.11 WLANs have received
a lot of attention from the research community, with most of the proposals
aiming at maximising throughput based on network conditions. Considering energy
consumption, an implicit assumption is that optimality in throughput implies
optimality in energy efficiency, but this assumption has been recently put into
question. In this paper, we address via analysis, simulation and
experimentation the relation between throughput performance and energy
efficiency in multi-rate 802.11 scenarios. We demonstrate the trade-off between
these performance figures, confirming that they may not be simultaneously
optimised, and analyse their sensitivity towards the energy consumption
parameters of the device. We analyse this trade-off in existing rate adaptation
with transmission power control algorithms, and discuss how to design novel
schemes taking energy consumption into account.
"
1177,"Theoretical Performance Analysis of Vehicular Broadcast Communications
  at Intersection and their Optimization","  In this paper, we propose an optimization method for the broadcast rate in
vehicle-to-vehicle (V2V) broadcast communications at an intersection on the
basis of theoretical analysis. We consider a model in which locations of
vehicles are modeled separately as queuing and running segments and derive key
performance metrics of V2V broadcast communications via a stochastic geometry
approach. Since these theoretical expressions are mathematically intractable,
we developed closed-form approximate formulae for them. Using them, we optimize
the broadcast rate such that the mean number of successful receivers per unit
time is maximized. Because of the closed form approximation, the optimal rate
can be used as a guideline for a real-time control-method, which is not
achieved through time-consuming simulations. We evaluated our method through
numerical examples and demonstrated the effectiveness of our method.
"
1178,Bolt: Accelerated Data Mining with Fast Vector Compression,"  Vectors of data are at the heart of machine learning and data mining.
Recently, vector quantization methods have shown great promise in reducing both
the time and space costs of operating on vectors. We introduce a vector
quantization algorithm that can compress vectors over 12x faster than existing
techniques while also accelerating approximate vector operations such as
distance and dot product computations by up to 10x. Because it can encode over
2GB of vectors per second, it makes vector quantization cheap enough to employ
in many more circumstances. For example, using our technique to compute
approximate dot products in a nested loop can multiply matrices faster than a
state-of-the-art BLAS implementation, even when our algorithm must first
compress the matrices.
  In addition to showing the above speedups, we demonstrate that our approach
can accelerate nearest neighbor search and maximum inner product search by over
100x compared to floating point operations and up to 10x compared to other
vector quantization methods. Our approximate Euclidean distance and dot product
computations are not only faster than those of related algorithms with slower
encodings, but also faster than Hamming distance computations, which have
direct hardware support on the tested platforms. We also assess the errors of
our algorithm's approximate distances and dot products, and find that it is
competitive with existing, slower vector quantization algorithms.
"
1179,A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs,"  Analysis of DNA samples is an important step in forensics, and the speed of
analysis can impact investigations. Comparison of DNA sequences is based on the
analysis of short tandem repeats (STRs), which are short DNA sequences of 2-5
base pairs. Current forensics approaches use 20 STR loci for analysis. The use
of single nucleotide polymorphisms (SNPs) has utility for analysis of complex
DNA mixtures. The use of tens of thousands of SNPs loci for analysis poses
significant computational challenges because the forensic analysis scales by
the product of the loci count and number of DNA samples to be analyzed. In this
paper, we discuss the implementation of a DNA sequence comparison algorithm by
re-casting the algorithm in terms of linear algebra primitives. By developing
an overloaded matrix multiplication approach to DNA comparisons, we can
leverage advances in GPU hardware and algoithms for Dense Generalized
Matrix-Multiply (DGEMM) to speed up DNA sample comparisons. We show that it is
possible to compare 2048 unknown DNA samples with 20 million known samples in
under 6 seconds using a NVIDIA K80 GPU.
"
1180,Modeling the Internet of Things: a simulation perspective,"  This paper deals with the problem of properly simulating the Internet of
Things (IoT). Simulating an IoT allows evaluating strategies that can be
employed to deploy smart services over different kinds of territories. However,
the heterogeneity of scenarios seriously complicates this task. This imposes
the use of sophisticated modeling and simulation techniques. We discuss novel
approaches for the provision of scalable simulation scenarios, that enable the
real-time execution of massively populated IoT environments. Attention is given
to novel hybrid and multi-level simulation techniques that, when combined with
agent-based, adaptive Parallel and Distributed Simulation (PADS) approaches,
can provide means to perform highly detailed simulations on demand. To support
this claim, we detail a use case concerned with the simulation of vehicular
transportation systems.
"
1181,"Efficient Strategy Iteration for Mean Payoff in Markov Decision
  Processes","  Markov decision processes (MDPs) are standard models for probabilistic
systems with non-deterministic behaviours. Mean payoff (or long-run average
reward) provides a mathematically elegant formalism to express performance
related properties. Strategy iteration is one of the solution techniques
applicable in this context. While in many other contexts it is the technique of
choice due to advantages over e.g. value iteration, such as precision or
possibility of domain-knowledge-aware initialization, it is rarely used for
MDPs, since there it scales worse than value iteration. We provide several
techniques that speed up strategy iteration by orders of magnitude for many
MDPs, eliminating the performance disadvantage while preserving all its
advantages.
"
1182,DCCast: Efficient Point to Multipoint Transfers Across Datacenters,"  Using multiple datacenters allows for higher availability, load balancing and
reduced latency to customers of cloud services. To distribute multiple copies
of data, cloud providers depend on inter-datacenter WANs that ought to be used
efficiently considering their limited capacity and the ever-increasing data
demands. In this paper, we focus on applications that transfer objects from one
datacenter to several datacenters over dedicated inter-datacenter networks. We
present DCCast, a centralized Point to Multi-Point (P2MP) algorithm that uses
forwarding trees to efficiently deliver an object from a source datacenter to
required destination datacenters. With low computational overhead, DCCast
selects forwarding trees that minimize bandwidth usage and balance load across
all links. With simulation experiments on Google's GScale network, we show that
DCCast can reduce total bandwidth usage and tail Transfer Completion Times
(TCT) by up to $50\%$ compared to delivering the same objects via independent
point-to-point (P2P) transfers.
"
1183,"Exploiting Parallelism in Optical Network Systems: A Case Study of
  Random Linear Network Coding (RLNC) in Ethernet-over-Optical Networks","  As parallelism becomes critically important in the semiconductor technology,
high-performance computing, and cloud applications, parallel network systems
will increasingly follow suit. Today, parallelism is an essential architectural
feature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet
systems are equipped with multiple parallel network interfaces. This creates
new network topology abstractions and new technology requirements: instead of a
single high capacity network link, multiple Ethernet end-points and interfaces
need to be considered together with multiple links in form of discrete parallel
paths. This new paradigm is enabling implementations of various new features to
improve overall system performance. In this paper, we analyze the performance
of parallel network systems with network coding. In particular, by using random
LNC (RLNC), - a code without the need for decoding, we can make use of the fact
that we have codes that are both distributed (removing the need for
coordination or optimization of resources) and composable (without the need to
exchange code information), leading to a fully stateless operation. We propose
a novel theoretical modeling framework, including derivation of the upper and
lower bounds as well as an expected value of the differential delay of parallel
paths, and the resulting queue size at the receiver. The results show a great
promise of network system parallelism in combination with RLNC: with a proper
set of design parameters, the differential delay and the buffer size at the
Ethernet receiver can be reduced significantly, while the cross-layer design
and routing can be greatly simplified.
"
1184,Synthesis of Optimal Resilient Control Strategies,"  Repair mechanisms are important within resilient systems to maintain the
system in an operational state after an error occurred. Usually, constraints on
the repair mechanisms are imposed, e.g., concerning the time or resources
required (such as energy consumption or other kinds of costs). For systems
modeled by Markov decision processes (MDPs), we introduce the concept of
resilient schedulers, which represent control strategies guaranteeing that
these constraints are always met within some given probability. Assigning
rewards to the operational states of the system, we then aim towards resilient
schedulers which maximize the long-run average reward, i.e., the expected mean
payoff. We present a pseudo-polynomial algorithm that decides whether a
resilient scheduler exists and if so, yields an optimal resilient scheduler. We
show also that already the decision problem asking whether there exists a
resilient scheduler is PSPACE-hard.
"
1185,"Benchmarking Data Analysis and Machine Learning Applications on the
  Intel KNL Many-Core Processor","  Knights Landing (KNL) is the code name for the second-generation Intel Xeon
Phi product family. KNL has generated significant interest in the data analysis
and machine learning communities because its new many-core architecture targets
both of these workloads. The KNL many-core vector processor design enables it
to exploit much higher levels of parallelism. At the Lincoln Laboratory
Supercomputing Center (LLSC), the majority of users are running data analysis
applications such as MATLAB and Octave. More recently, machine learning
applications, such as the UC Berkeley Caffe deep learning framework, have
become increasingly important to LLSC users. Thus, the performance of these
applications on KNL systems is of high interest to LLSC users and the broader
data analysis and machine learning communities. Our data analysis benchmarks of
these application on the Intel KNL processor indicate that single-core
double-precision generalized matrix multiply (DGEMM) performance on KNL systems
has improved by ~3.5x compared to prior Intel Xeon technologies. Our data
analysis applications also achieved ~60% of the theoretical peak performance.
Also a performance comparison of a machine learning application, Caffe, between
the two different Intel CPUs, Xeon E5 v3 and Xeon Phi 7210, demonstrated a 2.7x
improvement on a KNL node.
"
1186,"DCRoute: Speeding up Inter-Datacenter Traffic Allocation while
  Guaranteeing Deadlines","  Datacenters provide the infrastructure for cloud computing services used by
millions of users everyday. Many such services are distributed over multiple
datacenters at geographically distant locations possibly in different
continents. These datacenters are then connected through high speed WAN links
over private or public networks. To perform data backups or data
synchronization operations, many transfers take place over these networks that
have to be completed before a deadline in order to provide necessary service
guarantees to end users. Upon arrival of a transfer request, we would like the
system to be able to decide whether such a request can be guaranteed successful
delivery. If yes, it should provide us with transmission schedule in the
shortest time possible. In addition, we would like to avoid packet reordering
at the destination as it affects TCP performance. Previous work in this area
either cannot guarantee that admitted transfers actually finish before the
specified deadlines or use techniques that can result in packet reordering. In
this paper, we propose DCRoute, a fast and efficient routing and traffic
allocation technique that guarantees transfer completion before deadlines for
admitted requests. It assigns each transfer a single path to avoid packet
reordering. Through simulations, we show that DCRoute is at least 200 times
faster than other traffic allocation techniques based on linear programming
(LP) while admitting almost the same amount of traffic to the system.
"
1187,RCD: Rapid Close to Deadline Scheduling for Datacenter Networks,"  Datacenter-based Cloud Computing services provide a flexible, scalable and
yet economical infrastructure to host online services such as multimedia
streaming, email and bulk storage. Many such services perform geo-replication
to provide necessary quality of service and reliability to users resulting in
frequent large inter- datacenter transfers. In order to meet tenant service
level agreements (SLAs), these transfers have to be completed prior to a
deadline. In addition, WAN resources are quite scarce and costly, meaning they
should be fully utilized. Several recently proposed schemes, such as B4,
TEMPUS, and SWAN have focused on improving the utilization of inter-datacenter
transfers through centralized scheduling, however, they fail to provide a
mechanism to guarantee that admitted requests meet their deadlines. Also, in a
recent study, authors propose Amoeba, a system that allows tenants to define
deadlines and guarantees that the specified deadlines are met, however, to
admit new traffic, the proposed system has to modify the allocation of already
admitted transfers. In this paper, we propose Rapid Close to Deadline
Scheduling (RCD), a close to deadline traffic allocation technique that is fast
and efficient. Through simulations, we show that RCD is up to 15 times faster
than Amoeba, provides high link utilization along with deadline guarantees, and
is able to make quick decisions on whether a new request can be fully satisfied
before its deadline.
"
1188,"Hot-Rodding the Browser Engine: Automatic Configuration of JavaScript
  Compilers","  Modern software systems in many application areas offer to the user a
multitude of parameters, switches and other customisation hooks. Humans tend to
have difficulties determining the best configurations for particular
applications. Modern optimising compilers are an example of such software
systems; their many parameters need to be tuned for optimal performance, but
are often left at the default values for convenience. In this work, we
automatically determine compiler parameter settings that result in optimised
performance for particular applications. Specifically, we apply a
state-of-the-art automated parameter configuration procedure based on
cutting-edge machine learning and optimisation techniques to two prominent
JavaScript compilers and demonstrate that significant performance improvements,
more than 35% in some cases, can be achieved over the default parameter
settings on a diverse set of benchmarks.
"
1189,Language-based Abstractions for Dynamical Systems,"  Ordinary differential equations (ODEs) are the primary means to modelling
dynamical systems in many natural and engineering sciences. The number of
equations required to describe a system with high heterogeneity limits our
capability of effectively performing analyses. This has motivated a large body
of research, across many disciplines, into abstraction techniques that provide
smaller ODE systems while preserving the original dynamics in some appropriate
sense. In this paper we give an overview of a recently proposed
computer-science perspective to this problem, where ODE reduction is recast to
finding an appropriate equivalence relation over ODE variables, akin to
classical models of computation based on labelled transition systems.
"
1190,"Pushing the Limits of Online Auto-tuning: Machine Code Optimization in
  Short-Running Kernels","  We propose an online auto-tuning approach for computing kernels. Differently
from existing online auto-tuners, which regenerate code with long compilation
chains from the source to the binary code, our approach consists on deploying
auto-tuning directly at the level of machine code generation. This allows
auto-tuning to pay off in very short-running applications. As a proof of
concept, our approach is demonstrated in two benchmarks, which execute during
hundreds of milliseconds to a few seconds only. In a CPU-bound kernel, the
average speedups achieved are 1.10 to 1.58 depending on the target
micro-architecture, up to 2.53 in the most favourable conditions (all run-time
overheads included). In a memory-bound kernel, less favourable to our runtime
auto-tuning optimizations, the average speedups are 1.04 to 1.10, up to 1.30 in
the best configuration. Despite the short execution times of our benchmarks,
the overhead of our runtime auto-tuning is between 0.2 and 4.2% only of the
total application execution times. By simulating the CPU-bound application in
11 different CPUs, we showed that, despite the clear hardware disadvantage of
In-Order (io) cores vs. Out-of-Order (ooo) equivalent cores, online auto-tuning
in io CPUs obtained an average speedup of 1.03 and an energy efficiency
improvement of 39~\% over the SIMD reference in ooo CPUs.
"
1191,Cloud-based or On-device: An Empirical Study of Mobile Deep Inference,"  Modern mobile applications are benefiting significantly from the advancement
in deep learning, e.g., implementing real-time image recognition and
conversational system. Given a trained deep learning model, applications
usually need to perform a series of matrix operations based on the input data,
in order to infer possible output values. Because of computational complexity
and size constraints, these trained models are often hosted in the cloud. To
utilize these cloud-based models, mobile apps will have to send input data over
the network. While cloud-based deep learning can provide reasonable response
time for mobile apps, it restricts the use case scenarios, e.g. mobile apps
need to have network access. With mobile specific deep learning optimizations,
it is now possible to employ on-device inference. However, because mobile
hardware, such as GPU and memory size, can be very limited when compared to its
desktop counterpart, it is important to understand the feasibility of this new
on-device deep learning inference architecture. In this paper, we empirically
evaluate the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application we developed. Our measurement and
analysis suggest that on-device inference can cost up to two orders of
magnitude greater response time and energy when compared to cloud-based
inference, and that loading model and computing probability are two performance
bottlenecks for on-device deep inferences.
"
1192,"Performance Evaluation of Distributed Computing Environments with Hadoop
  and Spark Frameworks","  Recently, due to rapid development of information and communication
technologies, the data are created and consumed in the avalanche way.
Distributed computing create preconditions for analyzing and processing such
Big Data by distributing the computations among a number of compute nodes. In
this work, performance of distributed computing environments on the basis of
Hadoop and Spark frameworks is estimated for real and virtual versions of
clusters. As a test task, we chose the classic use case of word counting in
texts of various sizes. It was found that the running times grow very fast with
the dataset size and faster than a power function even. As to the real and
virtual versions of cluster implementations, this tendency is the similar for
both Hadoop and Spark frameworks. Moreover, speedup values decrease
significantly with the growth of dataset size, especially for virtual version
of cluster configuration. The problem of growing data generated by IoT and
multimodal (visual, sound, tactile, neuro and brain-computing, muscle and eye
tracking, etc.) interaction channels is presented. In the context of this
problem, the current observations as to the running times and speedup on Hadoop
and Spark frameworks in real and virtual cluster configurations can be very
useful for the proper scaling-up and efficient job management, especially for
machine learning and Deep Learning applications, where Big Data are widely
present.
"
1193,"Comparative Performance Analysis of Neural Networks Architectures on H2O
  Platform for Various Activation Functions","  Deep learning (deep structured learning, hierarchi- cal learning or deep
machine learning) is a branch of machine learning based on a set of algorithms
that attempt to model high- level abstractions in data by using multiple
processing layers with complex structures or otherwise composed of multiple
non-linear transformations. In this paper, we present the results of testing
neural networks architectures on H2O platform for various activation functions,
stopping metrics, and other parameters of machine learning algorithm. It was
demonstrated for the use case of MNIST database of handwritten digits in
single-threaded mode that blind selection of these parameters can hugely
increase (by 2-3 orders) the runtime without the significant increase of
precision. This result can have crucial influence for opitmization of available
and new machine learning methods, especially for image recognition problems.
"
1194,"Deterministic Memory Abstraction and Supporting Multicore System
  Architecture","  Poor time predictability of multicore processors has been a long-standing
challenge in the real-time systems community. In this paper, we make a case
that a fundamental problem that prevents efficient and predictable real-time
computing on multicore is the lack of a proper memory abstraction to express
memory criticality, which cuts across various layers of the system: the
application, OS, and hardware. We, therefore, propose a new holistic resource
management approach driven by a new memory abstraction, which we call
Deterministic Memory. The key characteristic of deterministic memory is that
the platform - the OS and hardware - guarantees small and tightly bounded
worst-case memory access timing. In contrast, we call the conventional memory
abstraction as best-effort memory in which only highly pessimistic worst-case
bounds can be achieved. We propose to utilize both abstractions to achieve high
time predictability but without significantly sacrificing performance. We
present deterministic memory-aware OS and architecture designs, including
OS-level page allocator, hardware-level cache, and DRAM controller designs. We
implement the proposed OS and architecture extensions on Linux and gem5
simulator. Our evaluation results, using a set of synthetic and real-world
benchmarks, demonstrate the feasibility and effectiveness of our approach.
"
1195,Study and Analysis of MAC/IPAD Lab Configuration,"  This paper is about three virtualization modes: VMware, Parallels, and Boot
Camping. The trade off of their testing is the hardware requirements. The main
question is, among the three, which is the most suitable? The answer actually
varies from user to user. It depends on the user needs. Moreover, it is
necessary to consider its performance, graphics, efficiency and reliability,
and interoperability, and that is our major scope. In order to take the final
decision in choosing one of the modes it is important to run some tests, which
costs a lot in terms of money, complexity, and time consumption. Therefore, in
order to overcome this trade off, most of the research has been done through
online benchmarking and my own anticipation. The final solution was extracted
after comparing all previously mentioned above and after rigorous testing made
which will be introduced later in this document.
"
1196,"Incorporating TSN/BLS in AFDX for Mixed-Criticality Avionics
  Applications: Specification and Analysis","  In this paper, we propose an extension of the AFDX standard, incorporating a
TSN/BLS shaper, to homogenize the avionics communication architecture, and
enable the interconnection of different avionics domains with mixed-criticality
levels, e.g., legacy AFDX traffic, Flight Control and In-Flight Entertainment.
First, we present the main specifications of such a proposed solution. Then, we
detail the corresponding worst-case timing analysis, using the Network Calculus
framework, to infer real-time guarantees. Finally, we conduct the performance
analysis of such a proposal on a realistic AFDX configuration. Results show the
efficiency of the Extended AFDX standard to noticeably enhance the medium
priority level delay bounds, while respecting the higher priority level
constraints, in comparison with the legacy AFDX standard.
"
1197,Domain-Sharding for Faster HTTP/2 in Lossy Cellular Networks,"  HTTP/2 (h2) is a new standard for Web communications that already delivers a
large share of Web traffic. Unlike HTTP/1, h2 uses only one underlying TCP
connection. In a cellular network with high loss and sudden spikes in latency,
which the TCP stack might interpret as loss, using a single TCP connection can
negatively impact Web performance. In this paper, we perform an extensive
analysis of real world cellular network traffic and design a testbed to emulate
loss characteristics in cellular networks. We use the emulated cellular network
to measure h2 performance in comparison to HTTP/1.1, for webpages synthesized
from HTTP Archive repository data.
  Our results show that, in lossy conditions, h2 achieves faster page load
times (PLTs) for webpages with small objects. For webpages with large objects,
h2 degrades the PLT. We devise a new domain-sharding technique that isolates
large and small object downloads on separate connections. Using sharding, we
show that under lossy cellular conditions, h2 over multiple connections
improves the PLT compared to h2 with one connection and HTTP/1.1 with six
connections. Finally, we recommend content providers and content delivery
networks to apply h2-aware domain-sharding on webpages currently served over h2
for improved mobile Web performance.
"
1198,Asymptotically Optimal Load Balancing Topologies,"  We consider a system of $N$ servers inter-connected by some underlying graph
topology $G_N$. Tasks arrive at the various servers as independent Poisson
processes of rate $\lambda$. Each incoming task is irrevocably assigned to
whichever server has the smallest number of tasks among the one where it
appears and its neighbors in $G_N$. Tasks have unit-mean exponential service
times and leave the system upon service completion.
  The above model has been extensively investigated in the case $G_N$ is a
clique. Since the servers are exchangeable in that case, the queue length
process is quite tractable, and it has been proved that for any $\lambda < 1$,
the fraction of servers with two or more tasks vanishes in the limit as $N \to
\infty$. For an arbitrary graph $G_N$, the lack of exchangeability severely
complicates the analysis, and the queue length process tends to be worse than
for a clique. Accordingly, a graph $G_N$ is said to be $N$-optimal or
$\sqrt{N}$-optimal when the occupancy process on $G_N$ is equivalent to that on
a clique on an $N$-scale or $\sqrt{N}$-scale, respectively.
  We prove that if $G_N$ is an Erd\H{o}s-R\'enyi random graph with average
degree $d(N)$, then it is with high probability $N$-optimal and
$\sqrt{N}$-optimal if $d(N) \to \infty$ and $d(N) / (\sqrt{N} \log(N)) \to
\infty$ as $N \to \infty$, respectively. This demonstrates that optimality can
be maintained at $N$-scale and $\sqrt{N}$-scale while reducing the number of
connections by nearly a factor $N$ and $\sqrt{N} / \log(N)$ compared to a
clique, provided the topology is suitably random. It is further shown that if
$G_N$ contains $\Theta(N)$ bounded-degree nodes, then it cannot be $N$-optimal.
In addition, we establish that an arbitrary graph $G_N$ is $N$-optimal when its
minimum degree is $N - o(N)$, and may not be $N$-optimal even when its minimum
degree is $c N + o(N)$ for any $0 < c < 1/2$.
"
1199,"On the Convergence of the TTL Approximation for an LRU Cache under
  Independent Stationary Request Processes","  The modeling and analysis of an LRU cache is extremely challenging as exact
results for the main performance metrics (e.g. hit rate) are either lacking or
cannot be used because of their high computational complexity for large caches.
As a result, various approximations have been proposed. The state-of-the-art
method is the so-called TTL approximation, first proposed and shown to be
asymptotically exact for IRM requests by Fagin. It has been applied to various
other workload models and numerically demonstrated to be accurate but without
theoretical justification. In this paper we provide theoretical justification
for the approximation in the case where distinct contents are described by
independent stationary and ergodic processes. We show that this approximation
is exact as the cache size and the number of contents go to infinity. This
extends earlier results for the independent reference model. Moreover, we
establish results not only for the aggregate cache hit probability but also for
every individual content. Last, we obtain bounds on the rate of convergence.
"
1200,Fluid and Diffusion Limits for Bike Sharing Systems,"  Bike sharing systems have rapidly developed around the world, and they are
served as a promising strategy to improve urban traffic congestion and to
decrease polluting gas emissions. So far performance analysis of bike sharing
systems always exists many difficulties and challenges under some more general
factors. In this paper, a more general large-scale bike sharing system is
discussed by means of heavy traffic approximation of multiclass closed queueing
networks with non-exponential factors. Based on this, the fluid scaled
equations and the diffusion scaled equations are established by means of the
numbers of bikes both at the stations and on the roads, respectively.
Furthermore, the scaling processes for the numbers of bikes both at the
stations and on the roads are proved to converge in distribution to a
semimartingale reflecting Brownian motion (SRBM) in a $N^{2}$-dimensional box,
and also the fluid and diffusion limit theorems are obtained. Furthermore,
performance analysis of the bike sharing system is provided. Thus the results
and methodology of this paper provide new highlight in the study of more
general large-scale bike sharing systems.
"
1201,Towards Optimality in Parallel Scheduling,"  To keep pace with Moore's law, chip designers have focused on increasing the
number of cores per chip rather than single core performance. In turn, modern
jobs are often designed to run on any number of cores. However, to effectively
leverage these multi-core chips, one must address the question of how many
cores to assign to each job. Given that jobs receive sublinear speedups from
additional cores, there is an obvious tradeoff: allocating more cores to an
individual job reduces the job's runtime, but in turn decreases the efficiency
of the overall system. We ask how the system should schedule jobs across cores
so as to minimize the mean response time over a stream of incoming jobs.
  To answer this question, we develop an analytical model of jobs running on a
multi-core machine. We prove that EQUI, a policy which continuously divides
cores evenly across jobs, is optimal when all jobs follow a single speedup
curve and have exponentially distributed sizes. EQUI requires jobs to change
their level of parallelization while they run. Since this is not possible for
all workloads, we consider a class of ""fixed-width"" policies, which choose a
single level of parallelization, k, to use for all jobs. We prove that,
surprisingly, it is possible to achieve EQUI's performance without requiring
jobs to change their levels of parallelization by using the optimal fixed level
of parallelization, k*. We also show how to analytically derive the optimal k*
as a function of the system load, the speedup curve, and the job size
distribution.
  In the case where jobs may follow different speedup curves, finding a good
scheduling policy is even more challenging. We find that policies like EQUI
which performed well in the case of a single speedup function now perform
poorly. We propose a very simple policy, GREEDY*, which performs near-optimally
when compared to the numerically-derived optimal policy.
"
1202,"Asymptotic Performance Evaluation of Battery Swapping and Charging
  Station for Electric Vehicles","  A battery swapping and charging station (BSCS) is an energy refueling
station, where i) electric vehicles (EVs) with depleted batteries (DBs) can
swap their DBs for fully-charged ones, and ii) the swapped DBs are then charged
until they are fully-charged. Successful deployment of a BSCS system
necessitates a careful planning of swapping- and charging-related
infrastructures, and thus a comprehensive performance evaluation of the BSCS is
becoming crucial. This paper studies such a performance evaluation problem with
a novel mixed queueing network (MQN) model and validates this model with
extensive numerical simulation. We adopt the EVs' blocking probability as our
quality-of-service measure and focus on studying the impact of the key
parameters of the BSCS (e.g., the numbers of parking spaces, swapping islands,
chargers, and batteries) on the blocking probability. We prove a necessary and
sufficient condition for showing the ergodicity of the MQN when the number of
batteries approaches infinity, and further prove that the blocking probability
has two different types of asymptotic behaviors. Meanwhile, for each type of
asymptotic behavior, we analytically derive the asymptotic lower bound of the
blocking probability.
"
1203,"A Nonlinear Solution to Closed Queueing Networks for Bike Sharing
  Systems with Markovian Arrival Processes and under an Irreducible Path Graph","  As a favorite urban public transport mode, the bike sharing system is a
large-scale and complicated system, and there exists a key requirement that a
user and a bike should be matched sufficiently in time. Such matched behavior
makes analysis of the bike sharing systems more difficult and challenging. This
paper considers a more general large-scale bike sharing system from two
important views: (a) Bikes move in an irreducible path graph, which is related
to geographical structure of the bike sharing system; and (b) Markovian arrival
processes (MAPs) are applied to describe the non-Poisson and burst behavior of
bike-user (abbreviated as user) arrivals, while the burstiness demonstrates
that the user arrivals are time-inhomogeneous and space-heterogeneous in
practice. For such a complicated bike sharing system, this paper establishes a
multiclass closed queueing network by means of some virtual ideas, for example,
bikes are abstracted as virtual customers; stations and roads are regarded as
virtual nodes. Thus user arrivals are related to service times at station
nodes; and users riding bikes on roads are viewed as service times at road
nodes. Further, to deal with this multiclass closed queueing network, we
provide a detailed observation practically on physical behavior of the bike
sharing system in order to establish the routing matrix, which gives a
nonlinear solution to compute the relative arrival rates in terms of the
product-form solution to the steady-state probabilities of joint queue lengths
at the virtual nodes. Based on this, we can compute the steady-state
probability of problematic stations, and also deal with other interesting
performance measures of the bike sharing system. We hope that the methodology
and results of this paper can be applicable in the study of more general bike
sharing systems through multiclass closed queueing networks.
"
1204,"Approximations and Bounds for (n, k) Fork-Join Queues: A Linear
  Transformation Approach","  Compared to basic fork-join queues, a job in (n, k) fork-join queues only
needs its k out of all n sub-tasks to be finished. Since (n, k) fork-join
queues are prevalent in popular distributed systems, erasure coding based cloud
storages, and modern network protocols like multipath routing, estimating the
sojourn time of such queues is thus critical for the performance measurement
and resource plan of computer clusters. However, the estimating keeps to be a
well-known open challenge for years, and only rough bounds for a limited range
of load factors have been given. In this paper, we developed a closed-form
linear transformation technique for jointly-identical random variables: An
order statistic can be represented by a linear combination of maxima. This
brand-new technique is then used to transform the sojourn time of non-purging
(n, k) fork-join queues into a linear combination of the sojourn times of basic
(k, k), (k+1, k+1), ..., (n, n) fork-join queues. Consequently, existing
approximations for basic fork-join queues can be bridged to the approximations
for non-purging (n, k) fork-join queues. The uncovered approximations are then
used to improve the upper bounds for purging (n, k) fork-join queues.
Simulation experiments show that this linear transformation approach is
practiced well for moderate n and relatively large k.
"
1205,Address Translation Design Tradeoffs for Heterogeneous Systems,"  This paper presents a broad, pathfinding design space exploration of memory
management units (MMUs) for heterogeneous systems. We consider a variety of
designs, ranging from accelerators tightly coupled with CPUs (and using their
MMUs) to fully independent accelerators that have their own MMUs. We find that
regardless of the CPU-accelerator communication, accelerators should not rely
on the CPU MMU for any aspect of address translation, and instead must have its
own, local, fully-fledged MMU. That MMU, however, can and should be as
application-specific as the accelerator itself, as our data indicates that even
a 100% hit rate in a small, standard L1 Translation Lookaside Buffer (TLB)
presents a substantial accelerator performance overhead. Furthermore, we
isolate the benefits of individual MMU components (e.g., TLBs versus page table
walkers) and discover that their relative performance, area, and energy are
workload dependent, with their interplay resulting in different area-optimal
and energy-optimal configurations.
"
1206,"Adaptive Performance Optimization under Power Constraint in Multi-thread
  Applications with Diverse Scalability","  In modern data centers, energy usage represents one of the major factors
affecting operational costs. Power capping is a technique that limits the power
consumption of individual systems, which allows reducing the overall power
demand at both cluster and data center levels. However, literature power
capping approaches do not fit well the nature of important applications based
on first-class multi-thread technology. For these applications performance may
not grow linearly as a function of the thread-level parallelism because of the
need for thread synchronization while accessing shared resources, such as
shared data. In this paper we consider the problem of maximizing the
application performance under a power cap by dynamically tuning the
thread-level parallelism and the power state of the CPU-cores. Based on
experimental observations, we design an adaptive technique that selects in
linear time the optimal combination of thread-level parallelism and CPU-core
power state for the specific workload profile of the multi-threaded
application. We evaluate our proposal by relying on different benchmarks,
configured to use different thread synchronization methods, and compare its
effectiveness to different state-of-the-art techniques.
"
1207,Performance Measurements of Supercomputing and Cloud Storage Solutions,"  Increasing amounts of data from varied sources, particularly in the fields of
machine learning and graph analytics, are causing storage requirements to grow
rapidly. A variety of technologies exist for storing and sharing these data,
ranging from parallel file systems used by supercomputers to distributed block
storage systems found in clouds. Relatively few comparative measurements exist
to inform decisions about which storage systems are best suited for particular
tasks. This work provides these measurements for two of the most popular
storage technologies: Lustre and Amazon S3. Lustre is an open-source, high
performance, parallel file system used by many of the largest supercomputers in
the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web
Services offering, and offers a scalable, distributed option to store and
retrieve data from anywhere on the Internet. Parallel processing is essential
for achieving high performance on modern storage systems. The performance tests
used span the gamut of parallel I/O scenarios, ranging from single-client,
single-node Amazon S3 and Lustre performance to a large-scale, multi-client
test designed to demonstrate the capabilities of a modern storage appliance
under heavy load. These results show that, when parallel I/O is used correctly
(i.e., many simultaneous read or write processes), full network bandwidth
performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3
connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These
results demonstrate that S3 is well-suited to sharing vast quantities of data
over the Internet, while Lustre is well-suited to processing large quantities
of data locally.
"
1208,On Evaluating Commercial Cloud Services: A Systematic Review,"  Background: Cloud Computing is increasingly booming in industry with many
competing providers and services. Accordingly, evaluation of commercial Cloud
services is necessary. However, the existing evaluation studies are relatively
chaotic. There exists tremendous confusion and gap between practices and theory
about Cloud services evaluation. Aim: To facilitate relieving the
aforementioned chaos, this work aims to synthesize the existing evaluation
implementations to outline the state-of-the-practice and also identify research
opportunities in Cloud services evaluation. Method: Based on a conceptual
evaluation model comprising six steps, the Systematic Literature Review (SLR)
method was employed to collect relevant evidence to investigate the Cloud
services evaluation step by step. Results: This SLR identified 82 relevant
evaluation studies. The overall data collected from these studies essentially
represent the current practical landscape of implementing Cloud services
evaluation, and in turn can be reused to facilitate future evaluation work.
Conclusions: Evaluation of commercial Cloud services has become a world-wide
research topic. Some of the findings of this SLR identify several research gaps
in the area of Cloud services evaluation (e.g., the Elasticity and Security
evaluation of commercial Cloud services could be a long-term challenge), while
some other findings suggest the trend of applying commercial Cloud services
(e.g., compared with PaaS, IaaS seems more suitable for customers and is
particularly important in industry). This SLR study itself also confirms some
previous experiences and reveals new Evidence-Based Software Engineering (EBSE)
lessons.
"
1209,"Boosting Metrics for Cloud Services Evaluation -- The Last Mile of Using
  Benchmark Suites","  Benchmark suites are significant for evaluating various aspects of Cloud
services from a holistic view. However, there is still a gap between using
benchmark suites and achieving holistic impression of the evaluated Cloud
services. Most Cloud service evaluation work intended to report individual
benchmarking results without delivering summary measures. As a result, it could
be still hard for customers with such evaluation reports to understand an
evaluated Cloud service from a global perspective. Inspired by the boosting
approaches to machine learning, we proposed the concept Boosting Metrics to
represent all the potential approaches that are able to integrate a suite of
benchmarking results. This paper introduces two types of preliminary boosting
metrics, and demonstrates how the boosting metrics can be used to supplement
primary measures of individual Cloud service features. In particular, boosting
metrics can play a summary Response role in applying experimental design to
Cloud services evaluation. Although the concept Boosting Metrics was refined
based on our work in the Cloud Computing domain, we believe it can be easily
adapted to the evaluation work of other computing paradigms.
"
1210,"DoKnowMe: Towards a Domain Knowledge-driven Methodology for Performance
  Evaluation","  Software engineering considers performance evaluation to be one of the key
portions of software quality assurance. Unfortunately, there seems to be a lack
of standard methodologies for performance evaluation even in the scope of
experimental computer science. Inspired by the concept of ""instantiation"" in
object-oriented programming, we distinguish the generic performance evaluation
logic from the distributed and ad-hoc relevant studies, and develop an abstract
evaluation methodology (by analogy of ""class"") we name Domain Knowledge-driven
Methodology (DoKnowMe). By replacing five predefined domain-specific knowledge
artefacts, DoKnowMe could be instantiated into specific methodologies (by
analogy of ""object"") to guide evaluators in performance evaluation of different
software and even computing systems. We also propose a generic validation
framework with four indicators (i.e.~usefulness, feasibility, effectiveness and
repeatability), and use it to validate DoKnowMe in the Cloud services
evaluation domain. Given the positive and promising validation result, we plan
to integrate more common evaluation strategies to improve DoKnowMe and further
focus on the performance evaluation of Cloud autoscaler systems.
"
1211,On Resource Pooling and Separation for LRU Caching,"  Caching systems using the Least Recently Used (LRU) principle have now become
ubiquitous. A fundamental question for these systems is whether the cache space
should be pooled together or divided to serve multiple flows of data item
requests in order to minimize the miss probabilities. In this paper, we show
that there is no straight yes or no answer to this question, depending on
complex combinations of critical factors, including, e.g., request rates,
overlapped data items across different request flows, data item popularities
and their sizes. Specifically, we characterize the asymptotic miss
probabilities for multiple competing request flows under resource pooling and
separation for LRU caching when the cache size is large.
  Analytically, we show that it is asymptotically optimal to jointly serve
multiple flows if their data item sizes and popularity distributions are
similar and their arrival rates do not differ significantly; the
self-organizing property of LRU caching automatically optimizes the resource
allocation among them asymptotically. Otherwise, separating these flows could
be better, e.g., when data sizes vary significantly. We also quantify critical
points beyond which resource pooling is better than separation for each of the
flows when the overlapped data items exceed certain levels. Technically, we
generalize existing results on the asymptotic miss probability of LRU caching
for a broad class of heavy-tailed distributions and extend them to multiple
competing flows with varying data item sizes, which also validates the Che
approximation under certain conditions. These results provide new insights on
improving the performance of caching systems.
"
1212,"GARDENIA: A Domain-specific Benchmark Suite for Next-generation
  Accelerators","  This paper presents the Graph Analytics Repository for Designing
Next-generation Accelerators (GARDENIA), a benchmark suite for studying
irregular algorithms on massively parallel accelerators. Existing generic
benchmarks for accelerators have mainly focused on high performance computing
(HPC) applications with limited control and data irregularity, while available
graph analytics benchmarks do not apply state-of-the-art algorithms and/or
optimization techniques. GARDENIA includes emerging irregular applications in
big-data and machine learning domains which mimic massively multithreaded
commercial programs running on modern large-scale datacenters. Our
characterization shows that GARDENIA exhibits irregular microarchitectural
behavior which is quite different from structured workloads and
straightforward-implemented graph benchmarks.
"
1213,"Strategies for Big Data Analytics through Lambda Architectures in
  Volatile Environments","  Expectations regarding the future growth of Internet of Things (IoT)-related
technologies are high. These expectations require the realization of a
sustainable general purpose application framework that is capable to handle
these kinds of environments with their complexity in terms of heterogeneity and
volatility. The paradigm of the Lambda architecture features key
characteristics (such as robustness, fault tolerance, scalability,
generalization, extensibility, ad-hoc queries, minimal maintenance, and
low-latency reads and updates) to cope with this complexity. The paper at hand
suggest a basic set of strategies to handle the arising challenges regarding
the volatility, heterogeneity, and desired low latency execution by reducing
the overall system timing (scheduling, execution, monitoring, and faults
recovery) as well as possible faults (churn, no answers to executions). The
proposed strategies make use of services such as migration, replication,
MapReduce simulation, and combined processing methods (batch- and
streaming-based). Via these services, a distribution of tasks for the best
balance of computational resources is achieved, while monitoring and management
can be performed asynchronously in the background. %An application of batch and
stream-based methods are proposed to reduce the latency.
"
1214,"Deep Learning at 15PF: Supervised and Semi-Supervised Classification for
  Scientific Data","  This paper presents the first, 15-PetaFLOP Deep Learning system for solving
scientific pattern classification problems on contemporary HPC architectures.
We develop supervised convolutional architectures for discriminating signals in
high-energy physics data as well as semi-supervised architectures for
localizing and classifying extreme weather in climate data. Our
Intelcaffe-based implementation obtains $\sim$2TFLOP/s on a single Cori
Phase-II Xeon-Phi node. We use a hybrid strategy employing synchronous
node-groups, while using asynchronous communication across groups. We use this
strategy to scale training of a single model to $\sim$9600 Xeon-Phi nodes;
obtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of
11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art
classification accuracy on a dataset with 10M images, exceeding that achieved
by selections on high-level physics-motivated features. Our semi-supervised
architecture successfully extracts weather patterns in a 15TB climate dataset.
Our results demonstrate that Deep Learning can be optimized and scaled
effectively on many-core, HPC systems.
"
1215,Unbounded product-form Petri nets,"  Computing steady-state distributions in infinite-state stochastic systems is
in general a very dificult task. Product-form Petri nets are those Petri nets
for which the steady-state distribution can be described as a natural product
corresponding, up to a normalising constant, to an exponentiation of the
markings. However, even though some classes of nets are known to have a
product-form distribution, computing the normalising constant can be hard. The
class of (closed) {\Pi}3-nets has been proposed in an earlier work, for which
it is shown that one can compute the steady-state distribution efficiently.
However these nets are bounded. In this paper, we generalise queuing Markovian
networks and closed {\Pi}3-nets to obtain the class of open {\Pi}3-nets, that
generate infinite-state systems. We show interesting properties of these nets:
(1) we prove that liveness can be decided in polynomial time, and that
reachability in live {\Pi}3-nets can be decided in polynomial time; (2) we show
that we can decide ergodicity of such nets in polynomial time as well; (3) we
provide a pseudo-polynomial time algorithm to compute the normalising constant.
"
1216,Optimal Threshold Policies for Robust Data Center Control,"  With the simultaneous rise of energy costs and demand for cloud computing,
efficient control of data centers becomes crucial. In the data center control
problem, one needs to plan at every time step how many servers to switch on or
off in order to meet stochastic job arrivals while trying to minimize
electricity consumption. This problem becomes particularly challenging when
servers can be of various types and jobs from different classes can only be
served by certain types of server, as it is often the case in real data
centers. We model this problem as a robust Markov Decision Process (i.e., the
transition function is not assumed to be known precisely). We give sufficient
conditions (which seem to be reasonable and satisfied in practice) guaranteeing
that an optimal threshold policy exists. This property can then be exploited in
the design of an efficient solving method, which we provide. Finally, we
present some experimental results demonstrating the practicability of our
approach and compare with a previous related approach based on model predictive
control.
"
1217,"Efficient Adaptive Implementation of the Serial Schedule Generation
  Scheme using Preprocessing and Bloom Filters","  The majority of scheduling metaheuristics use indirect representation of
solutions as a way to efficiently explore the search space. Thus, a crucial
part of such metaheuristics is a ""schedule generation scheme"" -- procedure
translating the indirect solution representation into a schedule. Schedule
generation scheme is used every time a new candidate solution needs to be
evaluated. Being relatively slow, it eats up most of the running time of the
metaheuristic and, thus, its speed plays significant role in performance of the
metaheuristic. Despite its importance, little attention has been paid in the
literature to efficient implementation of schedule generation schemes. We give
detailed description of serial schedule generation scheme, including new
improvements, and propose a new approach for speeding it up, by using Bloom
filters. The results are further strengthened by automated control of
parameters. Finally, we employ online algorithm selection to dynamically choose
which of the two implementations to use. This hybrid approach significantly
outperforms conventional implementation on a wide range of instances.
"
1218,Streaming Graph Challenge: Stochastic Block Partition,"  An important objective for analyzing real-world graphs is to achieve scalable
performance on large, streaming graphs. A challenging and relevant example is
the graph partition problem. As a combinatorial problem, graph partition is
NP-hard, but existing relaxation methods provide reasonable approximate
solutions that can be scaled for large graphs. Competitive benchmarks and
challenges have proven to be an effective means to advance state-of-the-art
performance and foster community collaboration. This paper describes a graph
partition challenge with a baseline partition algorithm of sub-quadratic
complexity. The algorithm employs rigorous Bayesian inferential methods based
on a statistical model that captures characteristics of the real-world graphs.
This strong foundation enables the algorithm to address limitations of
well-known graph partition approaches such as modularity maximization. This
paper describes various aspects of the challenge including: (1) the data sets
and streaming graph generator, (2) the baseline partition algorithm with
pseudocode, (3) an argument for the correctness of parallelizing the Bayesian
inference, (4) different parallel computation strategies such as node-based
parallelism and matrix-based parallelism, (5) evaluation metrics for partition
correctness and computational requirements, (6) preliminary timing of a
Python-based demonstration code and the open source C++ code, and (7)
considerations for partitioning the graph in streaming fashion. Data sets and
source code for the algorithm as well as metrics, with detailed documentation
are available at GraphChallenge.org.
"
1219,"Performance Analysis of Open Source Machine Learning Frameworks for
  Various Parameters in Single-Threaded and Multi-Threaded Modes","  The basic features of some of the most versatile and popular open source
frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are
considered and compared. Their comparative analysis was performed and
conclusions were made as to the advantages and disadvantages of these
platforms. The performance tests for the de facto standard MNIST data set were
carried out on H2O framework for deep learning algorithms designed for CPU and
GPU platforms for single-threaded and multithreaded modes of operation Also, we
present the results of testing neural networks architectures on H2O platform
for various activation functions, stopping metrics, and other parameters of
machine learning algorithm. It was demonstrated for the use case of MNIST
database of handwritten digits in single-threaded mode that blind selection of
these parameters can hugely increase (by 2-3 orders) the runtime without the
significant increase of precision. This result can have crucial influence for
optimization of available and new machine learning methods, especially for
image recognition problems.
"
1220,"Insensitivity of the mean-field Limit of Loss Systems Under Power-of-d
  Routing","  In this paper, we study large multi-server loss models under power-of-$d$
routing scheme when service time distributions are general with finite mean.
Previous works have addressed the exponential service time case when the number
of servers goes to infinity giving rise to a mean field model. The fixed point
of limiting mean field equations (MFE) was shown to be insensitive to the
service time distribution through simulation. Showing insensitivity to general
service time distributions has remained an open problem. Obtaining the MFE in
this case poses a challenge due to the resulting Markov description of the
system being in positive orthant as opposed to a finite chain in the
exponential case. In this paper, we first obtain the MFE and then show that the
MFE has a unique fixed point that coincides with the fixed point in the
exponential case thus establishing insensitivity. The approach is via a
measure-valued Markov process representation and the martingale problem to
establish the mean-field limit. The techniques can be applied to other queueing
models.
"
1221,"A domain-specific language and matrix-free stencil code for
  investigating electronic properties of Dirac and topological materials","  We introduce PVSC-DTM (Parallel Vectorized Stencil Code for Dirac and
Topological Materials), a library and code generator based on a domain-specific
language tailored to implement the specific stencil-like algorithms that can
describe Dirac and topological materials such as graphene and topological
insulators in a matrix-free way. The generated hybrid-parallel (MPI+OpenMP)
code is fully vectorized using Single Instruction Multiple Data (SIMD)
extensions. It is significantly faster than matrix-based approaches on the node
level and performs in accordance with the roofline model. We demonstrate the
chip-level performance and distributed-memory scalability of basic building
blocks such as sparse matrix-(multiple-) vector multiplication on modern
multicore CPUs. As an application example, we use the PVSC-DTM scheme to (i)
explore the scattering of a Dirac wave on an array of gate-defined quantum
dots, to (ii) calculate a bunch of interior eigenvalues for strong topological
insulators, and to (iii) discuss the photoemission spectra of a disordered Weyl
semimetal.
"
1222,"Galactos: Computing the Anisotropic 3-Point Correlation Function for 2
  Billion Galaxies","  The nature of dark energy and the complete theory of gravity are two central
questions currently facing cosmology. A vital tool for addressing them is the
3-point correlation function (3PCF), which probes deviations from a spatially
random distribution of galaxies. However, the 3PCF's formidable computational
expense has prevented its application to astronomical surveys comprising
millions to billions of galaxies. We present Galactos, a high-performance
implementation of a novel, O(N^2) algorithm that uses a load-balanced k-d tree
and spherical harmonic expansions to compute the anisotropic 3PCF. Our
implementation is optimized for the Intel Xeon Phi architecture, exploiting
SIMD parallelism, instruction and thread concurrency, and significant L1 and L2
cache reuse, reaching 39% of peak performance on a single node. Galactos scales
to the full Cori system, achieving 9.8PF (peak) and 5.06PF (sustained) across
9636 nodes, making the 3PCF easily computable for all galaxies in the
observable universe.
"
1223,Kafka versus RabbitMQ,"  Publish/subscribe is a distributed interaction paradigm well adapted to the
deployment of scalable and loosely coupled systems.
  Apache Kafka and RabbitMQ are two popular open-source and
commercially-supported pub/sub systems that have been around for almost a
decade and have seen wide adoption. Given the popularity of these two systems
and the fact that both are branded as pub/sub systems, two frequently asked
questions in the relevant online forums are: how do they compare against each
other and which one to use?
  In this paper, we frame the arguments in a holistic approach by establishing
a common comparison framework based on the core functionalities of pub/sub
systems. Using this framework, we then venture into a qualitative and
quantitative (i.e. empirical) comparison of the common features of the two
systems. Additionally, we also highlight the distinct features that each of
these systems has. After enumerating a set of use cases that are best suited
for RabbitMQ or Kafka, we try to guide the reader through a determination table
to choose the best architecture given his/her particular set of requirements.
"
1224,"Faster Concurrent Range Queries with Contention Adapting Search Trees
  Using Immutable Data","  The need for scalable concurrent ordered set data structures with
linearizable range query support is increasing due to the rise of multicore
computers, data processing platforms and in-memory databases. This paper
presents a new concurrent ordered set with linearizable range query support.
The new data structure is based on the contention adapting search tree and an
immutable data structure. Experimental results show that the new data structure
is as much as three times faster compared to related data structures. The data
structure scales well due to its ability to adapt the sizes of its immutable
parts to the contention level and the sizes of the range queries.
"
1225,"FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High
  Dimensional Similarity Search","  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
  We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force ($n^2D$), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.
"
1226,"Queueing systems with renovation vs. queues with RED. Supplementary
  Material","  In this note we consider M/D/1/N queue with renovation and derive analytic
expressions for the following performance characteristics: stationary loss
rate, moments of the number in the system. Moments of consecutive losses,
waiting/sojourn time are out of scope. The motivation for studying these
characteristics is in the comparison of renovation with known active queue
mechanisms like RED.
"
1227,A Parallel Linear Temporal Logic Tableau,"  For many applications, we are unable to take full advantage of the potential
massive parallelisation offered by supercomputers or cloud computing because it
is too hard to work out how to divide up the computation task between
processors in such a way to minimise the need for communication. However, a
recently developed branch-independent tableaux for the common LTL temporal
logic should intuitively be easy to parallelise as each branch can be developed
independently. Here we describe a simple technique for partitioning such a
tableau such that each partition can be processed independently without need
for interprocess communication. We investigate the extent to which this
technique improves the performance of the LTL tableau on standard benchmarks
and random formulas.
"
1228,"ParaPlan: A Tool for Parallel Reachability Analysis of Planar Polygonal
  Differential Inclusion Systems","  We present the ParaPlan tool which provides the reachability analysis of
planar hybrid systems defined by differential inclusions (SPDI). It uses the
parallelized and optimized version of the algorithm underlying the SPeeDI tool.
The performance comparison demonstrates the speed-up of up to 83 times with
respect to the sequential implementation on various benchmarks. Some of the
benchmarks we used are randomly generated with the novel approach based on the
partitioning of the plane with Voronoi diagrams.
"
1229,Enhancing KiWi - Scalable Concurrent Key-Value Map,"  We take a relatively fresh wait-free, concurrent sorted map called KiWi, fix
and enhance it. First, we test its linearizability by fuzzing and applying
Wing&Gong [2] linearizability test. After fixing a few bugs in the algorithm
design and its implementation, we enhance it. We design, implement and test two
new linearizable operations sizeLowerBound() and sizeUpperBound(). We further
compose these operations to create more useful operations. Last, we evaluate
the map performance because previous evaluations became obsolete due to our bug
corrections.
"
1230,"Transfer Learning for Performance Modeling of Configurable Systems: An
  Exploratory Analysis","  Modern software systems provide many configuration options which
significantly influence their non-functional properties. To understand and
predict the effect of configuration options, several sampling and learning
strategies have been proposed, albeit often with significant cost to cover the
highly dimensional configuration space. Recently, transfer learning has been
applied to reduce the effort of constructing performance models by transferring
knowledge about performance behavior across environments. While this line of
research is promising to learn more accurate models at a lower cost, it is
unclear why and when transfer learning works for performance modeling. To shed
light on when it is beneficial to apply transfer learning, we conducted an
empirical study on four popular software systems, varying software
configurations and environmental conditions, such as hardware, workload, and
software versions, to identify the key knowledge pieces that can be exploited
for transfer learning. Our results show that in small environmental changes
(e.g., homogeneous workload change), by applying a linear transformation to the
performance model, we can understand the performance behavior of the target
environment, while for severe environmental changes (e.g., drastic workload
change) we can transfer only knowledge that makes sampling more efficient,
e.g., by reducing the dimensionality of the configuration space.
"
1231,"Super-speeds with Zero-RAM: Next Generation Large-Scale Optimization in
  Your Laptop!","  This article presents the novel breakthrough general purpose algorithm for
large scale optimization problems. The novel algorithm is capable of achieving
breakthrough speeds for very large-scale optimization on general purpose
laptops and embedded systems. Application of the algorithm to the Griewank
function was possible in up to 1 billion decision variables in double precision
took only 64485 seconds (~18 hours) to solve, while consuming 7,630 MB (7.6 GB)
or RAM on a single threaded laptop CPU. It shows that the algorithm is
computationally and memory (space) linearly efficient, and can find the optimal
or near-optimal solution in a fraction of the time and memory that many
conventional algorithms require. It is envisaged that this will open up new
possibilities of real-time large-scale problems on personal laptops and
embedded systems.
"
1232,On-Disk Data Processing: Issues and Future Directions,"  In this paper, we present a survey of ""on-disk"" data processing (ODDP). ODDP,
which is a form of near-data processing, refers to the computing arrangement
where the secondary storage drives have the data processing capability.
Proposed ODDP schemes vary widely in terms of the data processing capability,
target applications, architecture and the kind of storage drive employed. Some
ODDP schemes provide only a specific but heavily used operation like sort
whereas some provide a full range of operations. Recently, with the advent of
Solid State Drives, powerful and extensive ODDP solutions have been proposed.
In this paper, we present a thorough review of architectures developed for
different on-disk processing approaches along with current and future
challenges and also identify the future directions which ODDP can take.
"
1233,"Report: Performance comparison between C2075 and P100 GPU cards using
  cosmological correlation functions","  In this report, some cosmological correlation functions are used to evaluate
the differential performance between C2075 and P100 GPU cards. In the past, the
correlation functions used in this work have been widely studied and exploited
on some previous GPU architectures. The analysis of the performance indicates
that a speedup in the range from 13 to 15 is achieved without any additional
optimization process for the P100 card.
"
1234,"Proceedings of the 4th OMNeT++ Community Summit, University of Bremen -
  Germany, September 7-8, 2017","  These are the Proceedings of the 4th OMNeT++ Community Summit, which was held
at the University of Bremen - Germany - on September 7-8, 2017.
"
1235,"Fast semi-supervised discriminant analysis for binary classification of
  large data-sets","  High-dimensional data requires scalable algorithms. We propose and analyze
three scalable and related algorithms for semi-supervised discriminant analysis
(SDA). These methods are based on Krylov subspace methods which exploit the
data sparsity and the shift-invariance of Krylov subspaces. In addition, the
problem definition was improved by adding centralization to the semi-supervised
setting. The proposed methods are evaluated on a industry-scale data set from a
pharmaceutical company to predict compound activity on target proteins. The
results show that SDA achieves good predictive performance and our methods only
require a few seconds, significantly improving computation time on previous
state of the art.
"
1236,Reactive User Behavior and Mobility Models,"  In this paper, we present a set of simulation models to more realistically
mimic the behaviour of users reading messages. We propose a User Behaviour
Model, where a simulated user reacts to a message by a flexible set of possible
reactions (e.g. ignore, read, like, save, etc.) and a mobility-based reaction
(visit a place, run away from danger, etc.). We describe our models and their
implementation in OMNeT++. We strongly believe that these models will
significantly contribute to the state of the art of simulating realistically
opportunistic networks.
"
1237,Weld: Rethinking the Interface Between Data-Intensive Applications,"  Data analytics applications combine multiple functions from different
libraries and frameworks. Even when each function is optimized in isolation,
the performance of the combined application can be an order of magnitude below
hardware limits due to extensive data movement across these functions. To
address this problem, we propose Weld, a new interface between data-intensive
libraries that can optimize across disjoint libraries and functions. Weld
exposes a lazily-evaluated API where diverse functions can submit their
computations in a simple but general intermediate representation that captures
their data-parallel structure. It then optimizes data movement across these
functions and emits efficient code for diverse hardware. Weld can be integrated
into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without
changing their user-facing APIs. We demonstrate that Weld can speed up
applications using these frameworks by up to 29x.
"
1238,Radio Irregularity Model in OMNeT++,"  Radio irregularity is a non-negligible phenomenon that has an impact on
protocol performances. For instance, irregularity in radio range leads to
asymmetric links that cause the loss of packets in different directions. In
order to investigate its effect, the Radio Irregularity Model (RIM) is proposed
that takes into account the irregularity of a radio range and estimates path
losses in an anisotropic environment. The purpose of this paper is to provide
details of the RIM model developed in the INET Framework of the OMNeT++
simulator that can be used to investigate the impact of radio irregularity on
protocol performance.
"
1239,Accelerating PageRank using Partition-Centric Processing,"  PageRank is a fundamental link analysis algorithm that also functions as a
key representative of the performance of Sparse Matrix-Vector (SpMV)
multiplication. The traditional PageRank implementation generates fine
granularity random memory accesses resulting in large amount of wasteful DRAM
traffic and poor bandwidth utilization. In this paper, we present a novel
Partition-Centric Processing Methodology (PCPM) to compute PageRank, that
drastically reduces the amount of DRAM communication while achieving high
sustained memory bandwidth. PCPM uses a Partition-centric abstraction coupled
with the Gather-Apply-Scatter (GAS) programming model. By carefully examining
how a PCPM based implementation impacts communication characteristics of the
algorithm, we propose several system optimizations that improve the execution
time substantially. More specifically, we develop (1) a new data layout that
significantly reduces communication and random DRAM accesses, and (2) branch
avoidance mechanisms to get rid of unpredictable data-dependent branches.
  We perform detailed analytical and experimental evaluation of our approach
using 6 large graphs and demonstrate an average 2.7x speedup in execution time
and 1.7x reduction in communication volume, compared to the state-of-the-art.
We also show that unlike other GAS based implementations, PCPM is able to
further reduce main memory traffic by taking advantage of intelligent node
labeling that enhances locality. Although we use PageRank as the target
application in this paper, our approach can be applied to generic SpMV
computation.
"
1240,"A Zero-Positive Learning Approach for Diagnosing Software Performance
  Regressions","  The field of machine programming (MP), the automation of the development of
software, is making notable research advances. This is, in part, due to the
emergence of a wide range of novel techniques in machine learning. In this
paper, we apply MP to the automation of software performance regression
testing. A performance regression is a software performance degradation caused
by a code change. We present AutoPerf - a novel approach to automate regression
testing that utilizes three core techniques: (i) zero-positive learning, (ii)
autoencoders, and (iii) hardware telemetry. We demonstrate AutoPerf's
generality and efficacy against 3 types of performance regressions across 10
real performance bugs in 7 benchmark and open-source programs. On average,
AutoPerf exhibits 4% profiling overhead and accurately diagnoses more
performance bugs than prior state-of-the-art approaches. Thus far, AutoPerf has
produced no false negatives.
"
1241,"GB-PANDAS: Throughput and heavy-traffic optimality analysis for affinity
  scheduling","  Dynamic affinity scheduling has been an open problem for nearly three
decades. The problem is to dynamically schedule multi-type tasks to
multi-skilled servers such that the resulting queueing system is both stable in
the capacity region (throughput optimality) and the mean delay of tasks is
minimized at high loads near the boundary of the capacity region (heavy-traffic
optimality). As for applications, data-intensive analytics like MapReduce,
Hadoop, and Dryad fit into this setting, where the set of servers is
heterogeneous for different task types, so the pair of task type and server
determines the processing rate of the task. The load balancing algorithm used
in such frameworks is an example of affinity scheduling which is desired to be
both robust and delay optimal at high loads when hot-spots occur. Fluid model
planning, the MaxWeight algorithm, and the generalized $c\mu$-rule are among
the first algorithms proposed for affinity scheduling that have theoretical
guarantees on being optimal in different senses, which will be discussed in the
related work section. All these algorithms are not practical for use in data
center applications because of their non-realistic assumptions. The
join-the-shortest-queue-MaxWeight (JSQ-MaxWeight), JSQ-Priority, and
weighted-workload algorithms are examples of load balancing policies for
systems with two and three levels of data locality with a rack structure. In
this work, we propose the Generalized-Balanced-Pandas algorithm (GB-PANDAS) for
a system with multiple levels of data locality and prove its throughput
optimality. We prove this result under an arbitrary distribution for service
times, whereas most previous theoretical work assumes geometric distribution
for service times. The extensive simulation results show that the GB-PANDAS
algorithm alleviates the mean delay and has a better performance than the
JSQ-MaxWeight algorithm by twofold
"
1242,"Report from GI-Dagstuhl Seminar 16394: Software Performance Engineering
  in the DevOps World","  This report documents the program and the outcomes of GI-Dagstuhl Seminar
16394 ""Software Performance Engineering in the DevOps World"".
  The seminar addressed the problem of performance-aware DevOps. Both, DevOps
and performance engineering have been growing trends over the past one to two
years, in no small part due to the rise in importance of identifying
performance anomalies in the operations (Ops) of cloud and big data systems and
feeding these back to the development (Dev). However, so far, the research
community has treated software engineering, performance engineering, and cloud
computing mostly as individual research areas. We aimed to identify
cross-community collaboration, and to set the path for long-lasting
collaborations towards performance-aware DevOps.
  The main goal of the seminar was to bring together young researchers (PhD
students in a later stage of their PhD, as well as PostDocs or Junior
Professors) in the areas of (i) software engineering, (ii) performance
engineering, and (iii) cloud computing and big data to present their current
research projects, to exchange experience and expertise, to discuss research
challenges, and to develop ideas for future collaborations.
"
1243,Tensors Come of Age: Why the AI Revolution will help HPC,"  This article discusses how the automation of tensor algorithms, based on A
Mathematics of Arrays and Psi Calculus, and a new way to represent numbers,
Unum Arithmetic, enables mechanically provable, scalable, portable, and more
numerically accurate software.
"
1244,Symbolic Computation of the Worst-Case Execution Time of a Program,"  Parametric Worst-case execution time (WCET) analysis of a sequential program
produces a formula that represents the worst-case execution time of the
program, where parameters of the formula are user-defined parameters of the
program (as loop bounds, values of inputs or internal variables, etc).
  In this paper we propose a novel methodology to compute the parametric WCET
of a program. Unlike other algorithms in the literature, our method is not
based on Integer Linear Programming (ILP). Instead, we follow an approach based
on the notion of symbolic computation of WCET formulae. After explaining our
methodology and proving its correctness, we present a set of experiments to
compare our method against the state of the art. We show that our approach
dominates other parametric analyses, and produces results that are very close
to those produced by non-parametric ILP-based approaches, while keeping very
good computing time.
"
1245,"Energy efficiency of finite difference algorithms on multicore CPUs,
  GPUs, and Intel Xeon Phi processors","  In addition to hardware wall-time restrictions commonly seen in
high-performance computing systems, it is likely that future systems will also
be constrained by energy budgets. In the present work, finite difference
algorithms of varying computational and memory intensity are evaluated with
respect to both energy efficiency and runtime on an Intel Ivy Bridge CPU node,
an Intel Xeon Phi Knights Landing processor, and an NVIDIA Tesla K40c GPU. The
conventional way of storing the discretised derivatives to global arrays for
solution advancement is found to be inefficient in terms of energy consumption
and runtime. In contrast, a class of algorithms in which the discretised
derivatives are evaluated on-the-fly or stored as thread-/process-local
variables (yielding high compute intensity) is optimal both with respect to
energy consumption and runtime. On all three hardware architectures considered,
a speed-up of ~2 and an energy saving of ~2 are observed for the high compute
intensive algorithms compared to the memory intensive algorithm. The energy
consumption is found to be proportional to runtime, irrespective of the power
consumed and the GPU has an energy saving of ~5 compared to the same algorithm
on a CPU node.
"
1246,"Performance Evaluation of Container-based Virtualization for High
  Performance Computing Environments","  Virtualization technologies have evolved along with the development of
computational environments since virtualization offered needed features at that
time such as isolation, accountability, resource allocation, resource fair
sharing and so on. Novel processor technologies bring to commodity computers
the possibility to emulate diverse environments where a wide range of
computational scenarios can be run. Along with processors evolution, system
developers have created different virtualization mechanisms where each new
development enhanced the performance of previous virtualized environments.
Recently, operating system-based virtualization technologies captured the
attention of communities abroad (from industry to academy and research) because
their important improvements on performance area.
  In this paper, the features of three container-based operating systems
virtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker,
Singularity and bare metal are put under test through a customized single node
HPL-Benchmark and a MPI-based application for the multi node testbed. Also the
disk I/O performance, Memory (RAM) performance, Network bandwidth and GPU
performance are tested for the COS technologies vs bare metal. Preliminary
results and conclusions around them are presented and discussed.
"
1247,Efficient Pattern Matching in Python,"  Pattern matching is a powerful tool for symbolic computations. Applications
include term rewriting systems, as well as the manipulation of symbolic
expressions, abstract syntax trees, and XML and JSON data. It also allows for
an intuitive description of algorithms in the form of rewrite rules. We present
the open source Python module MatchPy, which offers functionality and
expressiveness similar to the pattern matching in Mathematica. In particular,
it includes syntactic pattern matching, as well as matching for commutative
and/or associative functions, sequence variables, and matching with
constraints. MatchPy uses new and improved algorithms to efficiently find
matches for large pattern sets by exploiting similarities between patterns. The
performance of MatchPy is investigated on several real-world problems.
"
1248,An Efficient Load Balancing Method for Tree Algorithms,"  Nowadays, multiprocessing is mainstream with exponentially increasing number
of processors. Load balancing is, therefore, a critical operation for the
efficient execution of parallel algorithms. In this paper we consider the
fundamental class of tree-based algorithms that are notoriously irregular, and
hard to load-balance with existing static techniques. We propose a hybrid load
balancing method using the utility of statistical random sampling in estimating
the tree depth and node count distributions to uniformly partition an input
tree. To conduct an initial performance study, we implemented the method on an
Intel Xeon Phi accelerator system. We considered the tree traversal operation
on both regular and irregular unbalanced trees manifested by Fibonacci and
unbalanced (biased) randomly generated trees, respectively. The results show
scalable performance for up to the 60 physical processors of the accelerator,
as well as an extrapolated 128 processors case.
"
1249,Delay Asymptotics and Bounds for Multi-Task Parallel Jobs,"  We study delay of jobs that consist of multiple parallel tasks, which is a
critical performance metric in a wide range of applications such as data file
retrieval in coded storage systems and parallel computing. In this problem,
each job is completed only when all of its tasks are completed, so the delay of
a job is the maximum of the delays of its tasks. Despite the wide attention
this problem has received, tight analysis is still largely unknown since
analyzing job delay requires characterizing the complicated correlation among
task delays, which is hard to do.
  We first consider an asymptotic regime where the number of servers, $n$, goes
to infinity, and the number of tasks in a job, $k^{(n)}$, is allowed to
increase with $n$. We establish the asymptotic independence of any $k^{(n)}$
queues under the condition $k^{(n)} = o(n^{1/4})$. This greatly generalizes the
asymptotic-independence type of results in the literature where asymptotic
independence is shown only for a fixed constant number of queues. As a
consequence of our independence result, the job delay converges to the maximum
of independent task delays.
  We next consider the non-asymptotic regime. Here we prove that independence
yields a stochastic upper bound on job delay for any $n$ and any $k^{(n)}$ with
$k^{(n)}\le n$. The key component of our proof is a new technique we develop,
called ""Poisson oversampling"". Our approach converts the job delay problem into
a corresponding balls-and-bins problem. However, in contrast with typical
balls-and-bins problems where there is a negative correlation among bins, we
prove that our variant exhibits positive correlation.
"
1250,Straggler Mitigation by Delayed Relaunch of Tasks,"  Redundancy for straggler mitigation, originally in data download and more
recently in distributed computing context, has been shown to be effective both
in theory and practice. Analysis of systems with redundancy has drawn
significant attention and numerous papers have studied pain and gain of
redundancy under various service models and assumptions on the straggler
characteristics. We here present a cost (pain) vs. latency (gain) analysis of
using simple replication or erasure coding for straggler mitigation in
executing jobs with many tasks. We quantify the effect of the tail of task
execution times and discuss tail heaviness as a decisive parameter for the cost
and latency of using redundancy. Specifically, we find that coded redundancy
achieves better cost vs. latency tradeoff than simple replication and can yield
reduction in both cost and latency under less heavy tailed execution times. We
show that delaying redundancy is not effective in reducing cost and that
delayed relaunch of stragglers can yield significant reduction in cost and
latency. We validate these observations by comparing with the simulations that
use empirical distributions extracted from Google cluster data.
"
1251,Effective Straggler Mitigation: Which Clones Should Attack and When?,"  Redundancy for straggler mitigation, originally in data download and more
recently in distributed computing context, has been shown to be effective both
in theory and practice. Analysis of systems with redundancy has drawn
significant attention and numerous papers have studied pain and gain of
redundancy under various service models and assumptions on the straggler
characteristics. We here present a cost (pain) vs. latency (gain) analysis of
using simple replication or erasure coding for straggler mitigation in
executing jobs with many tasks. We quantify the effect of the tail of task
execution times and discuss tail heaviness as a decisive parameter for the cost
and latency of using redundancy. Specifically, we find that coded redundancy
achieves better cost vs. latency and allows for greater achievable latency and
cost tradeoff region compared to replication and can yield reduction in both
cost and latency under less heavy tailed execution times. We show that delaying
redundancy is not effective in reducing cost.
"
1252,"Optimal DNN Primitive Selection with Partitioned Boolean Quadratic
  Programming","  Deep Neural Networks (DNNs) require very large amounts of computation both
for training and for inference when deployed in the field. Many different
algorithms have been proposed to implement the most computationally expensive
layers of DNNs. Further, each of these algorithms has a large number of
variants, which offer different trade-offs of parallelism, data locality,
memory footprint, and execution time. In addition, specific algorithms operate
much more efficiently on specialized data layouts and formats.
  We state the problem of optimal primitive selection in the presence of data
format transformations, and show that it is NP-hard by demonstrating an
embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).
  We propose an analytic solution via a PBQP solver, and evaluate our approach
experimentally by optimizing several popular DNNs using a library of more than
70 DNN primitives, on an embedded platform and a general purpose platform. We
show experimentally that significant gains are possible versus the state of the
art vendor libraries by using a principled analytic solution to the problem of
layout selection in the presence of data format transformations.
"
1253,"Relocation in Car Sharing Systems with Shared Stackable Vehicles:
  Modelling Challenges and Outlook","  Car sharing is expected to reduce traffic congestion and pollution in cities
while at the same time improving accessibility to public transport. However,
the most popular form of car sharing, one-way car sharing, still suffers from
the vehicle unbalance problem. Innovative solutions to this issue rely on
custom vehicles with stackable capabilities: customers or operators can drive a
train of vehicles if necessary, thus efficiently bringing several cars from an
area with few requests to an area with many requests. However, how to model a
car sharing system with stackable vehicles is an open problem in the related
literature. In this paper, we propose a queueing theoretical model to fill this
gap, and we use this model to derive an upper-bound on user-based relocation
capabilities. We also validate, for the first time in the related literature,
legacy queueing theoretical models against a trace of real car sharing data.
Finally, we present preliminary results about the impact, on car availability,
of simple user-based relocation heuristics with stackable vehicles. Our results
indicate that user-based relocation schemes that exploit vehicle stackability
can significantly improve car availability at stations.
"
1254,"The Quest for Scalability and Accuracy in the Simulation of the Internet
  of Things: an Approach based on Multi-Level Simulation","  This paper presents a methodology for simulating the Internet of Things (IoT)
using multi-level simulation models. With respect to conventional simulators,
this approach allows us to tune the level of detail of different parts of the
model without compromising the scalability of the simulation. As a use case, we
have developed a two-level simulator to study the deployment of smart services
over rural territories. The higher level is base on a coarse grained,
agent-based adaptive parallel and distributed simulator. When needed, this
simulator spawns OMNeT++ model instances to evaluate in more detail the issues
concerned with wireless communications in restricted areas of the simulated
world. The performance evaluation confirms the viability of multi-level
simulations for IoT environments.
"
1255,"High-Speed Data Dissemination over Device-to-Device Millimeter-Wave
  Networks for Highway Vehicular Communication","  Gigabit-per-second connectivity among vehicles is expected to be a key
enabling technology for sensor information sharing, in turn, resulting in safer
Intelligent Transportation Systems (ITSs). Recently proposed millimeter-wave
(mmWave) systems appear to be the only solution capable of meeting the data
rate demand imposed by future ITS services. In this poster, we assess the
performance of a mmWave device-to-device (D2D) vehicular network by
investigating the impact of system and communication parameters on end-users.
"
1256,"Agile Calibration Process of Full-Stack Simulation Frameworks for V2X
  Communications","  Computer simulations and real-world car trials are essential to investigate
the performance of Vehicle-to-Everything (V2X) networks. However, simulations
are imperfect models of the physical reality and can be trusted only when they
indicate agreement with the real-world. On the other hand, trials lack
reproducibility and are subject to uncertainties and errors. In this paper, we
will illustrate a case study where the interrelationship between trials,
simulation, and the reality-of-interest is presented. Results are then compared
in a holistic fashion. Our study will describe the procedure followed to
macroscopically calibrate a full-stack network simulator to conduct
high-fidelity full-stack computer simulations.
"
1257,"BestConfig: Tapping the Performance Potential of Systems via Automatic
  Configuration Tuning","  An ever increasing number of configuration parameters are provided to system
users. But many users have used one configuration setting across different
workloads, leaving untapped the performance potential of systems. A good
configuration setting can greatly improve the performance of a deployed system
under certain workloads. But with tens or hundreds of parameters, it becomes a
highly costly task to decide which configuration setting leads to the best
performance. While such task requires the strong expertise in both the system
and the application, users commonly lack such expertise.
  To help users tap the performance potential of systems, we present
BestConfig, a system for automatically finding a best configuration setting
within a resource limit for a deployed system under a given application
workload. BestConfig is designed with an extensible architecture to automate
the configuration tuning for general systems. To tune system configurations
within a resource limit, we propose the divide-and-diverge sampling method and
the recursive bound-and-search algorithm. BestConfig can improve the throughput
of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce
the running time of Hive join job by about 50% and that of Spark join job by
about 80%, solely by configuration adjustment.
"
1258,Ciw: An open source discrete event simulation library,"  This paper introduces Ciw, an open source library for conducting discrete
event simulations that has been developed in Python. The strengths of the
library are illustrated in terms of best practice and reproducibility for
computational research. An analysis of Ciw's performance and comparison to
several alternative discrete event simulation frameworks is presented.
"
1259,"Validation of hardware events for successful performance pattern
  identification in High Performance Computing","  Hardware performance monitoring (HPM) is a crucial ingredient of performance
analysis tools. While there are interfaces like LIKWID, PAPI or the kernel
interface perf\_event which provide HPM access with some additional features,
many higher level tools combine event counts with results retrieved from other
sources like function call traces to derive (semi-)automatic performance
advice. However, although HPM is available for x86 systems since the early 90s,
only a small subset of the HPM features is used in practice. Performance
patterns provide a more comprehensive approach, enabling the identification of
various performance-limiting effects. Patterns address issues like bandwidth
saturation, load imbalance, non-local data access in ccNUMA systems, or false
sharing of cache lines. This work defines HPM event sets that are best suited
to identify a selection of performance patterns on the Intel Haswell processor.
We validate the chosen event sets for accuracy in order to arrive at a reliable
pattern detection mechanism and point out shortcomings that cannot be easily
circumvented due to bugs or limitations in the hardware.
"
1260,"NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural
  Networks","  ""How much energy is consumed for an inference made by a convolutional neural
network (CNN)?"" With the increased popularity of CNNs deployed on the
wide-spectrum of platforms (from mobile devices to workstations), the answer to
this question has drawn significant attention. From lengthening battery life of
mobile devices to reducing the energy bill of a datacenter, it is important to
understand the energy efficiency of CNNs during serving for making an
inference, before actually training the model. In this work, we propose
NeuralPower: a layer-wise predictive framework based on sparse polynomial
regression, for predicting the serving energy consumption of a CNN deployed on
any GPU platform. Given the architecture of a CNN, NeuralPower provides an
accurate prediction and breakdown for power and runtime across all layers in
the whole network, helping machine learners quickly identify the power,
runtime, or energy bottlenecks. We also propose the ""energy-precision ratio""
(EPR) metric to guide machine learners in selecting an energy-efficient CNN
architecture that better trades off the energy consumption and prediction
accuracy. The experimental results show that the prediction accuracy of the
proposed NeuralPower outperforms the best published model to date, yielding an
improvement in accuracy of up to 68.5%. We also assess the accuracy of
predictions at the network level, by predicting the runtime, power, and energy
of state-of-the-art CNN architectures, achieving an average accuracy of 88.24%
in runtime, 88.34% in power, and 97.21% in energy. We comprehensively
corroborate the effectiveness of NeuralPower as a powerful framework for
machine learners by testing it on different GPU platforms and Deep Learning
software tools.
"
1261,"LDPC Code Design for Distributed Storage: Balancing Repair Bandwidth,
  Reliability and Storage Overhead","  Distributed storage systems suffer from significant repair traffic generated
due to frequent storage node failures. This paper shows that properly designed
low-density parity-check (LDPC) codes can substantially reduce the amount of
required block downloads for repair thanks to the sparse nature of their factor
graph representation. In particular, with a careful construction of the factor
graph, both low repair-bandwidth and high reliability can be achieved for a
given code rate. First, a formula for the average repair bandwidth of LDPC
codes is developed. This formula is then used to establish that the minimum
repair bandwidth can be achieved by forcing a regular check node degree in the
factor graph. Moreover, it is shown that given a fixed code rate, the variable
node degree should also be regular to yield minimum repair bandwidth, under
some reasonable minimum variable node degree constraint. It is also shown that
for a given repair-bandwidth requirement, LDPC codes can yield substantially
higher reliability than currently utilized Reed-Solomon (RS) codes. Our
reliability analysis is based on a formulation of the general equation for the
mean-time-to-data-loss (MTTDL) associated with LDPC codes. The formulation
reveals that the stopping number is closely related to the MTTDL. It is further
shown that LDPC codes can be designed such that a small loss of
repair-bandwidth optimality may be traded for a large improvement in
erasure-correction capability and thus the MTTDL.
"
1262,ACCBench: A Framework for Comparing Causality Algorithms,"  Modern socio-technical systems are increasingly complex. A fundamental
problem is that the borders of such systems are often not well-defined
a-priori, which among other problems can lead to unwanted behavior during
runtime. Ideally, unwanted behavior should be prevented. If this is not
possible the system shall at least be able to help determine potential cause(s)
a-posterori, identify responsible parties and make them accountable for their
behavior. Recently, several algorithms addressing these concepts have been
proposed. However, the applicability of the corresponding approaches,
specifically their effectiveness and performance, is mostly unknown. Therefore,
in this paper, we propose ACCBench, a benchmark tool that allows to compare and
evaluate causality algorithms under a consistent setting. Furthermore, we
contribute an implementation of the two causality algorithms by G\""o{\ss}ler
and Metayer and G\""o{\ss}ler and Astefanoaei as well as of a policy compliance
approach based on some concepts of Main et al. Lastly, we conduct a case study
of an Intelligent Door Control System, which exposes concrete strengths and
weaknesses of all algorithms under different aspects. In the course of this, we
show that the effectiveness of the algorithms in terms of cause detection as
well as their performance differ to some extent. In addition, our analysis
reports on some qualitative aspects that should be considered when evaluating
each algorithm. For example, the human effort needed to configure the algorithm
and model the use case is analyzed.
"
1263,"Computation of gray-level co-occurrence matrix based on CUDA and its
  optimization","  As in various fields like scientific research and industrial application, the
computation time optimization is becoming a task that is of increasing
importance because of its highly parallel architecture. The graphics processing
unit is regarded as a powerful engine for application programs that demand
fairly high computation capabilities. Based on this, an algorithm was
introduced in this paper to optimize the method used to compute the gray-level
co-occurrence matrix (GLCM) of an image, and strategies (e.g., ""copying"",
""image partitioning"", etc.) were proposed to optimize the parallel algorithm.
Results indicate that without losing the computational accuracy, the speed-up
ratio of the GLCM computation of images with different resolutions by GPU by
the use of CUDA was 50 times faster than that of the GLCM computation by CPU,
which manifested significantly improved performance.
"
1264,Efficient Neighbor-Finding on Space-Filling Curves,"  Space-filling curves (SFC, also known as FASS-curves) are a useful tool in
scientific computing and other areas of computer science to sequentialize
multidimensional grids in a cache-efficient and parallelization-friendly way
for storage in an array. Many algorithms, for example grid-based numerical PDE
solvers, have to access all neighbor cells of each grid cell during a grid
traversal. While the array indices of neighbors can be stored in a cell, they
still have to be computed for initialization or when the grid is adaptively
refined. A fast neighbor-finding algorithm can thus significantly improve the
runtime of computations on multidimensional grids.
  In this thesis, we show how neighbors on many regular grids ordered by
space-filling curves can be found in an average-case time complexity of $O(1)$.
In general, this assumes that the local orientation (i.e. a variable of a
describing grammar) of the SFC inside the grid cell is known in advance, which
can be efficiently realized during traversals. Supported SFCs include Hilbert,
Peano and Sierpinski curves in arbitrary dimensions. We assume that integer
arithmetic operations can be performed in $O(1)$, i.e. independent of the size
of the integer. We do not deal with the case of adaptively refined grids here.
However, it appears that a generalization of the algorithm to suitable adaptive
grids is possible. To formulate the neighbor-finding algorithm and prove its
correctness and runtime properties, a modeling framework is introduced. This
framework extends the idea of vertex-labeling to a description using grammars
and matrices. With the sfcpp library, we provide a C++ implementation to render
SFCs generated by such models and automatically compute all lookup tables
needed for the neighbor-finding algorithm. Furthermore, optimized
neighbor-finding implementations for various SFCs are included for which we
provide runtime measurements.
"
1265,Network Load Balancing Methods: Experimental Comparisons and Improvement,"  Load balancing algorithms play critical roles in systems where the workload
has to be distributed across multiple resources, such as cores in
multiprocessor system, computers in distributed computing, and network links.
In this paper, we study and evaluate four load balancing methods: random, round
robin, shortest queue, and shortest queue with stale load information. We build
a simulation model and compare mean delay of the systems for the load balancing
methods. We also provide a method to improve shortest queue with stale load
information load balancing. A performance analysis for the improvement is also
presented in this paper.
"
1266,"SERENADE: A Parallel Randomized Algorithm Suite for Crossbar Scheduling
  in Input-Queued Switches","  Most of today's high-speed switches and routers adopt an input-queued
crossbar switch architecture. Such a switch needs to compute a matching
(crossbar schedule) between the input ports and output ports during each
switching cycle (time slot). A key research challenge in designing large (in
number of input/output ports $N$) input-queued crossbar switches is to develop
crossbar scheduling algorithms that can compute ""high quality"" matchings --
i.e., those that result in high switch throughput (ideally $100\%$) and low
queueing delays for packets -- at line rates. SERENA is one such algorithm: it
outputs excellent matching decisions that result in $100\%$ switch throughput
and reasonably good queueing delays. However, since SERENA is a centralized
algorithm with $O(N)$ computational complexity, it cannot support switches that
both are large and have a very high line rate per port. In this work, we
propose SERENADE (SERENA, the Distributed Edition), a parallel iterative
algorithm that emulates SERENA in only $O(\log N)$ iterations between input
ports and output ports, and hence has a time complexity of only $O(\log N)$ per
port. We prove that SERENADE can exactly emulate SERENA. We also propose an
early-stop version of SERENADE, called O-SERENADE, to only approximately
emulate SERENA. Through extensive simulations, we show that O-SERENADE can
achieve 100\% throughput and that it has similar as or slightly better delay
performance than SERENA under various load conditions and traffic patterns.
"
1267,"A JSON Token-Based Authentication and Access Management Schema for Cloud
  SaaS Applications","  Cloud computing is significantly reshaping the computing industry built
around core concepts such as virtualization, processing power, connectivity and
elasticity to store and share IT resources via a broad network. It has emerged
as the key technology that unleashes the potency of Big Data, Internet of
Things, Mobile and Web Applications, and other related technologies, but it
also comes with its challenges - such as governance, security, and privacy.
This paper is focused on the security and privacy challenges of cloud computing
with specific reference to user authentication and access management for cloud
SaaS applications. The suggested model uses a framework that harnesses the
stateless and secure nature of JWT for client authentication and session
management. Furthermore, authorized access to protected cloud SaaS resources
have been efficiently managed. Accordingly, a Policy Match Gate (PMG) component
and a Policy Activity Monitor (PAM) component have been introduced. In
addition, other subcomponents such as a Policy Validation Unit (PVU) and a
Policy Proxy DB (PPDB) have also been established for optimized service
delivery. A theoretical analysis of the proposed model portrays a system that
is secure, lightweight and highly scalable for improved cloud resource security
and management.
"
1268,BENCHIP: Benchmarking Intelligence Processors,"  The increasing attention on deep learning has tremendously spurred the design
of intelligence processing hardware. The variety of emerging intelligence
processors requires standard benchmarks for fair comparison and system
optimization (in both software and hardware). However, existing benchmarks are
unsuitable for benchmarking intelligence processors due to their non-diversity
and nonrepresentativeness. Also, the lack of a standard benchmarking
methodology further exacerbates this problem. In this paper, we propose
BENCHIP, a benchmark suite and benchmarking methodology for intelligence
processors. The benchmark suite in BENCHIP consists of two sets of benchmarks:
microbenchmarks and macrobenchmarks. The microbenchmarks consist of
single-layer networks. They are mainly designed for bottleneck analysis and
system optimization. The macrobenchmarks contain state-of-the-art industrial
networks, so as to offer a realistic comparison of different platforms. We also
propose a standard benchmarking methodology built upon an industrial software
stack and evaluation metrics that comprehensively reflect the various
characteristics of the evaluated intelligence processors. BENCHIP is utilized
for evaluating various hardware platforms, including CPUs, GPUs, and
accelerators. BENCHIP will be open-sourced soon.
"
1269,High-Performance Code Generation though Fusion and Vectorization,"  We present a technique for automatically transforming kernel-based
computations in disparate, nested loops into a fused, vectorized form that can
reduce intermediate storage needs and lead to improved performance on
contemporary hardware.
  We introduce representations for the abstract relationships and data
dependencies of kernels in loop nests and algorithms for manipulating them into
more efficient form; we similarly introduce techniques for determining data
access patterns for stencil-like array accesses and show how this can be used
to elide storage and improve vectorization.
  We discuss our prototype implementation of these ideas---named HFAV---and its
use of a declarative, inference-based front-end to drive transformations, and
we present results for some prominent codes in HPC.
"
1270,A Sequential Approximation Framework for Coded Distributed Optimization,"  Building on the previous work of Lee et al. and Ferdinand et al. on coded
computation, we propose a sequential approximation framework for solving
optimization problems in a distributed manner. In a distributed computation
system, latency caused by individual processors (""stragglers"") usually causes a
significant delay in the overall process. The proposed method is powered by a
sequential computation scheme, which is designed specifically for systems with
stragglers. This scheme has the desirable property that the user is guaranteed
to receive useful (approximate) computation results whenever a processor
finishes its subtask, even in the presence of uncertain latency. In this paper,
we give a coding theorem for sequentially computing matrix-vector
multiplications, and the optimality of this coding scheme is also established.
As an application of the results, we demonstrate solving optimization problems
using a sequential approximation approach, which accelerates the algorithm in a
distributed system with stragglers.
"
1271,"Analysis of the Leakage Queue: A Queueing Model for Energy Storage
  Systems with Self-discharge","  Energy storage is a crucial component of the smart grid, since it provides
the ability to buffer transient fluctuations of the energy supply from
renewable sources. Even without a load, energy storage systems experience a
reduction of the stored energy through self-discharge. In some storage
technologies, the rate of self-discharge can exceed 50% of the stored energy
per day. In this paper, we investigate the self-discharge phenomenon in energy
storage using a queueing system model, which we refer to as leakage queue. When
the average net charge is positive, we discover that the leakage queue operates
in one of two regimes: a leakage-dominated regime and a capacity-dominated
regime. We find that in the leakage-dominated regime, the stored energy
stabilizes at a point that is below the storage capacity. Under suitable
independence assumptions for energy supply and demand, the stored energy in
this regime closely follows a normal distribution. We present two methods for
computing probabilities of underflow and overflow at a leakage queue. The
methods are validated in a numerical example where the energy supply resembles
a wind energy source.
"
1272,Interference Queueing Networks on Grids,"  Consider a countably infinite collection of interacting queues, with a queue
located at each point of the $d$-dimensional integer grid, having independent
Poisson arrivals, but dependent service rates. The service discipline is of the
processor sharing type,with the service rate in each queue slowed down, when
the neighboring queues have a larger workload. The interactions are translation
invariant in space and is neither of the Jackson Networks type, nor of the
mean-field type. Coupling and percolation techniques are first used to show
that this dynamics has well defined trajectories. Coupling from the past
techniques are then proposed to build its minimal stationary regime. The rate
conservation principle of Palm calculus is then used to identify the stability
condition of this system, where the notion of stability is appropriately
defined for an infinite dimensional process. We show that the identified
condition is also necessary in certain special cases and conjecture it to be
true in all cases. Remarkably, the rate conservation principle also provides a
closed form expression for the mean queue size. When the stability condition
holds, this minimal solution is the unique translation invariant stationary
regime. In addition, there exists a range of small initial conditions for which
the dynamics is attracted to the minimal regime. Nevertheless, there exists
another range of larger though finite initial conditions for which the dynamics
diverges, even though stability criterion holds.
"
1273,"Performance optimizations for scalable CFD applications on hybrid
  CPU+MIC heterogeneous computing system with millions of cores","  For computational fluid dynamics (CFD) applications with a large number of
grid points/cells, parallel computing is a common efficient strategy to reduce
the computational time. How to achieve the best performance in the modern
supercomputer system, especially with heterogeneous computing resources such as
hybrid CPU+GPU, or a CPU + Intel Xeon Phi (MIC) co-processors, is still a great
challenge.
  An in-house parallel CFD code capable of simulating three dimensional
structured grid applications is developed and tested in this study. Several
methods of parallelization, performance optimization and code tuning both in
the CPU-only homogeneous system and in the heterogeneous system are proposed
based on identifying potential parallelism of applications, balancing the work
load among all kinds of computing devices, tuning the multi-thread code toward
better performance in intra-machine node with hundreds of CPU/MIC cores, and
optimizing the communication among inter-nodes, inter-cores, and between CPUs
and MICs.
  Some benchmark cases from model and/or industrial CFD applications are tested
on the Tianhe-1A and Tianhe-2 supercomputer to evaluate the performance. Among
these CFD cases, the maximum number of grid cells reached 780 billion. The
tuned solver successfully scales up to half of the entire Tianhe-2
supercomputer system with over 1.376 million of heterogeneous cores. The test
results and performance analysis are discussed in detail.
"
1274,Power Modelling for Heterogeneous Cloud-Edge Data Centers,"  Existing power modelling research focuses not on the method used for
developing models but rather on the model itself. This paper aims to develop a
method for deploying power models on emerging processors that will be used, for
example, in cloud-edge data centers. Our research first develops a hardware
counter selection method that appropriately selects counters most correlated to
power on ARM and Intel processors. Then, we propose a two stage power model
that works across multiple architectures. The key results are: (i) the
automated hardware performance counter selection method achieves comparable
selection to the manual selection methods reported in literature, and (ii) the
two stage power model can predict dynamic power more accurately on both ARM and
Intel processors when compared to classic power models.
"
1275,Distributed Server Allocation for Content Delivery Networks,"  We propose a dynamic formulation of file-sharing networks in terms of an
average cost Markov decision process with constraints. By analyzing a
Whittle-like relaxation thereof, we propose an index policy in the spirit of
Whittle and compare it by simulations with other natural heuristics.
"
1276,"Performance Optimization and Parallelization of a Parabolic Equation
  Solver in Computational Ocean Acoustics on Modern Many-core Computer","  As one of open-source codes widely used in computational ocean acoustics,
FOR3D can provide a very good estimate for underwater acoustic propagation. In
this paper, we propose a performance optimization and parallelization to speed
up the running of FOR3D. We utilized a variety of methods to enhance the entire
performance, such as using a multi-threaded programming model to exploit the
potential capability of the many-core node of high-performance computing (HPC)
system, tuning compile options, using efficient tuned mathematical library and
utilizing vectorization optimization instruction. In addition, we extended the
application from single-frequency calculation to multi-frequency calculation
successfully by using OpenMP+MPI hybrid programming techniques on the
mainstream HPC platform. A detailed performance evaluation was performed and
the results showed that the proposed parallelization obtained good accelerated
effect of 25.77X when testing a typical three-dimensional medium-sized case on
Tianhe-2 supercomputer. It also showed that the tuned parallel version has a
weak-scalability. The speed of calculation of underwater sound field can be
greatly improved by the strategy mentioned in this paper. The method used in
this paper is not only applicable to other similar computing models in
computational ocean acoustics but also a guideline of performance enhancement
for scientific and engineering application running on modern
many-core-computing platform.
"
1277,ThrottleBot - Performance without Insight,"  Large scale applications are increasingly built by composing sets of
microservices. In this model the functionality for a single application might
be split across 100s or 1000s of microservices. Resource provisioning for these
applications is complex, requiring administrators to understand both the
functioning of each microservice, and dependencies between microservices in an
application. In this paper we present ThrottleBot, a system that automates the
process of determining what resource when allocated to which microservice is
likely to have the greatest impact on application performance. We demonstrate
the efficacy of our approach by applying ThrottleBot to both synthetic and real
world applications. We believe that ThrottleBot when combined with existing
microservice orchestrators, e.g., Kubernetes, enables push-button deployment of
web scale applications.
"
1278,"Acceleration of tensor-product operations for high-order finite element
  methods","  This paper is devoted to GPU kernel optimization and performance analysis of
three tensor-product operators arising in finite element methods. We provide a
mathematical background to these operations and implementation details.
Achieving close-to-the-peak performance for these operators requires extensive
optimization because of the operators' properties: low arithmetic intensity,
tiered structure, and the need to store intermediate results inside the kernel.
We give a guided overview of optimization strategies and we present a
performance model that allows us to compare the efficacy of these optimizations
against an empirically calibrated roofline.
"
1279,Timing Aware Dummy Metal Fill Methodology,"  In this paper, we analyzed parasitic coupling capacitance coming from dummy
metal fill and its impact on timing. Based on the modeling, we proposed two
approaches to minimize the timing impact from dummy metal fill. The first
approach applies more spacing between critical nets and metal fill, while the
second approach leverages the shielding effects of reference nets. Experimental
results show consistent improvement compared to traditional metal fill method.
"
1280,"Comparison of Parallelisation Approaches, Languages, and Compilers for
  Unstructured Mesh Algorithms on GPUs","  Efficiently exploiting GPUs is increasingly essential in scientific
computing, as many current and upcoming supercomputers are built using them. To
facilitate this, there are a number of programming approaches, such as CUDA,
OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++
and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL)
each supporting different combinations of languages. In this study, we take a
detailed look at some of the currently available options, and carry out a
comprehensive analysis and comparison using computational loops and
applications from the domain of unstructured mesh computations. Beyond runtimes
and performance metrics (GB/s), we explore factors that influence performance
such as register counts, occupancy, usage of different memory types,
instruction counts, and algorithmic differences. Results of this work show how
clang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues
with directive-based approaches on complex kernels, and OpenMP 4 support
maturing in clang and XL; currently around 10% slower than CUDA.
"
1281,"Integrating Queuing Regime into Cognitive Radio Channel Aggregation
  Policies: A Performance Evaluation","  Channel aggregation (CA) is one of the newest concept which cognitive radio
network is bringing to bear for the smooth role out of fifth/next generation
wireless networks. This is the combining of several unused primary user
spectrum holes into a logic usable channel. However, several of these
strategies have been investigated considering the varying nature of wireless
link and adaptive modulation and coding (AMC). Examples are the instant
blocking strategy (IBS) and readjustment based strategy (RBS). This paper
develops and compares two CA policies with queue, which are the IBS with queue
(IBS + Q), and the RBS with queue (RBS+Q). This is in furtherance of previous
proposed work. The aim is to identifying the impact of a queuing regime on the
performance of the secondary network such that any secondary user (SU) that has
not completed its service, as an alternative to dropping or forcibly
terminating the service, it is queued in order to get another opportunity to
access the primary user (PU) channels. The performance is evaluated through a
simulation framework. The results validate that with a welldesigned queuing
regime, capacity, access and other metrics can be improved with significant
reduction in blocking and forced termination probabilities respectively.
"
1282,Performance Evaluation of Deep Learning Tools in Docker Containers,"  With the success of deep learning techniques in a broad range of application
domains, many deep learning software frameworks have been developed and are
being updated frequently to adapt to new hardware features and software
libraries, which bring a big challenge for end users and system administrators.
To address this problem, container techniques are widely used to simplify the
deployment and management of deep learning software. However, it remains
unknown whether container techniques bring any performance penalty to deep
learning applications. The purpose of this work is to systematically evaluate
the impact of docker container on the performance of deep learning
applications. We first benchmark the performance of system components (IO, CPU
and GPU) in a docker container and the host system and compare the results to
see if there's any difference. According to our results, we find that
computational intensive jobs, either running on CPU or GPU, have small overhead
indicating docker containers can be applied to deep learning programs. Then we
evaluate the performance of some popular deep learning tools deployed in a
docker container and the host system. It turns out that the docker container
will not cause noticeable drawbacks while running those deep learning tools. So
encapsulating deep learning tool in a container is a feasible solution.
"
1283,Practical Bounds on Optimal Caching with Variable Object Sizes,"  Many recent caching systems aim to improve miss ratios, but there is no good
sense among practitioners of how much further miss ratios can be improved. In
other words, should the systems community continue working on this problem?
Currently, there is no principled answer to this question. In practice, object
sizes often vary by several orders of magnitude, where computing the optimal
miss ratio (OPT) is known to be NP-hard. The few known results on caching with
variable object sizes provide very weak bounds and are impractical to compute
on traces of realistic length.
  We propose a new method to compute upper and lower bounds on OPT. Our key
insight is to represent caching as a min-cost flow problem, hence we call our
method the flow-based offline optimal (FOO). We prove that, under simple
independence assumptions, FOO's bounds become tight as the number of objects
goes to infinity. Indeed, FOO's error over 10M requests of production CDN and
storage traces is negligible: at most 0.3%. FOO thus reveals, for the first
time, the limits of caching with variable object sizes. While FOO is very
accurate, it is computationally impractical on traces with hundreds of millions
of requests. We therefore extend FOO to obtain more efficient bounds on OPT,
which we call practical flow-based offline optimal (PFOO). We evaluate PFOO on
several full production traces and use it to compare OPT to prior online
policies. This analysis shows that current caching systems are in fact still
far from optimal, suffering 11-43% more cache misses than OPT, whereas the best
prior offline bounds suggest that there is essentially no room for improvement.
"
1284,"Scheduling with regular performance measures and optional job rejection
  on a single machine","  We address single machine problems with optional jobs - rejection, studied
recently in Zhang et al. [21] and Cao et al. [2]. In these papers, the authors
focus on minimizing regular performance measures, i.e., functions that are
non-decreasing in the jobs completion time, subject to the constraint that the
total rejection cost cannot exceed a predefined upper bound. They also prove
that the considered problems are ordinary NP-hard and provide
pseudo-polynomial-time Dynamic Programming (DP) solutions. In this paper, we
focus on three of these problems: makespan with release-dates; total completion
times; and total weighted completion, and present enhanced DP solutions
demonstrating both theoretical and practical improvements. Moreover, we provide
extensive numerical studies verifying their efficiency.
"
1285,A TTL-based Approach for Content Placement in Edge Networks,"  Edge networks are promising to provide better services to users by
provisioning computing and storage resources at the edge of networks. However,
due to the uncertainty and diversity of user interests, content popularity,
distributed network structure, cache sizes, it is challenging to decide where
to place the content, and how long it should be cached. In this paper, we study
the utility optimization of content placement at edge networks through
timer-based (TTL) policies. We propose provably optimal distributed algorithms
that operate at each network cache to maximize the overall network utility. Our
TTL-based optimization model provides theoretical answers to how long each
content must be cached, and where it should be placed in the edge network.
Extensive evaluations show that our algorithm significantly outperforms path
replication with conventional caching algorithms over some network topologies.
"
1286,Differential Performance Debugging with Discriminant Regression Trees,"  Differential performance debugging is a technique to find performance
problems. It applies in situations where the performance of a program is
(unexpectedly) different for different classes of inputs. The task is to
explain the differences in asymptotic performance among various input classes
in terms of program internals. We propose a data-driven technique based on
discriminant regression tree (DRT) learning problem where the goal is to
discriminate among different classes of inputs. We propose a new algorithm for
DRT learning that first clusters the data into functional clusters, capturing
different asymptotic performance classes, and then invokes off-the-shelf
decision tree learning algorithms to explain these clusters. We focus on linear
functional clusters and adapt classical clustering algorithms (K-means and
spectral) to produce them. For the K-means algorithm, we generalize the notion
of the cluster centroid from a point to a linear function. We adapt spectral
clustering by defining a novel kernel function to capture the notion of linear
similarity between two data points. We evaluate our approach on benchmarks
consisting of Java programs where we are interested in debugging performance.
We show that our algorithm significantly outperforms other well-known
regression tree learning algorithms in terms of running time and accuracy of
classification.
"
1287,Depth First Always On Routing Trace Algorithm,"  In this paper, we discussed current limitation in the
electronic-design-automotation (EDA) tool on tracing the always on routing. We
developed an algorithm to efficiently track the secondary power routing and
accurately estimate the routing quality using approximate voltage drop as the
criteria. The fast check can identify potential hotspot issues without going
through sign-off checks. It helps designers to capture issues at early stages
and fix the issues with less design effort. We also discussed some limitations
to our algorithm.
"
1288,"Performance Analysis and Optimization of Sparse Matrix-Vector
  Multiplication on Modern Multi- and Many-Core Processors","  This paper presents a low-overhead optimizer for the ubiquitous sparse
matrix-vector multiplication (SpMV) kernel. Architectural diversity among
different processors together with structural diversity among different sparse
matrices lead to bottleneck diversity. This justifies an SpMV optimizer that is
both matrix- and architecture-adaptive through runtime specialization. To this
direction, we present an approach that first identifies the performance
bottlenecks of SpMV for a given sparse matrix on the target platform either
through profiling or by matrix property inspection, and then selects suitable
optimizations to tackle those bottlenecks. Our optimization pool is based on
the widely used Compressed Sparse Row (CSR) sparse matrix storage format and
has low preprocessing overheads, making our overall approach practical even in
cases where fast decision making and optimization setup is required. We
evaluate our optimizer on three x86-based computing platforms and demonstrate
that it is able to distinguish and appropriately optimize SpMV for the majority
of matrices in a representative test suite, leading to significant speedups
over the CSR and Inspector-Executor CSR SpMV kernels available in the latest
release of the Intel MKL library.
"
1289,A Stochastic Resource-Sharing Network for Electric Vehicle Charging,"  We consider a distribution grid used to charge electric vehicles such that
voltage drops stay bounded. We model this as a class of resource-sharing
networks, known as bandwidth-sharing networks in the communication network
literature. We focus on resource-sharing networks that are driven by a class of
greedy control rules that can be implemented in a decentralized fashion. For a
large number of such control rules, we can characterize the performance of the
system by a fluid approximation. This leads to a set of dynamic equations that
take into account the stochastic behavior of EVs. We show that the invariant
point of these equations is unique and can be computed by solving a specific
ACOPF problem, which admits an exact convex relaxation. We illustrate our
findings with a case study using the SCE 47-bus network and several special
cases that allow for explicit computations.
"
1290,"Deep Reinforcement Learning for Multi-Resource Multi-Machine Job
  Scheduling","  Minimizing job scheduling time is a fundamental issue in data center networks
that has been extensively studied in recent years. The incoming jobs require
different CPU and memory units, and span different number of time slots. The
traditional solution is to design efficient heuristic algorithms with
performance guarantee under certain assumptions. In this paper, we improve a
recently proposed job scheduling algorithm using deep reinforcement learning
and extend it to multiple server clusters. Our study reveals that deep
reinforcement learning method has the potential to outperform traditional
resource allocation algorithms in a variety of complicated environments.
"
1291,"Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users
  in Real-Time","  User experience in modern content discovery applications critically depends
on high-quality personalized recommendations. However, building systems that
provide such recommendations presents a major challenge due to a massive pool
of items, a large number of users, and requirements for recommendations to be
responsive to user actions and generated on demand in real-time. Here we
present Pixie, a scalable graph-based real-time recommender system that we
developed and deployed at Pinterest. Given a set of user-specific pins as a
query, Pixie selects in real-time from billions of possible pins those that are
most related to the query. To generate recommendations, we develop Pixie Random
Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and
17 billion edges. Experiments show that recommendations provided by Pixie lead
up to 50% higher user engagement when compared to the previous Hadoop-based
production system. Furthermore, we develop a graph pruning strategy at that
leads to an additional 58% improvement in recommendations. Last, we discuss
system aspects of Pixie, where a single server executes 1,200 recommendation
requests per second with 60 millisecond latency. Today, systems backed by Pixie
contribute to more than 80% of all user engagement on Pinterest.
"
1292,A note on using performance and data profilesfor training algorithms,"  It is shown how to use the performance and data profile benchmarking tools to
improve algorithms' performance. An illustration for the BFO derivative-free
optimizer suggests that the obtained gains are potentially significant.
"
1293,LP-Based Power Grid Enhancement Methodology,"  In this paper, we explored the opportunity to enhance power grid robustness
after routing stage, and propose a linear programming based algorithm that
maximizes the improvement of power grid strengthening with given available
routing resource. We further discussed some techniques to leverage tradeoffs
between runtime and optimality of the solutions. Experimental results show
substantial power integrity improvement with ""zero cost"".
"
1294,"Lattice Boltzmann Benchmark Kernels as a Testbed for Performance
  Analysis","  Lattice Boltzmann methods (LBM) are an important part of current
computational fluid dynamics (CFD). They allow easy implementations and
boundary handling. However, competitive time to solution not only depends on
the choice of a reasonable method, but also on an efficient implementation on
modern hardware. Hence, performance optimization has a long history in the
lattice Boltzmann community. A variety of options exists regarding the
implementation with direct impact on the solver performance. Experimenting and
evaluating each option often is hard as the kernel itself is typically embedded
in a larger code base. With our suite of lattice Boltzmann kernels we provide
the infrastructure for such endeavors. Already included are several kernels
ranging from simple to fully optimized implementations. Although these kernels
are not fully functional CFD solvers, they are equipped with a solid
verification method. The kernels may act as an reference for performance
comparisons and as a blue print for optimization strategies. In this paper we
give an overview of already available kernels, establish a performance model
for each kernel, and show a comparison of implementations and recent
architectures.
"
1295,"Understanding Quality of Experiences on Different Mobile Browsers:
  Measurements, Analysis, and Implications","  The web browser is one of the major channels to access the Internet on mobile
devices. Based on the smartphone usage logs from millions of real-world Android
users, it is interesting to find that about 38% users have more than one
browser on their devices. However, it is unclear whether the quality of
browsing experiences are different when visiting the same webpage on different
browsers. In this paper, we collect 3-week consecutive traces of 337 popular
webpages on three popular mobile browsers: Chrome, Firefox, and Opera. We first
use a list of metrics and conduct an empirical study to measure the differences
of these metrics on different browsers. Then, we explore the variety of loading
time and cache performance of different browsers when visiting the same
webpage, which has a great impact on the browsing experience. Furthermore, we
try to find which metrics have significant effect on the differences,
investigating the possible causes. Finally, according to our findings, we give
some recommendations to web developers, browser vendors, and end users.
"
1296,SOAP: One Clean Analysis of All Age-Based Scheduling Policies,"  We consider an extremely broad class of M/G/1 scheduling policies called
SOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost
all scheduling policies in the literature as well as an infinite number of
variants which have never been analyzed, or maybe not even conceived. SOAP
policies range from classic policies, like first-come, first-serve (FCFS),
foreground-background (FB), class-based priority, and shortest remaining
processing time (SRPT); to much more complicated scheduling rules, such as the
famously complex Gittins index policy and other policies in which a job's
priority changes arbitrarily with its age. While the response time of policies
in the former category is well understood, policies in the latter category have
resisted response time analysis. We present a universal analysis of all SOAP
policies, deriving the mean and Laplace-Stieltjes transform of response time.
"
1297,Performance Heuristics for GR(1) Synthesis and Related Algorithms,"  Reactive synthesis for the GR(1) fragment of LTL has been implemented and
studied in many works. In this workshop paper we present and evaluate a list of
heuristics to potentially reduce running times for GR(1) synthesis and related
algorithms. The list includes early detection of fixed-points and
unrealizability, fixed-point recycling, and heuristics for unrealizable core
computations. We evaluate the presented heuristics on SYNTECH15, a total of 78
specifications of 6 autonomous Lego robots, written by 3rd year undergraduate
computer science students in a project class we have taught, as well as on
several benchmarks from the literature. The evaluation investigates not only
the potential of the suggested heuristics to improve computation times, but
also the difference between existing benchmarks and the robot's specifications
in terms of the effectiveness of the heuristics.
"
1298,Speeding Up BigClam Implementation on SNAP,"  We perform a detailed analysis of the C++ implementation of the Cluster
Affiliation Model for Big Networks (BigClam) on the Stanford Network Analysis
Project (SNAP). BigClam is a popular graph mining algorithm that is capable of
finding overlapping communities in networks containing millions of nodes. Our
analysis shows a key stage of the algorithm - determining if a node belongs to
a community - dominates the runtime of the implementation, yet the computation
is not parallelized. We show that by parallelizing computations across multiple
threads using OpenMP we can speed up the algorithm by 5.3 times when solving
large networks for communities, while preserving the integrity of the program
and the result.
"
1299,An LLVM Instrumentation Plug-in for Score-P,"  Reducing application runtime, scaling parallel applications to higher numbers
of processes/threads, and porting applications to new hardware architectures
are tasks necessary in the software development process. Therefore, developers
have to investigate and understand application runtime behavior. Tools such as
monitoring infrastructures that capture performance relevant data during
application execution assist in this task. The measured data forms the basis
for identifying bottlenecks and optimizing the code. Monitoring infrastructures
need mechanisms to record application activities in order to conduct
measurements. Automatic instrumentation of the source code is the preferred
method in most application scenarios. We introduce a plug-in for the LLVM
infrastructure that enables automatic source code instrumentation at
compile-time. In contrast to available instrumentation mechanisms in
LLVM/Clang, our plug-in can selectively include/exclude individual application
functions. This enables developers to fine-tune the measurement to the required
level of detail while avoiding large runtime overheads due to excessive
instrumentation.
"
1300,Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search,"  Many multimedia information retrieval or machine learning problems require
efficient high-dimensional nearest neighbor search techniques. For instance,
multimedia objects (images, music or videos) can be represented by
high-dimensional feature vectors. Finding two similar multimedia objects then
comes down to finding two objects that have similar feature vectors. In the
current context of mass use of social networks, large scale multimedia
databases or large scale machine learning applications are more and more
common, calling for efficient nearest neighbor search approaches.
  This thesis builds on product quantization, an efficient nearest neighbor
search technique that compresses high-dimensional vectors into short codes.
This makes it possible to store very large databases entirely in RAM, enabling
low response times. We propose several contributions that exploit the
capabilities of modern CPUs, especially SIMD and the cache hierarchy, to
further decrease response times offered by product quantization.
"
1301,Task Scheduling for Heterogeneous Multicore Systems,"  In recent years, as the demand for low energy and high performance computing
has steadily increased, heterogeneous computing has emerged as an important and
promising solution. Because most workloads can typically run most efficiently
on certain types of cores, mapping tasks on the best available resources can
not only save energy but also deliver high performance. However, optimal task
scheduling for performance and/or energy is yet to be solved for heterogeneous
platforms. The work presented herein mathematically formulates the optimal
heterogeneous system task scheduling as an optimization problem using queueing
theory. We analytically solve for the common case of two processor types, e.g.,
CPU+GPU, and give an optimal policy (CAB). We design the GrIn heuristic to
efficiently solve for near-optimal policy for any number of processor types
(within 1.6% of the optimal). Both policies work for any task size distribution
and processing order, and are therefore, general and practical. We extensively
simulate and validate the theory, and implement the proposed policy in a
CPU-GPU real platform to show the optimal throughput and energy improvement.
Comparing to classic policies like load-balancing, our results range from
1.08x~2.24x better performance or 1.08x~2.26x better energy efficiency in
simulations, and 2.37x~9.07x better performance in experiments.
"
1302,"Priority-Aware Near-Optimal Scheduling for Heterogeneous Multi-Core
  Systems with Specialized Accelerators","  To deliver high performance in power limited systems, architects have turned
to using heterogeneous systems, either CPU+GPU or mixed CPU-hardware systems.
However, in systems with different processor types and task affinities,
scheduling tasks becomes more challenging than in homogeneous multi-core
systems or systems without task affinities. The problem is even more complex
when specialized accelerators and task priorities are included. In this paper,
we provide a formal proof for the optimal scheduling policy for heterogeneous
systems with arbitrary number of resource types, including specialized
accelerators, independent of the task arrival rate, task size distribution, and
resource processing order. We transform the optimal scheduling policy to a
nonlinear integer optimization problem and propose a fast, near-optimal
algorithm. An additional heuristic is proposed for the case of priority-aware
scheduling. Our experimental results demonstrate that the proposed algorithm is
only 0.3% from the optimal and superior to conventional scheduling policies.
"
1303,Datacenter Traffic Control: Understanding Techniques and Trade-offs,"  Datacenters provide cost-effective and flexible access to scalable compute
and storage resources necessary for today's cloud computing needs. A typical
datacenter is made up of thousands of servers connected with a large network
and usually managed by one operator. To provide quality access to the variety
of applications and services hosted on datacenters and maximize performance, it
deems necessary to use datacenter networks effectively and efficiently.
Datacenter traffic is often a mix of several classes with different priorities
and requirements. This includes user-generated interactive traffic, traffic
with deadlines, and long-running traffic. To this end, custom transport
protocols and traffic management techniques have been developed to improve
datacenter network performance.
  In this tutorial paper, we review the general architecture of datacenter
networks, various topologies proposed for them, their traffic properties,
general traffic control challenges in datacenters and general traffic control
objectives. The purpose of this paper is to bring out the important
characteristics of traffic control in datacenters and not to survey all
existing solutions (as it is virtually impossible due to massive body of
existing research). We hope to provide readers with a wide range of options and
factors while considering a variety of traffic control mechanisms. We discuss
various characteristics of datacenter traffic control including management
schemes, transmission control, traffic shaping, prioritization, load balancing,
multipathing, and traffic scheduling. Next, we point to several open challenges
as well as new and interesting networking paradigms. At the end of this paper,
we briefly review inter-datacenter networks that connect geographically
dispersed datacenters which have been receiving increasing attention recently
and pose interesting and novel research problems.
"
1304,"Optimal Energy Consumption with Communication, Computation, Caching and
  QoI-Guarantee","  Energy efficiency is a fundamental requirement of modern data communication
systems, and its importance is reflected in much recent work on performance
analysis of system energy consumption. However, most works have only focused on
communication and computation costs, but do not account for caching costs.
Given the increasing interest in cache networks, this is a serious deficiency.
{In this paper, we consider the problem of energy consumption in data
communication, compression and caching (C$3$) with a Quality of Information
(QoI) guarantee in a communication network. {Our goal is to identify the
optimal data compression rate and data placement over the network to minimize
the overall energy consumption in the network.} he formulated problem is a
\emph{Mixed Integer Non-Linear Programming} (MINLP) problem with non-convex
functions, which is NP-hard in general. } {We} propose a variant of spatial
branch and bound algorithm (V-SBB), that can {provide} the $\epsilon$-global
optimal solution to {our problem}. {We numerically show that our C3
optimization framework can improve the energy efficiency up to 88\% compared to
any C2 optimization between communication and computation or caching.
Furthermore, for our energy consumption problem, V-SBB {provides comparatively
better solution than some other MINLP solvers.}}
"
1305,"Collaborative Uploading in Heterogeneous Networks: Optimal and Adaptive
  Strategies","  Collaborative uploading describes a type of crowdsourcing scenario in
networked environments where a device utilizes multiple paths over neighboring
devices to upload content to a centralized processing entity such as a cloud
service. Intermediate devices may aggregate and preprocess this data stream.
Such scenarios arise in the composition and aggregation of information, e.g.,
from smartphones or sensors. We use a queuing theoretic description of the
collaborative uploading scenario, capturing the ability to split data into
chunks that are then transmitted over multiple paths, and finally merged at the
destination. We analyze replication and allocation strategies that control the
mapping of data to paths and provide closed-form expressions that pinpoint the
optimal strategy given a description of the paths' service distributions.
Finally, we provide an online path-aware adaptation of the allocation strategy
that uses statistical inference to sequentially minimize the expected waiting
time for the uploaded data. Numerical results show the effectiveness of the
adaptive approach compared to the proportional allocation and a variant of the
join-the-shortest-queue allocation, especially for bursty path conditions.
"
1306,"Better Algorithms for Hybrid Circuit and Packet Switching in Data
  Centers","  Hybrid circuit and packet switching for data center networking (DCN) has
received considerable research attention recently. A hybrid-switched DCN
employs a much faster circuit switch that is reconfigurable with a nontrivial
cost, and a much slower packet switch that is reconfigurable with no cost, to
interconnect its racks of servers. The research problem is, given a traffic
demand matrix (between the racks), how to compute a good circuit switch
configuration schedule so that the vast majority of the traffic demand is
removed by the circuit switch, leaving a remaining demand matrix that contains
only small elements for the packet switch to handle. In this paper, we propose
two new hybrid switch scheduling algorithms under two different scheduling
constraints. Our first algorithm, called 2-hop Eclipse, strikes a much better
tradeoff between the resulting performance (of the hybrid switch) and the
computational complexity (of the algorithm) than the state of the art solution
Eclipse/Eclipse++. Our second algorithm, called BFF (best first fit), is the
first hybrid switching solution that exploits the potential partial
reconfiguration capability of the circuit switch for performance gains.
"
1307,"Network Cache Design under Stationary Requests: Exact Analysis and
  Poisson Approximation","  The design of caching algorithms to maximize hit probability has been
extensively studied. In this paper, we associate each content with a utility,
which is a function of either the corresponding content hit rate or hit
probability. We formulate a cache optimization problem to maximize the sum of
utilities over all contents under stationary and ergodic request processes.
This problem is non-convex in general but we reformulate it as a convex
optimization problem when the inter-request time (irt) distribution has a
non-increasing hazard rate function. We provide explicit optimal solutions for
some irt distributions, and compare the solutions of the hit-rate based (HRB)
and hit-probability based (HPB) problems. We formulate a reverse engineering
based dual implementation of LRU under stationary arrivals. We also propose
decentralized algorithms that can be implemented using limited information and
use a discrete time Lyapunov technique (DTLT) to correctly characterize their
stability. We find that decentralized algorithms that solve HRB are more robust
than decentralized HPB algorithms. Informed by these results, we further
propose lightweight Poisson approximate decentralized and online algorithms
that are accurate and efficient in achieving optimal hit rates and hit
probabilities.
"
1308,"Block-Diagonal and LT Codes for Distributed Computing With Straggling
  Servers","  We propose two coded schemes for the distributed computing problem of
multiplying a matrix by a set of vectors. The first scheme is based on
partitioning the matrix into submatrices and applying maximum distance
separable (MDS) codes to each submatrix. For this scheme, we prove that up to a
given number of partitions the communication load and the computational delay
(not including the encoding and decoding delay) are identical to those of the
scheme recently proposed by Li et al., based on a single, long MDS code.
However, due to the use of shorter MDS codes, our scheme yields a significantly
lower overall computational delay when the delay incurred by encoding and
decoding is also considered. We further propose a second coded scheme based on
Luby Transform (LT) codes under inactivation decoding. Interestingly, LT codes
may reduce the delay over the partitioned scheme at the expense of an increased
communication load. We also consider distributed computing under a deadline and
show numerically that the proposed schemes outperform other schemes in the
literature, with the LT code-based scheme yielding the best performance for the
scenarios considered.
"
1309,"Grand Challenge: Optimized Stage Processing for Anomaly Detection on
  Numerical Data Streams","  The 2017 Grand Challenge focused on the problem of automatic detection of
anomalies for manufacturing equipment. This paper reports the technical details
of a solution focused on particular optimizations of the processing stages.
These included customized input parsing, fine tuning of a k-means clustering
algorithm and probability analysis using a lazy flavor of a Markov chain. We
have observed in our custom implementation that carefully tweaking these
processing stages at single node level by leveraging various data stream
characteristics can yield good performance results. We start the paper with
several observations concerning the input data stream, following with our
solution description with details on particular optimizations, and we conclude
with evaluation and a discussion of obtained results.
"
1310,"Scalable Load Balancing in Networked Systems: Universality Properties
  and Stochastic Coupling Methods","  We present an overview of scalable load balancing algorithms which provide
favorable delay performance in large-scale systems, and yet only require
minimal implementation overhead. Aimed at a broad audience, the paper starts
with an introduction to the basic load balancing scenario, consisting of a
single dispatcher where tasks arrive that must immediately be forwarded to one
of $N$ single-server queues.
  A popular class of load balancing algorithms are so-called power-of-$d$ or
JSQ($d$) policies, where an incoming task is assigned to a server with the
shortest queue among $d$ servers selected uniformly at random. This class
includes the Join-the-Shortest-Queue (JSQ) policy as a special case ($d = N$),
which has strong stochastic optimality properties and yields a mean waiting
time that vanishes as $N$ grows large for any fixed subcritical load. However,
a nominal implementation of the JSQ policy involves a prohibitive communication
burden in large-scale deployments. In contrast, a random assignment policy ($d
= 1$) does not entail any communication overhead, but the mean waiting time
remains constant as $N$ grows large for any fixed positive load.
  In order to examine the fundamental trade-off between performance and
implementation overhead, we consider an asymptotic regime where $d(N)$ depends
on $N$. We investigate what growth rate of $d(N)$ is required to match the
performance of the JSQ policy on fluid and diffusion scale. The results
demonstrate that the asymptotics for the JSQ($d(N)$) policy are insensitive to
the exact growth rate of $d(N)$, as long as the latter is sufficiently fast,
implying that the optimality of the JSQ policy can asymptotically be preserved
while dramatically reducing the communication overhead. We additionally show
how the communication overhead can be reduced yet further by the so-called
Join-the-Idle-Queue scheme, leveraging memory at the dispatcher.
"
1311,DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car,"  We present DeepPicar, a low-cost deep neural network based autonomous car
platform. DeepPicar is a small scale replication of a real self-driving car
called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),
which takes images from a front-facing camera as input and produces car
steering angles as output. DeepPicar uses the same network architecture---9
layers, 27 million connections and 250K parameters---and can drive itself in
real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using
DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end
deep learning based real-time control of autonomous vehicles. We also
systematically compare other contemporary embedded computing platforms using
the DeepPicar's CNN-based real-time control workload. We find that all tested
platforms, including the Pi 3, are capable of supporting the CNN-based
real-time control, from 20 Hz up to 100 Hz, depending on hardware platform.
However, we find that shared resource contention remains an important issue
that must be considered in applying CNN models on shared memory based embedded
computing platforms; we observe up to 11.6X execution time increase in the CNN
based control loop due to shared resource contention. To protect the CNN
workload, we also evaluate state-of-the-art cache partitioning and memory
bandwidth throttling techniques on the Pi 3. We find that cache partitioning is
ineffective, while memory bandwidth throttling is an effective solution.
"
1312,Protecting real-time GPU kernels on integrated CPU-GPU SoC platforms,"  Integrated CPU-GPU architecture provides excellent acceleration capabilities
for data parallel applications on embedded platforms while meeting the size,
weight and power (SWaP) requirements. However, sharing of main memory between
CPU applications and GPU kernels can severely affect the execution of GPU
kernels and diminish the performance gain provided by GPU. For example, in the
NVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we
noticed that in the worst case scenario, the GPU kernels can suffer as much as
4X slowdown in the presence of co-running memory intensive CPU applications
compared to their solo execution. In this paper, we propose a software
mechanism, which we call BWLOCK++, to protect the performance of GPU kernels
from co-scheduled memory intensive CPU applications.
"
1313,"The L-CSC cluster: greenest supercomputer in the world in Green500 list
  of November 2014","  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose
compute cluster built of commodity hardware installed at GSI. Its main
operational purpose is Lattice QCD (LQCD) calculations for physics simulations.
Quantum Chromo Dynamics (QCD) is the physical theory describing the strong
force, one of the four known fundamental interactions in the universe. L-CSC
leverages a multi-GPU design accommodating the huge demand of LQCD for memory
bandwidth. In recent years, heterogeneous clusters with accelerators such as
GPUs have become more and more powerful while supercomputers in general have
shown enormous increases in power consumption making electricity costs and
cooling a significant factor in the total cost of ownership. Using mainly GPUs
for processing, L-CSC is very power efficient, and its architecture was
optimized to provide the greatest possible power efficiency. This paper
presents the cluster design as well as optimizations to improve the power
efficiency. It examines the power measurements performed for the Green500 list
of the most power efficient supercomputers in the world which led to the number
1 position as the greenest supercomputer in November 2014.
"
1314,"Cuckoo++ Hash Tables: High-Performance Hash Tables for Networking
  Applications","  Hash tables are an essential data-structure for numerous networking
applications (e.g., connection tracking, firewalls, network address
translators). Among these, cuckoo hash tables provide excellent performance by
allowing lookups to be processed with very few memory accesses (2 to 3 per
lookup). Yet, for large tables, cuckoo hash tables remain memory bound and each
memory access impacts performance. In this paper, we propose algorithmic
improvements to cuckoo hash tables allowing to eliminate some unnecessary
memory accesses; these changes are conducted without altering the properties of
the original cuckoo hash table so that all existing theoretical analysis remain
applicable. On a single core, our hash table achieves 37M lookups per second
for positive lookups (i.e., when the key looked up is present in the table),
and 60M lookups per second for negative lookups, a 50% improvement over the
implementation included into the DPDK. On a 18-core, with mostly positive
lookups, our implementation achieves 496M lookups per second, a 45% improvement
over DPDK.
"
1315,"A Loop-Based Methodology for Reducing Computational Redundancy in
  Workload Sets","  The design of general purpose processors relies heavily on a workload
gathering step in which representative programs are collected from various
application domains. Processor performance, when running the workload set, is
profiled using simulators that model the targeted processor architecture.
However, simulating the entire workload set is prohibitively time-consuming,
which precludes considering a large number of programs. To reduce simulation
time, several techniques in the literature have exploited the internal program
repetitiveness to extract and execute only representative code segments.
Existing so- lutions are based on reducing cross-program computational
redundancy or on eliminating internal-program redundancy to decrease execution
time. In this work, we propose an orthogonal and complementary loop- centric
methodology that targets loop-dominant programs by exploiting internal-program
characteristics to reduce cross-program computational redundancy. The approach
employs a newly developed framework that extracts and analyzes core loops
within workloads. The collected characteristics model memory behavior,
computational complexity, and data structures of a program, and are used to
construct a signature vector for each program. From these vectors,
cross-workload similarity metrics are extracted, which are processed by a novel
heuristic to exclude similar programs and reduce redundancy within the set.
Finally, a reverse engineering approach that synthesizes executable
micro-benchmarks having the same instruction mix as the loops in the original
workload is introduced. A tool that automates the flow steps of the proposed
methodology is developed. Simulation results demonstrate that applying the
proposed methodology to a set of workloads reduces the set size by half, while
preserving the main characterizations of the initial workloads.
"
1316,A GPU Accelerated Discontinuous Galerkin Incompressible Flow Solver,"  We present a GPU-accelerated version of a high-order discontinuous Galerkin
discretization of the unsteady incompressible Navier-Stokes equations. The
equations are discretized in time using a semi-implicit scheme with explicit
treatment of the nonlinear term and implicit treatment of the split Stokes
operators. The pressure system is solved with a conjugate gradient method
together with a fully GPU-accelerated multigrid preconditioner which is
designed to minimize memory requirements and to increase overall performance. A
semi-Lagrangian subcycling advection algorithm is used to shift the
computational load per timestep away from the pressure Poisson solve by
allowing larger timestep sizes in exchange for an increased number of advection
steps. Numerical results confirm we achieve the design order accuracy in time
and space. We optimize the performance of the most time-consuming kernels by
tuning the fine-grain parallelism, memory utilization, and maximizing
bandwidth. To assess overall performance we present an empirically calibrated
roofline performance model for a target GPU to explain the achieved efficiency.
We demonstrate that, in the most cases, the kernels used in the solver are
close to their empirically predicted roofline performance.
"
1317,"QuickCast: Fast and Efficient Inter-Datacenter Transfers using
  Forwarding Tree Cohorts","  Large inter-datacenter transfers are crucial for cloud service efficiency and
are increasingly used by organizations that have dedicated wide area networks
between datacenters. A recent work uses multicast forwarding trees to reduce
the bandwidth needs and improve completion times of point-to-multipoint
transfers. Using a single forwarding tree per transfer, however, leads to poor
performance because the slowest receiver dictates the completion time for all
receivers. Using multiple forwarding trees per transfer alleviates this
concern--the average receiver could finish early; however, if done naively,
bandwidth usage would also increase and it is apriori unclear how best to
partition receivers, how to construct the multiple trees and how to determine
the rate and schedule of flows on these trees. This paper presents QuickCast, a
first solution to these problems. Using simulations on real-world network
topologies, we see that QuickCast can speed up the average receiver's
completion time by as much as $10\times$ while only using $1.04\times$ more
bandwidth; further, the completion time for all receivers also improves by as
much as $1.6\times$ faster at high loads.
"
1318,"Computing the sparse matrix vector product using block-based kernels
  without zero padding on processors with AVX-512 instructions","  The sparse matrix-vector product (SpMV) is a fundamental operation in many
scientific applications from various fields. The High Performance Computing
(HPC) community has therefore continuously invested a lot of effort to provide
an efficient SpMV kernel on modern CPU architectures. Although it has been
shown that block-based kernels help to achieve high performance, they are
difficult to use in practice because of the zero padding they require. In the
current paper, we propose new kernels using the AVX-512 instruction set, which
makes it possible to use a blocking scheme without any zero padding in the
matrix memory storage. We describe mask-based sparse matrix formats and their
corresponding SpMV kernels highly optimized in assembly language. Considering
that the optimal blocking size depends on the matrix, we also provide a method
to predict the best kernel to be used utilizing a simple interpolation of
results from previous executions. We compare the performance of our approach to
that of the Intel MKL CSR kernel and the CSR5 open-source package on a set of
standard benchmark matrices. We show that we can achieve significant
improvements in many cases, both for sequential and for parallel executions.
Finally, we provide the corresponding code in an open source library, called
SPC5.
"
1319,"Joint Data Compression and Caching: Approaching Optimality with
  Guarantees","  We consider the problem of optimally compressing and caching data across a
communication network. Given the data generated at edge nodes and a routing
path, our goal is to determine the optimal data compression ratios and caching
decisions across the network in order to minimize average latency, which can be
shown to be equivalent to maximizing the compression and caching gain under an
energy consumption constraint. We show that this problem is NP-hard in general
and the hardness is caused by the caching decision subproblem, while the
compression sub-problem is polynomial-time solvable. We then propose an
approximation algorithm that achieves a $(1-1/e)$-approximation solution to the
optimum in strongly polynomial time. We show that our proposed algorithm
achieve the near-optimal performance in synthetic-based evaluations. In this
paper, we consider a tree-structured network as an illustrative example, but
our results easily extend to general network topology at the expense of more
complicated notations.
"
1320,Asymptotic Miss Ratio of LRU Caching with Consistent Hashing,"  To efficiently scale data caching infrastructure to support emerging big data
applications, many caching systems rely on consistent hashing to group a large
number of servers to form a cooperative cluster. These servers are organized
together according to a random hash function. They jointly provide a unified
but distributed hash table to serve swift and voluminous data item requests.
Different from the single least-recently-used (LRU) server that has already
been extensively studied, theoretically characterizing a cluster that consists
of multiple LRU servers remains yet to be explored. These servers are not
simply added together; the random hashing complicates the behavior. To this
end, we derive the asymptotic miss ratio of data item requests on a LRU cluster
with consistent hashing. We show that these individual cache spaces on
different servers can be effectively viewed as if they could be pooled together
to form a single virtual LRU cache space parametrized by an appropriate cache
size. This equivalence can be established rigorously under the condition that
the cache sizes of the individual servers are large. For typical data caching
systems this condition is common. Our theoretical framework provides a
convenient abstraction that can directly apply the results from the simpler
single LRU cache to the more complex LRU cluster with consistent hashing.
"
1321,"Optimal Content Replication and Request Matching in Large Caching
  Systems","  We consider models of content delivery networks in which the servers are
constrained by two main resources: memory and bandwidth. In such systems, the
throughput crucially depends on how contents are replicated across servers and
how the requests of specific contents are matched to servers storing those
contents. In this paper, we first formulate the problem of computing the
optimal replication policy which if combined with the optimal matching policy
maximizes the throughput of the caching system in the stationary regime. It is
shown that computing the optimal replication policy for a given system is an
NP-hard problem. A greedy replication scheme is proposed and it is shown that
the scheme provides a constant factor approximation guarantee. We then propose
a simple randomized matching scheme which avoids the problem of interruption in
service of the ongoing requests due to re-assignment or repacking of the
existing requests in the optimal matching policy. The dynamics of the caching
system is analyzed under the combination of proposed replication and matching
schemes. We study a limiting regime, where the number of servers and the
arrival rates of the contents are scaled proportionally, and show that the
proposed policies achieve asymptotic optimality. Extensive simulation results
are presented to evaluate the performance of different policies and study the
behavior of the caching system under different service time distributions of
the requests.
"
1322,"A Stitch in Time Saves Nine -- SPARQL querying of Property Graphs using
  Gremlin Traversals","  Knowledge graphs have become popular over the past years and frequently rely
on the Resource Description Framework (RDF) or Property Graphs (PG) as
underlying data models. However, the query languages for these two data models
-- SPARQL for RDF and Gremlin for property graph traversal -- are lacking
interoperability. We present Gremlinator, a novel SPARQL to Gremlin translator.
Gremlinator translates SPARQL queries to Gremlin traversals for executing graph
pattern matching queries over graph databases. This allows to access and query
a wide variety of Graph Data Management Systems (DMS) using the W3C
standardized SPARQL query language and avoid the learning curve of a new Graph
Query Language. Gremlin is a system-agnostic traversal language covering both
OLTP graph database or OLAP graph processors, thus making it a desirable choice
for supporting interoperability wrt. querying Graph DMSs. We present a
comprehensive empirical evaluation of Gremlinator and demonstrate its validity
and applicability by executing SPARQL queries on top of the leading graph
stores Neo4J, Sparksee, and Apache TinkerGraph and compare the performance with
the RDF stores Virtuoso, 4Store and JenaTDB. Our evaluation demonstrates the
substantial performance gain obtained by the Gremlin counterparts of the SPARQL
queries, especially for star-shaped and complex queries.
"
1323,"Learning Aided Optimization for Energy Harvesting Devices with Outdated
  State Information","  This paper considers utility optimal power control for energy harvesting
wireless devices with a finite capacity battery. The distribution information
of the underlying wireless environment and harvestable energy is unknown and
only outdated system state information is known at the device controller. This
scenario shares similarity with Lyapunov opportunistic optimization and online
learning but is different from both. By a novel combination of Zinkevich's
online gradient learning technique and the drift-plus-penalty technique from
Lyapunov opportunistic optimization, this paper proposes a learning-aided
algorithm that achieves utility within $O(\epsilon)$ of the optimal, for any
desired $\epsilon>0$, by using a battery with an $O(1/\epsilon)$ capacity. The
proposed algorithm has low complexity and makes power investment decisions
based on system history, without requiring knowledge of the system state or its
probability distribution.
"
1324,"DuctTeip: An efficient programming model for distributed task based
  parallel computing","  Current high-performance computer systems used for scientific computing
typically combine shared memory computational nodes in a distributed memory
environment. Extracting high performance from these complex systems requires
tailored approaches. Task based parallel programming has been successful both
in simplifying the programming and in exploiting the available hardware
parallelism for shared memory systems. In this paper we focus on how to extend
task parallel programming to distributed memory systems. We use a hierarchical
decomposition of tasks and data in order to accommodate the different levels of
hardware. We test the proposed programming model on two different applications,
a Cholesky factorization, and a solver for the Shallow Water Equations. We also
compare the performance of our implementation with that of other frameworks for
distributed task parallel programming, and show that it is competitive.
"
1325,"Task parallel implementation of a solver for electromagnetic scattering
  problems","  Electromagnetic computations, where the wavelength is small in relation to
the geometry of interest, become computationally demanding. In order to manage
computations for realistic problems like electromagnetic scattering from
aircraft, the use of parallel computing is essential. In this paper, we
describe how a solver based on a hierarchical nested equivalent source
approximation can be implemented in parallel using a task based programming
model. We show that the effort for moving from the serial implementation to a
parallel implementation is modest due to the task based programming paradigm,
and that the performance achieved on a multicore system is excellent provided
that the task size, depending on the method parameters, is large enough.
"
1326,"Effect of Meltdown and Spectre Patches on the Performance of HPC
  Applications","  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
"
1327,Distributed dynamic load balancing for task parallel programming,"  In this paper, we derive and investigate approaches to dynamically load
balance a distributed task parallel application software. The load balancing
strategy is based on task migration. Busy processes export parts of their ready
task queue to idle processes. Idle--busy pairs of processes find each other
through a random search process that succeeds within a few steps with high
probability. We evaluate the load balancing approach for a block Cholesky
factorization implementation and observe a reduction in execution time on the
order of 5\% in the selected test cases.
"
1328,LCD: Low Latency Command Dissemination for A Platoon of Vehicles,"  In a vehicular platoon, a lead vehicle that is responsible for managing the
platoon's moving directions and velocity periodically disseminates control
commands to following vehicles based on vehicle-to-vehicle communications.
However, reducing command dissemination latency with multiple vehicles while
ensuring successful message delivery to the tail vehicle is challenging. We
propose a new linear dynamic programming algorithm using backward induction and
interchange arguments to minimize the dissemination latency of the vehicles.
Furthermore, a closed form of dissemination latency in vehicular platoon is
obtained by utilizing Markov chain with M/M/1 queuing model. Simulation results
confirm that the proposed dynamic programming algorithm improves the
dissemination rate by at least 50.9%, compared to similar algorithms in the
literature. Moreover, it also approximates the best performance with the
maximum gap of up to 0.2 second in terms of latency.
"
1329,"JointDNN: An Efficient Training and Inference Engine for Intelligent
  Mobile Cloud Computing Services","  Deep learning models are being deployed in many mobile intelligent
applications. End-side services, such as intelligent personal assistants,
autonomous cars, and smart home services often employ either simple local
models on the mobile or complex remote models on the cloud. However, recent
studies have shown that partitioning the DNN computations between the mobile
and cloud can increase the latency and energy efficiencies. In this paper, we
propose an efficient, adaptive, and practical engine, JointDNN, for
collaborative computation between a mobile device and cloud for DNNs in both
inference and training phase. JointDNN not only provides an energy and
performance efficient method of querying DNNs for the mobile side but also
benefits the cloud server by reducing the amount of its workload and
communications compared to the cloud-only approach. Given the DNN architecture,
we investigate the efficiency of processing some layers on the mobile device
and some layers on the cloud server. We provide optimization formulations at
layer granularity for forward- and backward-propagations in DNNs, which can
adapt to mobile battery limitations and cloud server load constraints and
quality of service. JointDNN achieves up to 18 and 32 times reductions on the
latency and mobile energy consumption of querying DNNs compared to the
status-quo approaches, respectively.
"
1330,"Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability,
  and Performance","  Basic mirroring (BM) classified as RAID level 1 replicates data on two disks,
thus doubling disk access bandwidth for read requests. RAID1/0 is an array of
BM pairs with balanced loads due to striping. When a disk fails the read load
on its pair is doubled, which results in halving the maximum attainable
bandwidth. We review RAID1 organizations which attain a balanced load upon disk
failure, but as shown by reliability analysis tend to be less reliable than
RAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend
to have a higher reliability than mirrored disks, but incur a higher overhead
in updating data. Read request response time can be improved by processing them
at a higher priority than writes, since they have a direct effect on
application response time. Shortest seek distance and affinity based routing
both shorten seek time. Anticipatory arm placement places arms optimally to
minimize the seek distance. The analysis of RAID1 in normal, degraded, and
rebuild mode is provided to quantify RAID1/0 performance. We compare the
reliability of mirrored disk organizations against each other and hybrid disks
and erasure coded disk arrays.
"
1331,"BOPS, Not FLOPS! A New Metric and Roofline Performance Model For
  Datacenter Computing","  For emerging datacenter (in short, DC) workloads, such as online Internet
services or offline data analytics, how to evaluate the upper bound performance
and provide apple-to-apple comparisons are fundamental problems. To this end, a
unified computation-centric metric is an essential requirement. As the most
important computation-centric performance metric, FLOPS has guided computing
systems evolutions for many years. However, our observations demonstrate that
the average FLOPS efficiency of the DC workloads is only 0.1%, which implies
that FLOPS is inappropriate for DC computing. To address the above issue, we
propose BOPS (Basic Operations Per Second), which is the average number of BOPs
(Basic OPerations) completed per second. We conduct the analysis on the
characteristics of seventeen typical DC workloads and extract the minimum
representative computation operations set, which is composed of integer and
floating point computation operations of arithmetic, comparing and array
addressing. Then, we propose the formalized BOPS definition and the BOPS based
upper bound performance model. Finally, the BOPS measuring tool is also
implemented. We perform experiments with seventeen DC workloads on three
typical Intel processors platforms. First, BOPS can reflect the performance gap
of different computing systems, the bias between the peak BOPS performance gap
and the average DC workloads' wall clock time gap is no more than 10%. Second,
the Sort workload can achieve 32% BOPS efficiency on the experimental platform.
At last, we present two use cases of BOPS. One is the BOPS based system
evaluation, we illustrate that BOPS can compare performance of workloads from
multiple domains. The other is BOPS based optimizations. We show that under the
guiding of the BOPS based upper bound model, the Sort workload and the Redis
workload achieve 4.4X and 1.2X performance improvements respectively.
"
1332,"Using Meta-heuristics and Machine Learning for Software Optimization of
  Parallel Computing Systems: A Systematic Literature Review","  While modern parallel computing systems offer high performance, utilizing
these powerful computing resources to the highest possible extent demands
advanced knowledge of various hardware architectures and parallel programming
models. Furthermore, optimized software execution on parallel computing systems
demands consideration of many parameters at compile-time and run-time.
Determining the optimal set of parameters in a given execution context is a
complex task, and therefore to address this issue researchers have proposed
different approaches that use heuristic search or machine learning. In this
paper, we undertake a systematic literature review to aggregate, analyze and
classify the existing software optimization methods for parallel computing
systems. We review approaches that use machine learning or meta-heuristics for
software optimization at compile-time and run-time. Additionally, we discuss
challenges and future research directions. The results of this study may help
to better understand the state-of-the-art techniques that use machine learning
and meta-heuristics to deal with the complexity of software optimization for
parallel computing systems. Furthermore, it may aid in understanding the
limitations of existing approaches and identification of areas for improvement.
"
1333,"diagnoseIT: Expertengest\""utzte automatische Diagnose von
  Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht)","  This is the final report of the collaborative research project diagnoseIT on
expert-guided automatic diagnosis of performance problems in enterprise
applications.
"
1334,"Data Dwarfs: A Lens Towards Fully Understanding Big Data and AI
  Workloads","  The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to
characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of unit of computations performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data dwarf. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data dwarfs that takes up most of run time, including Matrix, Sampling,
Logic, Transform, Set, Graph, Sort and Statistic. We implement the eight data
dwarfs on different software stacks as the micro benchmarks of an open-source
big data and AI benchmark suite, and perform comprehensive characterization of
those data dwarfs from perspective of data sizes, types, sources, and patterns
as a lens towards fully understanding big data and AI workloads.
"
1335,A Measurement Theory of Locality,"  Locality is a fundamental principle used extensively in program and system
optimization. It can be measured in many ways. This paper formalizes the
metrics of locality into a measurement theory. The new theory includes the
precise definition of locality metrics based on access frequency, reuse time,
reuse distance, working set, footprint, and the cache miss ratio. It gives the
formal relation between these definitions and the proofs of equivalence or
non-equivalence. It provides the theoretical justification for four successful
locality models in operating systems, programming languages, and computer
architectures which were developed empirically.
"
1336,"A large deviation approach to super-critical bootstrap percolation on
  the random graph $G_{n,p}$","  We consider the Erd\""{o}s--R\'{e}nyi random graph $G_{n,p}$ and we analyze
the simple irreversible epidemic process on the graph, known in the literature
as bootstrap percolation. We give a quantitative version of some results by
Janson et al. (2012), providing a fine asymptotic analysis of the final size
$A_n^*$ of active nodes, under a suitable super-critical regime. More
specifically, we establish large deviation principles for the sequence of
random variables $\{\frac{n- A_n^*}{f(n)}\}_{n\geq 1}$ with explicit rate
functions and allowing the scaling function $f$ to vary in the widest possible
range.
"
1337,"RDMAvisor: Toward Deploying Scalable and Simple RDMA as a Service in
  Datacenters","  RDMA is increasingly adopted by cloud computing platforms to provide low CPU
overhead, low latency, high throughput network services. On the other hand,
however, it is still challenging for developers to realize fast deployment of
RDMA-aware applications in the datacenter, since the performance is highly
related to many lowlevel details of RDMA operations. To address this problem,
we present a simple and scalable RDMA as Service (RaaS) to mitigate the impact
of RDMA operational details. RaaS provides careful message buffer management to
improve CPU/memory utilization and improve the scalability of RDMA operations.
These optimized designs lead to simple and flexible programming model for
common and knowledgeable users. We have implemented a prototype of RaaS, named
RDMAvisor, and evaluated its performance on a cluster with a large number of
connections. Our experiment results demonstrate that RDMAvisor achieves high
throughput for thousand of connections and maintains low CPU and memory
overhead through adaptive RDMA transport selection.
"
1338,Analytical Cost Metrics : Days of Future Past,"  As we move towards the exascale era, the new architectures must be capable of
running the massive computational problems efficiently. Scientists and
researchers are continuously investing in tuning the performance of
extreme-scale computational problems. These problems arise in almost all areas
of computing, ranging from big data analytics, artificial intelligence, search,
machine learning, virtual/augmented reality, computer vision, image/signal
processing to computational science and bioinformatics. With Moore's law
driving the evolution of hardware platforms towards exascale, the dominant
performance metric (time efficiency) has now expanded to also incorporate
power/energy efficiency. Therefore, the major challenge that we face in
computing systems research is: ""how to solve massive-scale computational
problems in the most time/power/energy efficient manner?""
  The architectures are constantly evolving making the current performance
optimizing strategies less applicable and new strategies to be invented. The
solution is for the new architectures, new programming models, and applications
to go forward together. Doing this is, however, extremely hard. There are too
many design choices in too many dimensions. We propose the following strategy
to solve the problem: (i) Models - Develop accurate analytical models (e.g.
execution time, energy, silicon area) to predict the cost of executing a given
program, and (ii) Complete System Design - Simultaneously optimize all the cost
models for the programs (computational problems) to obtain the most
time/area/power/energy efficient solution. Such an optimization problem evokes
the notion of codesign.
"
1339,"Tuning Streamed Applications on Intel Xeon Phi: A Machine Learning Based
  Approach","  Many-core accelerators, as represented by the XeonPhi coprocessors and
GPGPUs, allow software to exploit spatial and temporal sharing of computing
resources to improve the overall system performance. To unlock this performance
potential requires software to effectively partition the hardware resource to
maximize the overlap between hostdevice communication and accelerator
computation, and to match the granularity of task parallelism to the resource
partition. However, determining the right resource partition and task
parallelism on a per program, per dataset basis is challenging. This is because
the number of possible solutions is huge, and the benefit of choosing the right
solution may be large, but mistakes can seriously hurt the performance. In this
paper, we present an automatic approach to determine the hardware resource
partition and the task granularity for any given application, targeting the
Intel XeonPhi architecture. Instead of hand-crafting the heuristic for which
the process will have to repeat for each hardware generation, we employ machine
learning techniques to automatically learn it. We achieve this by first
learning a predictive model offline using training programs; we then use the
learned model to predict the resource partition and task granularity for any
unseen programs at runtime. We apply our approach to 23 representative parallel
applications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core
platform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which
translates to 94.5% of the performance delivered by a theoretically perfect
predictor.
"
1340,"A local parallel communication algorithm for polydisperse rigid body
  dynamics","  The simulation of large ensembles of particles is usually parallelized by
partitioning the domain spatially and using message passing to communicate
between the processes handling neighboring subdomains. The particles are
represented as individual geometric objects and are associated to the
subdomains. Handling collisions and migrating particles between subdomains, as
required for proper parallel execution, requires a complex communication
protocol. Typically, the parallelization is restricted to handling only
particles that are smaller than a subdomain. In many applications, however,
particle sizes may vary drastically with some of them being larger than a
subdomain. In this article we propose a new communication and synchronization
algorithm that can handle the parallelization without size restrictions on the
particles. Despite the additional complexity and extended functionality, the
new algorithm introduces only minimal overhead. We demonstrate the scalability
of the previous and the new communication algorithms up to almost two million
parallel processes and for handling ten billion (1e10) geometrically resolved
particles on a state-of-the-art petascale supercomputer. Different scenarios
are presented to analyze the performance of the new algorithm and to
demonstrate its capability to simulate polydisperse scenarios, where large
individual particles can extend across several subdomains.
"
1341,"Performance Analysis of Low Latency Multiple Full-Duplex Selective
  Decode and Forward Relays","  In order to follow up with mission-critical applications, new features need
to be carried to satisfy a reliable communication with reduced latency. With
this regard, this paper proposes a low latency cooperative transmission scheme,
where multiple full-duplex relays, simultaneously, assist the communication
between a source node and a destination node. First, we present the
communication model of the proposed transmission scheme. Then, we derive the
outage probability closed-form for two cases: asynchronous transmission (where
all relays have different processing delay) and synchronous transmissions
(where all relays have the same processing delay). Finally, using simulations,
we confirm the theoretical results and compare the proposed multi-relays
transmission scheme with relay selection schemes.
"
1342,On the Power-of-d-choices with Least Loaded Server Selection,"  Motivated by distributed schedulers that combine the power-of-d-choices with
late binding and systems that use replication with cancellation-on-start, we
study the performance of the LL(d) policy which assigns a job to a server that
currently has the least workload among d randomly selected servers in
large-scale homogeneous clusters. We consider general service time
distributions and propose a partial integro-differential equation to describe
the evolution of the system. This equation relies on the earlier proven ansatz
for LL(d) which asserts that the workload distribution of any finite set of
queues becomes independent of one another as the number of servers tends to
infinity. Based on this equation we propose a fixed point iteration for the
limiting workload distribution and study its convergence. For exponential job
sizes we present a simple closed form expression for the limiting workload
distribution that is valid for any work-conserving service discipline as well
as for the limiting response time distribution in case of
first-come-first-served scheduling. We further show that for phase-type
distributed job sizes the limiting workload and response time distribution can
be expressed via the unique solution of a simple set of ordinary differential
equations. Numerical and analytical results that compare response time of the
classic power-of-d-choices algorithm and the LL(d) policy are also presented
and the accuracy of the limiting response time distribution for finite systems
is illustrated using simulation.
"
1343,Power-of-$d$-Choices with Memory: Fluid Limit and Optimality,"  In multi-server distributed queueing systems, the access of stochastically
arriving jobs to resources is often regulated by a dispatcher, also known as
load balancer. A fundamental problem consists in designing a load balancing
algorithm that minimizes the delays experienced by jobs. During the last two
decades, the power-of-$d$-choice algorithm, based on the idea of dispatching
each job to the least loaded server out of $d$ servers randomly sampled at the
arrival of the job itself, has emerged as a breakthrough in the foundations of
this area due to its versatility and appealing asymptotic properties. In this
paper, we consider the power-of-$d$-choice algorithm with the addition of a
local memory that keeps track of the latest observations collected over time on
the sampled servers. Then, each job is sent to a server with the lowest
observation. We show that this algorithm is asymptotically optimal in the sense
that the load balancer can always assign each job to an idle server in the
large-system limit. This holds true if and only if the system load $\lambda$ is
less than $1-\frac{1}{d}$. If this condition is not satisfied, we show that
queue lengths are tightly bounded by $\left\lceil - \frac{ \log
(1-\lambda)}{\log (\lambda d +1)} \right\rceil$. This is in contrast with the
classic version of the power-of-$d$-choice algorithm, where at the fluid scale
a strictly positive proportion of servers containing $i$ jobs exists for all
$i\ge 0$, in equilibrium. Our results quantify and highlight the importance of
using memory as a means to enhance performance in randomized load balancing.
"
1344,On Learning the $c\mu$ Rule in Single and Parallel Server Networks,"  We consider learning-based variants of the $c \mu$ rule for scheduling in
single and parallel server settings of multi-class queueing systems.
  In the single server setting, the $c \mu$ rule is known to minimize the
expected holding-cost (weighted queue-lengths summed over classes and a fixed
time horizon). We focus on the problem where the service rates $\mu$ are
unknown with the holding-cost regret (regret against the $c \mu$ rule with
known $\mu$) as our objective. We show that the greedy algorithm that uses
empirically learned service rates results in a constant holding-cost regret
(the regret is independent of the time horizon). This free exploration can be
explained in the single server setting by the fact that any work-conserving
policy obtains the same number of samples in a busy cycle.
  In the parallel server setting, we show that the $c \mu$ rule may result in
unstable queues, even for arrival rates within the capacity region. We then
present sufficient conditions for geometric ergodicity under the $c \mu$ rule.
Using these results, we propose an almost greedy algorithm that explores only
when the number of samples falls below a threshold. We show that this algorithm
delivers constant holding-cost regret because a free exploration condition is
eventually satisfied.
"
1345,Asymptotic efficiency of restart and checkpointing,"  Many tasks are subject to failure before completion. Two of the most common
failure recovery strategies are restart and checkpointing. Under restart, once
a failure occurs, it is restarted from the beginning. Under checkpointing, the
task is resumed from the preceding checkpoint after the failure. We study
asymptotic efficiency of restart for an infinite sequence of tasks, whose sizes
form a stationary sequence. We define asymptotic efficiency as the limit of the
ratio of the total time to completion in the absence of failures over the total
time to completion when failures take place. Whether the asymptotic efficiency
is positive or not depends on the comparison of the tail of the distributions
of the task size and the random variables governing failures. Our framework
allows for variations in the failure rates and dependencies between task sizes.
We also study a similar notion of asymptotic efficiency for checkpointing when
the task is infinite a.s. and the inter-checkpoint times are i.i.d.. Moreover,
in checkpointing, when the failures are exponentially distributed, we prove the
existence of an infinite sequence of universal checkpoints, which are always
used whenever the system starts from any checkpoint that precedes them.
"
1346,"Comparative study of finite element methods using the Time-Accuracy-Size
  (TAS) spectrum analysis","  We present a performance analysis appropriate for comparing algorithms using
different numerical discretizations. By taking into account the total
time-to-solution, numerical accuracy with respect to an error norm, and the
computation rate, a cost-benefit analysis can be performed to determine which
algorithm and discretization are particularly suited for an application. This
work extends the performance spectrum model in Chang et. al. 2017 for
interpretation of hardware and algorithmic tradeoffs in numerical PDE
simulation. As a proof-of-concept, popular finite element software packages are
used to illustrate this analysis for Poisson's equation.
"
1347,"RT-DAP: A Real-Time Data Analytics Platform for Large-scale Industrial
  Process Monitoring and Control","  In most process control systems nowadays, process measurements are
periodically collected and archived in historians. Analytics applications
process the data, and provide results offline or in a time period that is
considerably slow in comparison to the performance of the manufacturing
process. Along with the proliferation of Internet-of-Things (IoT) and the
introduction of ""pervasive sensors"" technology in process industries,
increasing number of sensors and actuators are installed in process plants for
pervasive sensing and control, and the volume of produced process data is
growing exponentially. To digest these data and meet the ever-growing
requirements to increase production efficiency and improve product quality,
there needs to be a way to both improve the performance of the analytics system
and scale the system to closely monitor a much larger set of plant resources.
In this paper, we present a real-time data analytics platform, called RT-DAP,
to support large-scale continuous data analytics in process industries. RT-DAP
is designed to be able to stream, store, process and visualize a large volume
of realtime data flows collected from heterogeneous plant resources, and
feedback to the control system and operators in a realtime manner. A prototype
of the platform is implemented on Microsoft Azure. Our extensive experiments
validate the design methodologies of RT-DAP and demonstrate its efficiency in
both component and system levels.
"
1348,BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite,"  Several fundamental changes in technology indicate domain-specific hardware
and software co-design is the only path left. In this context, architecture,
system, data management, and machine learning communities pay greater attention
to innovative big data and AI algorithms, architecture, and systems.
Unfortunately, complexity, diversity, frequently-changed workloads, and rapid
evolution of big data and AI systems raise great challenges. First, the
traditional benchmarking methodology that creates a new benchmark or proxy for
every possible workload is not scalable, or even impossible for Big Data and AI
benchmarking. Second, it is prohibitively expensive to tailor the architecture
to characteristics of one or more application or even a domain of applications.
We consider each big data and AI workload as a pipeline of one or more classes
of units of computation performed on different initial or intermediate data
inputs, each class of which we call a data motif. On the basis of our previous
work that identifies eight data motifs taking up most of the run time of a wide
variety of big data and AI workloads, we propose a scalable benchmarking
methodology that uses the combination of one or more data motifs---to represent
diversity of big data and AI workloads. Following this methodology, we present
a unified big data and AI benchmark suite---BigDataBench 4.0, publicly
available from~\url{http://prof.ict.ac.cn/BigDataBench}. This unified benchmark
suite sheds new light on domain-specific hardware and software co-design:
tailoring the system and architecture to characteristics of the unified eight
data motifs other than one or more application case by case. Also, for the
first time, we comprehensively characterize the CPU pipeline efficiency using
the benchmarks of seven workload types in BigDataBench 4.0.
"
1349,Stochastic Gradient Descent on Highly-Parallel Architectures,"  There is an increased interest in building data analytics frameworks with
advanced algebraic capabilities both in industry and academia. Many of these
frameworks, e.g., TensorFlow and BIDMach, implement their compute-intensive
primitives in two flavors---as multi-thread routines for multi-core CPUs and as
highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is
the most popular optimization method for model training implemented extensively
on modern data analytics platforms. While the data-intensive properties of SGD
are well-known, there is an intense debate on which of the many SGD variants is
better in practice. In this paper, we perform a comprehensive study of parallel
SGD for training generalized linear models. We consider the impact of three
factors -- computing architecture (multi-core CPU or GPU), synchronous or
asynchronous model updates, and data sparsity -- on three measures---hardware
efficiency, statistical efficiency, and time to convergence. In the process, we
design an optimized asynchronous SGD algorithm for GPU that leverages warp
shuffling and cache coalescing for data and model access. We draw several
interesting findings from our extensive experiments with logistic regression
(LR) and support vector machines (SVM) on five real datasets. For synchronous
SGD, GPU always outperforms parallel CPU---they both outperform a sequential
CPU solution by more than 400X. For asynchronous SGD, parallel CPU is the
safest choice while GPU with data replication is better in certain situations.
The choice between synchronous GPU and asynchronous CPU depends on the task and
the characteristics of the data. As a reference, our best implementation
outperforms TensorFlow and BIDMach consistently. We hope that our insights
provide a useful guide for applying parallel SGD to generalized linear models.
"
1350,"Minimizing Flow Completion Times using Adaptive Routing over
  Inter-Datacenter Wide Area Networks","  Inter-datacenter networks connect dozens of geographically dispersed
datacenters and carry traffic flows with highly variable sizes and different
classes. Adaptive flow routing can improve efficiency and performance by
assigning paths to new flows according to network status and flow properties. A
popular approach widely used for traffic engineering is based on current
bandwidth utilization of links. We propose an alternative that reduces
bandwidth usage by up to at least 50% and flow completion times by up to at
least 40% across various scheduling policies and flow size distributions.
"
1351,"Less is More: Exploiting the Standard Compiler Optimization Levels for
  Better Performance and Energy Consumption","  This paper presents the interesting observation that by performing fewer of
the optimizations available in a standard compiler optimization level such as
-O2, while preserving their original ordering, significant savings can be
achieved in both execution time and energy consumption. This observation has
been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM
Cortex-M3, using two different versions of the LLVM compilation framework; v3.8
and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated
performance gains for at least half of the benchmarks for both processors. An
average execution time reduction of 2.4% and 5.3% was achieved across all the
benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with
execution time improvements ranging from 1% up to 90% over the -O2. The savings
that can be achieved are in the same range as what can be achieved by the
state-of-the-art compilation approaches that use iterative compilation or
machine learning to select flags or to determine phase orderings that result in
more efficient code. In contrast to these time consuming and expensive to apply
techniques, our approach only needs to test a limited number of optimization
configurations, less than 64, to obtain similar or even better savings.
Furthermore, our approach can support multi-criteria optimization as it targets
execution time, energy consumption and code size at the same time.
"
1352,"Time Varying Channel Tracking with Spatial and Temporal BEM for Massive
  MIMO Systems","  In this paper, we propose a channel tracking method for massive multi-input
and multi-output systems under both time-varying and spatial-varying
circumstance. Exploiting the characteristics of massive antenna array, a
spatial-temporal basis expansion model is designed to reduce the effective
dimensions of up-link and down-link channel, which decomposes channel state
information into the time-varying spatial information and gain information. We
firstly model the users movements as a one-order unknown Markov process, which
is blindly learned by the expectation and maximization (EM) approach. Then, the
up-link time varying spatial information can be blindly tracked by Taylor
series expansion of the steering vector, while the rest up-link channel gain
information can be trained by only a few pilot symbols. Due to angle
reciprocity (spatial reciprocity), the spatial information of the down-link
channel can be immediately obtained from the up-link counterpart, which greatly
reduces the complexity of down-link channel tracking. Various numerical results
are provided to demonstrate the effectiveness of the proposed method.
"
1353,"Technical Report about Tiramisu: a Three-Layered Abstraction for Hiding
  Hardware Complexity from DSL Compilers","  High-performance DSL developers work hard to take advantage of modern
hardware. The DSL compilers have to build their own complex middle-ends before
they can target a common back-end such as LLVM, which only handles single
instruction streams with SIMD instructions. We introduce Tiramisu, a common
middle-end that can generate efficient code for modern processors and
accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu
introduces a novel three-level IR that separates the algorithm, how that
algorithm is executed, and where intermediate data are stored. This separation
simplifies optimization and makes targeting multiple hardware architectures
from the same algorithm easier. As a result, DSL compilers can be made
considerably less complex with no loss of performance while immediately
targeting multiple hardware or hardware combinations such as distributed nodes
with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for
the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia
with many new capabilities including the ability to: express new algorithms
(such as recurrent filters and non-rectangular iteration spaces), perform new
complex loop nest transformations (such as wavefront parallelization, loop
shifting and loop fusion) and generate efficient code for more architectures
(such as combinations of distributed clusters, multicores, GPUs and FPGAs).
Finally, we demonstrate that Tiramisu can generate very efficient code that
matches the highly optimized Intel MKL gemm (generalized matrix multiplication)
implementation, we also show speedups reaching 4X in Halide and 16X in Julia
due to optimizations enabled by Tiramisu.
"
1354,"Online Scheduling of Spark Workloads with Mesos using Different Fair
  Allocation Algorithms","  In the following, we present example illustrative and experimental results
comparing fair schedulers allocating resources from multiple servers to
distributed application frameworks. Resources are allocated so that at least
one resource is exhausted in every server. Schedulers considered include DRF
(DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server
selection under Randomized Round Robin (RRR) and based on their residual
(unreserved) resources. In the following, we consider cases with frameworks of
equal priority and without server-preference constraints. We first give typical
results of a illustrative numerical study and then give typical results of a
study involving Spark workloads on Mesos which we have modified and
open-sourced to prototype different schedulers.
"
1355,"Design, Generation, and Validation of Extreme Scale Power-Law Graphs","  Massive power-law graphs drive many fields: metagenomics, brain mapping,
Internet-of-things, cybersecurity, and sparse machine learning. The development
of novel algorithms and systems to process these data requires the design,
generation, and validation of enormous graphs with exactly known properties.
Such graphs accelerate the proper testing of new algorithms and systems and are
a prerequisite for success on real applications. Many random graph generators
currently exist that require realizing a graph in order to know its exact
properties: number of vertices, number of edges, degree distribution, and
number of triangles. Designing graphs using these random graph generators is a
time-consuming trial-and-error process. This paper presents a novel approach
that uses Kronecker products to allow the exact computation of graph properties
prior to graph generation. In addition, when a real graph is desired, it can be
generated quickly in memory on a parallel computer with no-interprocessor
communication. To test this approach, graphs with $10^{12}$ edges are generated
on a 40,000+ core supercomputer in 1 second and exactly agree with those
predicted by the theory. In addition, to demonstrate the extensibility of this
approach, decetta-scale graphs with up to $10^{30}$ edges are simulated in a
few minutes on a laptop.
"
1356,"On the accuracy and usefulness of analytic energy models for
  contemporary multicore processors","  This paper presents refinements to the execution-cache-memory performance
model and a previously published power model for multicore processors. The
combination of both enables a very accurate prediction of performance and
energy consumption of contemporary multicore processors as a function of
relevant parameters such as number of active cores as well as core and Uncore
frequencies. Model validation is performed on the Sandy Bridge-EP and
Broadwell-EP microarchitectures. Production-related variations in chip quality
are demonstrated through a statistical analysis of the fit parameters obtained
on one hundred Broadwell-EP CPUs of the same model. Insights from the models
are used to explain the performance- and energy-related behavior of the
processors for scalable as well as saturating (i.e., memory-bound) codes. In
the process we demonstrate the models' capability to identify optimal operating
points with respect to highest performance, lowest energy-to-solution, and
lowest energy-delay product and identify a set of best practices for
energy-efficient execution.
"
1357,"Chebyshev Filter Diagonalization on Modern Manycore Processors and
  GPGPUs","  Chebyshev filter diagonalization is well established in quantum chemistry and
quantum physics to compute bulks of eigenvalues of large sparse matrices.
Choosing a block vector implementation, we investigate optimization
opportunities on the new class of high-performance compute devices featuring
both high-bandwidth and low-bandwidth memory. We focus on the transparent
access to the full address space supported by both architectures under
consideration: Intel Xeon Phi ""Knights Landing"" and Nvidia ""Pascal.""
  We propose two optimizations: (1) Subspace blocking is applied for improved
performance and data access efficiency. We also show that it allows
transparently handling problems much larger than the high-bandwidth memory
without significant performance penalties. (2) Pipelining of communication and
computation phases of successive subspaces is implemented to hide communication
costs without extra memory traffic.
  As an application scenario we use filter diagonalization studies on
topological insulator materials. Performance numbers on up to 512 nodes of the
OakForest-PACS and Piz Daint supercomputers are presented, achieving beyond 100
Tflop/s for computing 100 inner eigenvalues of sparse matrices of dimension one
billion.
"
1358,"Scaling Structured Multigrid to 500K+ Cores through Coarse-Grid
  Redistribution","  The efficient solution of sparse, linear systems resulting from the
discretization of partial differential equations is crucial to the performance
of many physics-based simulations. The algorithmic optimality of multilevel
approaches for common discretizations makes them a good candidate for an
efficient parallel solver. Yet, modern architectures for high-performance
computing systems continue to challenge the parallel scalability of multilevel
solvers. While algebraic multigrid methods are robust for solving a variety of
problems, the increasing importance of data locality and cost of data movement
in modern architectures motivates the need to carefully exploit structure in
the problem.
  Robust logically structured variational multigrid methods, such as Black Box
Multigrid (BoxMG), maintain structure throughout the multigrid hierarchy. This
avoids indirection and increased coarse-grid communication costs typical in
parallel algebraic multigrid. Nevertheless, the parallel scalability of
structured multigrid is challenged by coarse-grid problems where the overhead
in communication dominates computation. In this paper, an algorithm is
introduced for redistributing coarse-grid problems through incremental
agglomeration. Guided by a predictive performance model, this algorithm
provides robust redistribution decisions for structured multilevel solvers.
  A two-dimensional diffusion problem is used to demonstrate the significant
gain in performance of this algorithm over the previous approach that used
agglomeration to one processor. In addition, the parallel scalability of this
approach is demonstrated on two large-scale computing systems, with solves on
up to 500K+ cores.
"
1359,"TicTac: Accelerating Distributed Deep Learning with Communication
  Scheduling","  State-of-the-art deep learning systems rely on iterative distributed training
to tackle the increasing complexity of models and input data. The iteration
time in these communication-heavy systems depends on the computation time,
communication time and the extent of overlap of computation and communication.
  In this work, we identify a shortcoming in systems with graph representation
for computation, such as TensorFlow and PyTorch, that result in high variance
in iteration time --- random order of received parameters across workers. We
develop a system, TicTac, to improve the iteration time by fixing this issue in
distributed deep learning with Parameter Servers while guaranteeing
near-optimal overlap of communication and computation. TicTac identifies and
enforces an order of network transfers which improves the iteration time using
prioritization. Our system is implemented over TensorFlow and requires no
changes to the model or developer inputs. TicTac improves the throughput by up
to $37.7\%$ in inference and $19.2\%$ in training, while also reducing
straggler effect by up to $2.3\times$. Our code is publicly available.
"
1360,"Caching in the Clouds: Optimized Dynamic Cache Instantiation in Content
  Delivery Systems","  By caching content at geographically distributed servers, content delivery
applications can achieve scalability and reduce wide-area network traffic.
However, each deployed cache has an associated cost. When the request rate from
the local region is sufficiently high this cost will be justified, but as the
request rate varies, for example according to a daily cycle, there may be long
periods when the benefit of the cache does not justify the cost. Cloud
computing offers a solution to problems of this kind, by supporting the dynamic
allocation and release of resources according to need.
  In this paper, we analyze the potential benefits from dynamically
instantiating caches using resources from cloud service providers. We develop
novel analytic caching models that accommodate time-varying request rates,
transient behavior as a cache fills following instantiation, and selective
cache insertion policies. Using these models, within the context of a simple
cost model, we then develop bounds and compare policies with optimized
parameter selections to obtain insights into key cost/performance tradeoffs. We
find that dynamic cache instantiation has the potential to provide substantial
cost reductions in some cases, but that this potential is strongly dependent on
the object popularity skew. We also find that selective ""Cache on k-th request""
cache insertion policies can be even more beneficial in this context than with
conventional edge caches.
"
1361,"NVIDIA Tensor Core Programmability, Performance & Precision","  The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called
""Tensor Core"" that performs one matrix-multiply-and-accumulate on 4x4 matrices
per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta
microarchitecture, provides 640 Tensor Cores with a theoretical peak
performance of 125 Tflops/s in mixed precision. In this paper, we investigate
current approaches to program NVIDIA Tensor Cores, their performances and the
precision loss due to computation in mixed precision.
  Currently, NVIDIA provides three different ways of programming
matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply
Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS
GEMM. After experimenting with different approaches, we found that NVIDIA
Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100
GPU, seven and three times the performance in single and half precision
respectively. A WMMA implementation of batched GEMM reaches a performance of 4
Tflops/s. While precision loss due to matrix multiplication with half precision
input might be critical in many HPC applications, it can be considerably
reduced at the cost of increased computation. Our results indicate that HPC
applications using matrix multiplications can strongly benefit from using of
NVIDIA Tensor Cores.
"
1362,"DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression
  Framework","  As one of most fascinating machine learning techniques, deep neural network
(DNN) has demonstrated excellent performance in various intelligent tasks such
as image classification. DNN achieves such performance, to a large extent, by
performing expensive training over huge volumes of training data. To reduce the
data storage and transfer overhead in smart resource-limited Internet-of-Thing
(IoT) systems, effective data compression is a ""must-have"" feature before
transferring real-time produced dataset for training or classification. While
there have been many well-known image compression approaches (such as JPEG), we
for the first time find that a human-visual based image compression approach
such as JPEG compression is not an optimized solution for DNN systems,
especially with high compression ratios. To this end, we develop an image
compression framework tailored for DNN applications, named ""DeepN-JPEG"", to
embrace the nature of deep cascaded information process mechanism of DNN
architecture. Extensive experiments, based on ""ImageNet"" dataset with various
state-of-the-art DNNs, show that ""DeepN-JPEG"" can achieve ~3.5x higher
compression rate over the popular JPEG solution while maintaining the same
accuracy level for image recognition, demonstrating its great potential of
storage and power efficiency in DNN-based smart IoT system design.
"
1363,"Memory Slices: A Modular Building Block for Scalable, Intelligent Memory
  Systems","  While reduction in feature size makes computation cheaper in terms of
latency, area, and power consumption, performance of emerging data-intensive
applications is determined by data movement. These trends have introduced the
concept of scalability as reaching a desirable performance per unit cost by
using as few number of units as possible. Many proposals have moved compute
closer to the memory. However, these efforts ignored maintaining a balance
between bandwidth and compute rate of an architecture, with those of
applications, which is a key principle in designing scalable large systems.
This paper proposes the use of memory slices, a modular building block for
scalable memory systems integrated with compute, in which performance scales
with memory size (and volume of data). The slice architecture utilizes a
programmable memory interface feeding a systolic compute engine with high reuse
rate. The modularity feature of slice-based systems is exploited with a
partitioning and data mapping strategy across allocated memory slices where
training performance scales with the data size. These features enable shifting
the most pressure to cheap compute units rather than expensive memory accesses
or transfers via interconnection network. An application of the memory slices
to a scale-out memory system is accelerating the training of recurrent,
convolutional, and hybrid neural networks (RNNs and RNNs+CNN) that are forming
cloud workloads. The results of our cycle-level simulations show that memory
slices exhibits a superlinear speedup when the number of slices increases.
Furthermore, memory slices improve power efficiency to 747 GFLOPs/J for
training LSTMs. While our current evaluation uses memory slices with 3D
packaging, a major value is that slices can also be constructed with a variety
of packaging options, for example with DDR-based memory units.
"
1364,The ARM Scalable Vector Extension,"  This article describes the ARM Scalable Vector Extension (SVE). Several goals
guided the design of the architecture. First was the need to extend the vector
processing capability associated with the ARM AArch64 execution state to better
address the computational requirements in domains such as high-performance
computing, data analytics, computer vision, and machine learning. Second was
the desire to introduce an extension that can scale across multiple
implementations, both now and into the future, allowing CPU designers to choose
the vector length most suitable for their power, performance, and area targets.
Finally, the architecture should avoid imposing a software development cost as
the vector length changes and where possible reduce it by improving the reach
of compiler auto-vectorization technologies. SVE achieves these goals. It
allows implementations to choose a vector register length between 128 and 2,048
bits. It supports a vector-length agnostic programming model that lets code run
and scale automatically across all vector lengths without recompilation.
Finally, it introduces several innovative features that begin to overcome some
of the traditional barriers to autovectorization.
"
1365,"Join-Idle-Queue with Service Elasticity: Large-Scale Asymptotics of a
  Non-monotone System","  We consider the model of a token-based joint auto-scaling and load balancing
strategy, proposed in a recent paper by Mukherjee, Dhara, Borst, and van
Leeuwaarden (SIGMETRICS '17, arXiv:1703.08373), which offers an efficient
scalable implementation and yet achieves asymptotically optimal steady-state
delay performance and energy consumption as the number of servers $N\to\infty$.
In the above work, the asymptotic results are obtained under the assumption
that the queues have fixed-size finite buffers, and therefore the fundamental
question of stability of the proposed scheme with infinite buffers was left
open. In this paper, we address this fundamental stability question. The system
stability under the usual subcritical load assumption is not automatic.
Moreover, the stability may not even hold for all $N$. The key challenge stems
from the fact that the process lacks monotonicity, which has been the powerful
primary tool for establishing stability in load balancing models. We develop a
novel method to prove that the subcritically loaded system is stable for large
enough $N$, and establish convergence of steady-state distributions to the
optimal one, as $N \to \infty$. The method goes beyond the state of the art
techniques -- it uses an induction-based idea and a ""weak monotonicity""
property of the model; this technique is of independent interest and may have
broader applicability.
"
1366,"A Markov Chain Monte Carlo Approach to Cost Matrix Generation for
  Scheduling Performance Evaluation","  In high performance computing, scheduling of tasks and allocation to machines
is very critical especially when we are dealing with heterogeneous execution
costs. Simulations can be performed with a large variety of environments and
application models. However, this technique is sensitive to bias when it relies
on random instances with an uncontrolled distribution. We use methods from the
literature to provide formal guarantee on the distribution of the instance. In
particular, it is desirable to ensure a uniform distribution among the
instances with a given task and machine heterogeneity. In this article, we
propose a method that generates instances (cost matrices) with a known
distribution where tasks are scheduled on machines with heterogeneous execution
costs.
"
1367,Unix Memory Allocations are Not Poisson,"  In multitasking operating systems, requests for free memory are traditionally
modeled as a stochastic counting process with independent,
exponentially-distributed interarrival times because of the analytic simplicity
such Poisson models afford. We analyze the distribution of several million unix
page commits to show that although this approach could be valid over relatively
long timespans, the behavior of the arrival process over shorter periods is
decidedly not Poisson. We find that this result holds regardless of the
originator of the request: unlike network packets, there is little difference
between system- and user-level page-request distributions. We believe this to
be due to the bursty nature of page allocations, which tend to occur in either
small or extremely large increments. Burstiness and persistent variance have
recently been found in self-similar processes in computer networks, but we show
that although page commits are both bursty and possess high variance over long
timescales, they are probably not self-similar. These results suggest that
altogether different models are needed for fine-grained analysis of memory
systems, an important consideration not only for understanding behavior but
also for the design of online control systems.
"
1368,"Crossing the Architectural Barrier: Evaluating Representative Regions of
  Parallel HPC Applications","  Exascale computing will get mankind closer to solving important social,
scientific and engineering problems. Due to high prototyping costs, High
Performance Computing (HPC) system architects make use of simulation models for
design space exploration and hardware-software co-design. However, as HPC
systems reach exascale proportions, the cost of simulation increases, since
simulators themselves are largely single-threaded. Tools for selecting
representative parts of parallel applications to reduce running costs are
widespread, e.g., BarrierPoint achieves this by analysing, in simulation,
abstract characteristics such as basic blocks and reuse distances. However,
architectures new to HPC have a limited set of tools available.
  In this work, we provide an independent cross-architectural evaluation on
real hardware - across Intel and ARM - of the BarrierPoint methodology, when
applied to parallel HPC proxy applications. We present both cases: when the
methodology can be applied and when it cannot. In the former case, results show
that we can predict the performance of full application execution by running
shorter representative sections. In the latter case, we dive into the
underlying issues and suggest improvements. We demonstrate a total simulation
time reduction of up to 178x, whilst keeping the error below 2.3% for both
cycles and instructions.
"
1369,Accelerating Empowerment Computation with UCT Tree Search,"  Models of intrinsic motivation present an important means to produce sensible
behaviour in the absence of extrinsic rewards. Applications in video games are
varied, and range from intrinsically motivated general game-playing agents to
non-player characters such as companions and enemies. The information-theoretic
quantity of Empowerment is a particularly promising candidate motivation to
produce believable, generic and robust behaviour. However, while it can be used
in the absence of external reward functions that would need to be crafted and
learned, empowerment is computationally expensive. In this paper, we propose a
modified UCT tree search method to mitigate empowerment's computational
complexity in discrete and deterministic scenarios. We demonstrate how to
modify a Monte-Carlo Search Tree with UCT to realise empowerment maximisation,
and discuss three additional modifications that facilitate better sampling. We
evaluate the approach both quantitatively, by analysing how close our approach
gets to the baseline of exhaustive empowerment computation with varying amounts
of computational resources, and qualitatively, by analysing the resulting
behaviour in a Minecraft-like scenario.
"
1370,"Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave
  Scattering","  Algorithmic and architecture-oriented optimizations are essential for
achieving performance worthy of anticipated energy-austere exascale systems. In
this paper, we present an extreme scale FMM-accelerated boundary integral
equation solver for wave scattering, which uses FMM as a matrix-vector
multiplication inside the GMRES iterative method. Our FMM Helmholtz kernels
treat nontrivial singular and near-field integration points. We implement
highly optimized kernels for both shared and distributed memory, targeting
emerging Intel extreme performance HPC architectures. We extract the potential
thread- and data-level parallelism of the key Helmholtz kernels of FMM. Our
application code is well optimized to exploit the AVX-512 SIMD units of Intel
Skylake and Knights Landing architectures. We provide different performance
models for tuning the task-based tree traversal implementation of FMM, and
develop optimal architecture-specific and algorithm aware partitioning, load
balancing, and communication reducing mechanisms to scale up to 6,144 compute
nodes of a Cray XC40 with 196,608 hardware cores. With shared memory
optimizations, we achieve roughly 77% of peak single precision floating point
performance of a 56-core Skylake processor, and on average 60% of peak single
precision floating point performance of a 72-core KNL. These numbers represent
nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the
baseline scalar code. With distributed memory optimizations, on the other hand,
we report near-optimal efficiency in the weak scalability study with respect to
both the logarithmic communication complexity as well as the theoretical
scaling complexity of FMM. In addition, we exhibit up to 85% efficiency in
strong scaling. We compute in excess of 2 billion DoF on the full-scale of the
Cray XC40 supercomputer.
"
1371,"Effect of payload size on mean response time when message segmentations
  occur using $\rm{M}^{\rm X}/\rm{G}/1$ queueing model","  This paper proposes the $\rm{M}^{\rm X}/\rm{G}/1$ queueing model to represent
arrivals of segmented packets when message segmentations occur. This queueing
model enables us to derive the closed form of mean response time, given payload
size, message size distribution and message arrival rate. From a numerical
result, we show that the mean response time is more convex in payload sizes if
message arrival rate is larger in a scenario where Web objects are delivered
over a physical link.
"
1372,"Dispatching to Parallel Servers: Solutions of Poisson's Equation for
  First-Policy Improvement","  Policy iteration techniques for multiple-server dispatching rely on the
computation of value functions. In this context, we consider the
continuous-space M/G/1-FCFS queue endowed with an arbitrarily-designed cost
function for the waiting times of the incoming jobs. The associated value
function is a solution of Poisson's equation for Markov chains, which in this
work we solve in the Laplace transform domain by considering an ancillary,
underlying stochastic process extended to (imaginary) negative backlog states.
This construction enables us to issue closed-form value functions for
polynomial and exponential cost functions and for piecewise compositions of the
latter, in turn permitting the derivation of interval bounds for the value
function in the form of power series or trigonometric sums. We review various
cost approximation schemes and assess the convergence of the interval bounds
these induce on the value function. Namely: Taylor expansions (divergent,
except for a narrow class of entire functions with low orders of growth), and
uniform approximation schemes (polynomials, trigonometric), which achieve
optimal convergence rates over finite intervals. This study addresses all the
steps to implementing dispatching policies for systems of parallel servers,
from the specification of general cost functions towards the computation of
interval bounds for the value functions and the exact implementation of the
first-policy improvement step.
"
1373,"Fine-Grained Energy and Performance Profiling framework for Deep
  Convolutional Neural Networks","  There is a huge demand for on-device execution of deep learning algorithms on
mobile and embedded platforms. These devices present constraints on the
application due to limited resources and power. Hence, developing
energy-efficient solutions to address this issue will require innovation in
algorithmic design, software and hardware. Such innovation requires
benchmarking and characterization of Deep Neural Networks based on performance
and energy-consumption alongside accuracy. However, current benchmarks studies
in existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and
others) are based on performance of these applications on high-end CPUs and
GPUs. In this work, we introduce a benchmarking framework called ""SyNERGY"" to
measure the energy and time of 11 representative Deep Convolutional Neural
Networks on embedded platforms such as NVidia Jetson TX1. We integrate ARM's
Streamline Performance Analyser with standard deep learning frameworks such as
Caffe and CuDNNv5, to study the execution behaviour of current deep learning
models at a fine-grained level (or specific layers) on image processing tasks.
In addition, we build an initial multi-variable linear regression model to
predict energy consumption of unseen neural network models based on the number
of SIMD instructions executed and main memory accesses of the CPU cores of the
TX1 with an average relative test error rate of 8.04 +/- 5.96 %. Surprisingly,
we find that it is possible to refine the model to predict the number of SIMD
instructions and main memory accesses solely from the application's
Multiply-Accumulate (MAC) counts, hence, eliminating the need for actual
measurements. Our predicted results demonstrate 7.08 +/- 6.0 % average relative
error over actual energy measurements of all 11 networks tested, except
MobileNet. By including MobileNet the average relative test error increases to
17.33 +/- 12.2 %.
"
1374,Proof-of-Concept Examples of Performance-Transparent Programming Models,"  Machine-specific optimizations command the machine to behave in a specific
way. As current programming models largely leave machine details unexposed,
they cannot accommodate direct encoding of such commands. In previous work we
have proposed the design of performance-transparent programming models to
facilitate this use-case; this report contains proof-of-concept examples of
such programming models. We demonstrate how programming model abstractions may
reveal the memory footprint, vector unit utilization and data reuse of an
application, with prediction accuracy ranging from 0 to 25 \%.
"
1375,"ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for
  Architectural Exploration","  Design of next generation computer systems should be supported by simulation
infrastructure that must achieve a few contradictory goals such as fast
execution time, high accuracy, and enough flexibility to allow comparison
between large numbers of possible design points. Most existing architecture
level simulators are designed to be flexible and to execute the code in
parallel for greater efficiency, but at the cost of scarified accuracy. This
paper presents the ScaleSimulator simulation environment, which is based on a
new design methodology whose goal is to achieve near cycle accuracy while still
being flexible enough to simulate many different future system architectures
and efficient enough to run meaningful workloads. We achieve these goals by
making the parallelism a first-class citizen in our methodology. Thus, this
paper focuses mainly on the ScaleSimulator design points that enable better
parallel execution while maintaining the scalability and cycle accuracy of a
simulated architecture. The paper indicates that the new proposed
ScaleSimulator tool can (1) efficiently parallelize the execution of a
cycle-accurate architecture simulator, (2) efficiently simulate complex
architectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and
network) and massive parallel systems, and (3) use meaningful workloads, such
as full simulation of OLTP benchmarks, to examine future architectural choices.
"
1376,Decoding Superposed LoRa Signals,"  Long-range low-power wireless communications, such as LoRa, are used in many
IoT and environmental monitoring applications. They typically increase the
communication range to several kilometers, at the cost of reducing the bitrate
to a few bits per seconds. Collisions further reduce the performance of these
communications. In this paper, we propose two algorithms to decode colliding
signals: one algorithm requires the transmitters to be slightly desynchronized,
and the other requires the transmitters to be synchronized. To do so, we use
the timing information to match the correct symbols to the correct
transmitters. We show that our algorithms are able to significantly improve the
overall throughput of LoRa.
"
1377,Pigeonring: A Principle for Faster Thresholded Similarity Search,"  The pigeonhole principle states that if $n$ items are contained in $m$ boxes,
then at least one box has no more than $n / m$ items. It is utilized to solve
many data management problems, especially for thresholded similarity searches.
Despite many pigeonhole principle-based solutions proposed in the last few
decades, the condition stated by the principle is weak. It only constrains the
number of items in a single box. By organizing the boxes in a ring, we propose
a new principle, called the pigeonring principle, which constrains the number
of items in multiple boxes and yields stronger conditions. To utilize the new
principle, we focus on problems defined in the form of identifying data objects
whose similarities or distances to the query is constrained by a threshold.
Many solutions to these problems utilize the pigeonhole principle to find
candidates that satisfy a filtering condition. By the new principle, stronger
filtering conditions can be established. We show that the pigeonhole principle
is a special case of the new principle. This suggests that all the pigeonhole
principle-based solutions are possible to be accelerated by the new principle.
A universal filtering framework is introduced to encompass the solutions to
these problems based on the new principle. Besides, we discuss how to quickly
find candidates specified by the new principle. The implementation requires
only minor modifications on top of existing pigeonhole principle-based
algorithms. Experimental results on real datasets demonstrate the applicability
of the new principle as well as the superior performance of the algorithms
based on the new principle.
"
1378,Dynamic Load Balancing with Tokens,"  Efficiently exploiting the resources of data centers is a complex task that
requires efficient and reliable load balancing and resource allocation
algorithms. The former are in charge of assigning jobs to servers upon their
arrival in the system, while the latter are responsible for sharing server
resources between their assigned jobs. These algorithms should take account of
various constraints, such as data locality, that restrict the feasible job
assignments. In this paper, we propose a token-based mechanism that efficiently
balances load between servers without requiring any knowledge on job arrival
rates and server capacities. Assuming a balanced fair sharing of the server
resources, we show that the resulting dynamic load balancing is insensitive to
the job size distribution. Its performance is compared to that obtained under
the best static load balancing and in an ideal system that would constantly
optimize the resource utilization.
"
1379,A Survey of Miss-Ratio Curve Construction Techniques,"  Miss-ratio curve (MRC), or equivalently hit-ratio curve (HRC), construction
techniques have recently gathered the attention of many researchers. Recent
advancements have allowed for approximating these curves in constant time,
allowing for online working-set-size (WSS) measurement. Techniques span the
algorithmic design paradigm from classic dynamic programming to artificial
intelligence inspired techniques. Our survey produces broad classification of
the current techniques primarily based on \emph{what} locality metric is being
recorded and \emph{how} that metric is stored for processing.
  Applications of theses curves span from dynamic cache partitioning in the
processor, to improving block allocation at the operating system level. Our
survey will give an overview of the historical, exact MRC construction methods,
and compare them with the state-of-the-art methods present in today's
literature. In addition, we will show where there are still open areas of
research and remain excited to see what this domain can produce with a strong
theoretical background.
"
1380,"A Comparative Study of Full-Duplex Relaying Schemes for Low Latency
  Applications","  Various sectors are likely to carry a set of emerging applications while
targeting a reliable communication with low latency transmission. To address
this issue, upon a spectrally-efficient transmission, this paper investigates
the performance of a one full-dulpex (FD) relay system, and considers for that
purpose, two basic relaying schemes, namely the symbol-by-symbol transmission,
i.e., amplify-and-forward (AF) and the block-by-block transmission, i.e.,
selective decode-and-forward (SDF). The conducted analysis presents an
exhaustive comparison, covering both schemes, over two different transmission
modes, i.e., the non combining mode where the best link, direct or relay link
is decoded and the signals combining mode, where direct and relay links are
combined at the receiver side. While targeting latency purpose as a necessity,
simulations show a refined results of performed comparisons, and reveal that AF
relaying scheme is more adapted to combining mode, whereas the SDF relaying
scheme is more suitable for non combining mode.
"
1381,Restructuring expression dags for efficient parallelization,"  In the field of robust geometric computation it is often necessary to make
exact decisions based on inexact floating-point arithmetic. One common approach
is to store the computation history in an arithmetic expression dag and to
re-evaluate the expression with increasing precision until an exact decision
can be made. We show that exact-decisions number types based on expression dags
can be evaluated faster in practice through parallelization on multiple cores.
We compare the impact of several restructuring methods for the expression dag
on its running time in a parallel environment.
"
1382,Symbol Detection of Ambient Backscatter Systems with Manchester Coding,"  Ambient backscatter communication is a newly emerged paradigm, which utilizes
the ambient radio frequency (RF) signal as the carrier to reduce the system
battery requirement, and is regarded as a promising solution for enabling large
scale deployment of future Internet of Things (IoT) networks. The key issue of
ambient backscatter communication systems is how to perform reliable detection.
In this paper, we propose novel encoding methods at the information tag, and
devise the corresponding symbol detection methods at the reader. In particular,
Manchester coding and differential Manchester coding are adopted at the
information tag, and the corresponding semi-coherent Manchester (SeCoMC) and
non-coherent Manchester (NoCoMC) detectors are developed. In addition,
analytical bit error rate (BER) expressions are characterized for both
detectors assuming either complex Gaussian or unknown deterministic ambient
signal. Simulation results show that the BER performance of unknown
deterministic ambient signal is better, and the SeCoMC detector outperforms the
NoCoMC detector. Finally, compared with the prior detectors for ambient
backscatter communications, the proposed detectors have the advantages of
achieving superior BER performance with lower communication delay.
"
1383,"A Performance and Resource Consumption Assessment of Secure Multiparty
  Computation","  In recent years, secure multiparty computation (SMC) advanced from a
theoretical technique to a practically applicable technology. Several
frameworks were proposed of which some are still actively developed.
  We perform a first comprehensive study of performance characteristics of SMC
protocols using a promising implementation based on secret sharing, a common
and state-of-the-art foundation. Therefor, we analyze its scalability with
respect to environmental parameters as the number of peers, network properties
-- namely transmission rate, packet loss, network latency -- and
parallelization of computations as parameters and execution time, CPU cycles,
memory consumption and amount of transmitted data as variables.
  Our insights on the resource consumption show that such a solution is
practically applicable in intranet environments and -- with limitations -- in
Internet settings.
"
1384,"Big enterprise registration data imputation: Supporting spatiotemporal
  analysis of industries in China","  Big, fine-grained enterprise registration data that includes time and
location information enables us to quantitatively analyze, visualize, and
understand the patterns of industries at multiple scales across time and space.
However, data quality issues like incompleteness and ambiguity, hinder such
analysis and application. These issues become more challenging when the volume
of data is immense and constantly growing. High Performance Computing (HPC)
frameworks can tackle big data computational issues, but few studies have
systematically investigated imputation methods for enterprise registration data
in this type of computing environment. In this paper, we propose a big data
imputation workflow based on Apache Spark as well as a bare-metal computing
cluster, to impute enterprise registration data. We integrated external data
sources, employed Natural Language Processing (NLP), and compared several
machine-learning methods to address incompleteness and ambiguity problems found
in enterprise registration data. Experimental results illustrate the
feasibility, efficiency, and scalability of the proposed HPC-based imputation
framework, which also provides a reference for other big georeferenced text
data processing. Using these imputation results, we visualize and briefly
discuss the spatiotemporal distribution of industries in China, demonstrating
the potential applications of such data when quality issues are resolved.
"
1385,"Some parametrized dynamic priority policies for 2-class M/G/1 queues:
  completeness and applications","  Completeness of a dynamic priority scheduling scheme is of fundamental
importance for the optimal control of queues in areas as diverse as computer
communications, communication networks, supply chains and manufacturing
systems. Our first main contribution is to identify the mean waiting time
completeness as a unifying aspect for four different dynamic priority
scheduling schemes by proving their completeness and equivalence in 2-class
M/G/1 queue. These dynamic priority schemes are earliest due date based, head
of line priority jump, relative priority, and probabilistic priority.
  In our second main contribution, we characterize the optimal scheduling
policies for the case studies in different domains by exploiting the
completeness of above dynamic priority schemes. The major theme of second main
contribution is resource allocation/optimal control in revenue management
problems for contemporary systems such as cloud computing, high-performance
computing, etc., where congestion is inherent. Using completeness and
theoretically tractable nature of relative priority policy, we study the impact
of approximation in a fairly generic data network utility framework. We
introduce the notion of min-max fairness in multi-class queues and show that a
simple global FCFS policy is min-max fair. Next, we re-derive the celebrated
$c/\rho$ rule for 2-class M/G/1 queues by an elegant argument and also simplify
a complex joint pricing and scheduling problem for a wider class of scheduling
policies.
"
1386,"A Comparative Evaluation of Log-Based Process Performance Analysis
  Techniques","  Process mining has gained traction over the past decade and an impressive
body of research has resulted in the introduction of a variety of process
mining approaches measuring process performance. Having this set of techniques
available, organizations might find it difficult to identify which approach is
best suited considering context, performance indicator, and data availability.
In light of this challenge, this paper aims at introducing a framework for
categorizing and selecting performance analysis approaches based on existing
research. We start from a systematic literature review for identifying the
existing works discussing how to measure process performance based on
information retrieved from event logs. Then, the proposed framework is built
starting from the information retrieved from these studies taking into
consideration different aspects of performance analysis.
"
1387,EMPIOT: An Energy Measurement Platform for Wireless IoT Devices,"  Profiling and minimizing the energy consumption of resource-constrained
devices is an essential step towards employing IoT in various application
domains. Due to the large size and high cost of commercial energy measurement
platforms, alternative solutions have been proposed by the research community.
However, the three main shortcomings of existing tools are complexity, limited
measurement range, and low accuracy. Specifically, these tools are not suitable
for the energy measurement of new IoT devices such as those supporting the
802.11 technology. In this paper we propose EMPIOT, an accurate, low-cost, easy
to build, and flexible power measurement platform. We present the hardware and
software components of this platform and study the effect of various design
parameters on accuracy and overhead. In particular, we analyze the effects of
driver, bus speed, input voltage, and buffering mechanism on sampling rate,
measurement accuracy and processing demand. These extensive experimental
studies enable us to configure the system in order to achieve its highest
performance. We also propose a novel calibration technique and report the
calibration parameters under various settings. Using five different IoT devices
performing four types of workloads, we evaluate the performance of EMPIOT
against the ground truth obtained from a high-accuracy industrial-grade power
measurement tool. Our results show that, for very low-power devices that
utilize 802.15.4 wireless standard, the measurement error is less than 3.5%. In
addition, for 802.11-based devices that generate short and high power spikes,
the error is less than 2.5%.
"
1388,"Pliant: Leveraging Approximation to Improve Datacenter Resource
  Efficiency","  Cloud multi-tenancy is typically constrained to a single interactive service
colocated with one or more batch, low-priority services, whose performance can
be sacrificed when deemed necessary. Approximate computing applications offer
the opportunity to enable tighter colocation among multiple applications whose
performance is important. We present Pliant, a lightweight cloud runtime that
leverages the ability of approximate computing applications to tolerate some
loss in their output quality to boost the utilization of shared servers. During
periods of high resource contention, Pliant employs incremental and
interference-aware approximation to reduce contention in shared resources, and
prevent QoS violations for co-scheduled interactive, latency-critical services.
We evaluate Pliant across different interactive and approximate computing
applications, and show that it preserves QoS for all co-scheduled workloads,
while incurring a 2.1\% loss in output quality, on average.
"
1389,"Deep Learning on Operational Facility Data Related to Large-Scale
  Distributed Area Scientific Workflows","  Distributed computing platforms provide a robust mechanism to perform
large-scale computations by splitting the task and data among multiple
locations, possibly located thousands of miles apart geographically. Although
such distribution of resources can lead to benefits, it also comes with its
associated problems such as rampant duplication of file transfers increasing
congestion, long job completion times, unexpected site crashing, suboptimal
data transfer rates, unpredictable reliability in a time range, and suboptimal
usage of storage elements. In addition, each sub-system becomes a potential
failure node that can trigger system wide disruptions. In this vision paper, we
outline our approach to leveraging Deep Learning algorithms to discover
solutions to unique problems that arise in a system with computational
infrastructure that is spread over a wide area. The presented vision, motivated
by a real scientific use case from Belle II experiments, is to develop
multilayer neural networks to tackle forecasting, anomaly detection and
optimization challenges in a complex and distributed data movement environment.
Through this vision based on Deep Learning principles, we aim to achieve
reduced congestion events, faster file transfer rates, and enhanced site
reliability.
"
1390,"A General Formula for the Stationary Distribution of the Age of
  Information and Its Application to Single-Server Queues","  This paper considers the stationary distribution of the age of information
(AoI) in information update systems. We first derive a general formula for the
stationary distribution of the AoI, which holds for a wide class of information
update systems. The formula indicates that the stationary distribution of the
AoI is given in terms of the stationary distributions of the system delay and
the peak AoI. To demonstrate its applicability and usefulness, we analyze the
AoI in single-server queues with four different service disciplines: first-come
first-served (FCFS), preemptive last-come first-served (LCFS), and two variants
of non-preemptive LCFS service disciplines. For the FCFS and the preemptive
LCFS service disciplines, the GI/GI/1, M/GI/1, and GI/M/1 queues are
considered, and for the non-preemptive LCFS service disciplines, the M/GI/1 and
GI/M/1 queues are considered. With these results, we further show comparison
results for the mean AoI's in the M/GI/1 and GI/M/1 queues under those service
disciplines.
"
1391,Simplex Queues for Hot-Data Download,"  In cloud storage systems, hot data is usually replicated over multiple nodes
in order to accommodate simultaneous access by multiple users as well as
increase the fault tolerance of the system. Recent cloud storage research has
proposed using availability codes, which is a special class of erasure codes,
as a more storage efficient way to store hot data. These codes enable data
recovery from multiple, small disjoint groups of servers. The number of the
recovery groups is referred to as the availability and the size of each group
as the locality of the code. Until now, we have very limited knowledge on how
code locality and availability affect data access time. Data download from
these systems involves multiple fork-join queues operating in-parallel, making
the analysis of access time a very challenging problem. In this paper, we
present an approximate analysis of data access time in storage systems that
employ simplex codes, which are an important and in certain sense optimal class
of availability codes. We consider and compare three strategies in assigning
download requests to servers; first one aggressively exploits the storage
availability for faster download, second one implements only load balancing,
and the last one employs storage availability only for hot data download
without incurring any negative impact on the cold data download.
"
1392,"Software-defined Radios: Architecture, State-of-the-art, and Challenges","  Software-defined Radio (SDR) is a programmable transceiver with the
capability of operating various wireless communication protocols without the
need to change or update the hardware. Progress in the SDR field has led to the
escalation of protocol development and a wide spectrum of applications, with
more emphasis on programmability, flexibility, portability, and energy
efficiency, in cellular, WiFi, and M2M communication. Consequently, SDR has
earned a lot of attention and is of great significance to both academia and
industry. SDR designers intend to simplify the realization of communication
protocols while enabling researchers to experiment with prototypes on deployed
networks. This paper is a survey of the state-of-the-art SDR platforms in the
context of wireless communication protocols. We offer an overview of SDR
architecture and its basic components, then discuss the significant design
trends and development tools. In addition, we highlight key contrasts between
SDR architectures with regards to energy, computing power, and area, based on a
set of metrics. We also review existing SDR platforms and present an analytical
comparison as a guide to developers. Finally, we recognize a few of the related
research topics and summarize potential solutions.
"
1393,Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking,"  Every year, novel NVIDIA GPU designs are introduced. This rapid architectural
and technological progression, coupled with a reluctance by manufacturers to
disclose low-level details, makes it difficult for even the most proficient GPU
software designers to remain up-to-date with the technological advances at a
microarchitectural level. To address this dearth of public,
microarchitectural-level information on the novel NVIDIA GPUs, independent
researchers have resorted to microbenchmarks-based dissection and discovery.
This has led to a prolific line of publications that shed light on instruction
encoding, and memory hierarchy's geometry and features at each level. Namely,
research that describes the performance and behavior of the Kepler, Maxwell and
Pascal architectures. In this technical report, we continue this line of
research by presenting the microarchitectural details of the NVIDIA Volta
architecture, discovered through microbenchmarks and instruction set
disassembly. Additionally, we compare quantitatively our Volta findings against
its predecessors, Kepler, Maxwell and Pascal.
"
1394,On the cluster admission problem for cloud computing,"  Cloud computing providers face the problem of matching heterogeneous customer
workloads to resources that will serve them. This is particularly challenging
if customers, who are already running a job on a cluster, scale their resource
usage up and down over time. The provider therefore has to continuously decide
whether she can add additional workloads to a given cluster or if doing so
would impact existing workloads' ability to scale. Currently, this is often
done using simple threshold policies to reserve large parts of each cluster,
which leads to low efficiency (i.e., low average utilization of the cluster).
We propose more sophisticated policies for controlling admission to a cluster
and demonstrate that they significantly increase cluster utilization. We first
introduce the cluster admission problem and formalize it as a constrained
Partially Observable Markov Decision Process (POMDP). As it is infeasible to
solve the POMDP optimally, we then systematically design admission policies
that estimate moments of each workload's distribution of future resource usage.
Via extensive simulations grounded in a trace from Microsoft Azure, we show
that our admission policies lead to a substantial improvement over the simple
threshold policy. We then show that substantial further gains are possible if
high-quality information is available about arriving workloads. Based on this,
we propose an information elicitation approach to incentivize users to provide
this information and simulate its effects.
"
1395,A Fluid-Flow Interpretation of SCED Scheduling,"  We show that a fluid-flow interpretation of Service Curve Earliest Deadline
First (SCED) scheduling simplifies deadline derivations for this scheduler. By
exploiting the recently reported isomorphism between min-plus and max-plus
network calculus, and expressing deadlines in a max-plus algebra, deadline
computations no longer require pseudo-inverse computations. SCED deadlines are
provided for general convex or concave piecewise linear service curves.
"
1396,"Cellular Connectivity for UAVs: Network Modeling, Performance Analysis
  and Design Guidelines","  The growing use of aerial user equipments (UEs) in various applications
requires ubiquitous and reliable connectivity for safe control and data
exchange between these devices and ground stations. Key questions that need to
be addressed when planning the deployment of aerial UEs are whether the
cellular network is a suitable candidate for enabling such connectivity, and
how the inclusion of aerial UEs might impact the overall network efficiency.
This paper provides an in-depth analysis of user and network level performance
of a cellular network that serves both unmanned aerial vehicles (UAVs) and
ground users in the downlink. Our results show that the favorable propagation
conditions that UAVs enjoy due to their height often backfire on them, as the
increased co-channel interference received from neighboring ground BSs is not
compensated by the improved signal strength. When compared with a ground user
in an urban area, our analysis shows that a UAV flying at 100 meters can
experience a throughput decrease of a factor 10 and a coverage drop from 76% to
30%. Motivated by these findings, we develop UAV and network based solutions to
enable an adequate integration of UAVs into cellular networks. In particular,
we show that an optimal tilting of the UAV antenna can increase their coverage
and throughput from 23% to 89% and from 3.5 b/s/Hz to 5.8 b/s/Hz, respectively,
outperforming ground UEs. Furthermore, our findings reveal that depending on
UAV altitude, the aerial user performance can scale with respect to the network
density better than that of a ground user. Finally, our results show that
network densification and the use of micro cells limit UAV performance. While
UAV usage has the potential to increase area spectral efficiency (ASE) of
cellular networks with moderate number of cells, they might hamper the
development of future ultra dense networks.
"
1397,"BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First
  Parallelism","  Neural network frameworks such as PyTorch and TensorFlow are the workhorses
of numerous machine learning applications ranging from object recognition to
machine translation. While these frameworks are versatile and straightforward
to use, the training of and inference in deep neural networks is resource
(energy, compute, and memory) intensive. In contrast to recent works focusing
on algorithmic enhancements, we introduce BrainSlug, a framework that
transparently accelerates neural network workloads by changing the default
layer-by-layer processing to a depth-first approach, reducing the amount of
data required by the computations and thus improving the performance of the
available hardware caches. BrainSlug achieves performance improvements of up to
41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the
user as they do not require hardware changes and only need tiny adjustments to
the software.
"
1398,Robust Safety for Autonomous Vehicles through Reconfigurable Networking,"  Autonomous vehicles bring the promise of enhancing the consumer experience in
terms of comfort and convenience and, in particular, the safety of the
autonomous vehicle. Safety functions in autonomous vehicles such as Automatic
Emergency Braking and Lane Centering Assist rely on computation, information
sharing, and the timely actuation of the safety functions. One opportunity to
achieve robust autonomous vehicle safety is by enhancing the robustness of
in-vehicle networking architectures that support built-in resiliency
mechanisms. Software Defined Networking (SDN) is an advanced networking
paradigm that allows fine-grained manipulation of routing tables and routing
engines and the implementation of complex features such as failover, which is a
mechanism of protecting in-vehicle networks from failure, and in which a
standby link automatically takes over once the main link fails. In this paper,
we leverage SDN network programmability features to enable resiliency in the
autonomous vehicle realm. We demonstrate that a Software Defined In-Vehicle
Networking (SDIVN) does not add overhead compared to Legacy In-Vehicle Networks
(LIVNs) under non-failure conditions and we highlight its superiority in the
case of a link failure and its timely delivery of messages. We verify the
proposed architectures benefits using a simulation environment that we have
developed and we validate our design choices through testing and simulations
"
1399,On Access Control in Cabin-Based Transport Systems,"  We analyze a boarding solution for a transport system in which the number of
passengers allowed to enter a transport cabin is automatically controlled.
Expressions charac- terizing the stochastic properties of the passenger queue
length, waiting time, and cabin capacity are derived using queuing theory for a
transport line with deterministic arrivals of cabins and Poisson arrivals of
passengers. Expected cabin capacity and stability threshold for each station
are derived for a general passenger arrival distribution. Results show that a
significant reduction of the waiting time at a given station is only possible
at the cost of making the stability of one of the preceding stations worse than
that of the given station. Experimental studies with real passenger arrivals
are needed to draw firm conclusions.
"
1400,"Intermediate Data Caching Optimization for Multi-Stage and Parallel Big
  Data Frameworks","  In the era of big data and cloud computing, large amounts of data are
generated from user applications and need to be processed in the datacenter.
Data-parallel computing frameworks, such as Apache Spark, are widely used to
perform such data processing at scale. Specifically, Spark leverages
distributed memory to cache the intermediate results, represented as Resilient
Distributed Datasets (RDDs). This gives Spark an advantage over other parallel
frameworks for implementations of iterative machine learning and data mining
algorithms, by avoiding repeated computation or hard disk accesses to retrieve
RDDs. By default, caching decisions are left at the programmer's discretion,
and the LRU policy is used for evicting RDDs when the cache is full. However,
when the objective is to minimize total work, LRU is woefully inadequate,
leading to arbitrarily suboptimal caching decisions. In this paper, we design
an algorithm for multi-stage big data processing platforms to adaptively
determine and cache the most valuable intermediate datasets that can be reused
in the future. Our solution automates the decision of which RDDs to cache: this
amounts to identifying nodes in a direct acyclic graph (DAG) representing
computations whose outputs should persist in the memory. Our experiment results
show that our proposed cache optimization solution can improve the performance
of machine learning applications on Spark decreasing the total work to
recompute RDDs by 12%.
"
1401,"Queuing Theoretic Models for Multicast and Coded-Caching in Downlink
  Wireless Systems","  We consider a server connected to $L$ users over a shared finite capacity
link. Each user is equipped with a cache. File requests at the users are
generated as independent Poisson processes according to a popularity profile
from a library of $M$ files. The server has access to all the files in the
library. Users can store parts of the files or full files from the library in
their local caches. The server should send missing parts of the files requested
by the users. The server attempts to fulfill the pending requests with minimal
transmissions exploiting multicasting and coding opportunities among the
pending requests. We study the performance of this system in terms of queuing
delays for the naive multicasting and several coded multicasting schemes
proposed in the literature. We also provide approximate expressions for the
mean queuing delay for these models and establish their effectiveness with
simulations.
"
1402,Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code,"  This paper introduces Tiramisu, a polyhedral framework designed to generate
high performance code for multiple platforms including multicores, GPUs, and
distributed machines. Tiramisu introduces a scheduling language with novel
extensions to explicitly manage the complexities that arise when targeting
these systems. The framework is designed for the areas of image processing,
stencils, linear algebra and deep learning. Tiramisu has two main features: it
relies on a flexible representation based on the polyhedral model and it has a
rich scheduling language allowing fine-grained control of optimizations.
Tiramisu uses a four-level intermediate representation that allows full
separation between the algorithms, loop transformations, data layouts, and
communication. This separation simplifies targeting multiple hardware
architectures with the same algorithm. We evaluate Tiramisu by writing a set of
image processing, deep learning, and linear algebra benchmarks and compare them
with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu
matches or outperforms existing compilers and libraries on different hardware
architectures, including multicore CPUs, GPUs, and distributed machines.
"
1403,"A Basic Result on the Superposition of Arrival Processes in
  Deterministic Networks","  Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet) are
emerging standards to enable deterministic, delay-critical communication in
such networks. This naturally (re-)calls attention to the network calculus
theory (NC), since a rich set of results for delay guarantee analysis have
already been developed there. One could anticipate an immediate adoption of
those existing network calculus results to TSN and DetNet. However, the
fundamental difference between the traffic specification adopted in TSN and
DetNet and those traffic models in NC makes this difficult, let alone that
there is a long-standing open challenge in NC. To address them, this paper
considers an arrival time function based max-plus NC traffic model. In
particular, for the former, the mapping between the TSN / DetNet and the NC
traffic model is proved. For the latter, the superposition property of the
arrival time function based NC traffic model is found and proved. Appealingly,
the proved superposition property shows a clear analogy with that of a
well-known counterpart traffic model in NC. These results help make an
important step towards the development of a system theory for delay guarantee
analysis of TSN / DetNet networks.
"
1404,"Experimental Verification and Analysis of Dynamic Loop Scheduling in
  Scientific Applications","  Scientific applications are often irregular and characterized by large
computationally-intensive parallel loops. Dynamic loop scheduling (DLS)
techniques improve the performance of computationally-intensive scientific
applications via load balancing of their execution on high-performance
computing (HPC) systems. Identifying the most suitable choices of data
distribution strategies, system sizes, and DLS techniques which improve the
performance of a given application, requires intensive assessment and a large
number of exploratory native experiments (using real applications on real
systems), which may not always be feasible or practical due to associated time
and costs. In such cases, simulative experiments are more appropriate for
studying the performance of applications. This motivates the question of How
realistic are the simulations of executions of scientific applications using
DLS on HPC platforms? In the present work, a methodology is devised to answer
this question. It involves the experimental verification and analysis of the
performance of DLS in scientific applications. The proposed methodology is
employed for a computer vision application executing using four DLS techniques
on two different HPC plat- forms, both via native and simulative experiments.
The evaluation and analysis of the native and simulative results indicate that
the accuracy of the simulative experiments is strongly influenced by the
approach used to extract the computational effort of the application (FLOP- or
time-based), the choice of application model representation into simulation
(data or task parallel), and the available HPC subsystem models in the
simulator (multi-core CPUs, memory hierarchy, and network topology). The
minimum and the maximum percent errors achieved between the native and the
simulative experiments are 0.95% and 8.03%, respectively.
"
1405,BUDAMAF: Data Management in Cloud Federations,"  Data management has always been a multi-domain problem even in the simplest
cases. It involves, quality of service, security, resource management, cost
management, incident identification, disaster avoidance and/or recovery, as
well as many other concerns. In our case, this situation gets ever more
complicated because of the divergent nature of a cloud federation like BASMATI.
In this federation, the BASMATI Unified Data Management Framework (BUDaMaF),
tries to create an automated uniform way of managing all the data transactions,
as well as the data stores themselves, in a polyglot multi-cloud, consisting of
a plethora of different machines and data store systems.
"
1406,Computational Optimal Transport for 5G Massive C-RAN Device Association,"  The massive scale of future wireless networks will create computational
bottlenecks in performance optimization. In this paper, we study the problem of
connecting mobile traffic to Cloud RAN (C-RAN) stations. To balance station
load, we steer the traffic by designing device association rules. The baseline
association rule connects each device to the station with the strongest signal,
which does not account for interference or traffic hot spots, and leads to load
imbalances and performance deterioration. Instead, we can formulate an
optimization problem to decide centrally the best association rule at each time
instance. However, in practice this optimization has such high dimensions, that
even linear programming solvers fail to solve. To address the challenge of
massive connectivity, we propose an approach based on the theory of optimal
transport, which studies the economical transfer of probability between two
distributions. Our proposed methodology can further inspire scalable algorithms
for massive optimization problems in wireless networks.
"
1407,"AutoTiering: Automatic Data Placement Manager in Multi-Tier All-Flash
  Datacenter","  In the year of 2017, the capital expenditure of Flash-based Solid State
Drivers (SSDs) keeps declining and the storage capacity of SSDs keeps
increasing. As a result, the ""selling point"" of traditional spinning Hard Disk
Drives (HDDs) as a backend storage - low cost and large capacity - is no longer
unique, and eventually they will be replaced by low-end SSDs which have large
capacity but perform orders of magnitude better than HDDs. Thus, it is widely
believed that all-flash multi-tier storage systems will be adopted in the
enterprise datacenters in the near future. However, existing caching or tiering
solutions for SSD-HDD hybrid storage systems are not suitable for all-flash
storage systems. This is because that all-flash storage systems do not have a
large speed difference (e.g., 10x) among each tier. Instead, different
specialties (such as high performance, high capacity, etc.) of each tier should
be taken into consideration. Motivated by this, we develop an automatic data
placement manager called ""AutoTiering"" to handle virtual machine disk files
(VMDK) allocation and migration in an all-flash multi-tier datacenter to best
utilize the storage resource, optimize the performance, and reduce the
migration overhead. AutoTiering is based on an optimization framework, whose
core technique is to predict VM's performance change on different tiers with
different specialties without conducting real migration. As far as we know,
AutoTiering is the first optimization solution designed for all-flash
multi-tier datacenters. We implement AutoTiering on VMware ESXi, and
experimental results show that it can significantly improve the I/O performance
compared to existing solutions.
"
1408,Online normalizer calculation for softmax,"  The Softmax function is ubiquitous in machine learning, multiple previous
works suggested faster alternatives for it. In this paper we propose a way to
compute classical Softmax with fewer memory accesses and hypothesize that this
reduction in memory accesses should improve Softmax performance on actual
hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to
1.3x and Softmax+TopK combined and fused by up to 5x.
"
1409,"Dwarfs on Accelerators: Enhancing OpenCL Benchmarking for Heterogeneous
  Computing Architectures","  For reasons of both performance and energy efficiency, high-performance
computing (HPC) hardware is becoming increasingly heterogeneous. The OpenCL
framework supports portable programming across a wide range of computing
devices and is gaining influence in programming next-generation accelerators.
To characterize the performance of these devices across a range of applications
requires a diverse, portable and configurable benchmark suite, and OpenCL is an
attractive programming model for this purpose. We present an extended and
enhanced version of the OpenDwarfs OpenCL benchmark suite, with a strong focus
placed on the robustness of applications, curation of additional benchmarks
with an increased emphasis on correctness of results and choice of problem
size. Preliminary results and analysis are reported for eight benchmark codes
on a diverse set of architectures -- three Intel CPUs, five Nvidia GPUs, six
AMD GPUs and a Xeon Phi.
"
1410,"MPI+X: task-based parallelization and dynamic load balance of finite
  element assembly","  The main computing tasks of a finite element code(FE) for solving partial
differential equations (PDE's) are the algebraic system assembly and the
iterative solver. This work focuses on the first task, in the context of a
hybrid MPI+X paradigm. Although we will describe algorithms in the FE context,
a similar strategy can be straightforwardly applied to other discretization
methods, like the finite volume method. The matrix assembly consists of a loop
over the elements of the MPI partition to compute element matrices and
right-hand sides and their assemblies in the local system to each MPI
partition. In a MPI+X hybrid parallelism context, X has consisted traditionally
of loop parallelism using OpenMP. Several strategies have been proposed in the
literature to implement this loop parallelism, like coloring or substructuring
techniques to circumvent the race condition that appears when assembling the
element system into the local system. The main drawback of the first technique
is the decrease of the IPC due to bad spatial locality. The second technique
avoids this issue but requires extensive changes in the implementation, which
can be cumbersome when several element loops should be treated. We propose an
alternative, based on the task parallelism of the element loop using some
extensions to the OpenMP programming model. The taskification of the assembly
solves both aforementioned problems. In addition, dynamic load balance will be
applied using the DLB library, especially efficient in the presence of hybrid
meshes, where the relative costs of the different elements is impossible to
estimate a priori. This paper presents the proposed methodology, its
implementation and its validation through the solution of large computational
mechanics problems up to 16k cores.
"
1411,ProCal: A Low-Cost and Programmable Calibration Tool for IoT Devices,"  Calibration is an important step towards building reliable IoT systems. For
example, accurate sensor reading requires ADC calibration, and power monitoring
chips must be calibrated before being used for measuring the energy consumption
of IoT devices. In this paper, we present ProCal, a low-cost, accurate, and
scalable power calibration tool. ProCal is a programmable platform which
provides dynamic voltage and current output for calibration. The basic idea is
to use a digital potentiometer connected to a parallel resistor network
controlled through digital switches. The resistance and output frequency of
ProCal is controlled by a software communicating with the board through the SPI
interface. Our design provides a simple synchronization mechanism which
prevents the need for accurate time synchronization. We present mathematical
modeling and validation of the tool by incorporating the concept of Fibonacci
sequence. Our extensive experimental studies show that this tool can
significantly improve measurement accuracy. For example, for ATMega2560, the
ADC error reduces from 0.2% to 0.01%. ProCal not only costs less than 2\% of
the current commercial solutions, it is also highly accurate by being able to
provide extensive range of current and voltage values.
"
1412,Adaptive Selection of Deep Learning Models on Embedded Systems,"  The recent ground-breaking advances in deep learning networks ( DNNs ) make
them attractive for embedded systems. However, it can take a long time for DNNs
to make an inference on resource-limited embedded devices. Offloading the
computation into the cloud is often infeasible due to privacy concerns, high
latency, or the lack of connectivity. As such, there is a critical need to find
a way to effectively execute the DNN models locally on the devices. This paper
presents an adaptive scheme to determine which DNN model to use for a given
input, by considering the desired accuracy and inference time. Our approach
employs machine learning to develop a predictive model to quickly select a
pre-trained DNN to use for a given input and the optimization constraint. We
achieve this by first training off-line a predictive model, and then use the
learnt model to select a DNN model to use for new, unseen inputs. We apply our
approach to the image classification task and evaluate it on a Jetson TX2
embedded deep learning platform using the ImageNet ILSVRC 2012 validation
dataset. We consider a range of influential DNN models. Experimental results
show that our approach achieves a 7.52% improvement in inference accuracy, and
a 1.8x reduction in inference time over the most-capable single DNN model.
"
1413,Performance Study of LTE and mmWave in Vehicle-to-Network Communications,"  A key enabler for the emerging autonomous and cooperative driving services is
high-throughput and reliable Vehicle-to-Network (V2N) communication. In this
respect, the millimeter wave (mmWave) frequencies hold great promises because
of the large available bandwidth which may provide the required link capacity.
However, this potential is hindered by the challenging propagation
characteristics of high-frequency channels and the dynamic topology of the
vehicular scenarios, which affect the reliability of the connection. Moreover,
mmWave transmissions typically leverage beamforming gain to compensate for the
increased path loss experienced at high frequencies. This, however, requires
fine alignment of the transmitting and receiving beams, which may be difficult
in vehicular scenarios. Those limitations may undermine the performance of V2N
communications and pose new challenges for proper vehicular communication
design. In this paper, we study by simulation the practical feasibility of some
mmWave-aware strategies to support V2N, in comparison to the traditional LTE
connectivity below 6 GHz. The results show that the orchestration among
different radios represents a viable solution to enable both high-capacity and
robust V2N communications.
"
1414,"Enabling Cross-Event Optimization in Discrete-Event Simulation Through
  Compile-Time Event Batching","  A discrete-event simulation (DES) involves the execution of a sequence of
event handlers dynamically scheduled at runtime. As a consequence, a priori
knowledge of the control flow of the overall simulation program is limited. In
particular, powerful optimizations supported by modern compilers can only be
applied on the scope of individual event handlers, which frequently involve
only a few lines of code. We propose a method that extends the scope for
compiler optimizations in discrete-event simulations by generating batches of
multiple events that are subjected to compiler optimizations as contiguous
procedures. A runtime mechanism executes suitable batches at negligible
overhead. Our method does not require any compiler extensions and introduces
only minor additional effort during model development. The feasibility and
potential performance gains of the approach are illustrated on the example of
an idealized proof-ofconcept model. We believe that the applicability of the
approach extends to general event-driven programs.
"
1415,Towards scalable pattern-based optimization for dense linear algebra,"  Linear algebraic expressions are the essence of many computationally
intensive problems, including scientific simulations and machine learning
applications. However, translating high-level formulations of these expressions
to efficient machine-level representations is far from trivial: developers
should be assisted by automatic optimization tools so that they can focus their
attention on high-level problems, rather than low-level details. The
tractability of these optimizations is highly dependent on the choice of the
primitive constructs in terms of which the computations are to be expressed. In
this work we propose to describe operations on multi-dimensional arrays using a
selection of higher-order functions, inspired by functional programming, and we
present rewrite rules for these such that they can be automatically optimized
for modern hierarchical and heterogeneous architectures. Using this formalism
we systematically construct and analyse different subdivisions and permutations
of the dense matrix multiplication problem.
"
1416,"Ergodic Capacity Analysis of Free-Space Optical Links With Nonzero
  Boresight Pointing Errors","  A unified capacity analysis of a free-space optical (FSO) link that accounts
for nonzero boresight pointing errors and both types of detection techniques
(i.e. intensity modulation/direct detection as well as heterodyne detection) is
addressed in this work. More specifically, an exact closed-form expression for
the moments of the end-to-end signal-to-noise ratio (SNR) of a single link FSO
transmission system is presented in terms of well-known elementary functions.
Capitalizing on these new moments expressions, we present approximate and
simple closed-form results for the ergodic capacity at high and low SNR
regimes. All the presented results are verified via computer-based Monte-Carlo
simulations.
"
1417,"Performance Analysis of Free-Space Optical Links Over M\'{a}laga
  ($\mathcal{M}$) Turbulence Channels with Pointing Errors","  In this work, we present a unified performance analysis of a free-space
optical (FSO) link that accounts for pointing errors and both types of
detection techniques (i.e. intensity modulation/direct detection (IM/DD) as
well as heterodyne detection). More specifically, we present unified exact
closed-form expressions for the cumulative distribution function, the
probability density function, the moment generating function, and the moments
of the end-to-end signal-to-noise ratio (SNR) of a single link FSO transmission
system, all in terms of the Meijer's G function except for the moments that is
in terms of simple elementary functions. We then capitalize on these unified
results to offer unified exact closed-form expressions for various performance
metrics of FSO link transmission systems, such as, the outage probability, the
scintillation index (SI), the average error rate for binary and $M$-ary
modulation schemes, and the ergodic capacity (except for IM/DD technique, where
we present closed-form lower bound results), all in terms of Meijer's G
functions except for the SI that is in terms of simple elementary functions.
Additionally, we derive the asymptotic results for all the expressions derived
earlier in terms of Meijer's G function in the high SNR regime in terms of
simple elementary functions via an asymptotic expansion of the Meijer's G
function. We also derive new asymptotic expressions for the ergodic capacity in
the low as well as high SNR regimes in terms of simple elementary functions via
utilizing moments. All the presented results are verified via computer-based
Monte-Carlo simulations.
"
1418,Energy Efficient Task Assignment in Virtualized Wireless Sensor Networks,"  Wireless Sensor Networks (WSNs) are being used extensively today in various
domains. However, they are traditionally deployed with applications embedded in
them which precludes their re-use for new applications. Nowadays,
virtualization enables several applications on a same WSN by abstracting the
physical resources (i.e. sensing capabilities) into logical ones. However, this
comes at a cost, including an energy cost. It is therefore critical to ensure
the efficient allocation of these resources. In this paper, we study the
problem of assigning application sensing tasks to sensor devices, in
virtualized WSNs. Our goal is to minimize the overall energy consumption
resulting from the assignment. We focus on the static version of the problem
and formulate it using Integer Linear Programming (ILP), while accounting for
sensor nodes' available energy and virtualization overhead. We solve the
problem over different scenarios and compare the obtained solution to the case
of a traditional WSN, i.e. one with no support for virtualization. Our results
show that significant energy can be saved when tasks are appropriately assigned
in a WSN that supports virtualization.
"
1419,"Interference Analysis in Dynamic TDD System Combined or not With Cell
  Clustering Scheme","  Dynamic Time Division Duplex (TDD) has been introduced as a solution to deal
with the uplink and downlink traffic asymmetry, mainly observed for dense
heterogeneous network deployments. However, the use of this feature requires
new interference mitigation schemes capable to handle two additional types of
interferences between cells in opposite transmission cycle: downlink to uplink
and uplink to downlink interferences. Among them, Cell clustering has been
proposed as an efficient solution to minimize inter-cell interferences in
opposite transmission directions and somehow responds to the requirements of
enhanced Interference Mitigation and Traffic Adaptation (eIMTA) problem. This
work is devoted to provide a new analytical approach to model inter-cell
interferences and quantify performances of Dynamic TDD system in terms of SINR
(Signal to Interferences plus Noise Ratio) distribution. Analytical system
performance investigation concerns two scenarios: i) basic Dynamic TDD without
any other feature and ii) Dynamic TDD with interference mitigation schemes.
"
1420,"Hierarchical Beamforming: Resource Allocation, Fairness and Flow Level
  Performance","  We consider hierarchical beamforming in wireless networks. For a given
population of flows, we propose computationally efficient algorithms for fair
rate allocation including proportional fairness and max-min fairness. We next
propose closed-form formulas for flow level performance, for both elastic (with
either proportional fairness and max-min fairness) and streaming traffic. We
further assess the performance of hierarchical beamforming using numerical
experiments. Since the proposed solutions have low complexity compared to
conventional beamforming, our work suggests that hierarchical beamforming is a
promising candidate for the implementation of beamforming in future cellular
networks.
"
1421,"ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems
  with Online Workload Characterization","  In recent years, SSDs have gained tremendous attention in computing and
storage systems due to significant performance improvement over HDDs. The cost
per capacity of SSDs, however, prevents them from entirely replacing HDDs in
such systems. One approach to effectively take advantage of SSDs is to use them
as a caching layer to store performance critical data blocks to reduce the
number of accesses to disk subsystem. Due to characteristics of Flash-based
SSDs such as limited write endurance and long latency on write operations,
employing caching algorithms at the Operating System (OS) level necessitates to
take such characteristics into consideration. Previous caching techniques are
optimized towards only one type of application, which affects both generality
and applicability. In addition, they are not adaptive when the workload pattern
changes over time. This paper presents an efficient Reconfigurable Cache
Architecture (ReCA) for storage systems using a comprehensive workload
characterization to find an optimal cache configuration for I/O intensive
applications. For this purpose, we first investigate various types of I/O
workloads and classify them into five major classes. Based on this
characterization, an optimal cache configuration is presented for each class of
workloads. Then, using the main features of each class, we continuously monitor
the characteristics of an application during system runtime and the cache
organization is reconfigured if the application changes from one class to
another class of workloads. The cache reconfiguration is done online and
workload classes can be extended to emerging I/O workloads in order to maintain
its efficiency with the characteristics of I/O requests. Experimental results
obtained by implementing ReCA in a server running Linux show that the proposed
architecture improves performance and lifetime up to 24\% and 33\%,
respectively.
"
1422,Optimal Scheduling and Exact Response Time Analysis for Multistage Jobs,"  Scheduling to minimize mean response time in an M/G/1 queue is a classic
problem. The problem is usually addressed in one of two scenarios. In the
perfect-information scenario, the scheduler knows each job's exact size, or
service requirement. In the zero-information scenario, the scheduler knows only
each job's size distribution. The well-known shortest remaining processing time
(SRPT) policy is optimal in the perfect-information scenario, and the more
complex Gittins policy is optimal in the zero-information scenario.
  In real systems the scheduler often has partial but incomplete information
about each job's size. We introduce a new job model, that of multistage jobs,
to capture this partial-information scenario. A multistage job consists of a
sequence of stages, where both the sequence of stages and stage sizes are
unknown, but the scheduler always knows which stage of a job is in progress. We
give an optimal algorithm for scheduling multistage jobs in an M/G/1 queue and
an exact response time analysis of our algorithm.
"
1423,SRPT for Multiserver Systems,"  The Shortest Remaining Processing Time (SRPT) scheduling policy and its
variants have been extensively studied in both theoretical and practical
settings. While beautiful results are known for single-server SRPT, much less
is known for multiserver SRPT. In particular, stochastic analysis of the M/G/k
under multiserver SRPT is entirely open. Intuition suggests that multiserver
SRPT should be optimal or near-optimal for minimizing mean response time.
However, the only known analysis of multiserver SRPT is in the worst-case
adversarial setting, where SRPT can be far from optimal. In this paper, we give
the first stochastic analysis bounding mean response time of the M/G/k under
multiserver SRPT. Using our response time bound, we show that multiserver SRPT
has asymptotically optimal mean response time in the heavy-traffic limit. The
key to our bounds is a strategic combination of stochastic and worst-case
techniques. Beyond SRPT, we prove similar response time bounds and optimality
results for several other multiserver scheduling policies.
"
1424,"Performance Reproduction and Prediction of Selected Dynamic Loop
  Scheduling Experiments","  Scientific applications are complex, large, and often exhibit irregular and
stochastic behavior. The use of efficient loop scheduling techniques in
computationally-intensive applications is crucial for improving their
performance on high-performance computing (HPC) platforms. A number of dynamic
loop scheduling (DLS) techniques have been proposed between the late 1980s and
early 2000s, and efficiently used in scientific applications. In most cases,
the computing systems on which they have been tested and validated are no
longer available. This work is concerned with the minimization of the sources
of uncertainty in the implementation of DLS techniques to avoid unnecessary
influences on the performance of scientific applications. Therefore, it is
important to ensure that the DLS techniques employed in scientific applications
today adhere to their original design goals and specifications. The goal of
this work is to attain and increase the trust in the implementation of DLS
techniques in present studies. To achieve this goal, the performance of a
selection of scheduling experiments from the 1992 original work that introduced
factoring is reproduced and predicted via both, simulative and native
experimentation. The experiments show that the simulation reproduces the
performance achieved on the past computing platform and accurately predicts the
performance achieved on the present computing platform. The performance
reproduction and prediction confirm that the present implementation of the DLS
techniques considered both, in simulation and natively, adheres to their
original description. The results confirm the hypothesis that reproducing
experiments of identical scheduling scenarios on past and modern hardware leads
to an entirely different behavior from expected.
"
1425,"DRESS: Dynamic RESource-reservation Scheme for Congested Data-intensive
  Computing Platforms","  In the past few years, we have envisioned an increasing number of businesses
start driving by big data analytics, such as Amazon recommendations and Google
Advertisements. At the back-end side, the businesses are powered by big data
processing platforms to quickly extract information and make decisions. Running
on top of a computing cluster, those platforms utilize scheduling algorithms to
allocate resources. An efficient scheduler is crucial to the system performance
due to limited resources, e.g. CPU and Memory, and a large number of user
demands. However, besides requests from clients and current status of the
system, it has limited knowledge about execution length of the running jobs,
and incoming jobs' resource demands, which make assigning resources a
challenging task. If most of the resources are occupied by a long-running job,
other jobs will have to keep waiting until it releases them. This paper
presents a new scheduling strategy, named DRESS that particularly aims to
optimize the allocation among jobs with various demands. Specifically, it
classifies the jobs into two categories based on their requests, reserves a
portion of resources for each of category, and dynamically adjusts the reserved
ratio by monitoring the pending requests and estimating release patterns of
running jobs. The results demonstrate DRESS significantly reduces the
completion time for one category, up to 76.1% in our experiments, and in the
meanwhile, maintains a stable overall system performance.
"
1426,"Infinite-server queueing model with MAPkGk Markov arrival streams,
  random volume of customers in random environment subject to catastrophe","  In this paper the infinite server queue model in semi-Markov random
environment with k Markov arrival streams, random resources of customers, and
catastrophes is considered. After catastrophes occur, all customers in the
model are flashed out and the system jumps into recovery station. After the
recovery time the model works from the empty state. The transient and
stationary joint distributions of numbers of different types of customers in
the model at moment t, numbers of different types of served in some interval
customers, volume of accumulated resources in the model at moment t, and total
volume of served resources in an interval for the model without catastrophes
are found. The transient and stationary joint distributions of numbers of
different types of customers in the model at moment t, and volume of
accumulated resources in the model at moment t and their moments for the model
with catastrophes are obtained. All results are obtained using Danzig
collective marks method and renewal theory methods.
"
1427,"An infinite-server queueing model MMAPkGk in semi-Markov random
  environment with marked MAP arrival and subject to catastrophes","  In the present paper the infinite-server MMAPkGk queueing model with random
resource vector of customers, marked MAP arrival and semi-Markov (SM) arrival
of catastrophes is considered. The joint generating functions (PGF) of
transient and stationary distributions of number of busy servers and numbers of
different types served customers, as well as Laplace transformations (LT) of
joint distributions of total accumulated resources in the model at moment and
total accumulated resources of served customers during time interval are found.
The basic differential and renewal equations for transient and stationary PGF
of queue sizes of customers are found.
"
1428,"PAM: When Overloaded, Push Your Neighbor Aside!","  Recently SmartNICs are widely used to accelerate service chains in NFV.
However, when the SmartNIC is overloaded, casually migrating vNFs away from
SmartNIC to CPU may lead to additional packet transmissions between SmartNIC
and CPU. To address this problem, we present PAM, push aside migration to
effectively alleviate the hot spot on SmartNIC with no performance overhead.
Our key novelty is to push vNFs on the border of SmartNIC and CPU aside to
release resources for the bottleneck vNF. Evaluation shows that PAM could
efficiently alleviate the hot spot on SmartNIC and generate a service chain
with much lower latency compared with the naive solution.
"
1429,Maximizing Service Reward for Queues with Deadlines,"  In this paper we consider a real time queuing system with rewards and
deadlines. We assume that packet processing time is known upon arrival, as is
the case in communication networks. This assumption allows us to demonstrate
that the well known Earliest-Deadline-First policy performance can be improved.
We then propose a scheduling policy that provides excellent results for packets
with rewards and deadlines. We prove that the policy is optimal under
deterministic service time and binomial reward distribution. In the more
general case we prove that the policy processes the maximal number of packets
while collecting rewards higher than the expected reward. We present simulation
results that show its high performance in more generic cases compared to the
most commonly used scheduling policies.
"
1430,Confidence Interval Estimators for MOS Values,"  For the quantification of QoE, subjects often provide individual rating
scores on certain rating scales which are then aggregated into Mean Opinion
Scores (MOS). From the observed sample data, the expected value is to be
estimated. While the sample average only provides a point estimator, confidence
intervals (CI) are an interval estimate which contains the desired expected
value with a given confidence level. In subjective studies, the number of
subjects performing the test is typically small, especially in lab
environments. The used rating scales are bounded and often discrete like the
5-point ACR rating scale. Therefore, we review statistical approaches in the
literature for their applicability in the QoE domain for MOS interval
estimation (instead of having only a point estimator, which is the MOS). We
provide a conservative estimator based on the SOS hypothesis and binomial
distributions and compare its performance (CI width, outlier ratio of CI
violating the rating scale bounds) and coverage probability with well known CI
estimators. We show that the provided CI estimator works very well in practice
for MOS interval estimators, while the commonly used studentized CIs suffer
from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.
As an alternative, bootstrapping, i.e., random sampling of the subjective
ratings with replacement, is an efficient CI estimator leading to typically
smaller CIs, but lower coverage than the proposed estimator.
"
1431,"Modeling Impact of Human Errors on the Data Unavailability and Data Loss
  of Storage Systems","  Data storage systems and their availability play a crucial role in
contemporary datacenters. Despite using mechanisms such as automatic fail-over
in datacenters, the role of human agents and consequently their destructive
errors is inevitable. Due to very large number of disk drives used in exascale
datacenters and their high failure rates, the disk subsystem in storage systems
has become a major source of Data Unavailability (DU) and Data Loss (DL)
initiated by human errors. In this paper, we investigate the effect of
Incorrect Disk Replacement Service (IDRS) on the availability and reliability
of data storage systems. To this end, we analyze the consequences of IDRS in a
disk array, and conduct Monte Carlo simulations to evaluate DU and DL during
mission time. The proposed modeling framework can cope with a) different
storage array configurations and b) Data Object Survivability (DOS),
representing the effect of system level redundancies such as remote backups and
mirrors. In the proposed framework, the model parameters are obtained from
industrial and scientific reports alongside field data which have been
extracted from a datacenter operating with 70 storage racks. The results show
that ignoring the impact of IDRS leads to unavailability underestimation by up
to three orders of magnitude. Moreover, our study suggests that by considering
the effect of human errors, the conventional beliefs about the dependability of
different Redundant Array of Independent Disks (RAID) mechanisms should be
revised. The results show that RAID1 can result in lower availability compared
to RAID5 in the presence of human errors. The results also show that employing
automatic fail-over policy (using hot spare disks) can reduce the drastic
impacts of human errors by two orders of magnitude.
"
1432,"Evaluating Impact of Human Errors on the Availability of Data Storage
  Systems","  In this paper, we investigate the effect of incorrect disk replacement
service on the availability of data storage systems. To this end, we first
conduct Monte Carlo simulations to evaluate the availability of disk subsystem
by considering disk failures and incorrect disk replacement service. We also
propose a Markov model that corroborates the Monte Carlo simulation results. We
further extend the proposed model to consider the effect of automatic disk
fail-over policy. The results obtained by the proposed model show that
overlooking the impact of incorrect disk replacement can result up to three
orders of magnitude unavailability underestimation. Moreover, this study
suggests that by considering the effect of human errors, the conventional
believes about the dependability of different RAID mechanisms should be
revised. The results show that in the presence of human errors, RAID1 can
result in lower availability compared to RAID5.
"
1433,Improving rewards in overloaded real-time systems,"  Competitive analysis of online algorithms has commonly been applied to
understand the behaviour of real-time systems during overload conditions. While
competitive analysis provides insight into the behaviour of certain algorithms,
it is hard to make inferences about the performance of those algorithms in
practice. Other approaches to dealing with overload resort to heuristics that
seem to perform well but are hard to prove as being good. Further, most work on
handling overload in real-time systems does not consider using information
regarding the distribution of arrival rates of jobs and execution times to make
scheduling decisions. We present an scheduling policy (obtained through
stochastic approximation, and using information about the workload) to handle
overload in real-time systems and improve the revenue earned when each
successful job completion results in revenue accrual. We prove that the policy
we outline does lead to increased revenue when compared to a class of
scheduling policies that make static resource allocations to different service
classes. We also use empirical evidence to underscore the fact that this policy
performs better than a variety of other scheduling policies. The ideas
presented can be applied to several soft real-time systems, specifically
systems with multiple service classes.
"
1434,Fast Distributed Deep Learning via Worker-adaptive Batch Sizing,"  Deep neural network models are usually trained in cluster environments, where
the model parameters are iteratively refined by multiple worker machines in
parallel. One key challenge in this regard is the presence of stragglers, which
significantly degrades the learning performance. In this paper, we propose to
eliminate stragglers by adapting each worker's training load to its processing
capability; that is, slower workers receive a smaller batch of data to process.
  Following this idea, we develop a new synchronization scheme called LB-BSP
(Load-balanced BSP). It works by coordinately setting the batch size of each
worker so that they can finish batch processing at around the same time. A
prerequisite for deciding the workers' batch sizes is to know their processing
speeds before each iteration starts. For the best prediction accuracy, we adopt
NARX, an extended recurrent neural network that accounts for both the
historical speeds and the driving factors such as CPU and memory in prediction.
We have implemented LB-BSP for both TensorFlow and MXNet. EC2 experiments
against popular benchmarks show that LB-BSP can effectively accelerate the
training of deep models, with up to 2x speedup.
"
1435,"Scalable load balancing in networked systems: A survey of recent
  advances","  The basic load balancing scenario involves a single dispatcher where tasks
arrive that must immediately be forwarded to one of $N$ single-server queues.
We discuss recent advances on scalable load balancing schemes which provide
favorable delay performance when $N$ grows large, and yet only require minimal
implementation overhead.
  Join-the-Shortest-Queue (JSQ) yields vanishing delays as $N$ grows large, as
in a centralized queueing arrangement, but involves a prohibitive communication
burden. In contrast, power-of-$d$ or JSQ($d$) schemes that assign an incoming
task to a server with the shortest queue among $d$ servers selected uniformly
at random require little communication, but lead to constant delays. In order
to examine this fundamental trade-off between delay performance and
implementation overhead, we consider JSQ($d(N)$) schemes where the diversity
parameter $d(N)$ depends on $N$ and investigate what growth rate of $d(N)$ is
required to asymptotically match the optimal JSQ performance on fluid and
diffusion scale.
  Stochastic coupling techniques and stochastic-process limits play an
instrumental role in establishing the asymptotic optimality. We demonstrate how
this methodology carries over to infinite-server settings, finite buffers,
multiple dispatchers, servers arranged on graph topologies, and token-based
load balancing including the popular Join-the-Idle-Queue (JIQ) scheme. In this
way we provide a broad overview of the many recent advances in the field. This
survey extends the short review presented at ICM 2018 (arXiv:1712.08555).
"
1436,A model-driven approach for a new generation of adaptive libraries,"  Efficient high-performance libraries often expose multiple tunable parameters
to provide highly optimized routines. These can range from simple loop unroll
factors or vector sizes all the way to algorithmic changes, given that some
implementations can be more suitable for certain devices by exploiting hardware
characteristics such as local memories and vector units. Traditionally, such
parameters and algorithmic choices are tuned and then hard-coded for a specific
architecture and for certain characteristics of the inputs. However, emerging
applications are often data-driven, thus traditional approaches are not
effective across the wide range of inputs and architectures used in practice.
In this paper, we present a new adaptive framework for data-driven applications
which uses a predictive model to select the optimal algorithmic parameters by
training with synthetic and real datasets. We demonstrate the effectiveness of
a BLAS library and specifically on its matrix multiplication routine. We
present experimental results for two GPU architectures and show significant
performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an
embedded ARM Mali GPU) when compared to a traditionally optimized library.
"
1437,"Forest Packing: Fast, Parallel Decision Forests","  Machine learning has an emerging critical role in high-performance computing
to modulate simulations, extract knowledge from massive data, and replace
numerical models with efficient approximations. Decision forests are a critical
tool because they provide insight into model operation that is critical to
interpreting learned results. While decision forests are trivially
parallelizable, the traversals of tree data structures incur many random memory
accesses and are very slow. We present memory packing techniques that
reorganize learned forests to minimize cache misses during classification. The
resulting layout is hierarchical. At low levels, we pack the nodes of multiple
trees into contiguous memory blocks so that each memory access fetches data for
multiple trees. At higher levels, we use leaf cardinality to identify the most
popular paths through a tree and collocate those paths in cache lines. We
extend this layout with out-of-order execution and cache-line prefetching to
increase memory throughput. Together, these optimizations increase the
performance of classification in ensembles by a factor of four over an
optimized C++ implementation and a actor of 50 over a popular R language
implementation.
"
1438,"Optimising finite-difference methods for PDEs through parameterised
  time-tiling in Devito","  Finite-difference methods are widely used in solving partial differential
equations. In a large problem set, approximations can take days or weeks to
evaluate, yet the bulk of computation may occur within a single loop nest. The
modelling process for researchers is not straightforward either, requiring
models with differential equations to be translated into stencil kernels, then
optimised separately. One tool that seeks to speed up and eliminate mistakes
from this tedious procedure is Devito, used to efficiently employ
finite-difference methods.
  In this work, we implement time-tiling, a loop nest optimisation, in Devito
yielding a decrease in runtime of up to 45%, and at least 20% across stencils
from the acoustic wave equation family, widely used in Devito's target domain
of seismic imaging. We present an estimator for arithmetic intensity under
time-tiling and a model to predict runtime improvements in stencil
computations. We also consider generalisation of time-tiling to imperfect loop
nests, a less widely studied problem.
"
1439,"A NUMA-Aware Provably-Efficient Task-Parallel Platform Based on the
  Work-First Principle","  Task parallelism is designed to simplify the task of parallel programming.
When executing a task parallel program on modern NUMA architectures, it can
fail to scale due to the phenomenon called work inflation, where the overall
processing time that multiple cores spend on doing useful work is higher
compared to the time required to do the same amount of work on one core, due to
effects experienced only during parallel executions such as additional cache
misses, remote memory accesses, and memory bandwidth issues. It's possible to
mitigate work inflation by co-locating the computation with the data, but this
is nontrivial to do with task parallel programs. First, by design, the
scheduling for task parallel programs is automated, giving the user little
control over where the computation is performed. Second, the platforms tend to
employ work stealing, which provides strong theoretical guarantees, but its
randomized protocol for load balancing does not discern between work items that
are far away versus ones that are closer. In this work, we propose NUMA-WS, a
NUMA-aware task parallel platform engineering based on the work-first
principle. By abiding by the work-first principle, we are able to obtain a
platform that is work efficient, provides the same theoretical guarantees as
the classic work stealing scheduler, and mitigates work inflation. Furthermore,
we implemented a prototype platform by modifying Intel's Cilk Plus runtime
system and empirically demonstrate that the resulting system is work efficient
and scalable.
"
1440,Stress-testing memcomputing on hard combinatorial optimization problems,"  Memcomputing is a novel paradigm of computation that utilizes dynamical
elements with memory to both store and process information on the same physical
location. Its building blocks can be fabricated in hardware with standard
electronic circuits, thus offering a path to its practical realization. In
addition, since memcomputing is based on non-quantum elements, the equations of
motion describing these machines can be simulated efficiently on standard
computers. In fact, it was recently realized that memcomputing, and in
particular its digital (hence scalable) version, when simulated on a classical
machine provides a significant speed-up over state-of-the-art algorithms on a
variety of non-convex problems. Here, we stress-test the capabilities of this
approach on finding approximate solutions to hard combinatorial optimization
problems. These fall into a class which is known to require exponentially
growing resources in the worst cases, even to generate approximations. We
recently showed that in a region where state of the art algorithms demonstrate
this exponential growth, simulations of digital memcomputing machines performed
using the Falcon$^\copyright$ simulator of MemComputing, Inc. only require time
and memory resources that scale linearly. These results are extended in a
stress-test up to $64\times10^6$ variables (corresponding to about 1 billion
literals), namely the largest case that we could fit on a single node with 128
GB of DRAM. Since memcomputing can be applied to a wide variety of optimization
problems, this stress test shows the considerable advantage of
non-combinatorial, physics-inspired approaches over standard combinatorial
ones.
"
1441,"Measuring and comparing the scaling behaviour of a high-performance CFD
  code on different supercomputing infrastructures","  Parallel code design is a challenging task especially when addressing
petascale systems for massive parallel processing (MPP), i.e. parallel
computations on several hundreds of thousands of cores. An in-house
computational fluid dynamics code, developed by our group, was designed for
such high-fidelity runs in order to exhibit excellent scalability values. Basis
for this code is an adaptive hierarchical data structure together with an
efficient communication and (numerical) computation scheme that supports MPP.
For a detailled scalability analysis, we performed several experiments on two
of Germany's national supercomputers up to 140,000 processes. In this paper, we
will show the results of those experiments and discuss any bottlenecks that
could be observed while solving engineering-based problems such as porous media
flows or thermal comfort assessments for problem sizes up to several hundred
billion degrees of freedom.
"
1442,"Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy
  Consumption","  Compiler writers typically focus primarily on the performance of the
generated program binaries when selecting the passes and the order in which
they are applied in the standard optimization levels, such as GCC -O3. In some
domains, such as embedded systems and High-Performance Computing (HPC), it
might be sometimes acceptable to slowdown computations if the energy consumed
can be significantly decreased. Embedded systems often rely on a battery and
besides energy also have power dissipation limitations, while HPC centers have
a growing concern with electricity and cooling costs. Relying on power policies
to apply frequency/voltage scaling and/or change the CPU to idle states (e.g.,
alternate between power levels in bursts) as the main method to reduce energy
leaves potential for improvement using other orthogonal approaches. In this
work we evaluate the impact of compiler pass sequences specialization (also
known as compiler phase ordering) as a means to reduce the energy consumed by a
set of programs/functions when comparing with the use of the standard compiler
phase orders provided by, e.g., -OX flags. We use our phase selection and
ordering framework to explore the design space in the context of a Clang+LLVM
compiler targeting a multicore ARM processor in an ODROID board and a dual x86
desktop representative of a node in a Supercomputing center. Our experiments
with a set of representative kernels show that there we can reduce energy
consumption by up to 24% and that some of these improvements can only be
partially explained by improvements to execution time. The experiments show
cases where applications that run faster consume more energy. Additionally, we
make an effort to characterize the compiler sequence exploration space in terms
of their impact on performance and energy.
"
1443,A Survey on Agent-based Simulation using Hardware Accelerators,"  Due to decelerating gains in single-core CPU performance, computationally
expensive simulations are increasingly executed on highly parallel hardware
platforms. Agent-based simulations, where simulated entities act with a certain
degree of autonomy, frequently provide ample opportunities for parallelisation.
Thus, a vast variety of approaches proposed in the literature demonstrated
considerable performance gains using hardware platforms such as many-core CPUs
and GPUs, merged CPU-GPU chips as well as FPGAs. Typically, a combination of
techniques is required to achieve high performance for a given simulation
model, putting substantial burden on modellers. To the best of our knowledge,
no systematic overview of techniques for agent-based simulations on hardware
accelerators has been given in the literature. To close this gap, we provide an
overview and categorisation of the literature according to the applied
techniques. Since at the current state of research, challenges such as the
partitioning of a model for execution on heterogeneous hardware are still a
largely manual process, we sketch directions for future research towards
automating the hardware mapping and execution. This survey targets modellers
seeking an overview of suitable hardware platforms and execution techniques for
a specific simulation model, as well as methodology researchers interested in
potential research gaps requiring further exploration.
"
1444,Best-Effort FPGA Programming: A Few Steps Can Go a Long Way,"  FPGA-based heterogeneous architectures provide programmers with the ability
to customize their hardware accelerators for flexible acceleration of many
workloads. Nonetheless, such advantages come at the cost of sacrificing
programmability. FPGA vendors and researchers attempt to improve the
programmability through high-level synthesis (HLS) technologies that can
directly generate hardware circuits from high-level language descriptions.
However, reading through recent publications on FPGA designs using HLS, one
often gets the impression that FPGA programming is still hard in that it leaves
programmers to explore a very large design space with many possible
combinations of HLS optimization strategies.
  In this paper we make two important observations and contributions. First, we
demonstrate a rather surprising result: FPGA programming can be made easy by
following a simple best-effort guideline of five refinement steps using HLS. We
show that for a broad class of accelerator benchmarks from MachSuite, the
proposed best-effort guideline improves the FPGA accelerator performance by
42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator
performance is improved from an average 292.5x slowdown to an average 34.4x
speedup. Moreover, we show that the refinement steps in the best-effort
guideline, consisting of explicit data caching, customized pipelining,
processing element duplication, computation/communication overlapping and
scratchpad reorganization, correspond well to the best practice guidelines for
multicore CPU programming. Although our best-effort guideline may not always
lead to the optimal solution, it substantially simplifies the FPGA programming
effort, and will greatly support the wide adoption of FPGA-based acceleration
by the software programming community.
"
1445,Restructuring Batch Normalization to Accelerate CNN Training,"  Batch Normalization (BN) has become a core design block of modern
Convolutional Neural Networks (CNNs). A typical modern CNN has a large number
of BN layers in its lean and deep architecture. BN requires mean and variance
calculations over each mini-batch during training. Therefore, the existing
memory access reduction techniques, such as fusing multiple CONV layers, are
not effective for accelerating BN due to their inability to optimize mini-batch
related calculations during training. To address this increasingly important
problem, we propose to restructure BN layers by first splitting a BN layer into
two sub-layers (fission) and then combining the first sub-layer with its
preceding CONV layer and the second sub-layer with the following activation and
CONV layers (fusion). The proposed solution can significantly reduce
main-memory accesses while training the latest CNN models, and the experiments
on a chip multiprocessor show that the proposed BN restructuring can improve
the performance of DenseNet-121 by 25.7%.
"
1446,"A Comparative Study of Containers and Virtual Machines in Big Data
  Environment","  Container technique is gaining increasing attention in recent years and has
become an alternative to traditional virtual machines. Some of the primary
motivations for the enterprise to adopt the container technology include its
convenience to encapsulate and deploy applications, lightweight operations, as
well as efficiency and flexibility in resources sharing. However, there still
lacks an in-depth and systematic comparison study on how big data applications,
such as Spark jobs, perform between a container environment and a virtual
machine environment. In this paper, by running various Spark applications with
different configurations, we evaluate the two environments from many
interesting aspects, such as how convenient the execution environment can be
set up, what are makespans of different workloads running in each setup, how
efficient the hardware resources, such as CPU and memory, are utilized, and how
well each environment can scale. The results show that compared with virtual
machines, containers provide a more easy-to-deploy and scalable environment for
big data workloads. The research work in this paper can help practitioners and
researchers to make more informed decisions on tuning their cloud environment
and configuring the big data applications, so as to achieve better performance
and higher resources utilization.
"
1447,"TabulaROSA: Tabular Operating System Architecture for Massively Parallel
  Heterogeneous Compute Engines","  The rise in computing hardware choices is driving a reevaluation of operating
systems. The traditional role of an operating system controlling the execution
of its own hardware is evolving toward a model whereby the controlling
processor is distinct from the compute engines that are performing most of the
computations. In this context, an operating system can be viewed as software
that brokers and tracks the resources of the compute engines and is akin to a
database management system. To explore the idea of using a database in an
operating system role, this work defines key operating system functions in
terms of rigorous mathematical semantics (associative array algebra) that are
directly translatable into database operations. These operations possess a
number of mathematical properties that are ideal for parallel operating systems
by guaranteeing correctness over a wide range of parallel operations. The
resulting operating system equations provide a mathematical specification for a
Tabular Operating System Architecture (TabulaROSA) that can be implemented on
any platform. Simulations of forking in TabularROSA are performed using an
associative array implementation and compared to Linux on a 32,000+ core
supercomputer. Using over 262,000 forkers managing over 68,000,000,000
processes, the simulations show that TabulaROSA has the potential to perform
operating system functions on a massively parallel scale. The TabulaROSA
simulations show 20x higher performance as compared to Linux while managing
2000x more processes in fully searchable tables.
"
1448,Improving the Performance of WLANs by Reducing Unnecessary Active Scans,"  We consider the problem of excessive and unnecessary active scans in heavily
utilized WLANs during which low rate probe requests and responses are
broadcast. These management frames severely impact the goodput. Our analysis of
two production WLANs reveals that lesser number of non-overlapping channels in
$2.4$ GHz makes it more prone to the effects of increased probe frames than $5$
GHz. We find that not only up to $90$% of probe responses carry redundant
information but the probe traffic can be as high as $60$\% of the management
traffic. Furthermore, active scanning severely impacts real-time applications
at a client as it increases the latency by $91$ times.
  We present a detailed analysis of the impact of active scans on an individual
client and the whole network. We discuss three ways to control the probe
traffic in production WLANs -- access point configurations, network planning,
and client modification. Our proposals for access point configuration are in
line with current WLAN deployments, better network planning is device agnostic
in nature, and client modification reduces the average number of probe requests
per client by up to $50$% without hampering the ongoing WiFi connection.
"
1449,"Evaluation as a Service architecture and crowdsourced problems solving
  implemented in Optil.io platform","  Reliable and trustworthy evaluation of algorithms is a challenging process.
Firstly, each algorithm has its strengths and weaknesses, and the selection of
test instances can significantly influence the assessment process. Secondly,
the measured performance of the algorithm highly depends on the test
environment architecture, i.e., CPU model, available memory, cache
configuration, operating system's kernel, and even compilation flags. Finally,
it is often difficult to compare algorithm with software prepared by other
researchers. Evaluation as a Service (EaaS) is a cloud computing architecture
that tries to make assessment process more reliable by providing online tools
and test instances dedicated to the evaluation of algorithms. One of such
platforms is Optil.io which gives the possibility to define problems, store
evaluation data and evaluate solutions submitted by researchers in almost real
time. In this paper, we briefly present this platform together with four
challenges that were organized with its support.
"
1450,"Design and optimisation of an efficient HDF5 I/O kernel for massive
  parallel fluid flow simulations","  More and more massive parallel codes running on several hundreds of thousands
of cores enter the computational science and engineering domain, allowing
high-fidelity computations on up to trillions of unknowns for very detailed
analyses of the underlying problems. During such runs, typically gigabytes of
data are being produced, hindering both efficient storage and (interactive)
data exploration. Here, advanced approaches based on inherently distributed
data formats such as HDF5 become necessary in order to avoid long latencies
when storing the data and to support fast (random) access when retrieving the
data for visual processing. Avoiding file locking and using collective
buffering, write bandwidths to a single file close to the theoretical peak on a
modern supercomputing cluster were achieved. The structure of the output file
supports a very fast interactive visualisation and introduces additional
steering functionality.
"
1451,"A refined mean field approximation of synchronous discrete-time
  population models","  Mean field approximation is a popular method to study the behaviour of
stochastic models composed of a large number of interacting objects. When the
objects are asynchronous, the mean field approximation of a population model
can be expressed as an ordinary differential equation. When the objects are
(clock-) synchronous the mean field approximation is a discrete time dynamical
system. We focus on the latter.We study the accuracy of mean field
approximation when this approximation is a discrete-time dynamical system. We
extend a result that was shown for the continuous time case and we prove that
expected performance indicators estimated by mean field approximation are
$O(1/N)$-accurate. We provide simple expressions to effectively compute the
asymptotic error of mean field approximation, for finite time-horizon and
steady-state, and we use this computed error to propose what we call a
\emph{refined} mean field approximation. We show, by using a few numerical
examples, that this technique improves the quality of approximation compared to
the classical mean field approximation, especially for relatively small
population sizes.
"
1452,A Queuing Model for CPU Functional Unit and Issue Queue Configuration,"  In a superscalar processor, instructions of various types flow through an
execution pipeline, traversing hardware resources which are mostly shared among
many different instruction types. A notable exception to shared pipeline
resources is the collection of functional units, the hardware that performs
specific computations. In a trade-off of cost versus performance, a pipeline
designer must decide how many of each type of functional unit to place in a
processor's pipeline. In this paper, we model a superscalar processor's issue
queue and functional units as a novel queuing network. We treat the issue queue
as a finite-sized waiting area and the functional units as servers. In addition
to common queuing problems, customers of the network share the queue but wait
for specific servers to become ready (e.g., addition instructions wait for
adders). Furthermore, the customers in this queue are not necessary ready for
service, since instructions may be waiting for operands. In this paper we model
a novel queuing network that provides a solution to the expected queue length
of each type of instruction. This network and its solution can also be
generalized to other problems, notably other resource-allocation issues that
arise in superscalar pipelines.
"
1453,"Variational inequalities and mean-field approximations for partially
  observed systems of queueing networks","  Queueing networks are systems of theoretical interest that find widespread
use in the performance evaluation of interconnected resources. In comparison to
counterpart models in genetics or mathematical biology, the stochastic (jump)
processes induced by queueing networks have distinctive coupling and
synchronization properties. This has prevented the derivation of variational
approximations for conditional representations of transient dynamics, which
rely on simplifying independence assumptions. Here, we present a model
augmentation to a multivariate counting process for interactions across service
stations, and we enable the variational evaluation of mean-field measures for
partially-observed multi-class networks. We also show that our framework offers
an efficient and improved alternative for inference tasks, where existing
variational or numerically intensive solutions do not work.
"
1454,"GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering
  and Multimodal Analysis","  We utilize commonsense knowledge bases to address the problem of real- time
multimodal analysis. In particular, we focus on the problem of multimodal
sentiment analysis, which consists in the simultaneous analysis of different
modali- ties, e.g., speech and video, for emotion and polarity detection. Our
approach takes advantages of the massively parallel processing power of modern
GPUs to enhance the performance of feature extraction from different
modalities. In addition, in order to ex- tract important textual features from
multimodal sources we generate domain-specific graphs based on commonsense
knowledge and apply GPU-based graph traversal for fast feature detection. Then,
powerful ELM classifiers are applied to build the senti- ment analysis model
based on the extracted features. We conduct our experiments on the YouTube
dataset and achieve an accuracy of 78% which outperforms all previous systems.
In term of processing speed, our method shows improvements of several orders of
magnitude for feature extraction compared to CPU-based counterparts.
"
1455,Time-efficient Garbage Collection in SSDs,"  SSDs are currently replacing magnetic disks in many application areas. A
challenge of the underlying flash technology is that data cannot be updated
in-place. A block consisting of many pages must be completely erased before a
single page can be rewritten. This victim block can still contain valid pages
which need to be copied to other blocks before erasure. The objective of
garbage collection strategies is to minimize write amplification induced by
copying valid pages from victim blocks while minimizing the performance
overhead of the victim selection. Victim selection strategies minimizing write
amplification, like the cost-benefit approach, have linear runtime, while the
write amplifications of time-efficient strategies, like the greedy strategy,
significantly reduce the lifetime of SSDs. In this paper, we propose two
strategies which optimize the performance of cost-benefit, while (almost)
preserving its write amplification. Trace-driven simulations for single- and
multi-channel SSDs show that the optimizations help to keep the write
amplification low while improving the runtime by up to 24-times compared to the
original cost-benefit strategy, so that the new strategies can be used in
multi-TByte SSDs.
"
1456,"FPGA-Based CNN Inference Accelerator Synthesized from Multi-Threaded C
  Software","  A deep-learning inference accelerator is synthesized from a C-language
software program parallelized with Pthreads. The software implementation uses
the well-known producer/consumer model with parallel threads interconnected by
FIFO queues. The LegUp high-level synthesis (HLS) tool synthesizes threads into
parallel FPGA hardware, translating software parallelism into spatial
parallelism. A complete system is generated where convolution, pooling and
padding are realized in the synthesized accelerator, with remaining tasks
executing on an embedded ARM processor. The accelerator incorporates reduced
precision, and a novel approach for zero-weight-skipping in convolution. On a
mid-sized Intel Arria 10 SoC FPGA, peak performance on VGG-16 is 138 effective
GOPS.
"
1457,"Real Time Lidar and Radar High-Level Fusion for Obstacle Detection and
  Tracking with evaluation on a ground truth","  - Both Lidars and Radars are sensors for obstacle detection. While Lidars are
very accurate on obstacles positions and less accurate on their velocities,
Radars are more precise on obstacles velocities and less precise on their
positions. Sensor fusion between Lidar and Radar aims at improving obstacle
detection using advantages of the two sensors. The present paper proposes a
real-time Lidar/Radar data fusion algorithm for obstacle detection and tracking
based on the global nearest neighbour standard filter (GNN). This algorithm is
implemented and embedded in an automative vehicle as a component generated by a
real-time multisensor software. The benefits of data fusion comparing with the
use of a single sensor are illustrated through several tracking scenarios (on a
highway and on a bend) and using real-time kinematic sensors mounted on the ego
and tracked vehicles as a ground truth.
"
1458,Pareto-Optimization Framework for Automated Network-on-Chip Design,"  With the advent of multi-core processors, network-on-chip design has been key
in addressing network performances, such as bandwidth, power consumption, and
communication delays when dealing with on-chip communication between the
increasing number of processor cores. As the numbers of cores increase, network
design becomes more complex. Therefore, there is a critical need in soliciting
computer aid in determining network configurations that afford optimal
performance given resources and design constraints. We propose a
Pareto-optimization framework that explores the space of possible network
configurations to determine optimal network latencies, power consumption, and
the corresponding link allocations. For a given number of routers, average
network latency and power consumption as example performance objectives can be
displayed in form of Pareto-optimal fronts, thus not only offering a design
tool, but also enabling trade-off studies.
"
1459,t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data,"  Modern datasets and models are notoriously difficult to explore and analyze
due to their inherent high dimensionality and massive numbers of samples.
Existing visualization methods which employ dimensionality reduction to two or
three dimensions are often inefficient and/or ineffective for these datasets.
This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of
t-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and
models. t-SNE-CUDA significantly outperforms current implementations with
50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for
the first time, visualization of the neural network activations on the entire
ImageNet dataset - a feat that was previously computationally intractable. We
also demonstrate visualization performance in the NLP domain by visualizing the
GloVe embedding vectors. From these visualizations, we can draw interesting
conclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is
publicly available athttps://github.com/CannyLab/tsne-cuda
"
1460,"Relative Age of Information: Maintaining Freshness while Considering the
  Most Recently Generated Information","  A queueing system handling a sequence of message arrivals is considered where
each message obsoletes all previous messages. The objective is to assess the
freshness of the latest message/information that has been successfully
transmitted, i.e., ""age of information"" (AoI). We study a variation of
traditional AoI, the ""Relative AoI"", here defined so as to account for the
presence of newly arrived messages/information to the queue to be transmitted.
"
1461,"A Stochastic Model for File Lifetime and Security in Data Center
  Networks","  Data center networks are an important infrastructure in various applications
of modern information technologies. Note that each data center always has a
finite lifetime, thus once a data center fails, then it will lose all its
storage files and useful information. For this, it is necessary to replicate
and copy each important file into other data centers such that this file can
increase its lifetime of staying in a data center network. In this paper, we
describe a large-scale data center network with a file d-threshold policy,
which is to replicate each important file into at most d-1 other data centers
such that this file can maintain in the data center network under a given level
of data security in the long-term. To this end, we develop three relevant
Markov processes to propose two effective methods for assessing the file
lifetime and data security. By using the RG-factorizations, we show that the
two methods are used to be able to more effectively evaluate the file lifetime
of large-scale data center networks. We hope the methodology and results given
in this paper are applicable in the file lifetime study of more general data
center networks with replication mechanism.
"
1462,"Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair
  Bandwidth Sharing Policies","  We consider a connection-level model proposed by Massouli\'{e} and Roberts
for bandwidth sharing among file transfer flows in a communication network, and
we study weighted proportionally fair sharing policies where the weights
represent the relative importance of flows on different routes. We are
interested in characterizing performance in the heavy-traffic regime. Existing
work on this problem has focused on diffusion approximations, which were first
studied by Kang et al. (2009). However, except for the case where the weights
of all the routes are equal, the steady-state distribution of the limiting
diffusion process is unknown and thus there are no explicit-form
characterizations, even when exponential file size distributions are assumed.
For more general file size distributions, the diffusion approximation was
derived for the equal-weights case by Vlasiou, Zhang and Zwart (2014), but an
interchange-of-limits result was lacking.
  We take a Lyapunov-drift-based approach that is different from the diffusion
approximation approach, where we directly analyze the steady state of the
system. We first establish a state-space collapse result in steady state, and
then obtain explicit-form bounds on the weighted sum of the expected numbers of
flows on different routes, where the weights are the same as those used in the
weighted proportionally fair sharing policy. Our bounds hold for a class of
phase-type file size distributions; i.e., the bounds are heavy-traffic
insensitive to the distributions in this class. For the equal-weights case, the
upper and lower bounds coincide, which implies the heavy-traffic insensitivity
of the expectation of the total number of flows. Furthermore, our state-space
collapse result implies an interchange of limits as a by-product for the
diffusion approximation by Vlasiou, Zhang and Zwart (2014).
"
1463,Anonymity and Confidentiality in Secure Distributed Simulation,"  Research on data confidentiality, integrity and availability is gaining
momentum in the ICT community, due to the intrinsically insecure nature of the
Internet. While many distributed systems and services are now based on secure
communication protocols to avoid eavesdropping and protect confidentiality, the
techniques usually employed in distributed simulations do not consider these
issues at all. This is probably due to the fact that many real-world simulators
rely on monolithic, offline approaches and therefore the issues above do not
apply. However, the complexity of the systems to be simulated, and the rise of
distributed and cloud based simulation, now impose the adoption of secure
simulation architectures. This paper presents a solution to ensure both
anonymity and confidentiality in distributed simulations. A performance
evaluation based on an anonymized distributed simulator is used for quantifying
the performance penalty for being anonymous. The obtained results show that
this is a viable solution.
"
1464,Sprintz: Time Series Compression for the Internet of Things,"  Thanks to the rapid proliferation of connected devices, sensor-generated time
series constitute a large and growing portion of the world's data. Often, this
data is collected from distributed, resource-constrained devices and
centralized at one or more servers. A key challenge in this setup is reducing
the size of the transmitted data without sacrificing its quality. Lower quality
reduces the data's utility, but smaller size enables both reduced network and
storage costs at the servers and reduced power consumption in sensing devices.
A natural solution is to compress the data at the sensing devices.
Unfortunately, existing compression algorithms either violate the memory and
latency constraints common for these devices or, as we show experimentally,
perform poorly on sensor-generated time series.
  We introduce a time series compression algorithm that achieves
state-of-the-art compression ratios while requiring less than 1KB of memory and
adding virtually no latency. This method is suitable not only for low-power
devices collecting data, but also for servers storing and querying data; in the
latter context, it can decompress at over 3GB/s in a single thread, even faster
than many algorithms with much lower compression ratios. A key component of our
method is a high-speed forecasting algorithm that can be trained online and
significantly outperforms alternatives such as delta coding.
  Extensive experiments on datasets from many domains show that these results
hold not only for sensor data but also across a wide array of other time
series.
"
1465,"A Constrained Shortest Path Scheme for Virtual Network Service
  Management","  Virtual network services that span multiple data centers are important to
support emerging data-intensive applications in fields such as bioinformatics
and retail analytics. Successful virtual network service composition and
maintenance requires flexible and scalable 'constrained shortest path
management' both in the management plane for virtual network embedding (VNE) or
network function virtualization service chaining (NFV-SC), as well as in the
data plane for traffic engineering (TE). In this paper, we show analytically
and empirically that leveraging constrained shortest paths within recent VNE,
NFV-SC and TE algorithms can lead to network utilization gains (of up to 50%)
and higher energy efficiency. The management of complex VNE, NFV-SC and TE
algorithms can be, however, intractable for large scale substrate networks due
to the NP-hardness of the constrained shortest path problem. To address such
scalability challenges, we propose a novel, exact constrained shortest path
algorithm viz., 'Neighborhoods Method' (NM). Our NM uses novel search space
reduction techniques and has a theoretical quadratic speed-up making it
practically faster (by an order of magnitude) than recent branch-and-bound
exhaustive search solutions. Finally, we detail our NM-based SDN controller
implementation in a real-world testbed to further validate practical NM
benefits for virtual network services.
"
1466,MARS: Memory Aware Reordered Source,"  Memory bandwidth is critical in today's high performance computing systems.
The bandwidth is particularly paramount for GPU workloads such as 3D Gaming,
Imaging and Perceptual Computing, GPGPU due to their data-intensive nature. As
the number of threads and data streams in the GPUs increases with each
generation, along with a high available memory bandwidth, memory efficiency is
also crucial in order to achieve desired performance. In presence of multiple
concurrent data streams, the inherent locality in a single data stream is often
lost as these streams are interleaved while moving through multiple levels of
memory system. In DRAM based main memory, the poor request locality reduces
row-buffer reuse resulting in underutilized and inefficient memory bandwidth.
  In this paper we propose Memory-Aware Reordered Source (\textit{MARS})
architecture to address memory inefficiency arising from highly interleaved
data streams. The key idea of \textit{MARS} is that with a sufficiently large
lookahead before the main memory, data streams can be reordered based on their
row-buffer address to regain the lost locality and improve memory efficiency.
We show that \textit{MARS} improves achieved memory bandwidth by 11\% for a set
of synthetic microbenchmarks. Moreover, MARS does so without any specific
knowledge of the memory configuration.
"
1467,"On the Feasibility of FPGA Acceleration of Molecular Dynamics
  Simulations","  Classical molecular dynamics (MD) simulations are important tools in life and
material sciences since they allow studying chemical and biological processes
in detail. However, the inherent scalability problem of particle-particle
interactions and the sequential dependency of subsequent time steps render MD
computationally intensive and difficult to scale. To this end, specialized
FPGA-based accelerators have been repeatedly proposed to ameliorate this
problem. However, to date none of the leading MD simulation packages fully
support FPGA acceleration and a direct comparison of GPU versus FPGA
accelerated codes has remained elusive so far. With this report, we aim at
clarifying this issue by comparing measured application performance on
GPU-dense compute nodes with performance and cost estimates of a FPGA-based
single- node system. Our results show that an FPGA-based system can indeed
outperform a similarly configured GPU-based system, but the overall
application-level speedup remains in the order of 2x due to software overheads
on the host. Considering the price for GPU and FPGA solutions, we observe that
GPU-based solutions provide the better cost/performance tradeoff, and hence
pure FPGA-based solutions are likely not going to be commercially viable.
However, we also note that scaled multi-node systems could potentially benefit
from a hybrid composition, where GPUs are used for compute intensive parts and
FPGAs for latency and communication sensitive tasks.
"
1468,"Compiler Enhanced Scheduling for OpenMP for Heterogeneous
  Multiprocessors","  Scheduling in Asymmetric Multicore Processors (AMP), a special case of
Heterogeneous Multiprocessors, is a widely studied topic. The scheduling
techniques which are mostly runtime do not usually consider parallel
programming pattern used in parallel programming frameworks like OpenMP. On the
other hand, current compilers for these parallel programming platforms are
hardware oblivious which prevent any compile-time optimization for platforms
like big.LITTLE and has to completely rely on runtime optimization. In this
paper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the
common compiler transformations are coupled with compiler added scheduling
commands to take advantage of the hardware asymmetry and improve the runtime
efficiency. We implement a compiler for OpenMP and demonstrate its efficiency
in Samsung Exynos with big.LITTLE architecture. On an average, we see 18%
reduction in runtime and 14% reduction in energy consumption in standard NPB
and FSU benchmarks with CES across multiple frequencies and core configurations
in big.LITTLE.
"
1469,"Heavy-traffic Delay Optimality in Pull-based Load Balancing Systems:
  Necessary and Sufficient Conditions","  In this paper, we consider a load balancing system under a general pull-based
policy. In particular, each arrival is randomly dispatched to one of the
servers whose queue lengths are below a threshold, if there are any; otherwise,
this arrival is randomly dispatched to one of the entire set of servers. We are
interested in the fundamental relationship between the threshold and the delay
performance of the system in heavy traffic. To this end, we first establish the
following necessary condition to guarantee heavy-traffic delay optimality: the
threshold will grow to infinity as the exogenous arrival rate approaches the
boundary of the capacity region (i.e., the load intensity approaches one) but
the growth rate should be slower than a polynomial function of the mean number
of tasks in the system. As a special case of this result, we directly show that
the delay performance of the popular pull-based policy Join-Idle-Queue (JIQ)
lies strictly between that of any heavy-traffic delay optimal policy and that
of random routing. We further show that a sufficient condition for
heavy-traffic delay optimality is that the threshold grows logarithmically with
the mean number of tasks in the system. This result directly resolves a
generalized version of the conjecture by Kelly and Laws.
"
1470,On Deep Neural Networks for Detecting Heart Disease,"  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as ""at risk."" Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
"
1471,"Data Motifs: A Lens Towards Fully Understanding Big Data and AI
  Workloads","  The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to modelling
and characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of units of computation performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data motif. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data motifs that take up most of the run time of those workloads,
including Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.
We implement the eight data motifs on different software stacks as the micro
benchmarks of an open-source big data and AI benchmark suite ---BigDataBench
4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform
comprehensive characterization of those data motifs from perspective of data
sizes, types, sources, and patterns as a lens towards fully understanding big
data and AI workloads. We believe the eight data motifs are promising
abstractions and tools for not only big data and AI benchmarking, but also
domain-specific hardware and software co-design.
"
1472,Persistent Stochastic Non-Interference,"  In this paper we present an information flow security property for
stochastic, cooperating, processes expressed as terms of the Performance
Evaluation Process Algebra (PEPA). We introduce the notion of Persistent
Stochastic Non-Interference (PSNI) based on the idea that every state reachable
by a process satisfies a basic Stochastic Non-Interference (SNI) property. The
structural operational semantics of PEPA allows us to give two
characterizations of PSNI: the first involves a single bisimulation-like
equivalence check, while the second is formulated in terms of unwinding
conditions. The observation equivalence at the base of our definition relies on
the notion of lumpability and ensures that, for a secure process P, the steady
state probability of observing the system being in a specific state P' is
independent from its possible high level interactions.
"
1473,MemComputing Integer Linear Programming,"  Integer linear programming (ILP) encompasses a very important class of
optimization problems that are of great interest to both academia and industry.
Several algorithms are available that attempt to explore the solution space of
this class efficiently, while requiring a reasonable compute time. However,
although these algorithms have reached various degrees of success over the
years, they still face considerable challenges when confronted with
particularly hard problem instances, such as those of the MIPLIB 2010 library.
In this work we propose a radically different non-algorithmic approach to ILP
based on a novel physics-inspired computing paradigm: Memcomputing. This
paradigm is based on digital (hence scalable) machines represented by
appropriate electrical circuits with memory. These machines can be either built
in hardware or, as we do here, their equations of motion can be efficiently
simulated on our traditional computers. We first describe a new circuit
architecture of memcomputing machines specifically designed to solve for the
linear inequalities representing a general ILP problem. We call these
self-organizing algebraic circuits, since they self-organize dynamically to
satisfy the correct (algebraic) linear inequalities. We then show simulations
of these machines using MATLAB running on a single core of a Xeon processor for
several ILP benchmark problems taken from the MIPLIB 2010 library, and compare
our results against a renowned commercial solver. We show that our approach is
very efficient when dealing with these hard problems. In particular, we find
within minutes feasible solutions for one of these hard problems (f2000 from
MIPLIB 2010) whose feasibility, to the best of our knowledge, has remained
unknown for the past eight years.
"
1474,"Profiling and Improving the Duty-Cycling Performance of Linux-based IoT
  Devices","  Minimizing the energy consumption of Linux-based devices is an essential step
towards their wide deployment in various IoT scenarios. Energy saving methods
such as duty-cycling aim to address this constraint by limiting the amount of
time the device is powered on. In this work we study and improve the amount of
time a Linux-based IoT device is powered on to accomplish its tasks. We analyze
the processes of system boot up and shutdown on two platforms, the Raspberry Pi
3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by
identifying and disabling time-consuming or unnecessary units initialized in
the userspace. We also study whether SD card speed and SD card capacity
utilization affect boot up duration and energy consumption. In addition, we
propose 'Pallex', a parallel execution framework built on top of the 'systemd
init' system to run a user application concurrently with userspace
initialization. We validate the performance impact of Pallex when applied to
various IoT application scenarios: (i) capturing an image, (ii) capturing and
encrypting an image, (iii) capturing and classifying an image using the the
k-nearest neighbor algorithm, and (iv) capturing images and sending them to a
cloud server. Our results show that system lifetime is increased by 18.3%,
16.8%, 13.9% and 30.2%, for these application scenarios, respectively.
"
1475,"Evaluation of the performance challenges in automatic traffic report
  generation with huge data volumes","  In this paper we analyze the performance issues involved in the generation of
auto- mated traffic reports for large IT infrastructures. Such reports allows
the IT manager to proactively detect possible abnormal situations and roll out
the corresponding cor- rective actions. With the ever-increasing bandwidth of
current networks, the design of automated traffic report generation systems is
very challenging. In a first step, the huge volumes of collected traffic are
transformed into enriched flow records obtained from diverse collectors and
dissectors. Then, such flow records, along with time series obtained from the
raw traffic, are further processed to produce a usable report. As will be
shown, the data volume in flow records is very large as well and requires
careful selection of the Key Performance Indicators (KPIs) to be included in
the report. In this regard, we discuss the use of high-level languages versus
low- level approaches, in terms of speed and versatility. Furthermore, our
design approach is targeted for rapid development in commodity hardware, which
is essential to cost-effectively tackle demanding traffic analysis scenarios.
"
1476,Online Evaluation for Effective Web Service Development,"  Development of the majority of the leading web services and software products
today is generally guided by data-driven decisions based on evaluation that
ensures a steady stream of updates, both in terms of quality and quantity.
Large internet companies use online evaluation on a day-to-day basis and at a
large scale. The number of smaller companies using A/B testing in their
development cycle is also growing. Web development across the board strongly
depends on quality of experimentation platforms. In this tutorial, we overview
state-of-the-art methods underlying everyday evaluation pipelines at some of
the leading Internet companies. Software engineers, designers, analysts,
service or product managers --- beginners, advanced specialists, and
researchers --- can learn how to make web service development data-driven and
do it effectively.
"
1477,"Automated Instruction Stream Throughput Prediction for Intel and AMD
  Microarchitectures","  An accurate prediction of scheduling and execution of instruction streams is
a necessary prerequisite for predicting the in-core performance behavior of
throughput-bound loop kernels on out-of-order processor architectures. Such
predictions are an indispensable component of analytical performance models,
such as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a
deep understanding of the performance-relevant interactions between hardware
architecture and loop code. We present the Open Source Architecture Code
Analyzer (OSACA), a static analysis tool for predicting the execution time of
sequential loops comprising x86 instructions under the assumption of an
infinite first-level cache and perfect out-of-order scheduling. We show the
process of building a machine model from available documentation and
semi-automatic benchmarking, and carry it out for the latest Intel Skylake and
AMD Zen micro-architectures. To validate the constructed models, we apply them
to several assembly kernels and compare runtime predictions with actual
measurements. Finally we give an outlook on how the method may be generalized
to new architectures.
"
1478,Towards quantitative methods to assess network generative models,"  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
"
1479,"Power Flow Analysis Using Graph based Combination of Iterative Methods
  and Vertex Contraction Approach","  Compared with relational database (RDB), graph database (GDB) is a more
intuitive expression of the real world. Each node in the GDB is a both storage
and logic unit. Since it is connected to its neighboring nodes through edges,
and its neighboring information could be easily obtained in one-step graph
traversal. It is able to conduct local computation independently and all nodes
can do their local work in parallel. Then the whole system can be maximally
analyzed and assessed in parallel to largely improve the computation
performance without sacrificing the precision of final results. This paper
firstly introduces graph database, power system graph modeling and potential
graph computing applications in power systems. Two iterative methods based on
graph database and PageRank are presented and their convergence are discussed.
Vertex contraction is proposed to improve the performance by eliminating
zero-impedance branch. A combination of the two iterative methods is proposed
to make use of their advantages. Testing results based on a provincial 1425-bus
system demonstrate that the proposed comprehensive approach is a good candidate
for power flow analysis.
"
1480,Scalable Load Balancing Algorithms in Networked Systems,"  A fundamental challenge in large-scale networked systems viz., data centers
and cloud networks is to distribute tasks to a pool of servers, using minimal
instantaneous state information, while providing excellent delay performance.
In this thesis we design and analyze load balancing algorithms that aim to
achieve a highly efficient distribution of tasks, optimize server utilization,
and minimize communication overhead.
"
1481,Slotted ALOHA Overlay on LoRaWAN: a Distributed Synchronization Approach,"  LoRaWAN is one of the most promising standards for IoT applications.
Nevertheless, the high density of end-devices expected for each gateway, the
absence of an effective synchronization scheme between gateway and end-devices,
challenge the scalability of these networks. In this article, we propose to
regulate the communication of LoRaWAN networks using a Slotted-ALOHA (S-ALOHA)
instead of the classic ALOHA approach used by LoRa. The implementation is an
overlay on top of the standard LoRaWAN; thus no modification in pre-existing
LoRaWAN firmware and libraries is necessary. Our method is based on a novel
distributed synchronization service that is suitable for low-cost IoT
end-nodes. S-ALOHA supported by our synchronization service significantly
improves the performance of traditional LoRaWAN networks regarding packet loss
rate and network throughput.
"
1482,Performance Evaluation of the Quorum Blockchain Platform,"  Quorum is a permissioned blockchain platform built from the Ethereum codebase
with adaptations to make it a permissioned consortium platform. It is one of
the key contenders in the permissioned ledger space. Quorum supports
confidentiality and privacy of smart contracts and transactions, and crash and
Byzantine fault tolerant consensus algorithms. In this paper, we characterize
the performance features of Quorum. We study the throughput and latency
characteristics of Quorum with different workloads and consensus algorithms
that it supports. Through a suite of micro-benchmarks, we explore how certain
transaction and smart contract parameters can affect transaction latencies.
"
1483,"Zoom: SSD-based Vector Search for Optimizing Accuracy, Latency and
  Memory","  With the advancement of machine learning and deep learning, vector search
becomes instrumental to many information retrieval systems, to search and find
best matches to user queries based on their semantic similarities.These online
services require the search architecture to be both effective with high
accuracy and efficient with low latency and memory footprint, which existing
work fails to offer. We develop, Zoom, a new vector search solution that
collaboratively optimizes accuracy, latency and memory based on a multiview
approach. (1) A ""preview"" step generates a small set of good candidates,
leveraging compressed vectors in memory for reduced footprint and fast lookup.
(2) A ""fullview"" step on SSDs reranks those candidates with their full-length
vector, striking high accuracy. Our evaluation shows that, Zoom achieves an
order of magnitude improvements on efficiency while attaining equal or higher
accuracy, comparing with the state-of-the-art.
"
1484,Feature-Specific Profiling,"  While high-level languages come with significant readability and
maintainability benefits, their performance remains difficult to predict. For
example, programmers may unknowingly use language features inappropriately,
which cause their programs to run slower than expected. To address this issue,
we introduce feature-specific profiling, a technique that reports performance
costs in terms of linguistic constructs. Feature-specific profilers help
programmers find expensive uses of specific features of their language. We
describe the architecture of a profiler that implements our approach, explain
prototypes of the profiler for two languages with different characteristics and
implementation strategies, and provide empirical evidence for the approach's
general usefulness as a performance debugging tool.
"
1485,"Poster Abstract: LPWA-MAC - a Low Power Wide Area network MAC protocol
  for cyber-physical systems","  Low-Power Wide-Area Networks (LPWANs) are being successfully used for the
monitoring of large-scale systems that are delay-tolerant and which have
low-bandwidth requirements. The next step would be instrumenting these for the
control of Cyber-Physical Systems (CPSs) distributed over large areas which
require more bandwidth, bounded delays and higher reliability or at least more
rigorous guarantees therein. This paper presents LPWA-MAC, a novel Low Power
Wide-Area network MAC protocol, that ensures bounded end-to-end delays, high
channel utility and supports many of the different traffic patterns and
data-rates typical of CPS.
"
1486,"I/O Workload Management for All-Flash Datacenter Storage Systems Based
  on Total Cost of Ownership","  Recently, the capital expenditure of flash-based Solid State Driver (SSDs)
keeps declining and the storage capacity of SSDs keeps increasing. As a result,
all-flash storage systems have started to become more economically viable for
large shared storage installations in datacenters, where metrics like Total
Cost of Ownership (TCO) are of paramount importance. On the other hand, flash
devices suffer from write amplification, which, if unaccounted, can
substantially increase the TCO of a storage system. In this paper, we first
develop a TCO model for datacenter all-flash storage systems, and then plug a
Write Amplification model (WAF) of NVMe SSDs we build based on empirical data
into this TCO model. Our new WAF model accounts for workload characteristics
like write rate and percentage of sequential writes. Furthermore, using both
the TCO and WAF models as the optimization criterion, we design new flash
resource management schemes (MINTCO) to guide datacenter managers to make
workload allocation decisions under the consideration of TCO for SSDs. Based on
that, we also develop MINTCO-RAID to support RAID SSDs and MINTCO-OFFLINE to
optimize the offline workload-disk deployment problem during the initialization
phase. Experimental results show that MINTCO can reduce the TCO and keep
relatively high throughput and space utilization of the entire datacenter
storage resources.
"
1487,SECS: Efficient Deep Stream Processing via Class Skew Dichotomy,"  Despite that accelerating convolutional neural network (CNN) receives an
increasing research focus, the save on resource consumption always comes with a
decrease in accuracy. To both increase accuracy and decrease resource
consumption, we explore an environment information, called class skew, which is
easily available and exists widely in daily life. Since the class skew may
switch as time goes, we bring up probability layer to utilize class skew
without any overhead during the runtime. Further, we observe class skew
dichotomy that some class skew may appear frequently in the future, called hot
class skew, and others will never appear again or appear seldom, called cold
class skew. Inspired by techniques from source code optimization, two modes,
i.e., interpretation and compilation, are proposed. The interpretation mode
pursues efficient adaption during runtime for cold class skew and the
compilation mode aggressively optimize on hot ones for more efficient
deployment in the future. Aggressive optimization is processed by
class-specific pruning and provides extra benefit. Finally, we design a
systematic framework, SECS, to dynamically detect class skew, processing
interpretation and compilation, as well as select the most accurate
architectures under the runtime resource budget. Extensive evaluations show
that SECS can realize end-to-end classification speedups by a factor of 3x to
11x relative to state-of-the-art convolutional neural networks, at a higher
accuracy.
"
1488,"FastDeepIoT: Towards Understanding and Optimizing Neural Network
  Execution Time on Mobile and Embedded Devices","  Deep neural networks show great potential as solutions to many sensing
application problems, but their excessive resource demand slows down execution
time, pausing a serious impediment to deployment on low-end devices. To address
this challenge, recent literature focused on compressing neural network size to
improve performance. We show that changing neural network size does not
proportionally affect performance attributes of interest, such as execution
time. Rather, extreme run-time nonlinearities exist over the network
configuration space. Hence, we propose a novel framework, called FastDeepIoT,
that uncovers the non-linear relation between neural network structure and
execution time, then exploits that understanding to find network configurations
that significantly improve the trade-off between execution time and accuracy on
mobile and embedded devices. FastDeepIoT makes two key contributions. First,
FastDeepIoT automatically learns an accurate and highly interpretable execution
time model for deep neural networks on the target device. This is done without
prior knowledge of either the hardware specifications or the detailed
implementation of the used deep learning library. Second, FastDeepIoT informs a
compression algorithm how to minimize execution time on the profiled device
without impacting accuracy. We evaluate FastDeepIoT using three different
sensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.
FastDeepIoT further reduces the neural network execution time by $48\%$ to
$78\%$ and energy consumption by $37\%$ to $69\%$ compared with the
state-of-the-art compression algorithms.
"
1489,"Characterising Across-Stack Optimisations for Deep Convolutional Neural
  Networks","  Convolutional Neural Networks (CNNs) are extremely computationally demanding,
presenting a large barrier to their deployment on resource-constrained devices.
Since such systems are where some of their most useful applications lie (e.g.
obstacle detection for mobile robots, vision-based medical assistive
technology), significant bodies of work from both machine learning and systems
communities have attempted to provide optimisations that will make CNNs
available to edge devices. In this paper we unify the two viewpoints in a Deep
Learning Inference Stack and take an across-stack approach by implementing and
evaluating the most common neural network compression techniques (weight
pruning, channel pruning, and quantisation) and optimising their parallel
execution with a range of programming approaches (OpenMP, OpenCL) and hardware
architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct
trade-offs under constraints of accuracy, execution time, and memory space.
"
1490,"Next Stop ""NoOps"": Enabling Cross-System Diagnostics Through Graph-based
  Composition of Logs and Metrics","  Performing diagnostics in IT systems is an increasingly complicated task, and
it is not doable in satisfactory time by even the most skillful operators.
Systems and their architecture change very rapidly in response to business and
user demand. Many organizations see value in the maintenance and management
model of NoOps that stands for No Operations. One of the implementations of
this model is a system that is maintained automatically without any human
intervention. The path to NoOps involves not only precise and fast diagnostics
but also reusing as much knowledge as possible after the system is reconfigured
or changed. The biggest challenge is to leverage knowledge on one IT system and
reuse this knowledge for diagnostics of another, different system. We propose a
framework of weighted graphs which can transfer knowledge, and perform
high-quality diagnostics of IT systems. We encode all possible data in a graph
representation of a system state and automatically calculate weights of these
graphs. Then, thanks to the evaluation of similarity between graphs, we
transfer knowledge about failures from one system to another and use it for
diagnostics. We successfully evaluate the proposed approach on Spark, Hadoop,
Kafka and Cassandra systems.
"
1491,"On the Fly Orchestration of Unikernels: Tuning and Performance
  Evaluation of Virtual Infrastructure Managers","  Network operators are facing significant challenges meeting the demand for
more bandwidth, agile infrastructures, innovative services, while keeping costs
low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as
key trends of 5G network architectures, providing flexibility, fast
instantiation times, support of Commercial Off The Shelf hardware and
significant cost savings. NFV leverages Cloud Computing principles to move the
data-plane network functions from expensive, closed and proprietary hardware to
the so-called Virtual Network Functions (VNFs). In this paper we deal with the
management of virtual computing resources (Unikernels) for the execution of
VNFs. This functionality is performed by the Virtual Infrastructure Manager
(VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We
discuss the instantiation process of virtual resources and propose a generic
reference model, starting from the analysis of three open source VIMs, namely
OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing
the support for special-purpose Unikernels and aiming at reducing the duration
of the instantiation process. We evaluate some performance aspects of the VIMs,
considering both stock and tuned versions. The VIM extensions and performance
evaluation tools are available under a liberal open source licence.
"
1492,"FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why","  Winograd-based convolution has quickly gained traction as a preferred
approach to implement convolutional neural networks (ConvNet) on various
hardware platforms because it requires fewer floating point operations than
FFT-based or direct convolutions.
  This paper compares three highly optimized implementations (regular FFT--,
Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and
many--core CPUs. Although all three implementations employed the same
optimizations for modern CPUs, our experimental results with two popular
ConvNets (VGG and AlexNet) show that the FFT--based implementations generally
outperform the Winograd--based approach, contrary to the popular belief.
  To understand the results, we use a Roofline performance model to analyze the
three implementations in detail, by looking at each of their computation phases
and by considering not only the number of floating point operations, but also
the memory bandwidth and the cache sizes. The performance analysis explains
why, and under what conditions, the FFT--based implementations outperform the
Winograd--based one, on modern CPUs.
"
1493,SCOPE: C3SR Systems Characterization and Benchmarking Framework,"  This report presents the design of the Scope infrastructure for extensible
and portable benchmarking. Improvements in high- performance computing systems
rely on coordination across different levels of system abstraction. Developing
and defining accurate performance measurements is necessary at all levels of
the system hierarchy, and should be as accessible as possible to developers
with different backgrounds. The Scope project aims to lower the barrier to
entry for developing performance benchmarks by providing a software
architecture that allows benchmarks to be developed independently, by providing
useful C/C++ abstractions and utilities, and by providing a Python package for
generating publication-quality plots of resulting measurements.
"
1494,SDN Flow Entry Management Using Reinforcement Learning,"  Modern information technology services largely depend on cloud
infrastructures to provide their services. These cloud infrastructures are
built on top of datacenter networks (DCNs) constructed with high-speed links,
fast switching gear, and redundancy to offer better flexibility and resiliency.
In this environment, network traffic includes long-lived (elephant) and
short-lived (mice) flows with partitioned and aggregated traffic patterns.
Although SDN-based approaches can efficiently allocate networking resources for
such flows, the overhead due to network reconfiguration can be significant.
With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in
an OpenFlow enabled switch, it is crucial to determine which forwarding rules
should remain in the flow table, and which rules should be processed by the SDN
controller in case of a table-miss on the SDN switch. This is needed in order
to obtain the flow entries that satisfy the goal of reducing the long-term
control plane overhead introduced between the controller and the switches. To
achieve this goal, we propose a machine learning technique that utilizes two
variations of reinforcement learning (RL) algorithms-the first of which is
traditional reinforcement learning algorithm based while the other is deep
reinforcement learning based. Emulation results using the RL algorithm show
around 60% improvement in reducing the long-term control plane overhead, and
around 14% improvement in the table-hit ratio compared to the Multiple Bloom
Filters (MBF) method given a fixed size flow table of 4KB.
"
1495,Power and Energy-efficiency Roofline Model for GPUs,"  Energy consumption has been a great deal of concern in recent years and
developers need to take energy-efficiency into account when they design
algorithms. Their design needs to be energy-efficient and low-power while it
tries to achieve attainable performance provided by underlying hardware.
However, different optimization techniques have different effects on power and
energy-efficiency and a visual model would assist in the selection process.
  In this paper, we extended the roofline model and provided a visual
representation of optimization strategies for power consumption. Our model is
composed of various ceilings regarding each strategy we included in our models.
One roofline model for computational performance and one for memory performance
is introduced. We assembled our models based on some optimization strategies
for two widespread GPUs from NVIDIA: Geforce GTX 970 and Tesla K80.
"
1496,"Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural
  Networks","  Quantization of weights and activations in Deep Neural Networks (DNNs) is a
powerful technique for network compression, and has enjoyed significant
attention and success. However, much of the inference-time benefit of
quantization is accessible only through the use of customized hardware
accelerators or by providing an FPGA implementation of quantized arithmetic.
  Building on prior work, we show how to construct arbitrary bit-precise signed
and unsigned integer operations using a software technique which logically
\emph{embeds} a vector architecture with custom bit-width lanes in universally
available fixed-width scalar arithmetic.
  We evaluate our approach on a high-end Intel Haswell processor, and an
embedded ARM processor. Our approach yields very fast implementations of
bit-precise custom DNN operations, which often match or exceed the performance
of operations quantized to the sizes supported in native arithmetic. At the
strongest level of quantization, our approach yields a maximum speedup of
$\thicksim6\times$ on the Intel platform, and $\thicksim10\times$ on the ARM
platform versus quantization to native 8-bit integers.
"
1497,Predicting the confirmation time of Bitcoin transactions,"  We study the probabilistic distribution of the confirmation time of Bitcoin
transactions, conditional on the current memory pool (i.e., the queue of
transactions awaiting confirmation). The results of this paper are particularly
interesting for users that want to make a Bitcoin transaction during
`heavy-traffic situations', when the transaction demand exceeds the block
capacity. In such situations, Bitcoin users tend to bid up the transaction
fees, in order to gain priority over other users that pay a lower fee. We argue
that the time until a Bitcoin transaction is confirmed can be modelled as a
particular stochastic fluid queueing process (to be precise: a
Cram\'er-Lundberg process). We approximate the queueing process in two
different ways. The first approach leads to a lower bound on the confirmation
probability, which becomes increasingly tight as traffic decreases. The second
approach relies on a diffusion approximation with a continuity correction,
which becomes increasingly accurate as traffic intensifies. The accuracy of the
approximations under different traffic loads are evaluated in a simulation
study.
"
1498,Is Your Load Generator Launching Web Requests in Bunches?,"  One problem with load test quality, almost always overlooked, is the
potential for the load generator's user thread pool to sync up and dispatch
queries in bunches rather than independently from each other like real users
initiate their requests. A spiky launch pattern misrepresents workload flow as
well as yields erroneous application response time statistics. This paper
describes what a real user request timing pattern looks like, illustrates how
to identify it in the load generation environment, and exercises a free
downloadable tool which measures how well the load generator is mimicking the
timing pattern of real web user requests.
"
1499,New Thread Migration Strategies for NUMA Systems,"  Multicore systems present on-board memory hierarchies and communication
networks that influence performance when executing shared memory parallel
codes. Characterising this influence is complex, and understanding the effect
of particular hardware configurations on different codes is of paramount
importance. In previous works, monitoring information extracted from hardware
counters at runtime has been used to characterise the behaviour of each thread
in the parallel code in terms of the number of floating point operations per
second, operational intensity, and latency of memory access. We propose to use
this information to guide thread migration strategies that improve execution
efficiency by increasing locality and affinity. Different configurations of NAS
Parallel OpenMP benchmarks on multicores were used to validate the benefits of
the proposed thread migration strategies. Our proposed strategies produce up to
70% improvement in scenarios where locality and affinity are low, there being a
small degradation in performance for codes with high locality and affinity.
"
1500,"On Minimizing the Completion Times of Long Flows over Inter-Datacenter
  WAN","  Long flows contribute huge volumes of traffic over inter-datacenter WAN. The
Flow Completion Time (FCT) is a vital network performance metric that affects
the running time of distributed applications and the users' quality of
experience. Flow routing techniques based on propagation or queuing latency or
instantaneous link utilization are insufficient for minimization of the long
flows' FCT. We propose a routing approach that uses the remaining sizes and
paths of all ongoing flows to minimize the worst-case completion time of
incoming flows assuming no knowledge of future flow arrivals. Our approach can
be formulated as an NP-Hard graph optimization problem. We propose BWRH, a
heuristic to quickly generate an approximate solution. We evaluate BWRH against
several real WAN topologies and two different traffic patterns. We see that
BWRH provides solutions with an average optimality gap of less than $0.25\%$.
Furthermore, we show that compared to other popular routing heuristics, BWRH
reduces the mean and tail FCT by up to $1.46\times$ and $1.53\times$,
respectively.
"
1501,"Fault Tolerant Adaptive Parallel and Distributed Simulation through
  Functional Replication","  This paper presents FT-GAIA, a software-based fault-tolerant parallel and
distributed simulation middleware. FT-GAIA has being designed to reliably
handle Parallel And Distributed Simulation (PADS) models, which are needed to
properly simulate and analyze complex systems arising in any kind of scientific
or engineering field. PADS takes advantage of multiple execution units run in
multicore processors, cluster of workstations or HPC systems. However, large
computing systems, such as HPC systems that include hundreds of thousands of
computing nodes, have to handle frequent failures of some components. To cope
with this issue, FT-GAIA transparently replicates simulation entities and
distributes them on multiple execution nodes. This allows the simulation to
tolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some
protection against Byzantine failures, since interaction messages among the
simulated entities are replicated as well, so that the receiving entity can
identify and discard corrupted messages. Results from an analytical model and
from an experimental evaluation show that FT-GAIA provides a high degree of
fault tolerance, at the cost of a moderate increase in the computational load
of the execution units.
"
1502,"Heterogeneous MacroTasking (HeMT) for Parallel Processing in the Public
  Cloud","  Using tiny, equal-sized tasks (Homogeneous microTasking, HomT) has long been
regarded an effective way of load balancing in parallel computing systems. When
combined with nodes pulling in work upon becoming idle, HomT has the desirable
property of automatically adapting its load distribution to the processing
capacities of participating nodes - more powerful nodes finish their work
sooner and, therefore, pull in additional work faster. As a result, HomT is
deemed especially desirable in settings with heterogeneous (and possibly
possessing dynamically changing) processing capacities. However, HomT does have
additional scheduling and I/O overheads that might make this load balancing
scheme costly in some scenarios. In this paper, we first analyze these
advantages and disadvantages of HomT. We then propose an alternative load
balancing scheme - Heterogeneous MacroTasking (HeMT) - wherein workload is
intentionally partitioned according to nodes' processing capacity. Our goal is
to study when HeMT is able to overcome the performance disadvantages of HomT.
We implement a prototype of HeMT within the Apache Spark application framework
with complementary enhancements to the Apache Mesos cluster manager. Spark's
built-in scheduler, when parameterized appropriately, implements HomT. Our
experimental results show that HeMT out-performs HomT when accurate
workload-specific estimates of nodes' processing capacities are learned. As
representative results, Spark with HeMT offers about 10% better average
completion times for realistic data processing workloads over the default
system.
"
1503,The Effect of Data Marshalling on Computation Offloading Decisions,"  We conducted an extensive set of experiments with an offloading testbed to
understand the impact that data marshalling techniques have on computation
offloading decisions. We find that the popular JSON format to marshall data
between client and server comes at a significant computational expense compared
to a minimalistic raw data transfer. The computational time is significant in
that it affects computation offloading decisions in a variety of conditions. We
outline some of these conditions.
"
1504,"A Droplet Approach Based on Raptor Codes for Distributed Computing With
  Straggling Servers","  We propose a coded distributed computing scheme based on Raptor codes to
address the straggler problem. In particular, we consider a scheme where each
server computes intermediate values, referred to as droplets, that are either
stored locally or sent over the network. Once enough droplets are collected,
the computation can be completed. Compared to previous schemes in the
literature, our proposed scheme achieves lower computational delay when the
decoding time is taken into account.
"
1505,To Use or Not to Use: CPUs' Cache Optimization Techniques on GPGPUs,"  General Purpose Graphic Processing Unit(GPGPU) is used widely for achieving
high performance or high throughput in parallel programming. This capability of
GPGPUs is very famous in the new era and mostly used for scientific computing
which requires more processing power than normal personal computers. Therefore,
most of the programmers, researchers and industry use this new concept for
their work. However, achieving high-performance or high-throughput using GPGPUs
are not an easy task compared with conventional programming concepts in the CPU
side. In this research, the CPU's cache memory optimization techniques have
been adopted to the GPGPU's cache memory to identify rare performance
improvement techniques compared to GPGPU's best practices. The cache
optimization techniques of blocking, loop fusion, array merging and array
transpose were tested on GPGPUs for finding suitability of these techniques.
Finally, we identified that some of the CPU cache optimization techniques go
well with the cache memory system of the GPGPU and shows performance
improvements while some others show the opposite effect on the GPGPUs compared
with the CPUs.
"
1506,"Performance analysis and optimization of the JOREK code for many-core
  CPUs","  This report investigates the performance of the JOREK code on the Intel
Knights Landing and Skylake processor architectures. The OpenMP scaling of the
matrix construction part of the code was analyzed and improved synchronization
methods were implemented. A new switch was implemented to control the number of
threads used for the linear equation solver independently from other parts of
the code. The matrix construction subroutine was vectorized, and the data
locality was also improved. These steps led to a factor of two speedup for the
matrix construction.
"
1507,"LIRS: Enabling efficient machine learning on NVM-based storage via a
  lightweight implementation of random shuffling","  Machine learning algorithms, such as Support Vector Machine (SVM) and Deep
Neural Network (DNN), have gained a lot of interests recently. When training a
machine learning algorithm, randomly shuffle all the training data can improve
the testing accuracy and boost the convergence rate. Nevertheless, realizing
training data random shuffling in a real system is not a straightforward
process due to the slow random accesses in hard disk drive (HDD). To avoid
frequent random disk access, the effect of random shuffling is often limited in
existing approaches. With the emerging non-volatile memory-based storage
device, such as Intel Optane SSD, which provides fast random accesses, we
propose a lightweight implementation of random shuffling (LIRS) to randomly
shuffle the indexes of the entire training dataset, and the selected training
instances are directly accessed from the storage and packed into batches.
Experimental results show that LIRS can reduce the total training time of SVM
and DNN by 49.9% and 43.5% on average, and improve the final testing accuracy
on DNN by 1.01%.
"
1508,"uops.info: Characterizing Latency, Throughput, and Port Usage of
  Instructions on Intel Microarchitectures","  Modern microarchitectures are some of the world's most complex man-made
systems. As a consequence, it is increasingly difficult to predict, explain,
let alone optimize the performance of software running on such
microarchitectures. As a basis for performance predictions and optimizations,
we would need faithful models of their behavior, which are, unfortunately,
seldom available.
  In this paper, we present the design and implementation of a tool to
construct faithful models of the latency, throughput, and port usage of x86
instructions. To this end, we first discuss common notions of instruction
throughput and port usage, and introduce a more precise definition of latency
that, in contrast to previous definitions, considers dependencies between
different pairs of input and output operands. We then develop novel algorithms
to infer the latency, throughput, and port usage based on
automatically-generated microbenchmarks that are more accurate and precise than
existing work.
  To facilitate the rapid construction of optimizing compilers and tools for
performance prediction, the output of our tool is provided in a
machine-readable format. We provide experimental results for processors of all
generations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake,
and discuss various cases where the output of our tool differs considerably
from prior work.
"
1509,"Of Kernels and Queues: when network calculus meets analytic
  combinatorics","  Stochastic network calculus is a tool for computing error bounds on the
performance of queueing systems. However, deriving accurate bounds for networks
consisting of several queues or subject to non-independent traffic inputs is
challenging. In this paper, we investigate the relevance of the tools from
analytic combinatorics, especially the kernel method, to tackle this problem.
Applying the kernel method allows us to compute the generating functions of the
queue state distributions in the stationary regime of the network. As a
consequence, error bounds with an arbitrary precision can be computed. In this
preliminary work, we focus on simple examples which are representative of the
difficulties that the kernel method allows us to overcome.
"
1510,"SDN Architecture and Southbound APIs for IPv6 Segment Routing Enabled
  Wide Area Networks","  The SRv6 architecture (Segment Routing based on IPv6 data plane) is a
promising solution to support services like Traffic Engineering, Service
Function Chaining and Virtual Private Networks in IPv6 backbones and
datacenters. The SRv6 architecture has interesting scalability properties as it
reduces the amount of state information that needs to be configured in the
nodes to support the network services. In this paper, we describe the
advantages of complementing the SRv6 technology with an SDN based approach in
backbone networks. We discuss the architecture of a SRv6 enabled network based
on Linux nodes. In addition, we present the design and implementation of the
Southbound API between the SDN controller and the SRv6 device. We have defined
a data-model and four different implementations of the API, respectively based
on gRPC, REST, NETCONF and remote Command Line Interface (CLI). Since it is
important to support both the development and testing aspects we have realized
an Intent based emulation system to build realistic and reproducible
experiments. This collection of tools automate most of the configuration
aspects relieving the experimenter from a significant effort. Finally, we have
realized an evaluation of some performance aspects of our architecture and of
the different variants of the Southbound APIs and we have analyzed the effects
of the configuration updates in the SRv6 enabled nodes.
"
1511,CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles,"  Connected and autonomous vehicles (CAVs) have recently attracted a
significant amount of attention both from researchers and industry. Numerous
studies targeting algorithms, software frameworks, and applications on the CAVs
scenario have emerged. Meanwhile, several pioneer efforts have focused on the
edge computing system and architecture design for the CAVs scenario and
provided various heterogeneous platform prototypes for CAVs. However, a
standard and comprehensive application benchmark for CAVs is missing, hindering
the study of these emerging computing systems. To address this challenging
problem, we present CAVBench, the first benchmark suite for the edge computing
system in the CAVs scenario. CAVBench is comprised of six typical applications
covering four dominate CAVs scenarios and takes four datasets as standard
input. CAVBench provides quantitative evaluation results via application and
system perspective output metrics. We perform a series of experiments and
acquire three systemic characteristics of the applications in CAVBench. First,
the operation intensity of the applications is polarized, which explains why
heterogeneous hardware is important for a CAVs computing system. Second, all
applications in CAVBench consume high memory bandwidth, so the system should be
equipped with high bandwidth memory or leverage good memory bandwidth
management to avoid the performance degradation caused by memory bandwidth
competition. Third, some applications have worse data/instruction locality
based on the cache miss observation, so the computing system targeting these
applications should optimize the cache architecture. Last, we use the CAVBench
to evaluate a typical edge computing platform and present the quantitative and
qualitative analysis of the benchmarking results.
"
1512,Implementation and Analysis of QUIC for MQTT,"  Transport and security protocols are essential to ensure reliable and secure
communication between two parties. For IoT applications, these protocols must
be lightweight, since IoT devices are usually resource constrained.
Unfortunately, the existing transport and security protocols -- namely TCP/TLS
and UDP/DTLS -- fall short in terms of connection overhead, latency, and
connection migration when used in IoT applications. In this paper, after
studying the root causes of these shortcomings, we show how utilizing QUIC in
IoT scenarios results in a higher performance. Based on these observations, and
given the popularity of MQTT as an IoT application layer protocol, we integrate
MQTT with QUIC. By presenting the main APIs and functions developed, we explain
how connection establishment and message exchange functionalities work. We
evaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless,
and long-distance testbeds. Our results show that MQTTw/QUIC reduces connection
overhead in terms of the number of packets exchanged with the broker by up to
56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces
processor and memory usage by up to 83% and 50%, respectively. Furthermore, by
removing the head-of-line blocking problem, delivery latency is reduced by up
to 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a
connection migration happens is considerably lower than that of MQTTw/TCP.
"
1513,Load balancing with heterogeneous schedulers,"  Load balancing is a common approach in web server farms or inventory routing
problems. An important issue in such systems is to determine the server to
which an incoming request should be routed to optimize a given performance
criteria. In this paper, we assume the server's scheduling disciplines to be
heterogeneous. More precisely, a server implements a scheduling discipline
which belongs to the class of limited processor sharing (LPS-$d$) scheduling
disciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and
hence, includes as special cases First Come First Served ($d=1$) and Processor
Sharing ($d=\infty$).
  In order to obtain efficient heuristics, we model the above load-balancing
framework as a multi-armed restless bandit problem. Using the relaxation
technique, as first developed in the seminal work of Whittle, we derive
Whittle's index policy for general cost functions and obtain a closed-form
expression for Whittle's index in terms of the steady-state distribution.
Through numerical computations, we investigate the performance of Whittle's
index with two different performance criteria: linear cost criterion and a cost
criterion that depends on the first and second moment of the throughput. Our
results show that \emph{(i)} the structure of Whittle's index policy can
strongly depend on the scheduling discipline implemented in the server, i.e.,
on $d$, and that \emph{(ii)} Whittle's index policy significantly outperforms
standard dispatching rules such as Join the Shortest Queue (JSQ), Join the
Shortest Expected Workload (JSEW), and Random Server allocation (RSA).
"
1514,"To Compress, or Not to Compress: Characterizing Deep Learning Model
  Compression for Embedded Inference","  The recent advances in deep neural networks (DNNs) make them attractive for
embedded systems. However, it can take a long time for DNNs to make an
inference on resource-constrained computing devices. Model compression
techniques can address the computation issue of deep inference on embedded
devices. This technique is highly attractive, as it does not rely on
specialized hardware, or computation-offloading that is often infeasible due to
privacy concerns or high latency. However, it remains unclear how model
compression techniques perform across a wide range of DNNs. To design efficient
embedded deep learning solutions, we need to understand their behaviors. This
work develops a quantitative approach to characterize model compression
techniques on a representative embedded deep learning architecture, the NVIDIA
Jetson Tx2. We perform extensive experiments by considering 11 influential
neural network architectures from the image classification and the natural
language processing domains. We experimentally show that how two mainstream
compression techniques, data quantization and pruning, perform on these network
architectures and the implications of compression techniques to the model
storage size, inference time, energy consumption and performance metrics. We
demonstrate that there are opportunities to achieve fast deep inference on
embedded systems, but one must carefully choose the compression settings. Our
results provide insights on when and how to apply model compression techniques
and guidelines for designing efficient embedded deep learning systems.
"
1515,RCanopus: Making Canopus Resilient to Failures and Byzantine Faults,"  Distributed consensus is a key enabler for many distributed systems including
distributed databases and blockchains. Canopus is a scalable distributed
consensus protocol that ensures that live nodes in a system agree on an ordered
sequence of operations (called transactions). Unlike most prior consensus
protocols, Canopus does not rely on a single leader. Instead, it uses a virtual
tree overlay for message dissemination to limit network traffic across
oversubscribed links. It leverages hardware redundancies, both within a rack
and inside the network fabric, to reduce both protocol complexity and
communication overhead. These design decisions enable Canopus to support large
deployments without significant performance degradation.
  The existing Canopus protocol is resilient in the face of node and
communication failures, but its focus is primarily on performance, so does not
respond well to other types of failures. For example, the failure of a single
rack of servers causes all live nodes to stall. The protocol is also open to
attack by Byzantine nodes, which can cause different live nodes to conclude the
protocol with different transaction orders. In this paper, we describe RCanopus
(`resilent Canopus') which extends Canopus to add liveness, that is, allowing
live nodes to make progress, when possible, despite many types of failures.
This requires RCanopus to accurately detect and recover from failure despite
using unreliable failure detectors, and tolerance of Byzantine attacks. Second,
RCanopus guarantees safety, that is, agreement amongst live nodes of
transaction order, in the presence of Byzantine attacks and network
partitioning.
"
1516,Data Motif-based Proxy Benchmarks for Big Data and AI Workloads,"  For the architecture community, reasonable simulation time is a strong
requirement in addition to performance data accuracy. However, emerging big
data and AI workloads are too huge at binary size level and prohibitively
expensive to run on cycle-accurate simulators. The concept of data motif, which
is identified as a class of units of computation performed on initial or
intermediate data, is the first step towards building proxy benchmark to mimic
the real-world big data and AI workloads. However, there is no practical way to
construct a proxy benchmark based on the data motifs to help simulation-based
research. In this paper, we embark on a study to bridge the gap between data
motif and a practical proxy benchmark. We propose a data motif-based proxy
benchmark generating methodology by means of machine learning method, which
combine data motifs with different weights to mimic the big data and AI
workloads. Furthermore, we implement various data motifs using light-weight
stacks and apply the methodology to five real-world workloads to construct a
suite of proxy benchmarks, considering the data types, patterns, and
distributions. The evaluation results show that our proxy benchmarks shorten
the execution time by 100s times on real systems while maintaining the average
system and micro-architecture performance data accuracy above 90%, even
changing the input data sets or cluster configurations. Moreover, the generated
proxy benchmarks reflect consistent performance trends across different
architectures. To facilitate the community, we will release the proxy
benchmarks on the project homepage http://prof.ict.ac.cn/BigDataBench.
"
1517,"Exploiting the Space Filling Curve Ordering of Particles in the
  Neighbour Search of Gadget3","  Gadget3 is nowadays one of the most frequently used high performing parallel
codes for cosmological hydrodynamical simulations. Recent analyses have shown
t\ hat the Neighbour Search process of Gadget3 is one of the most
time-consuming parts. Thus, a considerable speedup can be expected from
improvements of the u\ nderlying algorithms. In this work we propose a novel
approach for speeding up the Neighbour Search which takes advantage of the
space-filling-curve particle ordering. Instead of performing Neighbour Search
for all particles individually, nearby active particles can be grouped and one
single Neighbour Search can be performed to obta\ in a common superset of
neighbours. Thus, with this approach we reduce the number of searches. On the
other hand, tree walks are performed within a larger searching radius. There is
an optimal size of grouping that maximize the speedup, which we found by
numerical experiments. We tested the algorithm within the boxes of the
Magneticum project. As a result we obtained a speedup of $1.65$ in the Density
and of $1.30$ in the Hydrodynamics computation, respectively, and a total
speedup of $1.34.$
"
1518,Distilling with Performance Enhanced Students,"  The task of accelerating large neural networks on general purpose hardware
has, in recent years, prompted the use of channel pruning to reduce network
size. However, the efficacy of pruning based approaches has since been called
into question. In this paper, we turn to distillation for model
compression---specifically, attention transfer---and develop a simple method
for discovering performance enhanced student networks. We combine channel
saliency metrics with empirical observations of runtime performance to design
more accurate networks for a given latency budget. We apply our methodology to
residual and densely-connected networks, and show that we are able to find
resource-efficient student networks on different hardware platforms while
maintaining very high accuracy. These performance-enhanced student networks
achieve up to 10% boosts in top-1 ImageNet accuracy over their channel-pruned
counterparts for the same inference time.
"
1519,"Improving OpenCL Performance by Specializing Compiler Phase Selection
  and Ordering","  Automatic compiler phase selection/ordering has traditionally been focused on
CPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler
phase ordering specialization of OpenCL kernels targeting a GPU. We use
iterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks
to an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various
versions to identify the main causes of the most significant improvements and
present results of a set of experiments that demonstrate the importance of
using specific phase orders. Using specialized compiler phase orders, we were
able to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x
(up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions
of the same kernels, and over execution of the OpenCL kernels compiled from
source with the NVIDIA OpenCL driver, respectively. We also evaluate the use of
code-features in the OpenCL kernels. More specifically, we evaluate an approach
that achieves geometric mean improvements of 1.49x and 1.56x over the same
OpenCL baseline, by using the compiler sequences of the 1 or 3 most similar
benchmarks, respectively.
"
1520,"Offloading Execution from Edge to Cloud: a Dynamic Node-RED Based
  Approach","  Fog computing enables use cases where data produced in end devices are
stored, processed, and acted on directly at the edges of the network, yet
computation can be offloaded to more powerful instances through the edge to
cloud continuum. Such offloading mechanism is especially needed in case of
modern multi-purpose IoT gateways, where both demand and operation conditions
can vary largely between deployments. To facilitate the development and
operations of gateways, we implement offloading directly as part of the IoT
rapid prototyping process embedded in the software stack, based on Node-RED. We
evaluate the implemented method using an image processing example, and compare
various offloading strategies based on resource consumption and other system
metrics, highlighting the differences in handling demand and service levels
reached.
"
1521,Learning with Analytical Models,"  To understand and predict the performance of scientific applications, several
analytical and machine learning approaches have been proposed, each having its
advantages and disadvantages. In this paper, we propose and validate a hybrid
approach for performance modeling and prediction, which combines analytical and
machine learning models. The proposed hybrid model aims to minimize prediction
cost while providing reasonable prediction accuracy. Our validation results
show that the hybrid model is able to learn and correct the analytical models
to better match the actual performance. Furthermore, the proposed hybrid model
improves the prediction accuracy in comparison to pure machine learning
techniques while using small training datasets, thus making it suitable for
hardware and workload changes.
"
1522,"FFT, FMM, and Multigrid on the Road to Exascale: performance challenges
  and opportunities","  FFT, FMM, and multigrid methods are widely used fast and highly scalable
solvers for elliptic PDEs. However, emerging large-scale computing systems are
introducing challenges in comparison to current petascale computers. Recent
efforts (Dongarra et al. 2011) have identified several constraints in the
design of exascale software that includes massive concurrency, resilience
management, exploiting the high performance of heterogeneous systems, energy
efficiency, and utilizing the deeper and more complex memory hierarchy expected
at exascale. In this paper, we perform a model-based comparison of the FFT,
FMM, and multigrid methods in the context of these projected constraints. In
addition, we use performance models to offer predictions about the expected
performance on upcoming exascale system configurations based on current
technology trends.
"
1523,"Accurate, Efficient and Scalable Graph Embedding","  The Graph Convolutional Network (GCN) model and its variants are powerful
graph embedding tools for facilitating classification and clustering on graphs.
However, a major challenge is to reduce the complexity of layered GCNs and make
them parallelizable and scalable on very large graphs -- state-of the art
techniques are unable to achieve scalability without losing accuracy and
efficiency. In this paper, we propose novel parallelization techniques for
graph sampling-based GCNs that achieve superior scalable performance on very
large graphs without compromising accuracy. Specifically, our GCN guarantees
work-efficient training and produces order of magnitude savings in computation
and communication. To scale GCN training on tightly-coupled shared memory
systems, we develop parallelization strategies for the key steps in training:
For the graph sampling step, we exploit parallelism within and across multiple
sampling instances, and devise an efficient data structure for concurrent
accesses that provides theoretical guarantee of near-linear speedup with number
of processing units. For the feature propagation step within the sampled graph,
we improve cache utilization and reduce DRAM communication by data
partitioning. We prove that our partitioning strategy is a 2-approximation for
minimizing the communication time compared to the optimal strategy. We
demonstrate that our parallel graph embedding outperforms state-of-the-art
methods in scalability (with respect to number of processors, graph size and
GCN model size), efficiency and accuracy on several large datasets. On a
40-core Xeon platform, our parallel training achieves $64\times$ speedup (with
AVX) in the sampling step and $25\times$ speedup in the feature propagation
step, compared to the serial implementation, resulting in a net speedup of
$21\times$.
"
1524,On Coding for Reliable VNF Chaining in DCNs,"  We study how erasure coding can improve service reliability in Data Center
Networks (DCN). To this end, we find that coding can be best deployed in
systems, where i) traffic is split into multiple parallel sub-flows, ii) each
sub-flow is encoded; iii) SFC along with their corresponding Virtual Network
Functions (VNF) concatenated are replicated into at least as many VNF instances
as there are sub-flows, resulting in parallel sub- SFCs; and iv) all coded
sub-flows are distributed over parallel paths and processed in parallel. We
study service reliability as function of the level of parallelization within
DCN and the resulting amount of redundancy. Based on the probability theory and
by considering failures of path segments, VNF and server failures, we
analytically derive the probability that parallel subflows are successfully
processed by the parallelized SFC and that the original serial traffic can be
successfully recovered without service interruptions.We compare the proposed
failure protection with coding and the standard backup protection and evaluate
the related overhead of both methods, including decoding, traffic redirection
and VNF migration. The results not only show the benefit of our scheme for
reliability, but also a reduced overhead required in comparison to backup
protection.
"
1525,A Comparative Measurement Study of Deep Learning as a Service Framework,"  Big data powered Deep Learning (DL) and its applications have blossomed in
recent years, fueled by three technological trends: a large amount of digitized
data openly accessible, a growing number of DL software frameworks in open
source and commercial markets, and a selection of affordable parallel computing
hardware devices. However, no single DL framework, to date, dominates in terms
of performance and accuracy even for baseline classification tasks on standard
datasets, making the selection of a DL framework an overwhelming task. This
paper takes a holistic approach to conduct empirical comparison and analysis of
four representative DL frameworks with three unique contributions. First, given
a selection of CPU-GPU configurations, we show that for a specific DL
framework, different configurations of its hyper-parameters may have a
significant impact on both performance and accuracy of DL applications. Second,
to the best of our knowledge, this study is the first to identify the
opportunities for improving the training time performance and the accuracy of
DL frameworks by configuring parallel computing libraries and tuning individual
and multiple hyper-parameters. Third, we also conduct a comparative measurement
study on the resource consumption patterns of four DL frameworks and their
performance and accuracy implications, including CPU and memory usage, and
their correlations to varying settings of hyper-parameters under different
configuration combinations of hardware, parallel computing libraries. We argue
that this measurement study provides in-depth empirical comparison and analysis
of four representative DL frameworks, and offers practical guidance for service
providers to deploying and delivering DL as a Service (DLaaS) and for
application developers and DLaaS consumers to select the right DL frameworks
for the right DL workloads.
"
1526,"Optimizing Data-Intensive Computations in Existing Libraries with Split
  Annotations","  Data movement between main memory and the CPU is a major bottleneck in
parallel data-intensive applications. In response, researchers have proposed
using compilers and intermediate representations (IRs) that apply optimizations
such as loop fusion under existing high-level APIs such as NumPy and
TensorFlow. Even though these techniques generally do not require changes to
user applications, they require intrusive changes to the library itself: often,
library developers must rewrite each function using a new IR. In this paper, we
propose a new technique called split annotations (SAs) that enables key data
movement optimizations over unmodified library functions. SAs only require
developers to annotate functions and implement an API that specifies how to
partition data in the library. The annotation and API describe how to enable
cross-function data pipelining and parallelization, while respecting each
function's correctness constraints. We implement a parallel runtime for SAs in
a system called Mozart. We show that Mozart can accelerate workloads in
libraries such as Intel MKL and Pandas by up to 15x, with no library
modifications. Mozart also provides performance gains competitive with
solutions that require rewriting libraries, and can sometimes outperform these
systems by up to 2x by leveraging existing hand-optimized code.
"
1527,"Randomized Work Stealing versus Sharing in Large-scale Systems with
  Non-exponential Job Sizes","  Work sharing and work stealing are two scheduling paradigms to redistribute
work when performing distributed computations. In work sharing, processors
attempt to migrate pending jobs to other processors in the hope of reducing
response times. In work stealing, on the other hand, underutilized processors
attempt to steal jobs from other processors. Both paradigms generate a certain
communication overhead and the question addressed in this paper is which of the
two reduces the response time the most given that they use the same amount of
communication overhead.
  Prior work presented explicit bounds, for large scale systems, on when
randomized work sharing outperforms randomized work stealing in case of Poisson
arrivals and exponential job durations and indicated that work sharing is best
when the load is below $\phi -1 \approx 0.6180$, with $\phi$ being the golden
ratio.
  In this paper we revisit this problem and study the impact of the job size
distribution using a mean field model. We present an efficient method to
determine the boundary between the regions where sharing or stealing is best
for a given job size distribution, as well as bounds that apply to any
(phase-type) job size distribution. The main insight is that work stealing
benefits significantly from having more variable job sizes and work sharing may
become inferior to work stealing for loads as small as $1/2 + \epsilon$ for any
$\epsilon > 0$.
"
1528,OpenCL Performance Prediction using Architecture-Independent Features,"  OpenCL is an attractive model for heterogeneous high-performance computing
systems, with wide support from hardware vendors and significant performance
portability. To support efficient scheduling on HPC systems it is necessary to
perform accurate performance predictions for OpenCL workloads on varied compute
devices, which is challenging due to diverse computation, communication and
memory access characteristics which result in varying performance between
devices. The Architecture Independent Workload Characterization (AIWC) tool can
be used to characterize OpenCL kernels according to a set of
architecture-independent features. This work presents a methodology where AIWC
features are used to form a model capable of predicting accelerator execution
times. We used this methodology to predict execution times for a set of 37
computational kernels running on 15 different devices representing a broad
range of CPU, GPU and MIC architectures. The predictions are highly accurate,
differing from the measured experimental run-times by an average of only 1.2%,
and correspond to actual execution time mispredictions of 9 {\mu}s to 1 sec
according to problem size. A previously unencountered code can be instrumented
once and the AIWC metrics embedded in the kernel, to allow performance
prediction across the full range of modelled devices. The results suggest that
this methodology supports correct selection of the most appropriate device for
a previously unencountered code, which is highly relevant to the HPC scheduling
setting.
"
1529,Generating Hard Instances for Robust Combinatorial Optimization,"  While research in robust optimization has attracted considerable interest
over the last decades, its algorithmic development has been hindered by several
factors. One of them is a missing set of benchmark instances that make
algorithm performance better comparable, and makes reproducing instances
unnecessary. Such a benchmark set should contain hard instances in particular,
but so far, the standard approach to produce instances has been to sample
values randomly from a uniform distribution.
  In this paper we introduce a new method to produce hard instances for min-max
combinatorial optimization problems, which is based on an optimization model
itself. Our approach does not make any assumptions on the problem structure and
can thus be applied to any combinatorial problem. Using the Selection and
Traveling Salesman problems as examples, we show that it is possible to produce
instances which are up to 500 times harder to solve for a mixed-integer
programming solver than the current state-of-the-art instances.
"
1530,Measuring Software Performance on Linux,"  Measuring and analyzing the performance of software has reached a high
complexity, caused by more advanced processor designs and the intricate
interaction between user programs, the operating system, and the processor's
microarchitecture. In this report, we summarize our experience about how
performance characteristics of software should be measured when running on a
Linux operating system and a modern processor. In particular, (1) We provide a
general overview about hardware and operating system features that may have a
significant impact on timing and how they interact, (2) we identify sources of
errors that need to be controlled in order to obtain unbiased measurement
results, and (3) we propose a measurement setup for Linux to minimize errors.
Although not the focus of this report, we describe the measurement process
using hardware performance counters, which can faithfully reflect the real
bottlenecks on a given processor. Our experiments confirm that our measurement
setup has a large impact on the results. More surprisingly, however, they also
suggest that the setup can be negligible for certain analysis methods.
Furthermore, we found that our setup maintains significantly better performance
under background load conditions, which means it can be used to improve
software in high-performance applications.
"
1531,"Stabilizing the virtual response time in single-server processor sharing
  queues with slowly time-varying arrival rates","  Motivated by the work of Whitt, who studied stabilization of the mean virtual
waiting time (excluding service time) in a $GI_t/GI_t/1/FCFS$ queue, this paper
investigates the stabilization of the mean virtual response time in a
single-server processor sharing (PS) queueing system with a time-varying
arrival rate and a service rate control (a $GI_t/GI_t/1/PS$ queue). We propose
and compare a modified square-root (SR) control and a difference-matching (DM)
control to stabilize the mean virtual response time of a $GI_t/GI_t/1/PS$
queue. Extensive simulation studies with various settings of arrival processes
and service times show that the DM control outperforms the SR control for
heavy-traffic conditions, and that the SR control performs better for
light-traffic conditions.
"
1532,"Defining Big Data Analytics Benchmarks for Next Generation
  Supercomputers","  The design and construction of high performance computing (HPC) systems
relies on exhaustive performance analysis and benchmarking. Traditionally this
activity has been geared exclusively towards simulation scientists, who,
unsurprisingly, have been the primary customers of HPC for decades. However,
there is a large and growing volume of data science work that requires these
large scale resources, and as such the calls for inclusion and investments in
data for HPC have been increasing. So when designing a next generation HPC
platform, it is necessary to have HPC-amenable big data analytics benchmarks.
In this paper, we propose a set of big data analytics benchmarks and sample
codes designed for testing the capabilities of current and next generation
supercomputers.
"
1533,Gravitational octree code performance evaluation on Volta GPU,"  In this study, the gravitational octree code originally optimized for the
Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta
architecture. The Volta architecture introduces independent thread scheduling
requiring either the insertion of the explicit synchronizations at appropriate
locations or the enforcement of the same implicit synchronizations as do the
Pascal or earlier architectures by specifying \texttt{-gencode
arch=compute\_60,code=sm\_70}. The performance measurements on Tesla V100, the
current flagship GPU by NVIDIA, revealed that the $N$-body simulations of the
Andromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \times
10^{-2}$~s or $3.3 \times 10^{-2}$~s per step for each case. Tesla V100
achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the
flagship GPU in the previous generation. The observed speed-up of 2.2 is
greater than 1.5, which is the ratio of the theoretical peak performance of the
two GPUs. The independence of the units for integer operations from those for
floating-point number operations enables the overlapped execution of integer
and floating-point number operations. It hides the execution time of the
integer operations leading to the speed-up rate above the theoretical peak
performance ratio. Tesla V100 can execute $N$-body simulation with up to $25
\times 2^{20} = 26214400$ particles, and it took $2.0 \times 10^{-1}$~s per
step. It corresponds to $3.5$~TFlop/s, which is 22\% of the single-precision
theoretical peak performance.
"
1534,Spatter: A Tool for Evaluating Gather / Scatter Performance,"  This paper describes a new benchmark tool, Spatter, for assessing memory
system architectures in the context of a specific category of indexed accesses
known as gather and scatter. These types of operations are increasingly used to
express sparse and irregular data access patterns, and they have widespread
utility in many modern HPC applications including scientific simulations, data
mining and analysis computations, and graph processing. However, many
traditional benchmarking tools like STREAM, STRIDE, and GUPS focus on
characterizing only uniform stride or fully random accesses despite evidence
that modern applications use varied sets of more complex access patterns.
  Spatter is an open-source benchmark that provides a tunable and configurable
framework to benchmark a variety of indexed access patterns, including
variations of gather/scatter that are seen in HPC mini-apps evaluated in this
work. The design of Spatter includes tunable backends for OpenMP and CUDA, and
experiments show how it can be used to evaluate 1) uniform access patterns for
CPU and GPU, 2) prefetching regimes for gather/scatter, 3) compiler
implementations of vectorization for gather/scatter, and 4) trace-driven ""proxy
patterns"" that reflect the patterns found in multiple applications. The results
from Spatter experiments show that GPUs typically outperform CPUs for these
operations, and that Spatter can better represent the performance of some
cache-dependent mini-apps than traditional STREAM bandwidth measurements.
"
1535,HPS: A C++11 High Performance Serialization Library,"  Data serialization is a common and crucial component in high performance
computing. In this paper, I present a C++11 based serialization library for
performance critical systems. It provides an interface similar to Boost but up
to 150% faster and beats several popular serialization libraries.
"
1536,Comparing Spark vs MPI/OpenMP On Word Count MapReduce,"  Spark provides an in-memory implementation of MapReduce that is widely used
in the big data industry. MPI/OpenMP is a popular framework for high
performance parallel computing. This paper presents a high performance
MapReduce design in MPI/OpenMP and uses that to compare with Spark on the
classic word count MapReduce task. My result shows that the MPI/OpenMP
MapReduce outperforms Apache Spark by about 300%.
"
1537,"Global attraction of ODE-based mean field models with hyperexponential
  job sizes","  Mean field modeling is a popular approach to assess the performance of large
scale computer systems. The evolution of many mean field models is
characterized by a set of ordinary differential equations that have a unique
fixed point. In order to prove that this unique fixed point corresponds to the
limit of the stationary measures of the finite systems, the unique fixed point
must be a global attractor. While global attraction was established for various
systems in case of exponential job sizes, it is often unclear whether these
proof techniques can be generalized to non-exponential job sizes. In this paper
we show how simple monotonicity arguments can be used to prove global
attraction for a broad class of ordinary differential equations that capture
the evolution of mean field models with hyperexponential job sizes. This class
includes both existing as well as previously unstudied load balancing schemes
and can be used for systems with either finite or infinite buffers. The main
novelty of the approach exists in using a Coxian representation for the
hyperexponential job sizes and a partial order that is stronger than the
componentwise partial order used in the exponential case.
"
1538,"Multi-level analysis of compiler induced variability and performance
  tradeoffs","  Successful HPC software applications are long-lived. When ported across
machines and their compilers, these applications often produce different
numerical results, many of which are unacceptable. Such variability is also a
concern while optimizing the code more aggressively to gain performance.
Efficient tools that help locate the program units (files and functions) within
which most of the variability occurs are badly needed, both to plan for code
ports and to root-cause errors due to variability when they happen in the
field. In this work, we offer an enhanced version of the open-source testing
framework FLiT to serve these roles. Key new features of FLiT include a suite
of bisection algorithms that help locate the root causes of variability.
Another added feature allows an analysis of the tradeoffs between performance
and the degree of variability. Our new contributions also include a collection
of case studies. Results on the MFEM finite-element library include
variability/performance tradeoffs, and the identification of a (hitherto
unknown) abnormal level of result-variability even under mild compiler
optimizations. Results from studying the Laghos proxy application include
identifying a significantly divergent floating-point result-variability and
successful root-causing down to the problematic function over as little as 14
program executions. Finally, in an evaluation of 4,376 controlled injections of
floating-point perturbations on the LULESH proxy application, we showed that
the FLiT framework has 100 precision and recall in discovering the file and
function locations of the injections all within an average of only 15 program
executions.
"
1539,"Optimizing System Quality of Service through Rejuvenation for
  Long-Running Applications with Real-Time Constraints","  Reliability, longevity, availability, and deadline guarantees are the four
most important metrics to measure the QoS of long-running safety-critical
real-time applications. Software aging is one of the major factors that impact
the safety of long-running real-time applications as the degraded performance
and increased failure rate caused by software aging can lead to deadline
missing and catastrophic consequences. Software rejuvenation is one of the most
commonly used approaches to handle issues caused by software aging. In this
paper, we study the optimal time when software rejuvenation shall take place so
that the system's reliability, longevity, and availability are maximized, and
application delays caused by software rejuvenation is minimized. In particular,
we formally analyze the relationships between software rejuvenation frequency
and system reliability, longevity, and availability. Based on the theoretic
analysis, we develop approaches to maximizing system reliability, longevity,
and availability, and use simulation to evaluate the developed approaches. In
addition, we design the MIN-DELAY semi-priority-driven scheduling algorithm to
minimize application delays caused by rejuvenation processes. The simulation
experiments show that the developed semi-priority-driven scheduling algorithm
reduces application delays by 9.01% and 14.24% over the earliest deadline first
(EDF) and least release time (LRT) scheduling algorithms, respectively.
"
1540,"On the Influence of Initial Qubit Placement During NISQ Circuit
  Compilation","  Noisy Intermediate-Scale Quantum (NISQ) machines are not fault-tolerant,
operate few qubits (currently, less than hundred), but are capable of executing
interesting computations. Above the quantum supremacy threshold (approx. 60
qubits), NISQ machines are expected to be more powerful than existing classical
computers. One of the most stringent problems is that computations (expressed
as quantum circuits) have to be adapted (compiled) to the NISQ hardware,
because the hardware does not support arbitrary interactions between the
qubits. This procedure introduces additional gates (e.g. SWAP gates) into the
circuits while leaving the implemented computations unchanged. Each additional
gate increases the failure rate of the adapted (compiled) circuits, because the
hardware and the circuits are not fault-tolerant. It is reasonable to expect
that the placement influences the number of additionally introduced gates.
Therefore, a combinatorial problem arises: how are circuit qubits allocated
(placed) initially to the hardware qubits? The novelty of this work relies on
the methodology used to investigate the influence of the initial placement. To
this end, we introduce a novel heuristic and cost model to estimate the number
of gates necessary to adapt a circuit to a given NISQ architecture. We
implement the heuristic (source code available on github) and benchmark it
using a standard compiler (e.g. from IBM Qiskit) treated as a black box.
Preliminary results indicate that cost reductions of up to 10\% can be achieved
for practical circuit instances on realistic NISQ architectures only by placing
qubits differently than default (trivial placement).
"
1541,Accelerating Reduction and Scan Using Tensor Core Units,"  Driven by deep learning, there has been a surge of specialized processors for
matrix multiplication, referred to as TensorCore Units (TCUs). These TCUs are
capable of performing matrix multiplications on small matrices (usually 4x4 or
16x16) to accelerate the convolutional and recurrent neural networks in deep
learning workloads. In this paper we leverage NVIDIA's TCU to express both
reduction and scan with matrix multiplication and show the benefits -- in terms
of program simplicity, efficiency, and performance. Our algorithm exercises the
NVIDIA TCUs which would otherwise be idle, achieves 89%-98% of peak memory copy
bandwidth, and is orders of magnitude faster (up to 100x for reduction and 3x
for scan) than state-of-the-art methods for small segment sizes -- common in
machine learning and scientific applications. Our algorithm achieves this while
decreasing the power consumption by up to 22% for reduction and16%for scan.
"
1542,Implementing Entangled States on a Quantum Computer,"  The study of tensor network theory is an important field and promises a wide
range of experimental and quantum information theoretical applications. Matrix
product state is the most well-known example of tensor network states, which
provides an effective and efficient representation of one-dimensional quantum
systems. Indeed, it lies at the heart of density matrix renormalization group
(DMRG), a most common method for simulation of one-dimensional strongly
correlated quantum systems. It has got attention from several areas varying
from solid-state systems to quantum computing and quantum simulators. We have
considered maximally entangled matrix product states (GHZ and W). Here, we
designed the quantum circuits for implementing the matrix product states. In
this paper, we simulated the matrix product states in customized IBM (2-qubit,
3-qubit and 4-qubit) quantum systems and determined the probability
distribution among the quantum states.
"
1543,"Evaluation of Intel Memory Drive Technology Performance for Scientific
  Applications","  In this paper, we present benchmark data for Intel Memory Drive Technology
(IMDT), which is a new generation of Software-defined Memory (SDM) based on
Intel ScaleMP collaboration and using 3D XPointTM based Intel Solid-State
Drives (SSDs) called Optane. We studied IMDT performance for synthetic
benchmarks, scientific kernels, and applications. We chose these benchmarks to
represent different patterns for computation and accessing data on disks and
memory. To put performance of IMDT in comparison, we used two memory
configurations: hybrid IMDT DDR4/Optane and DDR4 only systems. The performance
was measured as a percentage of used memory and analyzed in detail. We found
that for some applications DDR4/Optane hybrid configuration outperforms DDR4
setup by up to 20%.
"
1544,"An optimized Parallel Failure-less Aho-Corasick algorithm for DNA
  sequence matching","  The Aho-Corasick algorithm is multiple patterns searching algorithm running
sequentially in various applications like network intrusion detection and
bioinformatics for finding several input strings within a given large input
string. The parallel version of the Aho-Corasick algorithm is called as
Parallel Failure-less Aho-Corasick algorithm because it doesn't need failure
links like in the original Aho-Corasick algorithm. In this research, we
implemented an application specific parallel failureless Aho-Corasick algorithm
to the general purpose graphics processing unit by applying several cache
optimization techniques for matching DNA sequences. Our parallel Aho-Corasick
algorithm shows better performance than the available parallel Aho-Corasick
algorithm library due to its simplicity and optimized cache memory usage of
graphics processing units for matching DNA sequences.
"
1545,"The L-CSC cluster: Optimizing power efficiency to become the greenest
  supercomputer in the world in the Green500 list of November 2014","  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose
compute cluster built with commodity hardware installed at GSI. Its main
operational purpose is Lattice QCD (LQCD) calculations for physics simulations.
Quantum Chromo Dynamics (QCD) is the physical theory describing the strong
force, one of the four known fundamental interactions in the universe. L-CSC
leverages a multi-GPU design accommodating the huge demand of LQCD for memory
bandwidth. In recent years, heterogeneous clusters with accelerators such as
GPUs have become more and more powerful while supercomputers in general have
shown enormous increases in power consumption making electricity costs and
cooling a significant factor in the total cost of ownership. Using mainly GPUs
for processing, L-CSC is very power-efficient, and its architecture was
optimized to provide the greatest possible power efficiency. This paper
presents the cluster design as well as optimizations to improve the power
efficiency. It examines the power measurements performed for the Green500 list
of the most power-efficient supercomputers in the world which led to the number
1 position as the greenest supercomputer in November 2014.
"
1546,Linux-Tomcat Application Performance on Amazon AWS,"  The need for Linux system administrators to do performance management has
returned with a vengeance. Why? The cloud. Resource consumption in the cloud is
all about pay-as-you-go. This article shows you how performance models can find
the most cost-effective deployment of an application on Amazon's cloud.
"
1547,"Hoard: A Distributed Data Caching System to Accelerate Deep Learning
  Training on the Cloud","  Deep Learning system architects strive to design a balanced system where the
computational accelerator -- FPGA, GPU, etc, is not starved for data. Feeding
training data fast enough to effectively keep the accelerator utilization high
is difficult when utilizing dedicated hardware like GPUs. As accelerators are
getting faster, the storage media \& data buses feeding the data have not kept
pace and the ever increasing size of training data further compounds the
problem. We describe the design and implementation of a distributed caching
system called Hoard that stripes the data across fast local disks of multiple
GPU nodes using a distributed file system that efficiently feeds the data to
ensure minimal degradation in GPU utilization due to I/O starvation. Hoard can
cache the data from a central storage system before the start of the job or
during the initial execution of the job and feeds the cached data for
subsequent epochs of the same job and for different invocations of the jobs
that share the same data requirements, e.g. hyper-parameter tuning. Hoard
exposes a POSIX file system interface so the existing deep learning frameworks
can take advantage of the cache without any modifications. We show that Hoard,
using two NVMe disks per node and a distributed file system for caching,
achieves a 2.1x speed-up over a 10Gb/s NFS central storage system on a 16 GPU
(4 nodes, 4 GPUs per node) cluster for a challenging AlexNet ImageNet image
classification benchmark with 150GB of input dataset. As a result of the
caching, Hoard eliminates the I/O bottlenecks introduced by the shared storage
and increases the utilization of the system by 2x compared to using the shared
storage without the cache.
"
1548,Playing with and against Hedge,"  Hedge has been proposed as an adaptive scheme, which guides an agent's
decision in resource selection and distribution problems that can be modeled as
a multi-armed bandit full information game. Such problems are encountered in
the areas of computer and communication networks, e.g. network path selection,
load distribution, network interdiction, and also in problems in the area of
transportation. We study Hedge under the assumption that the total loss that
can be suffered by the player in each round is upper bounded. In this paper, we
study the worst performance of Hedge.
"
1549,"A Parallel Time-Integrator for Solving the Linearized Shallow Water
  Equations on the Rotating Sphere","  With the stagnation of processor core performance, further reductions in the
time-to-solution for geophysical fluid problems are becoming increasingly
difficult with standard time integrators. Parallel-in-time exposes and exploits
additional parallelism in the time dimension which is inherently sequential in
traditional methods. The rational approximation of exponential integrators
(REXI) method allows taking arbitrarily long time steps based on a sum over a
number of decoupled complex PDEs that can be solved independently massively
parallel. Hence REXI is assumed to be well suited for modern massively parallel
super computers which are currently trending. To date the study and development
of the REXI approach has been limited to linearized problems on the periodic 2D
plane. This work extends the REXI time stepping method to the linear
shallow-water equations (SWE) on the rotating sphere, thus moving the method
one step closer to solving fully nonlinear fluid problems of geophysical
interest on the sphere. The rotating sphere poses particular challenges for
finding an efficient solver due to the zonal dependence of the Coriolis term.
Here we present an efficient REXI solver based on spherical harmonics, showing
the results of: a geostrophic balance test, a comparison with alternative time
stepping methods, an analysis of dispersion relations, indicating superior
properties of REXI, and finally a performance comparison on Cheyenne
supercomputer. Our results indicate that REXI is not only able to take larger
time steps, but that REXI can also be used to gain higher accuracy and
significantly reduced time-to-solution compared to currently existing time
stepping methods.
"
1550,"Machine Learning-based Link Fault Identification and Localization in
  Complex Networks","  With the proliferation of network devices and rapid development in
information technology, networks such as Internet of Things are increasing in
size and becoming more complex with heterogeneous wired and wireless links. In
such networks, link faults may result in a link disconnection without immediate
replacement or a link reconnection, e.g., a wireless node changes its access
point. Identifying whether a link disconnection or a link reconnection has
occurred and localizing the failed link become a challenging problem. An active
probing approach requires a long time to probe the network by sending signaling
messages on different paths, thus incurring significant communication delay and
overhead. In this paper, we adopt a passive approach and develop a three-stage
machine learning-based technique, namely ML-LFIL that identifies and localizes
link faults by analyzing the measurements captured from the normal traffic
flows, including aggregate flow rate, end-to-end delay and packet loss. ML-LFIL
learns the traffic behavior in normal working conditions and different link
fault scenarios. We train the learning model using support vector machine,
multi-layer perceptron and random forest. We implement ML-LFIL and carry out
extensive experiments using Mininet platform. Performance studies show that
ML-LFIL achieves high accuracy while requiring much lower fault localization
time compared to the active probing approach.
"
1551,Speed Based Optimal Power Control in Small Cell Networks,"  Small cell networks promise good quality of service (QoS) even for cell edge
users, however pose challenges to cater to the high-speed users. The major
difficulty being that of frequent handovers and the corresponding handover
losses, which significantly depend upon the speed of the user. It was shown
previously that the optimal cell size increases with speed. Thus, in scenarios
with diverse users (speeds spanning over large ranges), it would be inefficient
to serve all users using common cell radius and it is practically infeasible to
design different cell sizes for different speeds. Alternatively, we propose to
allocate power to a user based on its speed, e.g., higher power virtually
increases the cell size. We solve well known Hamiltonian Jacobi equations under
certain assumptions to obtain a power law, optimal for load factor and busy
probability, for any given average power constraint and cell size. The optimal
power control turns out to be linear in speed. We build a system level
simulator for small cell network, using elaborate Monte-Carlo simulations, and
show that the performance of the system improves significantly with linear
power law. The power law is tested even for the cases, for which the system
does not satisfy the assumptions required by the theory. For example, the
linear power law has significant improvement in comparison with the 'equal
power' system, even in presence of time varying and random interference. We
observe good improvement in almost all cases with improvements up to 89\% for
certain configurations.
"
1552,An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs,"  SSDs are emerging storage devices which unlike HDDs, do not have mechanical
parts and therefore, have superior performance compared to HDDs. Due to the
high cost of SSDs, entirely replacing HDDs with SSDs is not economically
justified. Additionally, SSDs can endure a limited number of writes before
failing. To mitigate the shortcomings of SSDs while taking advantage of their
high performance, SSD caching is practiced in both academia and industry.
Previously proposed caching architectures have only focused on either
performance or endurance and neglected to address both parameters in suggested
architectures. Moreover, the cost, reliability, and power consumption of such
architectures is not evaluated. This paper proposes a hybrid I/O caching
architecture that while offers higher performance than previous studies, it
also improves power consumption with a similar budget. The proposed
architecture uses DRAM, Read-Optimized SSD, and Write-Optimized SSD in a
three-level cache hierarchy and tries to efficiently redirect read requests to
either DRAM or RO-SSD while sending writes to WO-SSD. To provide high
reliability, dirty pages are written to at least two devices which removes any
single point of failure. The power consumption is also managed by reducing the
number of accesses issued to SSDs. The proposed architecture reconfigures
itself between performance- and endurance-optimized policies based on the
workload characteristics to maintain an effective tradeoff between performance
and endurance. We have implemented the proposed architecture on a server
equipped with industrial SSDs and HDDs. The experimental results show that as
compared to state-of-the-art studies, the proposed architecture improves
performance and power consumption by an average of 8% and 28%, respectively,
and reduces the cost by 5% while increasing the endurance cost by 4.7% and
negligible reliability penalty.
"
1553,"R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled
  Look-Ahead Architectures","  Modern societies have developed insatiable demands for more computation
capabilities. Exploiting implicit parallelism to provide automatic performance
improvement remains a central goal in engineering future general-purpose
computing systems. One approach is to use a separate thread context to perform
continuous look-ahead to improve the data and instruction supply to the main
pipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective
in accelerating a broad range of applications in a relatively straightforward
implementation. It also has broad design flexibility as the look-ahead agent
need not be concerned with correctness constraints. In this paper, we explore a
number of optimizations that make the look-ahead agent more efficient and yet
extract more utility from it. With these optimizations, a DLA architecture can
achieve an average speedup of 1.4 over a state-of-the-art microarchitecture for
a broad set of benchmark suites, making it a powerful tool to enhance
single-thread performance.
"
1554,"Continuous evaluation of the performance of cloud infrastructure for
  scientific applications","  Cloud computing recently developed into a viable alternative to on-premises
systems for executing high-performance computing (HPC) applications. With the
emergence of new vendors and hardware options, there is now a growing need to
continuously evaluate the performance of the infrastructure with respect to the
most commonly-used simulation workflows. We present an online ecosystem and the
corresponding tools aimed at providing a collaborative and repeatable way to
assess the performance of the underlying hardware for multiple real-world
application-specific benchmark cases. The ecosystem allows for the benchmark
results to be stored and shared online in a centrally accessible database in
order to facilitate their comparison, traceability, and curation. We include
the current up-to-date example results for multiple cloud vendors and explain
how to contribute new results and benchmark cases.
"
1555,"Impact of Traditional Sparse Optimizations on a Migratory Thread
  Architecture","  Achieving high performance for sparse applications is challenging due to
irregular access patterns and weak locality. These properties preclude many
static optimizations and degrade cache performance on traditional systems. To
address these challenges, novel systems such as the Emu architecture have been
proposed. The Emu design uses light-weight migratory threads, narrow memory,
and near-memory processing capabilities to address weak locality and reduce the
total load on the memory system. Because the Emu architecture is fundamentally
different than cache based hierarchical memory systems, it is crucial to
understand the cost-benefit tradeoffs of standard sparse algorithm
optimizations on Emu hardware. In this work, we explore sparse matrix-vector
multiplication (SpMV) on the Emu architecture. We investigate the effects of
different sparse optimizations such as dense vector data layouts, work
distributions, and matrix reorderings. Our study finds that initially
distributing work evenly across the system is inadequate to maintain load
balancing over time due to the migratory nature of Emu threads. In severe
cases, matrix sparsity patterns produce hot-spots as many migratory threads
converge on a single resource. We demonstrate that known matrix reordering
techniques can improve SpMV performance on the Emu architecture by as much as
70% by encouraging more consistent load balancing. This can be compared with a
performance gain of no more than 16% on a cache-memory based system.
"
1556,Parallel Sparse Tensor Decomposition in Chapel,"  In big-data analytics, using tensor decomposition to extract patterns from
large, sparse multivariate data is a popular technique. Many challenges exist
for designing parallel, high performance tensor decomposition algorithms due to
irregular data accesses and the growing size of tensors that are processed.
There have been many efforts at implementing shared-memory algorithms for
tensor decomposition, most of which have focused on the traditional C/C++ with
OpenMP framework. However, Chapel is becoming an increasingly popular
programing language due to its expressiveness and simplicity for writing
scalable parallel programs. In this work, we port a state of the art C/OpenMP
parallel sparse tensor decomposition tool, SPLATT, to Chapel. We present a
performance study that investigates bottlenecks in our Chapel code and
discusses approaches for improving its performance. Also, we discuss features
in Chapel that would have been beneficial to our porting effort. We demonstrate
that our Chapel code is competitive with the C/OpenMP code for both runtime and
scalability, achieving 83%-96% performance of the original code and near linear
scalability up to 32 cores.
"
1557,An Empirical Evaluation of Allgatherv on Multi-GPU Systems,"  Applications for deep learning and big data analytics have compute and memory
requirements that exceed the limits of a single GPU. However, effectively
scaling out an application to multiple GPUs is challenging due to the
complexities of communication between the GPUs, particularly for collective
communication with irregular message sizes. In this work, we provide a
performance evaluation of the Allgatherv routine on multi-GPU systems, focusing
on GPU network topology and the communication library used. We present results
from the OSU-micro benchmark as well as conduct a case study for sparse tensor
factorization, one application that uses Allgatherv with highly irregular
message sizes. We extend our existing tensor factorization tool to run on
systems with different node counts and varying number of GPUs per node. We then
evaluate the communication performance of our tool when using traditional MPI,
CUDA-aware MVAPICH and NCCL across a suite of real-world data sets on three
different systems: a 16-node cluster with one GPU per node, NVIDIA's DGX-1 with
8 GPUs and Cray's CS-Storm with 16 GPUs. Our results show that irregularity in
the tensor data sets produce trends that contradict those in the OSU
micro-benchmark, as well as trends that are absent from the benchmark.
"
1558,"Fast and Efficient Bulk Multicasting over Dedicated Inter-Datacenter
  Networks","  Several organizations have built multiple datacenters connected via dedicated
wide area networks over which large inter-datacenter transfers take place. This
includes tremendous volumes of bulk multicast traffic generated as a result of
data and content replication. Although one can perform these transfers using a
single multicast forwarding tree, that can lead to poor performance as the
slowest receiver on each tree dictates the completion time for all receivers.
Using multiple trees per transfer each connected to a subset of receivers
alleviates this concern. The choice of multicast trees also determines the
total bandwidth usage. To further improve the performance, bandwidth over
dedicated inter-datacenter networks can be carved for different multicast trees
over specific time periods to avoid congestion and minimize the average
receiver completion times.
  In this paper, we break this problem into the three sub-problems of
partitioning, tree selection, and rate allocation. We present an algorithm
called QuickCast which is computationally fast and allows us to significantly
speed up multiple receivers per bulk multicast transfer with control over extra
bandwidth consumption. We evaluate QuickCast against a variety of synthetic and
real traffic patterns as well as real WAN topologies. Compared to performing
bulk multicast transfers as separate unicast transfers, QuickCast achieves up
to $3.64\times$ reduction in mean completion times while at the same time using
$0.71\times$ the bandwidth. Also, QuickCast allows the top $50\%$ of receivers
to complete between $3\times$ to $35\times$ faster on average compared with
when a single forwarding multicast tree is used for data delivery.
"
1559,"Worst-case Bounds and Optimized Cache on $M^{th}$ Request Cache
  Insertion Policies under Elastic Conditions","  Cloud services and other shared third-party infrastructures allow individual
content providers to easily scale their services based on current resource
demands. In this paper, we consider an individual content provider that wants
to minimize its delivery costs under the assumptions that the storage and
bandwidth resources it requires are elastic, the content provider only pays for
the resources that it consumes, and costs are proportional to the resource
usage. Within this context, we (i) derive worst-case bounds for the optimal
cost and competitive cost ratios of different classes of ""cache on $M^{th}$
request"" cache insertion policies, (ii) derive explicit average cost
expressions and bounds under arbitrary inter-request distributions, (iii)
derive explicit average cost expressions and bounds for short-tailed
(deterministic, Erlang, and exponential) and heavy-tailed (Pareto)
inter-request distributions, and (iv) present numeric and trace-based
evaluations that reveal insights into the relative cost performance of the
policies. Our results show that a window-based ""cache on $2^{nd}$ request""
policy using a single threshold optimized to minimize worst-case costs provides
good average performance across the different distributions and the full
parameter ranges of each considered distribution, making it an attractive
choice for a wide range of practical conditions where request rates of
individual file objects typically are not known and can change quickly.
"
1560,"Joint Rate and Resource Allocation in Hybrid Digital-Analog Transmission
  over Fading Channels","  In hybrid digital-analog (HDA) systems, resource allocation has been utilized
to achieve desired distortion performance. However, existing studies on this
issue assume error-free digital transmission, which is not valid for fading
channels. With time-varying channel fading, the exact channel state information
is not available at the transmitter. Thus, random outage and resulting digital
distortion cannot be ignored. Moreover, rate allocation should be considered in
resource allocation, since it not only determines the amount of information for
digital transmission and that for analog transmission, but also affects the
outage probability. Based on above observations, in this paper, we attempt to
perform joint rate and resource allocation strategies to optimize system
distortion in HDA systems over fading channels. Consider a bandwidth expansion
scenario where a memoryless Gaussian source is transmitted in an HDA system
with the entropy-constrained scalar quantizer (ECSQ). Firstly, we formulate the
joint allocation problem as an expected system distortion minimization problem
where both analog and digital distortion are considered. Then, in the limit of
low outage probability, we decompose the problem into two coupled sub-problems
based on the block coordinate descent method, and propose an iterative gradient
algorithm to approach the optimal solution. Furthermore, we extend our work to
the multivariate Gaussian source scenario where a two-stage fast algorithm
integrating rounding and greedy strategies is proposed to optimize the joint
rate and resource allocation problem. Finally, simulation results demonstrate
that the proposed algorithms can achieve up to 2.3dB gains in terms of
signal-to-distortion ratio over existing schemes under the single Gaussian
source scenario, and up to 3.5dB gains under the multivariate Gaussian source
scenario.
"
1561,"A Preliminary Study of Neural Network-based Approximation for HPC
  Applications","  Machine learning, as a tool to learn and model complicated (non)linear
relationships between input and output data sets, has shown preliminary success
in some HPC problems. Using machine learning, scientists are able to augment
existing simulations by improving accuracy and significantly reducing
latencies. Our ongoing research work is to create a general framework to apply
neural network-based models to HPC applications. In particular, we want to use
the neural network to approximate and replace code regions within the HPC
application to improve performance (i.e., reducing the execution time) of the
HPC application. In this paper, we present our preliminary study and results.
Using two applications (the Newton-Raphson method and the Lennard-Jones (LJ)
potential in LAMMP) for our case study, we achieve up to 2.7x and 2.46x
speedup, respectively.
"
1562,QAOA for Max-Cut requires hundreds of qubits for quantum speed-up,"  Computational quantum technologies are entering a new phase in which noisy
intermediate-scale quantum computers are available, but are still too small to
benefit from active error correction. Even with a finite coherence budget to
invest in quantum information processing, noisy devices with about 50 qubits
are expected to experimentally demonstrate quantum supremacy in the next few
years. Defined in terms of artificial tasks, current proposals for quantum
supremacy, even if successful, will not help to provide solutions to practical
problems. Instead, we believe that future users of quantum computers are
interested in actual applications and that noisy quantum devices may still
provide value by approximately solving hard combinatorial problems via hybrid
classical-quantum algorithms. To lower bound the size of quantum computers with
practical utility, we perform realistic simulations of the Quantum Approximate
Optimization Algorithm and conclude that quantum speedup will not be
attainable, at least for a representative combinatorial problem, until several
hundreds of qubits are available.
"
1563,AdaptMemBench: Application-Specific MemorySubsystem Benchmarking,"  Optimizing scientific applications to take full advan-tage of modern memory
subsystems is a continual challenge forapplication and compiler developers.
Factors beyond working setsize affect performance. A benchmark framework that
exploresthe performance in an application-specific manner is essential
tocharacterize memory performance and at the same time informmemory-efficient
coding practices. We present AdaptMemBench,a configurable benchmark framework
that measures achievedmemory performance by emulating application-specific
accesspatterns with a set of kernel-independent driver templates. Thisframework
can explore the performance characteristics of a widerange of access patterns
and can be used as a testbed for potentialoptimizations due to the flexibility
of polyhedral code generation.We demonstrate the effectiveness of AdaptMemBench
with casestudies on commonly used computational kernels such as triadand
multidimensional stencil patterns.
"
1564,"Fast and Accurate 3D Medical Image Segmentation with Data-swapping
  Method","  Deep neural network models used for medical image segmentation are large
because they are trained with high-resolution three-dimensional (3D) images.
Graphics processing units (GPUs) are widely used to accelerate the trainings.
However, the memory on a GPU is not large enough to train the models. A popular
approach to tackling this problem is patch-based method, which divides a large
image into small patches and trains the models with these small patches.
However, this method would degrade the segmentation quality if a target object
spans multiple patches. In this paper, we propose a novel approach for 3D
medical image segmentation that utilizes the data-swapping, which swaps out
intermediate data from GPU memory to CPU memory to enlarge the effective GPU
memory size, for training high-resolution 3D medical images without patching.
We carefully tuned parameters in the data-swapping method to obtain the best
training performance for 3D U-Net, a widely used deep neural network model for
medical image segmentation. We applied our tuning to train 3D U-Net with
full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,
communication overhead, which is the most important issue, was reduced by
17.1%. Compared with the patch-based method for patches of 128 x 128 x 128
voxels, our training for full-size images achieved improvement on the mean Dice
score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core
sub-region, respectively. The total training time was reduced from 164 hours to
47 hours, resulting in 3.53 times of acceleration.
"
1565,LBICA: A Load Balancer for I/O Cache Architectures,"  In recent years, enterprise Solid-State Drives (SSDs) are used in the caching
layer of high-performance servers to close the growing performance gap between
processing units and storage subsystem. SSD-based I/O caching is typically not
effective in workloads with burst accesses in which the caching layer itself
becomes the performance bottleneck because of the large number of accesses.
Existing I/O cache architectures mainly focus on maximizing the cache hit ratio
while they neglect the average queue time of accesses. Previous studies
suggested bypassing the cache when burst accesses are identified. These
schemes, however, are not applicable to a general cache configuration and also
result in significant performance degradation on burst accesses. In this paper,
we propose a novel I/O cache load balancing scheme (LBICA) with adaptive write
policy management to prevent the I/O cache from becoming performance bottleneck
in burst accesses. Our proposal, unlike previous schemes, which disable the I/O
cache or bypass the requests into the disk subsystem in burst accesses,
selectively reduces the number of waiting accesses in the SSD queue and
balances the load between the I/O cache and the disk subsystem while providing
the maximum performance. The proposed scheme characterizes the workload based
on the type of in-queue requests and assigns an effective cache write policy.
We aim to bypass the accesses which 1) are served faster by the disk subsystem
or 2) cannot be merged with other accesses in the I/O cache queue. Doing so,
the selected requests are responded by the disk layer, preventing from
overloading the I/O cache. Our evaluations on a physical system shows that
LBICA reduces the load on the I/O cache by 48% and improves the performance of
burst workloads by 30% compared to the latest state-of-the-art load balancing
scheme.
"
1566,Bootstrap percolation on the stochastic block model with k communities,"  We analyze the bootstrap percolation process on the stochastic block model
(SBM), a natural extension of the Erd\""{o}s--R\'{e}nyi random graph that allows
representing the ""community structure"" observed in many real systems. In the
SBM, nodes are partitioned into subsets, which represent different communities,
and pairs of nodes are independently connected with a probability that depends
on the communities they belong to. Under mild assumptions on system parameters,
we prove the existence of a sharp phase transition for the final number of
active nodes and characterize sub-critical and super-critical regimes in terms
of the number of initially active nodes, which are selected uniformly at random
in each community.
"
1567,"Quicker ADC : Unlocking the hidden potential of Product Quantization
  with SIMD","  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a
foundation of many multimedia retrieval systems. A common approach is to rely
on Product Quantization, which allows the storage of large vector databases in
memory and efficient distance computations. Yet, implementations of nearest
neighbor search with Product Quantization have their performance limited by the
many memory accesses they perform. Following this observation, Andr\'e et al.
proposed Quick ADC with up to $6\times$ faster implementations of $m\times{}4$
product quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a
generalization of Quick ADC not limited to $m\times{}4$ codes and supporting
AVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC
faces the challenge of using efficiently 5,6 and 7-bit shuffles that do not
align to computer bytes or words. To this end, we introduce (i) irregular
product quantizers combining sub-quantizers of different granularity and (ii)
split tables allowing lookup tables larger than registers. We evaluate Quicker
ADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and
show that it outperforms the reference optimized implementations (i.e., FAISS
and polysemous codes) for numerous configurations. Finally, we release an
open-source fork of FAISS enhanced with Quicker ADC at
http://github.com/nlescoua/faiss-quickeradc.
"
1568,"Batch Size Influence on Performance of Graphic and Tensor Processing
  Units during Training and Inference Phases","  The impact of the maximally possible batch size (for the better runtime) on
performance of graphic processing units (GPU) and tensor processing units (TPU)
during training and inference phases is investigated. The numerous runs of the
selected deep neural network (DNN) were performed on the standard MNIST and
Fashion-MNIST datasets. The significant speedup was obtained even for extremely
low-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite
powerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training
stage (without taking into account the overheads) and speedup up to 2x for
prediction stage (with and without taking into account overheads). The precise
speedup values depend on the utilization level of TPUv2 units and increase with
the increase of the data volume under processing, but for the datasets used in
this work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was
observed for batch sizes >512 images for training phase and >40 000 images for
prediction phase. It should be noted that these results were obtained without
detriment to the prediction accuracy and loss that were equal for both GPU and
TPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd
significant digit for Fashion-MNIST dataset.
"
1569,Computing the $k$-coverage of a wireless network,"  Coverage is one of the main quality of service of a wirelessnetwork.
$k$-coverage, that is to be covered simultaneously by $k$network nodes, is
synonym of reliability and numerous applicationssuch as multiple site MIMO
features, or handovers. We introduce here anew algorithm for computing the
$k$-coverage of a wirelessnetwork. Our method is based on the observation that
$k$-coverage canbe interpreted as $k$ layers of $1$-coverage, or simply
coverage. Weuse simplicial homology to compute the network's topology and
areduction algorithm to indentify the layers of $1$-coverage. Weprovide figures
and simulation results to illustrate our algorithm.
"
1570,"Towards Automatic Transformation of Legacy Scientific Code into OpenCL
  for Optimal Performance on FPGAs","  There is a large body of legacy scientific code written in languages like
Fortran that is not optimised to get the best performance out of heterogeneous
acceleration devices like GPUs and FPGAs, and manually porting such code into
parallel languages frameworks like OpenCL requires considerable effort. We are
working towards developing a turn-key, self-optimising compiler for
accelerating scientific applications, that can automatically transform legacy
code into a solution for heterogeneous targets. In this paper we focus on FPGAs
as the acceleration devices, and carry out our discussion in the context of the
OpenCL programming framework. We show a route to automatic creation of kernels
which are optimised for execution in a ""streaming"" fashion, which gives optimal
performance on FPGAs. We use a 2D shallow-water model as an illustration;
specifically we show how the use of \emph{channels} to communicate directly
between peer kernels and the use of on-chip memory to create stencil buffers
can lead to significant performance improvements. Our results show better FPGA
performance against a baseline CPU implementation, and better energy-efficiency
against both CPU and GPU implementations.
"
1571,"Towards the Tradeoff Between Service Performance and Information
  Freshness","  The last decade has witnessed an unprecedented growth in the demand for
data-driven real-time services. These services are fueled by emerging
applications that require rapidly injecting data streams and computing updated
analytics results in real-time. In many of such applications, the computing
resources are often shared for processing both updates from information sources
and queries from end users. This requires joint scheduling of updates and
queries because the service provider needs to make a critical decision upon
receiving a user query: either it responds immediately with currently available
but possibly stale information, or it first processes new updates and then
responds with fresher information. Hence, the tradeoff between service
performance and information freshness naturally arises in this context. To that
end, we propose a simple single-server two-queue model that captures the
coupled scheduling of updates and queries and aim to design scheduling policies
that can properly address the important tradeoff between performance and
freshness. Specifically, we consider the response time as a performance metric
and the Age of Information (AoI) as a freshness metric. After demonstrating the
limitations of the simplest FCFS policy, we propose two threshold-based
policies: the Query-k policy that prioritizes queries and the Update-k policy
that prioritizes updates. Then, we rigorously analyze both the response time
and the Peak AoI (PAoI) of the threshold-based policies. Further, we propose
the Joint-(M,N) policy, which allows flexibly prioritizing updates or queries
through choosing different values of two thresholds M and N. Finally, we
conduct simulations to evaluate the response time and the PAoI of the proposed
policies. The results show that our proposed threshold-based policies can
effectively control the balance between performance and freshness.
"
1572,Resource Allocation in One-dimensional Distributed Service Networks,"  We consider assignment policies that allocate resources to users, where both
resources and users are located on a one-dimensional line. First, we consider
unidirectional assignment policies that allocate resources only to users
located to their left. We propose the Move to Right (MTR) policy, which scans
from left to right assigning nearest rightmost available resource to a user,
and contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While
both these policies are optimal among all unidirectional policies, we show that
they are equivalent with respect to the expected distance traveled by a request
(request distance), although MTR is fairer. Moreover, we show that when user
and resource locations are modeled by statistical point processes, and
resources are allowed to satisfy more than one user, the spatial system under
unidirectional policies can be mapped into bulk service queuing systems, thus
allowing the application of a plethora of queuing theory results that yield
closed form expressions. As we consider a case where different resources can
satisfy different numbers of users, we also generate new results for bulk
service queues. We also consider bidirectional policies where there are no
directional restrictions on resource allocation and develop an algorithm for
computing the optimal assignment which is more efficient than known algorithms
in the literature when there are more resources than users. Finally, numerical
evaluation of performance of unidirectional and bidirectional allocation
schemes yields design guidelines beneficial for resource placement.
"
1573,Three Other Models of Computer System Performance,"  This note argues for more use of simple models beyond Amdahl's Law:
Bottleneck Analysis, Little's Law, and a M/M/1 Queue.
"
1574,Optimal Asynchronous Dynamic Policies in Energy-Efficient Data Centers,"  In this paper, we use a Markov decision process to find optimal asynchronous
policy of an energy-efficient data center with two groups of heterogeneous
servers, a finite buffer, and a fast setup process at sleep state. Servers in
Group 1 always work. Servers in Group 2 may either work or sleep, and a fast
setup process occurs when server's states are changed from sleep to work. In
such a data center, an asynchronous dynamic policy is designed as two
sub-policies: The setup policy and the sleep policy, which determine the switch
rule between the work and sleep states for the servers in Group 2. To analyze
the optimal asynchronous dynamic policy, we apply the Markov decision process
to establish a policy-based Poisson equation, which provides expression for the
unique solution of the performance potential by means of the RG-factorization.
Based on this, we can characterize the monotonicity and optimality of the
long-run average profit of the data center with respect to the asynchronous
dynamic policy under different service prices. Furthermore, we prove that the
bang-bang control is always optimal for this optimization problem, and supports
a threshold-type dynamic control in the energy-efficient data center. We hope
that the methodology and results derived in this paper can shed light to the
study of more general energy-efficient data centers.
"
1575,Star sampling with and without replacement,"  Star sampling (SS) is a random sampling procedure on a graph wherein each
sample consists of a randomly selected vertex (the star center) and its one-hop
neighbors (the star endpoints). We consider the use of star sampling to find
any member of an arbitrary target set of vertices in a graph, where the figure
of merit (cost) is either the expected number of samples (unit cost) or the
expected number of star centers plus star endpoints (linear cost) until a
vertex in the target set is encountered, either as a star center or as a star
point. We analyze this performance measure on three related star sampling
paradigms: SS with replacement (SSR), SS without center replacement (SSC), and
SS without star replacement (SSS). We derive exact and approximate expressions
for the expected unit and linear costs of SSR, SSC, and SSS on Erdos-Renyi (ER)
graphs. Our results show there is i) little difference in unit cost, but ii)
significant difference in linear cost, across the three paradigms. Although our
results are derived for ER graphs, experiments on ""real-world"" graphs suggest
our performance expressions are reasonably accurate for non-ER graphs.
"
1576,"Large Scale Studies of Memory, Storage, and Network Failures in a Modern
  Data Center","  The workloads running in the modern data centers of large scale Internet
service providers (such as Amazon, Baidu, Facebook, Google, and Microsoft)
support billions of users and span globally distributed infrastructure. Yet,
the devices used in modern data centers fail due to a variety of causes, from
faulty components to bugs to misconfiguration. Faulty devices make operating
large scale data centers challenging because the workloads running in modern
data centers consist of interdependent programs distributed across many
servers, so failures that are isolated to a single device can still have a
widespread effect on a workload. In this dissertation, we measure and model the
device failures in a large scale Internet service company, Facebook. We focus
on three device types that form the foundation of Internet service data center
infrastructure: DRAM for main memory, SSDs for persistent storage, and switches
and backbone links for network connectivity. For each of these device types, we
analyze long term device failure data broken down by important device
attributes and operating conditions, such as age, vendor, and workload. We also
build and release statistical models to examine the failure trends for the
devices we analyze. Our key conclusion in this dissertation is that we can gain
a deep understanding of why devices fail---and how to predict their
failure---using measurement and modeling. We hope that the analysis,
techniques, and models we present in this dissertation will enable the
community to better measure, understand, and prepare for the hardware
reliability challenges we face in the future.
"
1577,"Blind GB-PANDAS: A Blind Throughput-Optimal Load Balancing Algorithm for
  Affinity Scheduling","  Dynamic affinity load balancing of multi-type tasks on multi-skilled servers,
when the service rate of each task type on each of the servers is known and can
possibly be different from each other, is an open problem for over three
decades. The goal is to do task assignment on servers in a real time manner so
that the system becomes stable, which means that the queue lengths do not
diverge to infinity in steady state (throughput optimality), and the mean task
completion time is minimized (delay optimality). The fluid model planning,
Max-Weight, and c-$\mu$-rule algorithms have theoretical guarantees on
optimality in some aspects for the affinity problem, but they consider a
complicated queueing structure and either require the task arrival rates, the
service rates of tasks on servers, or both. In many cases that are discussed in
the introduction section, both task arrival rates and service rates of
different task types on different servers are unknown. In this work, the Blind
GB-PANDAS algorithm is proposed which is completely blind to task arrival rates
and service rates. Blind GB-PANDAS uses an exploration-exploitation approach
for load balancing. We prove that Blind GB-PANDAS is throughput optimal under
arbitrary and unknown distributions for service times of different task types
on different servers and unknown task arrival rates. Blind GB-PANDAS desires to
route an incoming task to the server with the minimum weighted-workload, but
since the service rates are unknown, such routing of incoming tasks is not
guaranteed which makes the throughput optimality analysis more complicated than
the case where service rates are known. Our extensive experimental results
reveal that Blind GB-PANDAS significantly outperforms existing methods in terms
of mean task completion time at high loads.
"
1578,Combinatorial Sleeping Bandits with Fairness Constraints,"  The multi-armed bandit (MAB) model has been widely adopted for studying many
practical optimization problems (network resource allocation, ad placement,
crowdsourcing, etc.) with unknown parameters. The goal of the player here is to
maximize the cumulative reward in the face of uncertainty. However, the basic
MAB model neglects several important factors of the system in many real-world
applications, where multiple arms can be simultaneously played and an arm could
sometimes be ""sleeping"". Besides, ensuring fairness is also a key design
concern in practice. To that end, we propose a new Combinatorial Sleeping MAB
model with Fairness constraints, called CSMAB-F, aiming to address the
aforementioned crucial modeling issues. The objective is now to maximize the
reward while satisfying the fairness requirement of a minimum selection
fraction for each individual arm. To tackle this new problem, we extend an
online learning algorithm, UCB, to deal with a critical tradeoff between
exploitation and exploration and employ the virtual queue technique to properly
handle the fairness constraints. By carefully integrating these two techniques,
we develop a new algorithm, called Learning with Fairness Guarantee (LFG), for
the CSMAB-F problem. Further, we rigorously prove that not only LFG is
feasibility-optimal, but it also has a time-average regret upper bounded by
$\frac{N}{2\eta}+\frac{\beta_1\sqrt{mNT\log{T}}+\beta_2 N}{T}$, where N is the
total number of arms, m is the maximum number of arms that can be
simultaneously played, T is the time horizon, $\beta_1$ and $\beta_2$ are
constants, and $\eta$ is a design parameter that we can tune. Finally, we
perform extensive simulations to corroborate the effectiveness of the proposed
algorithm. Interestingly, the simulation results reveal an important tradeoff
between the regret and the speed of convergence to a point satisfying the
fairness constraints.
"
1579,Mechanism to Mitigate AVX-Induced Frequency Reduction,"  Modern Intel CPUs reduce their frequency when executing wide vector
operations (AVX2 and AVX-512 instructions), as these instructions increase
power consumption. The frequency is only increased again two milliseconds after
the last code section containing such instructions has been executed in order
to prevent excessive numbers of frequency changes. Due to this delay,
intermittent use of wide vector operations can slow down the rest of the system
significantly. For example, previous work has shown the performance of web
servers to be reduced by up to 10% if the SSL library uses AVX-512 vector
instructions. These performance variations are hard to predict during software
development as the performance impact of vectorization depends on the specific
workload.
  We describe a mechanism to reduce the slowdown caused by wide vector
instructions without requiring extensive changes to existing software. Our
design allows the developer to mark problematic AVX code regions. The scheduler
then restricts execution of this code to a subset of the cores so that only
these cores' frequency is affected. Threads are automatically migrated to a
suitable core whenever necessary. We identify a suitable load balancing policy
to ensure good utilization of all available cores. Our approach is able to
reduce the performance variability caused by AVX2 and AVX-512 instructions by
over 70%.
"
1580,"Analytic Performance Modeling and Analysis of Detailed Neuron
  Simulations","  Big science initiatives are trying to reconstruct and model the brain by
attempting to simulate brain tissue at larger scales and with increasingly more
biological detail than previously thought possible. The exponential growth of
parallel computer performance has been supporting these developments, and at
the same time maintainers of neuroscientific simulation code have strived to
optimally and efficiently exploit new hardware features. Current state of the
art software for the simulation of biological networks has so far been
developed using performance engineering practices, but a thorough analysis and
modeling of the computational and performance characteristics, especially in
the case of morphologically detailed neuron simulations, is lacking. Other
computational sciences have successfully used analytic performance engineering
and modeling methods to gain insight on the computational properties of
simulation kernels, aid developers in performance optimizations and eventually
drive co-design efforts, but to our knowledge a model-based performance
analysis of neuron simulations has not yet been conducted.
  We present a detailed study of the shared-memory performance of
morphologically detailed neuron simulations based on the Execution-Cache-Memory
(ECM) performance model. We demonstrate that this model can deliver accurate
predictions of the runtime of almost all the kernels that constitute the neuron
models under investigation. The gained insight is used to identify the main
governing mechanisms underlying performance bottlenecks in the simulation. The
implications of this analysis on the optimization of neural simulation software
and eventually co-design of future hardware architectures are discussed. In
this sense, our work represents a valuable conceptual and quantitative
contribution to understanding the performance properties of biological networks
simulations.
"
1581,"A Multilevel Approach for the Performance Analysis of Parallel
  Algorithms","  We provide a multilevel approach for analysing performances of parallel
algorithms. The main outcome of such approach is that the algorithm is
described by using a set of operators which are related to each other according
to the problem decomposition. Decomposition level determines the granularity of
the algorithm. A set of block matrices (decomposition and execution) highlights
fundamental characteristics of the algorithm, such as inherent parallelism and
sources of overheads.
"
1582,"On the Capacity Region of Bipartite and Tripartite Entanglement
  Switching","  We study a quantum switch serving a set of users. The function of the switch
is to create bi- or tripartite entangled state among users at the highest
possible rates at a fixed ratio. We model a set of randomized switching
policies. Discovering that some are better than others, we present analytical
results for the case where the switch stores one qubit per user, and find that
the best policies outperform a time division multiplexing (TDM) policy for
sharing the switch between bipartite and tripartite state generation. This
performance improvement decreases as the number of users grows. The model is
easily augmented to study the capacity region in the presence of qubit
decoherence, obtaining similar results. Moreover, decoherence appears to have
little effect on capacity. We also study a smaller class of policies when the
switch stores two qubits per user.
"
1583,High order concentrated non-negative matrix-exponential functions,"  Highly concentrated functions play an important role in many research fields
including control system analysis and physics, and they turned out to be the
key idea behind inverse Laplace transform methods as well.
  This paper uses the matrix-exponential family of functions to create highly
concentrated functions, whose squared coefficient of variation (SCV) is very
low. In the field of stochastic modeling, matrix-exponential functions have
been used for decades. They have many advantages: they are easy to manipulate,
always non-negative, and integrals involving matrix-exponential functions often
have closed-form solutions. For the time being there is no symbolic
construction available to obtain the most concentrated matrix-exponential
functions, and the numerical optimization-based approach has many pitfalls,
too.
  In this paper, we present a numerical optimization-based procedure to
construct highly concentrated matrix-exponential functions. To make the
objective function explicit and easy to evaluate we introduce and use a new
representation called hyper-trigonometric representation. This representation
makes it possible to achieve very low SCV.
"
1584,Derandomized Load Balancing using Random Walks on Expander Graphs,"  In a computing center with a huge amount of machines, when a job arrives, a
dispatcher need to decide which machine to route this job to based on limited
information. A classical method, called the power-of-$d$ choices algorithm is
to pick $d$ servers independently at random and dispatch the job to the least
loaded server among the $d$ servers. In this paper, we analyze a low-randomness
variant of this dispatching scheme, where $d$ queues are sampled through $d$
independent non-backtracking random walks on a $k$-regular graph $G$. Under
certain assumptions of the graph $G$ we show that under this scheme, the
dynamics of the queuing system converges to the same deterministic ordinary
differential equation (ODE) for the power-of-$d$ choices scheme. We also show
that the system is stable under the proposed scheme, and the stationary
distribution of the system converges to the fixed point of the ODE.
"
1585,Competitive Online Optimization under Inventory Constraints,"  This paper studies online optimization under inventory (budget) constraints.
While online optimization is a well-studied topic, versions with inventory
constraints have proven difficult. We consider a formulation of
inventory-constrained optimization that is a generalization of the classic
one-way trading problem and has a wide range of applications. We present a new
algorithmic framework, \textsf{CR-Pursuit}, and prove that it achieves the
minimal competitive ratio among all deterministic algorithms (up to a
problem-dependent constant factor) for inventory-constrained online
optimization. Our algorithm and its analysis not only simplify and unify the
state-of-the-art results for the standard one-way trading problem, but they
also establish novel bounds for generalizations including concave revenue
functions. For example, for one-way trading with price elasticity, the
\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within a
small additive constant (i.e., 1/3) to the lower bound of $\ln \theta+1$, where
$\theta$ is the ratio between the maximum and minimum base prices.
"
1586,Overbooking Microservices in the Cloud,"  We consider the problem of scheduling serverless-computing instances such as
Amazon Lambda functions, or scheduling microservices within (privately held)
virtual machines (VMs). Instead of a quota per tenant/customer, we assume
demand for Lambda functions is modulated by token-bucket mechanisms per tenant.
Such quotas are due to, e.g., limited resources (as in a fog/edge-cloud
context) or to prevent excessive unauthorized invocation of numerous instances
by malware. Based on an upper bound on the stationary number of active ""Lambda
servers"" considering the execution-time distribution of Lambda functions, we
describe an approach that the cloud could use to overbook Lambda functions for
improved utilization of IT resources. An earlier bound for a single service
tier is extended to multiple service tiers. For the context of scheduling
microservices in a private setting, the framework could be used to determine
the required VM resources for a token-bucket constrained workload stream.
Finally, we note that the looser Markov inequality may be useful in settings
where the job service times are dependent.
"
1587,On transaction parallelizability in Ethereum,"  Ethereum clients execute transactions in a sequential order prescribed by the
consensus protocol. This is a safe and conservative approach to blockchain
transaction processing which forgoes running transactions in parallel even when
doing so would be beneficial and safe, e.g., when there is no intersection in
the sets of accounts that the transactions read or modify. In this work we
study the degree of transaction parallelizability and present results from
three different simulations using real Ethereum transaction data. Our
simulations demonstrate that notable gains are achievable with parallelization,
and suggest that the potential for parallelizability improves as transaction
rates increase.
"
1588,"A Modular Benchmarking Infrastructure for High-Performance and
  Reproducible Deep Learning","  We introduce Deep500: the first customizable benchmarking infrastructure that
enables fair comparison of the plethora of deep learning frameworks,
algorithms, libraries, and techniques. The key idea behind Deep500 is its
modular design, where deep learning is factorized into four distinct levels:
operators, network processing, training, and distributed training. Our
evaluation illustrates that Deep500 is customizable (enables combining and
benchmarking different deep learning codes) and fair (uses carefully selected
metrics). Moreover, Deep500 is fast (incurs negligible overheads), verifiable
(offers infrastructure to analyze correctness), and reproducible. Finally, as
the first distributed and reproducible benchmarking system for deep learning,
Deep500 provides software infrastructure to utilize the most powerful
supercomputers for extreme-scale workloads.
"
1589,Towards an Achievable Performance for the Loop Nests,"  Numerous code optimization techniques, including loop nest optimizations,
have been developed over the last four decades. Loop optimization techniques
transform loop nests to improve the performance of the code on a target
architecture, including exposing parallelism. Finding and evaluating an
optimal, semantic-preserving sequence of transformations is a complex problem.
The sequence is guided using heuristics and/or analytical models and there is
no way of knowing how close it gets to optimal performance or if there is any
headroom for improvement. This paper makes two contributions. First, it uses a
comparative analysis of loop optimizations/transformations across multiple
compilers to determine how much headroom may exist for each compiler. And
second, it presents an approach to characterize the loop nests based on their
hardware performance counter values and a Machine Learning approach that
predicts which compiler will generate the fastest code for a loop nest. The
prediction is made for both auto-vectorized, serial compilation and for
auto-parallelization. The results show that the headroom for state-of-the-art
compilers ranges from 1.10x to 1.42x for the serial code and from 1.30x to
1.71x for the auto-parallelized code. These results are based on the Machine
Learning predictions.
"
1590,"A Framework for Allocating Server Time to Spot and On-demand Services in
  Cloud Computing","  Cloud computing delivers value to users by facilitating their access to
computing capacity in periods when their need arises. An approach is to provide
both on-demand and spot services on shared servers. The former allows users to
access servers on demand at a fixed price and users occupy different periods of
servers. The latter allows users to bid for the remaining unoccupied periods
via dynamic pricing; however, without appropriate design, such periods may be
arbitrarily small since on-demand users arrive randomly. This is also the
current service model adopted by Amazon Elastic Cloud Compute. In this paper,
we provide the first integral framework for sharing the time of servers between
on-demand and spot services while optimally pricing spot instances. It
guarantees that on-demand users can get served quickly while spot users can
stably utilize servers for a properly long period once accepted, which is a key
feature to make both on-demand and spot services accessible. Simulation results
show that, by complementing the on-demand market with a spot market, a cloud
provider can improve revenue by up to 464.7%. The framework is designed under
assumptions which are met in real environments. It is a new tool that cloud
operators can use to quantify the advantage of a hybrid spot and on-demand
service, eventually making the case for operating such service model in their
own infrastructures.
"
1591,Blaze: Simplified High Performance Cluster Computing,"  MapReduce and its variants have significantly simplified and accelerated the
process of developing parallel programs. However, most MapReduce
implementations focus on data-intensive tasks while many real-world tasks are
compute intensive and their data can fit distributedly into the memory. For
these tasks, the speed of MapReduce programs can be much slower than those
hand-optimized ones. We present Blaze, a C++ library that makes it easy to
develop high performance parallel programs for such compute intensive tasks. At
the core of Blaze is a highly-optimized in-memory MapReduce function, which has
three main improvements over conventional MapReduce implementations: eager
reduction, fast serialization, and special treatment for a small fixed key
range. We also offer additional conveniences that make developing parallel
programs similar to developing serial programs. These improvements make Blaze
an easy-to-use cluster computing library that approaches the speed of
hand-optimized parallel code. We apply Blaze to some common data mining tasks,
including word frequency count, PageRank, k-means, expectation maximization
(Gaussian mixture model), and k-nearest neighbors. Blaze outperforms Apache
Spark by more than 10 times on average for these tasks, and the speed of Blaze
scales almost linearly with the number of nodes. In addition, Blaze uses only
the MapReduce function and 3 utility functions in its implementation while
Spark uses almost 30 different parallel primitives in its official
implementation.
"
1592,"Network Resilience Assessment via QoS Degradation Metrics: An
  Algorithmic Approach","  This paper focuses on network resilience to perturbation of edge weight.
Other than connectivity, many network applications nowadays rely upon some
measure of network distance between a pair of connected nodes. In these
systems, a metric related to network functionality is associated to each edge.
A pair of nodes only being functional if the weighted, shortest-path distance
between the pair is below a given threshold \texttt{T}. Consequently, a natural
question is on which degree the change of edge weights can damage the network
functionality? With this motivation, we study a new problem, \textit{Quality of
Service Degradation}: given a set of pairs, find a minimum budget to increase
the edge weights which ensures the distance between each pair exceeds
$\mathtt{T}$. We introduce four algorithms with theoretical performance
guarantees for this problem. Each of them has its own strength in trade-off
between effectiveness and running time, which are illustrated both in theory
and comprehensive experimental evaluation.
"
1593,"Faster Remainder by Direct Computation: Applications to Compilers and
  Software Libraries","  On common processors, integer multiplication is many times faster than
integer division. Dividing a numerator n by a divisor d is mathematically
equivalent to multiplication by the inverse of the divisor (n / d = n x 1/d).
If the divisor is known in advance---or if repeated integer divisions will be
performed with the same divisor---it can be beneficial to substitute a less
costly multiplication for an expensive division.
  Currently, the remainder of the division by a constant is computed from the
quotient by a multiplication and a subtraction. But if just the remainder is
desired and the quotient is unneeded, this may be suboptimal. We present a
generally applicable algorithm to compute the remainder more directly.
Specifically, we use the fractional portion of the product of the numerator and
the inverse of the divisor. On this basis, we also present a new, simpler
divisibility algorithm to detect nonzero remainders.
  We also derive new tight bounds on the precision required when representing
the inverse of the divisor. Furthermore, we present simple C implementations
that beat the optimized code produced by state-of-art C compilers on recent x64
processors (e.g., Intel Skylake and AMD Ryzen), sometimes by more than 25%. On
all tested platforms including 64-bit ARM and POWER8, our divisibility-test
functions are faster than state-of-the-art Granlund-Montgomery
divisibility-test functions, sometimes by more than 50%.
"
1594,A token-based central queue with order-independent service rates,"  We study a token-based central queue with multiple customer types. Customers
of each type arrive according to a Poisson process and have an associated set
of compatible tokens. Customers may only receive service when they have claimed
a compatible token. If upon arrival, more than one compatible token is
available, an assignment rule determines which token will be claimed. The
service rate obtained by a customer is state-dependent, i.e., it depends on the
set of claimed tokens and on the number of customers in the system. Our first
main result shows that, provided the assignment rule and the service rates
satisfy certain conditions, the steady-state distribution has a product form.
We show that our model subsumes known families of models that have product-form
steady-state distributions including the order-independent queue of Krzesinski
(2011) and the model of Visschers et al. (2012). Our second main contribution
involves the derivation of expressions for relevant performance measures such
as the sojourn time and the number of customers present in the system. We apply
our framework to relevant models, including an M/M/K queue with heterogeneous
service rates, the MSCCC queue, multi-server models with redundancy and
matching models. For some of these models, we present expressions for
performance measures that have not been derived before.
"
1595,Revec: Program Rejuvenation through Revectorization,"  Modern microprocessors are equipped with Single Instruction Multiple Data
(SIMD) or vector instructions which expose data level parallelism at a fine
granularity. Programmers exploit this parallelism by using low-level vector
intrinsics in their code. However, once programs are written using vector
intrinsics of a specific instruction set, the code becomes non-portable. Modern
compilers are unable to analyze and retarget the code to newer vector
instruction sets. Hence, programmers have to manually rewrite the same code
using vector intrinsics of a newer generation to exploit higher data widths and
capabilities of new instruction sets. This process is tedious, error-prone and
requires maintaining multiple code bases. We propose Revec, a compiler
optimization pass which revectorizes already vectorized code, by retargeting it
to use vector instructions of newer generations. The transformation is
transparent, happening at the compiler intermediate representation level, and
enables performance portability of hand-vectorized code.
  Revec can achieve performance improvements in real-world performance critical
kernels. In particular, Revec achieves geometric mean speedups of 1.160$\times$
and 1.430$\times$ on fast integer unpacking kernels, and speedups of
1.145$\times$ and 1.195$\times$ on hand-vectorized x265 media codec kernels
when retargeting their SSE-series implementations to use AVX2 and AVX-512
vector instructions respectively. We also extensively test Revec's impact on
216 intrinsic-rich implementations of image processing and stencil kernels
relative to hand-retargeting.
"
1596,"Accuracy vs. Computational Cost Tradeoff in Distributed Computer System
  Simulation","  Simulation is a fundamental research tool in the computer architecture field.
These kinds of tools enable the exploration and evaluation of architectural
proposals capturing the most relevant aspects of the highly complex systems
under study. Many state-of-the-art simulation tools focus on single-system
scenarios, but the scalability required by trending applications has shifted
towards distributed computing systems integrated via complex software stacks.
Web services with client-server architectures or distributed storage and
processing of scale-out data analytics (Big Data) are among the main exponents.
The complete simulation of a distributed computer system is the appropriate
methodology to conduct accurate evaluations. Unfortunately, this methodology
could have a significant impact on the already large computational effort
derived from detailed simulation. In this work, we conduct a set of experiments
to evaluate this accuracy/cost tradeoff. We measure the error made if
client-server applications are evaluated in a single-node environment, as well
as the overhead induced by the methodology and simulation tool employed for
multi-node simulations. We quantify this error for different micro-architecture
components, such as last-level cache and instruction/data TLB. Our findings
show that accuracy loss can lead to completely wrong conclusions about the
effects of proposed hardware optimizations. Fortunately, our results also
demonstrate that the computational overhead of a multi-node simulation
framework is affordable, suggesting multi-node simulation as the most
appropriate methodology.
"
1597,"Deterministic contention management for low latency Cloud RAN over an
  optical ring","  The N-GREEN project has for goal to design a low cost optical ring technology
with good performances (throughput, latency...) without using expensive
end-to-end connections. We study the compatibility of such a technology with
the development of the Cloud RAN, a latency critical application which is a
major aspect of 5G deployment. We show that deterministically managing Cloud
RAN traffic minimizes its latency while also improving the latency of the other
traffics.
"
1598,Performance Modeling of Microservice Platforms,"  Microservice architecture has transformed the way developers are building and
deploying applications in the nowadays cloud computing centers. This new
approach provides increased scalability, flexibility, manageability, and
performance while reducing the complexity of the whole software development
life cycle. The increase in cloud resource utilization also benefits
microservice providers. Various microservice platforms have emerged to
facilitate the DevOps of containerized services by enabling continuous
integration and delivery. Microservice platforms deploy application containers
on virtual or physical machines provided by public/private cloud
infrastructures in a seamless manner. In this paper, we study and evaluate the
provisioning performance of microservice platforms by incorporating the details
of all layers (i.e., both micro and macro layers) in the modelling process. To
this end, we first build a microservice platform on top of Amazon EC2 cloud and
then leverage it to develop a comprehensive performance model to perform
what-if analysis and capacity planning for microservice platforms at scale. In
other words, the proposed performance model provides a systematic approach to
measure the elasticity of the microservice platform by analyzing the
provisioning performance at both the microservice platform and the back-end
macroservice infrastructures.
"
1599,"Multi-tier Caching Analysis in CDN-based Over-the-top Video Streaming
  Systems","  Internet video traffic has been been rapidly increasing and is further
expected to increase with the emerging 5G applications such as higher
definition videos, IoT and augmented/virtual reality applications. As end-users
consume video in massive amounts and in an increasing number of ways, the
content distribution network (CDN) should be efficiently managed to improve the
system efficiency. The streaming service can include multiple caching tiers, at
the distributed servers and the edge routers, and efficient content management
at these locations affect the quality of experience (QoE) of the end users. In
this paper, we propose a model for video streaming systems, typically composed
of a centralized origin server, several CDN sites, and edge-caches located
closer to the end user. We comprehensively consider different systems design
factors including the limited caching space at the CDN sites, allocation of CDN
for a video request, choice of different ports (or paths) from the CDN and the
central storage, bandwidth allocation, the edge-cache capacity, and the caching
policy. We focus on minimizing a performance metric, stall duration tail
probability (SDTP), and present a novel and efficient algorithm accounting for
the multiple design flexibilities. The theoretical bounds with respect to the
SDTP metric are also analyzed and presented. The implementation on a
virtualized cloud system managed by Openstack demonstrate that the proposed
algorithms can significantly improve the SDTP metric, compared to the baseline
strategies.
"
1600,"WiseMove: A Framework for Safe Deep Reinforcement Learning for
  Autonomous Driving","  Machine learning can provide efficient solutions to the complex problems
encountered in autonomous driving, but ensuring their safety remains a
challenge. A number of authors have attempted to address this issue, but there
are few publicly-available tools to adequately explore the trade-offs between
functionality, scalability, and safety.
  We thus present WiseMove, a software framework to investigate safe deep
reinforcement learning in the context of motion planning for autonomous
driving. WiseMove adopts a modular learning architecture that suits our current
research questions and can be adapted to new technologies and new questions. We
present the details of WiseMove, demonstrate its use on a common traffic
scenario, and describe how we use it in our ongoing safe learning research.
"
1601,"Finite Horizon Throughput Maximization for a Wirelessly Powered Device
  over a Time Varying Channel","  In this work, we consider an energy harvesting device (EHD) served by an
access point with a single antenna that is used for both wireless power
transfer (WPT) and data transfer. The objective is to maximize the expected
throughput of the EHD over a finite horizon when the channel state information
is only available causally. The EHD is energized by WPT for a certain duration,
which is subject to optimization, and then, EHD transmits its information bits
to the AP until the end of the time horizon by employing optimal dynamic power
allocation. The joint optimization problem is modeled as a dynamic programming
problem. Based on the characteristic of the problem, we prove that a time
dependent threshold type structure exists for the optimal WPT duration, and we
obtain closed form solution to the dynamic power allocation in the uplink
period.
"
1602,"Communication over a time correlated channel with an energy harvesting
  transmitter","  In this work, communication over a time-correlated point-to-point wireless
channel is studied for an energy harvesting (EH) transmitter. In this model, we
take into account the time and energy cost of acquiring channel state
information. At the beginning of the time slot, the EH transmitter, has to
choose among three possible actions: i) deferring the transmission to save its
energy for future use, ii) transmitting without sensing, and iii) sensing the
channel before transmission. At each time slot, the transmitter chooses one of
the three possible actions to maximize the total expected discounted number of
bits transmitted over an infinite time horizon. This problem can be formulated
as a partially observable Markov decision process (POMDP) which is then
converted to an ordinary MDP by introducing a belief on the channel state, and
the optimal policy is shown to exhibit a threshold behavior on the belief
state, with battery-dependent threshold values. Optimal threshold values and
corresponding optimal performance are characterized through numerical
simulations, and it is shown that having the sensing action and intelligently
using it to track the channel state improves the achievable long-term
throughput significantly.
"
1603,Wireless energy and information transfer in networks with hybrid ARQ,"  In this paper, we consider a class of wireless powered communication devices
using hybrid automatic repeat request (HARQ) protocol to ensure reliable
communications. In particular, we analyze the trade-off between accumulating
mutual information and harvesting RF energy at the receiver of a point-to-point
link over a time-varying independent and identically distributed (i.i.d.)
channel. The transmitter is assumed to have a constant energy source while the
receiver relies, solely, on the RF energy harvested from the received signal.
At each time slot, the incoming RF signal is split between information
accumulation and energy accumulation with the objective of minimizing the
expected number of re-transmissions. A major finding of this work is that the
optimal policy minimizing the expected number of re-transmissions utilizes the
incoming RF signal to either exclusively harvest energy or to accumulate mutual
information. This finding enables achieving an optimal solution in feasible
time by converting a two dimensional uncountable state Markov decision process
(MDP) with continuous action space into a countable state MDP with binary
decision space.
"
1604,Mesh: Compacting Memory Management for C/C++ Applications,"  Programs written in C/C++ can suffer from serious memory fragmentation,
leading to low utilization of memory, degraded performance, and application
failure due to memory exhaustion. This paper introduces Mesh, a plug-in
replacement for malloc that, for the first time, eliminates fragmentation in
unmodified C/C++ applications. Mesh combines novel randomized algorithms with
widely-supported virtual memory operations to provably reduce fragmentation,
breaking the classical Robson bounds with high probability. Mesh generally
matches the runtime performance of state-of-the-art memory allocators while
reducing memory consumption; in particular, it reduces the memory of
consumption of Firefox by 16% and Redis by 39%.
"
1605,Energy harvesting wireless networks with correlated energy sources,"  This work considers a system with two energy harvesting (EH) nodes
transmitting to a common destination over a random access channel. The amount
of harvested energy is assumed to be random and independent over time, but
correlated among the nodes possibly with respect to their relative position. A
threshold-based transmission policy is developed for the maximization of the
expected aggregate network throughput. Assuming that there is no a priori
channel state or EH information available to the nodes, the aggregate network
throughput is obtained. The optimal thresholds are determined for two
practically important special cases: i) at any time only one of the sensors
harvests energy due to, for example, physical separation of the nodes; ii) the
nodes are spatially close, and at any time, either both nodes or none of them
harvests energy.
"
1606,Redundant Loads: A Software Inefficiency Indicator,"  Modern software packages have become increasingly complex with millions of
lines of code and references to many external libraries. Redundant operations
are a common performance limiter in these code bases. Missed compiler
optimization opportunities, inappropriate data structure and algorithm choices,
and developers' inattention to performance are some common reasons for the
existence of redundant operations. Developers mainly depend on compilers to
eliminate redundant operations. However, compilers' static analysis often
misses optimization opportunities due to ambiguities and limited analysis
scope; automatic optimizations to algorithmic and data structural problems are
out of scope.
  We develop LoadSpy, a whole-program profiler to pinpoint redundant memory
load operations, which are often a symptom of many redundant operations. The
strength of LoadSpy exists in identifying and quantifying redundant load
operations in programs and associating the redundancies with program execution
contexts and scopes to focus developers' attention on problematic code. LoadSpy
works on fully optimized binaries, adopts various optimization techniques to
reduce its overhead, and provides a rich graphic user interface, which make it
a complete developer tool. Applying LoadSpy showed that a large fraction of
redundant loads is common in modern software packages despite highest levels of
automatic compiler optimizations. Guided by LoadSpy, we optimize several
well-known benchmarks and real-world applications, yielding significant
speedups.
"
1607,"A Comparison of Random Task Graph Generation Methods for Scheduling
  Problems","  How to generate instances with relevant properties and without bias remains
an open problem of critical importance for a fair comparison of heuristics. In
the context of scheduling with precedence constraints, the instance consists of
a task graph that determines a partial order on task executions. To avoid
selecting instances among a set populated mainly with trivial ones, we rely on
properties that quantify the characteristics specific to difficult instances.
Among numerous identified such properties, the mass measures how much a task
graph can be decomposed into smaller ones. This property, together with an
in-depth analysis of existing random task graph generation methods, establishes
the sub-exponential generic time complexity of the studied problem. Empirical
observations on the impact of existing generation methods on scheduling
heuristics concludes our study.
"
1608,Graph Computing based Distributed Fast Decoupled Power Flow Analysis,"  Power flow analysis plays a fundamental and critical role in the energy
management system (EMS). It is required to well accommodate large and complex
power system. To achieve a high performance and accurate power flow analysis, a
graph computing based distributed power flow analysis approach is proposed in
this paper. Firstly, a power system network is divided into multiple areas.
Slack buses are selected for each area and, at each SCADA sampling period, the
inter-area transmission line power flows are equivalently allocated as extra
load injections to corresponding buses. Then, the system network is converted
into multiple independent areas. In this way, the power flow analysis could be
conducted in parallel for each area and the solved system states could be
guaranteed without compromise of accuracy. Besides, for each area, graph
computing based fast decoupled power flow (FDPF) is employed to quickly analyze
system states. IEEE 118-bus system and MP 10790-bus system are employed to
verify the results accuracy and present the promising computation performance
of the proposed approach.
"
1609,"Development of a Real-Time Software-Defined Radio GPS Receiver
  Exploiting a LabVIEW-based Instrumentation Environment","  The ubiquitousness of location based services (LBS) has proven effective for
many applications such as commercial, military, and emergency responders.
Software-defined radio (SDR) has emerged as an adequate framework for
development and testing of global navigational satellite systems (GNSS) such as
the Global Position System (GPS). SDR receivers are constantly developing in
terms of acceleration factors and accurate algorithms for precise user
navigation. However, many SDR options for GPS receivers currently lack
real-time operation or could be costly. This paper presents a LabVIEW (LV) and
C/C++ based GPS L1 receiver platform with real-time capabilities. The system
relies on LV acceleration factors as well as other C/C++ techniques such as
dynamic link library (DLL) integration into LV and parallelizable loop
structures, and single input multiple data (SIMD) methods which leverage host
PC multi-purpose processors. A hardware testbed is presented for compactness
and mobility, as well as software functionality and data flow handling inherent
in LV environment. Benchmarks and other real-time results are presented as well
as compared against other state-of-the-art open-source GPS receivers.
"
1610,Performance Analysis of Online Social Platforms,"  We introduce an original mathematical model to analyze the diffusion of posts
within a generic online social platform. Each user of such a platform has his
own Wall and Newsfeed, as well as his own self-posting and re-posting activity.
As a main result, using our developed model, we derive in closed form the
probabilities that posts originating from a given user are found on the Wall
and Newsfeed of any other. These probabilities are the solution of a linear
system of equations. Conditions of existence of the solution are provided, and
two ways of solving the system are proposed, one using matrix inversion and
another using fixed-point iteration. Comparisons with simulations show the
accuracy of our model and its robustness with respect to the modeling
assumptions. Hence, this article introduces a novel measure which allows to
rank users by their influence on the social platform, by taking into account
not only the social graph structure, but also the platform design, user
activity (self- and re-posting), as well as competition among posts.
"
1611,"JArena: Partitioned Shared Memory for NUMA-awareness in Multi-threaded
  Scientific Applications","  The distributed shared memory (DSM) architecture is widely used in today's
computer design to mitigate the ever-widening processing-memory gap, and
inevitably exhibits non-uniform memory access (NUMA) to shared-memory parallel
applications. Failure to achieve full NUMA-awareness can significantly
downgrade application performance, especially on today's manycore platforms
with tens to hundreds of cores. Yet traditional approaches such as first-touch
and memory policy fail short in either false page-sharing, fragmentation, or
ease-of-use. In this paper, we propose a partitioned shared memory approach
which allows multi-threaded applications to achieve full NUMA-awareness with
only minor code changes and develop a companying NUMA-aware heap manager which
eliminates false page-sharing and minimizes fragmentation. Experiments on a
256-core cc-NUMA computing node show that the proposed approach achieves true
NUMA-awareness and improves the performance of typical multi-threaded
scientific applications up to 4.3 folds with the increased use of cores.
"
1612,"Understanding the Interactions of Workloads and DRAM Types: A
  Comprehensive Experimental Study","  It has become increasingly difficult to understand the complex interaction
between modern applications and main memory, composed of DRAM chips.
Manufacturers are now selling and proposing many different types of DRAM, with
each DRAM type catering to different needs (e.g., high throughput, low power,
high memory density). At the same time, the memory access patterns of prevalent
and emerging workloads are rapidly diverging, as these applications manipulate
larger data sets in very different ways. As a result, the combined
DRAM-workload behavior is often difficult to intuitively determine today, which
can hinder memory optimizations in both hardware and software.
  In this work, we identify important families of workloads, as well as
prevalent types of DRAM chips, and rigorously analyze the combined
DRAM--workload behavior. To this end, we perform a comprehensive experimental
study of the interaction between nine different DRAM types and 115 modern
applications and multiprogrammed workloads. We draw 12 key observations from
our characterization, enabled in part by our development of new metrics that
take into account contention between memory requests due to hardware design.
Notably, we find that (1) newer DRAM types such as DDR4 and HMC often do not
outperform older types such as DDR3, due to higher access latencies and, in the
case of HMC, poor exploitation of locality; (2) there is no single DRAM type
that can cater to all components of a heterogeneous system (e.g., GDDR5
significantly outperforms other memories for multimedia acceleration, while HMC
significantly outperforms other memories for network acceleration); and (3)
there is still a strong need to lower DRAM latency, but unfortunately the
current design trend of commodity DRAM is toward higher latencies to obtain
other benefits. We hope that the trends we identify can drive optimizations in
both hardware and software design.
"
1613,Parsing Gigabytes of JSON per Second,"  JavaScript Object Notation or JSON is a ubiquitous data exchange format on
the Web. Ingesting JSON documents can become a performance bottleneck due to
the sheer volume of data. We are thus motivated to make JSON parsing as fast as
possible.
  Despite the maturity of the problem of JSON parsing, we show that substantial
speedups are possible. We present the first standard-compliant JSON parser to
process gigabytes of data per second on a single core, using commodity
processors. We can use a quarter or fewer instructions than a state-of-the-art
reference parser like RapidJSON. Unlike other validating parsers, our software
(simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD)
instructions. To ensure reproducibility, simdjson is freely available as
open-source software under a liberal license.
"
1614,Parallel Rendering and Large Data Visualization,"  We are living in the big data age: An ever increasing amount of data is being
produced through data acquisition and computer simulations. While large scale
analysis and simulations have received significant attention for cloud and
high-performance computing, software to efficiently visualise large data sets
is struggling to keep up.
  Visualization has proven to be an efficient tool for understanding data, in
particular visual analysis is a powerful tool to gain intuitive insight into
the spatial structure and relations of 3D data sets. Large-scale visualization
setups are becoming ever more affordable, and high-resolution tiled display
walls are in reach even for small institutions. Virtual reality has arrived in
the consumer space, making it accessible to a large audience.
  This thesis addresses these developments by advancing the field of parallel
rendering. We formalise the design of system software for large data
visualization through parallel rendering, provide a reference implementation of
a parallel rendering framework, introduce novel algorithms to accelerate the
rendering of large amounts of data, and validate this research and development
with new applications for large data visualization. Applications built using
our framework enable domain scientists and large data engineers to better
extract meaning from their data, making it feasible to explore more data and
enabling the use of high-fidelity visualization installations to see more
detail of the data.
"
1615,"Acceleration of expensive computations in Bayesian statistics using
  vector operations","  Many applications in Bayesian statistics are extremely computationally
intensive. However, they are also often inherently parallel, making them prime
targets for modern massively parallel central processing unit (CPU)
architectures. While the use of multi-core and distributed computing is widely
applied in the Bayesian community, very little attention has been given to
fine-grain parallelisation using single instruction multiple data (SIMD)
operations that are available on most modern commodity CPUs. Rather, most
fine-grain tuning in the literature has centred around general purpose graphics
processing units (GPGPUs). Since the effective utilisation of GPGPUs typically
requires specialised programming languages, such technologies are not ideal for
the wider Bayesian community. In this work, we practically demonstrate, using
standard programming libraries, the utility of the SIMD approach for several
topical Bayesian applications. In particular, we consider sampling of the prior
predictive distribution for approximate Bayesian computation (ABC), the
computation of Bayesian $p$-values for testing prior weak informativeness, and
inference on a computationally challenging econometrics model. Through minor
code alterations, we show that SIMD operations can improve the floating point
arithmetic performance resulting in up to $6\times$ improvement in the overall
serial algorithm performance. Furthermore $4$-way parallel versions can lead to
almost $19\times$ improvement over a na\""{i}ve serial implementation. We
illustrate the potential of SIMD operations for accelerating Bayesian
computations and provide the reader with essential implementation techniques
required to exploit modern massively parallel processing environments using
standard software development tools.
"
1616,"Stateful Dataflow Multigraphs: A Data-Centric Model for Performance
  Portability on Heterogeneous Architectures","  The ubiquity of accelerators in high-performance computing has driven
programming complexity beyond the skill-set of the average domain scientist. To
maintain performance portability in the future, it is imperative to decouple
architecture-specific programming paradigms from the underlying scientific
computations. We present the Stateful DataFlow multiGraph (SDFG), a
data-centric intermediate representation that enables separating program
definition from its optimization. By combining fine-grained data dependencies
with high-level control-flow, SDFGs are both expressive and amenable to program
transformations, such as tiling and double-buffering. These transformations are
applied to the SDFG in an interactive process, using extensible pattern
matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs
on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational
kernels to graph analytics. We show that SDFGs deliver competitive performance,
allowing domain scientists to develop applications naturally and port them to
approach peak hardware performance without modifying the original scientific
code.
"
1617,A Valgrind Tool to Compute the Working Set of a Software Process,"  This paper introduces a new open-source tool for the dynamic analyzer
Valgrind. The tool measures the amount of memory that is actively being used by
a process at any given point in time. While there exist numerous tools to
measure the memory requirements of a process, the vast majority only focuses on
metrics like resident or proportional set sizes, which include memory that was
once claimed, but is momentarily disused. Consequently, such tools do not
permit drawing conclusions about how much cache or RAM a process actually
requires at each point in time, and thus cannot be used for performance
debugging. The few tools which do measure only actively used memory, however,
have limitations in temporal resolution and introspection. In contrast, our
tool offers an easy way to compute the memory that has recently been accessed
at any point in time, reflecting how cache and RAM requirements change over
time. In particular, this tool computes the set of memory references made
within a fixed time interval before any point in time, known as the working
set, and captures call stacks for interesting peaks in the working set size. We
first introduce the tool, then we run some examples comparing the output from
our tool with similar memory tools, and we close with a discussion of
limitations
"
1618,Image Classification on IoT Edge Devices: Profiling and Modeling,"  With the advent of powerful, low-cost IoT systems, processing data closer to
where the data originates, known as edge computing, has become an increasingly
viable option. In addition to lowering the cost of networking infrastructures,
edge computing reduces edge-cloud delay, which is essential for
mission-critical applications. In this paper, we show the feasibility and study
the performance of image classification using IoT devices. Specifically, we
explore the relationships between various factors of image classification
algorithms that may affect energy consumption such as dataset size, image
resolution, algorithm type, algorithm phase, and device hardware. Our
experiments show a strong, positive linear relationship between three predictor
variables, namely model complexity, image resolution, and dataset size, with
respect to energy consumption. In addition, in order to provide a means of
predicting the energy consumption of an edge device performing image
classification, we investigate the usage of three machine learning algorithms
using the data generated from our experiments. The performance as well as the
trade offs for using linear regression, Gaussian process, and random forests
are discussed and validated. Our results indicate that the random forest model
outperforms the two former algorithms, with an R-squared value of 0.95 and 0.79
for two different validation datasets.
"
1619,Speeding up Deep Learning with Transient Servers,"  Distributed training frameworks, like TensorFlow, have been proposed as a
means to reduce the training time of deep learning models by using a cluster of
GPU servers. While such speedups are often desirable---e.g., for rapidly
evaluating new model designs---they often come with significantly higher
monetary costs due to sublinear scalability. In this paper, we investigate the
feasibility of using training clusters composed of cheaper transient GPU
servers to get the benefits of distributed training without the high costs.
  We conduct the first large-scale empirical analysis, launching more than a
thousand GPU servers of various capacities, aimed at understanding the
characteristics of transient GPU servers and their impact on distributed
training performance. Our study demonstrates the potential of transient servers
with a speedup of 7.7X with more than 62.9% monetary savings for some cluster
configurations. We also identify a number of important challenges and
opportunities for redesigning distributed training frameworks to be
transient-aware. For example, the dynamic cost and availability characteristics
of transient servers suggest the need for frameworks to dynamically change
cluster configurations to best take advantage of current conditions.
"
1620,Reliable Access to Massive Restricted Texts: Experience-based Evaluation,"  Libraries are seeing growing numbers of digitized textual corpora that
frequently come with restrictions on their content. Computational analysis
corpora that are large, while of interest to scholars, can be cumbersome
because of the combination of size, granularity of access, and access
restrictions. Efficient management of such a collection for general access
especially under failures depends on the primary storage system. In this paper,
we identify the requirements of managing for computational analysis a massive
text corpus and use it as basis to evaluate candidate storage solutions. The
study based on the 5.9 billion page collection of the HathiTrust digital
library. Our findings led to the choice of Cassandra 3.x for the primary back
end store, which is currently in deployment in the HathiTrust Research Center.
"
1621,CodeNet: Training Large Scale Neural Networks in Presence of Soft-Errors,"  This work proposes the first strategy to make distributed training of neural
networks resilient to computing errors, a problem that has remained unsolved
despite being first posed in 1956 by von Neumann. He also speculated that the
efficiency and reliability of the human brain is obtained by allowing for low
power but error-prone components with redundancy for error-resilience. It is
surprising that this problem remains open, even as massive artificial neural
networks are being trained on increasingly low-cost and unreliable processing
units. Our coding-theory-inspired strategy, ""CodeNet,"" solves this problem by
addressing three challenges in the science of reliable computing: (i) Providing
the first strategy for error-resilient neural network training by encoding each
layer separately; (ii) Keeping the overheads of coding
(encoding/error-detection/decoding) low by obviating the need to re-encode the
updated parameter matrices after each iteration from scratch. (iii) Providing a
completely decentralized implementation with no central node (which is a single
point of failure), allowing all primary computational steps to be error-prone.
We theoretically demonstrate that CodeNet has higher error tolerance than
replication, which we leverage to speed up computation time. Simultaneously,
CodeNet requires lower redundancy than replication, and equal computational and
communication costs in scaling sense. We first demonstrate the benefits of
CodeNet in reducing expected computation time over replication when accounting
for checkpointing. Our experiments show that CodeNet achieves the best
accuracy-runtime tradeoff compared to both replication and uncoded strategies.
CodeNet is a significant step towards biologically plausible neural network
training, that could hold the key to orders of magnitude efficiency
improvements.
"
1622,Performance evaluation of a foot-controlled human-robot interface,"  Robotic minimally invasive interventions typically require using more than
two instruments. We thus developed a foot pedal interface which allows the user
to control a robotic arm (simultaneously to working with the hands) with four
degrees of freedom in continuous directions and speeds. This paper evaluates
and compares the performances of ten naive operators in using this new pedal
interface and a traditional button interface in completing tasks. These tasks
are geometrically complex path-following tasks similar to those in laparoscopic
training, and the traditional button interface allows axis-by-axis control with
constant speeds. Precision, time, and smoothness of the subjects' control
movements for these tasks are analysed. The results demonstrate that the pedal
interface can be used to control a robot for complex motion tasks. The subjects
kept the average error rate at a low level of around 2.6% with both interfaces,
but the pedal interface resulted in about 30% faster operation speed and 60%
smoother movement, which indicates improved efficiency and user experience as
compared with the button interface. The results of a questionnaire show that
the operators found that controlling the robot with the pedal interface was
more intuitive, comfortable, and less tiring than using the button interface.
"
1623,"Primary User Emulation Attacks: A Detection Technique Based on Kalman
  Filter","  Cognitive radio technology addresses the problem of spectrum scarcity by
allowing secondary users to use the vacant spectrum bands without causing
interference to the primary users. However, several attacks could disturb the
normal functioning of the cognitive radio network. Primary user emulation
attacks are one of the most severe attacks in which a malicious user emulates
the primary user signal characteristics to either prevent other legitimate
secondary users from accessing the idle channels or causing harmful
interference to the primary users. There are several proposed approaches to
detect the primary user emulation attackers. However, most of these techniques
assume that the primary user location is fixed, which does not make them valid
when the primary user is mobile. In this paper, we propose a new approach based
on the Kalman filter framework for detecting the primary user emulation attacks
with a non-stationary primary user. Several experiments have been conducted and
the advantages of the proposed approach are demonstrated through the simulation
results.
"
1624,On the Stochastic Analysis of a Quantum Entanglement Switch,"  We study a quantum entanglement switch that serves $k$ users in a star
topology. We model variants of the system using Markov chains and standard
queueing theory and obtain expressions for switch capacity and the expected
number of qubits stored in memory at the switch. While it is more accurate to
use a discrete-time Markov chain (DTMC) to model such systems, we quickly
encounter practical constraints of using this technique and switch to using
continuous-time Markov chains (CTMCs). Using CTMCs allows us to obtain a number
of analytic results for systems in which the links are homogeneous or
heterogeneous and for switches that have infinite or finite buffer sizes. In
addition, we can model the effects of decoherence of quantum states fairly
easily using CTMCs. We also compare the results we obtain from the DTMC against
the CTMC in the case of homogeneous links and infinite buffer, and learn that
the CTMC is a reasonable approximation of the DTMC. From numerical
observations, we discover that decoherence has little effect on capacity and
expected number of stored qubits for homogeneous systems. For heterogeneous
systems, especially those operating close to stability constraints, buffer size
and decoherence can have significant effects on performance metrics. We also
learn that in general, increasing the buffer size from one to two qubits per
link is advantageous to most systems, while increasing the buffer size further
yields diminishing returns.
"
1625,"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and
  GPUDirect","  High performance multi-GPU computing becomes an inevitable trend due to the
ever-increasing demand on computation capability in emerging domains such as
deep learning, big data and planet-scale simulations. However, the lack of deep
understanding on how modern GPUs can be connected and the real impact of
state-of-the-art interconnect technology on multi-GPU application performance
become a hurdle. In this paper, we fill the gap by conducting a thorough
evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1,
NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC
platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit
supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080
GPUs. Based on the empirical evaluation, we have observed four new types of GPU
communication network NUMA effects: three are triggered by NVLink's topology,
connectivity and routing, while one is caused by PCIe chipset design issue.
These observations indicate that, for an application running in a multi-GPU
node, choosing the right GPU combination can impose considerable impact on GPU
communication efficiency, as well as the application's overall performance. Our
evaluation can be leveraged in building practical multi-GPU performance models,
which are vital for GPU task allocation, scheduling and migration in a shared
environment (e.g., AI cloud and HPC centers), as well as communication-oriented
performance tuning.
"
1626,"Basic Performance Measurements of the Intel Optane DC Persistent Memory
  Module","  Scalable nonvolatile memory DIMMs will finally be commercially available with
the release of the Intel Optane DC Persistent Memory Module (or just ""Optane DC
PMM""). This new nonvolatile DIMM supports byte-granularity accesses with access
times on the order of DRAM, while also providing data storage that survives
power outages. This work comprises the first in-depth, scholarly, performance
review of Intel's Optane DC PMM, exploring its capabilities as a main memory
device, and as persistent, byte-addressable memory exposed to user-space
applications. This report details the technologies performance under a number
of modes and scenarios, and across a wide variety of macro-scale benchmarks.
Optane DC PMMs can be used as large memory devices with a DRAM cache to hide
their lower bandwidth and higher latency. When used in this Memory (or cached)
mode, Optane DC memory has little impact on applications with small memory
footprints. Applications with larger memory footprints may experience some
slow-down relative to DRAM, but are now able to keep much more data in memory.
When used under a file system, Optane DC PMMs can result in significant
performance gains, especially when the file system is optimized to use the
load/store interface of the Optane DC PMM and the application uses many small,
persistent writes. For instance, using the NOVA-relaxed NVMM file system, we
can improve the performance of Kyoto Cabinet by almost 2x. Optane DC PMMs can
also enable user-space persistence where the application explicitly controls
its writes into persistent Optane DC media. In our experiments, modified
applications that used user-space Optane DC persistence generally outperformed
their file system counterparts. For instance, the persistent version of RocksDB
performed almost 2x faster than the equivalent program utilizing an NVMM-aware
file system.
"
1627,"High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core
  Processors","  IoT Edge intelligence requires Convolutional Neural Network (CNN) inference
to take place in the edge devices itself. ARM big.LITTLE architecture is at the
heart of prevalent commercial edge devices. It comprises of single-ISA
heterogeneous cores grouped into multiple homogeneous clusters that enable
power and performance trade-offs. All cores are expected to be simultaneously
employed in inference to attain maximal throughput. However, high communication
overhead involved in parallelization of computations from convolution kernels
across clusters is detrimental to throughput. We present an alternative
framework called Pipe-it that employs pipelined design to split convolutional
layers across clusters while limiting parallelization of their respective
kernels to the assigned cluster. We develop a performance-prediction model that
utilizes only the convolutional layer descriptors to predict the execution time
of each layer individually on all permitted core configurations (type and
count). Pipe-it then exploits the predictions to create a balanced pipeline
using an efficient design space exploration algorithm. Pipe-it on average
results in a 39% higher throughput than the highest antecedent throughput.
"
1628,More Bang for Your Buck: Improved use of GPU Nodes for GROMACS 2018,"  We identify hardware that is optimal to produce molecular dynamics
trajectories on Linux compute clusters with the GROMACS 2018 simulation
package. Therefore, we benchmark the GROMACS performance on a diverse set of
compute nodes and relate it to the costs of the nodes, which may include their
lifetime costs for energy and cooling. In agreement with our earlier
investigation using GROMACS 4.6 on hardware of 2014, the performance to price
ratio of consumer GPU nodes is considerably higher than that of CPU nodes.
However, with GROMACS 2018, the optimal CPU to GPU processing power balance has
shifted even more towards the GPU. Hence, nodes optimized for GROMACS 2018 and
later versions enable a significantly higher performance to price ratio than
nodes optimized for older GROMACS versions. Moreover, the shift towards GPU
processing allows to cheaply upgrade old nodes with recent GPUs, yielding
essentially the same performance as comparable brand-new hardware.
"
1629,"COCO: The Large Scale Black-Box Optimization Benchmarking
  (bbob-largescale) Test Suite","  The bbob-largescale test suite, containing 24 single-objective functions in
continuous domain, extends the well-known single-objective noiseless bbob test
suite, which has been used since 2009 in the BBOB workshop series, to large
dimension. The core idea is to make the rotational transformations R, Q in
search space that appear in the bbob test suite computationally cheaper while
retaining some desired properties. This documentation presents an approach that
replaces a full rotational transformation with a combination of a
block-diagonal matrix and two permutation matrices in order to construct test
functions whose computational and memory costs scale linearly in the dimension
of the problem.
"
1630,"Impact of Traffic Characteristics on Request Aggregation in an NDN
  Router","  The paper revisits the performance evaluation of caching in a Named Data
Networking (NDN) router where the content store (CS) is supplemented by a
pending interest table (PIT). The PIT aggregates requests for a given content
that arrive within the download delay and thus brings an additional reduction
in upstream bandwidth usage beyond that due to CS hits. We extend prior work on
caching with non-zero download delay (non-ZDD) by proposing a novel
mathematical framework that is more easily applicable to general traffic models
and by considering alternative cache insertion policies. Specifically we
evaluate the use of an LRU filter to improve CS hit rate performance in this
non-ZDD context. We also consider the impact of time locality in demand due to
finite content lifetimes. The models are used to quantify the impact of the PIT
on upstream bandwidth reduction, demonstrating notably that this is significant
only for relatively small content catalogues or high average request rate per
content. We further explore how the effectiveness of the filter with finite
content lifetimes depends on catalogue size and traffic intensity.
"
1631,PZnet: Efficient 3D ConvNet Inference on Manycore CPUs,"  Convolutional nets have been shown to achieve state-of-the-art accuracy in
many biomedical image analysis tasks. Many tasks within biomedical analysis
domain involve analyzing volumetric (3D) data acquired by CT, MRI and
Microscopy acquisition methods. To deploy convolutional nets in practical
working systems, it is important to solve the efficient inference problem.
Namely, one should be able to apply an already-trained convolutional network to
many large images using limited computational resources. In this paper we
present PZnet, a CPU-only engine that can be used to perform inference for a
variety of 3D convolutional net architectures. PZNet outperforms MKL-based CPU
implementations of PyTorch and Tensorflow by more than 3.5x for the popular
U-net architecture. Moreover, for 3D convolutions with low featuremap numbers,
cloud CPU inference with PZnet outperfroms cloud GPU inference in terms of cost
efficiency.
"
1632,Impact of network delays on Hyperledger Fabric,"  Blockchain has become one of the most attractive technologies for
applications, with a large range of deployments such as production, economy, or
banking. Under the hood, Blockchain technology is a type of distributed
database that supports untrusted parties. In this paper we focus Hyperledger
Fabric, the first blockchain in the market tailored for a private environment,
allowing businesses to create a permissioned network. Hyperledger Fabric
implements a PBFT consensus in order to maintain a non forking blockchain at
the application level. We deployed this framework over an area network between
France and Germany in order to evaluate its performance when potentially large
network delays are observed. Overall we found that when network delay increases
significantly (i.e. up to 3.5 seconds at network layer between two clouds), we
observed that the blocks added to our blockchain had up to 134 seconds offset
after 100 th block from one cloud to another. Thus by delaying block
propagation, we demonstrated that Hyperledger Fabric does not provide
sufficient consistency guaranties to be deployed in critical environments. Our
work, is the fist to evidence the negative impact of network delays on a
PBFT-based blockchain.
"
1633,heSRPT: Optimal Parallel Scheduling of Jobs With Known Sizes,"  When parallelizing a set of jobs across many servers, one must balance a
trade-off between granting priority to short jobs and maintaining the overall
efficiency of the system. When the goal is to minimize the mean flow time of a
set of jobs, it is usually the case that one wants to complete short jobs
before long jobs. However, since jobs usually cannot be parallelized with
perfect efficiency, granting strict priority to the short jobs can result in
very low system efficiency which in turn hurts the mean flow time across jobs.
In this paper, we derive the optimal policy for allocating servers to jobs at
every moment in time in order to minimize mean flow time across jobs. We assume
that jobs follow a sublinear, concave speedup function, and hence jobs
experience diminishing returns from being allocated additional servers. We show
that the optimal policy, heSRPT, will complete jobs according to their size
order, but maintains overall system efficiency by allocating some servers to
each job at every moment in time. We compare heSRPT with state-of-the-art
allocation policies from the literature and show that heSRPT outperforms its
competitors by at least 30%, and often by much more.
"
1634,"Understanding and taming SSD read performance variability: HDFS case
  study","  In this paper we analyze the influence that lower layers (file system, OS,
SSD) have on HDFS' ability to extract maximum performance from SSDs on the read
path. We uncover and analyze three surprising performance slowdowns induced by
lower layers that result in HDFS read throughput loss. First, intrinsic
slowdown affects reads from every new file system extent for a variable amount
of time. Second, temporal slowdown appears temporarily and periodically and is
workload-agnostic. Third, in permanent slowdown, some files can individually
and permanently become slower after a period of time. We analyze the impact of
these slowdowns on HDFS and show significant throughput loss. Individually,
each of the slowdowns can cause a read throughput loss of 10-15%. However,
their effect is cumulative. When all slowdowns happen concurrently, read
throughput drops by as much as 30%. We further analyze mitigation techniques
and show that two of the three slowdowns could be addressed via increased IO
request parallelism in the lower layers. Unfortunately, HDFS cannot
automatically adapt to use such additional parallelism. Our results point to a
need for adaptability in storage stacks. The reason is that an access pattern
that maximizes performance in the common case is not necessarily the same one
that can mask performance fluctuations.
"
1635,"Cost-effective Energy Monitoring of a Zynq-based Real-time System
  including dual Gigabit Ethernet","  The ongoing integration of fine-grained power management features already
established in CPU-driven Systems-on-Chip (SoCs) enables both traditional Field
Programmable Gate Arrays (FPGAs) and, more recently, hybrid Programmable SoCs
(pSoCs) to reach more energy-sensitive application domains (such as, e.g.,
automotive and robotics). By combining a fixed-function multi-core SoC with
flexible, configurable FPGA fabric, the latter can be used to realize
heterogeneous Real-time Systems (RTSs) commonly implementing complex
application-specific architectures with high computation and communication
(I/O) densities. Their dynamic changes in workload, currently active power
saving features and thus power consumption require precise voltage and current
sensing on all relevant supply rails to enable dependable evaluation of the
various power management techniques. In this paper, we propose a low-cost
18-channel 16-bit-resolution measurement (sub-)system capable of 200 kSPS
(kilo-samples per second) for instrumentation of current pSoC development
boards. To this end, we join simultaneously sampling analog-to-digital
converters (ADCs) and analog voltage/current sensing circuitry with a Cortex M7
microcontroller using an SD card for storage. In addition, we propose to
include crucial I/O components such as Ethernet PHYs into the power monitoring
to gain a holistic view on the RTS's temporal behavior covering not only
computation on FPGA and CPUs, but also communication in terms of, e.g.,
reception of sensor values and transmission of actuation signals. We present an
FMC-sized implementation of our measurement system combined with two Gigabit
Ethernet PHYs and one HDMI input. Paired with Xilinx' ZC702 development board,
we are able to synchronously acquire power traces of a Zynq pSoC and the two
PHYs precise enough to identify individual Ethernet frames.
"
1636,Improving Adversarial Robustness via Guided Complement Entropy,"  Adversarial robustness has emerged as an important topic in deep learning as
carefully crafted attack samples can significantly disturb the performance of a
model. Many recent methods have proposed to improve adversarial robustness by
utilizing adversarial training or model distillation, which adds additional
procedures to model training. In this paper, we propose a new training paradigm
called Guided Complement Entropy (GCE) that is capable of achieving
""adversarial defense for free,"" which involves no additional procedures in the
process of improving adversarial robustness. In addition to maximizing model
probabilities on the ground-truth class like cross-entropy, we neutralize its
probabilities on the incorrect classes along with a ""guided"" term to balance
between these two terms. We show in the experiments that our method achieves
better model robustness with even better performance compared to the commonly
used cross-entropy training objective. We also show that our method can be used
orthogonal to adversarial training across well-known methods with noticeable
robustness gain. To the best of our knowledge, our approach is the first one
that improves model robustness without compromising performance.
"
1637,"Effect of payload size on goodput when message segmentations occur for
  wireless networks: Case of packet corruptions recovered by stop-and-wait
  protocol","  This paper investigates the effect of payload size on goodput for wireless
networks where packets created from a message through a segmentation function
are lost due to bit errors and they are recovered by a stop-and-wait protocol.
To achieve this, we derive the exact analytical form of goodput using the
analytical form of a packet-size distribution, given a message-size
distribution and a payload size. In previous work, the packet sizes are assumed
to be constant, which are payload size plus header size, although actual
segmented packets are not constant in size. Hence, this constant packet-size
assumption may be not justified for goodput analysis. From numerical results,
we show that the constant packet-size assumption is not justified under low
bit-error rates. Furthermore, we indicate that the curves of goodput are
concave in payload size under high bit-error rates. In addition, we show that
the larger mean bit-error burst length yields less concave curves of goodput.
"
1638,"Matrix multiplication and universal scalability of the time on the Intel
  Scalable processors","  Matrix multiplication is one of the core operations in many areas of
scientific computing. We present the results of the experiments with the matrix
multiplication of the big size comparable with the big size of the onboard
memory, which is 1.5 terabyte in our case. We run experiments on the computing
board with two sockets and with two Intel Xeon Platinum 8164 processors, each
with 26 cores and with multi-threading. The most interesting result of our
study is the observation of the perfect scalability law of the matrix
multiplication, and of the universality of this law.
"
1639,"Algorithms of evaluation of the waiting time and the modelling of the
  terminal activity","  This paper approaches the application of the waiting model with Poisson
inputs and priorities in the port activity. The arrival of ships in the
maritime terminal is numerically modelled, and specific parameters for the
distribution functions of service and of inputs are determined, in order to
establish the waiting time of ships in the seaport and a stationary process.
The modelling is based on waiting times and on the traffic coefficient.
"
1640,"The Power of d Choices in Scheduling for Data Centers with Heterogeneous
  Servers","  MapReduce framework is the de facto in big data and its applications where a
big data-set is split into small data chunks that are replicated on different
servers among thousands of servers. The heterogeneous server structure of the
system makes the scheduling much harder than scheduling for systems with
homogeneous servers. Throughput optimality of the system on one hand and delay
optimality on the other hand creates a dilemma for assigning tasks to servers.
The JSQ-MaxWeight and Balanced-Pandas algorithms are the states of the arts
algorithms with theoretical guarantees on throughput and delay optimality for
systems with two and three levels of data locality. However, the scheduling
complexity of these two algorithms are way too much. Hence, we use the power of
$d$ choices algorithm combined with the Balanced-Pandas algorithm and the
JSQ-MaxWeight algorithm, and compare the complexity of the simple algorithms
and the power of $d$ choices versions of them. We will further show that the
Balanced-Pandas algorithm combined with the power of the $d$ choices,
Balanced-Pandas-Pod, not only performs better than simple Balanced-Pandas, but
also is less sensitive to the parameter $d$ than the combination of the
JSQ-MaxWeight algorithm and the power of the $d$ choices, JSQ-MaxWeight-Pod. In
fact in our extensive simulation results, the Balanced-Pandas-Pod algorithm is
performing better than the simple Balanced-Pandas algorithm in low and medium
loads, where data centers are usually performing at, and performs almost the
same as the Balanced-Pandas algorithm at high loads. Note that the load
balancing complexity of Balanced-Pandas and JSQ-MaxWeight algorithms are
$O(M)$, where $M$ is the number of servers in the system which is in the order
of thousands servers, whereas the complexity of Balanced-Pandas-Pod and
JSQ-MaxWeight-Pod are $O(1)$, that makes the central scheduler faster and saves
energy.
"
1641,Multigrid Solvers in Reconfigurable Hardware,"  The problem of finding the solution of Partial Differential Equations (PDEs)
plays a central role in modeling real world problems. Over the past years,
Multigrid solvers have showed their robustness over other techniques, due to
its high convergence rate which is independent of the problem size. For this
reason, many attempts for exploiting the inherent parallelism of Multigrid have
been made to achieve the desired efficiency and scalability of the method. Yet,
most efforts fail in this respect due to many factors (time, resources)
governed by software implementations. In this paper, we present a hardware
implementation of the V-cycle Multigrid method for finding the solution of a
2D-Poisson equation. We use Handel-C to implement our hardware design, which we
map onto available Field Programmable Gate Arrays (FPGAs). We analyze the
implementation performance using the FPGA vendor's tools. We demonstrate the
robustness of Multigrid over other iterative solvers, such as Jacobi and
Successive Over Relaxation (SOR), in both hardware and software. We compare our
findings with a C++ version of each algorithm. The obtained results show better
performance when compared to existing software versions.
"
1642,"An Analysis Framework for Hardware and Software Implementations with
  Applications from Cryptography","  With the richness of present-day hardware architectures, tightening the
synergy between hardware and software has attracted a great attention. The
interest in unified approaches paved the way for newborn frameworks that target
hardware and software co-design. This paper confirms that a unified statistical
framework can successfully classify algorithms based on a combination of the
heterogeneous characteristics of their hardware and software implementations.
The proposed framework produces customizable indicators for any hybridization
of processing systems and can be contextualized for any area of application.
The framework is used to develop the Lightness Indicator System (LIS) as a
case-study that targets a set of cryptographic algorithms that are known in the
literature to be tiny and light. The LIS targets state-of-the-art multi-core
processors and high-end Field Programmable Gate Arrays (FPGAs). The presented
work includes a generic benchmark model that aids the clear presentation of the
framework and extensive performance analysis and evaluation.
"
1643,"Model Slicing for Supporting Complex Analytics with Elastic Inference
  Cost and Resource Constraints","  Deep learning models have been used to support analytics beyond simple
aggregation, where deeper and wider models have been shown to yield great
results. These models consume a huge amount of memory and computational
operations. However, most of the large-scale industrial applications are often
computational budget constrained. In practice, the peak workload of inference
service could be 10x higher than the average cases, with the presence of
unpredictable extreme cases. Lots of computational resources could be wasted
during off-peak hours and the system may crash when the workload exceeds system
capacity. How to support deep learning services with dynamic workload
cost-efficiently remains a challenging problem. In this paper, we address the
challenge with a general and novel training scheme called model slicing, which
enables deep learning models to provide predictions within the prescribed
computational resource budget dynamically. Model slicing could be viewed as an
elastic computation solution without requiring more computational resources.
Succinctly, each layer in the model is divided into groups of contiguous block
of basic components (i.e. neurons in dense layers and channels in convolutional
layers), and then partially ordered relation is introduced to these groups by
enforcing that groups participated in each forward pass always starts from the
first group to the dynamically-determined rightmost group. Trained by
dynamically indexing the rightmost group with a single parameter slice rate,
the network is engendered to build up group-wise and residual representation.
Then during inference, a sub-model with fewer groups can be readily deployed
for efficiency whose computation is roughly quadratic to the width controlled
by the slice rate. Extensive experiments show that models trained with model
slicing can effectively support on-demand workload with elastic inference cost.
"
1644,"Efficient LBM on GPUs for dense moving objects using immersed boundary
  condition","  There exists an increasing interest for using immersed boundary methods
(IBMs) (Peskin 2000) to model moving objects in computational fluid dynamics.
Indeed, this approach is particularly efficient, because the fluid mesh does
not require to be body-fitted or to adjust dynamically to the motion of the
body. Frequently, IBMs are implemented in combination with the lattice
Boltzmann methods (LBM) (Kr\""uger 2016). They fit elegantly into the framework
of this method, and yield impressive parallel performances. It has also become
quite common to accelerate LBM simulations with the use of Graphics Processing
Units (GPUs) (T\""olke 2010), as the underlying algorithm adjusts naturally to
the architecture of such platforms. It is not uncommon that speedups of an
order of magnitude, or more, at equal financial cost or energy consumption are
observed, as compared to classical CPUs. IBM algorithms are however more
difficult to adapt to GPUs, because their complex memory access pattern
conflicts with a GPU's strategy of broadcasting data to a large number of GPU
cores in single memory accesses. In the existing literature, GPU
implementations of LBM-IBM codes are therefore restricted to situations in
which the immersed surfaces are very small compared to the total number of
fluid cells (Valero-Lara 2014), as is often the case in exterior flow
simulations around an obstacle. This assumption is however not valid in many
other cases of interest.
  We propose a new method for the implementation of a LBM-IBM on GPUs in the
CUDA language, which allows to handle a substantially larger immersed surfaces
with acceptable performance than previous implementations.
"
1645,"MoA Interpretation of the Iterative Conjugate Gradient Method with Psi
  Reduction - A Tutorial to teach the Mathematically literate in Linear and
  Tensor Algebra: Part I","  It is often difficult to learn new mathematics semantically and
syntactically, even when there are similarities in the words and meaning when
discussed aloud. The goal of this document is to facilitate learning through
explanations and definitions relating our common mathematical knowledge and
highlighting what is new. It is meant to be a working document that will evolve
based on feedback from target audiences, those mathematically literate in
linear and tensor algebra, those that want to learn MoA, Psi Calculus, and its
uses, those that want and need the ability to prove a design, either in
hardware or software through the ONF, Operational Normal Form, and those
wanting to exploit all resources optimally, especially when Tensor Algebra,
i.e. algorithms foundational to their application,are needed: Knowledge
Representation, Machine Learning, Signal Processing, AI, HPC, etc.
"
1646,Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN,"  Over the past few years machine learning has seen a renewed explosion of
interest, following a number of studies showing the effectiveness of neural
networks in a range of tasks which had previously been considered incredibly
hard. Neural networks' effectiveness in the fields of image recognition and
natural language processing stems primarily from the vast amounts of data
available to companies and researchers, coupled with the huge amounts of
compute power available in modern accelerators such as GPUs, FPGAs and ASICs.
There are a number of approaches available to developers for utilizing GPGPU
technologies such as SYCL, OpenCL and CUDA, however many applications require
the same low level mathematical routines. Libraries dedicated to accelerating
these common routines allow developers to easily make full use of the available
hardware without requiring low level knowledge of the hardware themselves,
however such libraries are often provided by hardware manufacturers for
specific hardware such as cuDNN for Nvidia hardware or MIOpen for AMD hardware.
  SYCL-DNN is a new open-source library dedicated to providing accelerated
routines for neural network operations which are hardware and vendor agnostic.
Built on top of the SYCL open standard and written entirely in standard C++,
SYCL-DNN allows a user to easily accelerate neural network code for a wide
range of hardware using a modern C++ interface. The library is tested on AMD's
OpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as
well as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we
will present performance figures for SYCL-DNN on this range of hardware, and
discuss how high performance was achieved on such a varied set of accelerators
with such different hardware features.
"
1647,A High-Performance Energy Management System based on Evolving Graph,"  As the fast growth and large integration of distributed generation, renewable
energy resource, energy storage system and load response, the modern power
system operation becomes much more complicated with increasing uncertainties
and frequent changes. Increased operation risks are introduced to the existing
commercial Energy Management System (EMS), due to its limited computational
capability. In this paper, a high-performance EMS analysis framework based on
the evolving graph is developed. A power grid is first modeled as an evolving
graph and then the power system dynamic analysis applications, like network
topology processing (NTP), state estimation (SE), power flow (PF), and
contingency analysis (CA), are efficiently implemented on the system evolving
graph to build a high-performance EMS analysis framework. Its computation
performance is field tested using a 2749-bus power system in Sichuan, China.
The results illustrate that the proposed EMS remarkably speeds up the
computation performance and reaches the goal of real-time power system
analysis.
"
1648,Modeling Corruption in Eventually-Consistent Graph Databases,"  We present a model and analysis of an eventually consistent graph database
where loosely cooperating servers accept concurrent updates to a partitioned,
distributed graph. The model is high-fidelity and preserves design choices from
contemporary graph database management systems. To explore the problem space,
we use two common graph topologies as data models for realistic
experimentation. The analysis reveals, even assuming completely fault-free
hardware and bug-free software, that if it is possible for updates to interfere
with one-another, corruption will occur and spread significantly through the
graph within the production database lifetime. Using our model, database
designers and operators can compute the rate of corruption for their systems
and determine whether they are sufficiently dependable for their intended use.
"
1649,"Optimisation of stochastic networks with blocking: a functional-form
  approach","  This paper introduces a class of stochastic networks with blocking, motivated
by applications arising in cellular network planning, mobile cloud computing,
and spare parts supply chains. Blocking results in lost revenue due to
customers or jobs being permanently removed from the system. We are interested
in striking a balance between mitigating blocking by increasing service
capacity, and maintaining low costs for service capacity. This problem is
further complicated by the stochastic nature of the system. Owing to the
complexity of the system there are no analytical results available that
formulate and solve the relevant optimization problem in closed form.
Traditional simulation-based methods may work well for small instances, but the
associated computational costs are prohibitive for networks of realistic size.
  We propose a hybrid functional-form based approach for finding the optimal
resource allocation, combining the speed of an analytical approach with the
accuracy of simulation-based optimisation. The key insight is to replace the
computationally expensive gradient estimation in simulation optimisation with a
closed-form analytical approximation that is calibrated using a single
simulation run. We develop two implementations of this approach and conduct
extensive computational experiments on complex examples to show that it is
capable of substantially improving system performance. We also provide evidence
that our approach has substantially lower computational costs compared to
stochastic approximation.
"
1650,"Cross-Platform Performance Portability Using Highly Parametrized SYCL
  Kernels","  Over recent years heterogeneous systems have become more prevalent across HPC
systems, with over 100 supercomputers in the TOP500 incorporating GPUs or other
accelerators. These hardware platforms have different performance
characteristics and optimization requirements. In order to make the most of
multiple accelerators a developer has to provide implementations of their
algorithms tuned for each device. Hardware vendors provide libraries targeting
their devices specifically, which provide good performance but frequently have
different API designs, hampering portability.
  The SYCL programming model allows users to write heterogeneous programs using
completely standard C++, and so developers have access to the power of C++
templates when developing compute kernels. In this paper we show that by
writing highly parameterized kernels for matrix multiplies and convolutions we
achieve performance competitive with vendor implementations across different
architectures. Furthermore, tuning for new devices amounts to choosing the
combinations of kernel parameters that perform best on the hardware.
"
1651,R-Storm: Resource-Aware Scheduling in Storm,"  The era of big data has led to the emergence of new systems for real-time
distributed stream processing, e.g., Apache Storm is one of the most popular
stream processing systems in industry today. However, Storm, like many other
stream processing systems lacks an intelligent scheduling mechanism. The
default round-robin scheduling currently deployed in Storm disregards resource
demands and availability, and can therefore be inefficient at times. We present
R-Storm (Resource-Aware Storm), a system that implements resource-aware
scheduling within Storm. R-Storm is designed to increase overall throughput by
maximizing resource utilization while minimizing network latency. When
scheduling tasks, R-Storm can satisfy both soft and hard resource constraints
as well as minimizing network distance between components that communicate with
each other. We evaluate R-Storm on set of micro-benchmark Storm applications as
well as Storm applications used in production at Yahoo! Inc. From our
experimental results we conclude that R-Storm achieves 30-47% higher throughput
and 69-350% better CPU utilization than default Storm for the micro-benchmarks.
For the Yahoo! Storm applications, R-Storm outperforms default Storm by around
50% based on overall throughput. We also demonstrate that R-Storm performs much
better when scheduling multiple Storm applications than default Storm.
"
1652,"A Processor-Sharing model for the Performance of Virtualized Network
  Functions","  The parallel execution of requests in a Cloud Computing platform, as for
Virtualized Network Functions, is modeled by an $M^{[X]}/M/1$ Processor-Sharing
(PS) system, where each request is seen as a batch of unit jobs. The
performance of such paralleled system can then be measured by the quantiles of
the batch sojourn time distribution. In this paper, we address the evaluation
of this distribution for the $M^{[X]}/M/1$-PS queue with batch arrivals and
geometrically distributed batch size. General results on the residual busy
period (after a tagged batch arrival time) and the number of unit jobs served
during this residual busy period are first derived. This enables us to provide
an approximation for the distribution tail of the batch sojourn time whose
accuracy is confirmed by simulation.
"
1653,"On the sojourn of an arbitrary customer in an $M/M/1$ Processor Sharing
  Queue","  In this paper, we consider the number of both arrivals and departures seen by
a tagged customer while in service in a classical $M/M/1$ processor sharing
queue. By exploiting the underlying orthogonal structure of this queuing system
revealed in an earlier study, we compute the distributions of these two
quantities and prove that they are equal in distribution. We moreover derive
the asymptotic behavior of this common distribution. The knowledge of the
number of departures seen by a tagged customer allows us to test the validity
of an approximation, which consists of assuming that the tagged customer is
randomly served among those customers in the residual busy period of the queue
following the arrival of the tagged customer. A numerical evidence shows that
this approximation is reasonable for moderate values of the number of
departures, given that the asymptotic behaviors of the distributions are very
different even if the exponential decay rates are equal.
"
1654,"Reducing Communication in Algebraic Multigrid with Multi-step Node Aware
  Communication","  Algebraic multigrid (AMG) is often viewed as a scalable $\mathcal{O}(n)$
solver for sparse linear systems. Yet, parallel AMG lacks scalability due to
increasingly large costs associated with communication, both in the initial
construction of a multigrid hierarchy as well as the iterative solve phase.
This work introduces a parallel implementation of AMG to reduce the cost of
communication, yielding an increase in scalability. Standard inter-process
communication consists of sending data regardless of the send and receive
process locations. Performance tests show notable differences in the cost of
intra- and inter-node communication, motivating a restructuring of
communication. In this case, the communication schedule takes advantage of the
less costly intra-node communication, reducing both the number and size of
inter-node messages. Node-centric communication extends to the range of
components in both the setup and solve phase of AMG, yielding an increase in
the weak and strong scalability of the entire method.
"
1655,"The distribution of age-of-information performance measures for message
  processing systems","  The idea behind the recently introduced ""age of information"" performance
measure of a networked message processing system is that it indicates our
knowledge regarding the ""freshness"" of the most recent piece of information
that can be used as a criterion for real-time control. In this foundational
paper, we examine two such measures, one that has been extensively studied in
the recent literature and a new one that could be more relevant from the point
of view of the processor. Considering these measures as stochastic processes in
a stationary environment (defined by the arrival processes, message processing
times and admission controls in bufferless systems), we characterize their
distributions using the Palm inversion formula. Under renewal assumptions we
derive explicit solutions for their Laplace transforms and show some
interesting decomposition properties. Previous work has mostly focused on
computation of expectations in very particular cases. We argue that using
bufferless or very small buffer systems is best and support this by simulation.
We also pose some open problems including assessment of enqueueing policies
that may be better in cases where one wishes to minimize more general
functionals of the age of information measures.
"
1656,Parallel algorithms development for programmable logic devices,"  Programmable Logic Devices (PLDs) continue to grow in size and currently
contain several millions of gates. At the same time, research effort is going
into higher-level hardware synthesis methodologies for reconfigurable computing
that can exploit PLD technology. In this paper, we explore the effectiveness
and extend one such formal methodology in the design of massively parallel
algorithms. We take a step-wise refinement approach to the development of
correct reconfigurable hardware circuits from formal specifications. A
functional programming notation is used for specifying algorithms and for
reasoning about them. The specifications are realised through the use of a
combination of function decomposition strategies, data refinement techniques,
and off-the-shelf refinements based upon higher-order functions. The
off-the-shelf refinements are inspired by the operators of Communicating
Sequential Processes (CSP) and map easily to programs in Handel-C (a hardware
description language). The Handel-C descriptions are directly compiled into
reconfigurable hardware. The practical realisation of this methodology is
evidenced by a case studying the matrix multiplication algorithm as it is
relatively simple and well known. In this paper, we obtain several hardware
implementations with different performance characteristics by applying
different refinements to the algorithm. The developed designs are compiled and
tested under Celoxica's RC-1000 reconfigurable computer with its 2 million
gates Virtex-E FPGA. Performance analysis and evaluation of these
implementations are included.
"
1657,"PerfVis: Pervasive Visualization in Immersive AugmentedReality for
  Performance Awareness","  Developers are usually unaware of the impact of code changes to the
performance of software systems. Although developers can analyze the
performance of a system by executing, for instance, a performance test to
compare the performance of two consecutive versions of the system, changing
from a programming task to a testing task would disrupt the development flow.
In this paper, we propose the use of a city visualization that dynamically
provides developers with a pervasive view of the continuous performance of a
system. We use an immersive augmented reality device (Microsoft HoloLens) to
display our visualization and extend the integrated development environment on
a computer screen to use the physical space. We report on technical details of
the design and implementation of our visualization tool, and discuss early
feedback that we collected of its usability. Our investigation explores a new
visual metaphor to support the exploration and analysis of possibly very large
and multidimensional performance data. Our initial result indicates that the
city metaphor can be adequate to analyze dynamic performance data on a large
and non-trivial software system.
"
1658,"Dynamic scheduling in a partially fluid, partially lossy queueing system","  We consider a single server queueing system with two classes of jobs: eager
jobs with small sizes that require service to begin almost immediately upon
arrival, and tolerant jobs with larger sizes that can wait for service. While
blocking probability is the relevant performance metric for the eager class,
the tolerant class seeks to minimize its mean sojourn time. In this paper, we
discuss the performance of each class under dynamic scheduling policies, where
the scheduling of both classes depends on the instantaneous state of the
system. This analysis is carried out under a certain fluid limit, where the
arrival rate and service rate of the eager class are scaled to infinity,
holding the offered load constant. Our performance characterizations reveal a
(dynamic) pseudo-conservation law that ties the performance of both the classes
to the standalone blocking probabilities of the eager class. Further, the
performance is robust to other specifics of the scheduling policies. We also
characterize the Pareto frontier of the achievable region of performance
vectors under the same fluid limit, and identify a (two-parameter) class of
Pareto-complete scheduling policies.
"
1659,"Performance Models for Data Transfers: A Case Study with Molecular
  Chemistry Kernels","  With increasing complexity of hardwares, systems with different memory nodes
are ubiquitous in High Performance Computing (HPC). It is paramount to develop
strategies to overlap the data transfers between memory nodes with computations
in order to exploit the full potential of these systems. In this article, we
consider the problem of deciding the order of data transfers between two memory
nodes for a set of independent tasks with the objective to minimize the
makespan. We prove that with limited memory capacity, obtaining the optimal
order of data transfers is a NP-complete problem. We propose several heuristics
for this problem and provide details about their favorable situations. We
present an analysis of our heuristics on traces, obtained by running 2
molecular chemistry kernels, namely, Hartree-Fock (HF) and Coupled Cluster
Single Double (CCSD) on 10 nodes of an HPC system. Our results show that some
of our heuristics achieve significant overlap for moderate memory capacities
and are very close to the lower bound of makespan.
"
1660,"Sound, Fine-Grained Traversal Fusion for Heterogeneous Trees - Extended
  Version","  Applications in many domains are based on a series of traversals of tree
structures, and fusing these traversals together to reduce the total number of
passes over the tree is a common, important optimization technique. In
applications such as compilers and render trees, these trees are heterogeneous:
different nodes of the tree have different types. Unfortunately, prior work for
fusing traversals falls short in different ways: they do not handle
heterogeneity; they require using domain-specific languages to express an
application; they rely on the programmer to aver that fusing traversals is
safe, without any soundness guarantee; or they can only perform coarse-grain
fusion, leading to missed fusion opportunities. This paper addresses these
shortcomings to build a framework for fusing traversals of heterogeneous trees
that is automatic, sound, and fine-grained. We show across several case studies
that our approach is able to allow programmers to write simple, intuitive
traversals, and then automatically fuse them to substantially improve
performance.
"
1661,Defence Efficiency,"  In order to automate actions, such as defences against network attacks, one
needs to quantify their efficiency. This can subsequently be used in
post-evaluation, learning, etc. In order to quantify the defence efficiency as
a function of the impact of the defence and its total cost, we present several
natural requirements from such a definition of efficiency and provide a natural
definition that complies with these requirements. Next, we precisely
characterize our definition of efficiency by the axiomatic approach; namely, we
strengthen the original requirements from such a definition and prove that the
given definition is the unique definition that satisfies those requirements.
Finally, we generalize the definition to the case of any number of input
variables in two natural ways, and compare these generalizations.
"
1662,"Low-Power Computer Vision: Status, Challenges, Opportunities","  Computer vision has achieved impressive progress in recent years. Meanwhile,
mobile phones have become the primary computing platforms for millions of
people. In addition to mobile phones, many autonomous systems rely on visual
data for making decisions and some of these systems have limited energy (such
as unmanned aerial vehicles also called drones and mobile robots). These
systems rely on batteries and energy efficiency is critical. This article
serves two main purposes: (1) Examine the state-of-the-art for low-power
solutions to detect objects in images. Since 2015, the IEEE Annual
International Low-Power Image Recognition Challenge (LPIRC) has been held to
identify the most energy-efficient computer vision solutions. This article
summarizes 2018 winners' solutions. (2) Suggest directions for research as well
as opportunities for low-power computer vision.
"
1663,Energy Saving Strategy Based on Profiling,"  Constraints imposed by power consumption and the related costs are one of the
key roadblocks to the design and development of next generation exascale
systems. To mitigate these issues, strategies that reduce the power consumption
of the processor are the need of the hour. Techniques such as Dynamic Voltage
and Frequency Scaling (DVFS) exist which reduce the power consumption of a
processor at runtime but they should be used in such a manner so that their
overhead does not hamper application performance. In this paper, we propose an
energy saving strategy which operates on timeslice basis to apply DVFS under a
user defined performance constraint. Results show energy savings up to 7% when
NAS benchmarks are tested on a laptop platform
"
1664,"Inversion formula with hypergeometric polynomials and its application to
  an integral equation","  For any complex parameters $x$ and $\nu$, we provide a new class of linear
inversion formulas $T = A(x,\nu) \cdot S \Leftrightarrow S = B(x,\nu) \cdot T$
between sequences $S = (S_n)_{n \in \mathbb{N}^*}$ and $T = (T_n)_{n \in
\mathbb{N}^*}$, where the infinite lower-triangular matrix $A(x,\nu)$ and its
inverse $B(x,\nu)$ involve Hypergeometric polynomials $F(\cdot)$, namely $$
  \left\{
  \begin{array}{ll}
  A_{n,k}(x,\nu) = \displaystyle (-1)^k\binom{n}{k}F(k-n,-n\nu;-n;x),
  \\
  B_{n,k}(x,\nu) = \displaystyle (-1)^k\binom{n}{k}F(k-n,k\nu;k;x)
  \end{array} \right. $$ for $1 \leqslant k \leqslant n$. Functional relations
between the ordinary (resp. exponential) generating functions of the related
sequences $S$ and $T$ are also given.
  These new inversion formulas have been initially motivated by the resolution
of an integral equation recently appeared in the field of Queuing Theory; we
apply them to the full resolution of this integral equation. Finally, matrices
involving generalized Laguerre polynomials polynomials are discussed as
specific cases of our general inversion scheme.
"
1665,Memory and Parallelism Analysis Using a Platform-Independent Approach,"  Emerging computing architectures such as near-memory computing (NMC) promise
improved performance for applications by reducing the data movement between CPU
and memory. However, detecting such applications is not a trivial task. In this
ongoing work, we extend the state-of-the-art platform-independent software
analysis tool with NMC related metrics such as memory entropy, spatial
locality, data-level, and basic-block-level parallelism. These metrics help to
identify the applications more suitable for NMC architectures.
"
1666,"A mechanism for balancing accuracy and scope in cross-machine black-box
  GPU performance modeling","  The ability to model, analyze, and predict execution time of computations is
an important building block supporting numerous efforts, such as load
balancing, performance optimization, and automated performance tuning for high
performance, parallel applications. In today's increasingly heterogeneous
computing environment, this task must be accomplished efficiently across
multiple architectures, including massively parallel coprocessors like GPUs. To
address this challenge, we present an approach for constructing customizable,
cross-machine performance models for GPU kernels, including a mechanism to
automatically and symbolically gather performance-relevant kernel operation
counts, a tool for formulating mathematical models using these counts, and a
customizable parameterized collection of benchmark kernels used to calibrate
models to GPUs in a black-box fashion. Our approach empowers a user to manage
trade-offs between model accuracy, evaluation speed, and generalizability. A
user can define a model and customize the calibration process, making it as
simple or complex as desired, and as application-targeted or general as
desired. To evaluate our approach, we demonstrate both linear and nonlinear
models; each example models execution times for multiple variants of a
particular computation: two matrix multiplication variants, four Discontinuous
Galerkin (DG) differentiation operation variants, and two 2-D five-point finite
difference stencil variants. For each variant, we present accuracy results on
GPUs from multiple vendors and hardware generations. We view this customizable
approach as a response to a central question in GPU performance modeling: how
can we model GPU performance in a cost-explanatory fashion while maintaining
accuracy, evaluation speed, portability, and ease of use, an attribute we
believe precludes manual collection of kernel or hardware statistics.
"
1667,DTLS Performance - How Expensive is Security?,"  Secure communication is an integral feature of many Internet services. The
widely deployed TLS protects reliable transport protocols. DTLS extends TLS
security services to protocols relying on plain UDP packet transport, such as
VoIP or IoT applications. In this paper, we construct a model to determine the
performance of generic DTLS-enabled applications. Our model considers basic
network characteristics, e.g., number of connections, and the chosen security
parameters, e.g., the encryption algorithm in use. Measurements are presented
demonstrating the applicability of our model. These experiments are performed
using a high-performance DTLS-enabled VPN gateway built on top of the
well-established libraries DPDK and OpenSSL. This VPN solution represents the
most essential parts of DTLS, creating a DTLS performance baseline. Using this
baseline the model can be extended to predict even more complex DTLS protocols
besides the measured VPN. Code and measured data used in this paper are
publicly available at https://git.io/MoonSec and https://git.io/Sdata.
"
1668,"Performance of a Quantum Annealer for Ising Ground State Computations on
  Chimera Graphs","  Quantum annealing is getting increasing attention in combinatorial
optimization. The quantum processing unit by D-Wave is constructed to
approximately solve Ising models on so-called Chimera graphs. Ising models are
equivalent to quadratic unconstrained binary optimization (QUBO) problems and
maximum cut problems on the associated graphs. We have tailored branch-and-cut
as well as semidefinite programming algorithms for solving Ising models for
Chimera graphs to provable optimality and use the strength of these approaches
for comparing our solution values to those obtained on the current quantum
annealing machine D-Wave 2000Q. This allows for the assessment of the quality
of solutions produced by the D-Wave hardware. It has been a matter of
discussion in the literature how well the D-Wave hardware performs at its
native task, and our experiments shed some more light on this issue.
"
1669,"Tracking Performance Limitations of MIMO Networked Control Systems with
  Multiple Communication Constraints","  In this paper, the tracking performance limitation of networked control
systems (NCSs) is studied. The NCSs is considered as continuous-time linear
multi-input multi-output (MIMO) systems with random reference noises. The
controlled plants include unstable poles and non-minimum phase (NMP) zeros. The
output feedback path is affected by multiple communication constraints. We
focus on some basic communication constraints, including additive white noise
(AWN), quantization noise, bandwidth, as well as encoder-decoder. The system
performance is evaluated with the tracking error energy, and used a two-degree
of freedom (2DOF) controller. The explicit representation of the tracking
performance is given in this paper. The results indicate the tracking
performance limitations rely to internal characteristics of the plant (unstable
poles and NMP zeros), reference noises (the reference noise power distribution
(RNPD) and its directions) and the characteristics of communication
constraints. Moreover, the tracking performance limitations are also affected
by the angles between the each transform NMP zero direction and RNPD direction,
and these angles between each transform unstable poles direction and the
direction of communication constraint distribution/allocation. In addition, for
MIMO NCSs, bandwidth (there are not identical two channels) always can affects
the direction of unstable poles, and the channel allocation of bandwidth and
encode-decode may be used for a feasible method for the performance allocation
of each channels. Lastly, a instance is given for verifying the effectiveness
of the theoretical outcomes.
"
1670,Reinforcement Learning Based Orchestration for Elastic Services,"  Due to the highly variable execution context in which edge services run,
adapting their behavior to the execution context is crucial to comply with
their requirements. However, adapting service behavior is a challenging task
because it is hard to anticipate the execution contexts in which it will be
deployed, as well as assessing the impact that each behavior change will
produce. In order to provide this adaptation efficiently, we propose a
Reinforcement Learning (RL) based Orchestration for Elastic Services. We
implement and evaluate this approach by adapting an elastic service in
different simulated execution contexts and comparing its performance to a
Heuristics based approach. We show that elastic services achieve high precision
and requirement satisfaction rates while creating an overhead of less than 0.5%
to the overall service. In particular, the RL approach proves to be more
efficient than its rule-based counterpart; yielding a 10 to 25% higher
precision while being 25% less computationally expensive.
"
1671,Computational Petri Nets: Adjunctions Considered Harmful,"  We review some of the endeavors in trying to connect Petri nets with free
symmetric monoidal categories. We give a list of requirement such connections
should respect if they are meant to be useful for practical/implementation
purposes. We show how previous approaches do not satisfy them, and give
compelling evidence that this depends on trying to make the correspondence
functorial in the direction from nets to free symmetric monoidal categories, in
order to produce an adjunction. We show that dropping this immediately honors
our desiderata, and conclude by introducing an Idris library which implements
them.
"
1672,Empirically Analyzing Ethereum's Gas Mechanism,"  Ethereum's Gas mechanism attempts to set transaction fees in accordance with
the computational cost of transaction execution: a cost borne by default by
every node on the network to ensure correct smart contract execution. Gas
encourages users to author transactions that are efficient to execute and in so
doing encourages node diversity, allowing modestly resourced nodes to join and
contribute to the security of the network.
  However, the effectiveness of this scheme relies on Gas costs being correctly
aligned with observed computational costs in reality. In this work, we
performed the first large scale empirical study to understand to what degree
this alignment exists in practice, by collecting and analyzing Tera-bytes worth
of nanosecond-precision transaction execution traces. Besides confirming
potential denial-of-service vectors, our results also shed light on the role of
I/O in transaction costs which remains poorly captured by the current Gas cost
model. Finally, our results suggest that under the current Gas cost model,
nodes with modest computational resources are disadvantaged compared to their
better resourced peers, which we identify as an ongoing threat to node
diversity and network decentralization.
"
1673,On Linear Learning with Manycore Processors,"  A new generation of manycore processors is on the rise that offers dozens and
more cores on a chip and, in a sense, fuses host processor and accelerator. In
this paper we target the efficient training of generalized linear models on
these machines. We propose a novel approach for achieving parallelism which we
call Heterogeneous Tasks on Homogeneous Cores (HTHC). It divides the problem
into multiple fundamentally different tasks, which themselves are parallelized.
For evaluation, we design a detailed, architecture-cognizant implementation of
our scheme on a recent 72-core Knights Landing processor that is adaptive to
the cache, memory, and core structure. Our library efficiently supports dense
and sparse datasets as well as 4-bit quantized data for further possible gains
in performance. We show benchmarks for Lasso and SVM with different data sets
against straightforward parallel implementations and prior software. In
particular, for Lasso on dense data, we improve the state-of-the-art by an
order of magnitude.
"
1674,On the Impact of Memory Allocation on High-Performance Query Processing,"  Somewhat surprisingly, the behavior of analytical query engines is crucially
affected by the dynamic memory allocator used. Memory allocators highly
influence performance, scalability, memory efficiency and memory fairness to
other processes. In this work, we provide the first comprehensive experimental
analysis on the impact of memory allocation for high-performance query engines.
We test five state-of-the-art dynamic memory allocators and discuss their
strengths and weaknesses within our DBMS. The right allocator can increase the
performance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.
"
1675,"Higher aggregation of gNodeBs in Cloud-RAN architectures via parallel
  computing","  In this paper, we address the virtualization and the centralization of
real-time network functions, notably in the framework of Cloud RAN (C-RAN). We
thoroughly analyze the required fronthaul capacity for the deployment of the
proposed C-RAN architecture. We are specifically interested in the performance
of the software based channel coding function. We develop a dynamic
multi-threading approach to achieve parallel computing on a multi-core
platform. Measurements from an OAI-based testbed show important gains in terms
of latency; this enables the increase of the distance between the radio
elements and the virtualized RAN functions and thus a higher aggregation of
gNodeBs in edge data centers, referred to as Central Offices (COs).
"
1676,RedisGraph GraphBLAS Enabled Graph Database,"  RedisGraph is a Redis module developed by Redis Labs to add graph database
functionality to the Redis database. RedisGraph represents connected data as
adjacency matrices. By representing the data as sparse matrices and employing
the power of GraphBLAS (a highly optimized library for sparse matrix
operations), RedisGraph delivers a fast and efficient way to store, manage and
process graphs. Initial benchmarks indicate that RedisGraph is significantly
faster than comparable graph databases.
"
1677,"An Improved Accurate Solver for the Time-Dependent RTE in Underwater
  Optical Wireless Communications","  In this paper, an improved numerical solver to evaluate the time-dependent
radiative transfer equation (RTE) for underwater optical wireless
communications (UOWC) is investigated. The RTE evaluates the optical path-loss
of light wave in an underwater channel in terms of the inherent optical
properties related to the environments, namely the absorption and scattering
coefficients as well as the phase scattering function (PSF). The proposed
numerical algorithm was improved based on the ones proposed in [1]-[4], by
modifying the finite difference scheme proposed in [1] as well as an
enhancement of the quadrature method proposed in [2] by involving a more
accurate 7-points quadrature scheme in order to calculate the quadrature weight
coefficients corresponding to the integral term of the RTE. Furthermore, the
scattering angular discretization algorithm used in [3] and [4] was modified,
based on which the receiver's field of view discretization was adapted
correspondingly. Interestingly, the RTE solver has been applied to three volume
scattering functions, namely: the single-term HG phase function, the two-term
HG phase function [5], and the Fournier-Forand phase function [6], over
Harbor-I and Harbor-II water types. Based on the normalized received power
evaluated through the proposed algorithm, the bit error rate performance of the
UOWC system is investigated in terms of system and channel parameters. The
enhanced algorithm gives a tightly close performance to its Monte Carlo
counterpart improved based on the simulations provided in [7], by adjusting the
numerical cumulative distribution function computation method as well as
optimizing the number of scattering angles. Matlab codes for the proposed RTE
solver are presented in [8].
"
1678,Evolutionary Optimisation of Real-Time Systems and Networks,"  The design space of networked embedded systems is very large, posing
challenges to the optimisation of such platforms when it comes to support
applications with real-time guarantees. Recent research has shown that a number
of inter-related optimisation problems have a critical influence over the
schedulability of a system, i.e. whether all its application components can
execute and communicate by their respective deadlines. Examples of such
optimization problems include task allocation and scheduling, communication
routing and arbitration, memory allocation, and voltage and frequency scaling.
In this paper, we advocate the use of evolutionary approaches to address such
optimization problems, aiming to evolve individuals of increased fitness over
multiple generations of potential solutions. We refer to plentiful evidence
that existing real-time schedulability tests can be used effectively to guide
evolutionary optimisation, either by themselves or in combination with other
metrics such as energy dissipation or hardware overheads. We then push that
concept one step further and consider the possibility of using evolutionary
techniques to evolve the schedulability tests themselves, aiming to support the
verification and optimisation of systems which are too complex for
state-of-the-art (manual) derivation of schedulability tests.
"
1679,"Internet Speed Measurement: Current Challenges and Future
  Recommendations","  Government organizations, regulators, consumers, Internet service providers,
and application providers alike all have an interest in measuring user Internet
""speed"". Access speeds have increased by an order of magnitude in past years,
with gigabit speeds available to tens of millions of homes. Approaches must
evolve to accurately reflect the changing user experience and network speeds.
This paper offers historical and technical background on current speed testing
methods, highlights their limitations as access network speeds continue to
increase, and offers recommendations for the next generation of Internet
""speed"" measurement.
"
1680,"Efficient Similarity-aware Compression to Reduce Bit-writes in
  Non-Volatile Main Memory for Image-based Applications","  Image bitmaps have been widely used in in-memory applications, which consume
lots of storage space and energy. Compared with legacy DRAM, non-volatile
memories (NVMs) are suitable for bitmap storage due to the salient features in
capacity and power savings. However, NVMs suffer from higher latency and energy
consumption in writes compared with reads. Although compressing data in write
accesses to NVMs on-the-fly reduces the bit-writes in NVMs, existing precise or
approximate compression schemes show limited performance improvements for data
of bitmaps, due to the irregular data patterns and variance in data. We observe
that the data containing bitmaps show the pixel-level similarity due to the
analogous contents in adjacent pixels. By exploiting the pixel-level
similarity, we propose SimCom, an efficient similarity-aware compression scheme
in hardware layer, to compress data for each write access on-the-fly. The idea
behind SimCom is to compress continuous similar words into the pairs of base
words with runs. With the aid of domain knowledge of images, SimCom adaptively
selects an appropriate compression mode to achieve an efficient trade-off
between image quality and memory performance. We implement SimCom on GEM5 with
NVMain and evaluate the performance with real-world workloads. Our results
demonstrate that SimCom reduces 33.0%, 34.8% write latency and saves 28.3%,
29.0% energy than state-of-the-art FPC and BDI with minor quality loss of 3%.
"
1681,"Performance Engineering for Real and Complex Tall & Skinny Matrix
  Multiplication Kernels on GPUs","  General matrix-matrix multiplications with double-precision real and complex
entries (DGEMM and ZGEMM) in vendor-supplied BLAS libraries are best optimized
for square matrices but often show bad performance for tall & skinny matrices,
which are much taller than wide. NVIDIA's current CUBLAS implementation
delivers only a fraction of the potential performance as indicated by the
roofline model in this case. We describe the challenges and key characteristics
of an implementation that can achieve close to optimal performance. We further
evaluate different strategies of parallelization and thread distribution, and
devise a flexible, configurable mapping scheme. To ensure flexibility and allow
for highly tailored implementations we use code generation combined with
autotuning. For a large range of matrix sizes in the domain of interest we
achieve at least 2/3 of the roofline performance and often substantially
outperform state-of-the art CUBLAS results on an NVIDIA Volta GPGPU.
"
1682,"SPH-EXA: Enhancing the Scalability of SPH codes Via an Exascale-Ready
  SPH Mini-App","  Numerical simulations of fluids in astrophysics and computational fluid
dynamics (CFD) are among the most computationally-demanding calculations, in
terms of sustained floating-point operations per second, or FLOP/s. It is
expected that these numerical simulations will significantly benefit from the
future Exascale computing infrastructures, that will perform 10^18 FLOP/s. The
performance of the SPH codes is, in general, adversely impacted by several
factors, such as multiple time-stepping, long-range interactions, and/or
boundary conditions. In this work an extensive study of three SPH
implementations SPHYNX, ChaNGa, and XXX is performed, to gain insights and to
expose any limitations and characteristics of the codes. These codes are the
starting point of an interdisciplinary co-design project, SPH-EXA, for the
development of an Exascale-ready SPH mini-app. We implemented a rotating square
patch as a joint test simulation for the three SPH codes and analyzed their
performance on a modern HPC system, Piz Daint. The performance profiling and
scalability analysis conducted on the three parent codes allowed to expose
their performance issues, such as load imbalance, both in MPI and OpenMP.
Two-level load balancing has been successfully applied to SPHYNX to overcome
its load imbalance. The performance analysis shapes and drives the design of
the SPH-EXA mini-app towards the use of efficient parallelization methods,
fault-tolerance mechanisms, and load balancing approaches.
"
1683,"Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low
  Response Times","  Load balancing systems, comprising a central dispatcher and a scheduling
policy at each server, are widely used in practice, and their response time has
been extensively studied in the theoretical literature. While much is known
about the scenario where the scheduling at the servers is
First-Come-First-Served (FCFS), to minimize mean response time we must use
Shortest-Remaining-Processing-Time (SRPT) scheduling at the servers. Much less
is known about dispatching polices when SRPT scheduling is used. Unfortunately,
traditional dispatching policies that are used in practice in systems with FCFS
servers often have poor performance in systems with SRPT servers. In this
paper, we devise a simple fix that can be applied to any dispatching policy.
This fix, called guardrails, ensures that the dispatching policy yields optimal
mean response time under heavy traffic when used in a system with SRPT servers.
Any dispatching policy, when augmented with guardrails, becomes heavy-traffic
optimal. Our results yield the first analytical bounds on mean response time
for load balancing systems with SRPT scheduling at the servers.
"
1684,Enhanced Performance and Privacy for TLS over TCP Fast Open,"  Small TCP flows make up the majority of web flows. For them, the TCP
three-way handshake induces significant delay overhead. The TCP Fast Open (TFO)
protocol can significantly decrease this delay via zero round-trip time (0-RTT)
handshakes for all TCP handshakes that follow a full initial handshake to the
same host. However, this comes at the cost of privacy limitations and also has
some performance limitations. In this paper, we investigate the TFP deployment
on popular websites and browsers. We found that a client revisiting a web site
for the first time fails to use an abbreviated TFO handshake in 40% of all
cases due to web server load-balancing using multiple IP addresses. Our
analysis further reveals significant privacy problems of the protocol design
and implementation. Network-based attackers and online trackers can exploit TFO
to track the online activities of users. As a countermeasure, we introduce a
novel protocol called TCP Fast Open Privacy (FOP). TCP FOP prevents tracking by
network attackers and impedes third-party tracking, while still allowing 0-RTT
handshakes as in TFO. As a proof-of-concept, we have implemented the proposed
protocol for the Linux kernel and a TLS library. Our measurements indicate that
TCP FOP outperforms TLS over TFO when websites are served from multiple IP
addresses.
"
1685,"Multiplica\c{c}\~ao de matrizes: uma compara\c{c}\~ao entre as
  abordagens sequencial (CPU) e paralela (GPU)","  Designing problems using matrices is very important in Computer Science.
Fields like graph computer, graphs theory, and machine learning use matrices
very often to solve their own problems. The most often matrix operation is the
multiplication. It may be time-consuming if the matrices to be multiplied are
large. For this reason, the parallel computer became a must to tackle this
problem. In this report, it is presented a comparison between sequential and
parallel approaches to computing the matrix multiplication using CUDA and
openMP. The results show the importance of parallelizing mainly when the
matrices are large.
  A modelagem de problemas utilizando matrizes \'e de extrema import\^ancia
para Ci\^encia da Computa\c{c}\~ao. \'Areas como computa\c{c}\~ao gr\'afica,
grafos e aprendizado de m\'aquina utilizam matrizes com alta frequ\^encia para
solucionar seus respectivos problemas. Dessa forma, operar matrizes de maneira
eficiente \'e muito importante para o desempenho de algoritmos. Uma das
opera\c{c}\~oes de matrizes mais utilizadas \'e a multiplica\c{c}\~ao, que se
torna um empecilho para o desempenho computacional de algoritmos na medida que
o tamanho das matrizes a serem multiplicadas aumentam. Por conta disso, a
computa\c{c}\~ao paralela se tornou uma solu\c{c}\~ao padr\~ao para abordar tal
problema. Neste trabalho \'e apresentado uma compara\c{c}\~ao entre as
abordagens sequencial e paralela para multiplica\c{c}\~ao de matrizes
utilizando CUDA e OpenMP. O resultado da an\'alise realizada entre o tamanho da
matriz e o desempenho da multiplica\c{c}\~ao mostra a import\^ancia da
paraleliza\c{c}\~ao principalmente para matrizes de ordem elevada.
"
1686,"Machine Learning Based Routing Congestion Prediction in FPGA High-Level
  Synthesis","  High-level synthesis (HLS) shortens the development time of hardware designs
and enables faster design space exploration at a higher abstraction level.
Optimization of complex applications in HLS is challenging due to the effects
of implementation issues such as routing congestion. Routing congestion
estimation is absent or inaccurate in existing HLS design methods and tools.
Early and accurate congestion estimation is of great benefit to guide the
optimization in HLS and improve the efficiency of implementation. However,
routability, a serious concern in FPGA designs, has been difficult to evaluate
in HLS without analyzing post-implementation details after Place and Route. To
this end, we propose a novel method to predict routing congestion in HLS using
machine learning and map the expected congested regions in the design to the
relevant high-level source code. This is greatly beneficial in early
identification of routability oriented bottlenecks in the high-level source
code without running time-consuming register-transfer level (RTL)
implementation flow. Experiments demonstrate that our approach accurately
estimates vertical and horizontal routing congestion with errors of 6.71% and
10.05% respectively. By presenting Face Detection application as a case study,
we show that by discovering the bottlenecks in high-level source code, routing
congestion can be easily and quickly resolved compared to the efforts involved
in RTL implementation and design feedback.
"
1687,"On the Distribution of AoI for the GI/GI/1/1 and GI/GI/1/2* Systems:
  Exact Expressions and Bounds","  Since Age of Information (AoI) has been proposed as a metric that quantifies
the freshness of information updates in a communication system, there has been
a constant effort in understanding and optimizing different statistics of the
AoI process for classical queueing systems. In addition to classical queuing
systems, more recently, systems with no queue or a unit capacity queue storing
the latest packet have been gaining importance as storing and transmitting
older packets do not reduce AoI at the receiver. Following this line of
research, we study the distribution of AoI for the GI/GI/1/1 and GI/GI/1/2*
systems, under non-preemptive scheduling. For any single-source-single-server
queueing system, we derive, using sample path analysis, a fundamental result
that characterizes the AoI violation probability, and use it to obtain
closed-form expressions for D/GI/1/1, M/GI/1/1 as well as systems that use
zero-wait policy. Further, when exact results are not tractable, we present a
simple methodology for obtaining upper bounds for the violation probability for
both GI/GI/1/1 and GI/GI/1/2* systems. An interesting feature of the proposed
upper bounds is that, if the departure rate is given, they overestimate the
violation probability by at most a value that decreases with the arrival rate.
Thus, given the departure rate and for a fixed average service, the bounds are
tighter at higher utilization.
"
1688,Inferring Catchment in Internet Routing,"  BGP is the de-facto Internet routing protocol for exchanging prefix
reachability information between Autonomous Systems (AS). It is a dynamic,
distributed, path-vector protocol that enables rich expressions of network
policies (typically treated as secrets). In this regime, where complexity is
interwoven with information hiding, answering questions such as ""what is the
expected catchment of the anycast sites of a content provider on the AS-level,
if new sites are deployed?"", or ""how will load-balancing behave if an ISP
changes its routing policy for a prefix?"", is a hard challenge. In this work,
we present a formal model and methodology that takes into account policy-based
routing and topological properties of the Internet graph, to predict the
routing behavior of networks. We design algorithms that provide new
capabilities for informative route inference (e.g., isolating the effect of
randomness that is present in prior simulation-based approaches). We analyze
the properties of these inference algorithms, and evaluate them using publicly
available routing datasets and real-world experiments. The proposed framework
can be useful in a number of applications: measurements, traffic engineering,
network planning, Internet routing models, etc. As a use case, we study the
problem of selecting a set of measurement vantage points to maximize route
inference. Our methodology is general and can capture standard valley-free
routing, as well as more complex topological and routing setups appearing in
practice.
"
1689,"K-Athena: a performance portable structured grid finite volume
  magnetohydrodynamics code","  Large scale simulations are a key pillar of modern research and require
ever-increasing computational resources. Different novel manycore architectures
have emerged in recent years on the way towards the exascale era. Performance
portability is required to prevent repeated non-trivial refactoring of a code
for different architectures. We combine Athena++, an existing
magnetohydrodynamics (MHD) CPU code, with Kokkos, a performance portable
on-node parallel programming paradigm, into K-Athena to allow efficient
simulations on multiple architectures using a single codebase. We present
profiling and scaling results for different platforms including Intel Skylake
CPUs, Intel Xeon Phis, and NVIDIA GPUs. K-Athena achieves $>10^8$
cell-updates/s on a single V100 GPU for second-order double precision MHD
calculations, and a speedup of 30 on up to 24,576 GPUs on Summit (compared to
172,032 CPU cores), reaching $1.94\times10^{12}$ total cell-updates/s at 76%
parallel efficiency. Using a roofline analysis we demonstrate that the overall
performance is currently limited by DRAM bandwidth and calculate a performance
portability metric of 62.8%. Finally, we present the implementation strategies
used and the challenges encountered in maximizing performance. This will
provide other research groups with a straightforward approach to prepare their
own codes for the exascale era. K-Athena is available at
https://gitlab.com/pgrete/kathena .
"
1690,"QPS-r: A Cost-Effective Crossbar Scheduling Algorithm and Its Stability
  and Delay Analysis","  In an input-queued switch, a crossbar schedule, or a matching between the
input ports and the output ports needs to be computed in each switching cycle,
or time slot. Designing switching algorithms with very low computational
complexity, that lead to high throughput and small delay is a challenging
problem. There appears to be a fundamental tradeoff between the computational
complexity of the switching algorithm and the resultants throughput and delay.
Parallel maximal matching algorithms (adapted for switching) appear to have
stricken a sweet spot in this tradeoff, and prior work has shown the following
performance guarantees. Using maximal matchings in every time slot results in
at least 50% switch throughput and order-optimal (i.e., independent of the
switch size N) average delay bounds for various traffic arrival processes. On
the other hand, their computational complexity can be as low as $O(log^2N)$ per
port/processor, which is much lower than those of the algorithms such as
maximum weighted matching which ensures better throughput performance.
  In this work, we propose QPS-r, a parallel iterative switching algorithm that
has the lowest possible computational complexity: O(1) per port. Using Lyapunov
stability analysis, we show that the throughput and delay performance is
identical to that of maximal matching algorithm. Although QPS-r builds upon an
existing technique called Queue-Proportional Sampling (QPS), in this paper, we
provide analytical guarantees on its throughput and delay under i.i.d. traffic
as well as a Markovian traffic model which can model many realistic traffic
patterns. We also demonstrate that QPS-3 (running 3 iterations) has comparable
empirical throughput and delay performances as iSLIP (running $log_2 N$
iterations), a refined and optimized representative maximal matching algorithm
adapted for switching.
"
1691,Coded Distributed Tracking,"  We consider the problem of tracking the state of a process that evolves over
time in a distributed setting, with multiple observers each observing parts of
the state, which is a fundamental information processing problem with a wide
range of applications. We propose a cloud-assisted scheme where the tracking is
performed over the cloud. In particular, to provide timely and accurate
updates, and alleviate the straggler problem of cloud computing, we propose a
coded distributed computing approach where coded observations are distributed
over multiple workers. The proposed scheme is based on a coded version of the
Kalman filter that operates on data encoded with an erasure correcting code,
such that the state can be estimated from partial updates computed by a subset
of the workers. We apply the proposed scheme to the problem of tracking
multiple vehicles. We show that replication achieves significantly higher
accuracy than the corresponding uncoded scheme. The use of maximum distance
separable (MDS) codes further improves accuracy for larger update intervals. In
both cases, the proposed scheme approaches the accuracy of an ideal centralized
scheme when the update interval is large enough. Finally, we observe a
trade-off between age-of-information and estimation accuracy for MDS codes.
"
1692,"Selection Combining Scheme over Non-identically Distributed
  Fisher-Snedecor $\mathcal{F}$ Fading Channels","  In this paper, the performance of the selection combining (SC) scheme over
Fisher-Snedecor $\mathcal{F}$ fading channels with independent and
non-identically distributed (i.n.i.d.) branches is analysed. The probability
density function (PDF) and the moment generating function (MGF) of the maximum
i.n.i.d. Fisher-Snedecor $\mathcal{F}$ variates are derived first in terms of
the multivariate Fox's $H$-function that has been efficiently implemented in
the technical literature by various software codes. Based on this, the average
bit error probability (ABEP) and the average channel capacity (ACC) of SC
diversity with i.n.i.d. receivers are investigated. Moreover, we analyse the
performance of the energy detection that is widely employed to perform the
spectrum sensing in cognitive radio networks via deriving the average detection
probability (ADP) and the average area under the receiver operating
characteristics curve (AUC). To validate our analysis, the numerical results
are affirmed by the Monte Carlo simulations.
"
1693,Performance Analysis of Non-DC-Biased OFDM,"  The performance analysis of a novel optical modulation scheme is presented in
this paper. The basic concept is to transmit signs of modulated optical
orthogonal frequency division multiplexing (O-OFDM) symbols and absolute values
of the symbols separately by two information carrying units: 1) indices of two
light emitting diodes (LED) transmitters that represent positive and negative
signs separately; and 2) optical intensity symbols that carry the absolute
values of signals. The new approach, referred as to non-DC-biased OFDM
(NDC-OFDM), uses the optical spatial modulation (OSM) technique to eliminate
the effect of the clipping distortion in DC-biased optical OFDM (DCO-OFDM). In
addition, it can achieve similar advantages as the conventional unipolar
modulation scheme, asymmetrically clipped optical OFDM (ACO-OFDM), without
using additional subcarriers. In this paper, the analytical BER performance is
compared with the Monte Carlo result in order to prove the reliability of the
new method. Moreover, the practical BER performance of NDC-OFDM with DCO-OFDM
and ACO-OFDM is compared for different constellation sizes to verify the
improvement of NDC-OFDM on the spectral and power efficiencies.
"
1694,"Significance of parallel computing on the performance of Digital Image
  Correlation algorithms in MATLAB","  Digital Image Correlation (DIC) is a powerful tool used to evaluate
displacements and deformations in a non-intrusive manner. By comparing two
images, one of the undeformed reference state of a specimen and another of the
deformed target state, the relative displacement between those two states is
determined. DIC is well known and often used for post-processing analysis of
in-plane displacements and deformation of specimen. Increasing the analysis
speed to enable real-time DIC analysis will be beneficial and extend the field
of use of this technique. Here we tested several combinations of the most
common DIC methods in combination with different parallelization approaches in
MATLAB and evaluated their performance to determine whether real-time analysis
is possible with these methods. To reflect improvements in computing technology
different hardware settings were also analysed. We found that implementation
problems can reduce the efficiency of a theoretically superior algorithm such
that it becomes practically slower than a sub-optimal algorithm. The
Newton-Raphson algorithm in combination with a modified Particle Swarm
algorithm in parallel image computation was found to be most effective. This is
contrary to theory, suggesting that the inverse-compositional Gauss-Newton
algorithm is superior. As expected, the Brute Force Search algorithm is the
least effective method. We also found that the correct choice of
parallelization tasks is crucial to achieve improvements in computing speed. A
poorly chosen parallelisation approach with high parallel overhead leads to
inferior performance. Finally, irrespective of the computing mode the correct
choice of combinations of integer-pixel and sub-pixel search algorithms is
decisive for an efficient analysis. Using currently available hardware
real-time analysis at high framerates remains an aspiration.
"
1695,"Optimizing the Linear Fascicle Evaluation Algorithm for Multi-Core and
  Many-Core Systems","  Sparse matrix-vector multiplication (SpMV) operations are commonly used in
various scientific applications. The performance of the SpMV operation often
depends on exploiting regularity patterns in the matrix. Various
representations have been proposed to minimize the memory bandwidth bottleneck
arising from the irregular memory access pattern involved. Among recent
representation techniques, tensor decomposition is a popular one used for very
large but sparse matrices. Post sparse-tensor decomposition, the new
representation involves indirect accesses, making it challenging to optimize
for multi-cores and GPUs.
  Computational neuroscience algorithms often involve sparse datasets while
still performing long-running computations on them. The LiFE application is a
popular neuroscience algorithm used for pruning brain connectivity graphs. The
datasets employed herein involve the Sparse Tucker Decomposition (STD), a
widely used tensor decomposition method. Using this decomposition leads to
irregular array references, making it very difficult to optimize for both CPUs
and GPUs. Recent codes of the LiFE algorithm show that its SpMV operations are
the key bottleneck for performance and scaling. In this work, we first propose
target-independent optimizations to optimize these SpMV operations, followed by
target-dependent optimizations for CPU and GPU systems. The target-independent
techniques include: (1) standard compiler optimizations, (2) data restructuring
methods, and (3) methods to partition computations among threads. Then we
present the optimizations for CPUs and GPUs to exploit platform-specific speed.
Our highly optimized CPU code obtain a speedup of 27.12x over the original
sequential CPU code running on 16-core Intel Xeon (Skylake-based) system, and
our optimized GPU code achieves a speedup of 5.2x over a reference optimized
GPU code version on NVIDIA's GeForce RTX 2080 Ti GPU.
"
1696,Performance Analysis of SPAD-based OFDM,"  In this paper, an analytical approach for the nonlinear distorted bit error
rate performance of optical orthogonal frequency division multiplexing (O-OFDM)
with single photon avalanche diode (SPAD) receivers is presented. Major
distortion effects of passive quenching (PQ) and active quenching (AQ) SPAD
receivers are analysed in this study. The performance analysis of DC-biased
O-OFDM and asymmetrically clipped O-OFDM with PQ and AQ SPAD are derived. The
comparison results show the maximum optical irradiance caused by the nonlinear
distortion, which limits the transmission power and bit rate. The theoretical
maximum bit rate of SPAD-based OFDM is found which is up to 1~Gbits/s. This
approach supplies a closed-form analytical solution for designing an optimal
SPAD-based system.
"
1697,Blockchain Goes Green? An Analysis of Blockchain on Low-Power Nodes,"  Motivated by the massive energy usage of blockchain, on the one hand, and by
significant performance improvements in low-power, wimpy systems, on the other
hand, we perform an in-depth time-energy analysis of blockchain systems on
low-power nodes in comparison to high-performance nodes. We use three low-power
systems to represent a wide range of the performance-power spectrum, while
covering both x86/64 and ARM architectures. We show that low-end wimpy nodes
are struggling to run full-fledged blockchains mainly due to their small and
low-bandwidth memory. On the other hand, wimpy systems with balanced
performance-to-power ratio achieve reasonable performance while saving
significant amounts of energy. For example, Jetson TX2 nodes achieve around 80%
and 30% of the throughput of Parity and Hyperledger, respectively, while using
18x and 23x less energy compared to traditional brawny servers with Intel Xeon
CPU.
"
1698,"EmBench: Quantifying Performance Variations of Deep Neural Networks
  across Modern Commodity Devices","  In recent years, advances in deep learning have resulted in unprecedented
leaps in diverse tasks spanning from speech and object recognition to context
awareness and health monitoring. As a result, an increasing number of
AI-enabled applications are being developed targeting ubiquitous and mobile
devices. While deep neural networks (DNNs) are getting bigger and more complex,
they also impose a heavy computational and energy burden on the host devices,
which has led to the integration of various specialized processors in commodity
devices. Given the broad range of competing DNN architectures and the
heterogeneity of the target hardware, there is an emerging need to understand
the compatibility between DNN-platform pairs and the expected performance
benefits on each platform. This work attempts to demystify this landscape by
systematically evaluating a collection of state-of-the-art DNNs on a wide
variety of commodity devices. In this respect, we identify potential
bottlenecks in each architecture and provide important guidelines that can
assist the community in the co-design of more efficient DNNs and accelerators.
"
1699,On a caching system with object sharing,"  We consider a content-caching system thatis shared by a number of proxies.
The cache could belocated in an edge-cloud datacenter and the proxies couldeach
serve a large population of mobile end-users. Eachproxy operates its own
LRU-list of a certain capacity inthe shared cache. The length of objects
simultaneouslyappearing in plural LRU-lists is equally divided amongthem,i.e.,
object sharing among the LRUs. We provide a ""working-set"" approximation for
this system to quicklyestimate the cache-hit probabilities under such
objectsharing, which can be used to facilitate admission control.Also, a way to
reduce ripple evictions,i.e.,setrequestoverhead, is suggested. We give
numerical results for ourMemCacheD with Object Sharing (MCD-OS) prototype.
"
1700,"rDLB: A Novel Approach for Robust Dynamic Load Balancing of Scientific
  Applications with Parallel Independent Tasks","  Scientific applications often contain large and computationally intensive
parallel loops. Dynamic loop self scheduling (DLS) is used to achieve a
balanced load execution of such applications on high performance computing
(HPC) systems. Large HPC systems are vulnerable to processors or node failures
and perturbations in the availability of resources. Most self-scheduling
approaches do not consider fault-tolerant scheduling or depend on failure or
perturbation detection and react by rescheduling failed tasks. In this work, a
robust dynamic load balancing (rDLB) approach is proposed for the robust self
scheduling of independent tasks. The proposed approach is proactive and does
not depend on failure or perturbation detection. The theoretical analysis of
the proposed approach shows that it is linearly scalable and its cost decrease
quadratically by increasing the system size. rDLB is integrated into an MPI DLS
library to evaluate its performance experimentally with two computationally
intensive scientific applications. Results show that rDLB enables the tolerance
of up to (P minus one) processor failures, where P is the number of processors
executing an application. In the presence of perturbations, rDLB boosted the
robustness of DLS techniques up to 30 times and decreased application execution
time up to 7 times compared to their counterparts without rDLB.
"
1701,"Exploiting Parallelism on Shared Memory in the QED Particle-in-Cell Code
  PICADOR with Greedy Load Balancing","  State-of-the-art numerical simulations of laser plasma by means of the
Particle-in-Cell method are often extremely computationally intensive.
Therefore there is a growing need for development of approaches for efficient
utilization of resources of modern supercomputers. In this paper, we address
the problem of a substantially non-uniform and dynamically varying distribution
of macroparticles in a computational area in simulating quantum electrodynamic
(QED) cascades. We propose and evaluate a load balancing scheme for shared
memory systems, which allows subdividing individual cells of the computational
domain into work portions with subsequent dynamic distribution of these
portions between OpenMP threads. Computational experiments on 1D, 2D, and 3D
QED simulations show that the proposed scheme outperforms the previously
developed standard and custom schemes in the PICADOR code by 2.1 to 10 times
when employing several Intel Cascade Lake CPUs.
"
1702,Scylla: A Mesos Framework for Container Based MPI Jobs,"  Open source cloud technologies provide a wide range of support for creating
customized compute node clusters to schedule tasks and managing resources. In
cloud infrastructures such as Jetstream and Chameleon, which are used for
scientific research, users receive complete control of the Virtual Machines
(VM) that are allocated to them. Importantly, users get root access to the VMs.
This provides an opportunity for HPC users to experiment with new resource
management technologies such as Apache Mesos that have proven scalability,
flexibility, and fault tolerance. To ease the development and deployment of HPC
tools on the cloud, the containerization technology has matured and is gaining
interest in the scientific community. In particular, several well known
scientific code bases now have publicly available Docker containers. While
Mesos provides support for Docker containers to execute individually, it does
not provide support for container inter-communication or orchestration of the
containers for a parallel or distributed application. In this paper, we present
the design, implementation, and performance analysis of a Mesos framework,
Scylla, which integrates Mesos with Docker Swarm to enable orchestration of MPI
jobs on a cluster of VMs acquired from the Chameleon cloud [1]. Scylla uses
Docker Swarm for communication between containerized tasks (MPI processes) and
Apache Mesos for resource pooling and allocation. Scylla allows a policy-driven
approach to determine how the containers should be distributed across the nodes
depending on the CPU, memory, and network throughput requirement for each
application.
"
1703,"Tromino: Demand and DRF Aware Multi-Tenant Queue Manager for Apache
  Mesos Cluster","  Apache Mesos, a two-level resource scheduler, provides resource sharing
across multiple users in a multi-tenant cluster environment. Computational
resources (i.e., CPU, memory, disk, etc. ) are distributed according to the
Dominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive
resources based on their current usage and are responsible for scheduling their
tasks within the allocation. We have observed that multiple frameworks can
cause fairness imbalance in a multiuser environment. For example, a greedy
framework consuming more than its fair share of resources can deny resource
fairness to others. The user with the least Dominant Share is considered first
by the DRF module to get its resource allocation. However, the default DRF
implementation, in Apache Mesos' Master allocation module, does not consider
the overall resource demands of the tasks in the queue for each user/framework.
This lack of awareness can result in users without any pending task receiving
more resource offers while users with a queue of pending tasks starve due to
their high dominant shares. We have developed a policy-driven queue manager,
Tromino, for an Apache Mesos cluster where tasks for individual frameworks can
be scheduled based on each framework's overall resource demands and current
resource consumption. Dominant Share and demand awareness of Tromino and
scheduling based on these attributes can reduce (1) the impact of unfairness
due to a framework specific configuration, and (2) unfair waiting time due to
higher resource demand in a pending task queue. In the best case, Tromino can
significantly reduce the average waiting time of a framework by using the
proposed Demand-DRF aware policy.
"
1704,"Exploring the Fairness and Resource Distribution in an Apache Mesos
  Environment","  Apache Mesos, a cluster-wide resource manager, is widely deployed in massive
scale at several Clouds and Data Centers. Mesos aims to provide high cluster
utilization via fine grained resource co-scheduling and resource fairness among
multiple users through Dominant Resource Fairness (DRF) based allocation. DRF
takes into account different resource types (CPU, Memory, Disk I/O) requested
by each application and determines the share of each cluster resource that
could be allocated to the applications. Mesos has adopted a two-level
scheduling policy: (1) DRF to allocate resources to competing frameworks and
(2) task level scheduling by each framework for the resources allocated during
the previous step. We have conducted experiments in a local Mesos cluster when
used with frameworks such as Apache Aurora, Marathon, and our own framework
Scylla, to study resource fairness and cluster utilization. Experimental
results show how informed decision regarding second level scheduling policy of
frameworks and attributes like offer holding period, offer refusal cycle and
task arrival rate can reduce unfair resource distribution. Bin-Packing
scheduling policy on Scylla with Marathon can reduce unfair allocation from
38\% to 3\%. By reducing unused free resources in offers we bring down the
unfairness from to 90\% to 28\%. We also show the effect of task arrival rate
to reduce the unfairness from 23\% to 7\%.
"
1705,Evaluation of Docker Containers for Scientific Workloads in the Cloud,"  The HPC community is actively researching and evaluating tools to support
execution of scientific applications in cloud-based environments. Among the
various technologies, containers have recently gained importance as they have
significantly better performance compared to full-scale virtualization, support
for microservices and DevOps, and work seamlessly with workflow and
orchestration tools. Docker is currently the leader in containerization
technology because it offers low overhead, flexibility, portability of
applications, and reproducibility. Singularity is another container solution
that is of interest as it is designed specifically for scientific applications.
It is important to conduct performance and feature analysis of the container
technologies to understand their applicability for each application and target
execution environment. This paper presents a (1) performance evaluation of
Docker and Singularity on bare metal nodes in the Chameleon cloud (2) mechanism
by which Docker containers can be mapped with InfiniBand hardware with RDMA
communication and (3) analysis of mapping elements of parallel workloads to the
containers for optimal resource management with container-ready orchestration
tools. Our experiments are targeted toward application developers so that they
can make informed decisions on choosing the container technologies and
approaches that are suitable for their HPC workloads on cloud infrastructure.
Our performance analysis shows that scientific workloads for both Docker and
Singularity based containers can achieve near-native performance. Singularity
is designed specifically for HPC workloads. However, Docker still has
advantages over Singularity for use in clouds as it provides overlay networking
and an intuitive way to run MPI applications with one container per rank for
fine-grained resources allocation.
"
1706,Performance Analysis of Deep Learning Workloads on Leading-edge Systems,"  This work examines the performance of leading-edge systems designed for
machine learning computing, including the NVIDIA DGX-2, Amazon Web Services
(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a
consumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning
workloads from the fields of computer vision and natural language processing
are the focus of the analysis. Performance analysis is performed along with a
number of important dimensions. Performance of the communication interconnects
and large and high-throughput deep learning models are considered. Different
potential use models for the systems as standalone and in the cloud also are
examined. The effect of various optimization of the deep learning models and
system configurations is included in the analysis.
"
1707,Low Overhead Instruction Latency Characterization for NVIDIA GPGPUs,"  The last decade has seen a shift in the computer systems industry where
heterogeneous computing has become prevalent. Graphics Processing Units (GPUs)
are now present in supercomputers to mobile phones and tablets. GPUs are used
for graphics operations as well as general-purpose computing (GPGPUs) to boost
the performance of compute-intensive applications. However, the percentage of
undisclosed characteristics beyond what vendors provide is not small. In this
paper, we introduce a very low overhead and portable analysis for exposing the
latency of each instruction executing in the GPU pipeline(s) and the access
overhead of the various memory hierarchies found in GPUs at the
micro-architecture level. Furthermore, we show the impact of the various
optimizations the CUDA compiler can perform over the various latencies. We
perform our evaluation on seven different high-end NVIDIA GPUs from five
different generations/architectures: Kepler, Maxwell, Pascal, Volta, and
Turing. The results in this paper can help architects to have an accurate
characterization of the latencies of these GPUs, which will help in modeling
the hardware accurately. Also, software developers can perform informed
optimizations to their applications.
"
1708,NTP : A Neural Network Topology Profiler,"  Performance of end-to-end neural networks on a given hardware platform is a
function of its compute and memory signature, which in-turn, is governed by a
wide range of parameters such as topology size, primitives used, framework
used, batching strategy, latency requirements, precision etc. Current
benchmarking tools suffer from limitations such as a) being either too granular
like DeepBench [1] (or) b) mandate a working implementation that is either
framework specific or hardware-architecture specific or both (or) c) provide
only high level benchmark metrics. In this paper, we present NTP (Neural Net
Topology Profiler), a sophisticated benchmarking framework, to effectively
identify memory and compute signature of an end-to-end topology on multiple
hardware architectures, without the need for an actual implementation. NTP is
tightly integrated with hardware specific benchmarking tools to enable
exhaustive data collection and analysis. Using NTP, a deep learning researcher
can quickly establish baselines needed to understand performance of an
end-to-end neural network topology and make high level architectural decisions.
Further, integration of NTP with frameworks like Tensorflow, Pytorch, Intel
OpenVINO etc. allows for performance comparison along several vectors like a)
Comparison of different frameworks on a given hardware b) Comparison of
different hardware using a given framework c) Comparison across different
heterogeneous hardware configurations for given framework etc. These
capabilities empower a researcher to effortlessly make architectural decisions
needed for achieving optimized performance on any hardware platform. The paper
documents the architectural approach of NTP and demonstrates the capabilities
of the tool by benchmarking Mozilla DeepSpeech, a popular Speech Recognition
topology.
"
1709,"Online Collection and Forecasting of Resource Utilization in Large-Scale
  Distributed Systems","  Large-scale distributed computing systems often contain thousands of
distributed nodes (machines). Monitoring the conditions of these nodes is
important for system management purposes, which, however, can be extremely
resource demanding as this requires collecting local measurements of each
individual node and constantly sending those measurements to a central
controller. Meanwhile, it is often useful to forecast the future system
conditions for various purposes such as resource planning/allocation and
anomaly detection, but it is usually too resource-consuming to have one
forecasting model running for each node, which may also neglect correlations in
observed metrics across different nodes. In this paper, we propose a mechanism
for collecting and forecasting the resource utilization of machines in a
distributed computing system in a scalable manner. We present an algorithm that
allows each local node to decide when to transmit its most recent measurement
to the central node, so that the transmission frequency is kept below a given
constraint value. Based on the measurements received from local nodes, the
central node summarizes the received data into a small number of clusters.
Since the cluster partitioning can change over time, we also present a method
to capture the evolution of clusters and their centroids. As an effective way
to reduce the amount of computation, time-series forecasting models are trained
on the time-varying centroids of each cluster, to forecast the future resource
utilizations of a group of local nodes. The effectiveness of our proposed
approach is confirmed by extensive experiments using multiple real-world
datasets.
"
1710,"The Stabilized Explicit Variable-Load Solver with Machine Learning
  Acceleration for the Rapid Solution of Stiff Chemical Kinetics","  In this study, a fast and stable machine-learned hybrid algorithm implemented
in TensorFlow for the integration of stiff chemical kinetics is introduced.
Numerical solutions to differential equations are at the core of computational
fluid dynamics calculations. As the size and complexity of the simulations
grow, so does the need for computational power and time. Many efforts have been
made to implement stiff chemistry solvers on GPUs but have not been highly
successful because of the logical divergence in traditional stiff solver
algorithms. Because of these constrains, a novel Explicit Stabilized
Variable-load (STEV) solver has been developed. Overstepping due to the
relatively large time steps is prevented by introducing limits to the maximum
changes of chemical species per time step. To prevent oscillations, a discrete
Fourier transform is introduced to dampen ringing. In contrast to conventional
explicit approaches, a variable-load approach is used where each cell in the
computational domain is advanced with its unique time step. This approach
allows cells to be integrated simultaneously while maintaining warp convergence
but finish at different iterations and be removed from the workload. To improve
the computational performance of the introduced solver, specific thermodynamic
quantities of interest were estimated using shallow neural networks in place of
polynomial fits, leading to an additional 10% savings in clock time with
minimal training and implementation requirements. However ML specific hardware
could increase the time savings to as much as 28%. While the complexity of
these particular machine learning models is not high by modern standards, the
impact on computational efficiency should not be ignored. The results show a
dramatic decrease in total chemistry solution time (over 200 times) while
maintaining a similar degree of accuracy.
"
1711,In-DRAM Bulk Bitwise Execution Engine,"  Many applications heavily use bitwise operations on large bitvectors as part
of their computation. In existing systems, performing such bulk bitwise
operations requires the processor to transfer a large amount of data on the
memory channel, thereby consuming high latency, memory bandwidth, and energy.
In this paper, we describe Ambit, a recently-proposed mechanism to perform bulk
bitwise operations completely inside main memory. Ambit exploits the internal
organization and analog operation of DRAM-based memory to achieve low cost,
high performance, and low energy. Ambit exposes a new bulk bitwise execution
model to the host processor. Evaluations show that Ambit significantly improves
the performance of several applications that use bulk bitwise operations,
including databases.
"
1712,Semi-Quantitative Abstraction and Analysis of Chemical Reaction Networks,"  Analysis of large continuous-time stochastic systems is a computationally
intensive task. In this work we focus on population models arising from
chemical reaction networks (CRNs), which play a fundamental role in analysis
and design of biochemical systems. Many relevant CRNs are particularly
challenging for existing techniques due to complex dynamics including
stochasticity, stiffness or multimodal population distributions. We propose a
novel approach allowing not only to predict, but also to explain both the
transient and steady-state behaviour. It focuses on qualitative description of
the behaviour and aims at quantitative precision only in orders of magnitude.
Firstly, we abstract the CRN into a compact model preserving rough timing
information, distinguishing only signifcinatly different populations, but
capturing relevant sequences of behaviour. Secondly, we approximately analyse
the most probable temporal behaviours of the model through most probable
transitions. As demonstrated on complex CRNs from literature, our approach
reproduces the known results, but in contrast to the state-of-the-art methods,
it runs with virtually no computational cost and thus offers
unprecedented~scalability.
"
1713,"Propagation and Decay of Injected One-Off Delays on Clusters: A Case
  Study","  Analytic, first-principles performance modeling of distributed-memory
applications is difficult due to a wide spectrum of random disturbances caused
by the application and the system. These disturbances (commonly called ""noise"")
destroy the assumptions of regularity that one usually employs when
constructing simple analytic models. Despite numerous efforts to quantify,
categorize, and reduce such effects, a comprehensive quantitative understanding
of their performance impact is not available, especially for long delays that
have global consequences for the parallel application. In this work, we
investigate various traces collected from synthetic benchmarks that mimic real
applications on simulated and real message-passing systems in order to pinpoint
the mechanisms behind delay propagation. We analyze the dependence of the
propagation speed of idle waves emanating from injected delays with respect to
the execution and communication properties of the application, study how such
delays decay under increased noise levels, and how they interact with each
other. We also show how fine-grained noise can make a system immune against the
adverse effects of propagating idle waves. Our results contribute to a better
understanding of the collective phenomena that manifest themselves in
distributed-memory parallel applications.
"
1714,"The Impact of GPU DVFS on the Energy and Performance of Deep Learning:
  an Empirical Study","  Over the past years, great progress has been made in improving the computing
power of general-purpose graphics processing units (GPGPUs), which facilitates
the prosperity of deep neural networks (DNNs) in multiple fields like computer
vision and natural language processing. A typical DNN training process
repeatedly updates tens of millions of parameters, which not only requires huge
computing resources but also consumes significant energy. In order to train
DNNs in a more energy-efficient way, we empirically investigate the impact of
GPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and
performance of deep learning. Our experiments cover a wide range of GPU
architectures, DVFS settings, and DNN configurations. We observe that, compared
to the default core frequency settings of three tested GPUs, the optimal core
frequency can help conserve 8.7%$\sim$23.1% energy consumption for different
DNN training cases. Regarding the inference, the benefits vary from
19.6%$\sim$26.4%. Our findings suggest that GPU DVFS has great potentials to
help develop energy efficient DNN training/inference schemes.
"
1715,Function-as-a-Service Benchmarking Framework,"  Cloud Service Providers deliver their products in form of 'as-a-Service',
which are typically categorized by the level of abstraction. This approach
hides the implementation details and shows only functionality to the user.
However, the problem is that it is hard to measure the performance of Cloud
services, because they behave like black boxes. Especially with
Function-as-a-Service it is even more difficult because it completely hides
server and infrastructure management from users by design. Cloud Service
Prodivers usually restrict the maximum size of code, memory and runtime of
Cloud Functions. Nevertheless, users need clarification if more ressources are
needed to deliver services in high quality. In this regard, we present the
architectural design of a new Function-as-a-Service benchmarking tool, which
allows users to evaluate the performance of Cloud Functions. Furthermore, the
capabilities of the framework are tested on an isolated platform with a
specific workload. The results show that users are able to get insights into
Function-as-a-Service environments. This, in turn, allows users to identify
factors which may slow down or speed up the performance of Cloud Functions.
"
1716,The Supermarket Model with Known and Predicted Service Times,"  The supermarket model refers to a system with a large number of queues, where
arriving customers choose $d$ queues at random and join the queue with the
fewest customers. The supermarket model demonstrates the power of even small
amounts of choice, as compared to simply joining a queue chosen uniformly at
random, for load balancing systems. In this work we perform simulation-based
studies to consider variations where service times for a customer are
predicted, as might be done in modern settings using machine learning
techniques or related mechanisms. Our primary takeaway is that using even
seemingly weak predictions of service times can yield significant benefits over
blind First In First Out queueing in this context. However, some care must be
taken when using predicted service time information to both choose a queue and
order elements for service within a queue; while in many cases using the
information for both choosing and ordering is beneficial, in many of our
simulation settings we find that simply using the number of jobs to choose a
queue is better when using predicted service times to order jobs in a queue.
Although this study is simulation based, our study leaves many natural
theoretical open questions for future work.
"
1717,"Categorization of Program Regions for Agile Compilation using Machine
  Learning and Hardware Support","  A compiler processes the code written in a high level language and produces
machine executable code. The compiler writers often face the challenge of
keeping the compilation times reasonable. That is because aggressive
optimization passes which potentially will give rise to high performance are
often expensive in terms of running time and memory footprint. Consequently the
compiler designers arrive at a compromise where they either simplify the
optimization algorithm which may decrease the performance of the produced code,
or they will restrict the optimization to the subset of the overall input
program in which case large parts of the input application will go
un-optimized.
  The problem we address in this paper is that of keeping the compilation times
reasonable, and at the same time optimizing the input program to the fullest
extent possible. Consequently, the performance of the produced code will match
the performance when all the aggressive optimization passes are applied over
the entire input program.
"
1718,Designing and Implementing Data Warehouse for Agricultural Big Data,"  In recent years, precision agriculture that uses modern information and
communication technologies is becoming very popular. Raw and semi-processed
agricultural data are usually collected through various sources, such as:
Internet of Thing (IoT), sensors, satellites, weather stations, robots, farm
equipment, farmers and agribusinesses, etc. Besides, agricultural datasets are
very large, complex, unstructured, heterogeneous, non-standardized, and
inconsistent. Hence, the agricultural data mining is considered as Big Data
application in terms of volume, variety, velocity and veracity. It is a key
foundation to establishing a crop intelligence platform, which will enable
resource efficient agronomy decision making and recommendations. In this paper,
we designed and implemented a continental level agricultural data warehouse by
combining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1)
flexible schema; (2) data integration from real agricultural multi datasets;
(3) data science and business intelligent support; (4) high performance; (5)
high storage; (6) security; (7) governance and monitoring; (8) replication and
recovery; (9) consistency, availability and partition tolerant; (10)
distributed and cloud deployment. We also evaluate the performance of our data
warehouse.
"
1719,Evaluation of pilot jobs for Apache Spark applications on HPC clusters,"  Big Data has become prominent throughout many scientific fields and, as a
result, scientific communities have sought out Big Data frameworks to
accelerate the processing of their increasingly data-intensive pipelines.
However, while scientific communities typically rely on High-Performance
Computing (HPC) clusters for the parallelization of their pipelines, many
popular Big Data frameworks such as Hadoop and Apache Spark were primarily
designed to be executed on dedicated commodity infrastructures. This paper
evaluates the benefits of pilot jobs over traditional batch submission for
Apache Spark on HPC clusters. Surprisingly, our results show that the speed-up
provided by pilot jobs over batch scheduling is moderate to inexistent (0.98 on
average) despite the presence of long queuing times. In addition, pilot jobs
provide an extra layer of scheduling that complexifies debugging and
deployment. We conclude that traditional batch scheduling should remain the
default strategy to deploy Apache Spark applications on HPC clusters.
"
1720,"Visualizing a Moving Target: A Design Study on Task Parallel Programs in
  the Presence of Evolving Data and Concerns","  Common pitfalls in visualization projects include lack of data availability
and the domain users' needs and focus changing too rapidly for the design
process to complete. While it is often prudent to avoid such projects, we argue
it can be beneficial to engage them in some cases as the visualization process
can help refine data collection, solving a ""chicken and egg"" problem of having
the data and tools to analyze it. We found this to be the case in the domain of
task parallel computing where such data and tooling is an open area of
research. Despite these hurdles, we conducted a design study. Through a
tightly-coupled iterative design process, we built Atria, a multi-view
execution graph visualization to support performance analysis. Atria simplifies
the initial representation of the execution graph by aggregating nodes as
related to their line of code. We deployed Atria on multiple platforms, some
requiring design alteration. We describe how we adapted the design study
methodology to the ""moving target"" of both the data and the domain experts'
concerns and how this movement kept both the visualization and programming
project healthy. We reflect on our process and discuss what factors allow the
project to be successful in the presence of changing data and user needs.
"
1721,Using Metrics Suites to Improve the Measurement of Privacy in Graphs,"  Social graphs are widely used in research (e.g., epidemiology) and business
(e.g., recommender systems). However, sharing these graphs poses privacy risks
because they contain sensitive information about individuals. Graph
anonymization techniques aim to protect individual users in a graph, while
graph de-anonymization aims to re-identify users. The effectiveness of
anonymization and de-anonymization algorithms is usually evaluated with privacy
metrics. However, it is unclear how strong existing privacy metrics are when
they are used in graph privacy. In this paper, we study 26 privacy metrics for
graph anonymization and de-anonymization and evaluate their strength in terms
of three criteria: monotonicity indicates whether the metric indicates lower
privacy for stronger adversaries; for within-scenario comparisons, evenness
indicates whether metric values are spread evenly; and for between-scenario
comparisons, shared value range indicates whether metrics use a consistent
value range across scenarios. Our extensive experiments indicate that no single
metric fulfills all three criteria perfectly. We therefore use methods from
multi-criteria decision analysis to aggregate multiple metrics in a metrics
suite, and we show that these metrics suites improve monotonicity compared to
the best individual metric. This important result enables more monotonic, and
thus more accurate, evaluations of new graph anonymization and de-anonymization
algorithms.
"
1722,"Fast Online ""Next Best Offers"" using Deep Learning","  In this paper, we present iPrescribe, a scalable low-latency architecture for
recommending 'next-best-offers' in an online setting. The paper presents the
design of iPrescribe and compares its performance for implementations using
different real-time streaming technology stacks. iPrescribe uses an ensemble of
deep learning and machine learning algorithms for prediction. We describe the
scalable real-time streaming technology stack and optimized machine-learning
implementations to achieve a 90th percentile recommendation latency of 38
milliseconds. Optimizations include a novel mechanism to deploy recurrent Long
Short Term Memory (LSTM) deep learning networks efficiently.
"
1723,Scaling Video Analytics on Constrained Edge Nodes,"  As video camera deployments continue to grow, the need to process large
volumes of real-time data strains wide area network infrastructure. When
per-camera bandwidth is limited, it is infeasible for applications such as
traffic monitoring and pedestrian tracking to offload high-quality video
streams to a datacenter. This paper presents FilterForward, a new edge-to-cloud
system that enables datacenter-based applications to process content from
thousands of cameras by installing lightweight edge filters that backhaul only
relevant video frames. FilterForward introduces fast and expressive
per-application microclassifiers that share computation to simultaneously
detect dozens of events on computationally constrained edge nodes. Only
matching events are transmitted to the cloud. Evaluation on two real-world
camera feed datasets shows that FilterForward reduces bandwidth use by an order
of magnitude while improving computational efficiency and event detection
accuracy for challenging video content.
"
1724,"A Technique for Finding Optimal Program Launch Parameters Targeting
  Manycore Accelerators","  In this paper, we present a new technique to dynamically determine the values
of program parameters in order to optimize the performance of a multithreaded
program P. To be precise, we describe a novel technique to statically build
another program, say, R, that can dynamically determine the optimal values of
program parameters to yield the best program performance for P given values for
its data and hardware parameters. While this technique can be applied to
parallel programs in general, we are particularly interested in programs
targeting manycore accelerators. Our technique has successfully been employed
for GPU kernels using the MWP-CWP performance model for CUDA.
"
1725,"Robust stability of moving horizon estimation for nonlinear systems with
  bounded disturbances using adaptive arrival cost","  In this paper, the robust stability and convergence to the true state of
moving horizon estimator based on an adaptive arrival cost are established for
nonlinear detectable systems. Robust global asymptotic stability is shown for
the case of non-vanishing bounded disturbances whereas the convergence to the
true state is proved for the case of vanishing disturbances. Several
simulations were made in order to show the estimator behaviour under different
operational conditions and to compare it with the state of the art estimation
methods.
"
1726,"Assessing Performance Implications of Deep Copy Operations via
  Microbenchmarking","  As scientific frameworks become sophisticated, so do their data structures.
Current data structures are no longer simple in design and they have been
progressively complicated. The typical trend in designing data structures in
scientific applications are basically nested data structures: pointing to a
data structure within another one. Managing nested data structures on a modern
heterogeneous system requires tremendous effort due to the separate memory
space design.
  In this paper, we will discuss the implications of deep copy on data
transfers on current heterogeneous. Then, we will discuss the two options that
are currently available to perform the memory copy operations on complex
structures and will introduce pointerchain directive that we proposed.
Afterwards, we will introduce a set of extensive benchmarks to compare the
available approaches. Our goal is to make our proposed benchmarks a basis to
examine the efficiency of upcoming approaches that address the challenge of
deep copy operations.
"
1727,pCAMP: Performance Comparison of Machine Learning Packages on the Edges,"  Machine learning has changed the computing paradigm. Products today are built
with machine intelligence as a central attribute, and consumers are beginning
to expect near-human interaction with the appliances they use. However, much of
the deep learning revolution has been limited to the cloud. Recently, several
machine learning packages based on edge devices have been announced which aim
to offload the computing to the edges. However, little research has been done
to evaluate these packages on the edges, making it difficult for end users to
select an appropriate pair of software and hardware. In this paper, we make a
performance comparison of several state-of-the-art machine learning packages on
the edges, including TensorFlow, Caffe2, MXNet, PyTorch, and TensorFlow Lite.
We focus on evaluating the latency, memory footprint, and energy of these tools
with two popular types of neural networks on different edge devices. This
evaluation not only provides a reference to select appropriate combinations of
hardware and software packages for end users but also points out possible
future directions to optimize packages for developers.
"
1728,"Performance Modelling of Deep Learning on Intel Many Integrated Core
  Architectures","  Many complex problems, such as natural language processing or visual object
detection, are solved using deep learning. However, efficient training of
complex deep convolutional neural networks for large data sets is
computationally demanding and requires parallel computing resources. In this
paper, we present two parameterized performance models for estimation of
execution time of training convolutional neural networks on the Intel many
integrated core architecture. While for the first performance model we
minimally use measurement techniques for parameter value estimation, in the
second model we estimate more parameters based on measurements. We evaluate the
prediction accuracy of performance models in the context of training three
different convolutional neural network architectures on the Intel Xeon Phi. The
achieved average performance prediction accuracy is about 15% for the first
model and 11% for second model.
"
1729,"Adroitness: An Android-based Middleware for Fast Development of
  High-performance Apps","  As smartphones become increasingly more powerful, a new generation of highly
interactive user-centric mobile apps emerge to make user's life simpler and
more productive. Mobile phones applications have to sustain limited resource
availability on mobile devices such as battery life, network connectivity while
also providing better responsiveness, lightweight interactions within the
application. Developers end up spending a considerable amount of time dealing
with the architecture constraints imposed by the wide variety of platforms,
tools, and devices offered by the mobile ecosystem, thereby diverting them from
their main goal of building such apps. Therefore, we propose a mobile-based
middleware architecture that alleviates the burdensome task of dealing with
low-level architectural decisions and fine-grained implementation details. We
achieve such a goal by focusing on the separation of concerns and abstracting
away the complexity of orchestrating device sensors and effectors,
decision-making processes, and connection to remote services, while providing
scaffolding for the development of higher-level functional features of
interactive high-performance mobile apps. We demonstrate the powerfulness of
our approach vs. Android's conventional framework by comparing different
software metric
"
1730,"Architectural Middleware that Supports Building High-performance,
  Scalable, Ubiquitous, Intelligent Personal Assistants","  Intelligent Personal Assistants (IPAs) are software agents that can perform
tasks on behalf of individuals and assist them on many of their daily
activities. IPAs capabilities are expanding rapidly due to the recent advances
on areas such as natural language processing, machine learning, artificial
cognition, and ubiquitous computing, which equip the agents with competences to
understand what users say, collect information from everyday ubiquitous devices
(e.g., smartphones, wearables, tablets, laptops, cars, household appliances,
etc.), learn user preferences, deliver data-driven search results, and make
decisions based on user's context. Apart from the inherent complexity of
building such IPAs, developers and researchers have to address many critical
architectural challenges (e.g., low-latency, scalability, concurrency,
ubiquity, code mobility, interoperability, support to cognitive services and
reasoning, to name a few.), thereby diverting them from their main goal:
building IPAs. Thus, our contribution in this paper is twofold: 1) we propose
an architecture for a platform-agnostic, high-performance, ubiquitous, and
distributed middleware that alleviates the burdensome task of dealing with
low-level implementation details when building IPAs by adding multiple
abstraction layers that hide the underlying complexity; and 2) we present an
implementation of the middleware that concretizes the aforementioned
architecture and allows the development of high-level capabilities while
scaling the system up to hundreds of thousands of IPAs with no extra effort. We
demonstrate the powerfulness of our middleware by analyzing software metrics
for complexity, effort, performance, cohesion and coupling when developing a
conversational IPA.
"
1731,LASSi: Metric based I/O analytics for HPC,"  LASSi is a tool aimed at analyzing application usage and contention caused by
use of shared resources (filesystem or network) in a HPC system. LASSi was
initially developed to support the ARCHER system where there are large
variations in application requirements and occasional user complaints regarding
filesystem performance manifested by variation in job runtimes or poor
interactive response. LASSi takes an approach of defining derivative risk and
ops metrics that relate to unusually high application I/O behaviour. The
metrics are shown to correlate to applications that can experience variable
performance or that may impact the performance of other applications. LASSi
uses I/O statistics over time to provide application I/O profiles and has been
automated to generate daily reports for ARCHER. We demonstrate how LASSi
provides holistic I/O analysis by monitoring filesystem I/O, generating coarse
profiles of filesystems and application runs and automating analysis of
application slowdown using metrics.
"
1732,"Analysis of parallel I/O use on the UK national supercomputing service,
  ARCHER using Cray LASSi and EPCC SAFE","  In this paper, we describe how we have used a combination of the LASSi tool
(developed by Cray) and the SAFE software (developed by EPCC) to collect and
analyse Lustre I/O performance data for all jobs running on the UK national
supercomputing service, ARCHER; and to provide reports on I/O usage for users
in our standard reporting framework. We also present results from analysis of
parallel I/O use on ARCHER and analysis on the potential impact of different
applications on file system performance using metrics we have derived from the
LASSi data. We show that the performance data from LASSi reveals how the same
application can stress different components of the file system depending on how
it is run, and how the LASSi risk metrics allow us to identify use cases that
could potentially cause issues for global I/O performance and work with users
to improve their I/O use. We use the IO-500 benchmark to help us understand how
LASSi risk metrics correspond to observed performance on the ARCHER file
systems. We also use LASSi data imported into SAFE to identify I/O use patterns
associated with different research areas, understand how the research workflow
gives rise to the observed patterns and project how this will affect I/O
requirements in the future. Finally, we provide an overview of likely future
directions for the continuation of this work.
"
1733,"Performance Analysis and Characterization of Training Deep Learning
  Models on Mobile Devices","  Training deep learning models on mobile devices recently becomes possible,
because of increasing computation power on mobile hardware and the advantages
of enabling high user experiences. Most of the existing work on machine
learning at mobile devices is focused on the inference of deep learning models
(particularly convolutional neural network and recurrent neural network), but
not training. The performance characterization of training deep learning models
on mobile devices is largely unexplored, although understanding the performance
characterization is critical for designing and implementing deep learning
models on mobile devices.
  In this paper, we perform a variety of experiments on a representative mobile
device (the NVIDIA TX2) to study the performance of training deep learning
models. We introduce a benchmark suite and tools to study performance of
training deep learning models on mobile devices, from the perspectives of
memory consumption, hardware utilization, and power consumption. The tools can
correlate performance results with fine-grained operations in deep learning
models, providing capabilities to capture performance variance and problems at
a fine granularity. We reveal interesting performance problems and
opportunities, including under-utilization of heterogeneous hardware, large
energy consumption of the memory, and high predictability of workload
characterization. Based on the performance analysis, we suggest interesting
research directions.
"
1734,"Towards Run Time Estimation of the Gaussian Chemistry Code for SEAGrid
  Science Gateway","  Accurate estimation of the run time of computational codes has a number of
significant advantages for scientific computing. It is required information for
optimal resource allocation, improving turnaround times and utilization of
science gateways. Furthermore, it allows users to better plan and schedule
their research, streamlining workflows and improving the overall productivity
of cyberinfrastructure. Predicting run time is challenging, however. The inputs
to scientific codes can be complex and high dimensional. Their relationship to
the run time may be highly non-linear, and, in the most general case is
completely arbitrary and thus unpredictable (i.e., simply a random mapping from
inputs to run time). Most codes are not so arbitrary, however, and there has
been significant prior research on predicting the run time of applications and
workloads. Such predictions are generally application-specific, however. In
this paper, we focus on the Gaussian computational chemistry code. We
characterize a data set of runs from the SEAGrid science gateway with a number
of different studies. We also explore a number of different potential
regression methods and present promising future directions.
"
1735,"ROOT I/O compression algorithms and their performance impact within Run
  3","  The LHCs Run3 will push the envelope on data-intensive workflows and, since
at the lowest level this data is managed using the ROOT software framework,
preparations for managing this data are starting already. At the beginning of
LHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then,
ROOT has added support for additional algorithms such as LZMA and LZ4, each
with unique strengths. This work must continue as industry introduces new
techniques - ROOT can benefit saving disk space or reducing the I/O and
bandwidth for online and offline needs of experiments by introducing better
compression algorithms. In addition to alternate algorithms, we have been
exploring alternate techniques to improve parallelism and apply
pre-conditioners to the serialized data.
  We have performed a survey of the performance of the new compression
techniques. Our survey includes various use cases of data compression of ROOT
files provided by different LHC experiments. We also provide insight into
solutions applied to resolve bottlenecks in compression algorithms, resulting
in improved ROOT performance.
"
1736,Power Gradient Descent,"  The development of machine learning is promoting the search for fast and
stable minimization algorithms. To this end, we suggest a change in the current
gradient descent methods that should speed up the motion in flat regions and
slow it down in steep directions of the function to minimize. It is based on a
""power gradient"", in which each component of the gradient is replaced by its
versus-preserving $H$-th power, with $0<H<1$. We test three modern gradient
descent methods fed by such variant and by standard gradients, finding the new
version to achieve significantly better performances for the Nesterov
accelerated gradient and AMSGrad. We also propose an effective new take on the
ADAM algorithm, which includes power gradients with varying $H$.
"
1737,Markovian model for Broadcast in Wireless Body Area Networks,"  Wireless body area networks became recently a vast field of investigation. A
large amount of research in this field is dedicated to the evaluation of
various communication protocols, e.g., broadcast or convergecast, against human
body mobility. Most of the time this evaluation is done via simulations and in
many situations only synthetic data is used for the human body mobility. In
this paper we propose for the first time in Wireless Body Area Networks a
Markovian analytical model specifically designed for WBAN networks. The main
objective of the model is to evaluate the efficiency of a multi-hop
transmission in the case of a diffusion-based broadcast protocol, with respect
to various performance parameters (e.g., cover probability, average cover
number, hitting probability or average cover time). We validate our model by
comparing its results to simulation and show its accuracy. Finally, but not
least, we show how our model can be used to analytically evaluate the trade-off
between transmission power and redundancy, when the same message is broadcasted
several times in order to increase the broadcast reliability while maintaining
a low transmission power.
"
1738,"Optimizing Redundancy Levels in Master-Worker Compute Clusters for
  Straggler Mitigation","  Runtime variability in computing systems causes some tasks to straggle and
take much longer than expected to complete. These straggler tasks are known to
significantly slowdown distributed computation. Job execution with speculative
execution of redundant tasks has been the most widely deployed technique for
mitigating the impact of stragglers, and many recent theoretical papers have
studied the advantages and disadvantages of using redundancy under various
system and service models. However, no clear guidelines could yet be found on
when, for which jobs, and how much redundancy should be employed in
Master-Worker compute clusters, which is the most widely adopted architecture
in modern compute systems. We are concerned with finding a strategy for
scheduling jobs with redundancy that works well in practice. This is a complex
optimization problem, which we address in stages. We first use Reinforcement
Learning (RL) techniques to learn good scheduling principles from realistic
experience. Building on these principles, we derive a simple scheduling policy
and present an approximate analysis of its performance. Specifically, we derive
expressions to decide when and which jobs should be scheduled with how much
redundancy. We show that policy that we devise in this way performs as good as
the more complex policies that are derived by RL. Finally, we extend our
approximate analysis to the case when system employs the other widely deployed
remedy for stragglers, which is relaunching straggler tasks after waiting some
time. We show that scheduling with redundancy significantly outperforms
straggler relaunch policy when the offered load on the system is low or
moderate, and performs slightly worse when the offered load is very high.
"
1739,A JIT Compiler for Neural Network Inference,"  This paper describes a C++ library that compiles neural network models at
runtime into machine code that performs inference. This approach in general
promises to achieve the best performance possible since it is able to integrate
statically known properties of the network directly into the code. In our
experiments on the NAO V6 platform, it outperforms existing implementations
significantly on small networks, while being inferior on large networks. The
library was already part of the B-Human code release 2018, but has been
extended since and is now available as a standalone version that can be
integrated into any C++14 code base.
"
1740,"Diffusing Your Mobile Apps: Extending In-Network Function Virtualization
  to Mobile Function Offloading","  Motivated by the huge disparity between the limited battery capacity of user
devices and the ever-growing energy demands of modern mobile apps, we propose
INFv. It is the first offloading system able to cache, migrate and dynamically
execute on demand functionality from mobile devices in ISP networks. It aims to
bridge this gap by extending the promising NFV paradigm to mobile applications
in order to exploit in-network resources. In this paper, we present the overall
design, state-of-the-art technologies adopted, and various engineering details
in the INFv system. We also carefully study the deployment configurations by
investigating over 20K Google Play apps, as well as thorough evaluations with
realistic settings. In addition to a significant improvement in battery life
(up to 6.9x energy reduction) and execution time (up to 4x faster), INFv has
two distinct advantages over previous systems: 1) a non-intrusive offloading
mechanism transparent to existing apps; 2) an inherent framework support to
effectively balance computation load and exploit the proximity of in-network
resources. Both advantages together enable a scalable and incremental
deployment of computation offloading framework in practical ISPs' networks.
"
1741,"Monotonically relaxing concurrent data-structure semantics for
  performance: An efficient 2D design framework","  There has been a significant amount of work in the literature proposing
semantic relaxation of concurrent data structures for improving scalability and
performance. By relaxing the semantics of a data structure, a bigger design
space, that allows weaker synchronization and more useful parallelism, is
unveiled. Investigating new data structure designs, capable of trading
semantics for achieving better performance in a monotonic way, is a major
challenge in the area. We algorithmically address this challenge in this paper.
We present an efficient, lock-free, concurrent data structure design framework
for out-of-order semantic relaxation. Our framework introduces a new two
dimensional algorithmic design, that uses multiple instances of a given data
structure. The first dimension of our design is the number of data structure
instances operations are spread to, in order to benefit from parallelism
through disjoint memory access. The second dimension is the number of
consecutive operations that try to use the same data structure instance in
order to benefit from data locality. Our design can flexibly explore this
two-dimensional space to achieve the property of monotonically relaxing
concurrent data structure semantics for achieving better throughput performance
within a tight deterministic relaxation bound, as we prove in the paper. We
show how our framework can instantiate lock-free out-of-order queues, stacks,
counters and dequeues. We provide implementations of these relaxed data
structures and evaluate their performance and behaviour on two parallel
architectures. Experimental evaluation shows that our two-dimensional data
structures significantly outperform the respected previous proposed ones with
respect to scalability and throughput performance. Moreover, their throughput
increases monotonically as relaxation increases.
"
1742,"MultiCloud Resource Management using Apache Mesos for Planned
  Integration with Apache Airavata","  We discuss initial results and our planned approach for incorporating Apache
Mesos based resource management that will enable design and development of
scheduling strategies for Apache Airavata jobs so that they can be launched on
multiple clouds, wherein several VMs do not have Public IP addresses. We
present initial work and next steps on the design of a meta-scheduler using
Apache Mesos. Apache Mesos presents a unified view of resources available
across several clouds and clusters. Our meta-scheduler can potentially examine
and identify the cases where multiple small jobs have been submitted by the
same scientists and then redirect job from the same community account or user
to different clusters. Our approach uses a NAT firewall to make nodes/VMs,
without a Public IP, visible to Mesos for the unified view.
"
1743,"Collecting and Presenting Reproducible Intranode Stencil Performance:
  INSPECT","  Stencil algorithms have been receiving considerable interest in HPC research
for decades. The techniques used to approach multi-core stencil performance
modeling and engineering span basic runtime measurements, elaborate performance
models, detailed hardware counter analysis, and thorough scaling behavior
evaluation. Due to the plurality of approaches and stencil patterns, we set out
to develop a generalizable methodology for reproducible measurements
accompanied by state-of-the-art performance models. Our open-source toolchain,
and collected results are publicly available in the ""Intranode Stencil
Performance Evaluation Collection"" (INSPECT). We present the underlying
methodologies, models and tools involved in gathering and documenting the
performance behavior of a collection of typical stencil patterns across
multiple architectures and hardware configuration options. Our aim is to endow
performance-aware application developers with reproducible baseline performance
data and validated models to initiate a well-defined process of performance
assessment and optimization.
"
1744,"Using Machine Learning to Optimize Web Interactions on Heterogeneous
  Mobile Multi-cores","  The web has become a ubiquitous application development platform for mobile
systems. Yet, web access on mobile devices remains an energy-hungry activity.
Prior work in the field mainly focuses on the initial page loading stage, but
fails to exploit the opportunities for energy-efficiency optimization while the
user is interacting with a loaded page. This paper presents a novel approach
for performing energy optimization for interactive mobile web browsing. At the
heart of our approach is a set of machine learning models, which estimate
\emph{at runtime} the frames per second for a given user interaction input by
running the computation-intensive web render engine on a specific processor
core under a given clock speed. We use the learned predictive models as a
utility function to quickly search for the optimal processor setting to
carefully trade responsive time for reduced energy consumption. We integrate
our techniques to the open-source Chromium browser and apply it to two
representative mobile user events: scrolling and pinching (i.e., zoom in and
out). We evaluate the developed system on the landing pages of the top-100
hottest websites and two big.LITTLE heterogeneous mobile platforms. Our
extensive experiments show that the proposed approach reduces the system-wide
energy consumption by over 36\% on average and up to 70\%. This translates to
an over 17\% improvement on energy-efficiency over a state-of-the-art
event-based web browser scheduler, but with significantly fewer violations on
the quality of service.
"
1745,Toward a Standard Interface for User-Defined Scheduling in OpenMP,"  Parallel loops are an important part of OpenMP programs. Efficient scheduling
of parallel loops can improve performance of the programs. The current OpenMP
specification only offers three options for loop scheduling, which are
insufficient in certain instances. Given the large number of other possible
scheduling strategies, it is infeasible to standardize each one. A more viable
approach is to extend the OpenMP standard to allow for users to define loop
scheduling strategies. The approach will enable standard-compliant
application-specific scheduling. This work analyzes the principal components
required by user-defined scheduling and proposes two competing interfaces as
candidates for the OpenMP standard. We conceptually compare the two proposed
interfaces with respect to the three host languages of OpenMP, i.e., C, C++,
and Fortran. These interfaces serve the OpenMP community as a basis for
discussion and prototype implementation for user-defined scheduling.
"
1746,"A Beaconless Asymmetric Energy-Efficient Time Synchronization Scheme for
  Resource-Constrained Multi-Hop Wireless Sensor Networks","  The ever-increasing number of WSN deployments based on a large number of
battery-powered, low-cost sensor nodes, which are limited in their computing
and power resources, puts the focus of WSN time synchronization research on
three major aspects, i.e., accuracy, energy consumption and computational
complexity. In the literature, the latter two aspects have not received much
attention compared to the accuracy of WSN time synchronization. Especially in
multi-hop WSNs, intermediate gateway nodes are overloaded with tasks for not
only relaying messages but also a variety of computations for their offspring
nodes as well as themselves. Therefore, not only minimizing the energy
consumption but also lowering the computational complexity while maintaining
the synchronization accuracy is crucial to the design of time synchronization
schemes for resource-constrained sensor nodes. In this paper, focusing on the
three aspects of WSN time synchronization, we introduce a framework of reverse
asymmetric time synchronization for resource-constrained multi-hop WSNs and
propose a beaconless energy-efficient time synchronization scheme based on
reverse one-way message dissemination. Experimental results with a WSN testbed
based on TelosB motes running TinyOS demonstrate that the proposed scheme
conserves up to 95% energy consumption compared to the flooding time
synchronization protocol while achieving microsecond-level synchronization
accuracy.
"
1747,"Optimal Message Bundling with Delay and Synchronization Constraints in
  Wireless Sensor Networks","  Message bundling is an effective way to reduce the energy consumption for
message transmissions in wireless sensor networks. However, bundling more
messages could increase both end-to-end delay and message transmission
interval; the former needs to be maintained within a certain value for
time-sensitive applications like environmental monitoring, while the latter
affects time synchronization accuracy when the bundling includes
synchronization messages as well. Taking as an example a novel time
synchronization scheme recently proposed for energy efficiency, we propose an
optimal message bundling approach to reduce the message transmissions while
maintaining the user-defined requirements on end-to-end delay and time
synchronization accuracy. Through translating the objective of joint
maintenance to an integer linear programming problem, we compute a set of
optimal bundling numbers for the sensor nodes to constrain their link-level
delays, thereby achieve and maintain the required end-to-end delay and
synchronization accuracy while the message transmission is minimized.
"
1748,"On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light
  Communications","  In this paper, we investigate the physical-layer security for a spatial
modulation (SM) based indoor visible light communication (VLC) system, which
includes multiple transmitters, a legitimate receiver, and a passive
eavesdropper (Eve). At the transmitters, the SM scheme is employed, i.e., only
one transmitter is active at each time instant. To choose the active
transmitter, a uniform selection (US) scheme is utilized. Two scenarios are
considered: one is with non-negativity and average optical intensity
constraints, the other is with non-negativity, average optical intensity and
peak optical intensity constraints. Then, lower and upper bounds on the secrecy
rate are derived for these two scenarios. Besides, the asymptotic behaviors for
the derived secrecy rate bounds at high signal-to-noise ratio (SNR) are
analyzed. To further improve the secrecy performance, a channel adaptive
selection (CAS) scheme and a greedy selection (GS) scheme are proposed to
select the active transmitter. Numerical results show that the lower and upper
bounds of the secrecy rate are tight. At high SNR, small asymptotic performance
gaps exist between the derived lower and upper bounds. Moreover, the proposed
GS scheme has the best performance, followed by the CAS scheme and the US
scheme.
"
1749,Retrial Queueing Models: A Survey on Theory and Applications,"  Retrial phenomenon naturally arises in various systems such as call centers,
cellular networks and random access protocols in local area networks. This
paper gives a comprehensive survey on theory and applications of retrial queues
in these systems. We investigate the state of the art of the theoretical
researches including exact solutions, stability, asymptotic analyses and
multidimensional models. We present an overview on retrial models arising from
real world applications. Some open problems and promising research directions
are also discussed.
"
1750,On The Performance of ARM TrustZone,"  The TrustZone technology, available in the vast majority of recent ARM
processors, allows the execution of code inside a so-called secure world. It
effectively provides hardware-isolated areas of the processor for sensitive
data and code, i.e., a trusted execution environment (TEE). The OP-TEE
framework provides a collection of toolchain, open-source libraries and secure
kernel specifically geared to develop applications for TrustZone. This paper
presents an in-depth performance- and energy-wise study of TrustZone using the
OP-TEE framework, including secure storage and the cost of switching between
secure and unsecure worlds, using emulated and hardware measurements.
"
1751,Platform Independent Software Analysis for Near Memory Computing,"  Near-memory Computing (NMC) promises improved performance for the
applications that can exploit the features of emerging memory technologies such
as 3D-stacked memory. However, it is not trivial to find such applications and
specialized tools are needed to identify them. In this paper, we present
PISA-NMC, which extends a state-of-the-art hardware agnostic profiling tool
with metrics concerning memory and parallelism, which are relevant for NMC. The
metrics include memory entropy, spatial locality, data-level, and
basic-block-level parallelism. By profiling a set of representative
applications and correlating the metrics with the application's performance on
a simulated NMC system, we verify the importance of those metrics. Finally, we
demonstrate which metrics are useful in identifying applications suitable for
NMC architectures.
"
1752,"EasyCrash: Exploring Non-Volatility of Non-Volatile Memory for High
  Performance Computing Under Failures","  Emerging non-volatile memory (NVM) is promising for building future HPC.
Leveraging the non-volatility of NVM as main memory, we can restart the
application using data objects remaining on NVM when the application crashes.
This paper explores this solution to handle HPC under failures, based on the
observation that many HPC applications have good enough intrinsic fault
tolerance. To improve the possibility of successful recomputation with correct
outcomes and ignorable performance loss, we introduce EasyCrash, a framework to
decide how to selectively persist application data objects during application
execution. Our evaluation shows that EasyCrash transforms 54% of crashes that
cannot correctly recompute into the correct computation while incurring a
negligible performance overhead (1.5% on average). Using EasyCrash and
application intrinsic fault tolerance, 82% of crashes can successfully
recompute. When EasyCrash is used with a traditional checkpoint scheme, it
enables up to 24% improvement (15% on average) in system efficiency.
"
1753,ALTIS: Modernizing GPGPU Benchmarking,"  This paper presents Altis, a benchmark suite for modern GPGPU computing.
Previous benchmark suites such as Rodinia and SHOC have served the research
community well, but were developed years ago when hardware was more limited,
software supported fewer features, and production hardware-accelerated
workloads were scarce. Since that time, GPU compute density and memory capacity
has grown exponentially, programmability features such as unified memory,
demand paging, and HyperQ have matured, and new workloads such as deep neural
networks (DNNs), graph analytics, and crypto-currencies have emerged in
production environments, stressing the hardware and software in ways that
previous benchmarks did not anticipate. Drawing inspiration from Rodinia and
SHOC, Altis is a benchmark suite designed for modern GPU architectures and
modern GPU runtimes, representing a diverse set of application domains. By
adopting and extending applications from Rodinia and SHOC, adding new
applications, and focusing on CUDA platforms, Altis better represents modern
GPGPU workloads to enable support GPGPU research in both architecture and
system software.
"
1754,Fast Data: Moving beyond from Big Data's map-reduce,"  Big Data may not be the solution many are looking for. The latest rise of Big
Data methods and systems is partly due to the new abilities these techniques
provide, partly to the simplicity of the software design and partly because the
buzzword itself has value to investors and clients. That said, popularity is
not a measure for suitability and the Big Data approach might not be the best
solution, or even an applicable one, to many common problems. Namely, time
dependent problems whose solution may be bound or cached in any manner can
benefit greatly from moving to partly stateless, flow oriented functions and
data models. This paper presents such a model to substitute the traditional
map-shuffle-reduce models.
"
1755,Straggler Mitigation at Scale,"  Runtime performance variability at the servers has been a major issue,
hindering the predictable and scalable performance in modern distributed
systems. Executing requests or jobs redundantly over multiple servers has been
shown to be effective for mitigating variability, both in theory and practice.
Systems that employ redundancy has drawn significant attention, and numerous
papers have analyzed the pain and gain of redundancy under various service
models and assumptions on the runtime variability. This paper presents a cost
(pain) vs. latency (gain) analysis of executing jobs of many tasks by employing
replicated or erasure coded redundancy. Tail heaviness of service time
variability is decisive on the pain and gain of redundancy and we quantify its
effect by deriving expressions for the cost and latency. Specifically, we try
to answer four questions: 1) How do replicated and coded redundancy compare in
the cost vs. latency tradeoff? 2) Can we introduce redundancy after waiting
some time and expect to reduce the cost? 3) Can relaunching the tasks that
appear to be straggling after some time help to reduce cost and/or latency? 4)
Is it effective to use redundancy and relaunching together? We validate the
answers we found for each of the questions via simulations that use empirical
distributions extracted from a Google cluster data.
"
1756,Security Rating Metrics for Distributed Wireless Systems,"  The paper examines quantitative assessment of wireless distribution system
security, as well as an assessment of risks from attacks and security
violations. Furthermore, it describes typical security breach and formal attack
models and five methods for assessing security. The proposed normalized method
for assessing the degree of security assurance operates with at least three
characteristics, which allows comparatively analyze heterogeneous information
systems. The improved calculating formulas have been proposed for two security
assessment methods, and the elements of functional-cost analysis have been
applied to calculate the degree of security. To check the results of the
analysis, the coefficient of concordance was calculated, which gives
opportunity to determine the quality of expert assessment. The simultaneous use
of several models to describe attacks and the effectiveness of countering them
allows us to create a comprehensive approach to countering modern security
threats to information networks at the commercial enterprises and critical
infrastructure facilities.
"
1757,Q-Learning Inspired Self-Tuning for Energy Efficiency in HPC,"  System self-tuning is a crucial task to lower the energy consumption of
computers. Traditional approaches decrease the processor frequency in idle or
synchronisation periods. However, in High-Performance Computing (HPC) this is
not sufficient: if the executed code is load balanced, there are neither idle
nor synchronisation phases that can be exploited. Therefore, alternative
self-tuning approaches are needed, which allow exploiting different compute
characteristics of HPC programs.
  The novel notion of application regions based on function call stacks,
introduced in the Horizon 2020 Project READEX, allows us to define such a
self-tuning approach. In this paper, we combine these regions with the
Q-Learning typical state-action maps, which save information about available
states, possible actions to take, and the expected rewards. By exploiting the
existing processor power interface, we are able to provide direct feedback to
the learning process. This approach allows us to save up to 15% energy, while
only adding a minor runtime overhead.
"
1758,FPGA-based Multi-Chip Module for High-Performance Computing,"  Current integration, architectural design and manufacturing technologies are
not suited for the computing density and power efficiency requested by Exascale
computing. New approaches in hardware architecture are thus needed to overcome
the technological barriers preventing the transition to the Exascale era. In
that scope, we report successful fabrication of first ExaNoDe's MCM prototypes
dedicated to Exascale computing applications. Each MCM was composed of 2 Xilinx
Zynq Ultrascale+ MPSoC, assembled on advanced 68.5 mm x 55 mm laminate
substrates specifically designed and fabricated for the project. Acoustic
microscopy, x-ray, cross-section and Thermo-Moire investigations revealed no
voids, shorts, delamination, cracks or warpage issues. Two MCMs were mounted on
a daughter board by FORTH for testing purposes. The DDR memories on the 4
SODIMMs of the daughter board were successfully tested by running extensive
Xilinx memory tests with clock frequencies of 1866 MHz and 2133 MHz. All 4
FPGAs were programmed with the Xilinx integrated bit error ratio test (IBERT)
tailored for this board for links testing. All intra-board high-speed links
between all FPGAs were stable at 10 Gbps, even under the more demanding 31-bit
PRBS (Pseudorandom Binary Sequence) tests.
"
1759,Stress-SGX: Load and Stress your Enclaves for Fun and Profit,"  The latest generation of Intel processors supports Software Guard Extensions
(SGX), a set of instructions that implements a Trusted Execution Environment
(TEE) right inside the CPU, by means of so-called enclaves. This paper presents
Stress-SGX, an easy-to-use stress-test tool to evaluate the performance of
SGX-enabled nodes. We build on top of the popular Stress-NG tool, while only
keeping the workload injectors (stressors) that are meaningful in the SGX
context. We report on several insights and lessons learned about porting legacy
code to run inside an SGX enclave, as well as the limitations introduced by
this process. Finally, we use Stress-SGX to conduct a study comparing the
performance of different SGX-enabled machines.
"
1760,"One Size Does Not Fit All: Quantifying and Exposing the Accuracy-Latency
  Trade-off in Machine Learning Cloud Service APIs via Tolerance Tiers","  Today's cloud service architectures follow a ""one size fits all"" deployment
strategy where the same service version instantiation is provided to the end
users. However, consumers are broad and different applications have different
accuracy and responsiveness requirements, which as we demonstrate renders the
""one size fits all"" approach inefficient in practice. We use a production-grade
speech recognition engine, which serves several thousands of users, and an open
source computer vision based system, to explain our point. To overcome the
limitations of the ""one size fits all"" approach, we recommend Tolerance Tiers
where each MLaaS tier exposes an accuracy/responsiveness characteristic, and
consumers can programmatically select a tier. We evaluate our proposal on the
CPU-based automatic speech recognition (ASR) engine and cutting-edge neural
networks for image classification deployed on both CPUs and GPUs. The results
show that our proposed approach provides an MLaaS cloud service architecture
that can be tuned by the end API user or consumer to outperform the
conventional ""one size fits all"" approach.
"
1761,Pinpointing Performance Inefficiencies in Java,"  Many performance inefficiencies such as inappropriate choice of algorithms or
data structures, developers' inattention to performance, and missed compiler
optimizations show up as wasteful memory operations. Wasteful memory operations
are those that produce/consume data to/from memory that may have been avoided.
We present, JXPerf, a lightweight performance analysis tool for pinpointing
wasteful memory operations in Java programs. Traditional byte-code
instrumentation for such analysis (1) introduces prohibitive overheads and (2)
misses inefficiencies in machine code generation. JXPerf overcomes both of
these problems. JXPerf uses hardware performance monitoring units to sample
memory locations accessed by a program and uses hardware debug registers to
monitor subsequent accesses to the same memory. The result is a lightweight
measurement at machine-code level with attribution of inefficiencies to their
provenance: machine and source code within full calling contexts. JXPerf
introduces only 7% runtime overhead and 7% memory overhead making it useful in
production. Guided by JXPerf, we optimize several Java applications by
improving code generation and choosing superior data structures and algorithms,
which yield significant speedups.
"
1762,"Bridging the Architecture Gap: Abstracting Performance-Relevant
  Properties of Modern Server Processors","  We describe a universal modeling approach for predicting single- and
multicore runtime of steady-state loops on server processors. To this end we
strictly differentiate between application and machine models: An application
model comprises the loop code, problem sizes, and other runtime parameters,
while a machine model is an abstraction of all performance-relevant properties
of a CPU. We introduce a generic method for determining machine models and
present results for relevant server-processor architectures by Intel, AMD, IBM,
and Marvell/Cavium. Considering this wide range of architectures, the set of
features required for adequate performance modeling is surprisingly small. To
validate our approach, we compare performance predictions to empirical data for
an OpenMP-parallel preconditioned CG algorithm, which includes compute- and
memory-bound kernels. Both single- and multicore analysis shows that the model
exhibits average and maximum relative errors of 5% and 10%. Deviations from the
model and insights gained are discussed in detail.
"
1763,State-of-the-Art on Query & Transaction Processing Acceleration,"  The vast amount of processing power and memory bandwidth provided by modern
Graphics Processing Units (GPUs) make them a platform for data-intensive
applications. The database community identified GPUs as effective co-processors
for data processing. In the past years, there were many approaches to make use
of GPUs at different levels of a database system. In this Internal Technical
Report, based on the [1] and some other research papers, we identify possible
research areas at LIP6 for GPU-accelerated database management systems. We
describe some key properties, typical challenges of GPU-aware database
architectures, and identify major open challenges.
"
1764,Open-MPI over MOSIX: paralleled computing in a clustered world,"  Recent increased interest in Cloud computing emphasizes the need to find an
adequate solution to the load-balancing problem in parallel computing --
efficiently running several jobs concurrently on a cluster of shared computers
(nodes). One approach to solve this problem is by preemptive process migration
-- the transfer of running processes between nodes. A possible drawback of this
approach is the increased overhead between heavily communicating processes.
This project presents a solution to this last problem by incorporating the
process migration capability of MOSIX into Open-MPI and by reducing the
resulting communication overhead. Specifically, we developed a module for
direct communication (DiCOM) between migrated Open-MPI processes, to overcome
the increased communication latency of TCP/IP between such processes. The
outcome is reduced run-time by improved resource allocation.
"
1765,"Exploiting Acceleration Features of LabVIEW platform for Real-Time GNSS
  Software Receiver Optimization","  This paper presents the new generation of LabVIEW-based GPS receiver testbed
that is based on National Instruments' (NI) LabVIEW (LV) platform in
conjunction to C/C++ dynamic link libraries (DLL) used inside the platform for
performance execution. This GPS receiver has been optimized for real-time
operation and has been developed for fast prototyping and easiness on future
additions and implementations to the system. The receiver DLLs are divided into
three baseband modules: acquisition, tracking, and navigation. The openness of
received baseband modules allows for extensive research topics such as signal
quality improvement on GPS-denied areas, signal spoofing, and signal
interferences. The hardware used in the system was chosen with an effort to
achieve portability and mobility in the SDR receiver. Several acceleration
factors that accomplish real-time operation and that are inherent to LabVIEW
mechanisms, such as multithreading, parallelization and dedicated
loop-structures, are discussed. The proposed SDR also exploits C/C++
optimization techniques for single-instruction multiple-data (SIMD) capable
processors in software correlators for real-time operation of GNSS tracking
loops. It is demonstrated that LabVIEW-based solutions provide competitive
real-time solutions for fast prototyping of receiver algorithms.
"
1766,"Fast prototyping of an SDR WLAN 802.11b receiver for an indoor
  positioning system","  Indoor positioning systems (IPS) are emerging technologies due to an
increasing popularity and demand in location based service (LBS). Because
traditional positioning systems such as GPS are limited to outdoor
applications, many IPS have been proposed in literature. WLAN-based IPS are the
most promising due to its proven accuracy and infrastructure deployment.
Several WLAN-based IPS have been proposed in the past, from which the best
results have been shown by so-called fingerprint-based systems. This paper
proposes an indoor positioning system which extends traditional WLAN
fingerprinting by using received signal strength (RSS) measurements along with
channel estimates as an effort to improve classification accuracy for scenarios
with a low number of Access Points (APs). The channel estimates aim to
characterize complex indoor environments making it a unique signature for
fingerprinting-based IPS and therefore improving pattern recognition in
radio-maps. Since commercial WLAN cards offer limited measurement information,
software-defined radio (SDR) as an emerging trend for fast prototyping and
research integration is chosen as the best cost-effective option to extract
channel estimates. Therefore, this paper first proposes an 802.11b WLAN SDR
beacon receiver capable of measuring RSS and channel estimates. The SDR is
designed using LabVIEW (LV) environment and leverages several inherent platform
acceleration features that achieve real-time capturing. The receiver achieves a
fast-rate measurement capture of 9 packets per second per AP. The
classification of the propose IPS uses a support vector machine (SVM) for
offline training and online navigation. Several tests are conducted in a
cluttered indoor environment with a single AP in 802.11b legacy mode. Finally,
navigation accuracy results are discussed.
"
1767,"A Fast-rate WLAN Measurement Tool for Improved Miss-rate in Indoor
  Navigation","  Recently, location-based services (LBS) have steered attention to indoor
positioning systems (IPS). WLAN-based IPSs relying on received signal strength
(RSS) measurements such as fingerprinting are gaining popularity due to proven
high accuracy of their results. Typically, sets of RSS measurements at selected
locations from several WLAN access points (APs) are used to calibrate the
system. Retrieval of such measurements from WLAN cards are commonly at one-Hz
rate. Such measurement collection is needed for offline radio-map surveying
stage which aligns fingerprints to locations, and for online navigation stage,
when collected measurements are associated with the radio-map for user
navigation. As WLAN network is not originally designed for positioning, an RSS
measurement miss could have a high impact on the fingerprinting system.
Additionally, measurement fluctuations require laborious signal processing, and
surveying process can be very time consuming. This paper proposes a fast-rate
measurement collection method that addresses previously mentioned problems by
achieving a higher probability of RSS measurement collection during a given
one-second window. This translates to more data for statistical processing and
faster surveying. The fast-rate collection approach is analyzed against the
conventional measurement rate in a proposed testing methodology that mimics
real-life scenarios related to IPS surveying and online navigation.
"
1768,Accelerator-level Parallelism,"  Future applications demand more performance, but technology advances have
been faltering. A promising approach to further improve computer system
performance under energy constraints is to employ hardware accelerators.
Already today, mobile systems concurrently employ multiple accelerators in what
we call accelerator-level parallelism (ALP). To spread the benefits of ALP more
broadly, we charge computer scientists to develop the science needed to best
achieve the performance and cost goals of ALP hardware and software.
"
1769,CloudCoaster: Transient-aware Bursty Datacenter Workload Scheduling,"  Today's clusters often have to divide resources among a diverse set of jobs.
These jobs are heterogeneous both in execution time and in their rate of
arrival. Execution time heterogeneity has lead to the development of hybrid
schedulers that can schedule both short and long jobs to ensure good task
placement. However, arrival rate heterogeneity, or burstiness, remains a
problem in existing schedulers. These hybrid schedulers manage resources on
statically provisioned cluster, which can quickly be overwhelmed by bursts in
the number of arriving jobs.
  In this paper we propose CloudCoaster, a hybrid scheduler that dynamically
resizes the cluster by leveraging cheap transient servers. CloudCoaster
schedules jobs in an intelligent way that increases job performance while
reducing overall resource cost. We evaluate the effectiveness of CloudCoaster
through simulations on real-world traces and compare it against a state-of-art
hybrid scheduler. CloudCoaster improves the average queueing delay time of
short jobs by 4.8X while maintaining long job performance. In addition,
CloudCoaster reduces the short partition budget by over 29.5%.
"
1770,"Energy of Computing on Multicore CPUs: Predictive Models and Energy
  Conservation Law","  Energy is now a first-class design constraint along with performance in all
computing settings. Energy predictive modelling based on performance monitoring
counts (PMCs) is the leading method used for prediction of energy consumption
during an application execution. We use a model-theoretic approach to formulate
the assumed properties of existing models in a mathematical form. We extend the
formalism by adding properties, heretofore unconsidered, that account for a
limited form of energy conservation law. The extended formalism defines our
theory of energy of computing. By applying the basic practical implications of
the theory, we improve the prediction accuracy of state-of-the-art energy
models from 31% to 18%. We also demonstrate that use of state-of-the-art
measurement tools for energy optimisation may lead to significant losses of
energy (ranging from 56% to 65% for applications used in experiments) since
they do not take into account the energy conservation properties.
"
1771,Automatic Differentiation for Adjoint Stencil Loops,"  Stencil loops are a common motif in computations including convolutional
neural networks, structured-mesh solvers for partial differential equations,
and image processing. Stencil loops are easy to parallelise, and their fast
execution is aided by compilers, libraries, and domain-specific languages.
Reverse-mode automatic differentiation, also known as algorithmic
differentiation, autodiff, adjoint differentiation, or back-propagation, is
sometimes used to obtain gradients of programs that contain stencil loops.
Unfortunately, conventional automatic differentiation results in a memory
access pattern that is not stencil-like and not easily parallelisable.
  In this paper we present a novel combination of automatic differentiation and
loop transformations that preserves the structure and memory access pattern of
stencil loops, while computing fully consistent derivatives. The generated
loops can be parallelised and optimised for performance in the same way and
using the same tools as the original computation. We have implemented this new
technique in the Python tool PerforAD, which we release with this paper along
with test cases derived from seismic imaging and computational fluid dynamics
applications.
"
1772,RegDem: Increasing GPU Performance via Shared Memory Register Spilling,"  GPU utilization, measured as occupancy, is limited by the parallel threads'
combined usage of on-chip resources, such as registers and the
programmer-managed shared memory. Higher resource demand means lower effective
parallel thread count, and therefore lower program performance. Our
investigation found that registers are often the occupancy limiters.
  The de-facto nvcc compiler-based approach spills excessive registers to the
off-chip memory, ignoring the shared memory and leaving the on-chip resources
underutilized. To mitigate the register demand, this paper presents a binary
translation technique, called RegDem, that spills excessive registers to the
underutilized shared memory by transforming the GPU assembly code (SASS). Most
GPU programs do not fully use shared memory, thus allowing RegDem to use it for
register spilling. The higher occupancy achieved by RegDem outweighs the
slightly higher cost of accessing shared memory instead of placing data in
registers. The paper also presents a compile-time performance predictor that
models instructions stalls to choose the best version from a set of program
variants. Cumulatively, these techniques outperform the nvcc compiler with a 9%
geometric mean, the highest observed being 18%.
"
1773,Optimizing Xeon Phi for Interactive Data Analysis,"  The Intel Xeon Phi manycore processor is designed to provide high performance
matrix computations of the type often performed in data analysis. Common data
analysis environments include Matlab, GNU Octave, Julia, Python, and R.
Achieving optimal performance of matrix operations within data analysis
environments requires tuning the Xeon Phi OpenMP settings, process pinning, and
memory modes. This paper describes matrix multiplication performance results
for Matlab and GNU Octave over a variety of combinations of process counts and
OpenMP threads and Xeon Phi memory modes. These results indicate that using
KMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode
allows both Matlab and GNU Octave to achieve 66% of the practical peak
performance for process counts ranging from 1 to 64 and OpenMP threads ranging
from 1 to 64. These settings have resulted in generally improved performance
across a range of applications and has enabled our Xeon Phi system to deliver
significant results in a number of real-world applications.
"
1774,"Etalumis: Bringing Probabilistic Programming to Scientific Simulators at
  Scale","  Probabilistic programming languages (PPLs) are receiving widespread attention
for performing Bayesian inference in complex generative models. However,
applications to science remain limited because of the impracticability of
rewriting complex scientific simulators in a PPL, the computational cost of
inference, and the lack of scalable implementations. To address these, we
present a novel PPL framework that couples directly to existing scientific
simulators through a cross-platform probabilistic execution protocol and
provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference
compilation (IC) engines for tractable inference. To guide IC inference, we
perform distributed training of a dynamic 3DCNN--LSTM architecture with a
PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori
supercomputer with a global minibatch size of 128k: achieving a performance of
450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron
Collider (LHC) use-case with the C++ Sherpa simulator and achieve the
largest-scale posterior inference in a Turing-complete PPL.
"
1775,"Guidelines for benchmarking of optimization approaches for fitting
  mathematical models","  Insufficient performance of optimization approaches for fitting of
mathematical models is still a major bottleneck in systems biology. In this
manuscript, the reasons and methodological challenges are summarized as well as
their impact in benchmark studies. Important aspects for increasing evidence of
outcomes of benchmark analyses are discussed. Based on general guidelines for
benchmarking in computational biology, a collection of tailored guidelines is
presented for performing informative and unbiased benchmarking of
optimization-based fitting approaches. Comprehensive benchmark studies based on
these recommendations are urgently required for establishing of a robust and
reliable methodology for the systems biology community.
"
1776,"Benchmarking Contemporary Deep Learning Hardware and Frameworks:A Survey
  of Qualitative Metrics","  This paper surveys benchmarking principles, machine learning devices
including GPUs, FPGAs, and ASICs, and deep learning software frameworks. It
also reviews these technologies with respect to benchmarking from the
perspectives of a 6-metric approach to frameworks and an 11-metric approach to
hardware platforms. Because MLPerf is a benchmark organization working with
industry and academia, and offering deep learning benchmarks that evaluate
training and inference on deep learning hardware devices, the survey also
mentions MLPerf benchmark results, benchmark metrics, datasets, deep learning
frameworks and algorithms. We summarize seven benchmarking principles,
differential characteristics of mainstream AI devices, and qualitative
comparison of deep learning hardware and frameworks.
"
1777,Metamorphic IOTA,"  IOTA opened recently a new line of research in distributed ledgers area by
targeting algorithms that ensure a high throughput for the transactions
generated in IoT systems. Transactions are continuously appended to an acyclic
structure called tangle and each new transaction selects as parents two
existing transactions (called tips) that it approves. G-IOTA, a very recent
improvement of IOTA, targets to protect tips left behind offering hence a good
confidence level. However, this improvement had a cost: the use of an
additional tip selection mechanism which may be critical in IoT systems since
it needs additional energy consumption. In this paper we propose a new
metamorphic algorithm for tip selection that offers the best guaranties of both
IOTA and G-IOTA. Our contribution is two fold. First, we propose a
parameterized algorithm, E-IOTA, for tip selection which targets to reduce the
number of random walks executed in previous versions (IOTA and G-IOTA) while
maintaining the same security guaranties as IOTA and the same confidence level
and fairness with respect to tips selection as G-IOTA. Then we propose a formal
analysis of the security guaranties offered by E-IOTA against various attacks
mentioned in the original IOTA proposal (e.g. large weight attack, parasite
chain attack and splitting attack). Interestingly, to the best of our knowledge
this is the first formal analysis of the security guaranties of IOTA and its
derivatives.
"
1778,"Barriers towards no-reference metrics application to compressed video
  quality analysis: on the example of no-reference metric NIQE","  This paper analyses the application of no-reference metric NIQE to the task
of video-codec comparison. A number of issues in the metric behaviour on videos
was detected and described. The metric has outlying scores on black and
solid-coloured frames. The proposed averaging technique for metric quality
scores helped to improve the results in some cases. Also, NIQE has low-quality
scores for videos with detailed textures and higher scores for videos of lower
bitrates due to the blurring of these textures after compression. Although NIQE
showed natural results for many tested videos, it is not universal and
currently can not be used for video-codec comparisons.
"
1779,"Bi-objective Optimisation of Data-parallel Applications on Heterogeneous
  Platforms for Performance and Energy via Workload Distribution","  Performance and energy are the two most important objectives for optimisation
on modern parallel platforms. Latest research demonstrated the importance of
workload distribution as a decision variable in the bi-objective optimisation
for performance and energy on homogeneous multicore clusters. We show in this
work that bi-objective optimisation for performance and energy on heterogeneous
processors results in a large number of Pareto-optimal optimal solutions
(workload distributions) even in the simple case of linear performance and
energy profiles. We then study performance and energy profiles of real-life
data-parallel applications and find that their shapes are non-linear, complex
and non-smooth. We, therefore, propose an efficient and exact global
optimisation algorithm, which takes as an input most general discrete
performance and dynamic energy profiles of the heterogeneous processors and
solves the bi-objective optimisation problem. The algorithm is also used as a
building block to solve the bi-objective optimisation problem for performance
and total energy. We also propose a novel methodology to build discrete dynamic
energy profiles of individual computing devices, which are input to the
algorithm. The methodology is based purely on system-level measurements and
addresses the fundamental challenge of accurate component-level energy
modelling of a hybrid data-parallel application running on a heterogeneous
platform integrating CPUs and accelerators. We experimentally validate the
proposed method using two data-parallel applications, matrix multiplication and
2D fast Fourier transform (2D-FFT).
"
1780,Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M,"  The Dynamic Distributed Dimensional Data Model (D4M) library implements
associative arrays in a variety of languages (Python, Julia, and Matlab/Octave)
and provides a lightweight in-memory database implementation of hypersparse
arrays that are ideal for analyzing many types of network data. D4M relies on
associative arrays which combine properties of spreadsheets, databases,
matrices, graphs, and networks, while providing rigorous mathematical
guarantees, such as linearity. Streaming updates of D4M associative arrays put
enormous pressure on the memory hierarchy. This work describes the design and
performance optimization of an implementation of hierarchical associative
arrays that reduces memory pressure and dramatically increases the update rate
into an associative array. The parameters of hierarchical associative arrays
rely on controlling the number of entries in each level in the hierarchy before
an update is cascaded. The parameters are easily tunable to achieve optimal
performance for a variety of applications. Hierarchical arrays achieve over
40,000 updates per second in a single instance. Scaling to 34,000 instances of
hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud
achieved a sustained update rate of 1,900,000,000 updates per second. This
capability allows the MIT SuperCloud to analyze extremely large streaming
network data sets.
"
1781,CHOP: Bypassing Runtime Bounds Checking Through Convex Hull OPtimization,"  Unsafe memory accesses in programs written using popular programming
languages like C/C++ have been among the leading causes for software
vulnerability. Prior memory safety checkers such as SoftBound enforce memory
spatial safety by checking if every access to array elements are within the
corresponding array bounds. However, it often results in high execution time
overhead due to the cost of executing the instructions associated with bounds
checking. To mitigate this problem, redundant bounds check elimination
techniques are needed. In this paper, we propose CHOP, a Convex Hull
OPtimization based framework, for bypassing redundant memory bounds checking
via profile-guided inferences. In contrast to existing check elimination
techniques that are limited by static code analysis, our solution leverages a
model-based inference to identify redundant bounds checking based on runtime
data from past program executions. For a given function, it rapidly derives and
updates a knowledge base containing sufficient conditions for identifying
redundant array bounds checking. We evaluate CHOP on real-world applications
and benchmark (such as SPEC) and the experimental results show that on average
80.12% of dynamic bounds check instructions can be avoided, resulting in
improved performance up to 95.80% over SoftBound.
"
1782,"Methodologies of Link-Level Simulator and System-Level Simulator for
  C-V2X Communication","  At the time of the development, standardization, and further improvement are
vital to the modern cellular systems such as the next generation wireless
communication (5G). Simulations are essential to test and optimize algorithms
and procedures prior to their implementation process of the equipment
manufactures. In order to evaluate system performance at different levels,
accurate simulations of simple setups, as well as simulations of more complex
systems via abstracted models are necessary. In this work, two new simulators
for the sidelink Cooperative-Vehicle-to-Everything (C-V2X) communication have
been implemented and carried out on both the physical layer (Link-Level (LL))
and network layer (System-Level (SL)). Detailed methodologies of the LL and SL
simulators for C-V2X communication have been illustrated. In the LL simulator,
we get the mapping curves of BLER and Signal-to-Noise-Ratio (SNR), which are
used as a baseline for measuring the performance of the LL simulation. In
addition, these mapping curves are used as the important Link-to-System (L2S)
interfaces. The SL simulator is utilized for measuring the performance of cell
networking and simulating large networks comprising of multiple eNBs and UEs.
Finally, the simulation results of both simulators for CV2X communication are
presented, which shows that different objectives can be met by using LL or SL
simulations types.
"
1783,"Scheduling With Inexact Job Sizes: The Merits of Shortest Processing
  Time First","  It is well known that size-based scheduling policies, which take into account
job size (i.e., the time it takes to run them), can perform very desirably in
terms of both response time and fairness. Unfortunately, the requirement of
knowing a priori the exact job size is a major obstacle which is frequently
insurmountable in practice. Often, it is possible to get a coarse estimation of
job size, but unfortunately analytical results with inexact job sizes are
challenging to obtain, and simulation-based studies show that several
size-based algorithm are severely impacted by job estimation errors. For
example, Shortest Remaining Processing Time (SRPT), which yields optimal mean
sojourn time when job sizes are known exactly, can drastically underperform
when it is fed inexact job sizes.
  Some algorithms have been proposed to better handle size estimation errors,
but they are somewhat complex and this makes their analysis challenging. We
consider Shortest Processing Time (SPT), a simplification of SRPT that skips
the update of ""remaining"" job size and results in a preemptive algorithm that
simply schedules the job with the shortest estimated processing time. When job
size is inexact, SPT performs comparably to the best known algorithms in the
presence of errors, while being definitely simpler. In this work, SPT is
evaluated through simulation, showing near-optimal performance in many cases,
with the hope that its simplicity can open the way to analytical evaluation
even when inexact inputs are considered.
"
1784,Profiling based Out-of-core Hybrid Method for Large Neural Networks,"  GPUs are widely used to accelerate deep learning with NNs (NNs). On the other
hand, since GPU memory capacity is limited, it is difficult to implement
efficient programs that compute large NNs on GPU. To compute NNs exceeding GPU
memory capacity, data-swapping method and recomputing method have been proposed
in existing work. However, in these methods, performance overhead occurs due to
data movement or increase of computation. In order to reduce the overhead, it
is important to consider characteristics of each layer such as sizes and cost
for recomputation. Based on this direction, we proposed Profiling based
out-of-core Hybrid method (PoocH). PoocH determines target layers of swapping
or recomputing based on runtime profiling. We implemented PoocH by extending a
deep learning framework, Chainer, and we evaluated its performance. With PoocH,
we successfully computed an NN requiring 50 GB memory on a single GPU with 16
GB memory. Compared with in-core cases, performance degradation was 38 \% on
x86 machine and 28 \% on POWER9 machine.
"
1785,Enhancing Spectral Utilization by Maximizing the Reuse in LTE Network,"  Need for increased spectral efficiency is key to improve the quality of
experience for next-generation wireless applications like online gaming, HD
Video, etc.,. In our work, we consider an LTE Device-to-device (D2D) network
where LTE UEs have primary access to the spectrum and D2D pairs have secondary
access. To enhance spectral efficiency, BS can offload the traffic by
activating multiple D2D pairs within the serving cell. This ensures that the
same radio resource will be reused across the primary LTE UEs and different D2D
pairs. In this context, we propose to enable more D2D secondary users in the
serving cell, by utilizing neighboring BS spectrum to fairly co-exist with
neighboring LTE primary users. We model the system and show via extensive
simulations, that the above configuration guarantees good throughput for the
D2D pairs in the serving cell while ensuring that the primary LTE throughput
demand is not compromised.
"
1786,"Simulating Nonlinear Neutrino Oscillations on Next-Generation Many-Core
  Architectures","  In this work an astrophysical simulation code, XFLAT, is developed to study
neutrino oscillations in supernovae. XFLAT is designed to utilize multiple
levels of parallelism through MPI, OpenMP, and SIMD instructions
(vectorization). It can run on both the CPU and the Xeon Phi co-processor, the
latter of which is based on the Intel Many Integrated Core Architecture (MIC).
The performance of XFLAT on configurations and scenarios has been analyzed. In
addition, the impact of I/O and the multi-node configuration on the Xeon
Phi-equipped heterogeneous supercomputers such as Stampede at the Texas
Advanced Computing Center (TACC) was investigated.
"
1787,A Unified Analysis Approach for Hardware and Software Implementations,"  Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.
"
1788,"A Recursive Algebraic Coloring Technique for Hardware-Efficient
  Symmetric Sparse Matrix-Vector Multiplication","  The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important
building block for many numerical linear algebra kernel operations or graph
traversal applications. Parallelizing SymmSpMV on today's multicore platforms
with up to 100 cores is difficult due to the need to manage conflicting updates
on the result vector. Coloring approaches can be used to solve this problem
without data duplication, but existing coloring algorithms do not take load
balancing and deep memory hierarchies into account, hampering scalability and
full-chip performance. In this work, we propose the recursive algebraic
coloring engine (RACE), a novel coloring algorithm and open-source library
implementation, which eliminates the shortcomings of previous coloring methods
in terms of hardware efficiency and parallelization overhead. We describe the
level construction, distance-k coloring, and load balancing steps in RACE, use
it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices
with other state-of-the-art coloring techniques and Intel MKL on two modern
multicore processors. RACE outperforms all other approaches substantially and
behaves in accordance with the Roofline model. Outliers are discussed and
analyzed in detail. While we focus on SymmSpMV in this paper, our algorithm and
software is applicable to any sparse matrix operation with data dependencies
that can be resolved by distance-k coloring.
"
1789,"Approximate Solution Approach and Performability Evaluation of Large
  Scale Beowulf Clusters","  Beowulf clusters are very popular and deployed worldwide in support of
scientific computing, because of the high computational power and performance.
However, they also pose several challenges, and yet they need to provide high
availability. The practical large-scale Beowulf clusters result in
unpredictable, fault-tolerant, often detrimental outcomes. Successful
development of high performance in storing and processing huge amounts of data
in large-scale clusters necessitates accurate quality of service (QoS)
evaluation. This leads to develop as well as design, analytical models to
understand and predict of complex system behaviour in order to ensure
availability of large-scale systems. Exact modelling of such clusters is not
feasible due to the nature of the large scale nodes and the diversity of user
requests. An analytical model for QoS of large-scale server farms and solution
approaches are necessary. In this paper, analytical modelling of large-scale
Beowulf clusters is considered together with availability issues. A generic and
flexible approximate solution approach is developed to handle large number of
nodes for performability evaluation. The proposed analytical model and the
approximate solution approach provide flexibility to evaluate the QoS
measurements for such systems. In order to show the efficacy and the accuracy
of the proposed approach, the results obtained from the analytical model are
validated with the results obtained from the discrete event simulations.
"
1790,"Quantitative Impact Evaluation of an Abstraction Layer for Data Stream
  Processing Systems","  With the demand to process ever-growing data volumes, a variety of new data
stream processing frameworks have been developed. Moving an implementation from
one such system to another, e.g., for performance reasons, requires adapting
existing applications to new interfaces. Apache Beam addresses these high
substitution costs by providing an abstraction layer that enables executing
programs on any of the supported streaming frameworks. In this paper, we
present a novel benchmark architecture for comparing the performance impact of
using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache
Flink, and Apache Apex. We find significant performance penalties when using
Apache Beam for application development in the surveyed systems. Overall, usage
of Apache Beam for the examined streaming applications caused a high variance
of query execution times with a slowdown of up to a factor of 58 compared to
queries developed without the abstraction layer. All developed benchmark
artifacts are publicly available to ensure reproducible results.
"
1791,"A Hermite-like basis for faster matrix-free evaluation of interior
  penalty discontinuous Galerkin operators","  This work proposes a basis for improved throughput of matrix-free evaluation
of discontinuous Galerkin symmetric interior penalty discretizations on
hexahedral elements. The basis relies on ideas of Hermite polynomials. It is
used in a fully discontinuous setting not for higher order continuity but to
minimize the effective stencil width, namely to limit the neighbor access of an
element to one data point for the function value and one for the derivative.
The basis is extended to higher orders with nodal contributions derived from
roots of Jacobi polynomials and extended to multiple dimensions with tensor
products, which enable the use of sum factorization. The beneficial effect of
the reduced data access on modern processors is shown. Furthermore, the
viability of the basis in the context of multigrid solvers is analyzed. While a
plain point-Jacobi approach is less efficient than with the best nodal
polynomials, a basis change via sum-factorization techniques enables the
combination of the fast matrix-vector products with effective multigrid
constituents. The basis change is essentially for free on modern hardware
because these computations can be hidden behind the cost of the data access.
"
1792,Multiple Server SRPT with speed scaling is competitive,"  Can the popular shortest remaining processing time (SRPT) algorithm achieve a
constant competitive ratio on multiple servers when server speeds are
adjustable (speed scaling) with respect to the flow time plus energy
consumption metric? This question has remained open for a while, where a
negative result in the absence of speed scaling is well known. The main result
of this paper is to show that multi-server SRPT can be constant competitive,
with a competitive ratio that only depends on the power-usage function of the
servers, but not on the number of jobs/servers or the job sizes (unlike when
speed scaling is not allowed). When all job sizes are unity, we show that
round-robin routing is optimal and can achieve the same competitive ratio as
the best known algorithm for the single server problem. Finally, we show that a
class of greedy dispatch policies, including policies that route to the least
loaded or the shortest queue, do not admit a constant competitive ratio. When
job arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we
show that random routing and a simple gated-static speed scaling algorithm
achieves a constant competitive ratio.
"
1793,"Benchmarking TPU, GPU, and CPU Platforms for Deep Learning","  Training deep learning models is compute-intensive and there is an
industry-wide trend towards hardware specialization to improve performance. To
systematically benchmark deep learning platforms, we introduce ParaDnn, a
parameterized benchmark suite for deep learning that generates end-to-end
models for fully connected (FC), convolutional (CNN), and recurrent (RNN)
neural networks. Along with six real-world models, we benchmark Google's Cloud
TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep
dive into TPU architecture, reveal its bottlenecks, and highlight valuable
lessons learned for future specialized system design. We also provide a
thorough comparison of the platforms and find that each has unique strengths
for some types of models. Finally, we quantify the rapid performance
improvements that specialized software stacks provide for the TPU and GPU
platforms.
"
1794,Simple Near-Optimal Scheduling for the M/G/1,"  We consider the problem of preemptively scheduling jobs to minimize mean
response time of an M/G/1 queue. When we know each job's size, the shortest
remaining processing time (SRPT) policy is optimal. Unfortunately, in many
settings we do not have access to each job's size. Instead, we know only the
job size distribution. In this setting the Gittins policy is known to minimize
mean response time, but its complex priority structure can be computationally
intractable. A much simpler alternative to Gittins is the shortest expected
remaining processing time (SERPT) policy. While SERPT is a natural extension of
SRPT to unknown job sizes, it is unknown whether or not SERPT is close to
optimal for mean response time.
  We present a new variant of SERPT called monotonic SERPT (M-SERPT) which is
as simple as SERPT but has provably near-optimal mean response time at all
loads for any job size distribution. Specifically, we prove the mean response
time ratio between M-SERPT and Gittins is at most 3 for load $\rho \leq 8/9$
and at most 5 for any load. This makes M-SERPT the only non-Gittins scheduling
policy known to have a constant-factor approximation ratio for mean response
time.
"
1795,MDS coding is better than replication for job completion times,"  In a multi-server system, how can one get better performance than random
assignment of jobs to servers if queue-states cannot be queried by the
dispatcher? A replication strategy has recently been proposed where $d$ copies
of each arriving job are sent to servers chosen at random. The job's completion
time is the first time that the service of any of its copies is complete. On
completion, redundant copies of the job are removed from other queues so as not
to overburden the system.
  For digital jobs, where the objects to be served can be algebraically
manipulated, and for servers whose output is a linear function of their input,
here we consider an alternate strategy: Maximum Distance Separable (MDS) codes.
For every batch of $n$ digital jobs that arrive, $n+m$ linear combinations are
created over the reals or a large finite field, and each coded job is sent to a
random server. The batch completion time is the first time that any $n$ of the
$n+m$ coded jobs are served, as the evaluation of $n$ original jobs can be
recovered by Gaussian elimination. If redundant jobs can be removed from queues
on batch completion, we establish that in order to get the improved
response-time performance of sending $d$ copies of each of $n$ jobs via the
replication strategy, with the MDS methodology it suffices to send $n+d$ jobs.
That is, while replication is multiplicative, MDS is linear.
"
1796,Anonymity Mixes as (Partial) Assembly Queues: Modeling and Analysis,"  Anonymity platforms route the traffic over a network of special routers that
are known as mixes and implement various traffic disruption techniques to hide
the communicating users' identities. Batch mixes in particular anonymize
communicating peers by allowing message exchange to take place only after a
sufficient number of messages (a batch) accumulate, thus introducing delay. We
introduce a queueing model for batch mix and study its delay properties. Our
analysis shows that delay of a batch mix grows quickly as the batch size gets
close to the number of senders connected to the mix. We then propose a
randomized batch mixing strategy and show that it achieves much better delay
scaling in terms of the batch size. However, randomization is shown to reduce
the anonymity preserving capabilities of the mix. We also observe that queueing
models are particularly useful to study anonymity metrics that are more
practically relevant such as the time-to-deanonymize metric.
"
1797,"The Preliminary Evaluation of a Hypervisor-based Virtualization
  Mechanism for Intel Optane DC Persistent Memory Module","  Non-volatile memory (NVM) technologies, being accessible in the same manner
as DRAM, are considered indispensable for expanding main memory capacities.
Intel Optane DCPMM is a long-awaited product that drastically increases main
memory capacities. However, a substantial performance gap exists between DRAM
and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and
407% higher than those of DRAM, respectively. The read/write bandwidths were
37% and 8% of those of DRAM. This performance gap in main memory presents a new
challenge to researchers; we need a new system software technology supporting
emerging hybrid memory architecture. In this paper, we present RAMinate, a
hypervisor-based virtualization mechanism for hybrid memory systems, and a key
technology to address the performance gap in main memory systems. It provides
great flexibility in memory management and maximizes the performance of virtual
machines (VMs) by dynamically optimizing memory mappings. Through experiments,
we confirmed that even though a VM has only 1% of DRAM in its RAM, the
performance degradation of the VM was drastically alleviated by memory mapping
optimization. The elapsed time to finish the build of Linux Kernel in the VM
was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495
seconds). When the optimization mechanism was disabled, the elapsed time
increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).
"
1798,"ICE: An Interactive Configuration Explorer for High Dimensional
  Categorical Parameter Spaces","  There are many applications where users seek to explore the impact of the
settings of several categorical variables with respect to one dependent
numerical variable. For example, a computer systems analyst might want to study
how the type of file system or storage device affects system performance. A
usual choice is the method of Parallel Sets designed to visualize multivariate
categorical variables. However, we found that the magnitude of the parameter
impacts on the numerical variable cannot be easily observed here. We also
attempted a dimension reduction approach based on Multiple Correspondence
Analysis but found that the SVD-generated 2D layout resulted in a loss of
information. We hence propose a novel approach, the Interactive Configuration
Explorer (ICE), which directly addresses the need of analysts to learn how the
dependent numerical variable is affected by the parameter settings given
multiple optimization objectives. No information is lost as ICE shows the
complete distribution and statistics of the dependent variable in context with
each categorical variable. Analysts can interactively filter the variables to
optimize for certain goals such as achieving a system with maximum performance,
low variance, etc. Our system was developed in tight collaboration with a group
of systems performance researchers and its final effectiveness was evaluated
with expert interviews, a comparative user study, and two case studies.
"
1799,"Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous
  Vehicles","  Driverless vehicles promise a host of societal benefits including
dramatically improved safety, increased accessibility, greater productivity,
and higher quality of life. As this new technology approaches widespread
deployment, both industry and government are making provisions for
teleoperations systems, in which remote human agents provide assistance to
driverless vehicles. This assistance can involve real-time remote operation and
even ahead-of-time input via human-in-the-loop artificial intelligence systems.
In this paper, we address the problem of staffing such a remote support center.
Our analysis focuses on the tradeoffs between the total number of remote
agents, the reliability of the remote support system, and the resulting safety
of the driverless vehicles. By establishing a novel connection between queues
with large batch arrivals and storage processes, we determine the probability
of the system exceeding its service capacity. This connection drives our
staffing methodology. We also develop a numerical method to compute the exact
staffing level needed to achieve various performance measures. This moment
generating function based technique may be of independent interest, and our
overall staffing analysis may be of use in other applications that combine
human expertise and automated systems.
"
1800,"Modeling Shared Cache Performance of OpenMP Programs using Reuse
  Distance","  Performance modeling of parallel applications on multicore computers remains
a challenge in computational co-design due to the complex design of multicore
processors including private and shared memory hierarchies. We present a
Scalable Analytical Shared Memory Model to predict the performance of parallel
applications that runs on a multicore computer and shares the same level of
cache in the hierarchy. This model uses a computationally efficient,
probabilistic method to predict the reuse distance profiles, where reuse
distance is a hardware architecture-independent measure of the patterns of
virtual memory accesses. It relies on a stochastic, static basic block-level
analysis of reuse profiles measured from the memory traces of applications ran
sequentially on small instances rather than using a multi-threaded trace. The
results indicate that the hit-rate predictions on the shared cache are
accurate.
"
1801,"Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore
  Systems","  Innovations in Next-Generation Sequencing are enabling generation of DNA
sequence data at ever faster rates and at very low cost. Large sequencing
centers typically employ hundreds of such systems. Such high-throughput and
low-cost generation of data underscores the need for commensurate acceleration
in downstream computational analysis of the sequencing data. A fundamental step
in downstream analysis is mapping of the reads to a long reference DNA
sequence, such as a reference human genome. Sequence mapping is a
compute-intensive step that accounts for more than 30% of the overall time of
the GATK workflow. BWA-MEM is one of the most widely used tools for sequence
mapping and has tens of thousands of users.
  In this work, we focus on accelerating BWA-MEM through an efficient
architecture aware implementation, while maintaining identical output. The
volume of data requires distributed computing environment, usually deploying
multicore processors. Since the application can be easily parallelized for
distributed memory systems, we focus on performance improvements on a single
socket multicore processor. BWA-MEM run time is dominated by three kernels,
collectively responsible for more than 85% of the overall compute time. We
improved the performance of these kernels by 1) improving cache reuse, 2)
simplifying the algorithms, 3) replacing small fragmented memory allocations
with a few large contiguous ones, 4) software prefetching, and 5) SIMD
utilization wherever applicable - and massive reorganization of the source code
enabling these improvements.
  As a result, we achieved nearly 2x, 183x, and 8x speedups on the three
kernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end
compute time over the original BWA-MEM on single thread and single socket of
Intel Xeon Skylake processor. To the best of our knowledge, this is the highest
reported speedup over BWA-MEM.
"
1802,"A performance comparison of Dask and Apache Spark for data-intensive
  neuroimaging pipelines","  In the past few years, neuroimaging has entered the Big Data era due to the
joint increase in image resolution, data sharing, and study sizes. However, no
particular Big Data engines have emerged in this field, and several
alternatives remain available. We compare two popular Big Data engines with
Python APIs, Apache Spark and Dask, for their runtime performance in processing
neuroimaging pipelines. Our evaluation uses two synthetic pipelines processing
the 81GB BigBrain image, and a real pipeline processing anatomical data from
more than 1,000 subjects. We benchmark these pipelines using various
combinations of task durations, data sizes, and numbers of workers, deployed on
an 8-node (8 cores ea.) compute cluster in Compute Canada's Arbutus cloud. We
evaluate PySpark's RDD API against Dask's Bag, Delayed and Futures. Results
show that despite slight differences between Spark and Dask, both engines
perform comparably. However, Dask pipelines risk being limited by Python's GIL
depending on task type and cluster configuration. In all cases, the major
limiting factor was data transfer. While either engine is suitable for
neuroimaging pipelines, more effort needs to be placed in reducing data
transfer time.
"
1803,"Testing performance with and without Block Low Rank Compression in MUMPS
  and the new PaStiX 6.0 for JOREK nonlinear MHD simulations","  The interface to the MUMPS solver was updated in the JOREK MHD code to
support Block Low Rank (BLR) compression and an interface to the new PaStiX
solver version 6 has been implemented supporting BLR as well. First tests were
carried out with JOREK, which solves a large sparse matrix system iteratively
in each time step. For the preconditioning, a direct solver is applied in the
code to sub-matrices, and at this point BLR was applied with the results being
summarized in this report. For a simple case with a linearly growing mode,
results with both solvers look promising with a considerable reduction of the
memory consumption by several ten percent was obtained. A direct increase in
performance was seen in particular configurations already.
  The choice of the BLR accuracy parameter $\epsilon$ proves to be critical in
this simple test and also in more realistic simulations, which were carried out
only with MUMPS due to the limited time available. The more realistic test
showed an increase in run time when using BLR, which was mitigated when using
larger values of $\epsilon$. However, the GMRes iterative solver does not reach
convergence anymore when $\epsilon$ is too large, since the preconditioner
becomes too inaccurate in that case. It is thus critical to use an $\epsilon$
as large as possible, while still reaching convergence. More tests regarding
this optimum will be necessary in the future. BLR can also lead to an indirect
speed-up in particular cases, when the simulation can be run on a smaller
number of compute nodes due to the reduced memory consumption.
"
1804,"Edge AIBench: Towards Comprehensive End-to-end Edge Computing
  Benchmarking","  In edge computing scenarios, the distribution of data and collaboration of
workloads on different layers are serious concerns for performance, privacy,
and security issues. So for edge computing benchmarking, we must take an
end-to-end view, considering all three layers: client-side devices, edge
computing layer, and cloud servers. Unfortunately, the previous work ignores
this most important point. This paper presents the BenchCouncil's coordinated e
ort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models
four typical application scenarios: ICU Patient Monitor, Surveillance Camera,
Smart Home, and Autonomous Vehicle with the focus on data distribution and
workload collaboration on three layers. Edge AIBench is a part of the
open-source AIBench project, publicly available from
http://www.benchcouncil.org/AIBench/index.html. We also build an edge computing
testbed with a federated learning framework to resolve performance, privacy,
and security issues.
"
1805,"Analytical Performance Models for NoCs with Multiple Priority Traffic
  Classes","  Networks-on-chip (NoCs) have become the standard for interconnect solutions
in industrial designs ranging from client CPUs to many-core
chip-multiprocessors. Since NoCs play a vital role in system performance and
power consumption, pre-silicon evaluation environments include cycle-accurate
NoC simulators. Long simulations increase the execution time of evaluation
frameworks, which are already notoriously slow, and prohibit design-space
exploration. Existing analytical NoC models, which assume fair arbitration,
cannot replace these simulations since industrial NoCs typically employ
priority schedulers and multiple priority classes. To address this limitation,
we propose a systematic approach to construct priority-aware analytical
performance models using micro-architecture specifications and input traffic.
Our approach consists of developing two novel transformations of queuing system
and designing an algorithm which iteratively uses these two transformations to
estimate end-to-end latency. Our approach decomposes the given NoC into
individual queues with modified service time to enable accurate and scalable
latency computations. Specifically, we introduce novel transformations along
with an algorithm that iteratively applies these transformations to decompose
the queuing system. Experimental evaluations using real architectures and
applications show high accuracy of 97% and up to 2.5x speedup in full-system
simulation.
"
1806,"Redundancy Scheduling in Systems with Bi-Modal Job Service Time
  Distribution","  Queuing systems with redundant requests have drawn great attention because of
their promise to reduce the job completion time and variability. Despite a
large body of work on the topic, we are still far from fully understanding the
benefits of redundancy in practice. We here take one step towards practical
systems by studying queuing systems with bi-modal job service time
distribution. Such distributions have been observed in practice, as can be seen
in, e.g., Google cluster traces. We develop an analogy to a classical urns and
balls problem, and use it to study the queuing time performance of two
non-adaptive classical scheduling policies: random and round-robin. We
introduce new performance indicators in the analogous model, and argue that
they are good predictors of the queuing time in non-adaptive scheduling
policies. We then propose a non-adaptive scheduling policy that is based on
combinatorial designs, and show that it has better performance indicators.
Simulations confirm that the proposed scheduling policy, as the performance
indicators suggest, reduces the queuing times compared to random and
round-robin scheduling.
"
1807,"A Repairable System Supported by Two Spare Units and Serviced by Two
  Types of Repairers","  We study a one-unit repairable system, supported by two identical spare units
on cold standby, and serviced by two types of repairers. The model applies, for
instance, to ANSI (American National Standard Institute) centrifugal pumps in a
chemical plant. The failed unit undergoes repair either by an in-house repairer
within a random or deterministic patience time, or else by a visiting expert
repairer. The expert repairs one or all failed units before leaving, and does
so faster but at a higher cost rate than the regular repairer. Four models
arise depending on the number of repairs done by the expert and the nature of
the patience time. We compare these models based on the limiting availability
$A_{\infty}$, and the limiting profit per unit time $\omega$, using semi-Markov
processes, when all distributions are exponential. As anticipated, to maximize
$A_{\infty}$, the expert should repair all failed units. To maximize $\omega$,
a suitably chosen deterministic patience time is better than a random patience
time. Furthermore, given all cost parameters, we determine the optimum number
of repairs the expert should complete, and the optimum patience time given to
the regular repairer in order to maximize $\omega$.
"
1808,HPC AI500: A Benchmark Suite for HPC AI Systems,"  In recent years, with the trend of applying deep learning (DL) in high
performance scientific computing, the unique characteristics of emerging DL
workloads in HPC raise great challenges in designing, implementing HPC AI
systems. The community needs a new yard stick for evaluating the future HPC
systems. In this paper, we propose HPC AI500 --- a benchmark suite for
evaluating HPC systems that running scientific DL workloads. Covering the most
representative scientific fields, each workload from HPC AI500 is based on
real-world scientific DL applications. Currently, we choose 14 scientific DL
benchmarks from perspectives of application scenarios, data sets, and software
stack. We propose a set of metrics for comprehensively evaluating the HPC AI
systems, considering both accuracy, performance as well as power and cost. We
provide a scalable reference implementation of HPC AI500. HPC AI500 is a part
of the open-source AIBench project, the specification and source code are
publicly available from \url{http://www.benchcouncil.org/AIBench/index.html}.
"
1809,"Near-Memory Computing: Past, Present, and Future","  The conventional approach of moving data to the CPU for computation has
become a significant performance bottleneck for emerging scale-out
data-intensive applications due to their limited data reuse. At the same time,
the advancement in 3D integration technologies has made the decade-old concept
of coupling compute units close to the memory --- called near-memory computing
(NMC) --- more viable. Processing right at the ""home"" of data can significantly
diminish the data movement problem of data-intensive applications.
  In this paper, we survey the prior art on NMC across various dimensions
(architecture, applications, tools, etc.) and identify the key challenges and
open issues with future research directions. We also provide a glimpse of our
approach to near-memory computing that includes i) NMC specific
microarchitecture independent application characterization ii) a compiler
framework to offload the NMC kernels on our target NMC platform and iii) an
analytical model to evaluate the potential of NMC.
"
1810,Performance Comparison for Neuroscience Application Benchmarks,"  Researchers within the Human Brain Project and related projects have in the
last couple of years expanded their needs for high-performance computing
infrastructures. The needs arise from a diverse set of science challenges that
range from large-scale simulations of brain models to processing of
extreme-scale experimental data sets. The ICEI project, which is in the process
of creating a distributed infrastructure optimised for brain research, started
to build-up a set of benchmarks that reflect the diversity of applications in
this field. In this paper we analyse the performance of some selected
benchmarks on an IBM POWER8 and Intel Skylake based systems with and without
GPUs.
"
1811,An Empirical Guide to the Behavior and Use of Scalable Persistent Memory,"  After nearly a decade of anticipation, scalable nonvolatile memory DIMMs are
finally commercially available with the release of Intel's 3D XPoint DIMM. This
new nonvolatile DIMM supports byte-granularity accesses with access times on
the order of DRAM, while also providing data storage that survives power
outages. Researchers have not idly waited for real nonvolatile DIMMs (NVDIMMs)
to arrive. Over the past decade, they have written a slew of papers proposing
new programming models, file systems, libraries, and applications built to
exploit the performance and flexibility that NVDIMMs promised to deliver. Those
papers drew conclusions and made design decisions without detailed knowledge of
how real NVDIMMs would behave or how industry would integrate them into
computer architectures. Now that 3D XPoint NVDIMMs are actually here, we can
provide detailed performance numbers, concrete guidance for programmers on
these systems, reevaluate prior art for performance, and reoptimize persistent
memory software for the real 3D XPoint DIMM. In this paper, we explore the
performance properties and characteristics of Intel's new 3D XPoint DIMM at the
micro and macro level. First, we investigate the basic characteristics of the
device, taking special note of the particular ways in which its performance is
peculiar relative to traditional DRAM or other past methods used to emulate
NVM. From these observations, we recommend a set of best practices to maximize
the performance of the device. With our improved understanding, we then explore
the performance of prior art in application-level software for persistent
memory, taking note of where their performance was influenced by our
guidelines.
"
1812,Performance of Devito on HPC-Optimised ARM Processors,"  We evaluate the performance of Devito, a domain specific language (DSL) for
finite differences on Arm ThunderX2 processors. Experiments with two common
seismic computational kernels demonstrate that Arm processors can deliver
competitive performance compared to other Intel Xeon processors.
"
1813,MLP Aware Scheduling Techniques in Multithreaded Processors,"  Major chip manufacturers have all introduced Multithreaded processors. These
processors are used for running a variety of workloads. Efficient resource
utilization is an important design aspect in such processors. Particularly, it
is important to take advantage of available memory-level parallelism(MLP). In
this paper I propose a MLP aware operating system (OS) scheduling algorithm for
Multithreaded Multi-core processors. By observing the MLP available in each
thread and by balancing it with available MLP resources in the system the OS
will come up with a new schedule of threads for the next quantum that could
potentially improve overall performance. We do a qualitative comparison of our
solution with other hardware and software techniques. This work can be extended
by doing a quantitative evaluation and by further refining the scheduling
optimization.
"
1814,"uPredict: A User-Level Profiler-Based Predictive Framework for Single VM
  Applications in Multi-Tenant Clouds","  Most existing studies on performance prediction for virtual machines (VMs) in
multi-tenant clouds are at system level and generally require access to
performance counters in Hypervisors. In this work, we propose uPredict, a
user-level profiler-based performance predictive framework for single-VM
applications in multi-tenant clouds. Here, three micro-benchmarks are specially
devised to assess the contention of CPUs, memory and disks in a VM,
respectively. Based on measured performance of an application and
micro-benchmarks, the application and VM-specific predictive models can be
derived by exploiting various regression and neural network based techniques.
These models can then be used to predict the application's performance using
the in-situ profiled resource contention with the micro-benchmarks. We
evaluated uPredict extensively with representative benchmarks from PARSEC, NAS
Parallel Benchmarks and CloudSuite, on both a private cloud and two public
clouds. The results show that the average prediction errors are between 9.8% to
17% for various predictive models on the private cloud with high resource
contention, while the errors are within 4% on public clouds. A smart
load-balancing scheme powered by uPredict is presented and can effectively
reduce the execution and turnaround times of the considered application by 19%
and 10%, respectively.
"
1815,"Type-Directed Program Synthesis and Constraint Generation for Library
  Portability","  Fast numerical libraries have been a cornerstone of scientific computing for
decades, but this comes at a price. Programs may be tied to vendor specific
software ecosystems resulting in polluted, non-portable code. As we enter an
era of heterogeneous computing, there is an explosion in the number of
accelerator libraries required to harness specialized hardware. We need a
system that allows developers to exploit ever-changing accelerator libraries,
without over-specializing their code.
  As we cannot know the behavior of future libraries ahead of time, this paper
develops a scheme that assists developers in matching their code to new
libraries, without requiring the source code for these libraries.
  Furthermore, it can recover equivalent code from programs that use existing
libraries and automatically port them to new interfaces. It first uses program
synthesis to determine the meaning of a library, then maps the synthesized
description into generalized constraints which are used to search the program
for replacement opportunities to present to the developer.
  We applied this approach to existing large applications from the scientific
computing and deep learning domains. Using our approach, we show speedups
ranging from 1.1$\times$ to over 10$\times$ on end to end performance when
using accelerator libraries.
"
1816,Enhanced Performance and Privacy via Resolver-Less DNS,"  The domain name resolution into IP addresses can significantly delay
connection establishments on the web. Moreover, the common use of recursive DNS
resolvers presents a privacy risk as they can closely monitor the user's
browsing activities. In this paper, we present a novel HTTP response header
allowing web server to provide their clients with relevant DNS records. Our
results indicate, that this resolver-less DNS mechanism allows user agents to
save the DNS lookup time for subsequent connection establishments. We find,
that this proposal saves at least 80ms per DNS lookup for the one percent of
users having the longest round-trip times towards their recursive resolver.
Furthermore, our proposal decreases the number of DNS lookups and thus improves
the privacy posture of the user towards the used recursive resolver. Comparing
the security guarantees of the traditional DNS to our proposal, we find that
resolver-less DNS achieves at least the same security properties. In detail, it
even improves the user's resilience against censorship through tampered DNS
resolvers.
"
1817,Exploiting Parallelism Opportunities with Deep Learning Frameworks,"  State-of-the-art machine learning frameworks support a wide variety of design
features to enable a flexible machine learning programming interface and to
ease the programmability burden on machine learning developers. Identifying and
using a performance-optimal setting in feature-rich frameworks, however,
involves a non-trivial amount of performance profiling efforts and often relies
on domain-specific knowledge. This paper takes a deep dive into analyzing the
performance impact of key design features in a machine learning framework and
quantifies the role of parallelism. The observations and insights distill into
a simple set of guidelines that one can use to achieve much higher training and
inference speedup. Across a diverse set of real-world deep learning models, the
evaluation results show that the proposed performance tuning guidelines
outperform the Intel and TensorFlow recommended settings by 1.29x and 1.34x,
respectively.
"
1818,Micro-architectural Analysis of OLAP: Limitations and Opportunities,"  Understanding micro-architectural behavior is profound in efficiently using
hardware resources. Recent work has shown that, despite being aggressively
optimized for modern hardware, in-memory online transaction processing (OLTP)
systems severely underutilize their core micro-architecture resources [25].
Online analytical processing (OLAP) workloads, on the other hand, exhibit a
completely different computing pattern. OLAP workloads are read-only,
bandwidth-intensive and include various data access patterns including both
sequential and random data accesses. In addition, with the rise of
column-stores, they run on high performance engines that are tightly optimized
for the efficient use of modern hardware. Hence, the micro-architectural
behavior of modern OLAP systems remains unclear.
  This work presents the micro-architectural analysis of a breadth of OLAP
systems. We examine CPU cycles and memory bandwidth utilization. The results
show that, unlike the traditional, commercial OLTP systems, traditional,
commercial OLAP systems do not suffer from instruction cache misses.
Nevertheless, they suffer from their large instruction footprint resulting in
slow response times. High performance OLAP engines execute tight instruction
streams; however, they spend 25 to 82% of the CPU cycles on stalls regardless
of the workload being sequential- or random-access-heavy. In addition, high
performance OLAP engines underutilize the multi-core CPU or memory bandwidth
resources due to their disproportional compute and memory demands. Hence,
analytical processing engines should carefully assign their compute and memory
resources for efficient multi-core micro-architectural utilization.
"
1819,"New Results on Parameter Estimation via Dynamic Regressor Extension and
  Mixing: Continuous and Discrete-time Cases","  We present some new results on the dynamic regressor extension and mixing
parameter estimators for linear regression models recently proposed in the
literature. This technique has proven instrumental in the solution of several
open problems in system identification and adaptive control. The new results
include: (i) a unified treatment of the continuous and the discrete-time cases;
(ii) the proposal of two new extended regressor matrices, one which guarantees
a quantifiable transient performance improvement, and the other exponential
convergence under conditions that are strictly weaker than regressor
persistence of excitation; and (iii) an alternative estimator ensuring
parameter estimation in finite-time that retains its alertness to track
time-varying parameters. Simulations that illustrate our results are also
presented.
"
1820,Workload-Aware Opportunistic Energy Efficiency in Multi-FPGA Platforms,"  The continuous growth of big data applications with high computational and
scalability demands has resulted in increasing popularity of cloud computing.
Optimizing the performance and power consumption of cloud resources is
therefore crucial to relieve the costs of data centers. In recent years,
multi-FPGA platforms have gained traction in data centers as low-cost yet
high-performance solutions particularly as acceleration engines, thanks to the
high degree of parallelism they provide. Nonetheless, the size of data centers
workloads varies during service time, leading to significant underutilization
of computing resources while consuming a large amount of power, which turns out
as a key factor of data center inefficiency, regardless of the underlying
hardware structure. In this paper, we propose an efficient framework to
throttle the power consumption of multi-FPGA platforms by dynamically scaling
the voltage and hereby frequency during runtime according to prediction of, and
adjustment to the workload level, while maintaining the desired Quality of
Service (QoS). This is in contrast to, and more efficient than, conventional
approaches that merely scale (i.e., power-gate) the computing nodes or
frequency. The proposed framework carefully exploits a pre-characterized
library of delay-voltage, and power-voltage information of FPGA resources,
which we show is indispensable to obtain the efficient operating point due to
the different sensitivity of resources w.r.t. voltage scaling, particularly
considering multiple power rails residing in these devices. Our evaluations by
implementing state-of-the-art deep neural network accelerators revealed that,
providing an average power reduction of 4.0X, the proposed framework surpasses
the previous works by 33.6% (up to 83%).
"
1821,"XSP: Across-Stack Profiling and Analysis of Machine Learning Models on
  GPUs","  There has been a rapid proliferation of machine learning/deep learning (ML)
models and wide adoption of them in many application domains. This has made
profiling and characterization of ML model performance an increasingly pressing
task for both hardware designers and system providers, as they would like to
offer the best possible system to serve ML models with the target latency,
throughput, cost, and energy requirements while maximizing resource
utilization. Such an endeavor is challenging as the characteristics of an ML
model depend on the interplay between the model, framework, system libraries,
and the hardware (or the HW/SW stack). Existing profiling tools are disjoint,
however, and only focus on profiling within a particular level of the stack,
which limits the thoroughness and usefulness of the profiling results.
  This paper proposes XSP - an across-stack profiling design that gives a
holistic and hierarchical view of ML model execution. XSP leverages distributed
tracing to aggregate and correlates profile data from different sources. XSP
introduces a leveled and iterative measurement approach that accurately
captures the latencies at all levels of the HW/SW stack in spite of the
profiling overhead. We couple the profiling design with an automated analysis
pipeline to systematically analyze 65 state-of-the-art ML models. We
demonstrate that XSP provides insights which would be difficult to discern
otherwise.
"
1822,"An Autonomous Performance Testing Framework using Self-Adaptive Fuzzy
  Reinforcement Learning","  Test automation brings the potential to reduce costs and human effort, but
several aspects of software testing remain challenging to automate. One such
example is automated performance testing to find performance breaking points.
Current approaches to tackle automated generation of performance test cases
mainly involve using source code or system model analysis or use-case based
techniques. However, source code and system models might not always be
available at testing time. On the other hand, if the optimal performance
testing policy for the intended objective in a testing process instead could be
learned by the testing system, then test automation without advanced
performance models could be possible. Furthermore, the learned policy could
later be reused for similar software systems under test, thus leading to higher
test efficiency. We propose SaFReL, a self-adaptive fuzzy reinforcement
learning-based performance testing framework. SaFReL learns the optimal policy
to generate performance test cases through an initial learning phase, then
reuses it during a transfer learning phase, while keeping the learning running
and updating the policy in the long term. Through multiple experiments on a
simulated environment, we demonstrate that our approach generates the target
performance test cases for different programs more efficiently than a typical
testing process, and performs adaptively without access to source code and
performance models.
"
1823,"Computing System Congestion Management Using Exponential Smoothing
  Forecasting","  An overloaded computer must finish what it starts and not start what will
fail or hang. A congestion management algorithm the author developed, and
Siemens Corporation patented for telecom products, effectively manages traffic
overload with its unique formulation of Exponential Smoothing forecasting.
Siemens filed for exclusive rights to this technique in 2003 and obtained US
patent US7301903B2 in 2007 with this author, an employee at the time of the
filing, the sole inventor. A computer program, written in C language, which
exercises the methodology is listed at the end of this document and available
on GitHub.
"
1824,Do Energy-oriented Changes Hinder Maintainability?,"  Energy efficiency is a crucial quality requirement for mobile applications.
However, improving energy efficiency is far from trivial as developers lack the
knowledge and tools to aid in this activity. In this paper we study the impact
of changes to improve energy efficiency on the maintainability of Android
applications. Using a dataset containing 539 energy efficiency-oriented
commits, we measure maintainability -- as computed by the Software Improvement
Group's web-based source code analysis service Better Code Hub (BCH) -- before
and after energy efficiency-related code changes. Results show that in general
improving energy efficiency comes with a significant decrease in
maintainability. This is particularly evident in code changes to accommodate
the Power Save Mode and Wakelock Addition energy patterns. In addition, we
perform manual analysis to assess how real examples of energy-oriented changes
affect maintainability. Our results help mobile app developers to 1) avoid
common maintainability issues when improving the energy efficiency of their
apps; and 2) adopt development processes to build maintainable and
energy-efficient code. We also support researchers by identifying challenges in
mobile app development that still need to be addressed.
"
1825,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"  Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.
"
1826,"Optimizing Inter-Datacenter Tail Flow Completion Times using Best
  Worst-case Routing","  Flow routing over inter-datacenter networks is a well-known problem where the
network assigns a path to a newly arriving flow potentially according to the
network conditions and the properties of the new flow. An essential system-wide
performance metric for a routing algorithm is the flow completion times, which
affect the performance of applications running across multiple datacenters.
Current static and dynamic routing approaches do not take advantage of flow
size information in routing, which is practical in a controlled environment
such as inter-datacenter networks that are managed by the datacenter operators.
In this paper, we discuss Best Worst-case Routing (BWR), which aims at
optimizing the tail completion times of long-running flows over
inter-datacenter networks with non-uniform link capacities. Since finding the
path with the best worst-case completion time for a new flow is NP-Hard, we
investigate two heuristics, BWRH and BWRHF, which use two different upper
bounds on the worst-case completion times for routing. We evaluate BWRH and
BWRHF against several real WAN topologies and multiple traffic patterns.
Although BWRH better models the BWR problem, BWRH and BWRHF show negligible
difference across various system-wide performance metrics, while BWRHF being
significantly faster. Furthermore, we show that compared to other popular
routing heuristics, BWRHF can reduce the mean and tail flow completion times by
over $1.5\times$ and $2\times$, respectively.
"
1827,"Red-blue pebbling revisited: near optimal parallel matrix-matrix
  multiplication","  We propose COSMA: a parallel matrix-matrix multiplication algorithm that is
near communication-optimal for all combinations of matrix dimensions, processor
counts, and memory sizes. The key idea behind COSMA is to derive an optimal (up
to a factor of 0.03\% for 10MB of fast memory) sequential schedule and then
parallelize it, preserving I/O optimality. To achieve this, we use the red-blue
pebble game to precisely model MMM dependencies and derive a constructive and
tight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D
algorithms, which fix processor decomposition upfront and then map it to the
matrix dimensions, it reduces communication volume by up to $\sqrt{3}$ times.
COSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all
scenarios up to 12.8x (2.2x on average), achieving up to 88\% of Piz Daint's
peak performance. Our work does not require any hand tuning and is maintained
as an open source implementation.
"
1828,Performance Analysis of Zippers,"  A zipper is a powerful technique of representing a purely functional data
structure in a way that allows fast access to a specific element. It is often
used in cases where the imperative data structures would use a mutable pointer.
However, the efficiency of zippers as a replacement for mutable pointers is not
sufficiently explored. We attempt to address this issue by comparing the
performance of zippers and mutable pointers in two common scenarios and three
different languages: C++, C#, and Haskell.
"
1829,"TapirXLA: Embedding Fork-Join Parallelism into the XLA Compiler in
  TensorFlow Using Tapir","  This work introduces TapirXLA, a replacement for TensorFlow's XLA compiler
that embeds recursive fork-join parallelism into XLA's low-level representation
of code. Machine-learning applications rely on efficient parallel processing to
achieve performance, and they employ a variety of technologies to improve
performance, including compiler technology. But compilers in machine-learning
frameworks lack a deep understanding of parallelism, causing them to lose
performance by missing optimizations on parallel computation. This work studies
how Tapir, a compiler intermediate representation (IR) that embeds parallelism
into a mainstream compiler IR, can be incorporated into a compiler for machine
learning to remedy this problem. TapirXLA modifies the XLA compiler in
TensorFlow to employ the Tapir/LLVM compiler to optimize low-level parallel
computation. TapirXLA encodes the parallelism within high-level TensorFlow
operations using Tapir's representation of fork-join parallelism. TapirXLA also
exposes to the compiler implementations of linear-algebra library routines
whose parallel operations are encoded using Tapir's representation. We compared
the performance of TensorFlow using TapirXLA against TensorFlow using an
unmodified XLA compiler. On four neural-network benchmarks, TapirXLA speeds up
the parallel running time of the network by a geometric-mean multiplicative
factor of 30% to 100%, across four CPU architectures.
"
1830,Survey and Benchmarking of Machine Learning Accelerators,"  Advances in multicore processors and accelerators have opened the flood gates
to greater exploration and application of machine learning techniques to a
variety of applications. These advances, along with breakdowns of several
trends including Moore's Law, have prompted an explosion of processors and
accelerators that promise even greater computational and machine learning
capabilities. These processors and accelerators are coming in many forms, from
CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys
the current state of these processors and accelerators that have been publicly
announced with performance and power consumption numbers. The performance and
power values are plotted on a scatter graph and a number of dimensions and
observations from the trends on this plot are discussed and analyzed. For
instance, there are interesting trends in the plot regarding power consumption,
numerical precision, and inference versus training. We then select and
benchmark two commercially-available low size, weight, and power (SWaP)
accelerators as these processors are the most interesting for embedded and
mobile machine learning inference applications that are most applicable to the
DoD and other SWaP constrained users. We determine how they actually perform
with real-world images and neural network models, compare those results to the
reported performance and power consumption values and evaluate them against an
Intel CPU that is used in some embedded applications.
"
1831,"Comparative study of performance of parallel Alpha Beta Pruning for
  different architectures","  Optimization of searching the best possible action depending on various
states like state of environment, system goal etc. has been a major area of
study in computer systems. In any search algorithm, searching best possible
solution from the pool of every possibility known can lead to the construction
of the whole state search space popularly called as minimax algorithm. This may
lead to a impractical time complexities which may not be suitable for real time
searching operations. One of the practical solution for the reduction in
computational time is Alpha Beta pruning. Instead of searching for the whole
state space, we prune the unnecessary branches, which helps reduce the time by
significant amount. This paper focuses on the various possible implementations
of the Alpha Beta pruning algorithms and gives an insight of what algorithm can
be used for parallelism. Various studies have been conducted on how to make
Alpha Beta pruning faster. Parallelizing Alpha Beta pruning for the GPUs
specific architectures like mesh(CUDA) etc. or shared memory model(OpenMP)
helps in the reduction of the computational time. This paper studies the
comparison between sequential and different parallel forms of Alpha Beta
pruning and their respective efficiency for the chess game as an application.
"
1832,SCALABLE INTERNETWORKING: Final Technical Report,"  This document describes the work completed at the University of California,
Santa Cruz under the project Scalable Internetworking sponsored by ARPA under
Contract No. F19628-93-C-0175. This report covers work performed from 1 April
1993 to 31 December 1995. Results on routing and multicasting for large-scale
internets are summarized. The technical material discussed assumes familiarity
with the content of our proposal and previous quarterly reports submitted in
this project.
"
1833,"Improving the Effective Utilization of Supercomputer Resources by Adding
  Low-Priority Containerized Jobs","  We propose an approach to utilize idle computational resources of
supercomputers. The idea is to maintain an additional queue of low-priority
non-parallel jobs and execute them in containers, using container migration
tools to break the execution down into separate intervals. We propose a
container management system that can maintain this queue and interact with the
supercomputer scheduler. We conducted a series of experiments simulating
supercomputer scheduler and the proposed system. The experiments demonstrate
that the proposed system increases the effective utilization of supercomputer
resources under most of the conditions, in some cases significantly improving
the performance.
"
1834,"Touch\'e: Towards Ideal and Efficient Cache Compression By Mitigating
  Tag Area Overheads","  Compression is seen as a simple technique to increase the effective cache
capacity. Unfortunately, compression techniques either incur tag area overheads
or restrict data placement to only include neighboring compressed cache blocks
to mitigate tag area overheads. Ideally, we should be able to place arbitrary
compressed cache blocks without any placement restrictions and tag area
overheads.
  This paper proposes Touch\'e, a framework that enables storing multiple
arbitrary compressed cache blocks within a physical cacheline without any tag
area overheads. The Touch\'e framework consists of three components. The first
component, called the ``Signature'' (SIGN) engine, creates shortened signatures
from the tag addresses of compressed blocks. Due to this, the SIGN engine can
store multiple signatures in each tag entry. On a cache access, the physical
cacheline is accessed only if there is a signature match (which has a
negligible probability of false positive). The second component, called the
``Tag Appended Data'' (TADA) mechanism, stores the full tag addresses with
data. TADA enables Touch\'e to detect false positive signature matches by
ensuring that the actual tag address is available for comparison. The third
component, called the ``Superblock Marker'' (SMARK) mechanism, uses a unique
marker in the tag entry to indicate the occurrence of compressed cache blocks
from neighboring physical addresses in the same cacheline. Touch\'e is
completely hardware-based and achieves an average speedup of 12\% (ideal 13\%)
when compared to an uncompressed baseline.
"
1835,Algorithm-Based Fault Tolerance for Parallel Stencil Computations,"  The increase in HPC systems size and complexity, together with increasing
on-chip transistor density, power limitations, and number of components, render
modern HPC systems subject to soft errors. Silent data corruptions (SDCs) are
typically caused by such soft errors in the form of bit-flips in the memory
subsystem and hinder the correctness of scientific applications. This work
addresses the problem of protecting a class of iterative computational kernels,
called stencils, against SDCs when executing on parallel HPC systems. Existing
SDC detection and correction methods are in general either inaccurate,
inefficient, or targeting specific application classes that do not include
stencils. This work proposes a novel algorithm-based fault tolerance (ABFT)
method to protect scientific applications that contain arbitrary stencil
computations against SDCs. The ABFT method can be applied both online and
offline to accurately detect and correct SDCs in 2D and 3D parallel stencil
computations. We present a formal model for the proposed method including
theorems and proofs for the computation of the associated checksums as well as
error detection and correction. We experimentally evaluate the use of the
proposed ABFT method on a real 3D stencil-based application (HotSpot3D) via a
fault-injection, detection, and correction campaign. Results show that the
proposed ABFT method achieves less than 8% overhead compared to the performance
of the unprotected stencil application. Moreover, it accurately detects and
corrects SDCs. While the offline ABFT version corrects errors more accurately,
it may incur a small additional overhead than its online counterpart.
"
1836,"Towards Models for Availability and Security Evaluation of Cloud
  Computing with Moving Target Defense","  Security is one of the most relevant concerns in cloud computing. With the
evolution of cyber-security threats, developing innovative techniques to thwart
attacks is of utmost importance. One recent method to improve cloud computing
security is Moving Target Defense (MTD). MTD makes use of dynamic
reconfiguration in virtualized environments to ""confuse"" attackers or to
nullify their knowledge about the system state. However, there is still no
consolidated mechanism to evaluate the trade-offs between availability and
security when using MTD on cloud computing. The evaluation through measurements
is complex as one needs to deal with unexpected events as failures and attacks.
To overcome this challenge, we intend to propose a set of models to evaluate
the availability and security of MTD in cloud computing environments. The
expected results include the quantification of availability and security levels
under different conditions (e.g., different software aging rates, varying
workloads, different attack intensities).
"
1837,ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference,"  Mobile applications are increasingly leveraging complex deep learning models
to deliver features, e.g., image recognition, that require high prediction
accuracy. Such models can be both computation and memory-intensive, even for
newer mobile devices, and are therefore commonly hosted in powerful remote
servers. However, current cloud-based inference services employ static model
selection approach that can be suboptimal for satisfying application SLAs
(service level agreements), as they fail to account for inherent dynamic mobile
environment.
  We introduce a cloud-based technique called ModiPick that dynamically selects
the most appropriate model for each inference request, and adapts its selection
to match different SLAs and execution time budgets that are caused by variable
mobile environments. The key idea of ModiPick is to make inference speed and
accuracy trade-offs at runtime with a pool of managed deep learning models. As
such, ModiPick masks unpredictable inference time budgets and therefore meets
SLA targets, while improving accuracy within mobile network constraints. We
evaluate ModiPick through experiments based on prototype systems and through
simulations. We show that ModiPick achieves comparable inference accuracy to a
greedy approach while improving SLA adherence by up to 88.5%.
"
1838,"ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution
  Neural Network Inference on Mobile GPUs","  Convolution neural networks are widely used for mobile applications. However,
GPU convolution algorithms are designed for mini-batch neural network training,
the single-image convolution neural network inference algorithm on mobile GPUs
is not well-studied. After discussing the usage difference and examining the
existing convolution algorithms, we proposed the HNTMP convolution algorithm.
The HNTMP convolution algorithm achieves $14.6 \times$ speedup than the most
popular \textit{im2col} convolution algorithm, and $2.30 \times$ speedup than
the fastest existing convolution algorithm (direct convolution) as far as we
know.
"
1839,Efficient Lock-Free Durable Sets,"  Non-volatile memory is expected to co-exist or replace DRAM in upcoming
architectures. Durable concurrent data structures for non-volatile memories are
essential building blocks for constructing adequate software for use with these
architectures. In this paper, we propose a new approach for durable concurrent
sets and use this approach to build the most efficient durable hash tables
available today. Evaluation shows a performance improvement factor of up to
3.3x over existing technology.
"
1840,MBWU: Benefit Quantification for Data Access Function Offloading,"  The storage industry is considering new kinds of storage devices that support
data access function offloading, i.e. the ability to perform data access
functions on the storage device itself as opposed to performing it on a
separate compute system to which the storage device is connected. But what is
the benefit of offloading to a storage device that is controlled by an embedded
platform, very different from a host platform? To quantify the benefit, we need
a measurement methodology that enables apple-to-apple comparisons between
different platforms. We propose a Media-based Work Unit (MBWU, pronounced
""MibeeWu""), and an MBWU-based measurement methodology to standardize the
platform efficiency evaluation so as to quantify the benefit of offloading. To
demonstrate the merit of this methodology, we implemented a prototype to
automate quantifying the benefit of offloading the key-value data access
function.
"
1841,"Characterizing the Deep Neural Networks Inference Performance of Mobile
  Applications","  Today's mobile applications are increasingly leveraging deep neural networks
to provide novel features, such as image and speech recognitions. To use a
pre-trained deep neural network, mobile developers can either host it in a
cloud server, referred to as cloud-based inference, or ship it with their
mobile application, referred to as on-device inference. In this work, we
investigate the inference performance of these two common approaches on both
mobile devices and public clouds, using popular convolutional neural networks.
Our measurement study suggests the need for both on-device and cloud-based
inferences for supporting mobile applications. In particular, newer mobile
devices is able to run mobile-optimized CNN models in reasonable time. However,
for older mobile devices or to use more complex CNN models, mobile applications
should opt in for cloud-based inference. We further demonstrate that variable
network conditions can lead to poor cloud-based inference end-to-end time. To
support efficient cloud-based inference, we propose a CNN model selection
algorithm called CNNSelect that dynamically selects the most appropriate CNN
model for each inference request, and adapts its selection to match different
SLAs and execution time budgets that are caused by variable mobile
environments. The key idea of CNNSelect is to make inference speed and accuracy
trade-offs at runtime using a set of CNN models. We demonstrated that CNNSelect
smoothly improves inference accuracy while maintaining SLA attainment in 88.5%
more cases than a greedy baseline.
"
1842,"Addressing Algorithmic Bottlenecks in Elastic Machine Learning with
  Chicle","  Distributed machine learning training is one of the most common and important
workloads running on data centers today, but it is rarely executed alone.
Instead, to reduce costs, computing resources are consolidated and shared by
different applications. In this scenario, elasticity and proper load balancing
are vital to maximize efficiency, fairness, and utilization. Currently, most
distributed training frameworks do not support the aforementioned properties. A
few exceptions that do support elasticity, imitate generic distributed
frameworks and use micro-tasks. In this paper we illustrate that micro-tasks
are problematic for machine learning applications, because they require a high
degree of parallelism which hinders the convergence of distributed training at
a pure algorithmic level (i.e., ignoring overheads and scalability
limitations). To address this, we propose Chicle, a new elastic distributed
training framework which exploits the nature of machine learning algorithms to
implement elasticity and load balancing without micro-tasks. We use Chicle to
train deep neural network as well as generalized linear models, and show that
Chicle achieves performance competitive with state of the art rigid frameworks,
while efficiently enabling elastic execution and dynamic load balancing.
"
1843,"Sentinel: Runtime Data Management on Heterogeneous Main MemorySystems
  for Deep Learning","  Software-managed heterogeneous memory (HM) provides a promising solution to
increase memory capacity and cost efficiency. However, to release the
performance potential of HM, we face a problem of data management. Given an
application with various execution phases and each with possibly distinct
working sets, we must move data between memory components of HM to optimize
performance. The deep neural network (DNN), as a common workload on data
centers, imposes great challenges on data management on HM. This workload often
employs a task dataflow execution model, and is featured with a large amount of
small data objects and fine-grained operations (tasks). This execution model
imposes challenges on memory profiling and efficient data migration.
  We present Sentinel, a runtime system that automatically optimizes data
migration (i.e., data management) on HM to achieve performance similar to that
on the fast memory-only system with a much smaller capacity of fast memory. To
achieve this,Sentinel exploits domain knowledge about deep learning to adopt a
custom approach for data management. Sentinel leverages workload repeatability
to break the dilemma between profiling accuracy and overhead; It enables
profiling and data migration at the granularity of data objects (not pages), by
controlling memory allocation. This method bridges the semantic gap between
operating system and applications. By associating data objects with the DNN
topology, Sentinel avoids unnecessary data movement and proactively triggers
data movement. Using only 20% of peak memory consumption of DNN models as fast
memory size, Sentinel achieves the same or comparable performance (at most 8%
performance difference) to that of the fast memory-only system on common DNN
models; Sentinel also consistently outperforms a state-of-the-art solution by
18%.
"
1844,"AITuning: Machine Learning-based Tuning Tool for Run-Time Communication
  Libraries","  In this work, we address the problem of tuning communication libraries by
using a deep reinforcement learning approach. Reinforcement learning is a
machine learning technique incredibly effective in solving game-like
situations. In fact, tuning a set of parameters in a communication library in
order to get better performance in a parallel application can be expressed as a
game: Find the right combination/path that provides the best reward. Even
though AITuning has been designed to be utilized with different run-time
libraries, we focused this work on applying it to the OpenCoarrays run-time
communication library, built on top of MPI-3. This work not only shows the
potential of using a reinforcement learning algorithm for tuning communication
libraries, but also demonstrates how the MPI Tool Information Interface,
introduced by the MPI-3 standard, can be used effectively by run-time libraries
to improve the performance without human intervention.
"
1845,"Efficiency Metrics for Data-Driven Models: A Text Summarization Case
  Study","  Using data-driven models for solving text summarization or similar tasks has
become very common in the last years. Yet most of the studies report basic
accuracy scores only, and nothing is known about the ability of the proposed
models to improve when trained on more data. In this paper, we define and
propose three data efficiency metrics: data score efficiency, data time
deficiency and overall data efficiency. We also propose a simple scheme that
uses those metrics and apply it for a more comprehensive evaluation of popular
methods on text summarization and title generation tasks. For the latter task,
we process and release a huge collection of 35 million abstract-title pairs
from scientific articles. Our results reveal that among the tested models, the
Transformer is the most efficient on both tasks.
"
1846,A First Look at Commercial 5G Performance on Smartphones,"  We conduct to our knowledge a first measurement study of commercial 5G
performance on smartphones by closely examining 5G networks of three carriers
(two mmWave carriers, one mid-band carrier) in three U.S. cities. We conduct
extensive field tests on 5G performance in diverse urban environments. We
systematically analyze the handoff mechanisms in 5G and their impact on network
performance. We explore the feasibility of using location and possibly other
environmental information to predict the network performance. We also study the
app performance (web browsing and HTTP download) over 5G. Our study consumes
more than 15 TB of cellular data. Conducted when 5G just made its debut, it
provides a ""baseline"" for studying how 5G performance evolves, and identifies
key research directions on improving 5G users' experience in a cross-layer
manner. We have released the data collected from our study (referred to as
5Gophers) at https://fivegophers.umn.edu/www20.
"
1847,"Mitigating Network Noise on Dragonfly Networks through Application-Aware
  Routing","  System noise can negatively impact the performance of HPC systems, and the
interconnection network is one of the main factors contributing to this
problem. To mitigate this effect, adaptive routing sends packets on non-minimal
paths if they are less congested. However, while this may mitigate interference
caused by congestion, it also generates more traffic since packets traverse
additional hops, causing in turn congestion on other applications and on the
application itself. In this paper, we first describe how to estimate network
noise. By following these guidelines, we show how noise can be reduced by using
routing algorithms which select minimal paths with a higher probability. We
exploit this knowledge to design an algorithm which changes the probability of
selecting minimal paths according to the application characteristics. We
validate our solution on microbenchmarks and real-world applications on two
systems relying on a Dragonfly interconnection network, showing noise reduction
and performance improvement.
"
1848,"Google vs IBM: A Constraint Solving Challenge on the Job-Shop Scheduling
  Problem","  The job-shop scheduling is one of the most studied optimization problems from
the dawn of computer era to the present day. Its combinatorial nature makes it
easily expressible as a constraint satisfaction problem. In this paper, we
compare the performance of two constraint solvers on the job-shop scheduling
problem. The solvers in question are: OR-Tools, an open-source solver developed
by Google and winner of the last MiniZinc Challenge, and CP Optimizer, a
proprietary IBM constraint solver targeted at industrial scheduling problems.
The comparison is based on the goodness of the solutions found and the time
required to solve the problem instances. First, we target the classic
benchmarks from the literature, then we carry out the comparison on a benchmark
that was created with known optimal solution, with size comparable to
real-world industrial problems.
"
1849,Branch prediction related Optimizations for Multithreaded Processors,"  Major chip manufacturers have all introduced Multithreaded processors. These
processors are used for running a variety of workloads. Efficient resource
utilization is an important design aspect in such processors. Depending on the
workload, mis-speculated execution can severely impact resource utilization and
power utilization. In general, compared to a uniprocessor, a multithreaded
processor may have better tolerance towards mis-speculation. However there can
still be phases where even a multi-threaded processor performance may get
impacted by branch induced mis-speculation. In this paper I propose monitoring
the branch predictor behavior of various hardware threads running on the
multi-threaded processor and use that information as a feedback to the thread
arbiter/picker which schedules the next thread to fetch instructions from. If I
find that a particular thread is going through a phase where it is consistently
mis-predicting its branches and its average branch misprediction stall is above
a specific threshold then I temporarily reduce the priority for picking that
thread. I do a qualitative comparison of various solutions to the problem of
resource inefficiency caused due to mis-speculated branches in multithreaded
processors. This work can be extended by doing a quantitative evaluation.
"
1850,"SPSC: a new execution policy for exploring discrete-time stochastic
  simulations","  In this paper, we introduce a new method called SPSC (Simulation,
Partitioning, Selection, Cloning) to estimate efficiently the probability of
possible solutions in stochastic simulations. This method can be applied to any
type of simulation, however it is particularly suitable for multi-agent-based
simulations (MABS). Therefore, its performance is evaluated on a well-known
MABS and compared to the classical approach, i.e., Monte Carlo.
"
1851,"An inversion formula with hypergeometric polynomials and application to
  singular integral operators","  Given parameters $x \notin \mathbb{R}^- \cup \{1\}$ and $\nu$,
$\mathrm{Re}(\nu) < 0$, and the space $\mathscr{H}_0$ of entire functions in
$\mathbb{C}$ vanishing at $0$, we consider the family of operators
$\mathfrak{L} = c_0 \cdot \delta \circ \mathfrak{M}$ with constant $c_0 =
\nu(1-\nu)x/(1-x)$, $\delta = z \, \mathrm{d}/\mathrm{d}z$ and integral
operator $\mathfrak{M}$ defined by $$ \mathfrak{M}f(z) = \int_0^1 e^{-
\frac{z}{x}t^{-\nu}(1-(1-x)t)} \, f \left ( \frac{z}{x} \, t^{-\nu}(1-t) \right
) \, \frac{\mathrm{d}t}{t}, \qquad z \in \mathbb{C}, $$ for all $f \in
\mathscr{H}_0$. Inverting $\mathfrak{L}$ or $\mathfrak{M}$ proves equivalent to
solve a singular Volterra equation of the first kind.
  The inversion of operator $\mathfrak{L}$ on $\mathscr{H}_0$ leads us to
derive a new class of linear inversion formulas $T = A(x,\nu) \cdot S
\Leftrightarrow S = B(x,\nu) \cdot T$ between sequences $S = (S_n)_{n \in
\mathbb{N}^*}$ and $T = (T_n)_{n \in \mathbb{N}^*}$, where the infinite
lower-triangular matrix $A(x,\nu)$ and its inverse $B(x,\nu)$ involve
Hypergeometric polynomials $F(\cdot)$, namely $$
  \left\{
  \begin{array}{ll}
  A_{n,k}(x,\nu) = \displaystyle (-1)^k\binom{n}{k}F(k-n,-n\nu;-n;x),
  B_{n,k}(x,\nu) = \displaystyle (-1)^k\binom{n}{k}F(k-n,k\nu;k;x)
  \end{array} \right. $$ for $1 \leqslant k \leqslant n$. Functional relations
between the ordinary (resp. exponential) generating functions of the related
sequences $S$ and $T$ are also given. These relations finally enable us to
derive the integral representation $$ \mathfrak{L}^{-1}f(z) = \frac{1-x}{2i\pi
x} \, e^{z} \int_{(0+)}^1 \frac{e^{-xtz}}{t(1-t)} \, f \left ( xz \,
(-t)^{\nu}(1-t)^{1-\nu} \right ) \, \mathrm{d}t, \quad z \in \mathbb{C}, $$ for
the inverse $\mathfrak{L}^{-1}$ of operator $\mathfrak{L}$ on $\mathscr{H}_0$,
where the integration contour encircles the point 0.
"
1852,HEAX: An Architecture for Computing on Encrypted Data,"  With the rapid increase in cloud computing, concerns surrounding data
privacy, security, and confidentiality also have been increased significantly.
Not only cloud providers are susceptible to internal and external hacks, but
also in some scenarios, data owners cannot outsource the computation due to
privacy laws such as GDPR, HIPAA, or CCPA. Fully Homomorphic Encryption (FHE)
is a groundbreaking invention in cryptography that, unlike traditional
cryptosystems, enables computation on encrypted data without ever decrypting
it. However, the most critical obstacle in deploying FHE at large-scale is the
enormous computation overhead.
  In this paper, we present HEAX, a novel hardware architecture for FHE that
achieves unprecedented performance improvement. HEAX leverages multiple levels
of parallelism, ranging from ciphertext-level to fine-grained modular
arithmetic level. Our first contribution is a new highly-parallelizable
architecture for number-theoretic transform (NTT) which can be of independent
interest as NTT is frequently used in many lattice-based cryptography systems.
Building on top of NTT engine, we design a novel architecture for computation
on homomorphically encrypted data. We also introduce several techniques to
enable an end-to-end, fully pipelined design as well as reducing on-chip memory
consumption. Our implementation on reconfigurable hardware demonstrates
164-268x performance improvement for a wide range of FHE parameters.
"
1853,Scale MLPerf-0.6 models on Google TPU-v3 Pods,"  The recent submission of Google TPU-v3 Pods to the industry wide MLPerf v0.6
training benchmark demonstrates the scalability of a suite of industry relevant
ML models. MLPerf defines a suite of models, datasets and rules to follow when
benchmarking to ensure results are comparable across hardware, frameworks and
companies. Using this suite of models, we discuss the optimizations and
techniques including choice of optimizer, spatial partitioning and weight
update sharding necessary to scale to 1024 TPU chips. Furthermore, we identify
properties of models that make scaling them challenging, such as limited data
parallelism and unscaled weights. These optimizations contribute to record
performance in transformer, Resnet-50 and SSD in the Google MLPerf-0.6
submission.
"
1854,"SplitFS: Reducing Software Overhead in File Systems for Persistent
  Memory","  We present SplitFS, a file system for persistent memory (PM) that reduces
software overhead significantly compared to state-of-the-art PM file systems.
SplitFS presents a novel split of responsibilities between a user-space library
file system and an existing kernel PM file system. The user-space library file
system handles data operations by intercepting POSIX calls, memory-mapping the
underlying file, and serving the read and overwrites using processor loads and
stores. Metadata operations are handled by the kernel PM file system (ext4
DAX). SplitFS introduces a new primitive termed relink to efficiently support
file appends and atomic data operations. SplitFS provides three consistency
modes, which different applications can choose from, without interfering with
each other. SplitFS reduces software overhead by up-to 4x compared to the NOVA
PM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks
and applications such as the LevelDB key-value store running the YCSB
benchmark, SplitFS increases application performance by up to 2x compared to
ext4 DAX and NOVA while providing similar consistency guarantees.
"
1855,AI Matrix: A Deep Learning Benchmark for Alibaba Data Centers,"  Alibaba has China's largest e-commerce platform. To support its diverse
businesses, Alibaba has its own large-scale data centers providing the
computing foundation for a wide variety of software applications. Among these
applications, deep learning (DL) has been playing an important role in
delivering services like image recognition, objection detection, text
recognition, recommendation, and language processing. To build more efficient
data centers that deliver higher performance for these DL applications, it is
important to understand their computational needs and use that information to
guide the design of future computing infrastructure. An effective way to
achieve this is through benchmarks that can fully represent Alibaba's DL
applications.
"
1856,"Eco: A Hardware-Software Co-Design for In Situ Power Measurement on
  Low-end IoT Systems","  Energy-constrained sensor nodes can adaptively optimize their energy
consumption if a continuous measurement exists. This is of particular
importance in scenarios of high dynamics such as energy harvesting or adaptive
task scheduling. However, self-measuring of power consumption at reasonable
cost and complexity is unavailable as a generic system service. In this paper,
we present Eco, a hardware-software co-design enabling generic energy
management on IoT nodes. Eco is tailored to devices with limited resources and
thus targets most of the upcoming IoT scenarios. The proposed measurement
module combines commodity components with a common system interfaces to achieve
easy, flexible integration with various hardware platforms and the RIOT IoT
operating system. We thoroughly evaluate and compare accuracy and overhead. Our
findings indicate that our commodity design competes well with highly optimized
solutions, while being significantly more versatile. We employ Eco for energy
management on RIOT and validate its readiness for deployment in a five-week
field trial integrated with energy harvesting.
"
1857,"Message Scheduling for Performant, Many-Core Belief Propagation","  Belief Propagation (BP) is a message-passing algorithm for approximate
inference over Probabilistic Graphical Models (PGMs), finding many applications
such as computer vision, error-correcting codes, and protein-folding. While
general, the convergence and speed of the algorithm has limited its practical
use on difficult inference problems. As an algorithm that is highly amenable to
parallelization, many-core Graphical Processing Units (GPUs) could
significantly improve BP performance. Improving BP through many-core systems is
non-trivial: the scheduling of messages in the algorithm strongly affects
performance. We present a study of message scheduling for BP on GPUs. We
demonstrate that BP exhibits a tradeoff between speed and convergence based on
parallelism and show that existing message schedulings are not able to utilize
this tradeoff. To this end, we present a novel randomized message scheduling
approach, Randomized BP (RnBP), which outperforms existing methods on the GPU.
"
1858,"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in
  Spatiotemporal Systems","  Extracting actionable insight from complex unlabeled scientific data is an
open challenge and key to unlocking data-driven discovery in science.
Complementary and alternative to supervised machine learning approaches,
unsupervised physics-based methods based on behavior-driven theories hold great
promise. Due to computational limitations, practical application on real-world
domain science problems has lagged far behind theoretical development. We
present our first step towards bridging this divide - DisCo - a
high-performance distributed workflow for the behavior-driven local causal
state theory. DisCo provides a scalable unsupervised physics-based
representation learning method that decomposes spatiotemporal systems into
their structurally relevant components, which are captured by the latent local
causal state variables. Complex spatiotemporal systems are generally highly
structured and organize around a lower-dimensional skeleton of coherent
structures, and in several firsts we demonstrate the efficacy of DisCo in
capturing such structures from observational and simulated scientific data. To
the best of our knowledge, DisCo is also the first application software
developed entirely in Python to scale to over 1000 machine nodes, providing
good performance along with ensuring domain scientists' productivity. We
developed scalable, performant methods optimized for Intel many-core processors
that will be upstreamed to open-source Python library packages. Our capstone
experiment, using newly developed DisCo workflow and libraries, performs
unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,
processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel
Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%
strong-scaling efficiency.
"
1859,"NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement
  Learning","  One of the key challenges arising when compilers vectorize loops for today's
SIMD-compatible architectures is to decide if vectorization or interleaving is
beneficial. Then, the compiler has to determine how many instructions to pack
together and how many loop iterations to interleave. Compilers are designed
today to use fixed-cost models that are based on heuristics to make
vectorization decisions on loops. However, these models are unable to capture
the data dependency, the computation graph, or the organization of
instructions. Alternatively, software engineers often hand-write the
vectorization factors of every loop. This, however, places a huge burden on
them, since it requires prior experience and significantly increases the
development time. In this work, we explore a novel approach for handling loop
vectorization and propose an end-to-end solution using deep reinforcement
learning (RL). We conjecture that deep RL can capture different instructions,
dependencies, and data structures to enable learning a sophisticated model that
can better predict the actual performance cost and determine the optimal
vectorization factors. We develop an end-to-end framework, from code to
vectorization, that integrates deep RL in the LLVM compiler. Our proposed
framework takes benchmark codes as input and extracts the loop codes. These
loop codes are then fed to a loop embedding generator that learns an embedding
for these loops. Finally, the learned embeddings are used as input to a Deep RL
agent, which determines the vectorization factors for all the loops. We further
extend our framework to support multiple supervised learning methods. We
evaluate our approaches against the currently used LLVM vectorizer and loop
polyhedral optimization techniques. Our experiments show 1.29X-4.73X
performance speedup compared to baseline and only 3% worse than the brute-force
search on a wide range of benchmarks.
"
1860,Serving Recurrent Neural Networks Efficiently with a Spatial Accelerator,"  Recurrent Neural Network (RNN) applications form a major class of AI-powered,
low-latency data center workloads. Most execution models for RNN acceleration
break computation graphs into BLAS kernels, which lead to significant
inter-kernel data movement and resource underutilization. We show that by
supporting more general loop constructs that capture design parameters in
accelerators, it is possible to improve resource utilization using cross-kernel
optimization without sacrificing programmability. Such abstraction level
enables a design space search that can lead to efficient usage of on-chip
resources on a spatial architecture across a range of problem sizes. We
evaluate our optimization strategy on such abstraction with DeepBench using a
configurable spatial accelerator. We demonstrate that this implementation
provides a geometric speedup of 30x in performance, 1.6x in area, and 2x in
power efficiency compared to a Tesla V100 GPU, and a geometric speedup of 2x
compared to Microsoft Brainwave implementation on a Stratix 10 FPGA.
"
1861,NGEMM: Optimizing GEMM for Deep Learning via Compiler-based Techniques,"  Quantization has emerged to be an effective way to significantly boost the
performance of deep neural networks (DNNs) by utilizing low-bit computations.
Despite having lower numerical precision, quantized DNNs are able to reduce
both memory bandwidth and computation cycles with little losses of accuracy.
Integer GEMM (General Matrix Multiplication) is critical to running quantized
DNN models efficiently, as GEMM operations often dominate the computations in
these models. Various approaches have been developed by leveraging techniques
such as vectorization and memory layout to improve the performance of integer
GEMM. However, these existing approaches are not fast enough in certain
scenarios. We developed NGEMM, a compiler-based GEMM implementation for
accelerating lower-precision training and inference. NGEMM has better use of
the vector units by avoiding unnecessary vector computation that is introduced
during tree reduction. We compared NGEMM's performance with the state-of-art
BLAS libraries such as MKL. Our experimental results showed that NGEMM
outperformed MKL non-pack and pack version by an average of 1.86x and 1.16x,
respectively. We have applied NGEMM to a number of production services in
Microsoft.
"
1862,"Automatic Throughput and Critical Path Analysis of x86 and ARM Assembly
  Kernels","  Useful models of loop kernel runtimes on out-of-order architectures require
an analysis of the in-core performance behavior of instructions and their
dependencies. While an instruction throughput prediction sets a lower bound to
the kernel runtime, the critical path defines an upper bound. Such predictions
are an essential part of analytic (i.e., white-box) performance models like the
Roofline and Execution-Cache-Memory (ECM) models. They enable a better
understanding of the performance-relevant interactions between hardware
architecture and loop code. The Open Source Architecture Code Analyzer (OSACA)
is a static analysis tool for predicting the execution time of sequential
loops. It previously supported only x86 (Intel and AMD) architectures and
simple, optimistic full-throughput execution. We have heavily extended OSACA to
support ARM instructions and critical path prediction including the detection
of loop-carried dependencies, which turns it into a versatile
cross-architecture modeling tool. We show runtime predictions for code on Intel
Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on
machine models from available documentation and semi-automatic benchmarking.
The predictions are compared with actual measurements.
"
1863,Memory Centric Characterization and Analysis of SPEC CPU2017 Suite,"  In this paper we provide a comprehensive, memory-centric characterization of
the SPEC CPU2017 benchmark suite, using a number of mechanisms including
dynamic binary instrumentation, measurements on native hardware using hardware
performance counters and OS based tools.
  We present a number of results including working set sizes, memory capacity
consumption and, memory bandwidth utilization of various workloads. Our
experiments reveal that the SPEC CPU2017 workloads are surprisingly memory
intensive, with approximately 50% of all dynamic instructions being memory
intensive ones. We also show that there is a large variation in the memory
footprint and bandwidth utilization profiles of the entire suite, with some
benchmarks using as much as 16 GB of main memory and up to 2.3 GB/s of memory
bandwidth.
  We also perform instruction execution and distribution analysis of the suite
and find that the average instruction count for SPEC CPU2017 workloads is an
order of magnitude higher than SPEC CPU2006 ones. In addition, we also find
that FP benchmarks of the SPEC 2017 suite have higher compute requirements: on
average, FP workloads execute three times the number of compute operations as
compared to INT workloads.
"
1864,"Modernizing Titan2D, a Parallel AMR Geophysical Flow Code to Support
  Multiple Rheologies and Extendability","  In this work, we report on strategies and results of our initial approach for
modernization of Titan2D code. Titan2D is a geophysical mass flow simulation
code designed for modeling of volcanic flows, debris avalanches and landslides
over a realistic terrain model. It solves an underlying hyperbolic system of
partial differential equations using parallel adaptive mesh Godunov scheme. The
following work was done during code refactoring and modernization. To
facilitate user input two level python interface was developed. Such design
permits large changes in C++ and Python low-level while maintaining stable
high-level interface exposed to the end user. Multiple diverged forks
implementing different material models were merged back together. Data storage
layout was changed from a linked list of structures to a structure of arrays
representation for better memory access and in preparation for further work on
better utilization of vectorized instruction. Existing MPI parallelization was
augmented with OpenMP parallelization. The performance of a hash table used to
store mesh elements and nodes references was improved by switching from a
linked list for overflow entries to dynamic arrays allowing the implementation
of the binary search algorithm. The introduction of the new data layout made
possible to reduce the number of hash table look-ups by replacing them with
direct use of indexes from the storage class. The modifications lead to 8-9
times performance improvement for serial execution.
"
1865,Accelerating Data Loading in Deep Neural Network Training,"  Data loading can dominate deep neural network training time on large-scale
systems. We present a comprehensive study on accelerating data loading
performance in large-scale distributed training. We first identify performance
and scalability issues in current data loading implementations. We then propose
optimizations that utilize CPU resources to the data loader design. We use an
analytical model to characterize the impact of data loading on the overall
training time and establish the performance trend as we scale up distributed
training. Our model suggests that I/O rate limits the scalability of
distributed training, which inspires us to design a locality-aware data loading
method. By utilizing software caches, our method can drastically reduce the
data loading communication volume in comparison with the original data loading
implementation. Finally, we evaluate the proposed optimizations with various
experiments. We achieved more than 30x speedup in data loading using 256 nodes
with 1,024 learners.
"
1866,"Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by
  Exploiting Ray Locality","  State-of-the-art ray tracing techniques operate on hierarchical acceleration
structures such as BVH trees which wrap objects in a scene into bounding
volumes of decreasing sizes. Acceleration structures reduce the amount of
ray-scene intersections that a ray has to perform to find the intersecting
object. However, we observe a large amount of redundancy when rays are
traversing these acceleration structures. While modern acceleration structures
explore the spatial organization of the scene, they neglect similarities
between rays that traverse the structures and thereby cause redundant
traversals. This paper provides a limit study of a new promising technique,
Hash-Based Ray Path Prediction (HRPP), which exploits the similarity between
rays to predict leaf nodes to avoid redundant acceleration structure
traversals. Our data shows that acceleration structure traversal consumes a
significant proportion of the ray tracing rendering time regardless of the
platform or the target image quality. Our study quantifies unused ray locality
and evaluates the theoretical potential for improved ray traversal performance
for both coherent and seemingly incoherent rays. We show that HRPP is able to
skip, on average, 40% of all hit-all traversal computations.
"
1867,Blockchains and Distributed Databases: a Twin Study,"  Blockchain has come a long way: a system that was initially proposed
specifically for cryptocurrencies is now being adapted and adopted as a
general-purpose transactional system. A blockchain is also a distributed
system, and as such it shares some similarities with distributed database
systems. Existing works that compare blockchains and distributed database
systems focus mainly on high-level properties, such as security and throughput.
They stop short of showing how the underlying design choices contribute to the
overall differences. Our paper is to fill this important gap. In this paper, we
perform a twin study of blockchains and distributed database systems as two
types of transactional systems. We propose a taxonomy that helps illustrate
their similarities and differences. In particular, we compare the systems along
four dimensions: replication, concurrency, storage, and sharding. We discuss
how the design choices have been driven by the system's goals: blockchain's
goal is security, whereas the distributed database's goal is performance. We
then conduct an extensive and in-depth performance study on two blockchains,
namely Quorum and Hyperledger Fabric, and three distributed databases, namely
CockroachDB, TiDB and etcd. We demonstrate how the different design choices in
the four dimensions lead to different performance. In addition, we show that
for most workloads, blockchain's performance is still lagging far behind that
of a distributed database. However, the gap is not as significant as previously
reported, and under high contention or constrained workloads, blockchains and
databases are even comparable. Our work provides a framework for exploring the
design space of hybrid database-blockchain systems.
"
1868,MLPerf Training Benchmark,"  Machine learning (ML) needs industry-standard performance benchmarks to
support design and competitive evaluation of the many emerging software and
hardware solutions for ML. But ML training presents three unique benchmarking
challenges absent from other domains: optimizations that improve training
throughput can increase the time to solution, training is stochastic and time
to solution exhibits high variance, and software and hardware systems are so
diverse that fair benchmarking with the same binary, code, and even
hyperparameters is difficult. We therefore present MLPerf, an ML benchmark that
overcomes these challenges. Our analysis quantitatively evaluates MLPerf's
efficacy at driving performance and scalability improvements across two rounds
of results from multiple vendors.
"
1869,"False Data Injection Attacks in Internet of Things and Deep Learning
  enabled Predictive Analytics","  Industry 4.0 is the latest industrial revolution primarily merging automation
with advanced manufacturing to reduce direct human effort and resources.
Predictive maintenance (PdM) is an industry 4.0 solution, which facilitates
predicting faults in a component or a system powered by state-of-the-art
machine learning (ML) algorithms and the Internet-of-Things (IoT) sensors.
However, IoT sensors and deep learning (DL) algorithms, both are known for
their vulnerabilities to cyber-attacks. In the context of PdM systems, such
attacks can have catastrophic consequences as they are hard to detect due to
the nature of the attack. To date, the majority of the published literature
focuses on the accuracy of DL enabled PdM systems and often ignores the effect
of such attacks. In this paper, we demonstrate the effect of IoT sensor attacks
on a PdM system. At first, we use three state-of-the-art DL algorithms,
specifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and
Convolutional Neural Network (CNN) for predicting the Remaining Useful Life
(RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results
show that the GRU-based PdM model outperforms some of the recent literature on
RUL prediction using the C-MAPSS dataset. Afterward, we model two different
types of false data injection attacks (FDIA) on turbofan engine sensor data and
evaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained
results demonstrate that FDI attacks on even a few IoT sensors can strongly
defect the RUL prediction. However, the GRU-based PdM model performs better in
terms of accuracy and resiliency. Lastly, we perform a study on the GRU-based
PdM model using four different GRU networks with different sequence lengths.
Our experiments reveal an interesting relationship between the accuracy,
resiliency and sequence length for the GRU-based PdM models.
"
1870,GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory,"  We present an implementation of the overlap-and-save method, a method for the
convolution of very long signals with short response functions, which is
tailored to GPUs. We have implemented several FFT algorithms (using the CUDA
programming language) which exploit GPU shared memory, allowing for GPU
accelerated convolution. We compare our implementation with an implementation
of the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We
demonstrate that by using a shared memory based FFT we can achieved significant
speed-ups for certain problem sizes and lower the memory requirements of the
overlap-and-save method on GPUs.
"
1871,"Dockless Bike-Sharing Systems with Unusable Bikes: Removing, Repair and
  Redistribution under Batch Policies","  This paper discusses a large-scale dockless bike-sharing system (DBSS) with
unusable bikes, which can be removed, repaired, redistributed and reused under
two batch policies: One for removing the unusable bikes from each parking
region to a maintenance shop, and the other for redistributing the repaired
bikes from the maintenance shop to some suitable parking regions. For such a
bike-sharing system, this paper proposes and develops a new computational
method by applying the RG-factorizations of block-structured Markov processes
in the closed queueing networks. Different from previous works in the
literature of queueing networks, a key contribution of our computational method
is to set up a new nonlinear matrix equation to determine the relative arrival
rates, and to show that the nonlinearity comes from two different groups of
processes: The failure and removing processes; and the repair and
redistributing processes. Once the relative arrival rate is introduced to each
node, these nodes are isolated from each other, so that the Markov processes of
all the nodes are independent of each other, thus the Markov system of each
node is described as an elegant block-structured Markov process whose
stationary probabilities can be easily computed by the RG-factorizations. Based
on this, this paper can establish a more general product-form solution of the
closed queueing network, and provides performance analysis of the DBSS through
a comprehensive discussion for the bikes' failure, removing, repair,
redistributing and reuse processes under two batch policies. We hope that our
method opens a new avenue to quantitative evaluation of more general DBSSs with
unusable bikes.
"
1872,Clustering case statements for indirect branch predictors,"  We present an O(nlogn) algorithm to compile a switch statement into jump
tables. To generate jump tables that can be efficiently predicted by current
hardware branch predictors, we added an upper bound on the number of entries
for each table. This modification of the previously best known algorithm
reduces the complexity from O(n^2) to O(nlogn).
"
1873,Optimising energy and overhead for large parameter space simulations,"  Many systems require optimisation over multiple objectives, where objectives
are characteristics of the system such as energy consumed or increase in time
to perform the work. Optimisation is performed by selecting the `best' set of
input parameters to elicit the desired objectives. However, the parameter
search space can often be far larger than can be searched in a reasonable time.
Additionally, the objectives are often mutually exclusive -- leading to a
decision being made as to which objective is more important or optimising over
a combination of the objectives. This work is an application of a Genetic
Algorithm to identify the Pareto frontier for finding the optimal parameter
sets for all combinations of objectives. A Pareto frontier can be used to
identify the sets of optimal parameters for which each is the `best' for a
given combination of objectives -- thus allowing decisions to be made with full
knowledge. We demonstrate this approach for the HTC-Sim simulation system in
the case where a Reinforcement Learning scheduler is tuned for the two
objectives of energy consumption and task overhead. Demonstrating that this
approach can reduce the energy consumed by ~36% over previously published work
without significantly increasing the overhead.
"
1874,Synthesizing Credit Card Transactions,"  Two elements have been essential to AI's recent boom: (1) deep neural nets
and the theory and practice behind them; and (2) cloud computing with its
abundant labeled data and large computing resources.
  Abundant labeled data is available for key domains such as images, speech,
natural language processing, and recommendation engines. However, there are
many other domains where such data is not available, or access to it is highly
restricted for privacy reasons, as with health and financial data. Even when
abundant data is available, it is often not labeled. Doing such labeling is
labor-intensive and non-scalable.
  As a result, to the best of our knowledge, key domains still lack labeled
data or have at most toy data; or the synthetic data must have access to real
data from which it can mimic new data. This paper outlines work to generate
realistic synthetic data for an important domain: credit card transactions.
  Some challenges: there are many patterns and correlations in real purchases.
There are millions of merchants and innumerable locations. Those merchants
offer a wide variety of goods. Who shops where and when? How much do people
pay? What is a realistic fraudulent transaction?
  We use a mixture of technical approaches and domain knowledge including
mechanics of credit card processing, a broad set of consumer domains:
electronics, clothing, hair styling, etc. Connecting everything is a virtual
world. This paper outlines some of our key techniques and provides evidence
that the data generated is indeed realistic.
  Beyond the scope of this paper: (1) use of our data to develop and train
models to predict fraud; (2) coupling models and the synthetic dataset to
assess performance in designing accelerators such as GPUs and TPUs.
"
1875,"Semi-Analytical Model for Design and Analysis of On-Orbit Servicing
  Architecture","  Robotic on-orbit servicing (OOS) is expected to be a key technology and
concept for future sustainable space exploration. This paper develops a
semi-analytical model for OOS systems analysis, responding to the growing needs
and ongoing trend of robotic OOS. An OOS infrastructure system is considered
whose goal is to provide responsive services to the random failures of a set of
customer modular satellites distributed in space (e.g., at the geosynchronous
equatorial orbit). The considered OOS architecture is comprised of a servicer
that travels and provides module-replacement services to the customer
satellites, an on-orbit depot to store the spares, and a series of launch
vehicles to replenish the depot. The OOS system performance is analyzed by
evaluating the mean waiting time before service completion for a given failure
and its relationship with the depot capacity. Leveraging the queueing theory
and inventory management methods, the developed semi-analytical model is
capable of analyzing the OOS system performance without relying on
computationally costly simulations. The effectiveness of the proposed model is
demonstrated using a case study compared with simulation results. This paper is
expected to provide a critical step to push the research frontier of
analytical/semi-analytical models development for complex space systems design.
"
1876,Performance Impact of Memory Channels on Sparse and Irregular Algorithms,"  Graph processing is typically considered to be a memory-bound rather than
compute-bound problem. One common line of thought is that more available memory
bandwidth corresponds to better graph processing performance. However, in this
work we demonstrate that the key factor in the utilization of the memory system
for graph algorithms is not necessarily the raw bandwidth or even the latency
of memory requests. Instead, we show that performance is proportional to the
number of memory channels available to handle small data transfers with limited
spatial locality.
  Using several widely used graph frameworks, including Gunrock (on the GPU)
and GAPBS \& Ligra (for CPUs), we evaluate key graph analytics kernels using
two unique memory hierarchies, DDR-based and HBM/MCDRAM. Our results show that
the differences in the peak bandwidths of several Pascal-generation GPU memory
subsystems aren't reflected in the performance of various analytics.
Furthermore, our experiments on CPU and Xeon Phi systems demonstrate that the
number of memory channels utilized can be a decisive factor in performance
across several different applications. For CPU systems with smaller thread
counts, the memory channels can be underutilized while systems with high thread
counts can oversaturate the memory subsystem, which leads to limited
performance. Finally, we model the potential performance improvements of adding
more memory channels with narrower access widths than are found in current
platforms, and we analyze performance trade-offs for the two most prominent
types of memory accesses found in graph algorithms, streaming and random
accesses.
"
1877,Bit Efficient Quantization for Deep Neural Networks,"  Quantization for deep neural networks have afforded models for edge devices
that use less on-board memory and enable efficient low-power inference. In this
paper, we present a comparison of model-parameter driven quantization
approaches that can achieve as low as 3-bit precision without affecting
accuracy. The post-training quantization approaches are data-free, and the
resulting weight values are closely tied to the dataset distribution on which
the model has converged to optimality. We show quantization results for a
number of state-of-art deep neural networks (DNN) using large dataset like
ImageNet. To better analyze quantization results, we describe the overall range
and local sparsity of values afforded through various quantization schemes. We
show the methods to lower bit-precision beyond quantization limits with object
class clustering.
"
1878,"Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory
  Machines","  Multi-socket machines with 1-100 TBs of physical memory are becoming
prevalent. Applications running on multi-socket machines suffer non-uniform
bandwidth and latency when accessing physical memory. Decades of research have
focused on data allocation and placement policies in NUMA settings, but there
have been no studies on the question of how to place page-tables amongst
sockets. We make the case for explicit page-table allocation policies and show
that page-table placement is becoming crucial to overall performance. We
propose Mitosis to mitigate NUMA effects on page-table walks by transparently
replicating and migrating page-tables across sockets without application
changes. This reduces the frequency of accesses to remote NUMA nodes when
performing page-table walks. Mitosis uses two components: (i) a mechanism to
enable efficient page-table replication and migration; and (ii) policies for
processes to efficiently manage and control page-table replication and
migration. We implement Mitosis in Linux and evaluate its benefits on real
hardware. Mitosis improves performance for large-scale multi-socket workloads
by up to 1.34x by replicating page-tables across sockets. Moreover, it improves
performance by up to 3.24x in cases when the OS migrates a process across
sockets by enabling cross-socket page-table migration.
"
1879,ClassyTune: A Performance Auto-Tuner for Systems in the Cloud,"  Performance tuning can improve the system performance and thus enable the
reduction of cloud computing resources needed to support an application. Due to
the ever increasing number of parameters and complexity of systems, there is a
necessity to automate performance tuning for the complicated systems in the
cloud. The state-of-the-art tuning methods are adopting either the
experience-driven tuning approach or the data-driven one. Data-driven tuning is
attracting increasing attentions, as it has wider applicability. But existing
data-driven methods cannot fully address the challenges of sample scarcity and
high dimensionality simultaneously. We present ClassyTune, a data-driven
automatic configuration tuning tool for cloud systems. ClassyTune exploits the
machine learning model of classification for auto-tuning. This exploitation
enables the induction of more training samples without increasing the input
dimension. Experiments on seven popular systems in the cloud show that
ClassyTune can effectively tune system performance to seven times higher for
high-dimensional configuration space, outperforming expert tuning and the
state-of-the-art auto-tuning solutions. We also describe a use case in which
performance tuning enables the reduction of 33% computing resources needed to
run an online stateless service.
"
1880,"On the Secrecy Performance of Random VLC Networks with Imperfect CSI and
  Protected Zone","  This paper investigates the physical-layer security for a random indoor
visible light communication (VLC) network with imperfect channel state
information (CSI) and a protected zone. The VLC network consists of three
nodes, i.e., a transmitter (Alice), a legitimate receiver (Bob), and an
eavesdropper (Eve). Alice is fixed in the center of the ceiling, and the
emitted signal at Alice satisfies the non-negativity and the dimmable average
optical intensity constraint. Bob and Eve are randomly deployed on the receiver
plane. By employing the protected zone and considering the imperfect CSI, the
stochastic characteristics of the channel gains for both the main and the
eavesdropping channels is first analyzed. After that, the closed-form
expressions of the average secrecy capacity and the lower bound of secrecy
outage probability are derived, respectively. Finally, Monte-Carlo simulations
are provided to verify the accuracy of the derived theoretical expressions.
Moreover, the impacts of the nominal optical intensity, the dimming target, the
protected zone and the imperfect CSI on secrecy performance are discussed,
respectively.
"
1881,"Constrained Linear Movement Model (CALM): Simulation of passenger
  movement in airplanes","  Pedestrian dynamics models the walking movement of individuals in a crowd. It
has recently been used in the analysis of procedures to reduce the risk of
disease spread in airplanes, relying on the SPED model. This is a social force
model inspired by molecular dynamics; pedestrians are treated as point
particles, and their trajectories are determined in a simulation. A parameter
sweep is performed to address uncertainties in human behavior, which requires a
large number of simulations.
  The SPED model's slow speed is a bottleneck to performing a large parameter
sweep. This is a severe impediment to delivering real-time results, which are
often required in the course of decision meetings, especially during
emergencies. We propose a new model, called CALM, to remove this limitation. It
is designed to simulate a crowd's movement in constrained linear passageways,
such as inside an aircraft. We show that CALM yields realistic results while
improving performance by two orders of magnitude over the SPED model.
"
1882,"Load Balancing Performance in Distributed Storage with Regular Balanced
  Redundancy","  Contention at the storage nodes is the main cause of long and variable data
access times in distributed storage systems. Offered load on the system must be
balanced across the storage nodes in order to minimize contention, and load
balance in the system should be robust against the skews and fluctuations in
content popularities. Data objects are replicated across multiple nodes in
practice to allow for load balancing. However redundancy increases the storage
requirement and should be used efficiently. We evaluate load balancing
performance of natural storage schemes in which each data object is stored at
$d$ different nodes and each node stores the same number of objects. We find
that load balance in a system of $n$ nodes improves multiplicatively with $d$
as long as $d = o\left(\log(n)\right)$, and improves exponentially as soon as
$d = \Theta\left(\log(n)\right)$. We show that the load balance in the system
improves the same way with $d$ when the service choices are created with XOR's
of $r$ objects rather than object replicas, which also reduces the storage
overhead multiplicatively by $r$. However, unlike accessing an object replica,
access through a recovery set composed by an XOR'ed object copy requires
downloading content from $r$ nodes, which increases the load imbalance in the
system additively by $r$.
"
1883,Characterizing Deep Learning Training Workloads on Alibaba-PAI,"  Modern deep learning models have been exploited in various domains, including
computer vision (CV), natural language processing (NLP), search and
recommendation. In practical AI clusters, workloads training these models are
run using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One
critical issue for efficiently operating practical AI clouds, is to
characterize the computing and data transfer demands of these workloads, and
more importantly, the training performance given the underlying software
framework and hardware configurations. In this paper, we characterize deep
learning training workloads from Platform of Artificial Intelligence (PAI) in
Alibaba. We establish an analytical framework to investigate detailed execution
time breakdown of various workloads using different training architectures, to
identify performance bottleneck. Results show that weight/gradient
communication during training takes almost 62% of the total execution time
among all our workloads on average. The computation part, involving both GPU
computing and memory access, are not the biggest bottleneck based on collective
behavior of the workloads. We further evaluate attainable performance of the
workloads on various potential software/hardware mappings, and explore
implications on software architecture selection and hardware configurations. We
identify that 60% of PS/Worker workloads can be potentially sped up when ported
to the AllReduce architecture exploiting the high-speed NVLink for GPU
interconnect, and on average 1.7X speedup can be achieved when Ethernet
bandwidth is upgraded from 25 Gbps to 100 Gbps.
"
1884,A High-Throughput Solver for Marginalized Graph Kernels on GPU,"  We present the design and optimization of a linear solver on General Purpose
GPUs for the efficient and high-throughput evaluation of the marginalized graph
kernel between pairs of labeled graphs. The solver implements a preconditioned
conjugate gradient (PCG) method to compute the solution to a generalized
Laplacian equation associated with the tensor product of two graphs. To cope
with the gap between the instruction throughput and the memory bandwidth of
current generation GPUs, our solver forms the tensor product linear system
on-the-fly without storing it in memory when performing matrix-vector dot
product operations in PCG. Such on-the-fly computation is accomplished by using
threads in a warp to cooperatively stream the adjacency and edge label matrices
of individual graphs by small square matrix blocks called tiles, which are then
staged in registers and the shared memory for later reuse. Warps across a
thread block can further share tiles via the shared memory to increase data
reuse. We exploit the sparsity of the graphs hierarchically by storing only
non-empty tiles using a coordinate format and nonzero elements within each tile
using bitmaps. Besides, we propose a new partition-based reordering algorithm
for aggregating nonzero elements of the graphs into fewer but denser tiles to
improve the efficiency of the sparse format.
  We carry out extensive theoretical analyses on the graph tensor product
primitives for tiles of various density and evaluate their performance on
synthetic and real-world datasets. Our solver delivers three to four orders of
magnitude speedup over existing CPU-based solvers such as GraKeL and
GraphKernels. The capability of the solver enables kernel-based learning tasks
at unprecedented scales.
"
1885,"TCD-NPE: A Re-configurable and Efficient Neural Processing Engine,
  Powered by Novel Temporal-Carry-deferring MACs","  In this paper, we first propose the design of Temporal-Carry-deferring MAC
(TCD-MAC) and illustrate how our proposed solution can gain significant energy
and performance benefit when utilized to process a stream of input data. We
then propose using the TCD-MAC to build a reconfigurable, high speed, and low
power Neural Processing Engine (TCD-NPE). We, further, propose a novel
scheduler that lists the sequence of needed processing events to process an MLP
model in the least number of computational rounds in our proposed TCD-NPE. We
illustrate that our proposed TCD-NPE significantly outperform similar neural
processing solutions that use conventional MACs in terms of both energy
consumption and execution time.
"
1886,AI Benchmark: All About Deep Learning on Smartphones in 2019,"  The performance of mobile AI accelerators has been evolving rapidly in the
past two years, nearly doubling with each new generation of SoCs. The current
4th generation of mobile NPUs is already approaching the results of
CUDA-compatible Nvidia graphics cards presented not long ago, which together
with the increased capabilities of mobile deep learning frameworks makes it
possible to run complex and deep AI models on mobile devices. In this paper, we
evaluate the performance and compare the results of all chipsets from Qualcomm,
HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware
acceleration for AI inference. We also discuss the recent changes in the
Android ML pipeline and provide an overview of the deployment of deep learning
models on mobile devices. All numerical results provided in this paper can be
found and are regularly updated on the official project website:
http://ai-benchmark.com.
"
1887,"Modern Multicore CPUs are not Energy Proportional: Opportunity for
  Bi-objective Optimization for Performance and Energy","  Energy proportionality is the key design goal followed by architects of
modern multicore CPUs. One of its implications is that optimization of an
application for performance will also optimize it for energy. In this work, we
show that energy proportionality does not hold true for multicore CPUs. This
finding creates the opportunity for bi-objective optimization of applications
for performance and energy. We propose and study the first application-level
method for bi-objective optimization of multithreaded data-parallel
applications for performance and energy. The method uses two decision
variables, the number of identical multithreaded kernels (threadgroups)
executing the application and the number of threads in each threadgroup, with
the workload always partitioned equally between the threadgroups. We
experimentally demonstrate the efficiency of the method using four highly
optimized multithreaded data-parallel applications, 2D fast Fourier transform
based on FFTW and Intel MKL, and dense matrix-matrix multiplication using
OpenBLAS and Intel MKL. Four modern multicore CPUs are used in the experiments.
The experiments show that optimization for performance alone results in the
increase in dynamic energy consumption by up to 89% and optimization for
dynamic energy alone degrades the performance by up to 49%. By solving the
bi-objective optimization problem, the method determines up to 11 globally
Pareto-optimal solutions. Finally, we propose a qualitative dynamic energy
model employing performance monitoring counters as parameters, which we use to
explain the discovered energy nonproportionality and the Pareto-optimal
solutions determined by our method. The model shows that the energy
nonproportionality in our case is due to the activity of the data translation
lookaside buffer (dTLB), which is disproportionately energy expensive.
"
1888,"An Approach for Realistically Simulating the Performance of Scientific
  Applications on High Performance Computing Systems","  Scientific applications often contain large, computationally-intensive, and
irregular parallel loops or tasks that exhibit stochastic characteristics.
Applications may suffer from load imbalance during their execution on
high-performance computing (HPC) systems due to such characteristics. Dynamic
loop self-scheduling (DLS) techniques are instrumental in improving the
performance of scientific applications on HPC systems via load balancing.
Selecting a DLS technique that results in the best performance for different
problems and system sizes requires a large number of exploratory experiments. A
theoretical model that can be used to predict the scheduling technique that
yields the best performance for a given problem and system has not yet been
identified. Therefore, simulation is the most appropriate approach for
conducting such exploratory experiments with reasonable costs. This work
devises an approach to realistically simulate computationally-intensive
scientific applications that employ DLS and execute on HPC systems. Several
approaches to represent the application tasks (or loop iterations) are compared
to establish their influence on the simulative application performance. A novel
simulation strategy is introduced, which transforms a native application code
into a simulative code. The native and simulative performance of two
computationally-intensive scientific applications are compared to evaluate the
realism of the proposed simulation approach. The comparison of the performance
characteristics extracted from the native and simulative performance shows that
the proposed simulation approach fully captured most of the performance
characteristics of interest. This work shows and establishes the importance of
simulations that realistically predict the performance of DLS techniques for
different applications and system configurations.
"
1889,Throughput and Delay Analysis of Slotted Aloha with Batch Service,"  In this paper, we study the throughput and delay performances of the slotted
Aloha with batch service, which has wide applications in random access
networks. Different from the classical slotted Aloha, each node in the slotted
Aloha with batch service can transmit up to M packets once it succeeds in
channel competition. The throughput is substantially improved because up to M
packets jointly undertake the overhead due to contention. In an innovative
vacation model developed in this paper, we consider each batch of data
transmission as a busy period of each node, and the process between two
successive busy periods as a vacation period. We then formulate the number of
arrivals during a vacation period in a renewal-type equation, which
characterizes the dependency between busy periods and vacation periods. Based
on this formulation, we derive the mean waiting time of a packet and the
bounded delay region for the slotted Aloha with batch service. Our results
indicate the throughput and delay performances are substantially improved with
the increase of batch sizeM, and the bounded delay region is enlarged
accordingly. As M goes to infinity, we find the saturated throughput can
approach 100% of channel capacity, and the system remains stable irrespective
of the population size and transmission probability.
"
1890,"Kriptosare.gen, a dockerized Bitcoin testbed: analysis of server
  performance","  Bitcoin is a peer-to-peer distributed cryptocurrency system, that keeps all
transaction history in a public ledger known as blockchain. The Bitcoin network
is implicitly pseudoanonymous and its nodes are controlled by independent
entities making network analysis difficult. This calls for the development of a
fully controlled testing environment.
  This paper presents Kriptosare.gen, a dockerized automatized Bitcoin testbed,
for deploying full-scale custom Bitcoin networks. The testbed is deployed in a
single machine executing four different experiments, each one with different
network configuration. We perform a cost analysis to investigate how the
resources are related with network parameters and provide experimental data
quantifying the amount of computational resources needed to run the different
types of simulations. Obtained results demonstrate that it is possible to run
the testbed with a configuration similar to a real Bitcoin system.
"
1891,"A Tool for Automatically Suggesting Source-Code Optimizations for
  Complex GPU Kernels","  Future computing systems, from handhelds to supercomputers, will undoubtedly
be more parallel and heterogeneous than todays systems to provide more
performance and energy efficiency. Thus, GPUs are increasingly being used to
accelerate general purpose applications, including applications with data
dependent, irregular control flow and memory access patterns. However, the
growing complexity, exposed memory hierarchy, incoherence, heterogeneity, and
parallelism will make accelerator based systems progressively more difficult to
program. In the foreseeable future, the vast majority of programmers will no
longer be able to extract additional performance or energy savings from next
generation systems be-cause the programming will be too difficult. Automatic
performance analysis and optimization recommendation tools have the potential
to avert this situation. They embody expert knowledge and make it available to
software developers when needed. In this paper, we describe and evaluate such a
tool.
"
1892,Visualizing the world's largest turbulence simulation,"  In this exploratory submission we present the visualization of the largest
interstellar turbulence simulations ever performed, unravelling key
astrophysical processes concerning the formation of stars and the relative role
of magnetic fields. The simulations, including pure hydrodynamical (HD) and
magneto-hydrodynamical (MHD) runs, up to a size of $10048^3$ grid elements,
were produced on the supercomputers of the Leibniz Supercomputing Centre and
visualized using the hybrid parallel (MPI+TBB) ray-tracing engine OSPRay
associated with VisIt. Besides revealing features of turbulence with an
unprecedented resolution, the visualizations brilliantly showcase the
stretching-and-folding mechanisms through which astrophysical processes such as
supernova explosions drive turbulence and amplify the magnetic field in the
interstellar gas, and how the first structures, the seeds of newborn stars are
shaped by this process.
"
1893,"Speeding simulation analysis up with yt and Intel Distribution for
  Python","  As modern scientific simulations grow ever more in size and complexity, even
their analysis and post-processing becomes increasingly demanding, calling for
the use of HPC resources and methods. yt is a parallel, open source
post-processing python package for numerical simulations in astrophysics, made
popular by its cross-format compatibility, its active community of developers
and its integration with several other professional Python instruments. The
Intel Distribution for Python enhances yt's performance and parallel
scalability, through the optimization of lower-level libraries Numpy and Scipy,
which make use of the optimized Intel Math Kernel Library (Intel-MKL) and the
Intel MPI library for distributed computing. The library package yt is used for
several analysis tasks, including integration of derived quantities, volumetric
rendering, 2D phase plots, cosmological halo analysis and production of
synthetic X-ray observation. In this paper, we provide a brief tutorial for the
installation of yt and the Intel Distribution for Python, and the execution of
each analysis task. Compared to the Anaconda python distribution, using the
provided solution one can achieve net speedups up to 4.6x on Intel Xeon
Scalable processors (codename Skylake).
"
1894,"BatteryLab, A Distributed Power Monitoring Platform For Mobile Devices","  Recent advances in cloud computing have simplified the way that both software
development and testing are performed. Unfortunately, this is not true for
battery testing for which state of the art test-beds simply consist of one
phone attached to a power meter. These test-beds have limited resources,
access, and are overall hard to maintain; for these reasons, they often sit
idle with no experiment to run. In this paper, we propose to share existing
battery testing setups and build BatteryLab, a distributed platform for battery
measurements. Our vision is to transform independent battery testing setups
into vantage points of a planetary-scale measurement platform offering
heterogeneous devices and testing conditions. In the paper, we design and
deploy a combination of hardware and software solutions to enable BatteryLab's
vision. We then preliminarily evaluate BatteryLab's accuracy of battery
reporting, along with some system benchmarking. We also demonstrate how
BatteryLab can be used by researchers to investigate a simple research
question.
"
1895,"PiBooster: A Light-Weight Approach to Performance Improvements in Page
  Table Management for Paravirtual Virtual-Machines","  In paravirtualization, the page table management components of the guest
operating systems are properly patched for the security guarantees of the
hypervisor. However, none of them pay enough attentions to the performance
improvements, which results in two noticeable performance issues. First, such
security patches exacerbate the problem that the execution paths of the guest
page table (de)allocations become extremely long, which would consequently
increase the latencies of process creations and exits. Second, the patches
introduce many additional IOTLB flushes, leading to extra IOTLB misses, and the
misses would have negative impacts on I/O performance of all peripheral
devices. In this paper, we propose PiBooster, a novel lightweight approach for
improving the performance in page table management. First, PiBooster shortens
the execution paths of the page table (de)allocations by the PiBooster cache,
which maintains dedicated buffers for serving page table (de)allocations.
Second, PiBooster eliminates the additional IOTLB misses with a fine-grained
validation scheme, which performs page table and DMA validations separately,
instead of doing both together. We implement a prototype on Xen with Linux as
the guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel
(350 SLoC). We evaluate the I/O performance in both micro and macro ways. The
micro experiment results indicate that PiBooster is able to completely
eliminate the additional IOTLB flushes in the workload-stable environments, and
effectively reduces (de)allocation time of the page table by 47% on average.
The macro benchmarks show that the latencies of the process creations and exits
are expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and
netperf results indicate that PiBooster has no negative performance impacts on
CPU computation, network I/O, and disk I/O.
"
1896,"Delay-optimal policies in partial fork-join systems with redundancy and
  random slowdowns","  We consider a large distributed service system consisting of $n$ homogeneous
servers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of
rate $\lambda n/k_n$ (for some positive constant $\lambda$ and integer $k_n$).
Each incoming job consists of $k_n$ identical tasks that can be executed in
parallel, and that can be encoded into at least $k_n$ ""replicas"" of the same
size (by introducing redundancy) so that the job is considered to be completed
when any $k_n$ replicas associated with it finish their service. Moreover, we
assume that servers can experience random slowdowns in their processing rate so
that the service time of a replica is the product of its size and a random
slowdown.
  First, we assume that the server slowdowns are shifted exponential and
independent of the replica sizes. In this setting we show that the delay of a
typical job is asymptotically minimized (as $n\to\infty$) when the number of
replicas per task is a constant that only depends on the arrival rate
$\lambda$, and on the expected slowdown of servers.
  Second, we introduce a new model for the server slowdowns in which larger
tasks experience less variable slowdowns than smaller tasks. In this setting we
show that, under the class of policies where all replicas start their service
at the same time, the delay of a typical job is asymptotically minimized (as
$n\to\infty$) when the number of replicas per task is made to depend on the
actual size of the tasks being replicated, with smaller tasks being replicated
more than larger tasks.
"
1897,"The Bitlet Model: Defining a Litmus Test for the Bitwise
  Processing-in-Memory Paradigm","  This paper describes an analytical modeling tool called Bitlet that can be
used, in a parameterized fashion, to understand the affinity of workloads to
processing-in-memory (PIM) as opposed to traditional computing. The tool
uncovers interesting trade-offs between operation complexity (cycles required
to perform an operation through PIM) and other key parameters, such as system
memory bandwidth, data transfer size, the extent of data alignment, and
effective memory capacity involved in PIM computations. Despite its simplicity,
the model has already proven useful. In the future, we intend to extend and
refine Bitlet to further increase its utility.
"
1898,"5G, OFDM, Cubic Metric, NR-U, Occupied Channel Bandwidth, CAZAC,
  Zadoff-Chu Sequence","  In NR-based Access to Unlicensed Spectrum (NR-U) of 5G system, to satisfy the
rules of Occupied Channel Bandwidth (OCB) of unlicensed spectrum, the channels
of PRACH and PUCCH have to use some sequence repetition mechanisms in frequency
domain. These repetition mechanisms will cause serious cubic metric(CM)
problems for these channels, although these two types of channels are composed
of Constant Amplitude Zero Auto-correlation(CAZAC) sequences.. Based on the
characteristics of CAZAC sequences, which are used for PRACH and PUCCH (refer
to PUCCH format 0 and format 1) in 5G NR, in this paper, we propose some new
mechanisms of CM reduction for these two types of channels considering the
design principles to ensure the sequence performance of the auto-correlation
and cross-correlation. Then the proposed CM schemes are evaluated and the
optimized parameters are further provided considering CM performance and the
complexity.
"
1899,User Data Sharing Frameworks: A Blockchain-Based Incentive Solution,"  Currently, there is no universal method to track who shared what, with whom,
when and for what purposes in a verifiable way to create an individual
incentive for data owners. A platform that allows data owners to control,
delete, and get rewards from sharing their data would be an important enabler
of user data-sharing. We propose a usable blockchain- and smart contracts-based
framework that allows users to store research data locally and share without
losing control and ownership of it. We have created smart contracts for
building automatic verification of the conditions for data access that also
naturally supports building up a verifiable record of the provenance,
incentives for users to share their data and accountability of access. The
paper presents a review of the existing work of research data sharing, the
proposed blockchain-based framework and an evaluation of the framework by
measuring the transaction cost for smart contracts deployment. The results show
that nodes responded quickly in all tested cases with a befitting transaction
cost.
"
1900,"Bounding Mean First Passage Times in Population Continuous-Time Markov
  Chains","  We consider the problem of bounding mean first passage times for a class of
continuous-time Markov chains that captures stochastic interactions between
groups of identical agents. The quantitative analysis of such probabilistic
population models is notoriously difficult since typically neither state-based
numerical approaches nor methods based on stochastic sampling give efficient
and accurate results. Here, we propose a technique that extends recently
developed methods using semi-definite programming to determine bounds on mean
first passage times. We further apply the technique to hybrid models and
demonstrate its accuracy and efficiency for some examples from biology.
"
1901,Please come back later: Benefiting from deferrals in service systems,"  The performance evaluation of loss service systems, where customers who
cannot be served upon arrival get dropped, has a long history going back to the
classical Erlang B model. In this paper, we consider the performance benefits
arising from the possibility of deferring customers who cannot be served upon
arrival. Specifically, we consider an Erlang B type loss system where the
system operator can, subject to certain constraints, ask a customer arriving
when all servers are busy, to come back at a specified time in the future. If
the system is still fully loaded when the deferred customer returns, she gets
dropped for good. For such a system, we ask: How should the system operator
determine the rearrival times of the deferred customers based on the state of
the system (which includes those customers already deferred and yet to arrive)?
How does one quantify the performance benefit of such a deferral policy? Our
contributions are as follows. We propose a simple state-dependent policy for
determining the rearrival times of deferred customers. For this policy, we
characterize the long run fraction of customers dropped. We also analyse a
relaxation where the deferral times are bounded in expectation. Via extensive
numerical evaluations, we demonstrate the superiority of the proposed
state-dependent policies over naive state-independent deferral policies.
"
1902,"Intelligent-Unrolling: Exploiting Regular Patterns in Irregular
  Applications","  Modern optimizing compilers are able to exploit memory access or computation
patterns to generate vectorization codes. However, such patterns in irregular
applications are unknown until runtime due to the input dependence. Thus,
either compiler's static optimization or profile-guided optimization based on
specific inputs cannot predict the patterns for any common input, which leads
to suboptimal code generation. To address this challenge, we develop
Intelligent-Unroll, a framework to automatically optimize irregular
applications with vectorization. Intelligent-Unroll allows the users to depict
the computation task using \textit{code seed} with the memory access and
computation patterns represented in \textit{feature table} and
\textit{information-code tree}, and generates highly efficient codes.
Furthermore, Intelligent-Unroll employs several novel optimization techniques
to optimize reduction operations and gather/scatter instructions. We evaluate
Intelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph
applications. Experimental results show that Intelligent-Unroll is able to
generate more efficient vectorization codes compared to the state-of-the-art
implementations.
"
1903,"Direct N-body application on low-power and energy-efficient parallel
  architectures","  The aim of this work is to quantitatively evaluate the impact of computation
on the energy consumption on ARM MPSoC platforms, exploiting CPUs, embedded
GPUs and FPGAs. One of them possibly represents the future of High Performance
Computing systems: a prototype of an Exascale supercomputer. Performance and
energy measurements are made using a state-of-the-art direct $N$-body code from
the astrophysical domain. We provide a comparison of the time-to-solution and
energy delay product metrics, for different software configurations. We have
shown that FPGA technologies can be used for application kernel acceleration
and are emerging as a promising alternative to ""traditional"" technologies for
HPC, which purely focus on peak-performance than on power-efficiency.
"
1904,Debian Package usage profiler for Debian based Systems,"  The embedded devices of today due to their CPU, RAM capabilities can run
various Linux distributions but in most cases they are different from general
purpose distributions as they are usually lighter and specific to the needs of
that particular system. In this project, we share the problems associated in
adopting a fully heavy-weight Debian based system like Ubuntu in
embedded/automotive platforms and provide solutions to optimize them to
identify unused/redundant content in the system. This helps developer to reduce
the hefty general purpose distribution to an application specific distribution.
The solution involves collecting usage data in the system in a non-invasive
manner (to avoid any drop in performance) to suggest users the redundant,
unused parts of the system that can be safely removed without impacting the
system functionality.
"
1905,ALERT: Accurate Learning for Energy and Timeliness,"  An increasing number of software applications incorporate runtime Deep Neural
Networks (DNNs) to process sensor data and return inference results to humans.
Effective deployment of DNNs in these interactive scenarios requires meeting
latency and accuracy constraints while minimizing energy, a problem exacerbated
by common system dynamics. Prior approaches handle dynamics through either (1)
system-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs,
or (2) application-oblivious system adaptation, which adjusts resources to
change latency/energy tradeoffs. In contrast, this paper improves on the
state-of-the-art by coordinating application- and system-level adaptation.
ALERT, our runtime scheduler, uses a probabilistic model to detect
environmental volatility and then simultaneously select both a DNN and a system
resource configuration to meet latency, accuracy, and energy constraints. We
evaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic
environments. ALERT's holistic approach achieves more than 13% energy
reduction, and 27% error reduction over prior approaches that adapt solely at
the application or system level. Furthermore, ALERT incurs only 3% more energy
consumption and 2% higher DNN-inference error than an oracle scheme with
perfect application and system knowledge.
"
1906,"A Data-Assisted Reliability Model for Carrier-Assisted Cold Data Storage
  Systems","  Cold data storage systems are used to allow long term digital preservation
for institutions' archives. The common functionality among cold and warm/hot
data storage is that the data is stored on some physical medium for read-back
at a later time. However in cold storage, write and read operations are not
necessarily done in the same exact geographical location. Hence, a third party
assistance is typically utilized to bring together the medium and the drive. On
the other hand, the reliability modeling of such a decomposed system poses few
challenges that do not necessarily exist in other warm/hot storage alternatives
such as fault detection and absence of the carrier, all totaling up to the data
unavailability issues. In this paper, we propose a generalized non-homogenous
Markov model that encompasses the aging of the carriers in order to address the
requirements of today's cold data storage systems in which the data is encoded
and spread across multiple nodes for the long-term data retention. We have
derived useful lower/upper bounds on the overall system availability.
Furthermore, the collected field data is used to estimate parameters of a
Weibull distribution to accurately predict the lifetime of the carriers in an
example scale-out setting. In this study, we numerically demonstrate the
significance of carriers' presence and the key role that their timely
maintenance plays on the long-term reliability and availability of the stored
content.
"
1907,"Memory Requirement Reduction of Deep Neural Networks Using Low-bit
  Quantization of Parameters","  Effective employment of deep neural networks (DNNs) in mobile devices and
embedded systems is hampered by requirements for memory and computational
power. This paper presents a non-uniform quantization approach which allows for
dynamic quantization of DNN parameters for different layers and within the same
layer. A virtual bit shift (VBS) scheme is also proposed to improve the
accuracy of the proposed scheme. Our method reduces the memory requirements,
preserving the performance of the network. The performance of our method is
validated in a speech enhancement application, where a fully connected DNN is
used to predict the clean speech spectrum from the input noisy speech spectrum.
A DNN is optimized and its memory footprint and performance are evaluated using
the short-time objective intelligibility, STOI, metric. The application of the
low-bit quantization allows a 50% reduction of the DNN memory footprint while
the STOI performance drops only by 2.7%.
"
1908,"LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long
  Short-Term Memory","  The effectiveness of LSTM neural networks for popular tasks such as Automatic
Speech Recognition has fostered an increasing interest in LSTM inference
acceleration. Due to the recurrent nature and data dependencies of LSTM
computations, designing a customized architecture specifically tailored to its
computation pattern is crucial for efficiency. Since LSTMs are used for a
variety of tasks, generalizing this efficiency to diverse configurations, i.e.,
adaptiveness, is another key feature of these accelerators. In this work, we
first show the problem of low resource-utilization and adaptiveness for the
state-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To
solve these issues, we propose an intelligent tiled-based dispatching mechanism
that efficiently handles the data dependencies and increases the adaptiveness
of LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator,
which pipelines LSTM computation using an effective scheduling scheme to hide
most of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic
reconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp
achieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art
ASIC, FPGA, and GPU implementations respectively, for different LSTM models and
resource budgets. Furthermore, we provide significant energy-reduction with
respect to the previous solutions, due to the low power dissipation of
LSTM-Sharp (383 GFLOPs/Watt).
"
1909,"KLARAPTOR: A Tool for Dynamically Finding Optimal Kernel Launch
  Parameters Targeting CUDA Programs","  In this paper we present KLARAPTOR (Kernel LAunch parameters RAtional Program
estimaTOR), a new tool built on top of the LLVM Pass Framework and NVIDIA CUPTI
API to dynamically determine the optimal values of kernel launch parameters of
a CUDA program P. To be precise, we describe a novel technique to statically
build (at the compile time of P) a so-called rational program R. Using a
performance prediction model, and knowing particular data and hardware
parameters of P at runtime, the program R can automatically and dynamically
determine the values of launch parameters of P that will yield optimal
performance. Our technique can be applied to parallel programs in general, as
well as to generic performance prediction models which account for program and
hardware parameters. We are particularly interested in programs targeting
manycore accelerators. We have implemented and successfully tested our
technique in the context of GPU kernels written in CUDA using the MWP-CWP
performance prediction model.
"
1910,"Graph-based Approach for Buffer-aware Timing Analysis of Heterogeneous
  Wormhole NoCs under Bursty Traffic","  This paper addresses the problem of worst-case timing analysis of
heterogeneous wormhole NoCs, i.e., routers with different buffer sizes and
transmission speeds, when consecutive packet queuing (CPQ) occurs. The latter
means that there are several consecutive packets of one flow queuing in the
network. This scenario happens in the case of bursty traffic but also for
non-schedulable traffic. Conducting such an analysis is known to be a
challenging issue due to the sophisticated congestion patterns when enabling
backpressure mechanisms. We tackle this problem through extending the
applicability domain of our previous work for computing maximum delay bounds
using Network Calculus, called Buffer-aware worst-case Timing Analysis (BATA).
We propose a new Graph-based approach to improve the analysis of indirect
blocking due to backpressure, while capturing the CPQ effect and keeping the
information about dependencies between flows. Furthermore, the introduced
approach improves the computation of indirect-blocking delay bounds in terms of
complexity and ensures the safety of these bounds even for non-schedulable
traffic. We provide further insights into the tightness and complexity issues
of worst-case delay bounds yielded by the extended BATA with the Graph-based
approach, denoted G-BATA. Our assessments show that the complexity has
decreased by up to 100 times while offering an average tightness ratio of 71%,
with reference to the basic BATA. Finally, we evaluate the yielded improvements
with G-BATA for a realistic use case against a recent state-of-the-art
approach. This evaluation shows the applicability of G-BATA under more general
assumptions and the impact of such a feature on the tightness and computation
time.
"
1911,MLPerf Inference Benchmark,"  Machine-learning (ML) hardware and software system demand is burgeoning.
Driven by ML applications, the number of different ML inference systems has
exploded. Over 100 organizations are building ML inference chips, and the
systems that incorporate existing models span at least three orders of
magnitude in power consumption and five orders of magnitude in performance;
they range from embedded devices to data-center solutions. Fueling the hardware
are a dozen or more software frameworks and libraries. The myriad combinations
of ML hardware and ML software make assessing ML-system performance in an
architecture-neutral, representative, and reproducible manner challenging.
There is a clear need for industry-wide standard ML benchmarking and evaluation
criteria. MLPerf Inference answers that call. In this paper, we present our
benchmarking method for evaluating ML inference systems. Driven by more than 30
organizations as well as more than 200 ML engineers and practitioners, MLPerf
prescribes a set of rules and best practices to ensure comparability across
systems with wildly differing architectures. The first call for submissions
garnered more than 600 reproducible inference-performance measurements from 14
organizations, representing over 30 systems that showcase a wide range of
capabilities. The submissions attest to the benchmark's flexibility and
adaptability.
"
1912,The Pitfall of Evaluating Performance on Emerging AI Accelerators,"  In recent years, domain-specific hardware has brought significant performance
improvements in deep learning (DL). Both industry and academia only focus on
throughput when evaluating these AI accelerators, which usually are custom
ASICs deployed in datacenter to speed up the inference phase of DL workloads.
Pursuing higher hardware throughput such as OPS (Operation Per Second) using
various optimizations seems to be their main design target. However, they
ignore the importance of accuracy in the DL nature. Motivated by this, this
paper argue that a single throughput metric can not comprehensively reflect the
real-world performance of AI accelerators. To reveal this pitfall, we evaluates
several frequently-used optimizations on a typical AI accelerator and
quantifies their impact on accuracy and throughout under representative DL
inference workloads. Based on our experimental results, we find that some
optimizations cause significant loss on accuracy in some workloads, although it
can improves the throughout. Furthermore, our results show the importance of
end-to-end evaluation in DL.
"
1913,Adaptive Kernel Value Caching for SVM Training,"  Support Vector Machines (SVMs) can solve structured multi-output learning
problems such as multi-label classification, multiclass classification and
vector regression. SVM training is expensive especially for large and high
dimensional datasets. The bottleneck of the SVM training often lies in the
kernel value computation. In many real-world problems, the same kernel values
are used in many iterations during the training, which makes the caching of
kernel values potentially useful. The majority of the existing studies simply
adopt the LRU (least recently used) replacement strategy for caching kernel
values. However, as we analyze in this paper, the LRU strategy generally
achieves high hit ratio near the final stage of the training, but does not work
well in the whole training process. Therefore, we propose a new caching
strategy called EFU (less frequently used) which replaces the less frequently
used kernel values that enhances LFU (least frequently used). Our experimental
results show that EFU often has 20\% higher hit ratio than LRU in the training
with the Gaussian kernel. To further optimize the strategy, we propose a
caching strategy called HCST (hybrid caching for the SVM training), which has a
novel mechanism to automatically adapt the better caching strategy in the
different stages of the training. We have integrated the caching strategy into
ThunderSVM, a recent SVM library on many-core processors. Our experiments show
that HCST adaptively achieves high hit ratios with little runtime overhead
among different problems including multi-label classification, multiclass
classification and regression problems. Compared with other existing caching
strategies, HCST achieves 20\% more reduction in training time on average.
"
1914,"Digital Blood in Massively Parallel CPU/GPU Systems for the Study of
  Platelet Transport","  We propose a highly versatile computational framework for the simulation of
cellular blood flow focusing on extreme performance without compromising
accuracy or complexity. The tool couples the lattice Boltzmann solver Palabos
for the simulation of the blood plasma, a novel finite element method (FEM)
solver for the resolution of the deformable blood cells, and an immersed
boundary method for the coupling of the two phases. The design of the tool
supports hybrid CPU-GPU executions (fluid, fluid-solid interaction on CPUs, the
FEM solver on GPUs), and is non-intrusive, as each of the three components can
be replaced in a modular way. The FEM-based kernel for solid dynamics
outperforms other FEM solvers and its performance is comparable to the
state-of-the-art mass-spring systems. We perform an exhaustive performance
analysis on Piz Daint at the Swiss National Supercomputing Centre and provide
case studies focused on platelet transport. The tests show that this versatile
framework combines unprecedented accuracy with massive performance, rendering
it suitable for the upcoming exascale architectures.
"
1915,"nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86
  Systems","  We present nanoBench, a tool for evaluating small microbenchmarks using
hardware performance counters on Intel and AMD x86 systems. Most existing tools
and libraries are intended to either benchmark entire programs, or program
segments in the context of their execution within a larger program. In
contrast, nanoBench is specifically designed to evaluate small, isolated pieces
of code. Such code is common in microbenchmark-based hardware analysis
techniques.
  Unlike previous tools, nanoBench can execute microbenchmarks directly in
kernel space. This allows to benchmark privileged instructions, and it enables
more accurate measurements. The reading of the performance counters is
implemented with minimal overhead avoiding functions calls and branches. As a
consequence, nanoBench is precise enough to measure individual memory accesses.
  We illustrate the utility of nanoBench at the hand of two case studies.
First, we briefly discuss how nanoBench has been used to determine the latency,
throughput, and port usage of more than 13,000 instruction variants on recent
x86 processors. Second, we show how to generate microbenchmarks to precisely
characterize the cache architectures of eleven Intel Core microarchitectures.
This includes the most comprehensive analysis of the employed cache replacement
policies to date.
"
1916,Parallel Data Distribution Management on Shared-Memory Multiprocessors,"  The problem of identifying intersections between two sets of d-dimensional
axis-parallel rectangles appears frequently in the context of agent-based
simulation studies. For this reason, the High Level Architecture (HLA)
specification -- a standard framework for interoperability among simulators --
includes a Data Distribution Management (DDM) service whose responsibility is
to report all intersections between a set of subscription and update regions.
The algorithms at the core of the DDM service are CPU-intensive, and could
greatly benefit from the large computing power of modern multi-core processors.
In this paper we propose two parallel solutions to the DDM problem that can
operate effectively on shared-memory multiprocessors. The first solution is
based on a data structure (the Interval Tree) that allows concurrent
computation of intersections between subscription and update regions. The
second solution is based on a novel parallel extension of the Sort Based
Matching algorithm, whose sequential version is considered among the most
efficient solutions to the DDM problem. Extensive experimental evaluation of
the proposed algorithms confirm their effectiveness on taking advantage of
multiple execution units in a shared-memory architecture.
"
1917,"Communication-Efficient Jaccard Similarity for High-Performance
  Distributed Genome Comparisons","  The Jaccard similarity index is an important measure of the overlap of two
sets, widely used in machine learning, computational genomics, information
retrieval, and many other areas. We design and implement SimilarityAtScale, the
first communication-efficient distributed algorithm for computing the Jaccard
similarity among pairs of large datasets. Our algorithm provides an efficient
encoding of this problem into a multiplication of sparse matrices. Both the
encoding and sparse matrix product are performed in a way that minimizes data
movement in terms of communication and synchronization costs. We apply our
algorithm to obtain similarity among all pairs of a set of large samples of
genomes. This task is a key part of modern metagenomics analysis and an
evergrowing need due to the increasing availability of high-throughput DNA
sequencing data. The resulting scheme is the first to enable accurate Jaccard
distance derivations for massive datasets, using largescale distributed-memory
systems. We package our routines in a tool, called GenomeAtScale, that combines
the proposed algorithm with tools for processing input sequences. Our
evaluation on real data illustrates that one can use GenomeAtScale to
effectively employ tens of thousands of processors to reach new frontiers in
large-scale genomic and metagenomic analysis. While GenomeAtScale can be used
to foster DNA research, the more general underlying SimilarityAtScale algorithm
may be used for high-performance distributed similarity computations in other
data analytics application domains.
"
1918,XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training,"  We propose XPipe, an efficient asynchronous pipeline model parallelism
approach for multi-GPU DNN training. XPipe is designed to use multiple GPUs to
concurrently and continuously train different parts of a DNN model. To improve
GPU utilization and achieve high throughput, it splits a mini-batch into a set
of micro-batches. It allows the overlapping of the pipelines of multiple
micro-batches, including those belonging to different mini-batches. Most
importantly, the novel weight prediction strategy adopted by XPipe enables it
to effectively address the weight inconsistency and staleness issues incurred
by the asynchronous pipeline parallelism. As a result, XPipe incorporates the
advantages of both synchronous and asynchronous pipeline model parallelism
approaches. Concretely, it can achieve very comparable (even slightly better)
model accuracy as its synchronous counterpart while obtaining higher throughput
than it. Experimental results show that XPipe outperforms other
state-of-the-art synchronous and asynchronous model parallelism approaches.
"
1919,Throughput Prediction of Asynchronous SGD in TensorFlow,"  Modern machine learning frameworks can train neural networks using multiple
nodes in parallel, each computing parameter updates with stochastic gradient
descent (SGD) and sharing them asynchronously through a central parameter
server. Due to communication overhead and bottlenecks, the total throughput of
SGD updates in a cluster scales sublinearly, saturating as the number of nodes
increases. In this paper, we present a solution to predicting training
throughput from profiling traces collected from a single-node configuration.
Our approach is able to model the interaction of multiple nodes and the
scheduling of concurrent transmissions between the parameter server and each
node. By accounting for the dependencies between received parts and pending
computations, we predict overlaps between computation and communication and
generate synthetic execution traces for configurations with multiple nodes. We
validate our approach on TensorFlow training jobs for popular image
classification neural networks, on AWS and on our in-house cluster, using nodes
equipped with GPUs or only with CPUs. We also investigate the effects of data
transmission policies used in TensorFlow and the accuracy of our approach when
combined with optimizations of the transmission schedule.
"
1920,"Optimizing Deep Learning Inference on Embedded Systems Through Adaptive
  Model Selection","  Deep neural networks ( DNNs ) are becoming a key enabling technology for many
application domains. However, on-device inference on battery-powered,
resource-constrained embedding systems is often infeasible due to prohibitively
long inferencing time and resource requirements of many DNNs. Offloading
computation into the cloud is often unacceptable due to privacy concerns, high
latency, or the lack of connectivity. While compression algorithms often
succeed in reducing inferencing times, they come at the cost of reduced
accuracy. This paper presents a new, alternative approach to enable efficient
execution of DNNs on embedded devices. Our approach dynamically determines
which DNN to use for a given input, by considering the desired accuracy and
inference time. It employs machine learning to develop a low-cost predictive
model to quickly select a pre-trained DNN to use for a given input and the
optimization constraint. We achieve this by first off-line training a
predictive model, and then using the learned model to select a DNN model to use
for new, unseen inputs. We apply our approach to two representative DNN
domains: image classification and machine translation. We evaluate our approach
on a Jetson TX2 embedded deep learning platform and consider a range of
influential DNN models including convolutional and recurrent neural networks.
For image classification, we achieve a 1.8x reduction in inference time with a
7.52% improvement in accuracy, over the most-capable single DNN model. For
machine translation, we achieve a 1.34x reduction in inference time over the
most-capable single model, with little impact on the quality of translation.
"
1921,"HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN
  Training using TensorFlow","  To reduce training time of large-scale DNNs, scientists have started to
explore parallelization strategies like data-parallelism, model-parallelism,
and hybrid-parallelism. While data-parallelism has been extensively studied and
developed, several problems exist in realizing model-parallelism and
hybrid-parallelism efficiently. Four major problems we focus on are: 1)
defining a notion of a distributed model across processes, 2) implementing
forward/back-propagation across process boundaries that requires explicit
communication, 3) obtaining parallel speedup on an inherently sequential task,
and 4) achieving scalability without losing out on a model's accuracy. To
address these problems, we create HyPar-Flow --- a model-size/-type agnostic,
scalable, practical, and user-transparent system for hybrid-parallel training
by exploiting MPI, Keras, and TensorFlow. HyPar-Flow provides a single API that
can be used to perform data, model, and hybrid parallel training of any Keras
model at scale. We create an internal distributed representation of the
user-provided Keras model, utilize TF's Eager execution features for
distributed forward/back-propagation across processes, exploit pipelining to
improve performance and leverage efficient MPI primitives for scalable
communication. Between model partitions, we use send and recv to exchange
layer-data/partial-errors while allreduce is used to accumulate/average
gradients across model replicas. Beyond the design and implementation of
HyPar-Flow, we also provide comprehensive correctness and performance results
on three state-of-the-art HPC systems including TACC Frontera (#5 on
Top500.org). For ResNet-1001, an ultra-deep model, HyPar-Flow provides: 1) Up
to 1.6x speedup over Horovod-based data-parallel training, 2) 110x speedup over
single-node on 128 Stampede2 nodes, and 3) 481x speedup over single-node on 512
Frontera nodes.
"
1922,"92c/MFlops/s, Ultra-Large-Scale Neural-Network Training on a PIII
  Cluster","  Artificial neural networks with millions of adjustable parameters and a
similar number of training examples are a potential solution for difficult,
large-scale pattern recognition problems in areas such as speech and face
recognition, classification of large volumes of web data, and finance. The
bottleneck is that neural network training involves iterative gradient descent
and is extremely computationally intensive. In this paper we present a
technique for distributed training of Ultra Large Scale Neural Networks (ULSNN)
on Bunyip, a Linux-based cluster of 196 Pentium III processors. To illustrate
ULSNN training we describe an experiment in which a neural network with 1.73
million adjustable parameters was trained to recognize machine-printed Japanese
characters from a database containing 9 million training patterns. The training
runs with a average performance of 163.3 GFlops/s (single precision). With a
machine cost of \$150,913, this yields a price/performance ratio of
92.4c/MFlops/s (single precision). For comparison purposes, training using
double precision and the ATLAS DGEMM produces a sustained performance of 70
MFlops/s or \$2.16 / MFlop/s (double precision).
"
1923,"Two-level Dynamic Load Balancing for High Performance Scientific
  Applications","  Scientific applications are often complex, irregular, and
computationally-intensive. To accommodate the ever-increasing computational
demands of scientific applications, high-performance computing (HPC) systems
have become larger and more complex, offering parallelism at multiple levels
(e.g., nodes, cores per node, threads per core). Scientific applications need
to exploit all the available multilevel hardware parallelism to harness the
available computational power. The performance of applications executing on
such HPC systems may adversely be affected by load imbalance at multiple
levels, caused by problem, algorithmic, and systemic characteristics.
Nevertheless, most existing load balancing methods do not simultaneously
address load imbalance at multiple levels. This work investigates the impact of
load imbalance on the performance of three scientific applications at the
thread and process levels. We jointly apply and evaluate selected dynamic loop
self-scheduling (DLS) techniques to both levels. Specifically, we employ the
extended LaPeSD OpenMP runtime library at the thread level and extend the
DLS4LB MPI-based dynamic load balancing library at the process level. This
approach is generic and applicable to any multiprocess-multithreaded
computationally-intensive application (programmed using MPI and OpenMP). We
conduct an exhaustive set of experiments to assess and compare six DLS
techniques at the thread level and eleven at the process level. The results
show that improved application performance, by up to 21%, can only be achieved
by jointly addressing load imbalance at the two levels. We offer insights into
the performance of the selected DLS techniques and discuss the interplay of
load balancing at the thread level and process level.
"
1924,"Benanza: Automatic $\mu$Benchmark Generation to Compute ""Lower-bound""
  Latency and Inform Optimizations of Deep Learning Models on GPUs","  As Deep Learning (DL) models have been increasingly used in latency-sensitive
applications, there has been a growing interest in improving their response
time. An important venue for such improvement is to profile the execution of
these models and characterize their performance to identify possible
optimization opportunities. However, the current profiling tools lack the
highly desired abilities to characterize ideal performance, identify sources of
inefficiency, and quantify the benefits of potential optimizations. Such
deficiencies have led to slow characterization/optimization cycles that cannot
keep up with the fast pace at which new DL models are introduced.
  We propose Benanza, a sustainable and extensible benchmarking and analysis
design that speeds up the characterization/optimization cycle of DL models on
GPUs. Benanza consists of four major components: a model processor that parses
models into an internal representation, a configurable benchmark generator that
automatically generates micro-benchmarks given a set of models, a database of
benchmark results, and an analyzer that computes the ""lower-bound"" latency of
DL models using the benchmark data and informs optimizations of model
execution. The ""lower-bound"" latency metric estimates the ideal model execution
on a GPU system and serves as the basis for identifying optimization
opportunities in frameworks or system libraries. We used Benanza to evaluate 30
ONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler
to the latest Turing, and identified optimizations in parallel layer execution,
cuDNN convolution algorithm selection, framework inefficiency, layer fusion,
and using Tensor Cores.
"
1925,Fast-MUSIC for Automotive Massive-MIMO Radar,"  Massive multiple-input multiple-output (MIMO) radar, assisted by
millimeter-wave band virtual MIMO techniques, provides great promises to the
high-resolution automotive sensing and target detection in unmanned
ground/aerial vehicles (UGA/UAV). As one long-standing challenging problem,
however existing subspace methods may suffer from either the low
resolution/accuracy or the high time complexity. In this study, we propose two
computational efficient methods to accomplish the high-resolution estimation of
angle of arrival (AoA) information. By leveraging randomized low-rank
approximation, our fast-MUSIC approaches, relying on random sampling and
projection techniques, would speed up the subspace computation by orders of
magnitude. At the same time, we establish the theoretical bounds of our
proposed approaches, which ensure the accuracy of approximated pseudo-spectrum.
As shown, in the case of high signal-to-noise ratio, the pseudo-spectrum
acquired by our fast-MUSIC is highly precise, when compared to the exact MUSIC.
Comprehensive numerical study demonstrates that our new methods are
tremendously faster than MUSIC, while the AoA estimation accuracy are almost as
good as MUSIC. As such, our fast-MUSIC enables the high-resolution yet
real-time sensing with massive MIMO radar, which has great potential in the
emerging mobile computing and automotive applications.
"
1926,"Understanding Open Source Serverless Platforms: Design Considerations
  and Performance","  Serverless computing is increasingly popular because of the promise of lower
cost and the convenience it provides to users who do not need to focus on
server management. This has resulted in the availability of a number of
proprietary and open-source serverless solutions. We seek to understand how the
performance of serverless computing depends on a number of design issues using
several popular open-source serverless platforms. We identify the
idiosyncrasies affecting performance (throughput and latency) for different
open-source serverless platforms. Further, we observe that just having either
resource-based (CPU and memory) or workload-based (request per second (RPS) or
concurrent requests) auto-scaling is inadequate to address the needs of the
serverless platforms.
"
1927,"Design and Implementation of Secret Key Agreement for Platoon-based
  Vehicular Cyber-Physical Systems","  In platoon-based vehicular cyber-physical system (PVCPS), a lead vehicle that
is responsible for managing the platoon's moving directions and velocity
periodically disseminates control messages to the vehicles that follow.
Securing wireless transmissions of the messages between the vehicles is
critical for privacy and confidentiality of platoon's driving pattern. However,
due to the broadcast nature of radio channels, the transmissions are vulnerable
to eavesdropping. In this paper, we propose a cooperative secret key agreement
(CoopKey) scheme for encrypting/decrypting the control messages, where the
vehicles in PVCPS generate a unified secret key based on the quantized fading
channel randomness. Channel quantization intervals are optimized by dynamic
programming to minimize the mismatch of keys. A platooning testbed is built
with autonomous robotic vehicles, where a TelosB wireless node is used for
onboard data processing and multi-hop dissemination. Extensive real-world
experiments demonstrate that CoopKey achieves significantly low secret bit
mismatch rate in a variety of settings. Moreover, the standard NIST test suite
is employed to verify randomness of the generated keys, where the p-values of
our CoopKey pass all the randomness tests. We also evaluate CoopKey with an
extended platoon size via simulations to investigate the effect of system
scalability on performance.
"
1928,Profile-based Resource Allocation for Virtualized Network Functions,"  The virtualization of compute and network resources enables an unseen
flexibility for deploying network services. A wide spectrum of emerging
technologies allows an ever-growing range of orchestration possibilities in
cloud-based environments. But in this context it remains challenging to rhyme
dynamic cloud configurations with deterministic performance. The service
operator must somehow map the performance specification in the Service Level
Agreement (SLA) to an adequate resource allocation in the virtualized
infrastructure. We propose the use of a VNF profile to alleviate this process.
This is illustrated by profiling the performance of four example network
functions (a virtual router, switch, firewall and cache server) under varying
workloads and resource configurations. We then compare several methods to
derive a model from the profiled datasets. We select the most accurate method
to further train a model which predicts the services' performance, in function
of incoming workload and allocated resources. Our presented method can offer
the service operator a recommended resource allocation for the targeted
service, in function of the targeted performance and maximum workload specified
in the SLA. This helps to deploy the softwarized service with an optimal amount
of resources to meet the SLA requirements, thereby avoiding unnecessary scaling
steps.
"
1929,"DLBricks: Composable Benchmark Generation to Reduce Deep Learning
  Benchmarking Effort on CPUs (Extended)","  The past few years have seen a surge of applying Deep Learning (DL) models
for a wide array of tasks such as image classification, object detection,
machine translation, etc. While DL models provide an opportunity to solve
otherwise intractable tasks, their adoption relies on them being optimized to
meet latency and resource requirements. Benchmarking is a key step in this
process but has been hampered in part due to the lack of representative and
up-to-date benchmarking suites. This is exacerbated by the fast-evolving pace
of DL models.
  This paper proposes DLBricks, a composable benchmark generation design that
reduces the effort of developing, maintaining, and running DL benchmarks on
CPUs. DLBricks decomposes DL models into a set of unique runnable networks and
constructs the original model's performance using the performance of the
generated benchmarks. DLBricks leverages two key observations: DL layers are
the performance building blocks of DL models and layers are extensively
repeated within and across DL models. Since benchmarks are generated
automatically and the benchmarking time is minimized, DLBricks can keep
up-to-date with the latest proposed models, relieving the pressure of selecting
representative DL models. Moreover, DLBricks allows users to represent
proprietary models within benchmark suites. We evaluate DLBricks using $50$
MXNet models spanning $5$ DL tasks on $4$ representative CPU systems. We show
that DLBricks provides an accurate performance estimate for the DL models and
reduces the benchmarking time across systems (e.g. within $95\%$ accuracy and
up to $4.4\times$ benchmarking time speedup on Amazon EC2 c5.xlarge).
"
1930,The Design and Implementation of a Scalable DL Benchmarking Platform,"  The current Deep Learning (DL) landscape is fast-paced and is rife with
non-uniform models, hardware/software (HW/SW) stacks, but lacks a DL
benchmarking platform to facilitate evaluation and comparison of DL
innovations, be it models, frameworks, libraries, or hardware. Due to the lack
of a benchmarking platform, the current practice of evaluating the benefits of
proposed DL innovations is both arduous and error-prone - stifling the adoption
of the innovations.
  In this work, we first identify $10$ design features which are desirable
within a DL benchmarking platform. These features include: performing the
evaluation in a consistent, reproducible, and scalable manner, being framework
and hardware agnostic, supporting real-world benchmarking workloads, providing
in-depth model execution inspection across the HW/SW stack levels, etc. We then
propose MLModelScope, a DL benchmarking platform design that realizes the $10$
objectives. MLModelScope proposes a specification to define DL model
evaluations and techniques to provision the evaluation workflow using the
user-specified HW/SW stack. MLModelScope defines abstractions for frameworks
and supports board range of DL models and evaluation scenarios. We implement
MLModelScope as an open-source project with support for all major frameworks
and hardware architectures. Through MLModelScope's evaluation and automated
analysis workflows, we performed case-study analyses of $37$ models across $4$
systems and show how model, hardware, and framework selection affects model
accuracy and performance under different benchmarking scenarios. We further
demonstrated how MLModelScope's tracing capability gives a holistic view of
model execution and helps pinpoint bottlenecks.
"
1931,"Characterizing Scalability of Sparse Matrix-Vector Multiplications on
  Phytium FT-2000+ Many-cores","  Understanding the scalability of parallel programs is crucial for software
optimization and hardware architecture design. As HPC hardware is moving
towards many-core design, it becomes increasingly difficult for a parallel
program to make effective use of all available processor cores. This makes
scalability analysis increasingly important. This paper presents a quantitative
study for characterizing the scalability of sparse matrix-vector
multiplications (SpMV) on Phytium FT-2000+, an ARM-based many-core architecture
for HPC computing. We choose to study SpMV as it is a common operation in
scientific and HPC applications. Due to the newness of ARM-based many-core
architectures, there is little work on understanding the SpMV scalability on
such hardware design. To close the gap, we carry out a large-scale empirical
evaluation involved over 1,000 representative SpMV datasets. We show that,
while many computation-intensive SpMV applications contain extensive
parallelism, achieving a linear speedup is non-trivial on Phytium FT-2000+. To
better understand what software and hardware parameters are most important for
determining the scalability of a given SpMV kernel, we develop a performance
analytical model based on the regression tree. We show that our model is highly
effective in characterizing SpMV scalability, offering useful insights to help
application developers for better optimizing SpMV on an emerging HPC
architecture.
"
1932,"A Comparative Analysis of Forecasting Financial Time Series Using ARIMA,
  LSTM, and BiLSTM","  Machine and deep learning-based algorithms are the emerging approaches in
addressing prediction problems in time series. These techniques have been shown
to produce more accurate results than conventional regression-based modeling.
It has been reported that artificial Recurrent Neural Networks (RNN) with
memory, such as Long Short-Term Memory (LSTM), are superior compared to
Autoregressive Integrated Moving Average (ARIMA) with a large margin. The
LSTM-based models incorporate additional ""gates"" for the purpose of memorizing
longer sequences of input data. The major question is that whether the gates
incorporated in the LSTM architecture already offers a good prediction and
whether additional training of data would be necessary to further improve the
prediction.
  Bidirectional LSTMs (BiLSTMs) enable additional training by traversing the
input data twice (i.e., 1) left-to-right, and 2) right-to-left). The research
question of interest is then whether BiLSTM, with additional training
capability, outperforms regular unidirectional LSTM. This paper reports a
behavioral analysis and comparison of BiLSTM and LSTM models. The objective is
to explore to what extend additional layers of training of data would be
beneficial to tune the involved parameters. The results show that additional
training of data and thus BiLSTM-based modeling offers better predictions than
regular LSTM-based models. More specifically, it was observed that BiLSTM
models provide better predictions compared to ARIMA and LSTM models. It was
also observed that BiLSTM models reach the equilibrium much slower than
LSTM-based models.
"
1933,"Gemmini: An Agile Systolic Array Generator Enabling Systematic
  Evaluations of Deep-Learning Architectures","  Advances in deep learning and neural networks have resulted in the rapid
development of hardware accelerators that support them. A large majority of
ASIC accelerators, however, target a single hardware design point to accelerate
the main computational kernels of deep neural networks such as convolutions or
matrix multiplication. On the other hand, the spectrum of use-cases for neural
network accelerators, ranging from edge devices to cloud, presents a prime
opportunity for agile hardware design and generator methodologies. We present
Gemmini -- an open source and agile systolic array generator enabling
systematic evaluations of deep-learning architectures. Gemmini generates a
custom ASIC accelerator for matrix multiplication based on a systolic array
architecture, complete with additional functions for neural network inference.
Gemmini runs with the RISC-V ISA, and is integrated with the Rocket Chip
System-on-Chip generator ecosystem, including Rocket in-order cores and BOOM
out-of-order cores. Through an elaborate design space exploration case study,
this work demonstrates the selection processes of various parameters for the
use-case of inference on edge devices. Selected design points achieve two to
three orders of magnitude speedup in deep neural network inference compared to
the baseline execution on a host processor. Gemmini-generated accelerators were
used in the fabrication of test systems-on-chip in TSMC 16nm and Intel 22FFL
process technologies.
"
1934,"CAMUS: A Framework to Build Formal Specifications for Deep Perception
  Systems Using Simulators","  The topic of provable deep neural network robustness has raised considerable
interest in recent years. Most research has focused on adversarial robustness,
which studies the robustness of perceptive models in the neighbourhood of
particular samples. However, other works have proved global properties of
smaller neural networks. Yet, formally verifying perception remains uncharted.
This is due notably to the lack of relevant properties to verify, as the
distribution of possible inputs cannot be formally specified. We propose to
take advantage of the simulators often used either to train machine learning
models or to check them with statistical tests, a growing trend in industry.
Our formulation allows us to formally express and verify safety properties on
perception units, covering all cases that could ever be generated by the
simulator, to the difference of statistical tests which cover only seen
examples. Along with this theoretical formulation , we provide a tool to
translate deep learning models into standard logical formulae. As a proof of
concept, we train a toy example mimicking an autonomous car perceptive unit,
and we formally verify that it will never fail to capture the relevant
information in the provided inputs.
"
1935,Efficient Saliency Maps for Explainable AI,"  We describe an explainable AI saliency map method for use with deep
convolutional neural networks (CNN) that is much more efficient than popular
fine-resolution gradient methods. It is also quantitatively similar or better
in accuracy. Our technique works by measuring information at the end of each
network scale which is then combined into a single saliency map. We describe
how saliency measures can be made more efficient by exploiting Saliency Map
Order Equivalence. We visualize individual scale/layer contributions by using a
Layer Ordered Visualization of Information. This provides an interesting
comparison of scale information contributions within the network not provided
by other saliency map methods. Using our method instead of Guided Backprop,
coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem
to yield demonstrably superior results without sacrificing speed. This will
make fine-resolution saliency methods feasible on resource limited platforms
such as robots, cell phones, low-cost industrial devices, astronomy and
satellite imagery.
"
1936,"Transaction Confirmation Time Prediction in Ethereum Blockchain Using
  Machine Learning","  Blockchain offers a decentralized, immutable, transparent system of records.
It offers a peer-to-peer network of nodes with no centralised governing entity
making it unhackable and therefore, more secure than the traditional
paper-based or centralised system of records like banks etc. While there are
certain advantages to the paper-based recording approach, it does not work well
with digital relationships where the data is in constant flux. Unlike
traditional channels, governed by centralized entities, blockchain offers its
users a certain level of anonymity by providing capabilities to interact
without disclosing their personal identities and allows them to build trust
without a third-party governing entity. Due to the aforementioned
characteristics of blockchain, more and more users around the globe are
inclined towards making a digital transaction via blockchain than via
rudimentary channels. Therefore, there is a dire need for us to gain insight on
how these transactions are processed by the blockchain and how much time it may
take for a peer to confirm a transaction and add it to the blockchain network.
This paper presents a novel approach that would allow one to estimate the time,
in block time or otherwise, it would take for a mining node to accept and
confirm a transaction to a block using machine learning. The paper also aims to
compare the predictive accuracy of two machine learning regression models-
Random Forest Regressor and Multilayer Perceptron against previously proposed
statistical regression model under a set evaluation criterion. The objective is
to determine whether machine learning offers a more accurate predictive model
than conventional statistical models. The proposed model results in improved
accuracy in prediction.
"
1937,"System Performance with varying L1 Instruction and Data Cache Sizes: An
  Empirical Analysis","  In this project, we investigate the fluctuations in performance caused by
changing the Instruction (I-cache) size and the Data (D-cache) size in the L1
cache. We employ the Gem5 framework to simulate a system with varying
specifications on a single host machine. We utilize the FreqMine benchmark
available under the PARSEC suite as the workload program to benchmark our
simulated system. The Out-order CPU (O3) with Ruby memory model was simulated
in a Full-System X86 environment with Linux OS. The chosen metrics deal with
Hit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the
system. Performance observed by varying L1 size within a certain range of
values was used to compute Confidence Interval based statistics for relevant
metrics. Our expectations, corresponding experimental observations, and
discrepancies are also discussed in this report.
"
1938,Rule Designs for Optimal Online Game Matchmaking,"  Online games are the most popular form of entertainment among youngsters as
well as elders. Recognized as e-Sports, they may become an official part of the
Olympic Games by 2020. However, a long waiting time for matchmaking will
largely affect players' experiences. We examine different matchmaking
mechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic
framework, we decompose the rule design process into a sequence of decision
making problems, and derive the optimal mechanism with minimum expected waiting
time. We further the result by exploring additional static as well as dynamic
rule designs' impacts. In the static setting, we consider the game allows
players to choose sides before the battle. In the dynamic setting, we consider
the game offers multiple zones for players of different skill levels. In both
settings, we examine the value of choice-free players. Closed form expressions
for the expected waiting time in different settings illuminate the guidelines
for online game rule designs.
"
1939,Dynamically Provisioning Cray DataWarp Storage,"  Complex applications and workflows needs are often exclusively expressed in
terms of computational resources on HPC systems. In many cases, other resources
like storage or network are not allocatable and are shared across the entire
HPC system. By looking at the storage resource in particular, any workflow or
application should be able to select both its preferred data manager and its
required storage capability or capacity. To achieve such a goal, new mechanisms
should be introduced. In this work, we introduce such a mechanism for
dynamically provision a data management system on top of storage devices. We
particularly focus our effort on deploying a BeeGFS instance across multiple
DataWarp nodes on a Cray XC50 system. However, we also demonstrate that the
same mechanism can be used to deploy BeeGFS on non-Cray system.
"
1940,GraphZero: Breaking Symmetry for Efficient Graph Mining,"  Graph mining for structural patterns is a fundamental task in many
applications. Compilation-based graph mining systems, represented by AutoMine,
generate specialized algorithms for the provided patterns and substantially
outperform other systems. However, the generated code causes substantial
computation redundancy and the compilation process incurs too much overhead to
be used online, both due to the inherent symmetry in the structural patterns.
  In this paper, we propose an optimizing compiler, GraphZero, to completely
address these limitations through symmetry breaking based on group theory.
GraphZero implements three novel techniques. First, its schedule explorer
efficiently prunes the schedule space without missing any high-performance
schedule. Second, it automatically generates and enforces a set of restrictions
to eliminate computation redundancy. Third, it generalizes orientation, a
surprisingly effective optimization that was mainly used for clique patterns,
to apply to arbitrary patterns. Evaluated on multiple graph mining applications
and complex patterns with 7 real-world graph datasets, GraphZero demonstrates
up to 40X performance improvement and up to 197X reduction on schedule
generation overhead over AutoMine.
"
1941,"A PHY Layer Security Analysis of Uplink Cooperative Jamming-Based
  Underlay CRNs with Multi-Eavesdroppers","  In this paper, the physical layer security of a dual-hop underlay uplink
cognitive radio network is investigated over Nakagami-m fading channels.
Specifically, multiple secondary sources are taking turns in accessing the
licensed spectrum of the primary users and communicating with a multiantenna
secondary base station D through the aid of a multiantenna relay R in the
presence of M eavesdroppers that are also equipped with multiple antennas.
Among the remaining nodes, one jammer is randomly selected to transmit an
artificial noise to disrupt all the eavesdroppers that are attempting to
intercept the communication of the legitimate links i.e., S-R and R-D. The
received signals at each node are combined using maximal-ratio combining.
Secrecy analysis is provided by deriving closed-form and asymptotic expressions
for the secrecy outage probability. The impact of several key parameters on the
system's secrecy e.g., transmit power of the sources, number of eavesdroppers,
maximum tolerated interference power, and the number of diversity branches is
investigated. Importantly, by considering two scenarios, namely (i) absence and
(ii) presence of a friendly jammer, new insights are obtained for the
considered communication system. Especially, we tend to answer to the following
question: Can better secrecy be achieved without jamming by considering a
single antenna at eavesdroppers and multiple-ones at the legitimate users
(i.e., relay and end-user) rather than sending permanently an artificial noise
and considering that both the relay and the destination are equipped with a
single antenna, while multiple antennas are used by the eavesdroppers? The
obtained results are corroborated through Monte Carlo simulation and show that
the system's security can be enhanced by adjusting the aforementioned
parameters.
"
1942,"Using performance analysis tools for parallel-in-time integrators --
  Does my time-parallel code do what I think it does?","  While many ideas and proofs of concept for parallel-in-time integration
methods exists, the number of large-scale, accessible time-parallel codes is
rather small. This is often due to the apparent or subtle complexity of the
algorithms and the many pitfalls awaiting developers of parallel numerical
software. One example of such a time-parallel code is pySDC, which implements,
among others, the parallel full approximation scheme in space and time
(PFASST). Inspired by nonlinear multigrid ideas, PFASST allows to integrate
multiple time-steps simultaneously using a space-time hierarchy of spectral
deferred corrections. In this paper we demonstrate the application of
performance analysis tools to the PFASST implementation pySDC. Tracing the path
we took for this work, we highlight the obstacles encountered, describe
remedies and explain the sometimes surprising findings made possible by the
tools. Although focusing only on a single implementation of a particular
parallel-in-time integrator, we hope that our results and in particular the way
we obtained them are a blueprint for other time-parallel codes.
"
1943,"Efficient method for parallel computation of geodesic transformation on
  CPU","  This paper introduces a fast Central Processing Unit (CPU) implementation of
geodesic morphological operations using stream processing. In contrast to the
current state-of-the-art, that focuses on achieving insensitivity to the filter
sizes with efficient data structures, the proposed approach achieves efficient
computation of long chains of elementary $3 \times 3$ filters using multicore
and Single Instruction Multiple Data (SIMD) processing. In comparison to the
related methods, up to $100$ times faster computation of common geodesic
operators is achieved in this way, allowing for real-time processing (with over
$30$ FPS) of up to $1500$ filters long chains, applied on $1024\times 1024$
images. In addition, the proposed approach outperformed GPGPU, and proved to be
more efficient than the comparable streaming method for the computation of
morphological erosions and dilations with window sizes up to $183\times 183$ in
the case of using char and $27\times27$ when using double data types.
"
1944,Hardware Versus Software Fault Injection of Modern Undervolted SRAMs,"  To improve power efficiency, researchers are experimenting with dynamically
adjusting the supply voltage of systems below the nominal operating points.
However, production systems are typically not allowed to function on voltage
settings that is below the reliable limit. Consequently, existing software
fault tolerance studies are based on fault models, which inject faults on
random fault locations using fault injection techniques. In this work we study
whether random fault injection is accurate to simulate the behavior of
undervolted SRAMs.
  Our study extends the Gem5 simulator to support fault injection on the caches
of the simulated system. The fault injection framework uses fault maps, which
describe the faulty bits of SRAMs, as inputs. To compare random fault injection
and hardware guided fault injection, we use two types of fault maps. The first
type of maps are created through undervolting real SRAMs and observing the
location of the erroneous bits, whereas the second type of maps are created by
corrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache
of the simulated system and we monitor the behavior of the two types of fault
maps on the resiliency of six benchmarks. The difference among the resiliency
of a benchmark when tested with the different fault maps can be up to 24%.
"
1945,BenchCouncil's View on Benchmarking AI and Other Emerging Workloads,"  This paper outlines BenchCouncil's view on the challenges, rules, and vision
of benchmarking modern workloads like Big Data, AI or machine learning, and
Internet Services. We conclude the challenges of benchmarking modern workloads
as FIDSS (Fragmented, Isolated, Dynamic, Service-based, and Stochastic), and
propose the PRDAERS benchmarking rules that the benchmarks should be specified
in a paper-and-pencil manner, relevant, diverse, containing different levels of
abstractions, specifying the evaluation metrics and methodology, repeatable,
and scaleable. We believe proposing simple but elegant abstractions that help
achieve both efficiency and general-purpose is the final target of benchmarking
in future, which may be not pressing. In the light of this vision, we shortly
discuss BenchCouncil's related projects.
"
1946,"Mobile Energy Requirements of the Upcoming NIST Post-Quantum
  Cryptography Standards","  Standardization of Post-Quantum Cryptography (PQC) was started by NIST in
2016 and has proceeded to its second elimination round. The upcoming standards
are intended to replace (or supplement) current RSA and Elliptic Curve
Cryptography (ECC) on all targets, including lightweight, embedded, and mobile
systems. We present an energy requirement analysis based on extensive
measurements of PQC candidate algorithms on a Cortex M4 - based reference
platform. We relate computational (energy) costs of PQC algorithms to their
data transmission costs which are expected to increase with new types of public
keys and ciphertext messages. The energy, bandwidth, and latency needs of PQC
algorithms span several orders of magnitude, which is substantial enough to
impact battery life, user experience, and application protocol design. We
propose metrics and guidelines for PQC algorithm usage in IoT and mobile
systems based on our findings. Our evidence supports the view that fast
structured-lattice PQC schemes are the preferred choice for cloud-connected
mobile devices in most use cases, even when per-bit data transmission energy
cost is relatively high.
"
1947,"SimAS: A Simulation-assisted Approach for the Scheduling Algorithm
  Selection under Perturbations","  Many scientific applications consist of large and computationally-intensive
loops. Dynamic loop self-scheduling (DLS) techniques are used to parallelize
and to balance the load during the execution of such applications. Load
imbalance arises from variations in the loop iteration (or tasks) execution
times, caused by problem, algorithmic, or systemic characteristics. The
variations in systemic characteristics are referred to as perturbations, and
can be caused by other applications or processes that share the same resources,
or a temporary system fault or malfunction. Therefore, the selection of the
most efficient DLS technique is critical to achieve the best application
performance. The following question motivates this work: Given an application,
an HPC system, and their characteristics and interplay, which DLS technique
will achieve improved performance under unpredictable perturbations? Existing
studies focus on variations in the delivered computational speed only as the
source of perturbations in the system. However, perturbations in available
network bandwidth or latency are inevitable on production HPC systems. A
Simulator-assisted scheduling (SimAS) is introduced as a new
control-theoretic-inspired approach to dynamically select DLS techniques that
improve the performance of applications executing on heterogeneous HPC systems
under perturbations. The present work examines the performance of seven
applications on a heterogeneous system under all the above system
perturbations. SimAS is evaluated as a proof of concept using native and
simulative experiments. The performance results confirm the original hypothesis
that no single DLS technique can deliver the absolute best performance in all
scenarios, whereas the SimAS-based DLS selection resulted in improved
application performance in most experiments.
"
1948,"Perseus: Characterizing Performance and Cost of Multi-Tenant Serving for
  CNN Models","  Deep learning models are increasingly used for end-user applications,
supporting both novel features such as facial recognition, and traditional
features, e.g. web search. To accommodate high inference throughput, it is
common to host a single pre-trained Convolutional Neural Network (CNN) in
dedicated cloud-based servers with hardware accelerators such as Graphics
Processing Units (GPUs). However, GPUs can be orders of magnitude more
expensive than traditional Central Processing Unit (CPU) servers. These
resources could also be under-utilized facing dynamic workloads, which may
result in inflated serving costs. One potential way to alleviate this problem
is by allowing hosted models to share the underlying resources, which we refer
to as multi-tenant inference serving. One of the key challenges is maximizing
the resource efficiency for multi-tenant serving given hardware with diverse
characteristics, models with unique response time Service Level Agreement
(SLA), and dynamic inference workloads. In this paper, we present Perseus, a
measurement framework that provides the basis for understanding the performance
and cost trade-offs of multi-tenant model serving. We implemented Perseus in
Python atop a popular cloud inference server called Nvidia TensorRT Inference
Server. Leveraging Perseus, we evaluated the inference throughput and cost for
serving various models and demonstrated that multi-tenant model serving led to
up to 12% cost reduction.
"
1949,"Survey of prognostics methods for condition-based maintenance in
  engineering systems","  It is not surprising that the idea of efficient maintenance algorithms
(originally motivated by strict emission regulations, and now driven by safety
issues, logistics and customer satisfaction) has culminated in the so-called
condition-based maintenance program. Condition-based program/monitoring
consists of two major tasks, i.e., \textit{diagnostics} and
\textit{prognostics} each of which has provided the impetus and technical
challenges to the scientists and engineers in various fields of engineering.
Prognostics deals with the prediction of the remaining useful life, future
condition, or probability of reliable operation of an equipment based on the
acquired condition monitoring data. This approach to modern maintenance
practice promises to reduce the downtime, spares inventory, maintenance costs,
and safety hazards. Given the significance of prognostics capabilities and the
maturity of condition monitoring technology, there have been an increasing
number of publications on machinery prognostics in the past few years. These
publications cover a wide range of issues important to prognostics.
Fortunately, improvement in computational resources technology has come to the
aid of engineers by presenting more powerful onboard computational resources to
make some aspects of these new problems tractable. In addition, it is possible
to even leverage connected vehicle information through cloud-computing. Our
goal is to review the state of the art and to summarize some of the recent
advances in prognostics with the emphasis on models, algorithms and
technologies used for data processing and decision making.
"
1950,Analysis of AeroMACS Data Link for Unmanned Aircraft Vehicles,"  Aeronautical Mobile Airport Communications System (AeroMACS) is based on the
IEEE 802.16e mobile wireless standard commonly known as WiMAX. It is expected
to be the main part of the next-generation aviation communication system to
support fixed and mobile services for manned and unmanned applications.
AeroMACS will be an essential technology helping pave the way toward full
integration of Unmanned Aircraft Vehicle (UAV) into the national airspace. A
number of practical tests and analyses have been done so far for AeroMACS. The
main contribution of this paper is to consider the theoretical concepts behind
its features and discuss their suitability for UAV applications. Mathematical
analyses of the AeroMACS physical layer framework are provided to show the
theoretical trade-offs. We mainly focus on the analysis of the AeroMACS OFDMA
structure, which affects the speed limits, coverage cell, channel estimation
requirements, and inter-carrier interference.
"
1951,Scheduling in the Presence of Data Intensive Compute Jobs,"  We study the performance of non-adaptive scheduling policies in computing
systems with multiple servers. Compute jobs are mostly regular, with modest
service requirements. However, there are sporadic data intensive jobs, whose
expected service time is much higher than that of the regular jobs. Forthis
model, we are interested in the effect of scheduling policieson the average
time a job spends in the system. To this end, we introduce two performance
indicators in a simplified, only-arrival system. We believe that these
performance indicators are good predictors of the relative performance of the
policies in the queuing system, which is supported by simulations results.
"
1952,Dissecting the Graphcore IPU Architecture via Microbenchmarking,"  This report focuses on the architecture and performance of the Intelligence
Processing Unit (IPU), a novel, massively parallel platform recently introduced
by Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML)
workloads. We dissect the IPU's performance behavior using microbenchmarks that
we crafted for the purpose. We study the IPU's memory organization and
performance. We study the latency and bandwidth that the on-chip and off-chip
interconnects offer, both in point-to-point transfers and in a spectrum of
collective operations, under diverse loads. We evaluate the IPU's compute power
over matrix multiplication, convolution, and AI/ML primitives. We discuss
actual performance in comparison with its theoretical limits. Our findings
reveal how the IPU's architectural design affects its performance. Moreover,
they offer simple mental models to predict an application's performance on the
IPU, on the basis of the computation and communication steps it involves. This
report is the natural extension to a novel architecture of a continuing effort
of ours that focuses on the microbenchmark-based discovery of massively
parallel architectures.
"
1953,General Matrix-Matrix Multiplication Using SIMD features of the PIII,"  Generalised matrix-matrix multiplication forms the kernel of many
mathematical algorithms. A faster matrix-matrix multiply immediately benefits
these algorithms. In this paper we implement efficient matrix multiplication
for large matrices using the floating point Intel Pentium SIMD (Single
Instruction Multiple Data) architecture. A description of the issues and our
solution is presented, paying attention to all levels of the memory hierarchy.
Our results demonstrate an average performance of 2.09 times faster than the
leading public domain matrix-matrix multiply routines.
"
1954,"Massive Wireless Energy Transfer: Enabling Sustainable IoT Towards 6G
  Era","  Recent advances on wireless energy transfer led to a promising solution for
powering future Internet of Things (IoT) devices enabled by the upcoming sixth
generation (6G) era. In this paper, we overview the main architectures,
challenges and techniques for efficient wireless powering. We emphasize on the
suitability of Channel State Information (CSI)-free strategies when powering
simultaneously a massive number of devices. We show that even without
considering the energy resources required for CSI acquisition, the gains from
operating with CSI decrease quickly as the number of powered devices increases,
while distributed CSI-free strategies become even more advantageous by widening
the energy coverage region with high reliability. Overall, advances in the
system design and in resource allocation strategies are required for optimizing
the energy supplying process, specially under the challenges imposed by future
networks.
"
1955,Breaking the Limits of Redundancy Systems Analysis,"  Redundancy mechanisms such as triple modular redundancy protect
safety-critical components by replication and thus improve systems fault
tolerance. However, the gained fault tolerance comes along with costs to be
invested, e.g., increasing execution time, energy consumption, or packaging
size, for which constraints have to be obeyed during system design. This turns
the question of finding suitable combinations of components to be protected
into a challenging task as the number of possible protection combinations grows
exponentially in the number of components. We propose family-based approaches
to tackle the combinatorial blowup in redundancy systems modeling and analysis
phases. Based on systems designed in SIMULINK we show how to obtain models that
include all possible protection combinations and present a tool chain that,
given a probabilistic error model, generates discrete Markov chain families.
Using symbolic techniques that enable concise family representation and
analysis, we show how SIMULINK models of realistic size can be protected and
analyzed with a single family-based analysis run while a one-by-one analysis of
each protection combination would clearly exceed any realistic time
constraints.
"
1956,"Seismic Imaging: An Overview and Parallel Implementation of Poststack
  Depth Migration","  Seismic migration is the core step of seismic data processing which is
important for oil exploration. Poststack depth migration in frequency-space
(f-x) domain is one of commonly used algorithms. The wave-equation solution can
be approximated as FIR filtering process to extrapolate the raw data and
extract the subsurface image. Because of its computational complexity, its
parallel implementation is encouraged. For calculating the next depth level,
previous depth level is required. So, this part cannot be parallelized because
of data dependence. But at each depth level there is plenty of roam for
parallelism and can be parallelized. In case of CUDA programming, each thread
calculate a single pixel on the next depth plan. After calculating the next
depth plan, we can calculate the depth row by summing over all the frequencies
and calculating all the depth rows results in the final migrated image. The
poststack depth migration is implemented in CUDA and its performance is
evaluated with the sequential code with different problem sizes.
"
1957,"Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching:
  A Closed-Form Characterization","  GPU-accelerated computing is a key technology to realize high-speed inference
servers using deep neural networks (DNNs). An important characteristic of
GPU-based inference is that the computational efficiency, in terms of the
processing speed and energy consumption, drastically increases by processing
multiple jobs together in a batch. In this paper, we formulate GPU-based
inference servers as a batch service queueing model with batch-size dependent
processing times. We first show that the energy efficiency of the server
monotonically increases with the arrival rate of inference jobs, which suggests
that it is energy-efficient to operate the inference server under a utilization
level as high as possible within a latency requirement of inference jobs. We
then derive a closed-form upper bound for the mean latency, which provides a
simple characterization of the latency performance. Through simulation and
numerical experiments, we show that the exact value of the mean latency is well
approximated by this upper bound.
"
1958,ORCA: a Benchmark for Data Web Crawlers,"  The number of RDF knowledge graphs available on the Web grows constantly.
Gathering these graphs at large scale for downstream applications hence
requires the use of crawlers. Although Data Web crawlers exist, and general Web
crawlers could be adapted to focus on the Data Web, there is currently no
benchmark to fairly evaluate their performance. Our work closes this gap by
presenting the Orca benchmark. Orca generates a synthetic Data Web, which is
decoupled from the original Web and enables a fair and repeatable comparison of
Data Web crawlers. Our evaluations show that Orca can be used to reveal the
different advantages and disadvantages of existing crawlers. The benchmark is
open-source and available at https://github.com/dice-group/orca.
"
1959,"Real-Time Prediction of Delay Distribution in Service Systems using
  Mixture Density Networks","  Motivated by interest in providing more efficient services in customer
service systems, we use statistical learning methods and delay history
information to predict the conditional distribution of the customers' waiting
times in queueing systems. From the predicted distributions, descriptive
statistics of the system such as the mean, variance and percentiles of the
waiting times can be obtained, which can be used for delay announcements, SLA
conformance and better system management. We model the conditional
distributions by mixtures of Gaussians, parameters of which can be estimated
using Mixture Density Networks. The evaluations show that exploiting more delay
history information can result in much more accurate predictions under
realistic time-varying arrival assumptions.
"
1960,"Slim Graph: Practical Lossy Graph Compression for Approximate Graph
  Processing, Storage, and Analytics","  We propose Slim Graph: the first programming model and framework for
practical lossy graph compression that facilitates high-performance approximate
graph processing, storage, and analytics. Slim Graph enables the developer to
express numerous compression schemes using small and programmable compression
kernels that can access and modify local parts of input graphs. Such kernels
are executed in parallel by the underlying engine, isolating developers from
complexities of parallel programming. Our kernels implement novel graph
compression schemes that preserve numerous graph properties, for example
connected components, minimum spanning trees, or graph spectra. Finally, Slim
Graph uses statistical divergences and other metrics to analyze the accuracy of
lossy graph compression. We illustrate both theoretically and empirically that
Slim Graph accelerates numerous graph algorithms, reduces storage used by graph
datasets, and ensures high accuracy of results. Slim Graph may become the
common ground for developing, executing, and analyzing emerging lossy graph
compression schemes.
"
1961,Is Big Data Performance Reproducible in Modern Cloud Networks?,"  Performance variability has been acknowledged as a problem for over a decade
by cloud practitioners and performance engineers. Yet, our survey of top
systems conferences reveals that the research community regularly disregards
variability when running experiments in the cloud. Focusing on networks, we
assess the impact of variability on cloud-based big-data workloads by gathering
traces from mainstream commercial clouds and private research clouds. Our data
collection consists of millions of datapoints gathered while transferring over
9 petabytes of data. We characterize the network variability present in our
data and show that, even though commercial cloud providers implement mechanisms
for quality-of-service enforcement, variability still occurs, and is even
exacerbated by such mechanisms and service provider policies. We show how
big-data workloads suffer from significant slowdowns and lack predictability
and replicability, even when state-of-the-art experimentation techniques are
used. We provide guidelines for practitioners to reduce the volatility of big
data performance, making experiments more repeatable.
"
1962,"Comparing Hierarchical Data Structures for Sparse Volume Rendering with
  Empty Space Skipping","  Empty space skipping can be efficiently implemented with hierarchical data
structures such as k-d trees and bounding volume hierarchies. This paper
compares several recently published hierarchical data structures with regard to
construction and rendering performance. The papers that form our prior work
have primarily focused on interactively building the data structures and only
showed that rendering performance is superior to using simple acceleration data
structures such as uniform grids with macro cells. In the area of surface ray
tracing, there exists a trade-off between construction and rendering
performance of hierarchical data structures. In this paper we present
performance comparisons for several empty space skipping data structures in
order to determine if such a trade-off also exists for volume rendering with
uniform data topologies.
"
1963,Speeding up Generalized PSR Parsers by Memoization Techniques,"  Predictive shift-reduce (PSR) parsing for hyperedge replacement (HR) grammars
is very efficient, but restricted to a subclass of unambiguous HR grammars. To
overcome this restriction, we have recently extended PSR parsing to generalized
PSR (GPSR) parsing along the lines of Tomita-style generalized LR parsing.
Unfortunately, GPSR parsers turned out to be too inefficient without manual
tuning. This paper proposes to use memoization techniques to speed up GPSR
parsers without any need of manual tuning, and which has been realized within
the graph parser distiller Grappa. We present running time measurements for
some example languages; they show a significant speed up by some orders of
magnitude when parsing valid graphs. But memoization techniques do not help
when parsing invalid graphs or if all parses of an ambiguous input graph shall
be determined.
"
1964,"Spatio-Temporal Correlation of Interference in MANET Under Spatially
  Correlated Shadowing Environment","  Correlation of interference affects spatio-temporal aspects of various
wireless mobile systems, such as retransmission, multiple antennas and
cooperative relaying. In this paper, we study the spatial and temporal
correlation of interference in mobile ad-hoc networks under a correlated
shadowing environment. By modeling the node locations as a Poisson point
process with an i.i.d. mobility model and considering Gudmundson (1991)' s
spatially correlated shadowing model, we theoretically analyze the relationship
between the correlation distance of log-normal shadowing and the spatial and
temporal correlation coefficients of interference. Since the exact expressions
of the correlation coefficients are intractable, we obtain their simple
asymptotic expressions as the variance of log-normal shadowing increases. We
found in our numerical examples that the asymptotic expansions can be used as
tight approximate formulas and useful for modeling general wireless systems
under spatially correlated shadowing.
"
1965,"Download Time Analysis for Distributed Storage Codes with Locality and
  Availability","  Availability codes have recently been proposed to facilitate efficient
retrieval of frequently accessed (hot) data objects in distributed storage
systems. This paper presents techniques for analyzing the download time of
systematic availability codes considering the Fork-Join scheme for data access.
Specifically, we consider the setup in which requests arrive for downloading
individual data objects, and each request is replicated (forked) to the
systematic server containing the object and all of its recovery groups. For
low-traffic regime, when there is at most one request in the system, we compute
the download time in closed-form and compare it across systems with
availability, maximum distance separable (MDS), and replication codes. We
demonstrate that availability codes can reduce download time in some settings,
but are not always optimal. When the low-traffic assumption does not hold,
system consists of multiple inter-dependent Fork-Join queues, which makes exact
analysis intractable due to state space explosion. Here, we present upper and
lower bounds on the download time, and an M/G/1 queue approximation for several
special cases of interest. Via extensive numerical simulations, we evaluate our
bounds, and demonstrate that the M/G/1 queue approximation has a high degree of
accuracy.
"
1966,"Performance and Cost Evaluation of Smart Contracts in Collaborative
  Health Care Environments","  Blockchain emerged as a solution for data integrity, non-repudiation, and
availability in different applications. Data sensitive scenarios, such as
Health Care, can also benefit from these blockchain properties. Consequently,
different research proposed the adoption of blockchain in Health Care
applications. However, few are discussed about incentive methods to attract new
users, as well as to motivate the system or application usage by existing
end-users. Also, little is discussed about performance during code execution in
blockchains. In order to tackle these issues, this work presents the
preliminary evaluation of TokenHealth, an application for collaborative health
practice monitoring with gamification and token-based incentives. The proposed
solution is implemented through smart contracts using Solidity in the Ethereum
blockchain. We evaluated the performance of both in Ropsten test network and in
a Private instance. The preliminary results show that the execution of smart
contracts takes less than a minute for a full cycle of different smart
contracts. Also, we present a discussion about costs for using a Private
instance and the public Ethereum main network.
"
1967,Community detection in node-attributed social networks: a survey,"  Community detection is a fundamental problem in social network analysis
consisting in unsupervised dividing social actors (nodes in a social graph)
with certain social connections (edges in a social graph) into densely knitted
and highly related groups with each group well separated from the others.
Classical approaches for community detection usually deal only with network
structure and ignore features of its nodes (called node attributes), although
many real-world social networks provide additional actors' information such as
interests. It is believed that the attributes may clarify and enrich the
knowledge about the actors and give sense to the communities. This belief has
motivated the progress in developing community detection methods that use both
the structure and the attributes of network (i.e. deal with a node-attributed
graph) to yield more informative and qualitative results.
  During the last decade many such methods based on different ideas have
appeared. Although there exist partial overviews of them, a recent survey is a
necessity as the growing number of the methods may cause repetitions in
methodology and uncertainty in practice.
  In this paper we aim at describing and clarifying the overall situation in
the field of community detection in node-attributed social networks. Namely, we
perform an exhaustive search of known methods and propose a classification of
them based on when and how structure and attributes are fused. We not only give
a description of each class but also provide general technical ideas behind
each method in the class. Furthermore, we pay attention to available
information which methods outperform others and which datasets and quality
measures are used for their evaluation. Basing on the information collected, we
make conclusions on the current state of the field and disclose several
problems that seem important to be resolved in future.
"
1968,"A QoS-aware workload routing and server speed scaling policy for
  energy-efficient data centers: a robust queueing theoretic approach","  Maintaining energy efficiency in large data centers depends on the ability to
manage workload routing and control server speeds according to fluctuating
demand. The use of dynamic algorithms often means that management has to
install the complicated software or expensive hardware needed to communicate
with routers and servers. This paper proposes a static routing and server speed
scaling policy that may achieve energy efficiency similar to dynamic algorithms
and eliminate the necessity of frequent communications among resources without
compromising quality of service (QoS). We use a robust queueing approach to
consider the response time constraints, e.g., service level agreements (SLAs).
We model each server as a $G/G/1$ processor sharing (PS) queue and use
uncertainty sets to define the domain of random variables. A comparison with a
dynamic algorithm shows that the proposed static policy provides competitive
solutions in terms of energy efficiency and satisfactory QoS.
"
1969,"Deadline-aware Scheduling for Maximizing Information Freshness in
  Industrial Cyber-Physical System","  Age of Information is an interesting metric that captures the freshness of
information in the underlying applications. It is a combination of both packets
inter-arrival time and packet transmission delay. In recent times, advanced
real-time systems rely on this metric for delivering status updates as timely
as possible. This paper aims to accomplish optimal transmission scheduling
policy to maintain the information freshness of real-time updates in the
industrial cyber-physical systems. Here the coexistence of both cyber and
physical units and their individual requirements to provide the quality of
service is one of the critical challenges to handle. A greedy scheduling policy
called deadline-aware highest latency first has been proposed for this purpose.
This paper also gives the analytical proof of its optimality, and finally, the
claim is validated by comparing the performance of our algorithm with other
scheduling policies by extensive simulations.
"
1970,Performance Tuning and Scaling Enterprise Blockchain Applications,"  Blockchain scalability can be complicated and costly. As enterprises begin to
adopt blockchain technology to solve business problems, there are valid
concerns if blockchain applications can support the transactional demands of
production systems. In fact, the multiple distributed components and protocols
that underlie blockchain applications makes performance optimization a
non-trivial task. Blockchain performance optimization and scalability require a
methodology to reduce complexity and cost. Furthermore, existing performance
results often lack the requirements, load, and infrastructure of a production
application. In this paper, we first develop a methodical approach to
performance tuning enterprise blockchain applications to increase performance
and transaction capacity. The methodology is applied to an enterprise
blockchain-based application (leveraging Hyperledger Fabric) for performance
tuning and optimization with the goal of bridging the gap between laboratory
and production deployed system performance. We then present extensive results
and analysis of our performance testing for on-premise and cloud deployments,
in which we were able to scale the application from 30 to 3000 TPS without
forking the Hyperledger Fabric source code and maintaining a reasonable
infrastructure footprint. We also provide blockchain application and platform
recommendations for performance improvement.
"
1971,Large fork-join networks with nearly deterministic service times,"  In this paper, we study an $N$ server fork-join queueing network with nearly
deterministic arrivals and service times. Specifically, we aim to approximate
the length of the largest of the $N$ queues in the network. From a practical
point of view, this has interesting applications, such as modeling the delays
in a large supply chain. We present a fluid limit and a steady-state result for
the maximum queue length, as $N\to\infty$. These results have remarkable
differences. The steady-state result depends on two model parameters, while the
fluid limit only depends on one model parameter. In addition, the fluid limit
requires a different spatial scaling than the backlog in steady state. In order
to prove these results, we use extreme value theory and diffusion
approximations for the queue lengths.
"
1972,"Performance benefits of Intel(R) OptaneTM DC persistent memory for the
  parallel processing of large neuroimaging data","  Open-access neuroimaging datasets have reached petabyte scale, and continue
to grow. The ability to leverage the entirety of these datasets is limited to a
restricted number of labs with both the capacity and infrastructure to process
the data. Whereas Big Data engines have significantly reduced application
performance penalties with respect to data movement, their applied strategies
(e.g. data locality, in-memory computing and lazy evaluation) are not
necessarily practical within neuroimaging workflows where intermediary results
may need to be materialized to shared storage for post-processing analysis. In
this paper we evaluate the performance advantage brought by Intel(R) OptaneTM
DC persistent memory for the processing of large neuroimaging datasets using
the two available configurations modes: Memory mode and App Direct mode. We
employ a synthetic algorithm on the 76 GiB and 603 GiB BigBrain, as well as
apply a standard neuroimaging application on the Consortium for Reliability and
Reproducibility (CoRR) dataset using 25 and 96 parallel processes in both
cases. Our results show that the performance of applications leveraging
persistent memory is superior to that of other storage devices,with the
exception of DRAM. This is the case in both Memory and App Direct mode and
irrespective of the amount of data and parallelism. Furthermore, persistent
memory in App Direct mode is believed to benefit from the use of DRAM as a
cache for writing when output data is significantly smaller than available
memory. We believe the use of persistent memory will be beneficial to both
neuroimaging applications running on HPC or visualization of large,
high-resolution images.
"
1973,"On the Reproducibility of Experiments of Indexing Repetitive Document
  Collections","  This work introduces a companion reproducible paper with the aim of allowing
the exact replication of the methods, experiments, and results discussed in a
previous work [5]. In that parent paper, we proposed many and varied techniques
for compressing indexes which exploit that highly repetitive collections are
formed mostly of documents that are near-copies of others. More concretely, we
describe a replication framework, called uiHRDC (universal indexes for Highly
Repetitive Document Collections), that allows our original experimental setup
to be easily replicated using various document collections. The corresponding
experimentation is carefully explained, providing precise details about the
parameters that can be tuned for each indexing solution. Finally, note that we
also provide uiHRDC as reproducibility package.
"
1974,Cache Optimization Models and Algorithms,"  Storage resources and caching techniques permeate almost every area of
communication networks today. In the near future, caching is set to play an
important role in storage-assisted Internet architectures, information-centric
networks, and wireless systems, reducing operating and capital expenditures and
improving the services offered to users. In light of the remarkable data
traffic growth and the increasing number of rich-media applications, the impact
of caching is expected to become even more profound than it is today.
Therefore, it is crucial to design these systems in an optimal fashion,
ensuring the maximum possible performance and economic benefits from their
deployment. To this end, this article presents a collection of detailed models
and algorithms, which are synthesized to build a powerful analytical framework
for caching optimization.
"
1975,VulnDS: Top-k Vulnerable SME Detection System in Networked-Loans,"  Groups of small and medium enterprises (SMEs) can back each other to obtain
loans from banks and thus form guarantee networks. If the loan repayment of a
small company in the network defaults, its backers are required to repay the
loan. Therefore, risk over networked enterprises may cause significant
contagious damage. In real-world applications, it is critical to detect top
vulnerable nodes in such complex financial network with near real-time
performance. To address this challenge, we introduce VulnDS: a top-k vulnerable
SME detection system for large-scale financial networks, which is deployed in
our collaborated bank. First, we model the risks of the guaranteed-loan network
by a probabilistic graph, which consists of the guarantee-loan network
structure, self-risk probability for the nodes and diffusion probability for
the edges. Moreover, to identify the vulnerable enterprises, we propose a
sampling-based approach with tight theoretical guarantee. Novel optimization
techniques are developed in order to scale for large networks. We conduct
extensive experiments on 3 real financial datasets, in addition with 5
large-scale benchmark networks. The evaluation results show that the proposed
method can achieve up to 100x speedup ratio compared with baseline methods.
Case studies are further conducted in the deployed system to demonstrate the
effectiveness of proposed model.
"
1976,"On the Asymptotic Optimality of Work-Conserving Disciplines in
  Completion Time Minimization","  In this paper, we prove that under mild stochastic assumptions,
work-conserving disciplines are asymptotic optimal for minimizing total
completion time. As a byproduct of our analysis, we obtain tight upper bound on
the competitive ratios of work-conserving disciplines on minimizing the metric
of flow time.
"
1977,"On the Performance and Energy Efficiency of the PGAS Programming Model
  on Multicore Architectures","  Using large-scale multicore systems to get the maximum performance and energy
efficiency with manageable programmability is a major challenge. The
partitioned global address space (PGAS) programming model enhances
programmability by providing a global address space over large-scale computing
systems. However, so far the performance and energy efficiency of the PGAS
model on multicore-based parallel architectures have not been investigated
thoroughly. In this paper we use a set of selected kernels from the well-known
NAS Parallel Benchmarks to evaluate the performance and energy efficiency of
the UPC programming language, which is a widely used implementation of the PGAS
model. In addition, the MPI and OpenMP versions of the same parallel kernels
are used for comparison with their UPC counterparts. The investigated hardware
platforms are based on multicore CPUs, both within a single 16-core node and
across multiple nodes involving up to 1024 physical cores. On the multi-node
platform we used the hardware measurement solution called High definition
Energy Efficiency Monitoring tool in order to measure energy. On the
single-node system we used the hybrid measurement solution to make an effort
into understanding the observed performance differences, we use the Intel
Performance Counter Monitor to quantify in detail the communication time, cache
hit/miss ratio and memory usage. Our experiments show that UPC is competitive
with OpenMP and MPI on single and multiple nodes, with respect to both the
performance and energy efficiency.
"
1978,"Performance optimization and modeling of fine-grained irregular
  communication in UPC","  The UPC programming language offers parallelism via logically partitioned
shared memory, which typically spans physically disjoint memory sub-systems.
One convenient feature of UPC is its ability to automatically execute
between-thread data movement, such that the entire content of a shared data
array appears to be freely accessible by all the threads. The programmer
friendliness, however, can come at the cost of substantial performance
penalties. This is especially true when indirectly indexing the elements of a
shared array, for which the induced between-thread data communication can be
irregular and have a fine-grained pattern. In this paper we study performance
enhancement strategies specifically targeting such fine-grained irregular
communication in UPC. Starting from explicit thread privatization, continuing
with block-wise communication, and arriving at message condensing and
consolidation, we obtained considerable performance improvement of UPC programs
that originally require fine-grained irregular communication. Besides the
performance enhancement strategies, the main contribution of the present paper
is to propose performance models for the different scenarios, in form of
quantifiable formulas that hinge on the actual volumes of various data
movements plus a small number of easily obtainable hardware characteristic
parameters. These performance models help to verify the enhancements obtained,
while also providing insightful predictions of similar parallel
implementations, not limited to UPC, that also involve between-thread or
between-process irregular communication. As a further validation, we also apply
our performance modeling methodology and hardware characteristic parameters to
an existing UPC code for solving a 2D heat equation on a uniform mesh.
"
1979,"Practice of Streaming and Dynamic Graphs: Concepts, Models, Systems, and
  Parallelism","  Graph processing has become an important part of various areas of computing,
including machine learning, medical applications, social network analysis,
computational sciences, and others. A growing amount of the associated graph
processing workloads are dynamic, with millions of edges added or removed per
second. Graph streaming frameworks are specifically crafted to enable the
processing of such highly dynamic workloads. Recent years have seen the
development of many such frameworks. However, they differ in their general
architectures (with key details such as the support for the parallel execution
of graph updates, or the incorporated graph data organization), the types of
updates and workloads allowed, and many others. To facilitate the understanding
of this growing field, we provide the first analysis and taxonomy of dynamic
and streaming graph processing. We focus on identifying the fundamental system
designs and on understanding their support for concurrency and parallelism, and
for different graph updates as well as analytics workloads. We also crystallize
the meaning of different concepts associated with streaming graph processing,
such as dynamic, temporal, online, and time-evolving graphs, edge-centric
processing, models for the maintenance of updates, and graph databases.
Moreover, we provide a bridge with the very rich landscape of graph streaming
theory by giving a broad overview of recent theoretical related advances, and
by analyzing which graph streaming models and settings could be helpful in
developing more powerful streaming frameworks and designs. We also outline
graph streaming workloads and research challenges.
"
1980,Quantifying the Performance of Federated Transfer Learning,"  The scarcity of data and isolated data islands encourage different
organizations to share data with each other to train machine learning models.
However, there are increasing concerns on the problems of data privacy and
security, which urges people to seek a solution like Federated Transfer
Learning (FTL) to share training data without violating data privacy. FTL
leverages transfer learning techniques to utilize data from different sources
for training, while achieving data privacy protection without significant
accuracy loss. However, the benefits come with a cost of extra computation and
communication consumption, resulting in efficiency problems. In order to
efficiently deploy and scale up FTL solutions in practice, we need a deep
understanding on how the infrastructure affects the efficiency of FTL. Our
paper tries to answer this question by quantitatively measuring a real-world
FTL implementation FATE on Google Cloud. According to the results of carefully
designed experiments, we verified that the following bottlenecks can be further
optimized: 1) Inter-process communication is the major bottleneck; 2) Data
encryption adds considerable computation overhead; 3) The Internet networking
condition affects the performance a lot when the model is large.
"
1981,"The MAP/M/s+G Call Center Model with General Patience Times: Stationary
  Solutions and First Passage Times","  We study the MAP/M/s+G queuing model with MAP (Markovian Arrival Process)
arrivals, exponentially distributed service times, infinite waiting room, and
generally distributed patience times. Using sample-path arguments, we propose
to obtain the steady-state distribution of the virtual waiting time and
subsequently the other relevant performance metrics of interest for the
MAP/M/s+G queue by means of finding the steady-state solution of a properly
constructed Continuous Feedback Fluid Queue (CFFQ). The proposed method is
exact when the patience time is a discrete random variable and is
asymptotically exact when it is continuous/hybrid for which case discretization
of the patience time distribution and subsequently the steady-state solution of
a Multi-Regime Markov Fluid Queue (MRMFQ) is required. Besides the steady-state
distribution, we also propose a new method to approximately obtain the first
passage time distribution for the virtual and actual waiting times in the
$MAP/M/s+G$ queue. Again, using sample-path arguments, finding the desired
distribution is also shown to reduce to obtaining the steady-state solution of
a larger dimensionality CFFQ where the deterministic time horizon is to be
approximated by Erlang or Concentrated Matrix Exponential (CME) distributions.
Numerical results are presented to validate the effectiveness of the proposed
method.
"
1982,BIRL: Benchmark on Image Registration methods with Landmark validation,"  This report presents a generic image registration benchmark with automatic
evaluation using landmark annotations. The key features of the BIRL framework
are: easily extendable, performance evaluation, parallel experimentation,
simple visualisations, experiment's time-out limit, resuming unfinished
experiments. From the research practice, we identified and focused on these two
main use-cases: (a) comparison of user's (newly developed) method with some
State-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA
methods on user's custom dataset (which should contain landmark annotation).
Moreover, we present an integration of several standard image registration
methods aiming at biomedical imaging into the BIRL framework. This report also
contains experimental results of these SOTA methods on the CIMA dataset, which
is a dataset of Whole Slice Imaging (WSI) from histology/pathology containing
several multi-stain tissue samples from three tissue kinds. Source and results:
https://borda.github.io/BIRL
"
1983,Deep Learning Training with Simulated Approximate Multipliers,"  This paper presents by simulation how approximate multipliers can be utilized
to enhance the training performance of convolutional neural networks (CNNs).
Approximate multipliers have significantly better performance in terms of
speed, power, and area compared to exact multipliers. However, approximate
multipliers have an inaccuracy which is defined in terms of the Mean Relative
Error (MRE). To assess the applicability of approximate multipliers in
enhancing CNN training performance, a simulation for the impact of approximate
multipliers error on CNN training is presented. The paper demonstrates that
using approximate multipliers for CNN training can significantly enhance the
performance in terms of speed, power, and area at the cost of a small negative
impact on the achieved accuracy. Additionally, the paper proposes a hybrid
training method which mitigates this negative impact on the accuracy. Using the
proposed hybrid method, the training can start using approximate multipliers
then switches to exact multipliers for the last few epochs. Using this method,
the performance benefits of approximate multipliers in terms of speed, power,
and area can be attained for a large portion of the training stage. On the
other hand, the negative impact on the accuracy is diminished by using the
exact multipliers for the last epochs of training.
"
1984,A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs,"  Tensor computations present significant performance challenges that impact a
wide spectrum of applications ranging from machine learning, healthcare
analytics, social network analysis, data mining to quantum chemistry and signal
processing. Efforts to improve the performance of tensor computations include
exploring data layout, execution scheduling, and parallelism in common tensor
kernels. This work presents a benchmark suite for arbitrary-order sparse tensor
kernels using state-of-the-art tensor formats: coordinate (COO) and
hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of
reference tensor kernel implementations that are compatible with real-world
tensors and power law tensors extended from synthetic graph generation
techniques. We also propose Roofline performance models for these kernels to
provide insights of computer platforms from sparse tensor view.
"
1985,Block-Structured Double-Ended Queues and Bilateral QBD Processes,"  This paper studies a block-structured double-ended queue, whose block
structure comes from two independent Markovian arrival processes (MAPs), and
its stability is guaranteed by customers' impatient behaviors. We show that
such a queue can be expressed as a new bilateral quasi birth-and-death (QBD)
process. For this purpose, we provide a detailed analysis for the bilateral QBD
process, including the system stability, the stationary probability vector, the
sojourn time, and so forth. Furthermore, we develop three effective algorithms
for computing the performance measures (i.e., the probabilities of stationary
queue lengths, the average stationary queue lengths, and the average sojourn
times) of the block-structured double-ended queue. Finally, numerical examples
are employed to verify the correctness of our theoretical results, and
illustrate how the performance measures of this queue are influenced by key
system parameters. We believe that the methodology and results described in
this paper can be applied to deal with general matching queues (e.g., bilateral
Markov processes of GI/M/1 type and those of M/G/1 type) via developing their
corresponding bilateral block-structured Markov processes, which are very
useful in analyzing many practical issues, such as those encountered in sharing
economy, organ transplantation, intelligent manufacturing, intelligent
transportation, and so on.
"
1986,"Finally, how many efficiencies supercomputers have? And, what do they
  measure?","  Using an extremely large number of processing elements in computing systems
leads to unexpected phenomena, such as different efficiencies of the same
system for different tasks, that cannot be explained in the frame of classical
computing paradigm. The simple non-technical (but considering the temporal
behavior of the components) model, introduced here, enables us to set up a
frame and formalism, needed to explain those unexpected experiences around
supercomputing. Introducing temporal behavior into computer science also
explains why only the extreme scale computing enabled us to reveal the
experienced limitations. The paper shows, that degradation of efficiency of
parallelized sequential systems is a natural consequence of the classical
computing paradigm, instead of being an engineering imperfectness. The
workload, that supercomputers run, is much responsible for wasting energy, as
well as limiting the size and type of tasks. Case studies provide insight, how
different contributions compete for dominating the resulting payload
performance of a computing system, and how enhancing the interconnection
technology made computing+communication to dominate in defining the efficiency
of supercomputers. Our model also enables to derive predictions about
supercomputer performance limitations for the near future, as well as it
provides hints for enhancing supercomputer components. Phenomena experienced in
large-scale computing show interesting parallels with phenomena experienced in
science, more than a century ago, and through their studying a modern science
was developed.
"
1987,A Fast Analytical Model of Fully Associative Caches,"  While the cost of computation is an easy to understand local property, the
cost of data movement on cached architectures depends on global state, does not
compose, and is hard to predict. As a result, programmers often fail to
consider the cost of data movement. Existing cache models and simulators
provide the missing information but are computationally expensive. We present a
lightweight cache model for fully associative caches with least recently used
(LRU) replacement policy that gives fast and accurate results. We count the
cache misses without explicit enumeration of all memory accesses by using
symbolic counting techniques twice: 1) to derive the stack distance for each
memory access and 2) to count the memory accesses with stack distance larger
than the cache size. While this technique seems infeasible in theory, due to
non-linearities after the first round of counting, we show that the counting
problems are sufficiently linear in practice. Our cache model often computes
the results within seconds and contrary to simulation the execution time is
mostly problem size independent. Our evaluation measures modeling errors below
0.6% on real hardware. By providing accurate data placement information we
enable memory hierarchy aware software development.
"
1988,The LDBC Social Network Benchmark,"  The Linked Data Benchmark Council's Social Network Benchmark (LDBC SNB) is an
effort intended to test various functionalities of systems used for graph-like
data management. For this, LDBC SNB uses the recognizable scenario of operating
a social network, characterized by its graph-shaped data. LDBC SNB consists of
two workloads that focus on different functionalities: the Interactive workload
(interactive transactional queries) and the Business Intelligence workload
(analytical queries). This document contains the definition of the Interactive
Workload and the first draft of the Business Intelligence Workload. This
includes a detailed explanation of the data used in the LDBC SNB benchmark, a
detailed description for all queries, and instructions on how to generate the
data and run the benchmark with the provided software.
"
1989,"Improving BLE Beacon Proximity Estimation Accuracy through Bayesian
  Filtering","  The interconnectedness of all things is continuously expanding which has
allowed every individual to increase their level of interaction with their
surroundings. Internet of Things (IoT) devices are used in a plethora of
context-aware application such as Proximity-Based Services (PBS), and
Location-Based Services (LBS). For these systems to perform, it is essential to
have reliable hardware and predict a user's position in the area with high
accuracy in order to differentiate between individuals in a small area. A
variety of wireless solutions that utilize Received Signal Strength Indicators
(RSSI) have been proposed to provide PBS and LBS for indoor environments,
though each solution presents its own drawbacks. In this work, Bluetooth Low
Energy (BLE) beacons are examined in terms of their accuracy in proximity
estimation. Specifically, a mobile application is developed along with three
Bayesian filtering techniques to improve the BLE beacon proximity estimation
accuracy. This includes a Kalman filter, a particle filter, and a
Non-parametric Information (NI) filter. Since the RSSI is heavily influenced by
the environment, experiments were conducted to examine the performance of
beacons from three popular vendors in two different environments. The error is
compared in terms of Mean Absolute Error (MAE) and Root Mean Squared Error
(RMSE). According to the experimental results, Bayesian filters can improve
proximity estimation accuracy up to 30 % in comparison with traditional
filtering, when the beacon and the receiver are within 3 m.
"
1990,High Performance Depthwise and Pointwise Convolutions on Mobile Devices,"  Lightweight convolutional neural networks (e.g., MobileNets) are specifically
designed to carry out inference directly on mobile devices. Among the various
lightweight models, depthwise convolution (DWConv) and pointwise convolution
(PWConv) are their key operations. In this paper, we observe that the existing
implementations of DWConv and PWConv are not well utilizing the ARM processors
in the mobile devices, and exhibit lots of cache misses under multi-core and
poor data reuse at register level. We propose techniques to re-optimize the
implementations of DWConv and PWConv based on ARM architecture. Experimental
results show that our implementation can respectively achieve a speedup of up
to 5.5x and 2.1x against TVM (Chen et al. 2018) on DWConv and PWConv.
"
1991,SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers,"  Fine-grained power monitoring of software activities becomes unavoidable to
maximize the power usage efficiency of data centers. In particular, achieving
an optimal scheduling of containers requires the deployment of software-defined
power~meters to go beyond the granularity of hardware power monitoring sensors,
such as Power Distribution Units (PDU) or Intel's Running Average Power Limit
(RAPL), to deliver power estimations of activities at the granularity of
software~containers. However, the definition of the underlying power models
that estimate the power consumption remains a long and fragile process that is
tightly coupled to the host machine.
  To overcome these limitations, this paper introduces SmartWatts: a
lightweight power monitoring system that adopts online calibration to
automatically adjust the CPU and DRAM power models in order to maximize the
accuracy of runtime power estimations of containers. Unlike state-of-the-art
techniques, SmartWatts does not require any a priori training phase or hardware
equipment to configure the power models and can therefore be deployed on a wide
range of machines including the latest power optimizations, at no cost.
"
1992,On Competitive Analysis for Polling Systems,"  Polling systems have been widely studied, however most of these studies focus
on polling systems with renewal processes for arrivals and random variables for
service times. There is a need driven by practical applications to study
polling systems with arbitrary arrivals (not restricted to time-varying or in
batches) and revealed service time upon a job's arrival. To address that need,
our work considers a polling system with generic setting and for the first time
provides the worst-case analysis for online scheduling policies in this system.
We provide conditions for the existence of constant competitive ratios, and
competitive lower bounds for general scheduling policies in polling systems.
Our work also bridges the queueing and scheduling communities by proving the
competitive ratios for several well-studied policies in the queueing
literature, such as cyclic policies with exhaustive, gated or l-limited service
disciplines for polling systems.
"
1993,"Optimal Scheduling for Maximizing Information Freshness & System
  Performance in Industrial Cyber-Physical Systems","  Age of Information is a newly introduced metric, getting vivid attention for
measuring the freshness of information in real-time networks. This parameter
has evolved to guarantee the reception of timely information from the latest
status update, received by a user from any real-time application. In this
paper, we study a centralized, closed-loop, networked controlled industrial
wireless sensor-actuator network for cyber-physical production systems. Here,
we jointly address the problem of transmission scheduling of sensor updates and
the restoration of an information flow-line after any real-time update having
hard-deadline drops from it, resulting a break in the loop. Unlike existing
real-time scheduling policies that only ensure timely updates, this work aims
to accomplish both the time-sensitivity and data freshness in new and
regenerative real-time updates in terms of the age of information. Here, the
coexistence of both cyber and physical units and their individual requirements
for providing the quality of service to the system, as a whole, seems to be one
of the major challenges to handle. In this work, minimization of staleness of
the time-critical updates to extract maximum utilization out of its information
content and its effects on other network performances are thoroughly
investigated. A greedy scheduling policy called Deadline-aware highest latency
first has been used to solve this problem; its performance optimality is proved
analytically. Finally, our claim is validated by comparing the results obtained
by our algorithm with those of other popular scheduling policies through
extensive simulations.
"
1994,The Two-Pass Softmax Algorithm,"  The softmax (also called softargmax) function is widely used in machine
learning models to normalize real-valued scores into a probability
distribution. To avoid floating-point overflow, the softmax function is
conventionally implemented in three passes: the first pass to compute the
normalization constant, and two other passes to compute outputs from normalized
inputs. We analyze two variants of the Three-Pass algorithm and demonstrate
that in a well-optimized implementation on HPC-class processors performance of
all three passes is limited by memory bandwidth. We then present a novel
algorithm for softmax computation in just two passes. The proposed Two-Pass
algorithm avoids both numerical overflow and the extra normalization pass by
employing an exotic representation for intermediate values, where each value is
represented as a pair of floating-point numbers: one representing the
""mantissa"" and another representing the ""exponent"". Performance evaluation
demonstrates that on out-of-cache inputs on an Intel Skylake-X processor the
new Two-Pass algorithm outperforms the traditional Three-Pass algorithm by up
to 28% in AVX512 implementation, and by up to 18% in AVX2 implementation. The
proposed Two-Pass algorithm also outperforms the traditional Three-Pass
algorithm on Intel Broadwell and AMD Zen 2 processors. To foster
reproducibility, we released an open-source implementation of the new Two-Pass
Softmax algorithm and other experiments in this paper as a part of XNNPACK
library at GitHub.com/google/XNNPACK.
"
1995,"Fundamental Limits of Age-of-Information in Stationary and
  Non-stationary Environments","  We study the multi-user scheduling problem for minimizing the Age of
Information (AoI) in cellular wireless networks under stationary and
non-stationary regimes. We derive fundamental lower bounds for the scheduling
problem and design efficient online policies with provable performance
guarantees. In the stationary setting, we consider the AoI optimization problem
for a set of mobile users travelling around multiple cells. In this setting, we
propose a scheduling policy and show that it is $2$-optimal. Next, we propose a
new adversarial channel model for studying the scheduling problem in
non-stationary environments. For $N$ users, we show that the competitive ratio
of any online scheduling policy in this setting is at least $\Omega(N)$. We
then propose an online policy and show that it achieves a competitive ratio of
$O(N^2)$. Finally, we introduce a relaxed adversarial model with channel state
estimations for the immediate future. We propose a heuristic model predictive
control policy that exploits this feature and compare its performance through
numerical simulations.
"
1996,"A C Code Generator for Fast Inference and Simple Deployment of
  Convolutional Neural Networks on Resource Constrained Systems","  Inference of Convolutional Neural Networks in time critical applications
usually requires a GPU. In robotics or embedded devices these are often not
available due to energy, space and cost constraints. Furthermore, installation
of a deep learning framework or even a native compiler on the target platform
is not possible. This paper presents a neural network code generator (NNCG)
that generates from a trained CNN a plain ANSI C code file that encapsulates
the inference in single a function. It can easily be included in existing
projects and due to lack of dependencies, cross compilation is usually
possible. Additionally, the code generation is optimized based on the known
trained CNN and target platform following four design principles. The system is
evaluated utilizing small CNN designed for this application. Compared to
TensorFlow XLA and Glow speed-ups of up to 11.81 can be shown and even GPUs are
outperformed regarding latency.
"
1997,Duet Benchmarking: Improving Measurement Accuracy in the Cloud,"  We investigate the duet measurement procedure, which helps improve the
accuracy of performance comparison experiments conducted on shared machines by
executing the measured artifacts in parallel and evaluating their relative
performance together, rather than individually. Specifically, we analyze the
behavior of the procedure in multiple cloud environments and use experimental
evidence to answer multiple research questions concerning the assumption
underlying the procedure. We demonstrate improvements in accuracy ranging from
2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo)
workloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017
workloads.
"
1998,"On Expert Behaviors and Question Types for Efficient Query-Based
  Ontology Fault Localization","  We challenge existing query-based ontology fault localization methods wrt.
assumptions they make, criteria they optimize, and interaction means they use.
We find that their efficiency depends largely on the behavior of the
interacting expert, that performed calculations can be inefficient or
imprecise, and that used optimization criteria are often not fully realistic.
As a remedy, we suggest a novel (and simpler) interaction approach which
overcomes all identified problems and, in comprehensive experiments on faulty
real-world ontologies, enables a successful fault localization while requiring
fewer expert interactions in 66 % of the cases, and always at least 80 % less
expert waiting time, compared to existing methods.
"
1999,"Performance Evaluation of Multiparty Authentication in 5G IIoT
  Environments","  With the rapid development of various emerging technologies such as the
Industrial Internet of Things (IIoT), there is a need to secure communications
between such devices. Communication system delays are one of the factors that
adversely affect the performance of an authentication system. 5G networks
enable greater data throughput and lower latency, which presents new
opportunities for the secure authentication of business transactions between
IIoT devices. We evaluate an approach to developing a flexible and secure model
for authenticating IIoT components in dynamic 5G environments.
"
2000,Approximating Activation Functions,"  ReLU is widely seen as the default choice for activation functions in neural
networks. However, there are cases where more complicated functions are
required. In particular, recurrent neural networks (such as LSTMs) make
extensive use of both hyperbolic tangent and sigmoid functions. These functions
are expensive to compute. We used function approximation techniques to develop
replacements for these functions and evaluated them empirically on three
popular network configurations. We find safe approximations that yield a 10% to
37% improvement in training times on the CPU. These approximations were
suitable for all cases we considered and we believe are appropriate
replacements for all networks using these activation functions. We also develop
ranged approximations which only apply in some cases due to restrictions on
their input domain. Our ranged approximations yield a performance improvement
of 20% to 53% in network training time. Our functions also match or
considerably out perform the ad-hoc approximations used in Theano and the
implementation of Word2Vec.
"
2001,Dynamic Weighted Fairness with Minimal Disruptions,"  In this paper, we consider the following dynamic fair allocation problem:
Given a sequence of job arrivals and departures, the goal is to maintain an
approximately fair allocation of the resource against a target fair allocation
policy, while minimizing the total number of disruptions, which is the number
of times the allocation of any job is changed. We consider a rich class of fair
allocation policies that significantly generalize those considered in previous
work.
  We first consider the models where jobs only arrive, or jobs only depart. We
present tight upper and lower bounds for the number of disruptions required to
maintain a constant approximate fair allocation every time step. In particular,
for the canonical case where jobs have weights and the resource allocation is
proportional to the job's weight, we show that maintaining a constant
approximate fair allocation requires $\Theta(\log^* n)$ disruptions per job,
almost matching the bounds in prior work for the unit weight case. For the more
general setting where the allocation policy only decreases the allocation to a
job when new jobs arrive, we show that maintaining a constant approximate fair
allocation requires $\Theta(\log n)$ disruptions per job. We then consider the
model where jobs can both arrive and depart. We first show strong lower bounds
on the number of disruptions required to maintain constant approximate fairness
for arbitrary instances. In contrast we then show that there there is an
algorithm that can maintain constant approximate fairness with $O(1)$ expected
disruptions per job if the weights of the jobs are independent of the jobs
arrival and departure order. We finally show how our results can be extended to
the setting with multiple resources.
"
2002,"75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse
  GraphBLAS Matrices","  The SuiteSparse GraphBLAS C-library implements high performance hypersparse
matrices with bindings to a variety of languages (Python, Julia, and
Matlab/Octave). GraphBLAS provides a lightweight in-memory database
implementation of hypersparse matrices that are ideal for analyzing many types
of network data, while providing rigorous mathematical guarantees, such as
linearity. Streaming updates of hypersparse matrices put enormous pressure on
the memory hierarchy. This work benchmarks an implementation of hierarchical
hypersparse matrices that reduces memory pressure and dramatically increases
the update rate into a hypersparse matrices. The parameters of hierarchical
hypersparse matrices rely on controlling the number of entries in each level in
the hierarchy before an update is cascaded. The parameters are easily tunable
to achieve optimal performance for a variety of applications. Hierarchical
hypersparse matrices achieve over 1,000,000 updates per second in a single
instance. Scaling to 31,000 instances of hierarchical hypersparse matrices
arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update
rate of 75,000,000,000 updates per second. This capability allows the MIT
SuperCloud to analyze extremely large streaming network data sets.
"
2003,"A Simple Model for Portable and Fast Prediction of Execution Time and
  Power Consumption of GPU Kernels","  Characterizing compute kernel execution behavior on GPUs for efficient task
scheduling is a non-trivial task. We address this with a simple model enabling
portable and fast predictions among different GPUs using only
hardware-independent features. This model is built based on random forests
using 189 individual compute kernels from benchmarks such as Parboil, Rodinia,
Polybench-GPU and SHOC. Evaluation of the model performance using
cross-validation yields a median Mean Average Percentage Error (MAPE) of
8.86-52.00% and 1.84-2.94%, for time respectively power prediction across five
different GPUs, while latency for a single prediction varies between 15 and 108
milliseconds.
"
2004,"Enabling Highly-Scalable Remote Memory Access Programming with MPI-3 One
  Sided","  Modern interconnects offer remote direct memory access (RDMA) features. Yet,
most applications rely on explicit message passing for communications albeit
their unwanted overheads. The MPI-3.0 standard defines a programming interface
for exploiting RDMA networks directly, however, it's scalability and
practicability has to be demonstrated in practice. In this work, we develop
scalable bufferless protocols that implement the MPI-3.0 specification. Our
protocols support scaling to millions of cores with negligible memory
consumption while providing highest performance and minimal overheads. To arm
programmers, we provide a spectrum of performance models for all critical
functions and demonstrate the usability of our library and models with several
application studies with up to half a million processes. We show that our
design is comparable to, or better than UPC and Fortran Coarrays in terms of
latency, bandwidth, and message rate. We also demonstrate application
performance improvements with comparable programming complexity.
"
2005,Performance-Driven Internet Path Selection,"  Internet routing can often be sub-optimal, with the chosen routes providing
worse performance than other available policy-compliant routes. This stems from
the lack of visibility into route performance at the network layer. While this
is an old problem, we argue that recent advances in programmable hardware
finally open up the possibility of performance-aware routing in a deployable,
BGP-compatible manner. We introduce ROUTESCOUT, a hybrid hardware/software
system supporting performance-based routing at ISP scale. In the data plane,
ROUTESCOUT leverages P4-enabled hardware to monitor performance across
policy-compliant route choices for each destination, at line-rate and with a
small memory footprint. ROUTESCOUT's control plane then asynchronously pulls
aggregated performance metrics to synthesize a performance-aware forwarding
policy. We show that ROUTESCOUT can monitor performance across most of an ISP's
traffic, using only 4 MB of memory. Further, its control can flexibly satisfy a
variety of operator objectives, with sub-second operating times.
"
2006,Automatically Harnessing Sparse Acceleration,"  Sparse linear algebra is central to many scientific programs, yet compilers
fail to optimize it well. High-performance libraries are available, but
adoption costs are significant. Moreover, libraries tie programs into
vendor-specific software and hardware ecosystems, creating non-portable code.
  In this paper, we develop a new approach based on our specification Language
for implementers of Linear Algebra Computations (LiLAC). Rather than requiring
the application developer to (re)write every program for a given library, the
burden is shifted to a one-off description by the library implementer. The
LiLAC-enabled compiler uses this to insert appropriate library routines without
source code changes.
  LiLAC provides automatic data marshaling, maintaining state between calls and
minimizing data transfers. Appropriate places for library insertion are
detected in compiler intermediate representation, independent of source
languages.
  We evaluated on large-scale scientific applications written in FORTRAN;
standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across
heterogeneous platforms, applications and data sets we show speedups of
1.1$\times$ to over 10$\times$ without user intervention.
"
2007,A Closer Look at Lightweight Graph Reordering,"  Graph analytics power a range of applications in areas as diverse as finance,
networking and business logistics. A common property of graphs used in the
domain of graph analytics is a power-law distribution of vertex connectivity,
wherein a small number of vertices are responsible for a high fraction of all
connections in the graph. These richly-connected (hot) vertices inherently
exhibit high reuse. However, their sparse distribution in memory leads to a
severe underutilization of on-chip cache capacity. Prior works have proposed
lightweight skew-aware vertex reordering that places hot vertices adjacent to
each other in memory, reducing the cache footprint of hot vertices. However, in
doing so, they may inadvertently destroy the inherent community structure
within the graph, which may negate the performance gains achieved from the
reduced footprint of hot vertices.
  In this work, we study existing reordering techniques and demonstrate the
inherent tension between reducing the cache footprint of hot vertices and
preserving original graph structure. We quantify the potential performance loss
due to disruption in graph structure for different graph datasets. We further
show that reordering techniques that employ fine-grain reordering significantly
increase misses in the higher level caches, even when they reduce misses in the
last-level cache.
  To overcome the limitations of existing reordering techniques, we propose
Degree-Based Grouping (DBG), a novel lightweight reordering technique that
employs a coarse-grain reordering to largely preserve graph structure while
reducing the cache footprint of hot vertices. Our evaluation on 40 combinations
of various graph applications and datasets shows that, compared to a baseline
with no reordering, DBG yields an average application speed-up of 16.8% vs
11.6% for the best-performing existing lightweight technique.
"
2008,"Are Distributed Ledger Technologies Ready for Smart Transportation
  Systems?","  The aim of this paper is to understand whether Distributed Ledger
Technologies (DLTs) are ready to support complex services, such as those
related to Intelligent Transportation Systems (ITS). In smart transportation
services, a huge amount of sensed data is generated by a multitude of vehicles.
While DLTs provide very interesting features, such as immutability,
traceability and verifiability of data, some doubts on the scalability and
responsiveness of these technologies appear to be well-founded. We propose an
architecture for ITS that resorts to DLT features. Moreover, we provide
experimental results of a real test-bed over IOTA, a promising DLT for IoT.
Results clearly show that, while the viability of the proposal cannot be
rejected, further work is needed on the responsiveness of DLT infrastructures.
"
2009,TiFL: A Tier-based Federated Learning System,"  Federated Learning (FL) enables learning a shared model across many clients
without violating the privacy requirements. One of the key attributes in FL is
the heterogeneity that exists in both resource and data due to the differences
in computation and communication capacity, as well as the quantity and content
of data among different clients. We conduct a case study to show that
heterogeneity in resource and data has a significant impact on training time
and model accuracy in conventional FL systems. To this end, we propose TiFL, a
Tier-based Federated Learning System, which divides clients into tiers based on
their training performance and selects clients from the same tier in each
training round to mitigate the straggler problem caused by heterogeneity in
resource and data quantity. To further tame the heterogeneity caused by non-IID
(Independent and Identical Distribution) data and resources, TiFL employs an
adaptive tier selection approach to update the tiering on-the-fly based on the
observed training performance and accuracy overtime. We prototype TiFL in a FL
testbed following Google's FL architecture and evaluate it using popular
benchmarks and the state-of-the-art FL benchmark LEAF. Experimental evaluation
shows that TiFL outperforms the conventional FL in various heterogeneous
conditions. With the proposed adaptive tier selection policy, we demonstrate
that TiFL achieves much faster training performance while keeping the same (and
in some cases - better) test accuracy across the board.
"
2010,A Visual Analytics Framework for Reviewing Streaming Performance Data,"  Understanding and tuning the performance of extreme-scale parallel computing
systems demands a streaming approach due to the computational cost of applying
offline algorithms to vast amounts of performance log data. Analyzing large
streaming data is challenging because the rate of receiving data and limited
time to comprehend data make it difficult for the analysts to sufficiently
examine the data without missing important changes or patterns. To support
streaming data analysis, we introduce a visual analytic framework comprising of
three modules: data management, analysis, and interactive visualization. The
data management module collects various computing and communication performance
metrics from the monitored system using streaming data processing techniques
and feeds the data to the other two modules. The analysis module automatically
identifies important changes and patterns at the required latency. In
particular, we introduce a set of online and progressive analysis methods for
not only controlling the computational costs but also helping analysts better
follow the critical aspects of the analysis results. Finally, the interactive
visualization module provides the analysts with a coherent view of the changes
and patterns in the continuously captured performance data. Through a
multi-faceted case study on performance analysis of parallel discrete-event
simulation, we demonstrate the effectiveness of our framework for identifying
bottlenecks and locating outliers.
"
2011,"Distributed systems and trusted execution environments: Trade-offs and
  challenges","  Security and privacy concerns in computer systems have grown in importance
with the ubiquity of connected devices. TEEs provide security guarantees based
on cryptographic constructs built in hardware. Intel software guard extensions
(SGX), in particular, implements powerful mechanisms that can shield sensitive
data even from privileged users with full control of system software. In this
work, we essentially explore some of the challenges of designing secure
distributed systems by using Intel SGX as cornerstone. We do so by designing
and experimentally evaluating several elementary systems ranging from
communication and processing middleware to a peer-to-peer privacy-preserving
solution. We start with support systems that naturally fit cloud deployment
scenarios, namely content-based routing, batching and stream processing
frameworks. We implement prototypes and use them to analyse the manifested
memory usage issues intrinsic to SGX. Next, we aim at protecting very sensitive
data: cryptographic keys. By leveraging TEEs, we design protocols for group
data sharing that have lower computational complexity than legacy methods. As a
bonus, our proposals allow large savings on metadata volume and processing time
of cryptographic operations, all with equivalent security guarantees. Finally,
we propose privacy-preserving systems against established services like
web-search engines. Our evaluation shows that we propose the most robust system
in comparison to existing solutions with regard to user re-identification rates
and results accuracy in a scalable way. Overall, this thesis proposes new
mechanisms that take advantage of TEEs for distributed system architectures. We
show through an empirical approach on top of Intel SGX what are the trade-offs
of distinct designs applied to distributed communication and processing,
cryptographic protocols and private web search.
"
2012,Domain-Specialized Cache Management for Graph Analytics,"  Graph analytics power a range of applications in areas as diverse as finance,
networking and business logistics. A common property of graphs used in the
domain of graph analytics is a power-law distribution of vertex connectivity,
wherein a small number of vertices are responsible for a high fraction of all
connections in the graph. These richly-connected, hot, vertices inherently
exhibit high reuse. However, this work finds that state-of-the-art hardware
cache management schemes struggle in capitalizing on their reuse due to highly
irregular access patterns of graph analytics.
  In response, we propose GRASP, domain-specialized cache management at the
last-level cache for graph analytics. GRASP augments existing cache policies to
maximize reuse of hot vertices by protecting them against cache thrashing,
while maintaining sufficient flexibility to capture the reuse of other vertices
as needed. GRASP keeps hardware cost negligible by leveraging lightweight
software support to pinpoint hot vertices, thus eliding the need for
storage-intensive prediction mechanisms employed by state-of-the-art cache
management schemes. On a set of diverse graph-analytic applications with large
high-skew graph datasets, GRASP outperforms prior domain-agnostic schemes on
all datapoints, yielding an average speed-up of 4.2% (max 9.4%) over the
best-performing prior scheme. GRASP remains robust on low-/no-skew datasets,
whereas prior schemes consistently cause a slowdown.
"
2013,"Detecting State Transitions of a Markov Source: Sampling Frequency and
  Age Trade-off","  We consider a finite-state Discrete-Time Markov Chain (DTMC) source that can
be sampled for detecting the events when the DTMC transits to a new state. Our
goal is to study the trade-off between sampling frequency and staleness in
detecting the events. We argue that, for the problem at hand, using Age of
Information (AoI) for quantifying the staleness of a sample is conservative and
therefore, introduce \textit{age penalty} for this purpose. We study two
optimization problems: minimize average age penalty subject to an average
sampling frequency constraint, and minimize average sampling frequency subject
to an average age penalty constraint; both are Constrained Markov Decision
Problems. We solve them using linear programming approach and compute Markov
policies that are optimal among all causal policies. Our numerical results
demonstrate that the computed Markov policies not only outperform optimal
periodic sampling policies, but also achieve sampling frequencies close to or
lower than that of an optimal clairvoyant (non-causal) sampling policy, if a
small age penalty is allowed.
"
2014,Parallel Binary Code Analysis,"  Binary code analysis is widely used to assess a program's correctness,
performance, and provenance. Binary analysis applications often construct
control flow graphs, analyze data flow, and use debugging information to
understand how machine code relates to source lines, inlined functions, and
data types. To date, binary analysis has been single-threaded, which is too
slow for applications such as performance analysis and software forensics,
where it is becoming common to analyze binaries that are gigabytes in size and
in large batches that contain thousands of binaries.
  This paper describes our design and implementation for accelerating the task
of constructing control flow graphs (CFGs) from binaries with multithreading.
Existing research focuses on addressing challenging code constructs encountered
during constructing CFGs, including functions sharing code, jump table
analysis, non-returning functions, and tail calls. However, existing analyses
do not consider the complex interactions between concurrent analysis of shared
code, making it difficult to extend existing serial algorithms to be parallel.
A systematic methodology to guide the design of parallel algorithms is
essential. We abstract the task of constructing CFGs as repeated applications
of several core CFG operations regarding to creating functions, basic blocks,
and edges. We then derive properties among CFG operations, including operation
dependency, commutativity, monotonicity. These operation properties guide our
design of a new parallel analysis for constructing CFGs. We achieved as much as
25$\times$ speedup for constructing CFGs on 64 hardware threads. Binary
analysis applications are significantly accelerated with the new parallel
analysis: we achieve 8$\times$ for a performance analysis tool and 7$\times$
for a software forensic tool with 16 hardware threads.
"
2015,Pre-defined Sparsity for Low-Complexity Convolutional Neural Networks,"  The high energy cost of processing deep convolutional neural networks impedes
their ubiquitous deployment in energy-constrained platforms such as embedded
systems and IoT devices. This work introduces convolutional layers with
pre-defined sparse 2D kernels that have support sets that repeat periodically
within and across filters. Due to the efficient storage of our periodic sparse
kernels, the parameter savings can translate into considerable improvements in
energy efficiency due to reduced DRAM accesses, thus promising significant
improvements in the trade-off between energy consumption and accuracy for both
training and inference. To evaluate this approach, we performed experiments
with two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse
variants of the ResNet18 and VGG16 architectures. Compared to baseline models,
our proposed sparse variants require up to 82% fewer model parameters with
5.6times fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10.
For VGG16 trained on Tiny ImageNet, our approach requires 5.8times fewer FLOPs
and up to 83.3% fewer model parameters with a drop in top-5 (top-1) accuracy of
only 1.2% (2.1%). We also compared the performance of our proposed
architectures with that of ShuffleNet andMobileNetV2. Using similar
hyperparameters and FLOPs, our ResNet18 variants yield an average accuracy
improvement of 2.8%.
"
2016,"Reconfigurable Intelligent Surface for MISO Systems with Proportional
  Rate Constraints","  This paper investigates the system spectral efficiency (SE) in reconfigurable
intelligent surface (RIS)-aided multiuser multiple-input single-output (MISO)
systems, where RIS can reconfigure the propagation environment via a large
number of controllable and intelligent phase shifters. In order to explore the
system SE performance behavior with user proportional fairness for such a
system, an optimization problem is formulated to maximize the SE by jointly
considering the power allocation at the base station (BS) and phase shift at
the RIS, under nonlinear proportional rate fairness constraints. To solve the
nonconvex optimization problem, an effective solution is developed, which
capitalizes on an iterative algorithm with closed-form expressions, i.e.,
alternatively optimizing the transmit power at the BS and the reflecting phase
shift at the RIS. Numerical simulations are provided to validate the
theoretical analysis and assess the performance of the proposed alternative
algorithm.
"
2017,A Scalable Framework for Quality Assessment of RDF Datasets,"  Over the last years, Linked Data has grown continuously. Today, we count more
than 10,000 datasets being available online following Linked Data standards.
These standards allow data to be machine readable and inter-operable.
Nevertheless, many applications, such as data integration, search, and
interlinking, cannot take full advantage of Linked Data if it is of low
quality. There exist a few approaches for the quality assessment of Linked
Data, but their performance degrades with the increase in data size and quickly
grows beyond the capabilities of a single machine. In this paper, we present
DistQualityAssessment -- an open source implementation of quality assessment of
large RDF datasets that can scale out to a cluster of machines. This is the
first distributed, in-memory approach for computing different quality metrics
for large RDF datasets using Apache Spark. We also provide a quality assessment
pattern that can be used to generate new scalable metrics that can be applied
to big data. The work presented here is integrated with the SANSA framework and
has been applied to at least three use cases beyond the SANSA community. The
results show that our approach is more generic, efficient, and scalable as
compared to previously proposed approaches.
"
2018,"Asymptotic regime analysis of NOMA uplink networks under QoS delay
  Constraints","  In the fifth generation and beyond (B5G) technologies, delay constrains
emerge as a topic of particular interest for ultra reliable low latency
communications (e.g., enhanced reality, haptic communications). In this report,
we study the performance of a two user uplink non orthogonal multiple access
(NOMA) network under quality of service (QoS) delay constraints, captured
through each user delay exponents in their effective capacity (EC). We propose
novel closed form expressions for the EC of the NOMA users and validate them
through Monte Carlo simulations. Interestingly, our study shows that in the
high signal to noise ratio (SNR) region, the strong NOMA user has a limited EC
no matter how large the transmit SNR is, under the same delay constraint as the
weak user. We show that for the weak user OMA achieves higher EC than NOMA at
small values of the transmit SNR and that NOMA become more beneficial at high
values of the transmit SNR. For the strong user, we show that NOMA achieves a
higher EC than OMA at small values of the transmit SNR and that at high values
of the transmit SNR OMA becomes more beneficial. By introducing user pairing
when more than two NOMA users are present, we show that NOMA with user pairing
outperforms OMA in term of the total link layer EC. Finally, we find the set of
pairs which gives the highest total link-layer in the uplink for NOMA with
multiple user-pairs.
"
2019,"Reflection Resource Management for Intelligent Reflecting Surface Aided
  Wireless Networks","  In this paper, the adoption of an intelligent reflecting surface (IRS) for
multiple single-antenna source terminal (ST)-DT pairs in two-hop networks is
investigated. Different from the previous studies on IRS that merely focused on
tuning the reflection coefficient of all the reflection elements at IRS, in
this paper, we consider the true reflection resource management. Specifically,
the true reflection resource management can be realized via trigger module
selection based on our proposed IRS architecture that all the reflection
elements are partially controlled by multiple parallel switches of controller.
As the number of reflection elements increases, the true reflection resource
management will become urgently needed in this context, which is due to the
non-ignorable energy consumption. Moreover, the proposed modular architecture
of IRS is designed to make the reflection elements part independent and
controllable. As such, our goal is to maximize the minimum
signal-to-interference-plus-noise ratio (SINR) at DTs via a joint trigger
module subset selection, transmit power allocation of STs, and the
corresponding passive beamforming of the trigger modules, subject to per ST
power budgets and module size constraint. Whereas this problem is NP-hard due
to the module size constraint, to deal with it, we transform the hard module
size constraint into the group sparse constraint by introducing the mixed row
block norm, which yields a suitable semidefinite relaxation. Additionally, the
parallel alternating direction method of multipliers (PADMM) is proposed to
identify the trigger module subset, and then subsequently the transmit power
allocation and passive beamforming can be obtained by solving the original
minimum SINR maximization problem without the group sparse constraint via
partial linearization for generalized fractional programs.
"
2020,"MKPipe: A Compiler Framework for Optimizing Multi-Kernel Workloads in
  OpenCL for FPGA","  OpenCL for FPGA enables developers to design FPGAs using a programming model
similar for processors. Recent works have shown that code optimization at the
OpenCL level is important to achieve high computational efficiency. However,
existing works either focus primarily on optimizing single kernels or solely
depend on channels to design multi-kernel pipelines. In this paper, we propose
a source-to-source compiler framework, MKPipe, for optimizing multi-kernel
workloads in OpenCL for FPGA. Besides channels, we propose new schemes to
enable multi-kernel pipelines. Our optimizing compiler employs a systematic
approach to explore the tradeoffs of these optimizations methods. To enable
more efficient overlapping between kernel execution, we also propose a novel
workitem/workgroup-id remapping technique. Furthermore, we propose new
algorithms for throughput balancing and resource balancing to tune the
optimizations upon individual kernels in the multi-kernel workloads. Our
results show that our compiler-optimized multi-kernels achieve up to 3.6x (1.4x
on average) speedup over the baseline, in which the kernels have already been
optimized individually.
"
2021,"Age of Information in a Decentralized Network of Parallel Queues with
  Routing and Packets Losses","  The paper deals with Age of Information in a network of multiple sources and
parallel servers/queues with buffering capabilities, preemption in service and
losses in served packets. The servers do not communicate between each other and
the packets are dispatched through the servers according to a predefined
probabilistic routing. By making use of the Stochastic Hybrid System (SHS)
method, we provide a derivation of the average Age of Information of a system
of two parallel servers (with and without buffer capabilities) and compare the
result with that of a single queue. We show known results of packets delay in
Queuing Theory do not hold for Age of Information. Unfortunately, the
complexity of computing the Age of Information using the SHS method increases
highly with the number of queues. We therefore provide an upper bound of the
average Age of Information in a parallel server system of an arbitrary number
of M/M/1/(N + 1) queues and its tightness in various regimes. This upper bound
allows providing a tight approximation of the Age of Information with a very
low complexity.
"
2022,CBR: Controlled Burst Recording,"  Collecting traces from software running in the field is both useful and
challenging. Traces may indeed help revealing unexpected usage scenarios,
detecting and reproducing failures, and building behavioral models that reflect
how the software is actually used. On the other hand, recording traces is an
intrusive activity that may annoy users, negatively affecting the usability of
the applications, if not properly designed. In this paper we address field
monitoring by introducing Controlled Burst Recording, a monitoring solution
that can collect comprehensive runtime data without compromising the quality of
the user experience. The technique encodes the knowledge extracted from the
monitored application as a finite state model that both represents the
sequences of operations that can be executed by the users and the corresponding
internal computations that might be activated by each operation. Our initial
assessment with information extracted from ArgoUML shows that Controlled Burst
Recording can reconstruct behavioral information more effectively than
competing sampling techniques, with a low impact on the system response time.
"
2023,Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images,"  In this paper we implemented the algorithm we developed in [1] called 3DPIFCM
in a parallel environment by using CUDA on a GPU. In our previous work we
introduced 3DPIFCM which performs segmentation of images in noisy conditions
and uses particle swarm optimization for finding the optimal algorithm
parameters to account for noise. This algorithm achieved state of the art
segmentation accuracy when compared to FCM (Fuzzy C-Means), IFCMPSO (Improved
Fuzzy C-Means with Particle Swarm Optimization), GAIFCM (Genetic Algorithm
Improved Fuzzy C-Means) on noisy MRI images of an adult Brain.
  When using a genetic algorithm or PSO (Particle Swarm Optimization) on a
single machine for optimization we witnessed long execution times for practical
clinical usage. Therefore, in the current paper our goal was to speed up the
execution of 3DPIFCM by taking out parts of the algorithm and executing them as
kernels on a GPU. The algorithm was implemented using the CUDA [13] framework
from NVIDIA and experiments where performed on a server containing 64GB RAM , 8
cores and a TITAN X GPU with 3072 SP cores and 12GB of GPU memory.
  Our results show that the parallel version of the algorithm performs up to
27x faster than the original sequential version and 68x faster than GAIFCM
algorithm. We show that the speedup of the parallel version increases as we
increase the size of the image due to better utilization of cores in the GPU.
Also, we show a speedup of up to 5x in our Brainweb experiment compared to
other generic variants such as IFCMPSO and GAIFCM.
"
2024,A Language for Describing Optimization Strategies,"  Optimizing programs to run efficiently on modern parallel hardware is hard
but crucial for many applications. The predominantly used imperative languages
- like C or OpenCL - force the programmer to intertwine the code describing
functionality and optimizations. This results in a nightmare for portability
which is particularly problematic given the accelerating trend towards
specialized hardware devices to further increase efficiency.
  Many emerging DSLs used in performance demanding domains such as deep
learning, automatic differentiation, or image processing attempt to simplify or
even fully automate the optimization process. Using a high-level - often
functional - language, programmers focus on describing functionality in a
declarative way. In some systems such as Halide or TVM, a separate schedule
specifies how the program should be optimized. Unfortunately, these schedules
are not written in well-defined programming languages. Instead, they are
implemented as a set of ad-hoc predefined APIs that the compiler writers have
exposed.
  In this paper, we present Elevate: a functional language for describing
optimization strategies. Elevate follows a tradition of prior systems used in
different contexts that express optimization strategies as composition of
rewrites. In contrast to systems with scheduling APIs, in Elevate programmers
are not restricted to a set of built-in optimizations but define their own
optimization strategies freely in a composable way. We show how user-defined
optimization strategies in Elevate enable the effective optimization of
programs expressed in a functional data-parallel language demonstrating
competitive performance with Halide and TVM.
"
2025,"Desynchronization and Wave Pattern Formation in MPI-Parallel and Hybrid
  Memory-Bound Programs","  Analytic, first-principles performance modeling of distributed-memory
parallel codes is notoriously imprecise. Even for applications with extremely
regular and homogeneous compute-communicate phases, simply adding communication
time to computation time does often not yield a satisfactory prediction of
parallel runtime due to deviations from the expected simple lockstep pattern
caused by system noise, variations in communication time, and inherent load
imbalance. In this paper, we highlight the specific cases of provoked and
spontaneous desynchronization of memory-bound, bulk-synchronous pure MPI and
hybrid MPI+OpenMP programs. Using simple microbenchmarks we observe that
although desynchronization can introduce increased waiting time per process, it
does not necessarily cause lower resource utilization but can lead to an
increase in available bandwidth per core. In case of significant communication
overhead, even natural noise can shove the system into a state of automatic
overlap of communication and computation, improving the overall time to
solution. The saturation point, i.e., the number of processes per memory domain
required to achieve full memory bandwidth, is pivotal in the dynamics of this
process and the emerging stable wave pattern. We also demonstrate how hybrid
MPI-OpenMP programming can prevent desirable desynchronization by eliminating
the bandwidth bottleneck among processes. A Chebyshev filter diagonalization
application is used to demonstrate some of the observed effects in a realistic
setting.
"
2026,"Performance Modeling and Analysis of a Hyperledger-based System Using
  GSPN","  As a highly scalable permissioned blockchain platform, Hyperledger Fabric
supports a wide range of industry use cases ranging from governance to finance.
In this paper, we propose a model to analyze the performance of a
Hyperledgerbased system by using Generalised Stochastic Petri Nets (GSPN). This
model decomposes a transaction flow into multiple phases and provides a
simulation-based approach to obtain the system latency and throughput with a
specific arrival rate. Based on this model, we analyze the impact of different
configurations of ordering service on system performance to find out the
bottleneck. Moreover, a mathematical configuration selection approach is
proposed to determine the best configuration which can maximize the system
throughput. Finally, extensive experiments are performed on a running system to
validate the proposed model and approaches.
"
2027,"Understanding HPC Benchmark Performance on Intel Broadwell and Cascade
  Lake Processors","  Hardware platforms in high performance computing are constantly getting more
complex to handle even when considering multicore CPUs alone. Numerous features
and configuration options in the hardware and the software environment that are
relevant for performance are not even known to most application users or
developers. Microbenchmarks, i.e., simple codes that fathom a particular aspect
of the hardware, can help to shed light on such issues, but only if they are
well understood and if the results can be reconciled with known facts or
performance models. The insight gained from microbenchmarks may then be applied
to real applications for performance analysis or optimization. In this paper we
investigate two modern Intel x86 server CPU architectures in depth: Broadwell
EP and Cascade Lake SP. We highlight relevant hardware configuration settings
that can have a decisive impact on code performance and show how to properly
measure on-chip and off-chip data transfer bandwidths. The new victim L3 cache
of Cascade Lake and its advanced replacement policy receive due attention.
Finally we use DGEMM, sparse matrix-vector multiplication, and the HPCG
benchmark to make a connection to relevant application scenarios.
"
2028,"AI-oriented Medical Workload Allocation for Hierarchical
  Cloud/Edge/Device Computing","  In a hierarchically-structured cloud/edge/device computing environment,
workload allocation can greatly affect the overall system performance. This
paper deals with AI-oriented medical workload generated in emergency rooms (ER)
or intensive care units (ICU) in metropolitan areas. The goal is to optimize
AI-workload allocation to cloud clusters, edge servers, and end devices so that
minimum response time can be achieved in life-saving emergency applications.
  In particular, we developed a new workload allocation method for the AI
workload in distributed cloud/edge/device computing systems. An efficient
scheduling and allocation strategy is developed in order to reduce the overall
response time to satisfy multi-patient demands. We apply several ICU AI
workloads from a comprehensive edge computing benchmark Edge AIBench. The
healthcare AI applications involved are short-of-breath alerts, patient
phenotype classification, and life-death threats. Our experimental results
demonstrate the high efficiency and effectiveness in real-life health-care and
emergency applications.
"
2029,"A Joint Precoding Framework for Wideband Reconfigurable Intelligent
  Surface-Aided Cell-Free Network","  Thanks to the strong ability against the inter-cell interference, cell-free
network has been considered as a promising technique to improve the network
capacity of future wireless systems. However, for further capacity enhancement,
it requires to deploy more base stations (BSs) with high cost and power
consumption. To address the issue, inspired by the recently proposed technique
called reconfigurable intelligent surface (RIS), we propose the concept of
RIS-aided cell-free network to improve the network capacity with low cost and
power consumption. The key idea is to replace some of the required BSs by
low-cost and energy-efficient RISs, and deploy more RISs in the cell-free
network for capacity enhancement. Then, for the proposed RIS-aided cell-free
network in the typical wideband scenario, we formulate the joint precoding
design problem at the BSs and RISs to maximize the network capacity. Due to the
non-convexity and high complexity of the formulated problem, we develop an
alternating optimization algorithm to solve this challenging problem. In
particular, we decouple this problem via Lagrangian dual transform and
fractional programming, and solve the subproblems alternatively. Note that most
of the considered scenarios in existing works are special cases of the general
scenario in this paper, and the proposed joint precoding framework can also
serve as a general solution to maximize the capacity in most of existing
RIS-aided scenarios. Finally, simulation results verify that, compared with the
conventional cell-free network, the network capacity of the proposed scheme can
be improved significantly.
"
2030,"On CSI-free Multi-Antenna Schemes for Massive RF Wireless Energy
  Transfer","  Wireless Energy Transfer (WET) is emerging as a potential green enabler for
massive Internet of Things (IoT). Herein, we analyze Channel State Information
(CSI)-free multi-antenna strategies for powering wirelessly a large set of
single-antenna IoT devices. The CSI-free schemes are AA-SS (AA-IS), where all
antennas transmit the same (independent) signal(s), and SA, where just one
antenna transmits at a time such that all antennas are utilized during the
coherence block. We characterize the distribution of the provided energy under
correlated Rician fading for each scheme and find out that while AA-IS and SA
cannot take advantage of the multiple antennas to improve the average provided
energy, its dispersion can be significantly reduced. Meanwhile, AA-SS provides
the greatest average energy, but also the greatest energy dispersion, and the
gains depend critically on the mean phase shifts between the antenna elements.
We find that consecutive antennas must be $\pi$ phase-shifted for optimum
average energy performance under AA-SS. Our numerical results evidenced that
correlation is beneficial under AA-SS, while a greater line of sight (LOS)
and/or number of antennas is not always beneficial under such scheme.
Meanwhile, both AA-IS and SA schemes benefit from small correlation, large LOS
and/or large number of antennas.
"
2031,The Deep Learning Compiler: A Comprehensive Survey,"  The difficulty of deploying various deep learning (DL) models on diverse DL
hardware has boosted the research and development of DL compilers in the
community. Several DL compilers have been proposed from both industry and
academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the
DL models described in different DL frameworks as input, and then generate
optimized codes for diverse DL hardware as output. However, none of the
existing survey has analyzed the unique design architecture of the DL compilers
comprehensively. In this paper, we perform a comprehensive survey of existing
DL compilers by dissecting the commonly adopted design in details, with
emphasis on the DL oriented multi-level IRs, and frontend/backend
optimizations. Specifically, we provide a comprehensive comparison among
existing DL compilers from various aspects. In addition, we present detailed
analysis on the design of multi-level IRs and illustrate the commonly adopted
optimization techniques. Finally, several insights are highlighted as the
potential research directions of DL compiler. This is the first survey paper
focusing on the design architecture of DL compilers, which we hope can pave the
road for future research towards DL compiler.
"
2032,"On Time Synchronization Issues in Time-Sensitive Networks with
  Regulators and Nonideal Clocks","  Flow reshaping is used in time-sensitive networks (as in the context of IEEE
TSN and IETF Detnet) in order to reduce burstiness inside the network and to
support the computation of guaranteed latency bounds. This is performed using
per-flow regulators (such as the Token Bucket Filter) or interleaved regulators
(as with IEEE TSN Asynchronous Traffic Shaping). Both types of regulators are
beneficial as they cancel the increase of burstiness due to multiplexing inside
the network. It was demonstrated, by using network calculus, that they do not
increase the worst-case latency. However, the properties of regulators were
established assuming that time is perfect in all network nodes. In reality,
nodes use local, imperfect clocks. Time-sensitive networks exist in two
flavours: (1) in non-synchronized networks, local clocks run independently at
every node and their deviations are not controlled and (2) in synchronized
networks, the deviations of local clocks are kept within very small bounds
using for example a synchronization protocol (such as PTP) or a satellite based
geo-positioning system (such as GPS). We revisit the properties of regulators
in both cases. In non-synchronized networks, we show that ignoring the timing
inaccuracies can lead to network instability due to unbounded delay in per-flow
or interleaved regulators. We propose and analyze two methods (rate and burst
cascade, and asynchronous dual arrival-curve method) for avoiding this problem.
In synchronized networks, we show that there is no instability with per-flow
regulators but, surprisingly, interleaved regulators can lead to instability.
To establish these results, we develop a new framework that captures industrial
requirements on clocks in both non-synchronized and synchronized networks, and
we develop a toolbox that extends network calculus to account for clock
imperfections.
"
2033,"Reproducibility Report for the Paper: Modeling of Request Cloning in
  Cloud Server Systems using Processor Sharing","  The authors have uploaded their artifact on Zenodo, which ensures a long-term
retention of the artifact. The code is suitably documented, and some examples
are given. A minimalistic overall description of the engine is provided. The
artifact allows to setup the environment quite quickly, and the dependencies
are well documented. The process to regenerate data for the figures in the
paper completes, and all results are reproducible.
  This paper can thus receive the Artifacts Available badge and the Artifacts
Evaluated-Functional. Given the high quality of the artifact, also the
Artifacts Evaluated-Reusable badge can be assigned.
"
2034,"AnySeq: A High Performance Sequence Alignment Library based on Partial
  Evaluation","  Sequence alignments are fundamental to bioinformatics which has resulted in a
variety of optimized implementations. Unfortunately, the vast majority of them
are hand-tuned and specific to certain architectures and execution models. This
not only makes them challenging to understand and extend, but also difficult to
port to other platforms. We present AnySeq - a novel library for computing
different types of pairwise alignments of DNA sequences. Our approach combines
high performance with an intuitively understandable implementation, which is
achieved through the concept of partial evaluation. Using the AnyDSL compiler
framework, AnySeq enables the compilation of algorithmic variants that are
highly optimized for specific usage scenarios and hardware targets with a
single, uniform codebase. The resulting domain-specific library thus allows the
variation of alignment parameters (such as alignment type, scoring scheme, and
traceback vs.~plain score) by simple function composition rather than
metaprogramming techniques which are often hard to understand. Our
implementation supports multithreading and SIMD vectorization on CPUs,
CUDA-enabled GPUs, and FPGAs. AnySeq is at most 7% slower and in many cases
faster (up to 12%) than state-of-the art manually optimized alignment libraries
on CPUs (SeqAn) and on GPUs (NVBio).
"
2035,"On the Performance Analysis of Epidemic Routing in Non-Sparse Delay
  Tolerant Networks","  We study the behavior of epidemic routing in a delay tolerant network as a
function of node density. Focusing on the probability of successful delivery to
a destination within a deadline (PS), we show that PS experiences a phase
transition as node density increases. Specifically, we prove that PS exhibits a
phase transition when nodes are placed according to a Poisson process and
allowed to move according to independent and identical processes with limited
speed. We then propose four fluid models to evaluate the performance of
epidemic routing in non-sparse networks. A model is proposed for supercritical
networks based on approximation of the infection rate as a function of time.
Other models are based on the approximation of the pairwise infection rate. Two
of them, one for subcritical networks and another for supercritical networks,
use the pairwise infection rate as a function of the number of infected nodes.
The other model uses pairwise infection rate as a function of time, and can be
applied for both subcritical and supercritical networks achieving good
accuracy. The model for subcritical networks is accurate when density is not
close to the percolation critical density. Moreover, the models that target
only supercritical regime are accurate.
"
2036,"CROFT: A scalable three-dimensional parallel Fast Fourier Transform
  (FFT) implementation for High Performance Clusters","  The FFT of three-dimensional (3D) input data is an important computational
kernel of numerical simulations and is widely used in High Performance
Computing (HPC) codes running on a large number of processors. Performance of
many scientific applications such as Molecular Dynamic simulations depends on
the underlying 3D parallel FFT library being used.
  In this paper, we present C-DACs three-dimensional Fast Fourier Transform
(CROFT) library which implements three-dimensional parallel FFT using pencil
decomposition. To exploit the hyperthreading capabilities of processor cores
without affecting performance, CROFT is designed to use multithreading along
with MPI. CROFT implementation has an innovative feature of overlapping compute
and memory-I/O with MPI communication using multithreading. As opposed to other
3D FFT implementations, CROFT uses only two threads where one thread is
dedicated for communication so that it can be effectively overlapped with
computations. Thus, depending on the number of processes used, CROFT achieves
performance improvement of about 51% to 42% as compared to FFTW3 library.
"
2037,"Eigenvector Component Calculation Speedup over NumPy for
  High-Performance Computing","  Applications related to artificial intelligence, machine learning, and system
identification simulations essentially use eigenvectors. Calculating
eigenvectors for very large matrices using conventional methods is
compute-intensive and renders the applications slow. Recently,
Eigenvector-Eigenvalue Identity formula promising significant speedup was
identified. We study the algorithmic implementation of the formula against the
existing state-of-the-art algorithms and their implementations to evaluate the
performance gains. We provide a first of its kind systematic study of the
implementation of the formula. We demonstrate further improvements using
high-performance computing concepts over native NumPy eigenvector
implementation which uses LAPACK and BLAS.
"
2038,"QPEP: A QUIC-Based Approach to Encrypted Performance Enhancing Proxies
  for High-Latency Satellite Broadband","  Satellite broadband services are critical infrastructures enabling advanced
technologies to function in the most remote regions of the globe. However,
status-quo services are often unencrypted by default and vulnerable to
eavesdropping attacks. In this paper, we challenge the historical perception
that over-the-air security must trade off with TCP performance in high-latency
satellite networks due to the deep-packet inspection requirements of
Performance Enhancing Proxies (PEPs).
  After considering why prior work in this area has failed to find wide
adoption, we present an open-source encrypted-by-default PEP - QPEP - which
seeks to address these issues. QPEP is built around the open QUIC standard and
designed so individual customers may adopt it without ISP involvement. QPEP's
performance is assessed through simulations in a replicable docker-based
testbed. Across many benchmarks and network conditions, QPEP is found to avoid
the perceived security-encryption trade-off in PEP design. Compared to
unencrypted PEP implementations, QPEP reduces average page load times by more
than 30% while also offering over-the-air privacy. Compared to the traditional
VPN encryption available to customers today, QPEP more than halves average page
load times. Together, these experiments lead to the conclusion that QPEP
represents a promising new approach to protecting modern satellite broadband
connections.
"
2039,adPerf: Characterizing the Performance of Third-party Ads,"  Monetizing websites and web apps through online advertising is widespread in
the web ecosystem. The online advertising ecosystem nowadays forces publishers
to integrate ads from these third-party domains. On the one hand, this raises
several privacy and security concerns that are actively studied in recent
years. On the other hand, given the ability of today's browsers to load dynamic
web pages with complex animations and Javascript, online advertising has also
transformed and can have a significant impact on webpage performance. The
performance cost of online ads is critical since it eventually impacts user
satisfaction as well as their Internet bill and device energy consumption.
  In this paper, we apply an in-depth and first-of-a-kind performance
evaluation of web ads. Unlike prior efforts that rely primarily on adblockers,
we perform a fine-grained analysis on the web browser's page loading process to
demystify the performance cost of web ads. We aim to characterize the cost by
every component of an ad, so the publisher, ad syndicate, and advertiser can
improve the ad's performance with detailed guidance. For this purpose, we
develop an infrastructure, adPerf, for the Chrome browser that classifies page
loading workloads into ad-related and main-content at the granularity of
browser activities (such as Javascript and Layout). Our evaluations show that
online advertising entails more than 15% of browser page loading workload and
approximately 88% of that is spent on JavaScript. We also track the sources and
delivery chain of web ads and analyze performance considering the origin of the
ad contents. We observe that 2 of the well-known third-party ad domains
contribute to 35% of the ads performance cost and surprisingly, top news
websites implicitly include unknown third-party ads which in some cases build
up to more than 37% of the ads performance cost.
"
2040,"Performance Modeling of Epidemic Routing in Mobile Social Networks with
  Emphasis on Scalability","  This paper investigates the performance of epidemic routing in mobile social
networks. It first analyzes the time taken for a node to meet the first node of
a set of nodes restricted to move in a specific subarea. Afterwards, a
monolithic Stochastic Reward Net (SRN) is proposed to evaluate the delivery
delay and the average number of transmissions under epidemic routing by
considering skewed location visiting preferences. This model is not scalable
enough, in terms of the number of nodes and frequently visited locations. In
order to achieve higher scalability, the folding technique is applied to the
monolithic model, and an approximate folded SRN is proposed to evaluate
performance of epidemic routing. Discrete-event simulation is used to validate
the proposed models. Both SRN models show high accuracy in predicting the
performance of epidemic routing. We also propose an Ordinary Differential
Equation (ODE) model for epidemic routing and compare it with the folded model.
Obtained results show that the folded model is more accurate than the ODE
model. Moreover, it is proved that the number of transmissions by the time of
delivery follows uniform distribution, in a general class of networks, where
positions of nodes are always independent and identically distributed.
"
2041,"A Prompt Report on the Performance of Intel Optane DC Persistent Memory
  Module","  In this prompt report, we present the basic performance evaluation of Intel
Optane Data Center Persistent Memory Module (Optane DCPMM), which is the first
commercially-available, byte-addressable non-volatile memory modules released
in April 2019. Since at the moment of writing only a few reports on its
performance were published, this letter is intended to complement other
performance studies. Through experiments using our own measurement tools, we
obtained that the latency of random read-only access was approximately 374 ns.
That of random writeback-involving access was 391 ns. The bandwidths of
read-only and writeback-involving access for interleaved memory modules were
approximately 38 GB/s and 3 GB/s, respectively.
"
2042,Deploying large fixed file datasets with SquashFS and Singularity,"  Shared high-performance computing (HPC) platforms, such as those provided by
XSEDE and Compute Canada, enable researchers to carry out large-scale
computational experiments at a fraction of the cost of the cloud. Most systems
require the use of distributed filesystems (e.g. Lustre) for providing a highly
multi-user, large capacity storage environment. These suffer performance
penalties as the number of files increases due to network contention and
metadata performance. We demonstrate how a combination of two technologies,
Singularity and SquashFS, can help developers, integrators, architects, and
scientists deploy large datasets (O(10M) files) on these shared systems with
minimal performance limitations. The proposed integration enables more
efficient access and indexing than normal file-based dataset installations,
while providing transparent file access to users and processes. Furthermore,
the approach does not require administrative privileges on the target system.
While the examples studied here have been taken from the field of neuroimaging,
the technologies adopted are not specific to that field. Currently, this
solution is limited to read-only datasets. We propose the adoption of this
technology for the consumption and dissemination of community datasets across
shared computing resources.
"
2043,"MDInference: Balancing Inference Accuracy and Latency for Mobile
  Applications","  Deep Neural Networks are allowing mobile devices to incorporate a wide range
of features into user applications. However, the computational complexity of
these models makes it difficult to run them effectively on resource-constrained
mobile devices. Prior work approached the problem of supporting deep learning
in mobile applications by either decreasing model complexity or utilizing
powerful cloud servers. These approaches each only focus on a single aspect of
mobile inference and thus they often sacrifice overall performance.
  In this work we introduce a holistic approach to designing mobile deep
inference frameworks. We first identify the key goals of accuracy and latency
for mobile deep inference and the conditions that must be met to achieve them.
We demonstrate our holistic approach through the design of a hypothetical
framework called MDInference. This framework leverages two complementary
techniques; a model selection algorithm that chooses from a set of cloud-based
deep learning models to improve inference accuracy and an on-device request
duplication mechanism to bound latency. Through empirically-driven simulations
we show that MDInference improves aggregate accuracy over static approaches by
over 40% without incurring SLA violations. Additionally, we show that with a
target latency of 250ms, MDInference increased the aggregate accuracy in 99.74%
cases on faster university networks and 96.84% cases on residential networks.
"
2044,Performance Analysis of Load Balancing Policies with Memory,"  Joining the shortest or least loaded queue among $d$ randomly selected queues
are two fundamental load balancing policies. Under both policies the dispatcher
does not maintain any information on the queue length or load of the servers.
In this paper we analyze the performance of these policies when the dispatcher
has some memory available to store the ids of some of the idle servers. We
consider methods where the dispatcher discovers idle servers as well as methods
where idle servers inform the dispatcher about their state.
  We focus on large-scale systems and our analysis uses the cavity method. The
main insight provided is that the performance measures obtained via the cavity
method for a load balancing policy with memory reduce to the performance
measures for the same policy without memory provided that the arrival rate is
properly scaled. Thus, we can study the performance of load balancers with
memory in the same manner as load balancers without memory. In particular this
entails closed form solutions for joining the shortest or least loaded queue
among $d$ randomly selected queues with memory in case of exponential job
sizes.
  We present simulation results that support our belief that the approximation
obtained by the cavity method becomes exact as the number of servers tends to
infinity.
"
2045,"An optimal scheduling architecture for accelerating batch algorithms on
  Neural Network processor architectures","  In neural network topologies, algorithms are running on batches of data
tensors. The batches of data are typically scheduled onto the computing cores
which execute in parallel. For the algorithms running on batches of data, an
optimal batch scheduling architecture is very much needed by suitably utilizing
hardware resources - thereby resulting in significant reduction training and
inference time. In this paper, we propose to accelerate the batch algorithms
for neural networks through a scheduling architecture enabling optimal compute
power utilization. The proposed optimal scheduling architecture can be built
into HW or can be implemented in SW alone which can be leveraged for
accelerating batch algorithms. The results demonstrate that the proposed
architecture speeds up the batch algorithms compared to the previous solutions.
The proposed idea applies to any HPC architecture meant for neural networks.
"
2046,"AIBench: An Agile Domain-specific Benchmarking Methodology and an AI
  Benchmark Suite","  Domain-specific software and hardware co-design is encouraging as it is much
easier to achieve efficiency for fewer tasks. Agile domain-specific
benchmarking speeds up the process as it provides not only relevant design
inputs but also relevant metrics, and tools. Unfortunately, modern workloads
like Big data, AI, and Internet services dwarf the traditional one in terms of
code size, deployment scale, and execution path, and hence raise serious
benchmarking challenges.
  This paper proposes an agile domain-specific benchmarking methodology.
Together with seventeen industry partners, we identify ten important end-to-end
application scenarios, among which sixteen representative AI tasks are
distilled as the AI component benchmarks. We propose the permutations of
essential AI and non-AI component benchmarks as end-to-end benchmarks. An
end-to-end benchmark is a distillation of the essential attributes of an
industry-scale application. We design and implement a highly extensible,
configurable, and flexible benchmark framework, on the basis of which, we
propose the guideline for building end-to-end benchmarks, and present the first
end-to-end Internet service AI benchmark.
  The preliminary evaluation shows the value of our benchmark suite---AIBench
against MLPerf and TailBench for hardware and software designers,
micro-architectural researchers, and code developers. The specifications,
source code, testbed, and results are publicly available from the web site
\url{http://www.benchcouncil.org/AIBench/index.html}.
"
2047,"IoTRepair: Systematically Addressing Device Faults in Commodity IoT
  (Extended Paper)","  IoT devices are decentralized and deployed in un-stable environments, which
causes them to be prone to various kinds of faults, such as device failure and
network disruption. Yet, current IoT platforms require programmers to handle
faults manually, a complex and error-prone task. In this paper, we present
IoTRepair, a fault-handling system for IoT that (1)integrates a fault
identification module to track faulty devices,(2) provides a library of
fault-handling functions for effectively handling different fault types, (3)
provides a fault handler on top of the library for autonomous IoT fault
handling, with user and developer configuration as input. Through an evaluation
in a simulated lab environment and with various fault injectio
nmethods,IoTRepair is compared with current fault-handling solutions. The fault
handler reduces the incorrect states on average 50.01%, which corresponds to
less unsafe and insecure device states. Overall, through a systematic design of
an IoT fault handler, we provide users flexibility and convenience in handling
complex IoT fault handling, allowing safer IoT environments.
"
2048,"Marvel: A Data-centric Compiler for DNN Operators on Spatial
  Accelerators","  The efficiency of a spatial DNN accelerator depends heavily on the compiler
and its cost model ability to generate optimized mappings for various operators
of DNN models on to the accelerator's compute and memory resources. But,
existing cost models lack a formal boundary over the operators for precise and
tractable analysis, which poses adaptability challenges for new DNN operators.
To address this challenge, we leverage the recently introduced Maestro
Data-Centric (MDC) notation. We develop a formal understanding of DNN operators
whose mappings can be described in the MDC notation, because any mapping
adhering to the notation is always analyzable by the MDC's cost model.
Furthermore, we introduce a transformation for translating mappings into the
MDC notation for exploring the mapping space.
  Searching for the optimal mappings is challenging because of the large space
of mappings, and this challenge gets exacerbated with new operators and diverse
accelerator configurations.To address this challenge, we propose a decoupled
off-chip/on-chip approach that decomposes the mapping space into off-chip and
on-chip subspaces, and first optimizes the off-chip subspace followed by the
on-chip subspace. The motivation for this decomposition is to reduce the size
of the search space dramatically and also to prioritize the optimization of
off-chip data movement, which is 2-3 orders of magnitude more compared to the
on-chip data movement. We implemented our approach in a tool called {\em
Marvel}, and another major benefit of our approach is that it is applicable to
any DNN operator conformable with the MDC notation.
"
2049,"Verified Instruction-Level Energy Consumption Measurement for NVIDIA
  GPUs","  GPUs are prevalent in modern computing systems at all scales. They consume a
significant fraction of the energy in these systems. However, vendors do not
publish the actual cost of the power/energy overhead of their internal
microarchitecture. In this paper, we accurately measure the energy consumption
of various PTX instructions found in modern NVIDIA GPUs. We provide an
exhaustive comparison of more than 40 instructions for four high-end NVIDIA
GPUs from four different generations (Maxwell, Pascal, Volta, and Turing).
Furthermore, we show the effect of the CUDA compiler optimizations on the
energy consumption of each instruction. We use three different software
techniques to read the GPU on-chip power sensors, which use NVIDIA's NVML API
and provide an in-depth comparison between these techniques. Additionally, we
verified the software measurement techniques against a custom-designed hardware
power measurement. The results show that Volta GPUs have the best energy
efficiency of all the other generations for the different categories of the
instructions. This work should aid in understanding NVIDIA GPUs'
microarchitecture. It should also make energy measurements of any GPU kernel
both efficient and accurate.
"
2050,Performance Analysis of Single-Cell Adaptive Data Rate-Enabled LoRaWAN,"  LoRaWAN enables massive connectivity for Internet-of-Things applications.
Many published works employ stochastic geometry to derive outage models of
LoRaWAN over fading channels assuming fixed transmit power and distance-based
spreading factor (SF) allocation. However, in practice, LoRaWAN employs the
Adaptive Data Rate (ADR) mechanism, which dynamically adjusts SF and transmit
power of nodes based on channel state. The community addressed the performance
of ADR using simulations, but analytical models have not been introduced. In
this letter, we seek to close this gap. We build over an analytical LoRaWAN
model to consider the performance of steady-state ADR-enabled LoRaWAN. We
derive outage expressions and an optimization procedure to maximize the number
of users under reliability constraints. Results show that power allocation
reduces interference and improves network capacity while reducing average
power.
"
2051,"Throughput Optimal Decentralized Scheduling with Single-bit State
  Feedback for a Class of Queueing Systems","  Motivated by medium access control for resource-challenged wireless Internet
of Things (IoT), we consider the problem of queue scheduling with reduced queue
state information. In particular, we consider a time-slotted scheduling model
with $N$ sensor nodes, with pair-wise dependence, such that Nodes $i$ and $i +
1,~0 < i < N$ cannot transmit together. We develop new throughput-optimal
scheduling policies requiring only the empty-nonempty state of each queue that
we term Queue Nonemptiness-Based (QNB) policies. We propose a Policy Splicing
technique to combine scheduling policies for small networks in order to
construct throughput-optimal policies for larger networks, some of which also
aim for low delay. For $N = 3,$ there exists a sum-queue length optimal QNB
scheduling policy. We show, however, that for $N > 4,$ there is no QNB policy
that is sum-queue length optimal over all arrival rate vectors in the capacity
region.
  We then extend our results to a more general class of interference
constraints that we call cluster-of-cliques (CoC) conflict graphs. We consider
two types of CoC networks, namely, Linear Arrays of Cliques (LAoC) and
Star-of-Cliques (SoC) networks. We develop QNB policies for these classes of
networks, study their stability and delay properties, and propose and analyze
techniques to reduce the amount of state information to be disseminated across
the network for scheduling. In the SoC setting, we propose a throughput-optimal
policy that only uses information that nodes in the network can glean by
sensing activity (or lack thereof) on the channel. Our throughput-optimality
results rely on two new arguments: a Lyapunov drift lemma specially adapted to
policies that are queue length-agnostic, and a priority queueing analysis for
showing strong stability.
"
2052,"Honing and proofing Astrophysical codes on the road to Exascale.
  Experiences from code modernization on many-core systems","  The complexity of modern and upcoming computing architectures poses severe
challenges for code developers and application specialists, and forces them to
expose the highest possible degree of parallelism, in order to make the best
use of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second
generation (code-named Knights Landing, henceforth KNL) is the latest many-core
system, which implements several interesting hardware features like for example
a large number of cores per node (up to 72), the 512 bits-wide vector registers
and the high-bandwidth memory. The unique features of KNL make this platform a
powerful testbed for modern HPC applications. The performance of codes on KNL
is therefore a useful proxy of their readiness for future architectures. In
this work we describe the lessons learnt during the optimisation of the widely
used codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover,
we present results for the visualisation and analysis tools VisIt and yt. These
examples show that modern architectures benefit from code optimisation at
different levels, even more than traditional multi-core systems. However, the
level of modernisation of typical community codes still needs improvements, for
them to fully utilise resources of novel architectures.
"
2053,Interface Modeling for Quality and Resource Management,"  We develop an interface-modeling framework for quality and resource
management that captures configurable working points of hardware and software
components in terms of functionality, resource usage and provision, and quality
indicators such as performance and energy consumption. We base these aspects on
partially-ordered sets to capture quality levels, budget sizes, and functional
compatibility. This makes the framework widely applicable and domain
independent (although we aim for embedded and cyber-physical systems). The
framework paves the way for dynamic (re-)configuration and multi-objective
optimization of component-based systems for quality- and resource-management
purposes.
"
2054,Beyond 5G Low-Power Wide-Area Networks: A LoRaWAN Suitability Study,"  In this paper, we deliver a discussion regarding the role of Low-Power
Wide-Area Networks (LPWAN) in the cellular Internet-of-Things (IoT)
infrastructure to support massive Machine-Type Communications (mMTC) in
next-generation wireless systems beyond 5G. We commence by presenting a
performance analysis of current LPWAN systems, specifically LoRaWAN, in terms
of coverage and throughput. The results obtained using analytic methods and
network simulations are combined in the paper for getting a more comprehensive
vision. Next, we identify possible performance bottlenecks, speculate on the
characteristics of coming IoT applications, and seek to identify potential
enhancements to the current technologies that may overcome the identified
shortcomings.
"
2055,Re-evaluating scaling methods for distributed parallel systems,"  The paper explains why Amdahl's Law shall be interpreted specifically for
distributed parallel systems and why it generated so many debates, discussions,
and abuses. We set up a general model and list many of the terms affecting
parallel processing. We scrutinize the validity of neglecting certain terms in
different approximations, with special emphasis on the famous scaling laws of
parallel processing. We clarify that when using the right interpretation of
terms, Amdahl's Law is the governing law of all kinds of parallel processing.
Amdahl's Law describes among others the history of supercomputing, the inherent
performance limitation of the different kinds of parallel processing and it is
the basic Law of the 'modern computing' paradigm, that the computing systems
working under extreme computing conditions are desperately needed.
"
2056,"Asymptotically Optimal Load Balancing in Large-scale Heterogeneous
  Systems with Multiple Dispatchers","  We consider the load balancing problem in large-scale heterogeneous systems
with multiple dispatchers. We introduce a general framework called
Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps
local (possibly outdated) estimates of queue lengths for all the servers, and
the dispatching decision is made purely based on these local estimates. The
local estimates are updated via infrequent communications between dispatchers
and servers. We derive sufficient conditions for LED policies to achieve
throughput optimality and delay optimality in heavy-traffic, respectively.
These conditions directly imply delay optimality for many previous local-memory
based policies in heavy traffic. Moreover, the results enable us to design new
delay optimal policies for heterogeneous systems with multiple dispatchers.
Finally, the heavy-traffic delay optimality of the LED framework directly
resolves a recent open problem on how to design optimal load balancing schemes
using delayed information.
"
2057,Taurus: An Intelligent Data Plane,"  Emerging applications -- cloud computing, the internet of things, and
augmented/virtual reality -- need responsive, available, secure, ubiquitous,
and scalable datacenter networks. Network management currently uses simple,
per-packet, data-plane heuristics (e.g., ECMP and sketches) under an
intelligent, millisecond-latency control plane that runs data-driven
performance and security policies. However, to meet users' quality-of-service
expectations in a modern data center, networks must operate intelligently at
line rate. In this paper, we present Taurus, an intelligent data plane capable
of machine-learning inference at line rate. Taurus adds custom hardware based
on a map-reduce abstraction to programmable network devices, such as switches
and NICs; this new hardware uses pipelined and SIMD parallelism for fast
inference. Our evaluation of a Taurus-enabled switch ASIC -- supporting several
real-world benchmarks -- shows that Taurus operates three orders of magnitude
faster than a server-based control plane, while increasing area by 24% and
latency, on average, by 178 ns. On the long road to self-driving networks,
Taurus is the equivalent of adaptive cruise control: deterministic rules steer
flows, while machine learning tunes performance and heightens security.
"
2058,"Simplified Ray Tracing for the Millimeter Wave Channel: A Performance
  Evaluation","  Millimeter-wave (mmWave) communication is one of the cornerstone innovations
of fifth-generation (5G) wireless networks, thanks to the massive bandwidth
available in these frequency bands. To correctly assess the performance of such
systems, however, it is essential to have reliable channel models, based on a
deep understanding of the propagation characteristics of the mmWave signal. In
this respect, ray tracers can provide high accuracy, at the expense of a
significant computational complexity, which limits the scalability of
simulations. To address this issue, in this paper we present possible
simplifications that can reduce the complexity of ray tracing in the mmWave
environment, without significantly affecting the accuracy of the model. We
evaluate the effect of such simplifications on link-level metrics, testing
different configuration parameters and propagation scenarios.
"
2059,Graph Computing based Distributed State Estimation with PMUs,"  Power system state estimation plays a fundamental and critical role in the
energy management system (EMS). To achieve a high performance and accurate
system states estimation, a graph computing based distributed state estimation
approach is proposed in this paper. Firstly, a power system network is divided
into multiple areas. Reference buses are selected with PMUs being installed at
these buses for each area. Then, the system network is converted into multiple
independent areas. In this way, the power system state estimation could be
conducted in parallel for each area and the estimated system states are
obtained without compromise of accuracy. IEEE 118-bus system and MP 10790-bus
system are employed to verify the results accuracy and present the promising
computation performance.
"
2060,"A New Approach for Improvement Security against DoS Attacks in Vehicular
  Ad-hoc Network","  Vehicular Ad-Hoc Networks (VANET) are a proper subset of mobile wireless
networks, where nodes are revulsive, the vehicles are armed with special
electronic devices on the motherboard OBU (On Board Unit) which enables them to
trasmit and receive messages from other vehicles in the VANET. Furthermore the
communication between the vehicles, the VANET interface is donated by the
contact points with road infrastructure. VANET is a subgroup of MANETs. Unlike
the MANETs nodes, VANET nodes are moving very fast. Impound a permanent route
for the dissemination of emergency messages and alerts from a danger zone is a
very challenging task. Therefore, routing plays a significant duty in VANETs.
decreasing network overhead, avoiding network congestion, increasing traffic
congestion and packet delivery ratio are the most important issues associated
with routing in VANETs. In addition, VANET network is subject to various
security attacks. In base VANET systems, an algorithm is used to dicover
attacks at the time of confirmation in which overhead delay occurs. This paper
proposes (P-Secure) approach which is used for the detection of DoS attacks
before the confirmation time. This reduces the overhead delays for processing
and increasing the security in VANETs. Simulation results show that the
P-Secure approach, is more efficient than OBUmodelVaNET approach in terms of
PDR, e2e_delay, throughput and drop packet rate.
"
2061,Learning Queuing Networks by Recurrent Neural Networks,"  It is well known that building analytical performance models in practice is
difficult because it requires a considerable degree of proficiency in the
underlying mathematics. In this paper, we propose a machine-learning approach
to derive performance models from data. We focus on queuing networks, and
crucially exploit a deterministic approximation of their average dynamics in
terms of a compact system of ordinary differential equations. We encode these
equations into a recurrent neural network whose weights can be directly related
to model parameters. This allows for an interpretable structure of the neural
network, which can be trained from system measurements to yield a white-box
parameterized model that can be used for prediction purposes such as what-if
analyses and capacity planning. Using synthetic models as well as a real case
study of a load-balancing system, we show the effectiveness of our technique in
yielding models with high predictive power.
"
2062,Towards a Geometry Automated Provers Competition,"  The geometry automated theorem proving area distinguishes itself by a large
number of specific methods and implementations, different approaches
(synthetic, algebraic, semi-synthetic) and different goals and applications
(from research in the area of artificial intelligence to applications in
education).
  Apart from the usual measures of efficiency (e.g. CPU time), the possibility
of visual and/or readable proofs is also an expected output against which the
geometry automated theorem provers (GATP) should be measured.
  The implementation of a competition between GATP would allow to create a test
bench for GATP developers to improve the existing ones and to propose new ones.
It would also allow to establish a ranking for GATP that could be used by
""clients"" (e.g. developers of educational e-learning systems) to choose the
best implementation for a given intended use.
"
2063,Optimizing Memory-Access Patterns for Deep Learning Accelerators,"  Deep learning (DL) workloads are moving towards accelerators for faster
processing and lower cost. Modern DL accelerators are good at handling the
large-scale multiply-accumulate operations that dominate DL workloads; however,
it is challenging to make full use of the compute power of an accelerator since
the data must be properly staged in a software-managed scratchpad memory.
Failing to do so can result in significant performance loss. This paper
proposes a systematic approach which leverages the polyhedral model to analyze
all operators of a DL model together to minimize the number of memory accesses.
Experiments show that our approach can substantially reduce the impact of
memory accesses required by common neural-network models on a homegrown AWS
machine-learning inference chip named Inferentia, which is available through
Amazon EC2 Inf1 instances.
"
2064,High Performance Code Generation in MLIR: An Early Case Study with GEMM,"  This article is primarily meant to present an early case study on using MLIR,
a new compiler intermediate representation infrastructure, for high-performance
code generation. Aspects of MLIR covered in particular include memrefs, the
affine dialect, and polyhedral utilities and pass infrastructure surrounding
those. This article is also aimed at showing the role compiler infrastructure
could play in generating code that is competitive with highly tuned manually
developed libraries, albeit in a more modular, reusable, and automatable way.
"
2065,Change Point Detection in Software Performance Testing,"  We describe our process for automatic detection of performance changes for a
software product in the presence of noise. A large collection of tests run
periodically as changes to our software product are committed to our source
repository, and we would like to identify the commits responsible for
performance regressions. Previously, we relied on manual inspection of time
series graphs to identify significant changes. That was later replaced with a
threshold-based detection system, but neither system was sufficient for finding
changes in performance in a timely manner. This work describes our recent
implementation of a change point detection system built upon the E-Divisive
means algorithm. The algorithm produces a list of change points representing
significant changes from a given history of performance results. A human
reviews the list of change points for actionable changes, which are then
triaged for further inspection. Using change point detection has had a dramatic
impact on our ability to detect performance changes. Quantitatively, it has
dramatically dropped our false positive rate for performance changes, while
qualitatively it has made the entire performance evaluation process easier,
more productive (ex. catching smaller regressions), and more timely.
"
2066,GPU-Accelerated Mobile Multi-view Style Transfer,"  An estimated 60% of smartphones sold in 2018 were equipped with multiple rear
cameras, enabling a wide variety of 3D-enabled applications such as 3D Photos.
The success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a
steady influx of user generated content. These platforms must provide simple
image manipulation tools to facilitate content creation, akin to traditional
photo platforms. Artistic neural style transfer, propelled by recent
advancements in GPU technology, is one such tool for enhancing traditional
photos. However, naively extrapolating single-view neural style transfer to the
multi-view scenario produces visually inconsistent results and is prohibitively
slow on mobile devices. We present a GPU-accelerated multi-view style transfer
pipeline which enforces style consistency between views with on-demand
performance on mobile platforms. Our pipeline is modular and creates high
quality depth and parallax effects from a stereoscopic image pair.
"
2067,Quantized Neural Network Inference with Precision Batching,"  We present PrecisionBatching, a quantized inference algorithm for speeding up
neural network execution on traditional hardware platforms at low bitwidths
without the need for retraining or recalibration. PrecisionBatching decomposes
a neural network into individual bitlayers and accumulates them using fast
1-bit operations while maintaining activations in full precision.
PrecisionBatching not only facilitates quantized inference at low bitwidths (<
8 bits) without the need for retraining/recalibration, but also 1) enables
traditional hardware platforms the ability to realize inference speedups at a
finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows
accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers
to accumulate as a tunable parameter. Across a variety of applications (MNIST,
language modeling, natural language inference) and neural network architectures
(fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of
over 8x on a GPU within a < 1% error margin of the full precision baseline,
outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same
error tolerance.
"
2068,"An Efficient Routing Protocol in Mobile Ad-hoc Networks by using
  Artificial Immune System","  Characteristics of the mobile ad-hoc networks such as nodes high mobility and
limited energy are regarded as the routing challenges in these networks. OLSR
protocol is one of the routing protocols in mobile ad hoc network that selects
the shortest route between source and destination through Dijkstra's algorithm.
However, OLSR suffers from a major problem. It does not consider parameters
such as nodes energy level and links length in its route processing. This paper
employs the artificial immune system (AIS) to enhance efficiency of OLSR
routing protocol. The proposed algorithm, called AIS-OLSR, considers hop count,
remaining energy in the intermediate nodes, and distance among node, which is
realized by negative selection and ClonalG algorithms of AIS. Widespread packet
- level simulation in ns-2 environment, shows that AIS-OLSR outperforms OLSR
and EA-OLSR in terms of packet delivery ratio, throughput, end-end delay and
lifetime.
"
2069,"An Artificial Immune Based Approach for Detection and Isolation
  Misbehavior Attacks in Wireless Networks","  MANETs (Mobile Ad-hoc Networks) is a temporal network, which is managed by
autonomous nodes, which have the ability to communicate with each other without
having fixed network infrastructure or any central base station. Due to some
reasons such as dynamic changes of the network topology, trusting the nodes to
each other, lack of fixed substructure for the analysis of nodes behaviors and
loss of specific offensive lines, this type of networks is not supportive
against malicious nodes attacks. One of these attacks is black hole attack. In
this attack, the malicious nodes absorb data packets and destroy them. Thus, it
is essential to present an algorithm against the black hole attacks. This paper
proposed a new approach, which improvement the security of DSR routing protocol
to encounter the black hole attacks. This schema tries to identify malicious
nodes according to nodes behaviors in a MANETs and isolate them from routing.
The proposed protocol, called AIS-DSR (Artificial Immune System DSR) employ AIS
(Artificial Immune System) to defend against black hole attacks. AIS-DSR is
evaluated through extensive simulations in the ns-2 environment. The results
show that AIS-DSR outperforms other existing solutions in terms of throughput,
end-to-end delay, packets loss ratio and packets drop ratio.
"
2070,"Efficient statistical validation with edge cases to evaluate Highly
  Automated Vehicles","  The widescale deployment of Autonomous Vehicles (AV) seems to be imminent
despite many safety challenges that are yet to be resolved. It is well known
that there are no universally agreed Verification and Validation (VV)
methodologies to guarantee absolute safety, which is crucial for the acceptance
of this technology. Existing standards focus on deterministic processes where
the validation requires only a set of test cases that cover the requirements.
Modern autonomous vehicles will undoubtedly include machine learning and
probabilistic techniques that require a much more comprehensive testing regime
due to the non-deterministic nature of the operating design domain. A rigourous
statistical validation process is an essential component required to address
this challenge. Most research in this area focuses on evaluating system
performance in large scale real-world data gathering exercises (number of miles
travelled), or randomised test scenarios in simulation.
  This paper presents a new approach to compute the statistical characteristics
of a system's behaviour by biasing automatically generated test cases towards
the worst case scenarios, identifying potential unsafe edge cases.We use
reinforcement learning (RL) to learn the behaviours of simulated actors that
cause unsafe behaviour measured by the well established RSS safety metric. We
demonstrate that by using the method we can more efficiently validate a system
using a smaller number of test cases by focusing the simulation towards the
worst case scenario, generating edge cases that correspond to unsafe
situations.
"
2071,Enabling URLLC for Low-Cost IoT Devices via Diversity Combining Schemes,"  Supporting Ultra-Reliable Low-Latency Communication (URLLC) in the Internet
of Things (IoT) era is challenging due to stringent constraints on latency and
reliability combined with the simple circuitry of IoT nodes. Diversity is
usually required for sustaining the reliability levels of URLLC, but there is
an additional delay associated to auxiliary procedures to be considered,
specially when communication includes low-cost IoT devices. Herein, we analyze
Selection Combining (SC) and Switch and Stay Combining (SSC) diversity schemes
as plausible solutions for enabling ultra-reliable low-latency downlink
communications to low-cost IoT devices. We demonstrate the necessity of
considering the time spent in auxiliary procedures, which has not been
traditionally taken into account, while we show its impact on the reliability
performance. We show there is an optimum number of receive antennas, which
suggests that under certain conditions it might be required to turn off some of
them, specially under the SC operation. We highlight the superiority of SSC
with respect to SC as long the associated Signal-to-Noise Ratio threshold is
properly selected. We propose using a fixed threshold relying only on long-term
channel fading statistics, which leads to near-optimum results.
"
2072,Optimizing JPEG Quantization for Classification Networks,"  Deep learning for computer vision depends on lossy image compression: it
reduces the storage required for training and test data and lowers transfer
costs in deployment. Mainstream datasets and imaging pipelines all rely on
standard JPEG compression. In JPEG, the degree of quantization of frequency
coefficients controls the lossiness: an 8 by 8 quantization table (Q-table)
decides both the quality of the encoded image and the compression ratio. While
a long history of work has sought better Q-tables, existing work either seeks
to minimize image distortion or to optimize for models of the human visual
system. This work asks whether JPEG Q-tables exist that are ""better"" for
specific vision networks and can offer better quality--size trade-offs than
ones designed for human perception or minimal distortion. We reconstruct an
ImageNet test set with higher resolution to explore the effect of JPEG
compression under novel Q-tables. We attempt several approaches to tune a
Q-table for a vision task. We find that a simple sorted random sampling method
can exceed the performance of the standard JPEG Q-table. We also use
hyper-parameter tuning techniques including bounded random search, Bayesian
optimization, and composite heuristic optimization methods. The new Q-tables we
obtained can improve the compression rate by 10% to 200% when the accuracy is
fixed, or improve accuracy up to $2\%$ at the same compression rate.
"
2073,Me Love (SYN-)Cookies: SYN Flood Mitigation in Programmable Data Planes,"  The SYN flood attack is a common attack strategy on the Internet, which tries
to overload services with requests leading to a Denial-of-Service (DoS). Highly
asymmetric costs for connection setup - putting the main burden on the attackee
- make SYN flooding an efficient and popular DoS attack strategy. Abusing the
widely used TCP as an attack vector complicates the detection of malicious
traffic and its prevention utilizing naive connection blocking strategies.
Modern programmable data plane devices are capable of handling traffic in the
10 Gbit/s range without overloading. We discuss how we can harness their
performance to defend entire networks against SYN flood attacks. Therefore, we
analyze different defense strategies, SYN authentication and SYN cookie, and
discuss implementation difficulties when ported to different target data
planes: software, network processors, and FPGAs. We provide prototype
implementations and performance figures for all three platforms. Further, we
fully disclose the artifacts leading to the experiments described in this work.
"
2074,"Performance and energy footprint assessment of FPGAs and GPUs on HPC
  systems using Astrophysics application","  New challenges in Astronomy and Astrophysics (AA) are urging the need for a
large number of exceptionally computationally intensive simulations. ""Exascale""
(and beyond) computational facilities are mandatory to address the size of
theoretical problems and data coming from the new generation of observational
facilities in AA. Currently, the High Performance Computing (HPC) sector is
undergoing a profound phase of innovation, in which the primary challenge to
the achievement of the ""Exascale"" is the power-consumption. The goal of this
work is to give some insights about performance and energy footprint of
contemporary architectures for a real astrophysical application in an HPC
context. We use a state-of-the-art N-body application that we re-engineered and
optimized to exploit the heterogeneous underlying hardware fully. We
quantitatively evaluate the impact of computation on energy consumption when
running on four different platforms. Two of them represent the current HPC
systems (Intel-based and equipped with NVIDIA GPUs), one is a micro-cluster
based on ARM-MPSoC, and one is a ""prototype towards Exascale"" equipped with
ARM-MPSoCs tightly coupled with FPGAs. We investigate the behavior of the
different devices where the high-end GPUs excel in terms of time-to-solution
while MPSoC-FPGA systems outperform GPUs in power consumption. Our experience
reveals that considering FPGAs for computationally intensive application seems
very promising, as their performance is improving to meet the requirements of
scientific applications. This work can be a reference for future platforms
development for astrophysics applications where computationally intensive
calculations are required.
"
2075,"Towards Green Computing: A Survey of Performance and Energy Efficiency
  of Different Platforms using OpenCL","  When considering different hardware platforms, not just the time-to-solution
can be of importance but also the energy necessary to reach it. This is not
only the case with battery powered and mobile devices but also with
high-performance parallel cluster systems due to financial and practical limits
on power consumption and cooling. Recent developments in hard- and software
have given programmers the ability to run the same code on a range of different
devices giving rise to the concept of heterogeneous computing. Many of these
devices are optimized for certain types of applications. To showcase the
differences and give a basic outlook on the applicability of different
architectures for specific problems, the cross-platform OpenCL framework was
used to compare both time- and energy-to-solution. A large set of devices
ranging from ARM processors to server CPUs and consumer and enterprise level
GPUs has been used with different benchmarking testcases taken from applied
research applications. While the results show the overall advantages of GPUs in
terms of both runtime and energy efficiency compared to CPUs, ARM devices show
potential for certain applications in massively parallel systems. This study
also highlights how OpenCL enables the use of the same codebase on many
different systems and hardware platforms without specific code adaptations.
"
2076,Modeling the Invariance of Virtual Pointers in LLVM,"  Devirtualization is a compiler optimization that replaces indirect (virtual)
function calls with direct calls. It is particularly effective in
object-oriented languages, such as Java or C++, in which virtual methods are
typically abundant.
  We present a novel abstract model to express the lifetimes of C++ dynamic
objects and invariance of virtual table pointers in the LLVM intermediate
representation. The model and the corresponding implementation in Clang and
LLVM enable full devirtualization of virtual calls whenever the dynamic type is
statically known and elimination of redundant virtual table loads in other
cases.
  Due to the complexity of C++, this has not been achieved by any other C++
compiler so far. Although our model was designed for C++, it is also applicable
to other languages that use virtual dispatch. Our benchmarks show an average of
0.8% performance improvement on real-world C++ programs, with more than 30%
speedup in some cases. The implementation is already a part of the upstream
LLVM/Clang and can be enabled with the -fstrict-vtable-pointers flag.
"
2077,"Optimizing Streaming Parallelism on Heterogeneous Many-Core
  Architectures: A Machine Learning Based Approach","  This article presents an automatic approach to quickly derive a good solution
for hardware resource partition and task granularity for task-based parallel
applications on heterogeneous many-core architectures. Our approach employs a
performance model to estimate the resulting performance of the target
application under a given resource partition and task granularity
configuration. The model is used as a utility to quickly search for a good
configuration at runtime. Instead of hand-crafting an analytical model that
requires expert insights into low-level hardware details, we employ machine
learning techniques to automatically learn it. We achieve this by first
learning a predictive model offline using training programs. The learnt model
can then be used to predict the performance of any unseen program at runtime.
We apply our approach to 39 representative parallel applications and evaluate
it on two representative heterogeneous many-core platforms: a CPU-XeonPhi
platform and a CPU-GPU platform. Compared to the single-stream version, our
approach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the
GPU platform, respectively. These results translate to over 93% of the
performance delivered by a theoretically perfect predictor.
"
2078,Data Warehouse and Decision Support on Integrated Crop Big Data,"  In recent years, precision agriculture is becoming very popular. The
introduction of modern information and communication technologies for
collecting and processing Agricultural data revolutionise the agriculture
practises. This has started a while ago (early 20th century) and it is driven
by the low cost of collecting data about everything; from information on fields
such as seed, soil, fertiliser, pest, to weather data, drones and satellites
images. Specially, the agricultural data mining today is considered as Big Data
application in terms of volume, variety, velocity and veracity. Hence it leads
to challenges in processing vast amounts of complex and diverse information to
extract useful knowledge for the farmer, agronomist, and other businesses. It
is a key foundation to establishing a crop intelligence platform, which will
enable efficient resource management and high quality agronomy decision making
and recommendations. In this paper, we designed and implemented a continental
level agricultural data warehouse (ADW). ADW is characterised by its (1)
flexible schema; (2) data integration from real agricultural multi datasets;
(3) data science and business intelligent support; (4) high performance; (5)
high storage; (6) security; (7) governance and monitoring; (8) consistency,
availability and partition tolerant; (9) cloud compatibility. We also evaluate
the performance of ADW and present some complex queries to extract and return
necessary knowledge about crop management.
"
2079,"The Locus Algorithm IV: Performance metrics of a grid computing system
  used to create catalogues of optimised pointings","  This paper discusses the requirements for and performance metrics of the the
Grid Computing system used to implement the Locus Algorithm to identify optimum
pointings for differential photometry of 61,662,376 stars and 23,779 quasars.
Initial operational tests indicated a need for a software system to analyse the
data and a High Performance Computing system to run that software in a scalable
manner. Practical assessments of the performance of the software in a serial
computing environment were used to provide a benchmark against which the
performance metrics of the HPC solution could be compared, as well as to
indicate any bottlenecks in performance. These performance metrics indicated a
distinct split in the performance dictated more by differences in the input
data than by differences in the design of the systems used. This indicates a
need for experimental analysis of system performance, and suggests that
algorithmic complexity analyses may lead to incorrect or naive conclusions,
especially in systems with high data I/O overhead such as grid computing.
Further, it implies that systems which reduce or eliminate this bottleneck such
as in-memory processing could lead to a substantial increase in performance.
"
2080,Benchmarking TinyML Systems: Challenges and Direction,"  Recent advancements in ultra-low-power machine learning (TinyML) hardware
promises to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted benchmark for
these systems. Benchmarking allows us to measure and thereby systematically
compare, evaluate, and improve the performance of systems and is therefore
fundamental to a field reaching maturity. In this position paper, we present
the current landscape of TinyML and discuss the challenges and direction
towards developing a fair and useful hardware benchmark for TinyML workloads.
Furthermore, we present our three preliminary benchmarks and discuss our
selection methodology. Our viewpoints reflect the collective thoughts of the
TinyMLPerf working group that is comprised of 30 organizations.
"
2081,"In Datacenter Performance, The Only Constant Is Change","  All computing infrastructure suffers from performance variability, be it
bare-metal or virtualized. This phenomenon originates from many sources: some
transient, such as noisy neighbors, and others more permanent but sudden, such
as changes or wear in hardware, changes in the underlying hypervisor stack, or
even undocumented interactions between the policies of the computing resource
provider and the active workloads. Thus, performance measurements obtained on
clouds, HPC facilities, and, more generally, datacenter environments are almost
guaranteed to exhibit performance regimes that evolve over time, which leads to
undesirable nonstationarities in application performance. In this paper, we
present our analysis of performance of the bare-metal hardware available on the
CloudLab testbed where we focus on quantifying the evolving performance regimes
using changepoint detection. We describe our findings, backed by a dataset with
nearly 6.9M benchmark results collected from over 1600 machines over a period
of 2 years and 9 months. These findings yield a comprehensive characterization
of real-world performance variability patterns in one computing facility, a
methodology for studying such patterns on other infrastructures, and contribute
to a better understanding of performance variability in general.
"
2082,"Securing of Unmanned Aerial Systems (UAS) against security threats using
  human immune system","  UASs form a large part of the fighting ability of the advanced military
forces. In particular, these systems that carry confidential information are
subject to security attacks. Accordingly, an Intrusion Detection System (IDS)
has been proposed in the proposed design to protect against the security
problems using the human immune system (HIS). The IDSs are used to detect and
respond to attempts to compromise the target system. Since the UASs operate in
the real world, the testing and validation of these systems with a variety of
sensors is confronted with problems. This design is inspired by HIS. In the
mapping, insecure signals are equivalent to an antigen that are detected by
antibody-based training patterns and removed from the operation cycle. Among
the main uses of the proposed design are the quick detection of intrusive
signals and quarantining their activity. Moreover, SUAS-HIS method is evaluated
here via extensive simulations carried out in NS-3 environment. The simulation
results indicate that the UAS network performance metrics are improved in terms
of false positive rate, false negative rate, detection rate, and packet
delivery rate.
"
2083,Scaling Hyperledger Fabric Using Pipelined Execution and Sparse Peers,"  Many proofs of concept blockchain applications built using Hyperledger
Fabric, a permissioned blockchain platform, have recently been transformed into
production. However, the performance provided by Hyperledger Fabric is of
significant concern for enterprises due to steady growth in network usage.
Hence, in this paper, we study the performance achieved in a Fabric network
using vertical scaling (i.e., by adding more vCPUs) and horizontal scaling
(i.e., by adding more nodes) techniques. We observe that network scales very
poorly with both of these techniques. With vertical scaling, due to serial
execution of validation & commit phases of transactions, the allocated vCPUs
are underutilized. With horizontal scaling, due to redundant work between
nodes, allocated resources are wasted though it is utilized. Further, we
identify these techniques to be unsuited for dynamically scaling a network
quickly to mitigate an overload situation, and hence, it results in a 30% drop
in the performance. To increase the CPU utilization and hence the performance,
we re-architect Fabric to enable pipelined execution of validation & commit
phases by introducing dirty state management using a trie data structure.
Additionally, we facilitated the validation phase to validate transactions in
parallel by introducing a waiting-transactions dependency graph. To avoid
redundant work performed between nodes and to quickly scale up a network, we
propose a new type of peer node called sparse peer, which selective commits
transactions. Overall, we improved the throughput by 3x and reduced the time
taken to scale up a network by 96%.
"
2084,Covert Cycle Stealing in a Single FIFO Server,"  Consider a setting where Willie generates a Poisson stream of jobs and routes
them to a single server that follows the first-in first-out discipline. Suppose
there is an adversary Alice, who desires to receive service without being
detected. We ask the question: what is the amount of service that she can
receive covertly, i.e. without being detected by Willie? In the case where both
Willie and Alice jobs have exponential service times with respective rates
$\mu_1$ and $\mu_2$, we demonstrate a phase-transition when Alice adopts the
strategy of inserting a single job probabilistically when the server idles :
over $n$ busy periods, she can achieve a covert throughput of
$\mathcal{O}(\sqrt{n})$ when $\mu_1 < 2\mu_2$, $\mathcal{O}(\sqrt{n/\log n})$
when $\mu_1 = 2\mu_2$, and $\mathcal{O}(n^{\mu_2/\mu_1})$ when $\mu_1 >
2\mu_2$. When both Willie and Alice jobs have general service times we
establish an upper bound for the amount of service Alice can get covertly. This
bound is related to the Fisher information. Additional upper bounds are
obtained for more general insertion policies.
"
2085,"In Situ Network and Application Performance Measurement on Android
  Devices and the Imperfections","  Understanding network and application performance are essential for
debugging, improving user experience, and performance comparison. Meanwhile,
modern mobile systems are optimized for energy-efficient computation and
communications that may limit the performance of network and applications. In
recent years, several tools have emerged that analyze network performance of
mobile applications in~situ with the help of the VPN service. There is a
limited understanding of how these measurement tools and system optimizations
affect the network and application performance. In this study, we first
demonstrate that mobile systems employ energy-aware system hardware tuning,
which affects application performance and network throughput. We next show that
the VPN-based application performance measurement tools, such as Lumen,
PrivacyGuard, and Video Optimizer, aid in ambiguous network performance
measurements and degrade the application performance. Our findings suggest that
sound application and network performance measurement on Android devices
requires a good understanding of the device, networks, measurement tools, and
applications.
"
2086,How Fast Can We Insert? A Performance Study of Apache Kafka,"  Message brokers see widespread adoption in modern IT landscapes, with Apache
Kafka being one of the most employed platforms. These systems feature
well-defined APIs for use and configuration and present flexible solutions for
various data storage scenarios. Their ability to scale horizontally enables
users to adapt to growing data volumes and changing environments. However, one
of the main challenges concerning message brokers is the danger of them
becoming a bottleneck within an IT architecture. To prevent this, knowledge
about the amount of data a message broker using a specific configuration can
handle needs to be available. In this paper, we propose a monitoring
architecture for message brokers and similar Java Virtual Machine-based
systems. We present a comprehensive performance analysis of the popular Apache
Kafka platform using our approach. As part of the benchmark, we study selected
data ingestion scenarios with respect to their maximum data ingestion rates.
The results show that we can achieve an ingestion rate of about 420,000
messages/second on the used commodity hardware and with the developed data
sender tool.
"
2087,Web Performance with Android's Battery-Saver Mode,"  A Web browser utilizes a device's CPU to parse HTML, build a Document Object
Model, a Cascading Style Sheets Object Model, and render trees, and parse,
compile, and execute computationally-heavy JavaScript. A powerful CPU is
required to perform these tasks as quickly as possible and provide the user
with a great experience. However, increased CPU performance comes with
increased power consumption and reduced battery life on mobile devices. As an
option to extend battery life, Android offers a battery-saver mode that when
activated, turns off the power-hungry and faster processor cores and turns on
the battery-conserving and slower processor cores on the device. The transition
from using faster processor cores to using slower processor cores throttles the
CPU clock speed on the device, and therefore impacts the webpage load process.
We utilize a large-scale data-set collected by a real user monitoring system of
a major content delivery network to investigate the impact of Android's
battery-saver mode on various mobile Web performance metrics. Our analysis
suggests that users of select smartphones of Huawei and Sony experience a
sudden or gradual degradation in Web performance when battery-saver mode is
active. Battery-saver mode on newer flagship smartphones, however, does not
impact the mobile Web performance. Finally, we encourage for new website design
goals that treat slow (and throttled-CPU) devices kindly in favor of improving
end-user experience and suggest that Web performance measurements should be
aware of user device battery charge levels to correctly associate Web
performance.
"
2088,"CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation
  Co-Design Goes a Long Way","  Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance.
"
2089,"Towards automated kernel selection in machine learning systems: A SYCL
  case study","  Automated tuning of compute kernels is a popular area of research, mainly
focused on finding optimal kernel parameters for a problem with fixed input
sizes. This approach is good for deploying machine learning models, where the
network topology is constant, but machine learning research often involves
changing network topologies and hyperparameters. Traditional kernel auto-tuning
has limited impact in this case; a more general selection of kernels is
required for libraries to accelerate machine learning research.
  In this paper we present initial results using machine learning to select
kernels in a case study deploying high performance SYCL kernels in libraries
that target a range of heterogeneous devices from desktop GPUs to embedded
accelerators. The techniques investigated apply more generally and could
similarly be integrated with other heterogeneous programming systems. By
combining auto-tuning and machine learning these kernel selection processes can
be deployed with little developer effort to achieve high performance on new
hardware.
"
2090,"Causal datasheet: An approximate guide to practically assess Bayesian
  networks in the real world","  In solving real-world problems like changing healthcare-seeking behaviors,
designing interventions to improve downstream outcomes requires an
understanding of the causal links within the system. Causal Bayesian Networks
(BN) have been proposed as one such powerful method. In real-world
applications, however, confidence in the results of BNs are often moderate at
best. This is due in part to the inability to validate against some ground
truth, as the DAG is not available. This is especially problematic if the
learned DAG conflicts with pre-existing domain doctrine. At the policy level,
one must justify insights generated by such analysis, preferably accompanying
them with uncertainty estimation. Here we propose a causal extension to the
datasheet concept proposed by Gebru et al (2018) to include approximate BN
performance expectations for any given dataset. To generate the results for a
prototype Causal Datasheet, we constructed over 30,000 synthetic datasets with
properties mirroring characteristics of real data. We then recorded the results
given by state-of-the-art structure learning algorithms. These results were
used to populate the Causal Datasheet, and recommendations were automatically
generated dependent on expected performance. As a proof of concept, we used our
Causal Datasheet Generation Tool (CDG-T) to assign expected performance
expectations to a maternal health survey we conducted in Uttar Pradesh, India.
"
2091,Developing a Recommendation Benchmark for MLPerf Training and Inference,"  Deep learning-based recommendation models are used pervasively and broadly,
for example, to recommend movies, products, or other information most relevant
to users, in order to enhance the user experience. Among various application
domains which have received significant industry and academia research
attention, such as image classification, object detection, language and speech
translation, the performance of deep learning-based recommendation models is
less well explored, even though recommendation tasks unarguably represent
significant AI inference cycles at large-scale datacenter fleets. To advance
the state of understanding and enable machine learning system development and
optimization for the commerce domain, we aim to define an industry-relevant
recommendation benchmark for the MLPerf Training andInference Suites. The paper
synthesizes the desirable modeling strategies for personalized recommendation
systems. We lay out desirable characteristics of recommendation model
architectures and data sets. We then summarize the discussions and advice from
the MLPerf Recommendation Advisory Board.
"
2092,"Towards High Performance, Portability, and Productivity: Lightweight
  Augmented Neural Networks for Performance Prediction","  Writing high-performance code requires significant expertise in the
programming language, compiler optimizations, and hardware knowledge. This
often leads to poor productivity and portability and is inconvenient for a
non-programmer domain-specialist such as a Physicist. More desirable is a
high-level language where the domain-specialist simply specifies the workload
in terms of high-level operations (e.g., matrix-multiply(A, B)), and the
compiler identifies the best implementation fully utilizing the heterogeneous
platform. For creating a compiler that supports productivity, portability, and
performance simultaneously, it is crucial to predict the performance of various
available implementations (variants) of the dominant operations (kernels)
contained in the workload on various hardware to decide (a) which variant
should be chosen for each kernel in the workload, and (b) on which hardware
resource the variant should run. To enable the performance prediction, we
propose lightweight augmented neural networks for arbitrary combinations of
kernel-variant-hardware. A key innovation is utilizing the mathematical
complexity of the kernels as a feature to achieve higher accuracy. These models
are compact to reduce training time and fast inference during compile-time and
run-time. Using models with less than 75 parameters, and only 250 training data
instances, we are able to obtain a low MAPE of 3%, significantly outperforming
traditional feed-forward neural networks on 48 kernel-variant-hardware
combinations. We further demonstrate that our variant-selection approach can be
used in Halide implementations to obtain up to 1.7x speedup over Halide's
auto-scheduler.
"
2093,"ContainerStress: Autonomous Cloud-Node Scoping Framework for Big-Data ML
  Use Cases","  Deploying big-data Machine Learning (ML) services in a cloud environment
presents a challenge to the cloud vendor with respect to the cloud container
configuration sizing for any given customer use case. OracleLabs has developed
an automated framework that uses nested-loop Monte Carlo simulation to
autonomously scale any size customer ML use cases across the range of cloud
CPU-GPU ""Shapes"" (configurations of CPUs and/or GPUs in Cloud containers
available to end customers). Moreover, the OracleLabs and NVIDIA authors have
collaborated on a ML benchmark study which analyzes the compute cost and GPU
acceleration of any ML prognostic algorithm and assesses the reduction of
compute cost in a cloud container comprising conventional CPUs and NVIDIA GPUs.
"
2094,"Integrating State of the Art Compute, Communication, and Autotuning
  Strategies to Multiply the Performance of the Application Programm CPMD for
  Ab Initio Molecular Dynamics Simulations","  We present our recent code modernizations of the of the ab initio molecular
dynamics program CPMD (www.cpmd.org) with a special focus on the ultra-soft
pseudopotential (USPP) code path. Following the internal instrumentation of
CPMD, all time critical routines have been revised to maximize the
computational throughput and to minimize the communication overhead for optimal
performance. Throughout the program missing hybrid MPI+OpenMP parallelization
has been added to optimize scaling. For communication intensive routines, as
the multiple distributed 3d FFTs of the electronic states and distributed
matrix-matrix multiplications related to the $\beta$-projectors of the
pseudopotentials, this MPI+OpenMP parallelization now overlaps computation and
communication. The necessary partitioning of the workload is optimized by an
auto-tuning algorithm. In addition, the largest global MPI_Allreduce operation
has been replaced by highly tuned node-local parallelized operations using MPI
shared-memory windows to avoid inter-node communication. A batched algorithm
for the multiple 3d FFTs improves the throughput of the MPI_Alltoall
communication and, thus, the scalability of the implementation, both for USPP
and for the frequently used norm-conserving pseudopotential code path. The
enhanced performance and scalability is demonstrated on a mid-sized benchmark
system of 256 water molecules and further water systems of from 32 up to 2048
molecules.
"
2095,GraphChallenge.org Triangle Counting Performance,"  The rise of graph analytic systems has created a need for new ways to measure
and compare the capabilities of graph processing systems. The MIT/Amazon/IEEE
Graph Challenge has been developed to provide a well-defined community venue
for stimulating research and highlighting innovations in graph analysis
software, hardware, algorithms, and systems. GraphChallenge.org provides a wide
range of pre-parsed graph data sets, graph generators, mathematically defined
graph algorithms, example serial implementations in a variety of languages, and
specific metrics for measuring performance. The triangle counting component of
GraphChallenge.org tests the performance of graph processing systems to count
all the triangles in a graph and exercises key graph operations found in many
graph algorithms. In 2017, 2018, and 2019 many triangle counting submissions
were received from a wide range of authors and organizations. This paper
presents a performance analysis of the best performers of these submissions.
These submissions show that their state-of-the-art triangle counting execution
time, $T_{\rm tri}$, is a strong function of the number of edges in the graph,
$N_e$, which improved significantly from 2017 ($T_{\rm tri} \approx
(N_e/10^8)^{4/3}$) to 2018 ($T_{\rm tri} \approx N_e/10^9$) and remained
comparable from 2018 to 2019. Graph Challenge provides a clear picture of
current graph analysis systems and underscores the need for new innovations to
achieve high performance on very large graphs.
"
2096,A Transactional Perspective on Execute-order-validate Blockchains,"  Smart contracts have enabled blockchain systems to evolve from simple
cryptocurrency platforms, such as Bitcoin, to general transactional systems,
such as Ethereum. Catering for emerging business requirements, a new
architecture called execute-order-validate has been proposed in Hyperledger
Fabric to support parallel transactions and improve the blockchain's
throughput. However, this new architecture might render many invalid
transactions when serializing them. This problem is further exaggerated as the
block formation rate is inherently limited due to other factors beside data
processing, such as cryptography and consensus.
  In this work, we propose a novel method to enhance the execute-order-validate
architecture, by reducing invalid transactions to improve the throughput of
blockchains. Our method is inspired by state-of-the-art optimistic concurrency
control techniques in modern database systems. In contrast to existing
blockchains that adopt database's preventive approaches which might abort
serializable transactions, our method is theoretically more fine-grained.
Specifically, unserializable transactions are aborted before ordering and the
remaining transactions are guaranteed to be serializable. For evaluation, we
implement our method in two blockchains respectively, FabricSharp on top of
Hyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the
performance of FabricSharp with vanilla Fabric and three related systems, two
of which are respectively implemented with one standard and one
state-of-the-art concurrency control techniques from databases. The results
demonstrate that FabricSharp achieves 25% higher throughput compared to the
other systems in nearly all experimental scenarios. Moreover, the
FastFabricSharp's improvement over FastFabric is up to 66%.
"
2097,Efficient Tensor Kernel methods for sparse regression,"  Recently, classical kernel methods have been extended by the introduction of
suitable tensor kernels so to promote sparsity in the solution of the
underlying regression problem. Indeed, they solve an lp-norm regularization
problem, with p=m/(m-1) and m even integer, which happens to be close to a
lasso problem. However, a major drawback of the method is that storing tensors
requires a considerable amount of memory, ultimately limiting its
applicability. In this work we address this problem by proposing two advances.
First, we directly reduce the memory requirement, by intriducing a new and more
efficient layout for storing the data. Second, we use a Nystrom-type
subsampling approach, which allows for a training phase with a smaller number
of data points, so to reduce the computational cost. Experiments, both on
synthetic and read datasets, show the effectiveness of the proposed
improvements. Finally, we take case of implementing the cose in C++ so to
further speed-up the computation.
"
2098,"ProGraML: Graph-based Deep Learning for Program Optimization and
  Analysis","  The increasing complexity of computing systems places a tremendous burden on
optimizing compilers, requiring ever more accurate and aggressive
optimizations. Machine learning offers significant benefits for constructing
optimization heuristics but there remains a gap between what state-of-the-art
methods achieve and the performance of an optimal heuristic. Closing this gap
requires improvements in two key areas: a representation that accurately
captures the semantics of programs, and a model architecture with sufficient
expressiveness to reason about this representation.
  We introduce ProGraML - Program Graphs for Machine Learning - a novel
graph-based program representation using a low level, language agnostic, and
portable format; and machine learning models capable of performing complex
downstream tasks over these graphs. The ProGraML representation is a directed
attributed multigraph that captures control, data, and call relations, and
summarizes instruction and operand types and ordering. Message Passing Neural
Networks propagate information through this structured representation, enabling
whole-program or per-vertex classification tasks.
  ProGraML provides a general-purpose program representation that equips
learnable models to perform the types of program analysis that are fundamental
to optimization. To this end, we evaluate the performance of our approach first
on a suite of traditional compiler analysis tasks: control flow reachability,
dominator trees, data dependencies, variable liveness, and common subexpression
detection. On a benchmark dataset of 250k LLVM-IR files covering six source
programming languages, ProGraML achieves an average 94.0 F1 score,
significantly outperforming the state-of-the-art approaches. We then apply our
approach to two high-level tasks - heterogeneous device mapping and program
classification - setting new state-of-the-art performance in both.
"
2099,Slow and Stale Gradients Can Win the Race,"  Distributed Stochastic Gradient Descent (SGD) when run in a synchronous
manner, suffers from delays in runtime as it waits for the slowest workers
(stragglers). Asynchronous methods can alleviate stragglers, but cause gradient
staleness that can adversely affect the convergence error. In this work, we
present a novel theoretical characterization of the speedup offered by
asynchronous methods by analyzing the trade-off between the error in the
trained model and the actual training runtime(wallclock time). The main novelty
in our work is that our runtime analysis considers random straggling delays,
which helps us design and compare distributed SGD algorithms that strike a
balance between straggling and staleness. We also provide a new error
convergence analysis of asynchronous SGD variants without bounded or
exponential delay assumptions. Finally, based on our theoretical
characterization of the error-runtime trade-off, we propose a method of
gradually varying synchronicity in distributed SGD and demonstrate its
performance on CIFAR10 dataset.
"
2100,Towards Fine-Grained Billing For Cloud Networking,"  We revisit multi-tenant network virtualization in data centers, and make the
case for tenant-specific virtual switches. In particular, tenant-specific
virtual switches allow cloud providers to extend fine-grained billing (known,
e.g., from serverless architectures) to the network, accounting not only for
IO, but also CPU or energy. We sketch an architecture and present economical
motivation and recent technological enablers. We also find that virtual
switches today do not offer sufficient multi-tenancy and can introduce
artificial performance bottlenecks, e.g., in load balancers. We conclude by
discussing additional use cases for tentant-specific switches.
"
2101,Gadget3 on GPUs with OpenACC,"  We present preliminary results of a GPU porting of all main Gadget3 modules
(gravity computation, SPH density computation, SPH hydrodynamic force, and
thermal conduction) using OpenACC directives. Here we assign one GPU to each
MPI rank and exploit both the host and accellerator capabilities by overlapping
computations on the CPUs and GPUs: while GPUs asynchronously compute
interactions between particles within their MPI ranks, CPUs perform tree-walks
and MPI communications of neighbouring particles. We profile various portions
of the code to understand the origin of our speedup, where we find that a peak
speedup is not achieved because of time-steps with few active particles. We run
a hydrodynamic cosmological simulation from the Magneticum project, with
$2\cdot10^{7}$ particles, where we find a final total speedup of $\approx 2.$
We also present the results of an encouraging scaling test of a preliminary
gravity-only OpenACC porting, run in the context of the EuroHack17 event, where
the prototype of the porting proved to keep a constant speedup up to $1024$
GPUs.
"
2102,A Robust Queueing Network Analyzer Based on Indices of Dispersion,"  We develop a robust queueing network analyzer algorithm to approximate the
steady-state performance of a single-class open queueing network of
single-server queues with Markovian routing. The algorithm allows non-renewal
external arrival processes, general service-time distributions and customer
feedback. We focus on the customer flows, defined as the continuous-time
processes counting customers flowing into or out of the network, or flowing
from one queue to another. Each flow is partially characterized by its rate and
a continuous function that measures the stochastic variability over time. This
function is a scaled version of the variance-time curve, called the index of
dispersion for counts (IDC). The required IDC functions for the flows can be
calculated from the model primitives, estimated from data or approximated by
solving a set of linear equations. A robust queueing technique is used to
generate approximations of the mean steady-state performance at each queue from
the IDC of the total arrival flow and the service specification at that queue.
The algorithm effectiveness is supported by extensive simulation studies and
heavy-traffic limits.
"
2103,"Next-Generation Information Technology Systems for Fast Detectors in
  Electron Microscop","  The Gatan K2 IS direct electron detector (Gatan Inc., 2018), which was
introduced in 2014, marked a watershed moment in the development of cameras for
transmission electron microscopy (TEM) (Pan & Czarnik, 2016). Its pixel
frequency, i.e. the number of data points (pixels) recorded per second, was two
orders of magnitude higher than the fastest cameras available only five years
before. Starting from 2009, the data rate of TEM cameras has outpaced the
development of network, mass storage and memory bandwidth by almost two orders
of magnitude. Consequently, solutions based on personal computers (PCs) that
were adequate until then are no longer able to handle the resulting data rates.
Instead, tailored high-performance setups are necessary. Similar developments
have occurred for advanced X-ray sources such as the European XFEL, requiring
special information technology (IT) systems for data handling (Sauter, Hattne,
Grosse-Kunstleve, & Echols, 2013) (Fangohr, et al., 2018). Information and
detector technology are currently under rapid development and involve
disruptive technological innovations. This chapter briefly reviews the
technological developments of the past 20 years, presents a snapshot of the
current situation at the beginning of 2019 with many practical considerations,
and looks forward to future developments.
"
2104,Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic,"  We consider scheduling to minimize mean response time of the M/G/k queue with
unknown job sizes. In the single-server case, the optimal policy is the Gittins
policy, but it is not known whether Gittins or any other policy is optimal in
the multiserver case. Exactly analyzing the M/G/k under any scheduling policy
is intractable, and Gittins is a particularly complicated policy that is hard
to analyze even in the single-server case.
  In this work we introduce monotonic Gittins (M-Gittins), a new variation of
the Gittins policy, and show that it minimizes mean response time in the
heavy-traffic M/G/k for a wide class of finite-variance job size distributions.
We also show that the monotonic shortest expected remaining processing time
(M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for
mean response time in the heavy traffic M/G/k under similar conditions. These
results constitute the most general optimality results to date for the M/G/k
with unknown job sizes. Our techniques build upon work by Grosof et al., who
study simple policies, such as SRPT, in the M/G/k; Bansal et al., Kamphorst and
Zwart, and Lin et al., who analyze mean response time scaling of simple
policies in the heavy-traffic M/G/1; and Aalto et al. and Scully et al., who
characterize and analyze the Gittins policy in the M/G/1.
"
2105,Fundamental Limits of Online Network-Caching,"  Optimal caching of files in a content distribution network (CDN) is a problem
of fundamental and growing commercial interest. Although many different caching
algorithms are in use today, the fundamental performance limits of network
caching algorithms from an online learning point-of-view remain poorly
understood to date. In this paper, we resolve this question in the following
two settings: (1) a single user connected to a single cache, and (2) a set of
users and a set of caches interconnected through a bipartite network. Recently,
an online gradient-based coded caching policy was shown to enjoy sub-linear
regret. However, due to the lack of known regret lower bounds, the question of
the optimality of the proposed policy was left open. In this paper, we settle
this question by deriving tight non-asymptotic regret lower bounds in both of
the above settings. In addition to that, we propose a new
Follow-the-Perturbed-Leader-based uncoded caching policy with near-optimal
regret. Technically, the lower-bounds are obtained by relating the online
caching problem to the classic probabilistic paradigm of balls-into-bins. Our
proofs make extensive use of a new result on the expected load in the most
populated half of the bins, which might also be of independent interest. We
evaluate the performance of the caching policies by experimenting with the
popular MovieLens dataset and conclude the paper with design recommendations
and a list of open problems.
"
2106,Static vs accumulating priorities in healthcare queues under heavy loads,"  Amid unprecedented times caused by COVID-19, healthcare systems all over the
world are strained to the limits of, or even beyond, capacity. A similar event
is experienced by some healthcare systems regularly, due to for instance
seasonal spikes in the number of patients. We model this as a queueing system
in heavy traffic (where the arrival rate is approaching the service rate from
below) or in overload (where the arrival rate exceeds the service rate). In
both cases we assume that customers (patients) may have different priorities
and we consider two popular service disciplines: static priorities and
accumulating priorities. It has been shown that the latter allows for patients
of all classes to be seen in a timely manner as long as the system is stable.
We demonstrate however that if accumulating priorities are used in the heavy
traffic or overload regime, then all patients, including those with the highest
priority, will experience very long waiting times. If on the other hand static
priorities are applied, then one can ensure that the highest-priority patients
will be seen in a timely manner even in overloaded systems.
"
2107,"Scheduling Parallel-Task Jobs Subject to Packing and Placement
  Constraints","  Motivated by modern parallel computing applications, we consider the problem
of scheduling parallel-task jobs with heterogeneous resource requirements in a
cluster of machines. Each job consists of a set of tasks that can be processed
in parallel, however, the job is considered completed only when all its tasks
finish their processing, which we refer to as ""synchronization"" constraint.
Further, assignment of tasks to machines is subject to ""placement"" constraints,
i.e., each task can be processed only on a subset of machines, and processing
times can also be machine dependent. Once a task is scheduled on a machine, it
requires a certain amount of resource from that machine for the duration of its
processing. A machine can process (""pack"") multiple tasks at the same time,
however the cumulative resource requirement of the tasks should not exceed the
machine's capacity.
  Our objective is to minimize the weighted average of the jobs' completion
times. The problem, subject to synchronization, packing and placement
constraints, is NP-hard, and prior theoretical results only concern much
simpler models. For the case that migration of tasks among the
placement-feasible machines is allowed, we propose a preemptive algorithm with
an approximation ratio of $(6+\epsilon)$. In the special case that only one
machine can process each task, we design an algorithm with improved
approximation ratio of $4$. Finally, in the case that migrations (and
preemptions) are not allowed, we design an algorithm with an approximation
ratio of $24$. Our algorithms use a combination of linear program relaxation
and greedy packing techniques. We present extensive simulation results, using a
real traffic trace, that demonstrate that our algorithms yield significant
gains over the prior approaches.
"
2108,"Heavy Traffic Analysis of the Mean Response Time for Load Balancing
  Policies in the Mean Field Regime","  Mean field models are a popular tool used to analyse load balancing policies.
In some exceptional cases the response time distribution of the mean field
limit has an explicit form. In most cases it can be computed using either a
recursion or a differential equation (for exponential job sizes with mean one).
In this paper we study the value of the mean response time $E[R_\lambda]$ as
the arrival rate $\lambda$ approaches $1$ (i.e. the system gets close to
instability). As $E[R_\lambda]$ diverges to infinity, we scale with
$-\log(1-\lambda)$ and present a method to compute the limit
$\lim_{\lambda\rightarrow 1^-}-E[R_\lambda]/\log(1-\lambda)$.
  This limit has been previously determined for SQ(d) and LL(d), two well-known
policies that assign an incoming job to a server with either the shortest queue
or least work left among $d$ randomly selected servers. However, the derivation
of the result for SQ(d) relied on the closed form representation of the mean
response time and does not seem to generalize well, moreover the proof for
LL(d) is incomplete. In contrast, we present a general result that holds for
any policy for which the associated recursion or differential equation
satisfies a list of criteria. For SQ(d) and LL(d) these criteria are trivially
verified.
  We apply our method to SQ(d,K) resp LL(d,K) with exponential job sizes of
mean one. For these policies, jobs arrive in batches of size $K$ and join the
$K$ servers with the shortest queue resp least amount of work left. For SQ(d,K)
we obtain $\frac{1}{\log(d/K)}$ as limiting value, while for LL(d,K) we find
the limit to be equal to $\frac{K}{d-K}$. We further analyse a policy where
SQ(d_i) resp LL($d_i$) is used with probability $p_i$. For the shortest queue
variant, we obtain the limit $\frac{1}{\log\left(\sum_{i=1}^np_id_i\right)}$,
while for the least loaded variant, we obtain $\frac{1}{\sum_{i=1}^np_id_i-1}$.
"
2109,"Computational Performance of a Germline Variant Calling Pipeline for
  Next Generation Sequencing","  With the booming of next generation sequencing technology and its
implementation in clinical practice and life science research, the need for
faster and more efficient data analysis methods becomes pressing in the field
of sequencing. Here we report on the evaluation of an optimized germline
mutation calling pipeline, HummingBird, by assessing its performance against
the widely accepted BWA-GATK pipeline. We found that the HummingBird pipeline
can significantly reduce the running time of the primary data analysis for
whole genome sequencing and whole exome sequencing while without significantly
sacrificing the variant calling accuracy. Thus, we conclude that expansion of
such software usage will help to improve the primary data analysis efficiency
for next generation sequencing.
"
2110,"Using HEP experiment workflows for the benchmarking and accounting of
  WLCG computing resources","  Benchmarking of CPU resources in WLCG has been based on the HEP-SPEC06 (HS06)
suite for over a decade. It has recently become clear that HS06, which is based
on real applications from non-HEP domains, no longer describes typical HEP
workloads. The aim of the HEP-Benchmarks project is to develop a new benchmark
suite for WLCG compute resources, based on real applications from the LHC
experiments. By construction, these new benchmarks are thus guaranteed to have
a score highly correlated to the throughputs of HEP applications, and a CPU
usage pattern similar to theirs. Linux containers and the CernVM-FS filesystem
are the two main technologies enabling this approach, which had been considered
impossible in the past. In this paper, we review the motivation, implementation
and outlook of the new benchmark suite.
"
2111,Achieving Zero Asymptotic Queueing Delay for Parallel Jobs,"  Zero queueing delay is highly desirable in large-scale computing systems.
Existing work has shown that it can be asymptotically achieved by using the
celebrated Power-of-$d$-choices (pod) policy with a probe overhead $d =
\omega\left(\frac{\log N}{1-\lambda}\right)$, and it is impossible when $d =
O\left(\frac{1}{1-\lambda}\right)$, where $N$ is the number of servers and
$\lambda$ is the load of the system. However, these results are based on the
model where each job is an indivisible unit, which does not capture the
parallel structure of jobs in today's predominant parallel computing paradigm.
  This paper thus considers a model where each job consists of a batch of
parallel tasks. Under this model, we propose a new notion of zero (asymptotic)
queueing delay that requires the job delay under a policy to approach the job
delay given by the max of its tasks' service times, i.e., the job delay
assuming its tasks entered service right upon arrival. This notion quantifies
the effect of queueing on a job level for jobs consisting of multiple tasks,
and thus deviates from the conventional zero queueing delay for single-task
jobs in the literature.
  We show that zero queueing delay for parallel jobs can be achieved using the
batch-filling policy (a variant of the celebrated pod policy) with a probe
overhead $d = \omega\left(\frac{1}{(1-\lambda)\log k}\right)$ in the
sub-Halfin-Whitt heavy-traffic regime, where $k$ is the number of tasks in each
job { and $k$ properly scales with $N$ (the number of servers)}. This result
demonstrates that for parallel jobs, zero queueing delay can be achieved with a
smaller probe overhead. We also establish an impossibility result: we show that
zero queueing delay cannot be achieved if $d = \exp\left({o\left(\frac{\log
N}{\log k}\right)}\right)$.
"
2112,"Characterizing and Modeling Distributed Training with Transient Cloud
  GPU Servers","  Cloud GPU servers have become the de facto way for deep learning
practitioners to train complex models on large-scale datasets. However, it is
challenging to determine the appropriate cluster configuration---e.g., server
type and number---for different training workloads while balancing the
trade-offs in training time, cost, and model accuracy. Adding to the complexity
is the potential to reduce the monetary cost by using cheaper, but revocable,
transient GPU servers.
  In this work, we analyze distributed training performance under diverse
cluster configurations using CM-DARE, a cloud-based measurement and training
framework. Our empirical datasets include measurements from three GPU types,
six geographic regions, twenty convolutional neural networks, and thousands of
Google Cloud servers. We also demonstrate the feasibility of predicting
training speed and overhead using regression-based models. Finally, we discuss
potential use cases of our performance modeling such as detecting and
mitigating performance bottlenecks.
"
2113,"Function-as-a-Service Performance Evaluation: A Multivocal Literature
  Review","  Function-as-a-Service (FaaS) is one form of the serverless cloud computing
paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing
event-triggered code snippets (i.e., functions). Many studies that empirically
evaluate the performance of such FaaS platforms have started to appear but we
are currently lacking a comprehensive understanding of the overall domain. To
address this gap, we conducted a multivocal literature review (MLR) covering
112 studies from academic (51) and grey (61) literature. We find that existing
work mainly studies the AWS Lambda platform and focuses on micro-benchmarks
using simple functions to measure CPU speed and FaaS platform overhead (i.e.,
container cold starts). Further, we discover a mismatch between academic and
industrial sources on tested platform configurations, find that function
triggers remain insufficiently studied, and identify HTTP API gateways and
cloud storages as the most used external service integrations. Following
existing guidelines on experimentation in cloud systems, we discover many flaws
threatening the reproducibility of experiments presented in the surveyed
studies. We conclude with a discussion of gaps in literature and highlight
methodological suggestions that may serve to improve future FaaS performance
evaluation studies.
"
2114,"Offsite Autotuning Approach -- Performance Model Driven Autotuning
  Applied to Parallel Explicit ODE Methods","  Autotuning techniques are a promising approach to minimize the otherwise
tedious manual effort of optimizing scientific applications for a specific
target platform. Ideally, an autotuning approach is capable of reliably
identifying the most efficient implementation variant(s) for a new target
system or new characteristics of the input by applying suitable program
transformations and analytic models. In this work, we introduce Offsite, an
offline autotuning approach which automates this selection process at
installation time by rating implementation variants based on an analytic
performance model without requiring time-consuming runtime experiments. From
abstract multilevel YAML description languages, Offsite automatically derives
optimized, platform-specific and problem-specific code of possible
implementation variants and applies the performance model to these
implementation variants.
  We apply Offsite to parallel numerical methods for ordinary differential
equations (ODEs). In particular, we investigate tuning a specific class of
explicit ODE solvers (PIRK methods) for various initial value problems (IVPs)
on shared-memory systems. Our experiments demonstrate that Offsite is able to
reliably identify a set of the most efficient implementation variants for given
test configurations (ODE solver, IVP, platform) and is capable of effectively
handling important autotuning scenarios.
"
2115,"How Crisp is the Crease? A Subjective Study on Web Browsing Perception
  of Above-The-Fold","  Quality of Experience (QoE) for various types of websites has gained
significant attention in recent years. In order to design and evaluate
websites, a metric that can estimate a user's experienced quality robustly for
diverse content is necessary. SpeedIndex (SI) has been widely adopted to
estimate perceived web page loading progress. It measures the speed of
rendering pixels for the webpage that is visible in the browser window. This is
termed Above-The-Fold (ATF). The influence of animated content on the
perception of ATF has been less comprehensively explored. In this paper, we
present an experimental design and methodology to measure ATF perception for
websites with and without animated elements for various page content
categories. We found that pages with animated elements caused people to have
more varied perceptions of ATF under different network conditions. Animated
content also impacts the page load estimation accuracy of SI for websites. We
discuss how the difference in the perception of ATF will impact the QoE
management of web applications. We explain the necessity of revisiting the
visual assessment of ATF to include the animated contents and improve the
robustness of metrics like SI.
"
2116,"FPDetect: Efficient Reasoning About Stencil Programs Using Selective
  Direct Evaluation","  We present FPDetect, a low overhead approach for detecting logical errors and
soft errors affecting stencil computations without generating false positives.
We develop an offline analysis that tightly estimates the number of
floating-point bits preserved across stencil applications. This estimate
rigorously bounds the values expected in the data space of the computation.
Violations of this bound can be attributed with certainty to errors. FPDetect
helps synthesize error detectors customized for user-specified levels of
accuracy and coverage. FPDetect also enables overhead reduction techniques
based on deploying these detectors coarsely in space and time. Experimental
evaluations demonstrate the practicality of our approach.
"
2117,Towards Rigorous Validation of Energy Optimisation Experiments,"  The optimisation of software energy consumption is of growing importance
across all scales of modern computing, i.e., from embedded systems to
data-centres. Practitioners in the field of Search-Based Software Engineering
and Genetic Improvement of Software acknowledge that optimising software energy
consumption is difficult due to noisy and expensive fitness evaluations.
However, it is apparent from results to date that more progress needs to be
made in rigorously validating optimisation results. This problem is pressing
because modern computing platforms have highly complex and variable behaviour
with respect to energy consumption. To compare solutions fairly we propose in
this paper a new validation approach called R3-validation which exercises
software variants in a rotated-round-robin order. Using a case study, we
present an in-depth analysis of the impacts of changing system states on
software energy usage, and we show how R3-validation mitigates these. We
compare it with current validation approaches across multiple devices and
operating systems, and we show that it aligns better with actual platform
behaviour.
"
2118,"Energy Predictive Models for Convolutional Neural Networks on Mobile
  Platforms","  Energy use is a key concern when deploying deep learning models on mobile and
embedded platforms. Current studies develop energy predictive models based on
application-level features to provide researchers a way to estimate the energy
consumption of their deep learning models. This information is useful for
building resource-aware models that can make efficient use of the hard-ware
resources. However, previous works on predictive modelling provide little
insight into the trade-offs involved in the choice of features on the final
predictive model accuracy and model complexity. To address this issue, we
provide a comprehensive analysis of building regression-based predictive models
for deep learning on mobile devices, based on empirical measurements gathered
from the SyNERGY framework.Our predictive modelling strategy is based on two
types of predictive models used in the literature:individual layers and
layer-type. Our analysis of predictive models show that simple layer-type
features achieve a model complexity of 4 to 32 times less for convolutional
layer predictions for a similar accuracy compared to predictive models using
more complex features adopted by previous approaches. To obtain an overall
energy estimate of the inference phase, we build layer-type predictive models
for the fully-connected and pooling layers using 12 representative
Convolutional NeuralNetworks (ConvNets) on the Jetson TX1 and the Snapdragon
820using software backends such as OpenBLAS, Eigen and CuDNN. We obtain an
accuracy between 76% to 85% and a model complexity of 1 for the overall energy
prediction of the test ConvNets across different hardware-software
combinations.
"
2119,Bit-Parallel Vector Composability for Neural Acceleration,"  Conventional neural accelerators rely on isolated self-sufficient functional
units that perform an atomic operation while communicating the results through
an operand delivery-aggregation logic. Each single unit processes all the bits
of their operands atomically and produce all the bits of the results in
isolation. This paper explores a different design style, where each unit is
only responsible for a slice of the bit-level operations to interleave and
combine the benefits of bit-level parallelism with the abundant data-level
parallelism in deep neural networks. A dynamic collection of these units
cooperate at runtime to generate bits of the results, collectively. Such
cooperation requires extracting new grouping between the bits, which is only
possible if the operands and operations are vectorizable. The abundance of Data
Level Parallelism and mostly repeated execution patterns, provides a unique
opportunity to define and leverage this new dimension of Bit-Parallel Vector
Composability. This design intersperses bit parallelism within data-level
parallelism and dynamically interweaves the two together. As such, the building
block of our neural accelerator is a Composable Vector Unit that is a
collection of Narrower-Bitwidth Vector Engines, which are dynamically composed
or decomposed at the bit granularity. Using six diverse CNN and LSTM deep
networks, we evaluate this design style across four design points: with and
without algorithmic bitwidth heterogeneity and with and without availability of
a high-bandwidth off-chip memory. Across these four design points, Bit-Parallel
Vector Composability brings (1.4x to 3.5x) speedup and (1.1x to 2.7x) energy
reduction. We also comprehensively compare our design style to the Nvidia RTX
2080 TI GPU, which also supports INT-4 execution. The benefits range between
28.0x and 33.7x improvement in Performance-per-Watt.
"
2120,"GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in
  Parallel Linux Applications","  We present a parallel profiling tool, GAPP, that identifies serialization
bottlenecks in parallel Linux applications arising from load imbalance or
contention for shared resources . It works by tracing kernel context switch
events using kernel probes managed by the extended Berkeley Packet Filter
(eBPF) framework. The overhead is thus extremely low (an average 4% run time
overhead for the applications explored), the tool requires no program
instrumentation and works for a variety of serialization bottlenecks. We
evaluate GAPP using the Parsec3.0 benchmark suite and two large open-source
projects: MySQL and Nektar++ (a spectral/hp element framework). We show that
GAPP is able to reveal a wide range of bottleneck-related performance issues,
for example arising from synchronization primitives, busy-wait loops, memory
operations, thread imbalance and resource contention.
"
2121,Comparisons of Algorithms in Big Data Processing,"  Parallel computing is the fundamental base for MapReduce framework in Hadoop.
Each data chunk is replicated over 3 servers for increasing availability of
data and decreasing probability of data loss. Hence, the 3 servers that have
Map task stored on their disk are fastest servers to process them, which are
called local servers. All servers in the same rack as local servers are called
rack-local servers that are slower than local servers since data chunk
associated with Map task should be fetched through top of the rack switch. All
other servers are called remote servers that are slowest servers since they
need to fetch data from a local server in another rack, so data should be
transmitted through at least 2 top of rack switches and a core switch. Note
that number of switches in path of data transfer depends on internal network
structure of data centers. The First-In-First-Out (FIFO) and Hadoop Fair
Scheduler (HFS) algorithms do not take rack structure of data centers into
account, so they are known to not be heavy-traffic delay optimal or even
throughput optimal. The recent advances on scheduling for data centers
considering rack structure of them and heterogeneity of servers resulted in
state-of-the-art Balanced-PANDAS algorithm that outperforms classic MaxWeight
algorithm. In both Balanced-PANDAS and MaxWeight algorithms, processing rate of
local, rack-local, and remote servers are assumed to be known. However, with
the change of traffic over time in addition to estimation errors of processing
rates, it is not realistic to consider processing rates to be known. In this
work, we study robustness of Balanced-PANDAS and MaxWeight algorithms in terms
of inaccurate estimations of processing rates. We observe that Balanced-PANDAS
is not as sensitive as MaxWeight on the accuracy of processing rates, making it
more appealing to use in data centers.
"
2122,"Reduction Methods on Probabilistic Control-flow Programs for Reliability
  Analysis","  Modern safety-critical systems are heterogeneous, complex, and highly
dynamic. They require reliability evaluation methods that go beyond the
classical static methods such as fault trees, event trees, or reliability block
diagrams. Promising dynamic reliability analysis methods employ probabilistic
model checking on various probabilistic state-based models. However, such
methods have to tackle the well-known state-space explosion problem. To compete
with this problem, reduction methods such as symmetry reduction and
partial-order reduction have been successfully applied to probabilistic models
by means of discrete Markov chains or Markov decision processes. Such models
are usually specified using probabilistic programs provided in guarded command
language. In this paper, we propose two automated reduction methods for
probabilistic programs that operate on a purely syntactic level: reset value
optimization and register allocation optimization. The presented techniques
rely on concepts well known from compiler construction such as live range
analysis and register allocation through interference graph coloring. Applied
on a redundancy system model for an aircraft velocity control loop modeled in
SIMULINK, we show effectiveness of our implementation of the reduction methods.
We demonstrate that model-size reductions in three orders of magnitude are
possible and show that we can achieve significant speedups for a reliability
analysis.
"
2123,Scalability of High-Performance PDE Solvers,"  Performance tests and analyses are critical to effective HPC software
development and are central components in the design and implementation of
computational algorithms for achieving faster simulations on existing and
future computing architectures for large-scale application problems. In this
paper, we explore performance and space-time trade-offs for important
compute-intensive kernels of large-scale numerical solvers for PDEs that govern
a wide range of physical applications. We consider a sequence of PDE- motivated
bake-off problems designed to establish best practices for efficient high-order
simulations across a variety of codes and platforms. We measure peak
performance (degrees of freedom per second) on a fixed number of nodes and
identify effective code optimization strategies for each architecture. In
addition to peak performance, we identify the minimum time to solution at 80%
parallel efficiency. The performance analysis is based on spectral and p-type
finite elements but is equally applicable to a broad spectrum of numerical PDE
discretizations, including finite difference, finite volume, and h-type finite
elements.
"
2124,Fair and Efficient Gossip in Hyperledger Fabric,"  Permissioned blockchains are supported by identified but individually
untrustworthy nodes, collectively maintaining a replicated ledger whose content
is trusted. The Hyperledger Fabric permissioned blockchain system targets
high-throughput transaction processing. Fabric uses a set of nodes tasked with
the ordering of transactions using consensus. Additional peers endorse and
validate transactions, and maintain a copy of the ledger. The ability to
quickly disseminate new transaction blocks from ordering nodes to all peers is
critical for both performance and consistency. Broadcast is handled by a gossip
protocol, using randomized exchanges of blocks between peers. We show that the
current implementation of gossip in Fabric leads to heavy tail distributions of
block propagation latencies, impacting performance, consistency, and fairness.
We contribute a novel design for gossip in Fabric that simultaneously optimizes
propagation time, tail latency and bandwidth consumption. Using a 100-node
cluster, we show that our enhanced gossip allows the dissemination of blocks to
all peers more than 10 times faster than with the original implementation,
while decreasing the overall network bandwidth consumption by more than 40%.
With a high throughput and concurrent application, this results in 17% to 36%
fewer invalidated transactions for different block sizes.
"
2125,"Refined Mean Field Analysis of the Gossip Shuffle Protocol -- extended
  version --","  Gossip protocols form the basis of many smart collective adaptive systems.
They are a class of fully decentralised, simple but robust protocols for the
distribution of information throughout large scale networks with hundreds or
thousands of nodes. Mean field analysis methods have made it possible to
approximate and analyse performance aspects of such large scale protocols in an
efficient way. Taking the gossip shuffle protocol as a benchmark, we evaluate a
recently developed refined mean field approach. We illustrate the gain in
accuracy this can provide for the analysis of medium size models analysing two
key performance measures. We also show that refined mean field analysis
requires special attention to correctly capture the coordination aspects of the
gossip shuffle protocol.
"
2126,"Novel Binary-Addition Tree Algorithm (BAT) for Binary-State Network
  Reliability Problem","  Network structures and models have been widely adopted, e.g., for Internet of
Things, wireless sensor networks, smart grids, transportation networks,
communication networks, social networks, and computer grid systems. Network
reliability is an effective and popular technique to estimate the probability
that the network is still functioning. Networks composed of binary-state (e.g.,
working or failed) components (arcs and/or nodes) are called binary-state
networks. The binary-state network is the fundamental type of network; thus,
there is always a need for a more efficient algorithm to calculate the network
reliability. Thus, a novel binary-addition tree (BAT) algorithm that employs
binary addition for finding all the possible state vectors and the path-based
layered-search algorithm for filtering out all the connected vectors is
proposed for calculating the binary-state network reliability. According to the
time complexity and numerical examples, the efficiency of the proposed BAT is
higher than those of traditional algorithms for solving the binary-state
network reliability problem.
"
2127,"Learning Based Hybrid Beamforming Design for Full-Duplex Millimeter Wave
  Systems","  Millimeter Wave (mmWave) communications with full-duplex (FD) have the
potential of increasing the spectral efficiency, relative to those with
half-duplex. However, the residual self-interference (SI) from FD and high
pathloss inherent to mmWave signals may degrade the system performance.
Meanwhile, hybrid beamforming (HBF) is an efficient technology to enhance the
channel gain and mitigate interference with reasonable complexity. However,
conventional HBF approaches for FD mmWave systems are based on optimization
processes, which are either too complex or strongly rely on the quality of
channel state information (CSI). We propose two learning schemes to design HBF
for FD mmWave systems, i.e., extreme learning machine based HBF (ELM-HBF) and
convolutional neural networks based HBF (CNN-HBF). Specifically, we first
propose an alternating direction method of multipliers (ADMM) based algorithm
to achieve SI cancellation beamforming, and then use a
majorization-minimization (MM) based algorithm for joint transmitting and
receiving HBF optimization. To train the learning networks, we simulate noisy
channels as input, and select the hybrid beamformers calculated by proposed
algorithms as targets. Results show that both learning based schemes can
provide more robust HBF performance and achieve at least 22.1% higher spectral
efficiency compared to orthogonal matching pursuit (OMP) algorithms. Besides,
the online prediction time of proposed learning based schemes is almost 20
times faster than the OMP scheme. Furthermore, the training time of ELM-HBF is
about 600 times faster than that of CNN-HBF with 64 transmitting and receiving
antennas.
"
2128,Automated System Performance Testing at MongoDB,"  Distributed Systems Infrastructure (DSI) is MongoDB's framework for running
fully automated system performance tests in our Continuous Integration (CI)
environment. To run in CI it needs to automate everything end-to-end:
provisioning and deploying multi-node clusters, executing tests, tuning the
system for repeatable results, and collecting and analyzing the results. Today
DSI is MongoDB's most used and most useful performance testing tool. It runs
almost 200 different benchmarks in daily CI, and we also use it for manual
performance investigations. As we can alert the responsible engineer in a
timely fashion, all but one of the major regressions were fixed before the
4.2.0 release. We are also able to catch net new improvements, of which DSI
caught 17. We open sourced DSI in March 2020.
"
2129,"Extended Abstract of Performance Analysis and Prediction of Model
  Transformation","  In the software development process, model transformation is increasingly
assimilated. However, systems being developed with model transformation
sometimes grow in size and become complex. Meanwhile, the performance of model
transformation tends to decrease. Hence, performance is an important quality of
model transformation. According to current research model transformation
performance focuses on optimising the engines internally. However, there exists
no research activities to support transformation engineer to identify
performance bottleneck in the transformation rules and hence, to predict the
overall performance. In this paper we vision our aim at providing an approach
of monitoring and profiling to identify the root cause of performance issues in
the transformation rules and to predict the performance of model
transformation. This will enable software engineers to systematically identify
performance issues as well as predict the performance of model transformation.
"
2130,Robust and Scalable Entity Alignment in Big Data,"  Entity alignment has always had significant uses within a multitude of
diverse scientific fields. In particular, the concept of matching entities
across networks has grown in significance in the world of social science as
communicative networks such as social media have expanded in scale and
popularity. With the advent of big data, there is a growing need to provide
analysis on graphs of massive scale. However, with millions of nodes and
billions of edges, the idea of alignment between a myriad of graphs of similar
scale using features extracted from potentially sparse or incomplete datasets
becomes daunting. In this paper we will propose a solution to the issue of
large-scale alignments in the form of a multi-step pipeline. Within this
pipeline we introduce scalable feature extraction for robust temporal
attributes, accompanied by novel and efficient clustering algorithms in order
to find groupings of similar nodes across graphs. The features and their
clusters are fed into a versatile alignment stage that accurately identifies
partner nodes among millions of possible matches. Our results show that the
pipeline can process large data sets, achieving efficient runtimes within the
memory constraints.
"
2131,"Demonstrating a Pre-Exascale, Cost-Effective Multi-Cloud Environment for
  Scientific Computing","  Scientific computing needs are growing dramatically with time and are
expanding in science domains that were previously not compute intensive. When
compute workflows spike well in excess of the capacity of their local compute
resource, capacity should be temporarily provisioned from somewhere else to
both meet deadlines and to increase scientific output. Public Clouds have
become an attractive option due to their ability to be provisioned with minimal
advance notice. The available capacity of cost-effective instances is not well
understood. This paper presents expanding the IceCube's production HTCondor
pool using cost-effective GPU instances in preemptible mode gathered from the
three major Cloud providers, namely Amazon Web Services, Microsoft Azure and
the Google Cloud Platform. Using this setup, we sustained for a whole workday
about 15k GPUs, corresponding to around 170 PFLOP32s, integrating over one
EFLOP32 hour worth of science output for a price tag of about $60k. In this
paper, we provide the reasoning behind Cloud instance selection, a description
of the setup and an analysis of the provisioned resources, as well as a short
description of the actual science output of the exercise.
"
2132,Flattening the Curve: Insights From Queueing Theory,"  The worldwide outbreak of the coronavirus was first identified in 2019 in
Wuhan, China. Since then, the disease has spread worldwide. As it currently
spreading in the United States, policy makers, public health officials and
citizens are racing to understand the impact of this virus on the United States
healthcare system. They fear that the rapid influx of patients will overwhelm
the healthcare system leading to unnecessary fatalities. Most countries and
states in America have introduced mitigation strategies, such as social
distancing, to decrease the rate of newly infected people, i.e. flattening the
curve.In this paper, we analyze the time evolution of the number of people
hospitalized due to the coronavirus using the methods of queueing theory. Given
that the rate of new infections varies over time as the pandemic evolves, we
model the number of coronavirus patients as a dynamical system based on the
theory of infinite server queues with non-stationary Poisson arrival rates.
With this model we are able to quantify how flattening the curve affects the
peak demand for hospital resources. This allows us to characterize how
aggressively society must flatten the curve in order to avoid overwhelming the
capacity of healthcare system. We also demonstrate how flattening the curve
impacts the elapsed time between the peak rate of hospitalizations and the time
of the peak demand for the hospital resources. Finally, we present empirical
evidence from China, South Korea, Italy and the United States that supports the
insights from the model.
"
2133,"PMEvo: Portable Inference of Port Mappings for Out-of-Order Processors
  by Evolutionary Optimization","  Achieving peak performance in a computer system requires optimizations in
every layer of the system, be it hardware or software. A detailed understanding
of the underlying hardware, and especially the processor, is crucial to
optimize software. One key criterion for the performance of a processor is its
ability to exploit instruction-level parallelism. This ability is determined by
the port mapping of the processor, which describes the execution units of the
processor for each instruction.
  Processor manufacturers usually do not share the port mappings of their
microarchitectures. While approaches to automatically infer port mappings from
experiments exist, they are based on processor-specific hardware performance
counters that are not available on every platform.
  We present PMEvo, a framework to automatically infer port mappings solely
based on the measurement of the execution time of short instruction sequences.
PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate
mappings with an analytical throughput model formulated as a linear program.
Our prototype implementation infers a port mapping for Intel's Skylake
architecture that predicts measured instruction throughput with an accuracy
that is competitive to existing work. Furthermore, it finds port mappings for
AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of
scope of existing techniques.
"
2134,"Hardware and Interference Limited Cooperative CR-NOMA Networks under
  Imperfect SIC and CSI","  The conflation of cognitive radio (CR) and nonorthogonal multiple access
(NOMA) concepts is a promising approach to fulfil the massive connectivity
goals of future networks given the spectrum scarcity. Accordingly, this letter
investigates the outage performance of imperfect cooperative CR-NOMA networks
under hardware impairments and interference. Our analysis is involved with the
derivation of the end-to-end outage probability (OP) for secondary NOMA users
by accounting for imperfect channel state information (CSI), as well as the
residual interference caused by successive interference cancellation (SIC)
errors and coexisting primary/secondary users. The numerical results validated
by Monte Carlo simulations show that CR-NOMA network provides a superior outage
performance over orthogonal multiple access. As imperfections become more
significant, CR-NOMA is observed to deliver relatively poor outage performance.
"
2135,"A Non-Ideal NOMA-based mmWave D2D Networks with Hardware and CSI
  Imperfections","  This letter investigates a non-orthogonal multiple access (NOMA) assisted
millimeter-wave device-to-device (D2D) network practically limited by multiple
interference noises, transceiver hardware impairments, imperfect successive
interference cancellation, and channel state information mismatch. Generalized
outage probability expressions for NOMA-D2D users are deduced and achieved
results, validated by Monte Carlo simulations, are compared with the orthogonal
multiple access to show the superior performance of the proposed network model
"
2136,"Scaling through abstractions -- high-performance vectorial wave
  simulations for seismic inversion with Devito","  [Devito] is an open-source Python project based on domain-specific language
and compiler technology. Driven by the requirements of rapid HPC applications
development in exploration seismology, the language and compiler have evolved
significantly since inception. Sophisticated boundary conditions, tensor
contractions, sparse operations and features such as staggered grids and
sub-domains are all supported; operators of essentially arbitrary complexity
can be generated. To accommodate this flexibility whilst ensuring performance,
data dependency analysis is utilized to schedule loops and detect
computational-properties such as parallelism. In this article, the generation
and simulation of MPI-parallel propagators (along with their adjoints) for the
pseudo-acoustic wave-equation in tilted transverse isotropic media and the
elastic wave-equation are presented. Simulations are carried out on industry
scale synthetic models in a HPC Cloud system and reach a performance of
28TFLOP/s, hence demonstrating Devito's suitability for production-grade
seismic inversion problems.
"
2137,"Performance Evaluation of Secure Multi-party Computation on
  Heterogeneous Nodes","  Secure multi-party computation (MPC) is a broad cryptographic concept that
can be adopted for privacy-preserving computation. With MPC, a number of
parties can collaboratively compute a function, without revealing the actual
input or output of the plaintext to others. The applications of MPC range from
privacy-preserving voting, arithmetic calculation, and large-scale data
analysis. From the system perspective, each party in MPC can run on one compute
node. The compute nodes of multiple parties could be either homogeneous or
heterogeneous; however, the distributed workloads from the MPC protocols tend
to be always homogeneous (symmetric). In this work, we study a representative
MPC framework and a set of MPC applications from the system performance
perspective. We show the detailed online computation workflow of a
state-of-the-art MPC protocol and analyze the root cause of its stall time and
performance bottleneck on homogeneous and heterogeneous compute nodes.
"
2138,"Outage Analysis of Cognitive Electric Vehicular Networks over Mixed
  RF/VLC Channels","  Modern transportation infrastructures are considered as one of the main
sources of the greenhouse gases emitted into the atmosphere. This situation
requires the decision-making players to enact the mass use of electric vehicles
(EVs) which, in turn, highly demand novel secure communication technologies
robust to various cyber-attacks. Therefore, in this paper, we propose a novel
jamming-robust communication technique for different outdoor cognitive
EV-enabled network cases over mixed radio-frequency (RF)/visible light
communication (VLC) channels. One EV acts as a relaying node to allow an
aggregator to reach the jammed EV and, at the same time, operates in both RF
and VLC spectrum bands while satisfying interference constraints imposed by the
primary network entities. We derive exact closed-form analytical expressions
for the outage probability and also provide their asymptotic analysis while
considering various channel state information quality scenarios. Moreover, we
quantify the outage reduction achievable by deploying such mixed VLC/RF
channels. Finally, analytical and simulation results validate the accuracy of
our analysis.
"
2139,Age of Information for Single Buffer Systems with Vacation Server,"  In this research, we consider age-related metrics for queueing systems with
vacation server. Assuming that there is a single buffer at the queue to receive
packets, we consider three variations of this single buffer system, namely
Conventional Buffer System (CBS), Buffer Relaxation System (BRS), and
Conventional Buffer System with Preemption in Service (CBS-P). We introduce a
decomposition approach to derive the closed-form expressions for expected Age
of Information (AoI), expected Peak Age of Information (PAoI) as well as the
variance of peak age for these systems. We then consider these three systems
with non-independent vacations, and use polling system as an example to show
that the decomposition approach can be applied to derive closed-form
expressions of PAoI for general situation. We explore the conditions under
which one of these systems has advantage over the others, and we further
perform numerical studies to validate our results and develop insights.
"
2140,Data-Driven Model-Based Analysis of the Ethereum Verifier's Dilemma,"  In proof-of-work based blockchains such as Ethereum, verification of blocks
is an integral part of establishing consensus across nodes. However, in
Ethereum, miners do not receive a reward for verifying. This implies that
miners face the Verifier's Dilemma: use resources for verification, or use them
for the more lucrative mining of new blocks? We provide an extensive analysis
of the Verifier's Dilemma, using a data-driven model-based approach that
combines closed-form expressions, machine learning techniques and
discrete-event simulation. We collect data from over 300,000 smart contracts
and experimentally obtain their CPU execution times. Gaussian Mixture Models
and Random Forest Regression transform the data into distributions and inputs
suitable for the simulator. We show that, indeed, it is often economically
rational not to verify. We consider two approaches to mitigate the implications
of the Verifier's Dilemma, namely parallelization and active insertion of
invalid blocks, both will be shown to be effective.
"
2141,Iterative Variable Reordering: Taming Huge System Families,"  For the verification of systems using model-checking techniques, symbolic
representations based on binary decision diagrams (BDDs) often help to tackle
the well-known state-space explosion problem. Symbolic BDD-based
representations have been also shown to be successful for the analysis of
families of systems that arise, e.g., through configurable parameters or
following the feature-oriented modeling approach. The state space of such
system families face an additional exponential blowup in the number of
parameters or features. It is well known that the order of variables in ordered
BDDs is crucial for the size of the model representation. Especially for
automatically generated models from real-world systems, family models might
even be not constructible due to bad variable orders. In this paper we describe
a technique, called iterative variable reordering, that can enable the
construction of large-scale family models. We exemplify feasibility of our
approach by means of an aircraft velocity control system with redundancy
mechanisms modeled in the input language of the probabilistic model checker
PRISM. We show that standard reordering and dynamic reordering techniques fail
to construct the family model due to memory and time constraints, respectively,
while the new iterative approach succeeds to generate a symbolic family model.
"
2142,"Enabling EASEY deployment of containerized applications for future HPC
  systems","  The upcoming exascale era will push the changes in computing architecture
from classical CPU-based systems in hybrid GPU-heavy systems with much higher
levels of complexity. While such clusters are expected to improve the
performance of certain optimized HPC applications, it will also increase the
difficulties for those users who have yet to adapt their codes or are starting
from scratch with new programming paradigms. Since there are still no
comprehensive automatic assistance mechanisms to enhance application
performance on such systems, we are proposing a support framework for future
HPC architectures, called EASEY (Enable exASclae for EverYone). The solution
builds on a layered software architecture, which offers different mechanisms on
each layer for different tasks of tuning. This enables users to adjust the
parameters on each of the layers, thereby enhancing specific characteristics of
their codes. We introduce the framework with a Charliecloud-based solution,
showcasing the LULESH benchmark on the upper layers of our framework. Our
approach can automatically deploy optimized container computations with
negligible overhead and at the same time reduce the time a scientist needs to
spent on manual job submission configurations.
"
2143,BlockSim: An Extensible Simulation Tool for Blockchain Systems,"  Both in the design and deployment of blockchain solutions many
performance-impacting configuration choices need to be made. We introduce
BlockSim, a framework and software tool to build and simulate discrete-event
dynamic systems models for blockchain systems. BlockSim is designed to support
the analysis of a large variety of blockchains and blockchain deployments as
well as a wide set of analysis questions. At the core of BlockSim is a Base
Model, which contains the main model constructs common across various
blockchain systems organized in three abstraction layers (network, consensus
and incentives layer). The Base Model is usable for a wide variety of
blockchain systems and can be extended easily to include system or deployment
particulars. The BlockSim software tool provides a simulator that implements
the Base Model in Python. This paper describes the Base Model, the simulator
implementation, and the application of BlockSim to Bitcoin, Ethereum and other
consensus algorithms. We validate BlockSim simulation results by comparison
with performance results from actual systems and from other studies in the
literature. We close the paper by a BlockSim simulation study of the impact of
uncle blocks rewards on mining decentralization, for a variety of blockchain
configurations.
"
2144,Towards Faster Reasoners By Using Transparent Huge Pages,"  Various state-of-the-art automated reasoning (AR) tools are widely used as
backend tools in research of knowledge representation and reasoning as well as
in industrial applications. In testing and verification, those tools often run
continuously or nightly. In this work, we present an approach to reduce the
runtime of AR tools by 10% on average and up to 20% for long running tasks. Our
improvement addresses the high memory usage that comes with the data structures
used in AR tools, which are based on conflict driven no-good learning. We
establish a general way to enable faster memory access by using the memory
cache line of modern hardware more effectively. Therefore, we extend the
standard C library (glibc) by dynamically allowing to use a memory management
feature called huge pages. Huge pages allow to reduce the overhead that is
required to translate memory addresses between the virtual memory of the
operating system and the physical memory of the hardware. In that way, we can
reduce runtime, costs, and energy consumption of AR tools and applications with
similar memory access patterns simply by linking the tool against this new
glibc library when compiling it. In every day industrial applications this
easily allows to be more eco-friendly in computation. To back up the claimed
speed-up, we present experimental results for tools that are commonly used in
the AR community, including the domains ASP, BMC, MaxSAT, SAT, and SMT.
"
2145,"Communication-Aware Scheduling of Precedence-Constrained Tasks on
  Related Machines","  Scheduling precedence-constrained tasks is a classical problem that has been
studied for more than fifty years. However, little progress has been made in
the setting where there are communication delays between tasks. Results for the
case of identical machines were derived nearly thirty years ago, and yet no
results for related machines have followed. In this work, we propose a new
scheduler, Generalized Earliest Time First (GETF), and provide the first
provable, worst-case approximation guarantees for the goals of minimizing both
the makespan and total weighted completion time of tasks with precedence
constraints on related machines with machine-dependent communication times.
"
2146,"A Lower Bound on the stability region of Redundancy-d with FIFO service
  discipline","  Redundancy-d (R(d)) is a load balancing method used to route incoming jobs to
K servers, each with its own queue. Every arriving job is replicated into
2<=d<=K tasks, which are then routed to d servers chosen uniformly at random.
When the first task finishes service, the remaining d-1 tasks are cancelled and
the job departs the system.
  Despite the fact that R(d) is known, under certain conditions, to
substantially improve job completion times compared to not using redundancy at
all, little is known on a more fundamental performance criterion: what is the
set of arrival rates under which the R(d) queueing system with FIFO service
discipline is stable? In this context, due to the complex dynamics of systems
with redundancy and cancellations, existing results are scarce and are limited
to very special cases with respect to the joint service time distribution of
tasks.
  In this paper we provide a non-trivial, closed form lower bound on the
stability region of R(d) for a general joint service time distribution of tasks
with finite first and second moments. We consider a discrete time system with
Bernoulli arrivals and assume that jobs are processed by their order of
arrival. We use the workload processes and a quadratic Lyapunov function to
characterize the set of arrival rates for which the system is stable. While
simulation results indicate our bound is not tight, it provides an
easy-to-check performance guarantee.
"
2147,Efficiently Reclaiming Space in a Log Structured Store,"  A log structured store uses a single write I/O for a number of diverse and
non-contiguous pages within a large buffer instead of using a write I/O for
each page separately. This requires that pages be relocated on every write,
because pages are never updated in place. Instead, pages are dynamically
remapped on every write. Log structuring was invented for and used initially in
file systems. Today, a form of log structuring is used in SSD controllers
because an SSD requires the erasure of a large block of pages before flash
storage can be reused. No update-in-place requires that the storage for
out-of-date pages be reclaimed (garbage collected or ""cleaned""). We analyze
cleaning performance and introduce a cleaning strategy that uses a new way to
prioritize the order in which stale pages are garbage collected. Our cleaning
strategy approximates an ""optimal cleaning strategy"". Simulation studies
confirm the results of the analysis. This strategy is a significant improvement
over previous cleaning strategies.
"
2148,"Smart, Adaptive Energy Optimization for Mobile Web Interactions","  Web technology underpins many interactive mobile applications. However,
energy-efficient mobile web interactions is an outstanding challenge. Given the
increasing diversity and complexity of mobile hardware, any practical
optimization scheme must work for a wide range of users, mobile platforms and
web workloads. This paper presents CAMEL , a novel energy optimization system
for mobile web interactions. CAMEL leverages machine learning techniques to
develop a smart, adaptive scheme to judiciously trade performance for reduced
power consumption. Unlike prior work, C AMEL directly models how a given web
content affects the user expectation and uses this to guide energy
optimization. It goes further by employing transfer learning and conformal
predictions to tune a previously learned model in the end-user environment and
improve it over time. We apply CAMEL to Chromium and evaluate it on four
distinct mobile systems involving 1,000 testing webpages and 30 users. Compared
to four state-of-the-art web-event optimizers, CAMEL delivers 22% more energy
savings, but with 49% fewer violations on the quality of user experience, and
exhibits orders of magnitudes less overhead when targeting a new computing
environment.
"
2149,CPU and GPU Accelerated Fully Homomorphic Encryption,"  Fully Homomorphic Encryption (FHE) is one of the most promising technologies
for privacy protection as it allows an arbitrary number of function
computations over encrypted data. However, the computational cost of these FHE
systems limits their widespread applications. In this paper, our objective is
to improve the performance of FHE schemes by designing efficient parallel
frameworks. In particular, we choose Torus Fully Homomorphic Encryption (TFHE)
as it offers exact results for an infinite number of boolean gate (e.g., AND,
XOR) evaluations. We first extend the gate operations to algebraic circuits
such as addition, multiplication, and their vector and matrix equivalents.
Secondly, we consider the multi-core CPUs to improve the efficiency of both the
gate and the arithmetic operations. Finally, we port the TFHE to the Graphics
Processing Units (GPU) and device novel optimizations for boolean and
arithmetic circuits employing the multitude of cores. We also experimentally
analyze both the CPU and GPU parallel frameworks for different numeric
representations (16 to 32-bit). Our GPU implementation outperforms the existing
technique, and it achieves a speedup of 20x for any 32-bit boolean operation
and 14.5x for multiplications.
"
2150,Performance of RPL in Healthcare Wireless Sensor Network,"  The new advances of the Internet of Things (IoT) technology can be utilized
to promote service delivery in several real-life applications such as
healthcare systems. The Routing Protocol for Low Power and Loss Network (RPL)
is a routing protocol designed to serve as a proper routing protocol for
packets in Wireless Sensor Networks (WSN). Among the most prominent issues
exist in the RPL protocol are packet loss within the WSN and sensors power
consumption especially in healthcare WSNs. Multiple Objective Functions (OF) in
RPL intended to find the routes from source nodes to a destination node. This
paper presents an evaluation to discover which OF is more efficient for a WSN
in a healthcare scenario where the Packet Delivery Ratio (PDR) of WSN and the
sensors' power consumption are prominent concerns. Expected transmission Count
(ETX) and Objective Function Zero (OF0) of RPL were examined in various network
densities and network topologies such as the grid and random topology. The
simulation outcomes revealed that the OF0 is more efficient regarding the PDR
and power consumption compared to the ETX in random
"
2151,Analysis of the Symmetric Join the Shortest Orbit Queue,"  This work introduces the join the shortest queue policy in the retrial
setting. We consider a Markovian single server retrial system with two infinite
capacity orbits. An arriving job finding the server busy, it is forwarded to
the least loaded orbit. Otherwise, it is forwarded to an orbit randomly.
Orbiting jobs of either type retry to access the server independently. We
investigate the stability condition, the stationary tail decay rate, and obtain
the equilibrium distribution by using the compensation method.
"
2152,"An Overview of Self-Similar Traffic: Its Implications in the Network
  Design","  The knowledge about the true nature of the traffic in computer networking is
a key requirement in the design of such networks. The phenomenon of
self-similarity is a characteristic of the traffic of current client/server
packet networks in LAN/WAN environments dominated by network technologies such
as Ethernet and the TCP/IP protocol stack. The development of networks traffic
simulators, which take into account this attribute, is necessary for a more
realistic description the traffic on these networks and their use in the design
of resources (contention elements) and protocols of flow control and network
congestion. In this scenario it is recommended do not adopt standard traffic
models of the Poisson type.
"
2153,"Near-optimal Detector for SWIPT-enabled Differential DF Relay Networks
  with SER Analysis","  In this paper, we analyze the symbol error rate (SER) performance of the
simultaneous wireless information and power transfer (SWIPT) enabled three-node
differential decode-and-forward (DDF) relay networks, which adopt the power
splitting (PS) protocol at the relay. The use of non-coherent differential
modulation eliminates the need for sending training symbols to estimate the
instantaneous channel state informations (CSIs) at all network nodes, and
therefore improves the power efficiency, as compared with the coherent
modulation. However, performance analysis results are not yet available for the
state-of-the-art detectors such as the approximate maximum-likelihood detector.
Existing works rely on Monte-Carlo simulation to show that there exists an
optimal PS ratio that minimizes the overall SER. In this work, we propose a
near-optimal detector with linear complexity with respect to the modulation
size. We derive an accurate approximate SER expression, based on which the
optimal PS ratio can be accurately estimated without requiring any Monte-Carlo
simulation.
"
2154,Catch Me If You Can: Using Power Analysis to Identify HPC Activity,"  Monitoring users on large computing platforms such as high performance
computing (HPC) and cloud computing systems is non-trivial. Utilities such as
process viewers provide limited insight into what users are running, due to
granularity limitation, and other sources of data, such as system call tracing,
can impose significant operational overhead. However, despite technical and
procedural measures, instances of users abusing valuable HPC resources for
personal gains have been documented in the past \cite{hpcbitmine}, and systems
that are open to large numbers of loosely-verified users from around the world
are at risk of abuse. In this paper, we show how electrical power consumption
data from an HPC platform can be used to identify what programs are executed.
The intuition is that during execution, programs exhibit various patterns of
CPU and memory activity. These patterns are reflected in the power consumption
of the system and can be used to identify programs running. We test our
approach on an HPC rack at Lawrence Berkeley National Laboratory using a
variety of scientific benchmarks. Among other interesting observations, our
results show that by monitoring the power consumption of an HPC rack, it is
possible to identify if particular programs are running with precision up to
and recall of 95\% even in noisy scenarios.
"
2155,Fast Mapping onto Census Blocks,"  Pandemic measures such as social distancing and contact tracing can be
enhanced by rapidly integrating dynamic location data and demographic data.
Projecting billions of longitude and latitude locations onto hundreds of
thousands of highly irregular demographic census block polygons is
computationally challenging in both research and deployment contexts. This
paper describes two approaches labeled ""simple"" and ""fast"". The simple approach
can be implemented in any scripting language (Matlab/Octave, Python, Julia, R)
and is easily integrated and customized to a variety of research goals. This
simple approach uses a novel combination of hierarchy, sparse bounding boxes,
polygon crossing-number, vectorization, and parallel processing to achieve
100,000,000+ projections per second on 100 servers. The simple approach is
compact, does not increase data storage requirements, and is applicable to any
country or region. The fast approach exploits the thread, vector, and memory
optimizations that are possible using a low-level language (C++) and achieves
similar performance on a single server. This paper details these approaches
with the goal of enabling the broader community to quickly integrate location
and demographic data.
"
2156,AIBench: Scenario-distilling AI Benchmarking,"  Real-world application scenarios like modern Internet services consist of
diversity of AI and non-AI modules with very long and complex execution paths.
Using component or micro AI benchmarks alone can lead to error-prone
conclusions. This paper proposes a scenario-distilling AI benchmarking
methodology. Instead of using real-world applications, we propose the
permutations of essential AI and non-AI tasks as a scenario-distilling
benchmark. We consider scenario-distilling benchmarks, component and micro
benchmarks as three indispensable parts of a benchmark suite. Together with
seventeen industry partners, we identify nine important real-world application
scenarios. We design and implement a highly extensible, configurable, and
flexible benchmark framework. On the basis of the framework, we propose the
guideline for building scenario-distilling benchmarks, and present two Internet
service AI ones. The preliminary evaluation shows the advantage of
scenario-distilling AI benchmarking against using component or micro AI
benchmarks alone. The specifications, source code, testbed, and results are
publicly available from the web site
\url{http://www.benchcouncil.org/AIBench/index.html}.
"
2157,"A Collaborative Filtering Approach for the Automatic Tuning of Compiler
  Optimisations","  Selecting the right compiler optimisations has a severe impact on programs'
performance. Still, the available optimisations keep increasing, and their
effect depends on the specific program, making the task human intractable.
Researchers proposed several techniques to search in the space of compiler
optimisations. Some approaches focus on finding better search algorithms, while
others try to speed up the search by leveraging previously collected knowledge.
The possibility to effectively reuse previous compilation results inspired us
toward the investigation of techniques derived from the Recommender Systems
field. The proposed approach exploits previously collected knowledge and
improves its characterisation over time. Differently from current
state-of-the-art solutions, our approach is not based on performance counters
but relies on Reaction Matching, an algorithm able to characterise programs
looking at how they react to different optimisation sets. The proposed approach
has been validated using two widely used benchmark suites, cBench and
PolyBench, including 54 different programs. Our solution, on average, extracted
90% of the available performance improvement 10 iterations before current
state-of-the-art solutions, which corresponds to 40% fewer compilations and
performance tests to perform.
"
2158,"Importing Relationships into a Running Graph Database Using Parallel
  Processing","  Importing relationships into a running graph database using multiple threads
running concurrently is a difficult task, as multiple threads cannot write
information to the same node at the same time. Here we present an algorithm in
which relationships are sorted into bins, then imported such that no two
threads ever access the same node concurrently. When this algorithm was
implemented as a procedure to run on the Neo4j graph database, it reduced the
time to import relationships by up to 69% when 32 threads were used.
"
2159,"Optimizing Deep Learning Recommender Systems' Training On CPU Cluster
  Architectures","  During the last two years, the goal of many researchers has been to squeeze
the last bit of performance out of HPC system for AI tasks. Often this
discussion is held in the context of how fast ResNet50 can be trained.
Unfortunately, ResNet50 is no longer a representative workload in 2020. Thus,
we focus on Recommender Systems which account for most of the AI cycles in
cloud computing centers. More specifically, we focus on Facebook's DLRM
benchmark. By enabling it to run on latest CPU hardware and software tailored
for HPC, we are able to achieve more than two-orders of magnitude improvement
in performance (110x) on a single socket compared to the reference CPU
implementation, and high scaling efficiency up to 64 sockets, while fitting
ultra-large datasets. This paper discusses the optimization techniques for the
various operators in DLRM and which component of the systems are stressed by
these different operators. The presented techniques are applicable to a broader
set of DL workloads that pose the same scaling challenges/characteristics as
DLRM.
"
2160,"Comparison and Benchmarking of AI Models and Frameworks on Mobile
  Devices","  Due to increasing amounts of data and compute resources, deep learning
achieves many successes in various domains. The application of deep learning on
the mobile and embedded devices is taken more and more attentions, benchmarking
and ranking the AI abilities of mobile and embedded devices becomes an urgent
problem to be solved. Considering the model diversity and framework diversity,
we propose a benchmark suite, AIoTBench, which focuses on the evaluation of the
inference abilities of mobile and embedded devices. AIoTBench covers three
typical heavy-weight networks: ResNet50, InceptionV3, DenseNet121, as well as
three light-weight networks: SqueezeNet, MobileNetV2, MnasNet. Each network is
implemented by three frameworks which are designed for mobile and embedded
devices: Tensorflow Lite, Caffe2, Pytorch Mobile. To compare and rank the AI
capabilities of the devices, we propose two unified metrics as the AI scores:
Valid Images Per Second (VIPS) and Valid FLOPs Per Second (VOPS). Currently, we
have compared and ranked 5 mobile devices using our benchmark. This list will
be extended and updated soon after.
"
2161,Learning Algorithms for Minimizing Queue Length Regret,"  We consider a system consisting of a single transmitter/receiver pair and $N$
channels over which they may communicate. Packets randomly arrive to the
transmitter's queue and wait to be successfully sent to the receiver. The
transmitter may attempt a frame transmission on one channel at a time, where
each frame includes a packet if one is in the queue. For each channel, an
attempted transmission is successful with an unknown probability. The
transmitter's objective is to quickly identify the best channel to minimize the
number of packets in the queue over $T$ time slots. To analyze system
performance, we introduce queue length regret, which is the expected difference
between the total queue length of a learning policy and a controller that knows
the rates, a priori. One approach to designing a transmission policy would be
to apply algorithms from the literature that solve the closely-related
stochastic multi-armed bandit problem. These policies would focus on maximizing
the number of successful frame transmissions over time. However, we show that
these methods have $\Omega(\log{T})$ queue length regret. On the other hand, we
show that there exists a set of queue-length based policies that can obtain
order optimal $O(1)$ queue length regret. We use our theoretical analysis to
devise heuristic methods that are shown to perform well in simulation.
"
2162,Demonstrating 100 Gbps in and out of the public Clouds,"  There is increased awareness and recognition that public Cloud providers do
provide capabilities not found elsewhere, with elasticity being a major driver.
The value of elastic scaling is however tightly coupled to the capabilities of
the networks that connect all involved resources, both in the public Clouds and
at the various research institutions. This paper presents results of
measurements involving file transfers inside public Cloud providers, fetching
data from on-prem resources into public Cloud instances and fetching data from
public Cloud storage into on-prem nodes. The networking of the three major
Cloud providers, namely Amazon Web Services, Microsoft Azure and the Google
Cloud Platform, has been benchmarked. The on-prem nodes were managed by either
the Pacific Research Platform or located at the University of Wisconsin -
Madison. The observed sustained throughput was of the order of 100 Gbps in all
the tests moving data in and out of the public Clouds and throughput reaching
into the Tbps range for data movements inside the public Cloud providers
themselves. All the tests used HTTP as the transfer protocol.
"
2163,Understanding Memory Access Patterns Using the BSC Performance Tools,"  The growing gap between processor and memory speeds results in complex memory
hierarchies as processors evolve to mitigate such divergence by taking
advantage of the locality of reference. In this direction, the BSC performance
analysis tools have been recently extended to provide insight relative to the
application memory accesses depicting their temporal and spatial
characteristics, correlating with the source-code and the achieved performance
simultaneously. These extensions rely on the Precise Event-Based Sampling
(PEBS) mechanism available in recent Intel processors to capture information
regarding the application memory accesses. The sampled information is later
combined with the Folding technique to represent a detailed temporal evolution
of the memory accesses and in conjunction with the achieved performance and the
source-code counterpart. The results obtained from the combination of these
tools help not only application developers but also processor architects to
understand better how the application behaves and how the system performs. In
this paper, we describe a tighter integration of the sampling mechanism into
the monitoring package. We also demonstrate the value of the complete workflow
by exploring already optimized state--of--the--art benchmarks, providing
detailed insight of their memory access behavior. We have taken advantage of
this insight to apply small modifications that improve the applications'
performance.
"
2164,Competitive Algorithms for Minimizing the Maximum Age-of-Information,"  In this short paper, we consider the problem of designing a near-optimal
competitive scheduling policy for $N$ mobile users, to maximize the freshness
of available information uniformly across all users. Prompted by the
unreliability and non-stationarity of the emerging 5G-mmWave channels for
high-speed users, we forego of any statistical assumptions of the wireless
channels and user-mobility. Instead, we allow the channel states and the
mobility patterns to be dictated by an omniscient adversary. It is not
difficult to see that no competitive scheduling policy can exist for the
corresponding throughput-maximization problem in this adversarial model.
Surprisingly, we show that there exists a simple online distributed scheduling
policy with a finite competitive ratio for maximizing the freshness of
information in this adversarial model. Moreover, we also prove that the
proposed policy is competitively optimal up to an $O(\ln N)$ factor.
"
2165,"High Performance and Portable Convolution Operators for ARM-based
  Multicore Processors","  The considerable impact of Convolutional Neural Networks on many Artificial
Intelligence tasks has led to the development of various high performance
algorithms for the convolution operator present in this type of networks. One
of these approaches leverages the \imcol transform followed by a general matrix
multiplication (GEMM) in order to take advantage of the highly optimized
realizations of the GEMM kernel in many linear algebra libraries. The main
problems of this approach are 1) the large memory workspace required to host
the intermediate matrices generated by the IM2COL transform; and 2) the time to
perform the IM2COL transform, which is not negligible for complex neural
networks. This paper presents a portable high performance convolution algorithm
based on the BLIS realization of the GEMM kernel that avoids the use of the
intermediate memory by taking advantage of the BLIS structure. In addition, the
proposed algorithm eliminates the cost of the explicit IM2COL transform, while
maintaining the portability and performance of the underlying realization of
GEMM in BLIS.
"
2166,"Performance Analysis for Multi-Antenna Small Cell Networks with
  Clustered Dynamic TDD","  Small cell networks with dynamic time-division duplex (D-TDD) have emerged as
a potential solution to address the asymmetric traffic demands in 5G wireless
networks. By allowing the dynamic adjustment of cell-specific UL/DL
configuration, D-TDD flexibly allocates percentage of subframes to UL and DL
transmissions to accommodate the traffic within each cell. However, the
unaligned transmissions bring in extra interference which degrades the
potential gain achieved by D-TDD. In this work, we propose an analytical
framework to study the performance of multi-antenna small cell networks with
clustered D-TDD, where cell clustering is employed to mitigate the interference
from opposite transmission direction in neighboring cells. With tools from
stochastic geometry, we derive explicit expressions and tractable tight upper
bounds for success probability and network throughput. The proposed analytical
framework allows to quantify the effect of key system parameters, such as UL/DL
configuration, cluster size, antenna number, and SINR threshold. Our results
show the superiority of the clustered D-TDD over the traditional D-TDD, and
reveal the fact that there exists an optimal cluster size for DL performance,
while UL performance always benefits from a larger cluster.
"
2167,"Latency Analysis of Multiple Classes of AVB Traffic in TSN with Standard
  Credit Behavior using Network Calculus","  Time-Sensitive Networking (TSN) is a set of amendments that extend Ethernet
to support distributed safety-critical and real-time applications in the
industrial automation, aerospace and automotive areas. TSN integrates multiple
traffic types and supports interactions in several combinations. In this paper
we consider the configuration supporting Scheduled Traffic (ST) traffic
scheduled based on Gate-Control-Lists (GCLs), Audio-Video-Bridging (AVB)
traffic according to IEEE 802.1BA that has bounded latencies, and Best-Effort
(BE) traffic, for which no guarantees are provided. The paper extends the
timing analysis method to multiple AVB classes and proofs the credit bounds for
multiple classes of AVB traffic, respectively under frozen and non-frozen
behaviors of credit during guard band (GB). They are prerequisites for
non-overflow credits of Credit-Based Shaper (CBS) and preventing starvation of
AVB traffic. Moreover, this paper proposes an improved timing analysis method
reducing the pessimism for the worst-case end-to-end delays of AVB traffic by
considering the limitations from the physical link rate and the output of CBS.
Finally, we evaluate the improved analysis method on both synthetic and
real-world test cases, showing the significant reduction of pessimism on
latency bounds compared to related work, and presenting the correctness
validation compared with simulation results. We also compare the AVB latency
bounds in the case of frozen and non-frozen credit during GB. Additionally, we
evaluate the scalability of our method with variation of the load of ST flows
and of the bandwidth reservation for AVB traffic.
"
2168,"SoS-RPL: Securing Internet of Things Against Sinkhole Attack Using RPL
  Protocol-Based Node Rating and Ranking Mechanism","  Through the Internet of Things (IoT) the internet scope is established by the
integration of physical things to classify themselves into mutual things. A
physical thing can be created by this inventive perception to signify itself in
the digital world. Regarding the physical things that are related to the
internet, it is worth noting that considering numerous theories and upcoming
predictions, they mostly require protected structures, moreover, they are at
risk of several attacks. IoTs are endowed with particular routing disobedience
called sinkhole attack owing to their distributed features. In these attacks, a
malicious node broadcasts illusive information regarding the routings to impose
itself as a route towards specific nodes for the neighboring nodes and thus,
attract data traffic. RPL (IP-V6 routing protocol for efficient and low-energy
networks) is a standard routing protocol which is mainly employed in sensor
networks and IoT. This protocol is called SoS-RPL consisting of two key
sections of the sinkhole detection. In the first section rating and ranking the
nodes in the RPL is carried out based on distance measurements. The second
section is in charge of discovering the misbehavior sources within the IoT
network through, the Average Packet Transmission RREQ (APT-RREQ). Here, the
technique is assessed through wide simulations performed within the NS-3
environment. Based on the results of the simulation, it is indicated that the
IoT network behavior metrics are enhanced based on the detection rate,
false-negative rate, false-positive rate, packet delivery rate, maximum
throughput, and packet loss rate.
"
2169,Optimal Resource Allocation for Elastic and Inelastic Jobs,"  Modern data centers are tasked with processing heterogeneous workloads
consisting of various classes of jobs. These classes differ in their arrival
rates, size distributions, and job parallelizability. With respect to
paralellizability, some jobs are elastic, meaning they can parallelize linearly
across many servers. Other jobs are inelastic, meaning they can only run on a
single server. Although job classes can differ drastically, they are typically
forced to share a single cluster. When sharing a cluster among heterogeneous
jobs, one must decide how to allocate servers to each job at every moment in
time. In this paper, we design and analyze allocation policies which aim to
minimize the mean response time across jobs, where a job's response time is the
time from when it arrives until it completes.
  We model this problem in a stochastic setting where each job may be elastic
or inelastic. Job sizes are drawn from exponential distributions, but are
unknown to the system. We show that, in the common case where elastic jobs are
larger on average than inelastic jobs, the optimal allocation policy is
Inelastic-First, giving inelastic jobs preemptive priority over elastic jobs.
We obtain this result by introducing a novel sample path argument. We also show
that there exist cases where Elastic-First (giving priority to elastic jobs)
performs better than Inelastic-First. We then provide the first analysis of
mean response time under both Elastic-First and Inelastic-First by leveraging
recent techniques for solving high-dimensional Markov chains.
"
2170,Mapping Matters: Application Process Mapping on 3-D Processor Topologies,"  Applications' performance is influenced by the mapping of processes to
computing nodes, the frequency and volume of exchanges among processing
elements, the network capacity, and the routing protocol. A poor mapping of
application processes degrades performance and wastes resources. Process
mapping is frequently ignored as an explicit optimization step since the system
typically offers a default mapping, users may lack awareness of their
applications' communication behavior, and the opportunities for improving
performance through mapping are often unclear. This work studies the impact of
application process mapping on several processor topologies. We propose a
workflow that renders mapping as an explicit optimization step for parallel
applications. We apply the workflow to a set of four applications, twelve
mapping algorithms, and three direct network topologies. We assess the
mappings' quality in terms of volume, frequency, and distance of exchanges
using metrics such as dilation (measured in hop$\cdot$Byte). With a parallel
trace-based simulator, we predict the applications' execution on the three
topologies using the twelve mappings. We evaluate the impact of process mapping
on the applications' simulated performance in terms of execution and
communication times and identify the mappings that achieve the highest
performance in both cases. To ensure the correctness of the simulations, we
compare the pre- and post-simulation results. This work emphasizes the
importance of process mapping as an explicit optimization step and offers a
solution for parallel applications to exploit the full potential of the
allocated resources on a given system.
"
2171,"Autonomous Task Dropping Mechanism to Achieve Robustness in
  Heterogeneous Computing Systems","  Robustness of a distributed computing system is defined as the ability to
maintain its performance in the presence of uncertain parameters. Uncertainty
is a key problem in heterogeneous (and even homogeneous) distributed computing
systems that perturbs system robustness. Notably, the performance of these
systems is perturbed by uncertainty in both task execution time and arrival.
Accordingly, our goal is to make the system robust against these uncertainties.
Considering task execution time as a random variable, we use probabilistic
analysis to develop an autonomous proactive task dropping mechanism to attain
our robustness goal. Specifically, we provide a mathematical model that
identifies the optimality of a task dropping decision, so that the system
robustness is maximized. Then, we leverage the mathematical model to develop a
task dropping heuristic that achieves the system robustness within a feasible
time complexity. Although the proposed model is generic and can be applied to
any distributed system, we concentrate on heterogeneous computing (HC) systems
that have a higher degree of exposure to uncertainty than homogeneous systems.
Experimental results demonstrate that the autonomous proactive dropping
mechanism can improve the system robustness by up to 20%.
"
2172,Profiling Resource Utilization of Bioinformatics Workflows,"  We present a software tool, the Container Profiler, that measures and records
the resource usage of any containerized task. Our tool profiles the CPU,
memory, disk, and network utilization of a containerized job by collecting
Linux operating system metrics at the virtual machine, container, and process
levels. The Container Profiler can produce utilization snapshots at multiple
time points, allowing for continuous monitoring of the resources consumed by a
container workflow.
  To investigate the utility of the Container Profiler we profiled the resource
utilization requirements of a multi-stage bioinformatics analytical workflow
(RNA sequencing using unique molecular identifiers). We examined the collected
profile metrics and confirmed that they were consistent with the expected CPU,
disk, network resource utilization patterns for the different stages of the
workflow. We also quantified the profiling overhead and found that this was
negligible.
  The Container Profiler is a useful tool that can be used to continuously
monitor the resource consumption of long and complex containerized workflows
that run locally or on the cloud. This can identify bottlenecks where more
resources are needed to improve performance.
"
2173,"A Comprehensive Study on Software Aging across Android Versions and
  Vendors","  This paper analyzes the phenomenon of software aging - namely, the gradual
performance degradation and resource exhaustion in the long run - in the
Android OS. The study intends to highlight if, and to what extent, devices from
different vendors, under various usage conditions and configurations, are
affected by software aging and which parts of the system are the main
contributors. The results demonstrate that software aging systematically
determines a gradual loss of responsiveness perceived by the user, and an
unjustified depletion of physical memory. The analysis reveals differences in
the aging trends due to the workload factors and to the type of running
applications, as well as differences due to vendors' customization. Moreover,
we analyze several system-level metrics to trace back the software aging
effects to their main causes. We show that bloated Java containers are a
significant contributor to software aging, and that it is feasible to mitigate
aging through a micro-rejuvenation solution at the container level.
"
2174,"Benchmarking and Performance Modelling of MapReduce Communication
  Pattern","  Understanding and predicting the performance of big data applications running
in the cloud or on-premises could help minimise the overall cost of operations
and provide opportunities in efforts to identify performance bottlenecks. The
complexity of the low-level internals of big data frameworks and the ubiquity
of application and workload configuration parameters makes it challenging and
expensive to come up with comprehensive performance modelling solutions.
  In this paper, instead of focusing on a wide range of configurable
parameters, we studied the low-level internals of the MapReduce communication
pattern and used a minimal set of performance drivers to develop a set of phase
level parametric models for approximating the execution time of a given
application on a given cluster. Models can be used to infer the performance of
unseen applications and approximate their performance when an arbitrary dataset
is used as input. Our approach is validated by running empirical experiments in
two setups. On average the error rate in both setups is plus or minus 10% from
the measured values.
"
2175,Tsunami propagation for singular topographies,"  We consider a tsunami wave equation with singular coefficients and prove that
it has a very weak solution. Moreover, we show the uniqueness results and
consistency theorem of the very weak solution with the classical one in some
appropriate sense. Numerical experiments are done for the families of
regularised problems in one- and two-dimensional cases. In particular, the
appearance of a substantial second wave is observed, travelling in the opposite
direction from the point/line of singularity. Its structure and strength are
analysed numerically. In addition, for the two-dimensional tsunami wave
equation, we develop GPU computing algorithms to reduce the computational cost.
"
2176,TeaMPI -- Replication-based Resilience without the (Performance) Pain,"  In an era where we can not afford to checkpoint frequently, replication is a
generic way forward to construct numerical simulations that can continue to run
even if hardware parts fail. Yet, replication often is not employed on larger
scales, as na\""ively mirroring a computation once effectively halves the
machine size, and as keeping replicated simulations consistent with each other
is not trivial. We demonstrate for the ExaHyPE engine -- a task-based solver
for hyperbolic equation systems -- that it is possible to realise resiliency
without major code changes on the user side, while we introduce a novel
algorithmic idea where replication reduces the time-to-solution. The redundant
CPU cycles are not burned ""for nothing"". Our work employs a weakly consistent
data model where replicas run independently yet inform each other through
heartbeat messages whether they are still up and running. Our key performance
idea is to let the tasks of the replicated simulations share some of their
outcomes, while we shuffle the actual task execution order per replica. This
way, replicated ranks can skip some local computations and automatically start
to synchronise with each other. Our experiments with a production-level seismic
wave-equation solver provide evidence that this novel concept has the potential
to make replication affordable for large-scale simulations in high-performance
computing.
"
2177,Benchmarking Graph Data Management and Processing Systems: A Survey,"  The development of scalable, representative, and widely adopted benchmarks
for graph data systems have been a question for which answers has been sought
for decades. We conduct an in-depth study of the existing literature on
benchmarks for graph data management and processing, covering 20 different
benchmarks developed during the last 15 years. We categorize the benchmarks
into three areas focusing on benchmarks for graph processing systems, graph
database benchmarks, and bigdata benchmarks with graph processing workloads.
This systematic approach allows us to identify multiple issues existing in this
area, including i) few benchmarks exist which can produce high workload
scenarios, ii) no significant work done on benchmarking graph stream processing
as well as graph based machine learning, iii) benchmarks tend to use
conventional metrics despite new meaningful metrics have been around for years,
iv) increasing number of big data benchmarks appear with graph processing
workloads. Following these observations, we conclude the survey by describing
key challenges for future research on graph data systems benchmarking.
"
2178,"A review of analytical performance modeling and its role in computer
  engineering and science","  This article is a review of analytical performance modeling for computer
systems. It discusses the motivation for this area of research, examines key
issues, introduces some ideas, illustrates how it is applied, and points out a
role that it can play in developing Computer Science.
"
2179,"Threshold-based rerouting and replication for resolving job-server
  affinity relations","  We consider a system with several job types and two parallel server pools.
Within the pools the servers are homogeneous, but across pools possibly not in
the sense that the service speed of a job may depend on its type as well as the
server pool. Immediately upon arrival, jobs are assigned to a server pool. This
could be based on (partial) knowledge of their type, but such knowledge might
not be available. Information about the job type can however be obtained while
the job is in service; as the service progresses, the likelihood that the
service speed of this job type is low increases, creating an incentive to
execute the job on different, possibly faster, server(s). Two policies are
considered: reroute the job to the other server pool, or replicate it there.
  We determine the effective load per server under both the rerouting and
replication policy for completely unknown as well as partly known job types. We
also examine the impact of these policies on the stability bound, and find that
the uncertainty in job types may significantly degrade the performance. For
(highly) unbalanced service speeds full replication achieves the largest
stability bound while for (nearly) balanced service speeds no replication
maximizes the stability bound. Finally, we discuss how the use of
threshold-based policies can help improve the expected latency for completely
or partly unknown job types.
"
2180,IoT-based Emergency Evacuation Systems,"  Fires, earthquakes, floods, hurricanes, overcrowding, or and even pandemic
viruses endanger human lives. Hence, designing infrastructures to handle
possible emergencies has become an ever-increasing need. The safe evacuation of
occupants from the building takes precedence when dealing with the necessary
mitigation and disaster risk management. This thesis deals with designing an
IoT system to provide safe and quick evacuation suggestions. The IoT-based
evacuation system provides optimal evacuation paths that can be continuously
updated based on run-time sensory data, so evacuation guidelines can be
adjusted according to visitors occupants that evolve over time. This thesis
makes the following main contributions: i) Addressing an up to date state of
the art class for IoT architectural styles and patterns; ii) Proposing a set of
self-adaptive IoT patterns and assessing their specific quality attributes
(fault-tolerance, energy consumption, and performance); iii) Designing an IoT
infrastructure and testing its performance in both real-time and design-time
applications; iv) Developing a network flow algorithm that facilitates
minimizing the time necessary to evacuate people from a scene of a disaster; v)
Modeling various social agents and their interactions during an emergency to
improve the IoT system accordingly; vi) Evaluating the system by using
empirical and real case studies.
"
2181,ProTuner: Tuning Programs with Monte Carlo Tree Search,"  We explore applying the Monte Carlo Tree Search (MCTS) algorithm in a
notoriously difficult task: tuning programs for high-performance deep learning
and image processing. We build our framework on top of Halide and show that
MCTS can outperform the state-of-the-art beam-search algorithm. Unlike beam
search, which is guided by greedy intermediate performance comparisons between
partial and less meaningful schedules, MCTS compares complete schedules and
looks ahead before making any intermediate scheduling decision. We further
explore modifications to the standard MCTS algorithm as well as combining real
execution time measurements with the cost model. Our results show that MCTS can
outperform beam search on a suite of 16 real benchmarks.
"
2182,"Age of Information in an Overtake-Free Network of Quasi-Reversible
  Queues","  We show how to calculate the Age of Information in an overtake-free network
of quasi-reversible queues, with exponential exogenous interarrivals of
multiple classes of update packets and exponential service times at all nodes.
Results are provided for any number of M/M/1 First-Come-First-Served (FCFS)
queues in tandem, and for a network with two classes of update packets,
entering through different queues in the network and exiting through the same
queue. The main takeaway is that in a network with different classes of update
packets, individual classes roughly preserve the ages they would achieve if
they were alone in the network, except when shared queues become saturated, in
which case the ages increase considerably. The results are extensible for other
quasi-reversible queues for which sojourn time distributions are known, such as
M/M/c FCFS queues and processor-sharing queues.
"
2183,"Cloud-scale VM Deflation for Running Interactive Applications On
  Transient Servers","  Transient computing has become popular in public cloud environments for
running delay-insensitive batch and data processing applications at low cost.
Since transient cloud servers can be revoked at any time by the cloud provider,
they are considered unsuitable for running interactive application such as web
services. In this paper, we present VM deflation as an alternative mechanism to
server preemption for reclaiming resources from transient cloud servers under
resource pressure. Using real traces from top-tier cloud providers, we show the
feasibility of using VM deflation as a resource reclamation mechanism for
interactive applications in public clouds. We show how current hypervisor
mechanisms can be used to implement VM deflation and present cluster deflation
policies for resource management of transient and on-demand cloud VMs.
Experimental evaluation of our deflation system on a Linux cluster shows that
microservice-based applications can be deflated by up to 50\% with negligible
performance overhead. Our cluster-level deflation policies allow overcommitment
levels as high as 50\%, with less than a 1\% decrease in application
throughput, and can enable cloud platforms to increase revenue by 30\%.
"
2184,Staffing for many-server systems facing non-standard arrival processes,"  Arrival processes to service systems often display (i) larger than
anticipated fluctuations, (ii) a time-varying rate, and (iii) temporal
correlation. Motivated by this, we introduce a specific non-homogeneous Poisson
process that incorporates these three features. The resulting arrival process
is fed into an infinite-server system, which is then used as a proxy for its
many-server counterpart. This leads to a staffing rule based on the square-root
staffing principle that acknowledges the three features. After a slight
rearrangement of servers over the time slots, we succeed to stabilize system
performance even under highly varying and strongly correlated conditions. We
fit the arrival stream model to real data from an emergency department and
demonstrate (by simulation) the performance of the novel staffing rule.
"
2185,"Detecting and Understanding Real-World Differential Performance Bugs in
  Machine Learning Libraries","  Programming errors that degrade the performance of systems are widespread,
yet there is little tool support for analyzing these bugs. We present a method
based on differential performance analysis---we find inputs for which the
performance varies widely, despite having the same size. To ensure that the
differences in the performance are robust (i.e. hold also for large inputs), we
compare the performance of not only single inputs, but of classes of inputs,
where each class has similar inputs parameterized by their size. Thus, each
class is represented by a performance function from the input size to
performance. Importantly, we also provide an explanation for why the
performance differs in a form that can be readily used to fix a performance
bug.
  The two main phases in our method are discovery with fuzzing and explanation
with decision tree classifiers, each of which is supported by clustering.
First, we propose an evolutionary fuzzing algorithm to generate inputs. For
this fuzzing task, the unique challenge is that we not only need the input
class with the worst performance, but rather a set of classes exhibiting
differential performance. We use clustering to merge similar input classes
which significantly improves the efficiency of our fuzzer. Second, we explain
the differential performance in terms of program inputs and internals. We adapt
discriminant learning approaches with clustering and decision trees to localize
suspicious code regions.
  We applied our techniques to a set of applications. On a set of
micro-benchmarks, we show that our approach outperforms state-of-the-art
fuzzers in finding inputs to characterize the differential performance. On a
set of case-studies, we discover and explain multiple performance bugs in
popular machine learning frameworks. Four of these bugs, reported first in this
paper, have since been fixed by the developers.
"
2186,"The Art of CPU-Pinning: Evaluating and Improving the Performance of
  Virtualization and Containerization Platforms","  Cloud providers offer a variety of execution platforms in form of bare-metal,
VM, and containers. However, due to the pros and cons of each execution
platform, choosing the appropriate platform for a specific cloud-based
application has become a challenge for solution architects. The possibility to
combine these platforms (e.g. deploying containers within VMs) offers new
capacities that makes the challenge even further complicated. However, there is
a little study in the literature on the pros and cons of deploying different
application types on various execution platforms. In particular, evaluation of
diverse hardware configurations and different CPU provisioning methods, such as
CPU pinning, have not been sufficiently studied in the literature. In this
work, the performance overhead of container, VM, and bare-metal execution
platforms are measured and analyzed for four categories of real-world
applications, namely video processing, parallel processing (MPI), web
processing, and No-SQL, respectively representing CPU intensive, parallel
processing, and two IO intensive processes. Our analyses reveal a set of
interesting and sometimes counterintuitive findings that can be used as best
practices by the solution architects to efficiently deploy cloud-based
applications. Here are some notable mentions: (A) Under specific circumstances,
containers can impose a higher overhead than VMs; (B) Containers on top of VMs
can mitigate the overhead of VMs for certain applications; (C) Containers with
a large number of cores impose a lower overhead than those with a few cores.
"
2187,MLOS: An Infrastructure for Automated Software Performance Engineering,"  Developing modern systems software is a complex task that combines business
logic programming and Software Performance Engineering (SPE). The later is an
experimental and labor-intensive activity focused on optimizing the system for
a given hardware, software, and workload (hw/sw/wl) context.
  Today's SPE is performed during build/release phases by specialized teams,
and cursed by: 1) lack of standardized and automated tools, 2) significant
repeated work as hw/sw/wl context changes, 3) fragility induced by a
""one-size-fit-all"" tuning (where improvements on one workload or component may
impact others). The net result: despite costly investments, system software is
often outside its optimal operating point - anecdotally leaving 30% to 40% of
performance on the table.
  The recent developments in Data Science (DS) hints at an opportunity:
combining DS tooling and methodologies with a new developer experience to
transform the practice of SPE. In this paper we present: MLOS, an ML-powered
infrastructure and methodology to democratize and automate Software Performance
Engineering. MLOS enables continuous, instance-level, robust, and trackable
systems optimization. MLOS is being developed and employed within Microsoft to
optimize SQL Server performance. Early results indicated that component-level
optimizations can lead to 20%-90% improvements when custom-tuning for a
specific hw/sw/wl, hinting at a significant opportunity. However, several
research challenges remain that will require community involvement. To this
end, we are in the process of open-sourcing the MLOS core infrastructure, and
we are engaging with academic institutions to create an educational program
around Software 2.0 and MLOS ideas.
"
2188,Efficient Replication for Straggler Mitigation in Distributed Computing,"  The potential of distributed computing to improve the performance of big data
processing engines is contingent on mitigation of several challenges. In
particular, by relying on multiple commodity servers, the performance of a
distributed computing engine is dictated by the slowest servers, known as
stragglers. Redundancy could mitigate stragglers by reducing the dependence of
the computing engine on every server. Nevertheless, redundancy could yet be a
burden to the system and aggravate stragglers. In this paper, we consider task
replication as the redundancy technique and study the optimum redundancy
planning to improve the performance of a master-worker distributed computing
system. We start with optimum assignment policy of a given set of redundant
tasks to a set of workers. Using the results from majorization theory, we show
that if the service time of workers is a stochastically (decreasing and) convex
random variable, a balanced assignment of non-overlapping batches of tasks
minimizes the average job compute time. With that results, we then study the
optimum level of redundancy from the perspective of average job compute time
and compute time predictability. We derive the efficient redundancy level as a
function of tasks' service time distribution. We observe that, the redundancy
level that minimizes average compute time is not necessarily the same as the
redundancy level that maximized compute time predictability. Finally, by
running experiments on Google cluster traces, we show that a careful planning
of redundancy according to the tasks' service time distribution can speed up
the computing job by an order of magnitude.
"
2189,"Multi-GPU Performance Optimization of a CFD Code using OpenACC on
  Different Platforms","  This paper investigates the multi-GPU performance of a 3D buoyancy driven
cavity solver using MPI and OpenACC directives on different platforms. The
paper shows that decomposing the total problem in different dimensions affects
the strong scaling performance significantly for the GPU. Without proper
performance optimizations, it is shown that 1D domain decomposition scales
poorly on multiple GPUs due to the noncontiguous memory access. The performance
using whatever decompositions can be benefited from a series of performance
optimizations in the paper. Since the buoyancy driven cavity code is
latency-bounded on the clusters examined, a series of optimizations both
agnostic and tailored to the platforms are designed to reduce the latency cost
and improve memory throughput between hosts and devices efficiently. First, the
parallel message packing/unpacking strategy developed for noncontiguous data
movement between hosts and devices improves the overall performance by about a
factor of 2. Second, transferring different data based on the stencil sizes for
different variables further reduces the communication overhead. These two
optimizations are general enough to be beneficial to stencil computations
having ghost changes on all of the clusters tested. Third, GPUDirect is used to
improve the communication on clusters which have the hardware and software
support for direct communication between GPUs without staging CPU's memory.
Finally, overlapping the communication and computations is shown to be not
efficient on multi-GPUs if only using MPI or MPI+OpenACC. Although we believe
our implementation has revealed enough overlap, the actual running does not
utilize the overlap well due to a lack of asynchronous progression.
"
2190,Unstable Throughput: When the Difficulty Algorithm Breaks,"  Difficulty algorithms are a fundamental component of Proof-of-Work
blockchains, aimed at maintaining stable block production times by dynamically
adjusting the network difficulty in response to the miners' constantly changing
computational power. Targeting stable block times is critical, as this ensures
consistent transaction throughput. Some blockchains need difficulty algorithms
that react quickly to severe hash rate fluctuations. However, without careful
design this could create vulnerabilities that incentivize miners to engage in
coin-hopping strategies which yield an unreliable system due to unstable
processing of transactions.
  We provide an empirical analysis of how Bitcoin Cash exhibits cyclicality in
block solve times as a consequence of a positive feedback loop in its
difficulty algorithm design. Additionally, we examine the extent to which
miners' behavior contributes towards this phenomenon over time. In response, we
mathematically derive a difficulty algorithm based on a negative exponential
filter that prohibits the formation of positive feedback loops and exhibits
additional desirable properties, such as history agnosticism. We compare the
described algorithm to that of Bitcoin Cash in a simulated mining environment
and verify that the former would eliminate the severe oscillations in block
solve times. Lastly, we outline how this model can more generally replace
difficulty algorithms in other Proof-of-Work blockchains.
"
2191,"Daydream: Accurately Estimating the Efficacy of Optimizations for DNN
  Training","  Modern deep neural network (DNN) training jobs use complex and heterogeneous
software/hardware stacks. The efficacy of software-level optimizations can vary
significantly when used in different deployment configurations. It is onerous
and error-prone for ML practitioners and system developers to implement each
optimization separately, and determine which ones will improve performance in
their own configurations. Unfortunately, existing profiling tools do not aim to
answer predictive questions such as ""How will optimization X affect the
performance of my model?"". We address this critical limitation, and proposes a
new profiling tool, Daydream, to help programmers efficiently explore the
efficacy of DNN optimizations. Daydream models DNN execution with a
fine-grained dependency graph based on low-level traces collected by CUPTI, and
predicts runtime by simulating execution based on the dependency graph.
Daydream maps the low-level traces using DNN domain-specific knowledge, and
introduces a set of graph-transformation primitives that can easily model a
wide variety of optimizations. We show that Daydream is able to model most
mainstream DNN optimization techniques, and accurately predict the efficacy of
optimizations that will result in significant performance improvements.
"
2192,"Joint performance analysis of ages of information in a multi-source
  pushout server","  Age of information (AoI) has been widely accepted as a measure quantifying
freshness of status information in real-time status update systems. In many of
such systems, multiple sources share a limited network resource and therefore
the AoIs defined for the individual sources should be correlated with each
other. However, there are not found any results studying the correlation of two
or more AoIs in a status update system with multiple sources. In this work, we
consider a multi-source system sharing a common service facility and provide a
framework to investigate joint performance of the multiple AoIs. We then apply
our framework to a simple pushout server with multiple sources and derive a
closed-form formula of the joint Laplace transform of the AoIs in the case with
independent M/G inputs. We further show some properties of the correlation
coefficient of AoIs in the two-source system.
"
2193,"High-level Modeling of Manufacturing Faults in Deep Neural Network
  Accelerators","  The advent of data-driven real-time applications requires the implementation
of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's
Tensor Processing Unit (TPU) is one such neural network accelerator that uses
systolic array-based matrix multiplication hardware for computation in its
crux. Manufacturing faults at any state element of the matrix multiplication
unit can cause unexpected errors in these inference networks. In this paper, we
propose a formal model of permanent faults and their propagation in a TPU using
the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed
using the probabilistic model checking technique to reason about the likelihood
of faulty outputs. The obtained quantitative results show that the
classification accuracy is sensitive to the type of permanent faults as well as
their location, bit position and the number of layers in the neural network.
The conclusions from our theoretical model have been validated using
experiments on a digit recognition-based DNN.
"
2194,"Toward a Better Understanding and Evaluation of Tree Structures on Flash
  SSDs","  Solid-state drives (SSDs) are extensively used to deploy persistent data
stores, as they provide low latency random access, high write throughput, high
data density, and low cost. Tree-based data structures are widely used to build
persistent data stores, and indeed they lie at the backbone of many of the data
management systems used in production and research today. In this paper, we
show that benchmarking a persistent tree-based data structure on an SSD is a
complex process, which may easily incur subtle pitfalls that can lead to an
inaccurate performance assessment. At a high-level, these pitfalls stem from
the interaction of complex software running on complex hardware. On one hand,
tree structures implement internal operations that have nontrivial effects on
performance. On the other hand, SSDs employ firmware logic to deal with the
idiosyncrasies of the underlying flash memory, which are well known to lead to
complex performance dynamics. We identify seven benchmarking pitfalls using
RocksDB and WiredTiger, two widespread implementations of an LSM-Tree and a
B+Tree, respectively. We show that such pitfalls can lead to incorrect
measurements of key performance indicators, hinder the reproducibility and the
representativeness of the results, and lead to suboptimal deployments in
production environments. We also provide guidelines on how to avoid these
pitfalls to obtain more reliable performance measurements, and to perform more
thorough and fair comparison among different design points.
"
2195,Scalability in Computing and Robotics,"  Efficient engineered systems require scalability. A scalable system has
increasing performance with increasing system size. In an ideal case, the
increase in performance (e.g., speedup) corresponds to the number of units that
are added to the system. However, if multiple units work on the same task, then
coordination among these units is required. This coordination can introduce
overheads with an impact on system performance. The coordination costs can lead
to sublinear improvement or even diminishing performance with increasing system
size. However, there are also systems that implement efficient coordination and
exploit collaboration of units to attain superlinear improvement. Modeling the
scalability dynamics is key to understanding efficient systems. Known laws of
scalability, such as Amdahl's law, Gustafson's law, and Gunther's Universal
Scalability Law, are minimalistic phenomenological models that explain a rich
variety of system behaviors through concise equations. While useful to gain
general insights, the phenomenological nature of these models may limit the
understanding of the underlying dynamics, as they are detached from first
principles that could explain coordination overheads among units. Through a
decentralized system approach, we propose a general model based on generic
interactions between units that is able to describe, as specific cases, any
general pattern of scalability included by previously reported laws. The
proposed general model of scalability is built on first principles, or at least
on a microscopic description of interaction between units, and therefore has
the potential to contribute to a better understanding of system behavior and
scalability. We show that this model can be applied to a diverse set of
systems, such as parallel supercomputers, robot swarms, or wireless sensor
networks, creating a unified view on interdisciplinary design for scalability.
"
2196,"Stochastic Automata Network for Performance Evaluation of Heterogeneous
  SoC Communication","  To meet ever increasing demand for performance of emerging System-on-Chip
(SoC) applications, designer employ techniques for concurrent communication
between components. Hence communication architecture becomes complex and major
performance bottleneck. An early performance evaluation of communication
architecture is the key to reduce design time, time-to-market and consequently
cost of the system. Moreover, it helps to optimize system performance by
selecting appropriate communication architecture. However, performance model of
concurrent communication is complex to describe and hard to solve. In this
paper, we propose methodology for performance evaluation of bus based
communication architectures, modeling for which is based on modular Stochastic
Automata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model
for each module of the SAN that emulates dynamic behavior of a Processing
Element (PE) of an SoC architecture. The proposed modeling approach provides an
early estimation of performance parameters viz. memory bandwidth, average queue
length at memory and average waiting time seen by a processing element; while
we provide parameters viz. number of processing elements, the mean computation
time of processing elements and the first and second moments of connection time
between processing elements and memories, as input to the model.
"
2197,"Product Forms for FCFS Queueing Models with Arbitrary Server-Job
  Compatibilities: An Overview","  In recent years a number of models involving different compatibilities
between jobs and servers in queueing systems, or between agents and resources
in matching systems, have been studied, and, under Markov assumptions and
appropriate stability conditions, the stationary distributions have been shown
to have product forms. We survey these results and show how, under an
appropriate detailed description of the state, many are corollaries of similar
results for the Order Independent Queue. We also discuss how to use the product
form results to determine distributions for steady-state response times.
"
2198,"Performance Analysis of Modified SRPT in Multiple-Processor Multitask
  Scheduling","  In this paper we study the multiple-processor multitask scheduling problem in
both deterministic and stochastic models. We consider and analyze Modified
Shortest Remaining Processing Time (M-SRPT) scheduling algorithm, a simple
modification of SRPT, which always schedules jobs according to SRPT whenever
possible, while processes tasks in an arbitrary order. The M-SRPT algorithm is
proved to achieve a competitive ratio of $\Theta(\log \alpha +\beta)$ for
minimizing response time, where $\alpha$ denotes the ratio between maximum job
workload and minimum job workload, $\beta$ represents the ratio between maximum
non-preemptive task workload and minimum job workload. In addition, the
competitive ratio achieved is shown to be optimal (up to a constant factor),
when there are constant number of machines. We further consider the problem
under Poisson arrival and general workload distribution (\ie, $M/GI/N$ system),
and show that M-SRPT achieves asymptotic optimal mean response time when the
traffic intensity $\rho$ approaches $1$, if job size distribution has finite
support. Beyond bounded job workload, the asymptotic optimality of M-SRPT also
holds for unbounded job size distributions with certain probabilistic
assumptions, for example, $M/M/N$ system with upper bounded task workload. An
byproduct of our analysis is a tight characterization of the heavy traffic
behavior of work-conserving algorithms in single-task job scheduling. We prove
that the average response time in $GI/GI/1$ scales with $1/(1-\rho)$, if the
job size distribution has finite support, which generalizes the growth rate in
[Lin, Wierman and Zwart, 2011] to general arrival processes and all
work-conserving algorithms.
"
2199,Ansor : Generating High-Performance Tensor Programs for Deep Learning,"  High-performance tensor programs are crucial to guarantee efficient execution
of deep neural networks. However, obtaining performant tensor programs for
different operators on various hardware platforms is notoriously challenging.
Currently, deep learning systems rely on vendor-provided kernel libraries or
various search strategies to get performant tensor programs. These approaches
either require significant engineering effort to develop platform-specific
optimization code or fall short of finding high-performance programs due to
restricted search space and ineffective exploration strategy.
  We present Ansor, a tensor program generation framework for deep learning
applications. Compared with existing search strategies, Ansor explores many
more optimization combinations by sampling programs from a hierarchical
representation of the search space. Ansor then fine-tunes the sampled programs
with evolutionary search and a learned cost model to identify the best
programs. Ansor can find high-performance programs that are outside the search
space of existing state-of-the-art approaches. In addition, Ansor utilizes a
task scheduler to simultaneously optimize multiple subgraphs in deep neural
networks. We show that Ansor improves the execution performance of deep neural
networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA
GPU by up to $3.8\times$, $2.6\times$, and $1.7\times$, respectively.
"
2200,"Heterogeneous Parallelization and Acceleration of Molecular Dynamics
  Simulations in GROMACS","  The introduction of accelerator devices such as graphics processing units
(GPUs) has had profound impact on molecular dynamics simulations and has
enabled order-of-magnitude performance advances using commodity hardware. To
fully reap these benefits, it has been necessary to reformulate some of the
most fundamental algorithms, including the Verlet list, pair searching and
cut-offs. Here, we present the heterogeneous parallelization and acceleration
design of molecular dynamics implemented in the GROMACS codebase over the last
decade. The setup involves a general cluster-based approach to pair lists and
non-bonded pair interactions that utilizes both GPUs and CPU SIMD acceleration
efficiently, including the ability to load-balance tasks between CPUs and GPUs.
The algorithm work efficiency is tuned for each type of hardware, and to use
accelerators more efficiently we introduce dual pair lists with rolling pruning
updates. Combined with new direct GPU-GPU communication as well as GPU
integration, this enables excellent performance from single GPU simulations
through strong scaling across multiple GPUs and efficient multi-node
parallelization.
"
2201,Guiding Optimizations with Meliora: A Deep Walk down Memory Lane,"  Performance models can be very useful for understanding the behavior of
applications and hence can help guide design and optimization decisions.
Unfortunately, performance modeling of nontrivial computations typically
requires significant expertise and human effort. Moreover, even when performed
by experts, it is necessarily limited in scope, accuracy, or both. However,
since models are not typically available, programmers, compilers or autotuners
cannot use them easily to guide optimizations and are limited to
heuristic-based methods that potentially take a lot of time to perform
unnecessary transformations. We believe that streamlining model generation and
making it scalable (both in terms of human effort and code size) would enable
dramatic improvements in compilation techniques, as well as manual optimization
and autotuning. To that end, we are building the Meliora code analysis
infrastructure for machine learning-based performance model generation of
arbitrary codes based on static analysis of intermediate language
representations. We demonstrate good accuracy in matching known codes and show
how Meliora can be used to optimize new codes though reusing optimization
knowledge, either manually or in conjunction with an autotuner. When
autotuning, Meliora eliminates or dramatically reduces the empirical search
space, while generally achieving competitive performance.
"
2202,Scalable Load Balancing in the Presence of Heterogeneous Servers,"  Heterogeneity is becoming increasingly ubiquitous in modern large-scale
computer systems. Developing good load balancing policies for systems whose
resources have varying speeds is crucial in achieving low response times.
Indeed, how best to dispatch jobs to servers is a classical and well-studied
problem in the queueing literature. Yet the bulk of existing work on
large-scale systems assumes homogeneous servers; unfortunately, policies that
perform well in the homogeneous setting can cause unacceptably poor
performance---or even instability---in heterogeneous systems.
  We adapt the ""power-of-d"" versions of both the Join-the-Idle-Queue and
Join-the-Shortest-Queue policies to design two corresponding families of
heterogeneity-aware dispatching policies, each of which is parameterized by a
pair of routing probabilities. Unlike their heterogeneity-unaware counterparts,
our policies use server speed information both when choosing which servers to
query and when probabilistically deciding where (among the queried servers) to
dispatch jobs. Both of our policy families are analytically tractable: our mean
response time and queue length distribution analyses are exact as the number of
servers approaches infinity, under standard assumptions. Furthermore, our
policy families achieve maximal stability and outperform well-known dispatching
rules---including heterogeneity-aware policies such as
Shortest-Expected-Delay---with respect to mean response time.
"
2203,Design And Develop Network Storage Virtualization By Using GNS3,"  Virtualization is an emerging and optimistic prospect in the IT industry. Its
impact has a footprint widely in digital infrastructure. Many innovativeness
sectors utilized the concept of virtualization to reduce the cost of
frameworks. In this paper, we have designed and developed storage
virtualization for physical functional solutions. It is an auspicious type of
virtualization that is accessible, secure, scalable, and manageable. In the
paper, we have proposed the pool storage method used the RAID-Z file system
with the ZFS model which provides the duplication of site approach, compression
blueprint, adequate backup methods, expansion in error-correcting techniques,
and tested procedure on the real-time network location. Therefore, this study
provides useful guidelines to design and develop optimized storage
virtualization.
"
2204,Queues with Small Advice,"  Motivated by recent work on scheduling with predicted job sizes, we consider
the performance of scheduling algorithms with minimal advice, namely a single
bit. Besides demonstrating the power of very limited advice, such schemes are
quite natural. In the prediction setting, one bit of advice can be used to
model a simple prediction as to whether a job is ""large"" or ""small""; that is,
whether a job is above or below a given threshold. Further, one-bit advice
schemes can correspond to mechanisms that tell whether to put a job at the
front or the back for the queue, a limitation which may be useful in many
implementation settings. Finally, queues with a single bit of advice have a
simple enough state that they can be analyzed in the limiting mean-field
analysis framework for the power of two choices. Our work follows in the path
of recent work by showing that even small amounts of even possibly inaccurate
information can greatly improve scheduling performance.
"
2205,"GPU-Accelerated Discontinuous Galerkin Methods: 30x Speedup on 345
  Billion Unknowns","  A discontinuous Galerkin method for the discretization of the compressible
Euler equations, the governing equations of inviscid fluid dynamics, on
Cartesian meshes is developed for use of Graphical Processing Units via OCCA, a
unified approach to performance portability on multi-threaded hardware
architectures. A 30x time-to-solution speedup over CPU-only implementations
using non-CUDA-Aware MPI communications is demonstrated up to 1,536 NVIDIA V100
GPUs and parallel strong scalability is shown up to 6,144 NVIDIA V100 GPUs for
a problem containing 345 billion unknowns. A comparison of CUDA-Aware MPI
communication to non-GPUDirect communication is performed demonstrating an
additional 24% speedup on eight nodes composed of 32 NVIDIA V100 GPUs.
"
2206,"Probabilistic Bounds on the End-to-End Delay of Service Function Chains
  using Deep MDN","  Ensuring the conformance of a service system's end-to-end delay to service
level agreement (SLA) constraints is a challenging task that requires
statistical measures beyond the average delay. In this paper, we study the
real-time prediction of the end-to-end delay distribution in systems with
composite services such as service function chains. In order to have a general
framework, we use queueing theory to model service systems, while also adopting
a statistical learning approach to avoid the limitations of queueing-theoretic
methods such as stationarity assumptions or other approximations that are often
used to make the analysis mathematically tractable. Specifically, we use deep
mixture density networks (MDN) to predict the end-to-end distribution of the
delay given the network's state. As a result, our method is sufficiently
general to be applied in different contexts and applications. Our evaluations
show a good match between the learned distributions and the simulations, which
suggest that the proposed method is a good candidate for providing
probabilistic bounds on the end-to-end delay of more complex systems where
simulations or theoretical methods are not applicable.
"
2207,"HPC AI500: The Methodology, Tools, Roofline Performance Models, and
  Metrics for Benchmarking HPC AI Systems","  The recent years witness a trend of applying large-scale distributed deep
learning in both business and scientific computing areas, whose goal is to
speed up the training time to achieve a state-of-the-art quality. The HPC
community feels a great interest in building the HPC AI systems that are
dedicated to running those workloads. The HPC AI benchmarks accelerate the
process. Unfortunately, benchmarking HPC AI systems at scale raises serious
challenges. None of previous HPC AI benchmarks achieve the goal of being
equivalent, relevant, representative, affordable, and repeatable. This paper
presents a comprehensive methodology, tools, Roofline performance models, and
innovative metrics for benchmarking, optimizing, and ranking HPC AI systems,
which we call HPC AI500 V2.0. We abstract the HPC AI system into nine
independent layers, and present explicit benchmarking rules and procedures to
assure equivalence of each layer, repeatability, and replicability. On the
basis of AIBench -- by far the most comprehensive AI benchmarks suite, we
present and build two HPC AI benchmarks from both business and scientific
computing: Image Classification, and Extreme Weather Analytics, achieving both
representativeness and affordability. To rank the performance and
energy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS
per watt, which impose a penalty on failing to achieve the target quality. We
propose using convolution and GEMM -- the two most intensively-used kernel
functions to measure the upper bound performance of the HPC AI systems, and
present HPC AI roofline models for guiding performance optimizations. The
evaluations show our methodology, benchmarks, performance models, and metrics
can measure, optimize, and rank the HPC AI systems in a scalable, simple, and
affordable way. HPC AI500 V2.0 are publicly available from
http://www.benchcouncil.org/benchhub/hpc-ai500-benchmark.
"
2208,"Benchmarking for Metaheuristic Black-Box Optimization: Perspectives and
  Open Challenges","  Research on new optimization algorithms is often funded based on the
motivation that such algorithms might improve the capabilities to deal with
real-world and industrially relevant optimization challenges. Besides a huge
variety of different evolutionary and metaheuristic optimization algorithms,
also a large number of test problems and benchmark suites have been developed
and used for comparative assessments of algorithms, in the context of global,
continuous, and black-box optimization. For many of the commonly used synthetic
benchmark problems or artificial fitness landscapes, there are however, no
methods available, to relate the resulting algorithm performance assessments to
technologically relevant real-world optimization problems, or vice versa. Also,
from a theoretical perspective, many of the commonly used benchmark problems
and approaches have little to no generalization value. Based on a mini-review
of publications with critical comments, advice, and new approaches, this
communication aims to give a constructive perspective on several open
challenges and prospective research directions related to systematic and
generalizable benchmarking for black-box optimization.
"
2209,"COCOA: Cold Start Aware Capacity Planning for Function-as-a-Service
  Platforms","  Function-as-a-Service (FaaS) is increasingly popular in the software industry
due to the implied cost-savings in event-driven workloads and its synergy with
DevOps. To size an on-premise FaaS platform, it is important to estimate the
required CPU and memory capacity to serve the expected loads. Given the
service-level agreements, it is however challenging to take the cold start
issue into account during the sizing process. We have investigated the
similarity of this problem with the hit rate improvement problem in TTL caches
and concluded that solutions for TTL cache, although potentially applicable,
lead to over-provisioning in FaaS. Thus, we propose a novel approach, COCOA, to
solve this issue. COCOA uses a queueing-based approach to assess the effect of
cold starts on FaaS response times. It also considers different memory
consumption values depending on whether the function is idle or in execution.
Using an event-driven FaaS simulator, FaasSim, we have developed, we show that
COCOA can reduce over-provisioning by over 70% in some workloads, while
satisfying the service-level agreements.
"
2210,Scalable Comparative Visualization of Ensembles of Call Graphs,"  Optimizing the performance of large-scale parallel codes is critical for
efficient utilization of computing resources. Code developers often explore
various execution parameters, such as hardware configurations, system software
choices, and application parameters, and are interested in detecting and
understanding bottlenecks in different executions. They often collect
hierarchical performance profiles represented as call graphs, which combine
performance metrics with their execution contexts. The crucial task of
exploring multiple call graphs together is tedious and challenging because of
the many structural differences in the execution contexts and significant
variability in the collected performance metrics (e.g., execution runtime). In
this paper, we present an enhanced version of CallFlow to support the
exploration of ensembles of call graphs using new types of visualizations,
analysis, graph operations, and features. We introduce ensemble-Sankey, a new
visual design that combines the strengths of resource-flow (Sankey) and
box-plot visualization techniques. Whereas the resource-flow visualization can
easily and intuitively describe the graphical nature of the call graph, the box
plots overlaid on the nodes of Sankey convey the performance variability within
the ensemble. Our interactive visual interface provides linked views to help
explore ensembles of call graphs, e.g., by facilitating the analysis of
structural differences, and identifying similar or distinct call graphs. We
demonstrate the effectiveness and usefulness of our design through case studies
on large-scale parallel codes.
"
2211,"A New Theoretical Framework of Pyramid Markov Processes for Blockchain
  Selfish Mining","  In this paper, we provide a new theoretical framework of pyramid Markov
processes to solve some open and fundamental problems of blockchain selfish
mining. To this end, we first describe a more general blockchain selfish mining
with both a two-block leading competitive criterion and a new economic
incentive, and establish a pyramid Markov process to express the dynamic
behavior of the selfish mining from both consensus protocol and economic
incentive. Then we show that the pyramid Markov process is stable and so is the
blockchain, and its stationary probability vector is matrix-geometric with an
explicitly representable rate matrix. Furthermore, we use the stationary
probability vector to be able to analyze the waste of computational resource
due to generating a lot of orphan (or stale) blocks. Nextly, we set up a
pyramid Markov reward process to investigate the long-run average profits of
the honest and dishonest mining pools, respectively. Specifically, we show that
the long-run average profits are multivariate linear such that we can measure
the improvement of mining efficiency of the dishonest mining pool comparing to
the honest mining pool. As a by-product, we build three approximative Markov
processes when the system states are described as the block-number difference
of two forked block branches. Also, by using their special cases with non
network latency, we can further provide some useful interpretation for both the
Markov chain (Figure 1) and the revenue analysis ((1) to (3)) of the seminal
work by Eyal and Sirer (2014). Finally, we use some numerical examples to
verify the correctness and computability of our theoretical results. We hope
that the methodology and results developed in this paper shed light on the
blockchain selfish mining such that a series of promising research can be
produced potentially.
"
2212,The prolonged service time at non-dedicated servers in a pooling system,"  In this paper, we investigate the effect of the prolonged service time at the
non-dedicated servers in a pooling system on the system performance. We
consider the two-server loss model with exponential interarrival and service
times. We show that if the ratio of the mean service time at the dedicated
server and the mean prolonged service time at the non-dedicated server exceeds
a certain threshold, pooling would become unfavourable. In particular, the
threshold is explicitly provided. Moreover, when the degree of the prolonged
service time is pre-specified, we show that the pooling system with prolonged
service time at non-dedicated servers is not preferred when the work load in
the system is greater than a certain threshold.
"
2213,A Machine Learning Pipeline Stage for Adaptive Frequency Adjustment,"  A machine learning (ML) design framework is proposed for adaptively adjusting
clock frequency based on propagation delay of individual instructions. A random
forest model is trained to classify propagation delays in real time, utilizing
current operation type, current operands, and computation history as ML
features. The trained model is implemented in Verilog as an additional pipeline
stage within a baseline processor. The modified system is experimentally tested
at the gate level in 45 nm CMOS technology, exhibiting a speedup of 70% and
energy reduction of 30% with coarse-grained ML classification. A speedup of 89%
is demonstrated with finer granularities with 15.5% reduction in energy
consumption.
"
2214,Cost-Efficient Storage for On-Demand Video Streaming on Cloud,"  Video stream is converted to several formats to support the user's device,
this conversion process is called video transcoding, which imposes high storage
and powerful resources. With emerging of cloud technology, video stream
companies adopted to process video on the cloud. Generally, many formats of the
same video are made (pre-transcoded) and streamed to the adequate user's
device. However, pre-transcoding demands huge storage space and incurs a
high-cost to the video stream companies. More importantly, the pre-transcoding
of video streams could be hierarchy carried out through different storage types
in the cloud. To minimize the storage cost, in this paper, we propose a method
to store video streams in the hierarchical storage of the cloud. Particularly,
we develop a method to decide which video stream should be pre-transcoded in
its suitable cloud storage to minimize the overall cost. Experimental
simulation and results show the effectiveness of our approach, specifically,
when the percentage of frequently accessed videos is high in repositories, the
proposed approach minimizes the overall cost by up to 40 percent.
"
2215,"Analytics of Longitudinal System Monitoring Data for Performance
  Prediction","  In recent years, several HPC facilities have started continuous monitoring of
their systems and jobs to collect performance-related data for understanding
performance and operational efficiency. Such data can be used to optimize the
performance of individual jobs and the overall system by creating data-driven
models that can predict the performance of pending jobs. In this paper, we
model the performance of representative control jobs using longitudinal
system-wide monitoring data to explore the causes of performance variability.
Using machine learning, we are able to predict the performance of unseen jobs
before they are executed based on the current system state. We analyze these
prediction models in great detail to identify the features that are dominant
predictors of performance. We demonstrate that such models can be
application-agnostic and can be used for predicting performance of applications
that are not included in training.
"
2216,Benchmarking in Optimization: Best Practice and Open Issues,"  This survey compiles ideas and recommendations from more than a dozen
researchers with different backgrounds and from different institutes around the
world. Promoting best practice in benchmarking is its main goal. The article
discusses eight essential topics in benchmarking: clearly stated goals,
well-specified problems, suitable algorithms, adequate performance measures,
thoughtful analysis, effective and efficient designs, comprehensible
presentations, and guaranteed reproducibility. The final goal is to provide
well-accepted guidelines (rules) that might be useful for authors and
reviewers. As benchmarking in optimization is an active and evolving field of
research this manuscript is meant to co-evolve over time by means of periodic
updates.
"
2217,"On the Efficiency of Decentralized File Storage for Personal Information
  Management Systems","  This paper presents an architecture, based on Distributed Ledger Technologies
(DLTs) and Decentralized File Storage (DFS) systems, to support the use of
Personal Information Management Systems (PIMS). DLT and DFS are used to manage
data sensed by mobile users equipped with devices with sensing capability. DLTs
guarantee the immutability, traceability and verifiability of references to
personal data, that are stored in DFS. In fact, the inclusion of data digests
in the DLT makes it possible to obtain an unalterable reference and a
tamper-proof log, while remaining compliant with the regulations on personal
data, i.e. GDPR. We provide an experimental evaluation on the feasibility of
the use of DFS. Three different scenarios have been studied: i) a proprietary
IPFS approach with a dedicated node interfacing with the data producers, ii) a
public IPFS service and iii) Sia Skynet. Results show that through proper
configuration of the system infrastructure, it is viable to build a
decentralized Personal Data Storage (PDS).
"
2218,"High-Performance Routing with Multipathing and Path Diversity in
  Ethernet and HPC Networks","  The recent line of research into topology design focuses on lowering network
diameter. Many low-diameter topologies such as Slim Fly or Jellyfish that
substantially reduce cost, power consumption, and latency have been proposed. A
key challenge in realizing the benefits of these topologies is routing. On one
hand, these networks provide shorter path lengths than established topologies
such as Clos or torus, leading to performance improvements. On the other hand,
the number of shortest paths between each pair of endpoints is much smaller
than in Clos, but there is a large number of non-minimal paths between router
pairs. This hampers or even makes it impossible to use established multipath
routing schemes such as ECMP. In this work, to facilitate high-performance
routing in modern networks, we analyze existing routing protocols and
architectures, focusing on how well they exploit the diversity of minimal and
non-minimal paths. We first develop a taxonomy of different forms of support
for multipathing and overall path diversity. Then, we analyze how existing
routing schemes support this diversity. Among others, we consider multipathing
with both shortest and non-shortest paths, support for disjoint paths, or
enabling adaptivity. To address the ongoing convergence of HPC and ""Big Data""
domains, we consider routing protocols developed for both HPC systems and for
data centers as well as general clusters. Thus, we cover architectures and
protocols based on Ethernet, InfiniBand, and other HPC networks such as
Myrinet. Our review will foster developing future high-performance multipathing
routing protocols in supercomputers and data centers.
"
2219,"Performance and energy consumption of HPC workloads on a cluster based
  on Arm ThunderX2 CPU","  In this paper, we analyze the performance and energy consumption of an
Arm-based high-performance computing (HPC) system developed within the European
project Mont-Blanc 3. This system, called Dibona, has been integrated by
ATOS/Bull, and it is powered by the latest Marvell's CPU, ThunderX2. This CPU
is the same one that powers the Astra supercomputer, the first Arm-based
supercomputer entering the Top500 in November 2018. We study from
micro-benchmarks up to large production codes. We include an interdisciplinary
evaluation of three scientific applications (a finite-element fluid dynamics
code, a smoothed particle hydrodynamics code, and a lattice Boltzmann code) and
the Graph 500 benchmark, focusing on parallel and energy efficiency as well as
studying their scalability up to thousands of Armv8 cores. For comparison, we
run the same tests on state-of-the-art x86 nodes included in Dibona and the
Tier-0 supercomputer MareNostrum4. Our experiments show that the ThunderX2 has
a 25% lower performance on average, mainly due to its small vector unit yet
somewhat compensated by its 30% wider links between the CPU and the main
memory. We found that the software ecosystem of the Armv8 architecture is
comparable to the one available for Intel. Our results also show that ThunderX2
delivers similar or better energy-to-solution and scalability, proving that
Arm-based chips are legitimate contenders in the market of next-generation HPC
systems.
"
2220,"Accurate Closed-Form Approximations to Channel Distributions of
  RIS-Aided Wireless Systems","  This paper proposes highly accurate closed-form approximations to channel
distributions of two different reconfigurable intelligent surface (RIS)-based
wireless system setups, namely, dual-hop RIS-aided (RIS-DH) scheme and
RIS-aided transmit (RIS-T) scheme. Differently from previous works, the
proposed approximations reveal to be very tight for arbitrary number $N$ of
reflecting metasurface's elements. Our findings are then applied to the
performance analysis of the considered systems, in which the outage
probability, bit error rate, and average channel capacity are derived. Results
show that the achievable diversity orders $G_d$ for RIS-DH and RIS-T schemes
are $N-1<G_d<N$ and $N$, respectively. Furthermore, it is revealed that both
schemes can not provide the multiplexing gain and only diversity gains are
achieved. For the RIS-DH scheme, the channels are similar to the keyhole
multiple-input multiple-output (MIMO) channels with only one degree of freedom,
while the RIS-T scheme is like the transmit diversity structure.
"
2221,"Self-healing Dilemmas in Distributed Systems: Fault-correction vs.
  Fault-tolerance","  Large-scale decentralized systems of autonomous agents interacting via
asynchronous communication often experience the following self-healing dilemma:
Fault-detection inherits network uncertainties making a faulty process
indistinguishable from a slow process. The implications can be dramatic:
Self-healing mechanisms become biased and cost-ineffective. In particular,
triggering an undesirable fault-correction results in new faults that could be
prevented with fault-tolerance instead. Nevertheless, fault-tolerance alone
without eventually correcting persistent faults makes systems underperforming
as well. Measuring, understanding and resolving such self-healing dilemmas is a
timely challenge and critical requirement given the rise of distributed
ledgers, edge computing, the Internet of Things in several application domains
of energy, transport and health. This paper introduces a novel and
general-purpose modeling of fault scenarios. They can accurately measure and
predict inconsistencies generated by fault-correction and fault-tolerance when
each node in a network can monitor the health status of another node, while
both can defect. In contrast to related work, no information about the
computational/application scenario, overlying algorithms or application data is
required. A rigorous experimental methodology is designed that evaluates 696
experimental settings of different fault scales, fault profiles and fault
detection thresholds, each with almost 9M measurements of inconsistencies in a
prototyped decentralized network of 3000 nodes. The prediction performance of
the modeled fault scenarios is validated in a challenging application scenario
of decentralized and dynamic in-network aggregation using real-world data from
a Smart Grid pilot project. Findings confirm the origin of inconsistencies at
design phase and provide new insights how to tune self-healing mechanisms at
design phase.
"
2222,Accuracy vs. Complexity for mmWave Ray-Tracing: A Full Stack Perspective,"  The millimeter wave (mmWave) band will provide multi-gigabits-per-second
connectivity in the radio access of future wireless systems. The high
propagation loss in this portion of the spectrum calls for the deployment of
large antenna arrays to compensate for the loss through high directional gain,
thus introducing a spatial dimension in the channel model to accurately
represent the performance of a mmWave network. In this perspective, ray-tracing
can characterize the channel in terms of Multi Path Components (MPCs) to
provide a highly accurate model, at the price of extreme computational
complexity (e.g., for processing detailed environment information about the
propagation), which limits the scalability of the simulations. In this paper,
we present possible simplifications to improve the trade-off between accuracy
and complexity in ray-tracing simulations at mmWaves by reducing the total
number of MPCs. The effect of such simplifications is evaluated from a
full-stack perspective through end-to-end simulations, testing different
configuration parameters, propagation scenarios, and higher-layer protocol
implementations. We then provide guidelines on the optimal degree of
simplification, for which it is possible to reduce the complexity of
simulations with a minimal reduction in accuracy for different deployment
scenarios.
"
2223,"Stability, memory, and messaging tradeoffs in heterogeneous service
  systems","  We consider a heterogeneous distributed service system, consisting of $n$
servers with unknown and possibly different processing rates. Jobs with unit
mean and independent processing times arrive as a renewal process of rate
$\lambda n$, with $0<\lambda<1$, to the system. Incoming jobs are immediately
dispatched to one of several queues associated with the $n$ servers. We assume
that the dispatching decisions are made by a central dispatcher endowed with a
finite memory, and with the ability to exchange messages with the servers.
  We study the fundamental resource requirements (memory bits and message
exchange rate) in order for a dispatching policy to be {\bf maximally stable},
i.e., stable whenever the processing rates are such that the arrival rate is
less than the total available processing rate. First, for the case of Poisson
arrivals and exponential service times, we present a policy that is maximally
stable while using a positive (but arbitrarily small) message rate, and
$\log_2(n)$ bits of memory. Second, we show that within a certain broad class
of policies, a dispatching policy that exchanges $o\big(n^2\big)$ messages per
unit of time, and with $o(\log(n))$ bits of memory, cannot be maximally stable.
Thus, as long as the message rate is not too excessive, a logarithmic memory is
necessary and sufficient for maximal stability.
"
2224,"Layer-Parallel Training with GPU Concurrency of Deep Residual Neural
  Networks via Nonlinear Multigrid","  A Multigrid Full Approximation Storage algorithm for solving Deep Residual
Networks is developed to enable neural network parallelized layer-wise training
and concurrent computational kernel execution on GPUs. This work demonstrates a
10.2x speedup over traditional layer-wise model parallelism techniques using
the same number of compute units.
"
2225,"Accelerating Geometric Multigrid Preconditioning with Half-Precision
  Arithmetic on GPUs","  With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs,
high-performance computing applications can benefit from lower precision at
appropriate spots to speed up the overall execution time. In this paper, we
investigate a mixed-precision geometric multigrid method to solve large sparse
systems of equations stemming from discretization of elliptic PDEs. While the
final solution is always computed with high-precision accuracy, an iterative
refinement approach with multigrid preconditioning in lower precision and
residuum scaling is employed. We compare the FP64 baseline for Poisson's
equation to purely FP16 multigrid preconditioning and to the employment of
FP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count
is almost not affected by using lower accuracy, the solver runtime is
considerably decreased due to the reduced memory transfer and a speedup of up
to 2.5x is gained for the overall solver. We investigate the performance of
selected kernels with the hierarchical Roofline model.
"
2226,"Lazy State Determination: More concurrency for contending linearizable
  transactions","  The concurrency control algorithms in transactional systems limits
concurrency to provide strong semantics, which leads to poor performance under
high contention. As a consequence, many transactional systems eschew strong
semantics to achieve acceptable performance. We show that by leveraging
semantic information associated with the transactional programs to increase
concurrency, it is possible to significantly improve performance while
maintaining linearizability. To this end, we introduce the lazy state
determination API to easily expose the semantics of application transactions to
the database, and propose new optimistic and pessimistic concurrency control
algorithms that leverage this information to safely increase concurrency in the
presence of contention. Our evaluation shows that our approach can achieve up
to 5x more throughput with 1.5c less latency than standard techniques in the
popular TPC-C benchmark.
"
2227,"Wireless Performance Evaluation of Building Layouts: Closed-Form
  Computation of Figures of Merit","  This paper presents a part of our ground-breaking work on evaluation of
buildings in terms of wireless friendliness in the building-design stage. The
main goal is to devise construction practices that provide for a good
performance of wireless networks deployed in buildings. In this paper, the
interference gain (IG) and power gain (PG) are defined as two figures of merit
(FoM) of the wireless performance of buildings. The FoMs bridge the gap between
building design and wireless communications industries. An approach to derive
exact closed-form equations for these FoMs is proposed for the first time. The
derived analytic expressions facilitate straightforward and more
computationally efficient numerical evaluation of the proposed FoMs as compared
to Monte Carlo simulations for well-known indoor propagation models. It is
shown that the derived closed-form expression can be readily employed to
evaluate the impact of building properties, such as the sizes and the aspect
ratios (ARs) of rooms, on the wireless performance. The proposed approach sheds
light to architects on evaluation and design of wireless-friendly building
layouts.
"
2228,AI Tax: The Hidden Cost of AI Data Center Applications,"  Artificial intelligence and machine learning are experiencing widespread
adoption in industry and academia. This has been driven by rapid advances in
the applications and accuracy of AI through increasingly complex algorithms and
models; this, in turn, has spurred research into specialized hardware AI
accelerators. Given the rapid pace of advances, it is easy to forget that they
are often developed and evaluated in a vacuum without considering the full
application environment. This paper emphasizes the need for a holistic,
end-to-end analysis of AI workloads and reveals the ""AI tax."" We deploy and
characterize Face Recognition in an edge data center. The application is an
AI-centric edge video analytics application built using popular open source
infrastructure and ML tools. Despite using state-of-the-art AI and ML
algorithms, the application relies heavily on pre-and post-processing code. As
AI-centric applications benefit from the acceleration promised by accelerators,
we find they impose stresses on the hardware and software infrastructure:
storage and network bandwidth become major bottlenecks with increasing AI
acceleration. By specializing for AI applications, we show that a purpose-built
edge data center can be designed for the stresses of accelerated AI at 15%
lower TCO than one derived from homogeneous servers and infrastructure.
"
2229,On the benchmarking of partitioned real-time systems,"  Avionic software is the subject of critical real time, determinism and safety
constraints. Software designers face several challenges, one of them being the
estimation of worst-case execution time (WCET) of applications, that dictates
the execution time of the system. A pessimistic WCET estimation can lead to low
execution performances of the system, while an over-optimistic estimation can
lead to deadline misses, breaking one the basic constraints of critical
real-time systems (RTS). Partitioned systems are one special category of real
time systems, employed by the avionic community to deploy avionic software. The
ARINC-653 standard is one common avionic standard that employs the concept of
partitions. This standard defines partitioned architectures where one partition
should never directly interfere with another one. Assessing WCET of general
purpose RTSs is achievable by the usage of one of the many published benchmark
or WCET estimation frameworks. Contrarily, partitioned RTSs are special cases,
in which common benchmark tools may not capture all the metrics. In this
document, we present SFPBench, a generic benchmark framework for the assessment
of performance metrics on partitioned RTSs. The general organization of the
framework and its applications are illustrated, as well as an use-case,
employing SFPBench on an industrial partitioned operating system (OS) executing
on a Commercial Off-The-shelf (COTS) processor.
"
2230,"Discrete-time Queueing Model of Age of Information with Multiple
  Information Sources","  Information freshness in IoT-based status update systems has recently been
studied through the Age of Information (AoI) and Peak AoI (PAoI) performance
metrics. In this paper, we study a discrete-time server arising in multi-source
IoT systems which accepts incoming information packets from multiple
information sources so as to be forwarded to a remote monitor for status update
purposes. Under the assumption of Bernoulli information packet arrivals and a
common geometric service time distribution across all the sources, we
numerically obtain the exact per-source distributions of AoI and PAoI in
matrix-geometric form for three different queueing disciplines: i)
Non-Preemptive Bufferless (NPB) ii) Preemptive Bufferless (PB) iii)
Non-Preemptive Single Buffer with Replacement (NPSBR). The proposed numerical
algorithm employs the theory of Discrete-Time Markov Chains (DTMC) of
Quasi-Birth-Death (QBD) type and is matrix analytical, i.e, the algorithm is
based on numerically stable and efficient vector-matrix operations.Numerical
examples are provided to validate the accuracy and effectiveness of the
proposed queueing model. We also present a numerical example on the optimum
choice of the Bernoulli parameters in a practical IoT system with two sources
with diverse AoI requirements.
"
2231,"The Multi-Source Preemptive M/PH/1/1 Queue with Packet Errors: Exact
  Distribution of the Age of Information and Its Peak","  Age of Information (AoI) and Peak AoI (PAoI) and their analytical models have
recently drawn substantial amount of attention in information theory and
wireless communications disciplines, in the context of qualitative assessment
of information freshness in status update systems. We take a queueing-theoretic
approach and study a probabilistically preemptive bufferless $M/PH/1/1$
queueing system with arrivals stemming from $N$ separate information sources,
with the aim of modeling a generic status update system. In this model, a new
information packet arrival from source $m$ is allowed to preempt a packet from
source $n$ in service, with a probability depending on $n$ and $m$. To make the
model even more general than the existing ones, for each of the information
sources, we assume a distinct PH-type service time distribution and a distinct
packet error probability. Subsequently, we obtain the exact distributions of
the AoI and PAoI for each of the information sources using matrix-analytical
algorithms and in particular the theory of Markov fluid queues and sample path
arguments. This is in contrast with existing methods that rely on Stochastic
Hybrid Systems (SHS) which obtain only the average values and in less general
settings. Numerical examples are provided to validate the proposed approach as
well as to give engineering insight on the impact of preemption probabilities
on certain AoI and PAoI performance figures.
"
2232,"Reinforcement Learning Assisted Load Test Generation for E-Commerce
  Applications","  Background: End-user satisfaction is not only dependent on the correct
functioning of the software systems but is also heavily dependent on how well
those functions are performed. Therefore, performance testing plays a critical
role in making sure that the system responsively performs the indented
functionality. Load test generation is a crucial activity in performance
testing. Existing approaches for load test generation require expertise in
performance modeling, or they are dependent on the system model or the source
code.
  Aim: This thesis aims to propose and evaluate a model-free learning-based
approach for load test generation, which doesn't require access to the system
models or source code.
  Method: In this thesis, we treated the problem of optimal load test
generation as a reinforcement learning (RL) problem. We proposed two RL-based
approaches using q-learning and deep q-network for load test generation. In
addition, we demonstrated the applicability of our tester agents on a
real-world software system. Finally, we conducted an experiment to compare the
efficiency of our proposed approaches to a random load test generation approach
and a baseline approach.
  Results: Results from the experiment show that the RL-based approaches
learned to generate effective workloads with smaller sizes and in fewer steps.
The proposed approaches led to higher efficiency than the random and baseline
approaches.
  Conclusion: Based on our findings, we conclude that RL-based agents can be
used for load test generation, and they act more efficiently than the random
and baseline approaches.
"
2233,"Detection and Performance Analysis for Non-Coherent DF Relay Networks
  with Optimized Generalized Differential Modulation","  This paper studies the detection and performance analysis problems for a
relay network with $N$ parallel decode-and-forward (DF) relays. Due to the
distributed nature of this network, it is practically very challenging to
fulfill the requirement of instantaneous channel state information for coherent
detection. To bypass this requirement, we consider the use of non-coherent DF
relaying based on a generalized differential modulation (GDM) scheme, in which
transmission power allocation over the $M$-ary phase shift keying symbols is
exploited when performing differential encoding. In this paper, a novel
detector at the destination of such a non-coherent DF relay network is
proposed. It is an accurate approximation of the state-of-the-art detector,
called the almost maximum likelihood detector (AMLD), but the detection
complexity is considerably reduced from $\mathcal{O}(M^2N)$ to
$\mathcal{O}(MN)$. By characterizing the dominant error terms, we derive an
accurate approximate symbol error rate (SER) expression. An optimized power
allocation scheme for GDM is further designed based on this SER expression. Our
simulation demonstrates that the proposed non-coherent scheme can perform close
to the coherent counterpart as the block length increases. Additionally, we
prove that the diversity order of both the proposed detector and the AMLD is
exactly $\lceil N/2 \rceil + 1$. Extensive simulation results further verify
the accuracy of our results in various scenarios.
"
2234,"Orpheus: A New Deep Learning Framework for Easy Deployment and
  Evaluation of Edge Inference","  Optimising deep learning inference across edge devices and optimisation
targets such as inference time, memory footprint and power consumption is a key
challenge due to the ubiquity of neural networks. Today, production deep
learning frameworks provide useful abstractions to aid machine learning
engineers and systems researchers. However, in exchange they can suffer from
compatibility challenges (especially on constrained platforms), inaccessible
code complexity, or design choices that otherwise limit research from a systems
perspective. This paper presents Orpheus, a new deep learning framework for
easy prototyping, deployment and evaluation of inference optimisations. Orpheus
features a small codebase, minimal dependencies, and a simple process for
integrating other third party systems. We present some preliminary evaluation
results.
"
2235,"Analytical Performance Modeling of NoCs under Priority Arbitration and
  Bursty Traffic","  Networks-on-Chip (NoCs) used in commercial many-core processors typically
incorporate priority arbitration. Moreover, they experience bursty traffic due
to application workloads. However, most state-of-the-art NoC analytical
performance analysis techniques assume fair arbitration and simple traffic
models. To address these limitations, we propose an analytical modeling
technique for priority-aware NoCs under bursty traffic. Experimental
evaluations with synthetic and bursty traffic show that the proposed approach
has less than 10% modeling error with respect to cycle-accurate NoC simulator.
"
2236,Monocular Real-Time Volumetric Performance Capture,"  We present the first approach to volumetric performance capture and
novel-view rendering at real-time speed from monocular video, eliminating the
need for expensive multi-view systems or cumbersome pre-acquisition of a
personalized template model. Our system reconstructs a fully textured 3D human
from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While
PIFu achieves high-resolution reconstruction in a memory-efficient manner, its
computationally expensive inference prevents us from deploying such a system
for real-time applications. To this end, we propose a novel hierarchical
surface localization algorithm and a direct rendering method without explicitly
extracting surface meshes. By culling unnecessary regions for evaluation in a
coarse-to-fine manner, we successfully accelerate the reconstruction by two
orders of magnitude from the baseline without compromising the quality.
Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that
effectively suppresses failure modes due to the rare occurrence of challenging
examples. We adaptively update the sampling probability of the training data
based on the current reconstruction accuracy, which effectively alleviates
reconstruction artifacts. Our experiments and evaluations demonstrate the
robustness of our system to various challenging angles, illuminations, poses,
and clothing styles. We also show that our approach compares favorably with the
state-of-the-art monocular performance capture. Our proposed approach removes
the need for multi-view studio settings and enables a consumer-accessible
solution for volumetric capture.
"
2237,"Implications of Dissemination Strategies on the Security of Distributed
  Ledgers","  This paper describes a simulation study on security attacks over Distributed
Ledger Technologies (DLTs). We specifically focus on attacks at the underlying
peer-to-peer layer of these systems, that is in charge of disseminating
messages containing data and transaction to be spread among all participants.
In particular, we consider the Sybil attack, according to which a malicious
node creates many Sybils that drop messages coming from a specific attacked
node, or even all messages from honest nodes. Our study shows that the
selection of the specific dissemination protocol, as well as the amount of
connections each peer has, have an influence on the resistance to this attack.
"
2238,"Delay and Price Differentiation in Cloud Computing: A Service Model,
  Supporting Architectures, and Performance","  Many cloud service providers (CSPs) provide on-demand service at a price with
a small delay. We propose a QoS-differentiated model where multiple SLAs
deliver both on-demand service for latency-critical users and delayed services
for delay-tolerant users at lower prices. Two architectures are considered to
fulfill SLAs. The first is based on priority queues. The second simply
separates servers into multiple modules, each for one SLA. As an ecosystem, we
show that the proposed framework is dominant-strategy incentive compatible.
Although the first architecture appears more prevalent in the literature, we
prove the superiority of the second architecture, under which we further
leverage queueing theory to determine the optimal SLA delays and prices.
Finally, the viability of the proposed framework is validated through numerical
comparison with the on-demand service and it exhibits a revenue improvement in
excess of 200%. Our results can help CSPs design optimal delay-differentiated
services and choose appropriate serving architectures.
"
2239,Traffic Optimization for TCP-based Massive Multiplayer Online Games,"  This paper studies the use of a traffic optimization technique named TCM
(Tunneling, Compressing and Multiplexing) to reduce the bandwidth of MMORPGs
(Massively Multiplayer Online Role-Playing Games), which employ TCP to provide
a soft real-time service. In order to optimize the traffic and to improve
bandwidth efficiency, TCM can be applied when the packets of a number of
players share the same link, which occurs in some scenarios, as e.g. the
traffic between proxies and servers of game-supporting infrastructures. First,
TCP/IP headers are compressed using standard algorithms that avoid sending
repeated fields; next, a number of packets are blended into a bigger one and
finally, they are sent using a tunnel. The expected compressed header size has
been obtained using traffic traces of a real game. Next, simulations using a
traffic model of a popular MMORPG have been performed in order to estimate the
expected bandwidth savings and the reduction in packets per second. The
obtained bandwidth saving is about 60 percent. Packets per second are also
significantly reduced. In addition, the added delays are shown to be small
enough so as not to impair layers' experienced quality.
"
2240,"The Effect of TCP Variants on the Coexistence of MMORPG and Best-Effort
  Traffic","  We study TCP flows coexistence between Massive Multiplayer Online Role
Playing Games (MMORPGs) and other TCP applications, by taking World of Warcraft
(WoW) and a file transfer application based on File Transfer Protocol (FTP) as
an example. Our focus is on the effects of the sender buffer size and FTP
cross-traffic on the queuing delay experienced by the (MMORPG) game traffic. A
network scenario corresponding to a real life situation in an ADSL access
network has been simulated by using NS2. Three TCP variants, namely TCP SACK,
TCP New Reno, and TCP Vegas, have been considered for cross-traffic. The
results show that TCP Vegas is able to maintain a constant rate while competing
with the game traffic, since it prevents packet loss and high queuing delays by
not increasing the sender window size. TCP SACK and TCP New Reno, on the other
hand, tend to continuously increase the sender window size, thus potentially
allowing higher packet loss and causing undesired delays for the game traffic.
In terms of buffer size, we have established that smaller buffers are better
for MMORPG applications, while larger buffers contribute to a higher overall
delay.
"
2241,Custom Tailored Suite of Random Forests for Prefetcher Adaptation,"  To close the gap between memory and processors, and in turn improve
performance, there has been an abundance of work in the area of
data/instruction prefetcher designs. Prefetchers are deployed in each level of
the memory hierarchy, but typically, each prefetcher gets designed without
comprehensively accounting for other prefetchers in the system. As a result,
these individual prefetcher designs do not always complement each other, and
that leads to low average performance gains and/or many negative outliers. In
this work, we propose SuitAP (Suite of random forests for Adaptation of
Prefetcher system configuration), which is a hardware prefetcher adapter that
uses a suite of random forests to determine at runtime which prefetcher should
be ON at each memory level, such that they complement each other. Compared to a
design with no prefetchers, using SuitAP we improve IPC by 46% on average
across traces generated from SPEC2017 suite with 12KB overhead. Moreover, we
also reduce negative outliers using SuitAP.
"
2242,A Survey on the Evolution of Stream Processing Systems,"  Stream processing has been an active research field for more than 20 years,
but it is now witnessing its prime time due to recent successful efforts by the
research community and numerous worldwide open-source communities. This survey
provides a comprehensive overview of fundamental aspects of stream processing
systems and their evolution in the functional areas of out-of-order data
management, state management, fault tolerance, high availability, load
management, elasticity, and reconfiguration. We review noteworthy past research
findings, outline the similarities and differences between early ('00-'10) and
modern ('11-'18) streaming systems, and discuss recent trends and open
problems.
"
2243,A Learned Performance Model for the Tensor Processing Unit,"  Accurate hardware performance models are critical to efficient code
generation. They can be used by compilers to make heuristic decisions, by
superoptimizers as an minimization objective, or by autotuners to find an
optimal configuration of a specific program. However, they are difficult to
develop because contemporary processors are complex, and the recent
proliferation of deep learning accelerators has increased the development
burden. We demonstrate a method of learning performance models from a corpus of
tensor computation graph programs for the Tensor Processing Unit (TPU). We
train a neural network over kernel-level sub-graphs from the corpus and find
that the learned model is competitive to a heavily-optimized analytical cost
model used in the production XLA compiler.
"
2244,"High performance on-demand de-identification of a petabyte-scale medical
  imaging data lake","  With the increase in Artificial Intelligence driven approaches, researchers
are requesting unprecedented volumes of medical imaging data which far exceed
the capacity of traditional on-premise client-server approaches for making the
data research analysis-ready. We are making available a flexible solution for
on-demand de-identification that combines the use of mature software
technologies with modern cloud-based distributed computing techniques to enable
faster turnaround in medical imaging research. The solution is part of a
broader platform that supports a secure high performance clinical data science
platform.
"
2245,Achievable Stability in Redundancy Systems,"  We consider a system with $N$ parallel servers where incoming jobs are
immediately replicated to, say, $d$ servers. Each of the $N$ servers has its
own queue and follows a FCFS discipline. As soon as the first job replica is
completed, the remaining replicas are abandoned. We investigate the achievable
stability region for a quite general workload model with different job types
and heterogeneous servers, reflecting job-server affinity relations which may
arise from data locality issues and soft compatibility constraints. Under the
assumption that job types are known beforehand we show for New-Better-than-Used
(NBU) distributed speed variations that no replication $(d=1)$ gives a strictly
larger stability region than replication $(d>1)$. Strikingly, this does not
depend on the underlying distribution of the intrinsic job sizes, but observing
the job types is essential for this statement to hold. In case of
non-observable job types we show that for New-Worse-than-Used (NWU) distributed
speed variations full replication ($d=N$) gives a larger stability region than
no replication $(d=1)$.
"
2246,"BSF: a parallel computation model for scalability estimation of
  iterative numerical algorithms on cluster computing systems","  This paper examines a new parallel computation model called bulk synchronous
farm (BSF) that focuses on estimating the scalability of compute-intensive
iterative algorithms aimed at cluster computing systems. In the BSF model, a
computer is a set of processor nodes connected by a network and organized
according to the mas-ter/slave paradigm. A cost metric of the BSF model is
presented. This cost metric requires the algorithm to be represented in the
form of operations on lists. This allows us to derive an equation that predicts
the scalability boundary of a parallel program: the maximum number of processor
nodes after which the speedup begins to de-crease. The paper includes several
examples of applying the BSF model to designing and analyzing parallel
nu-merical algorithms. The large-scale computational experiments conducted on a
cluster computing system confirm the adequacy of the analytical estimations
obtained using the BSF model.
"
2247,"Performance Analysis of Priority-Aware NoCs with Deflection Routing
  under Traffic Congestion","  Priority-aware networks-on-chip (NoCs) are used in industry to achieve
predictable latency under different workload conditions. These NoCs incorporate
deflection routing to minimize queuing resources within routers and achieve low
latency during low traffic load. However, deflected packets can exacerbate
congestion during high traffic load since they consume the NoC bandwidth.
State-of-the-art analytical models for priority-aware NoCs ignore deflected
traffic despite its significant latency impact during congestion. This paper
proposes a novel analytical approach to estimate end-to-end latency of
priority-aware NoCs with deflection routing under bursty and heavy traffic
scenarios. Experimental evaluations show that the proposed technique
outperforms alternative approaches and estimates the average latency for real
applications with less than 8% error compared to cycle-accurate simulations.
"
2248,PROFIT: A Novel Training Method for sub-4-bit MobileNet Models,"  4-bit and lower precision mobile models are required due to the
ever-increasing demand for better energy efficiency in mobile devices. In this
work, we report that the activation instability induced by weight quantization
(AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To
alleviate the AIWQ problem, we propose a novel training method called
PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze
layers whose weights are affected by the instability problem stronger than the
other layers. We also propose a differentiable and unified quantization method
(DuQ) and a negative padding idea to support asymmetric activation functions
such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1,
v2, and v3 on ImageNet and report that 4-bit quantization offers comparable
(within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the
ablation study of the 3-bit quantization of MobileNet-v3, our proposed method
outperforms the state-of-the-art method by a large margin, 12.86 % of top-1
accuracy.
"
2249,"Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation
  in Autonomous Driving","  Semantic segmentation is important for many real-world systems, e.g.,
autonomous vehicles, which predict the class of each pixel. Recently, deep
networks achieved significant progress w.r.t. the mean Intersection-over Union
(mIoU) with the cross-entropy loss. However, the cross-entropy loss can
essentially ignore the difference of severity for an autonomous car with
different wrong prediction mistakes. For example, predicting the car to the
road is much more servery than recognize it as the bus. Targeting for this
difficulty, we develop a Wasserstein training framework to explore the
inter-class correlation by defining its ground metric as misclassification
severity. The ground metric of Wasserstein distance can be pre-defined
following the experience on a specific task. From the optimization perspective,
we further propose to set the ground metric as an increasing function of the
pre-defined ground metric. Furthermore, an adaptively learning scheme of the
ground matrix is proposed to utilize the high-fidelity CARLA simulator.
Specifically, we follow a reinforcement alternative learning scheme. The
experiments on both CamVid and Cityscapes datasets evidenced the effectiveness
of our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be
adapted following a plug-in manner. We achieve significant improvements on the
predefined important classes, and much longer continuous playtime in our
simulator.
"
2250,"Study on State-of-the-art Cloud Services Integration Capabilities with
  Autonomous Ground Vehicles","  Computing and intelligence are substantial requirements for the accurate
performance of autonomous ground vehicles (AGVs). In this context, the use of
cloud services in addition to onboard computers enhances computing and
intelligence capabilities of AGVs. In addition, the vast amount of data
processed in a cloud system contributes to overall performance and capabilities
of the onboard system. This research study entails a qualitative analysis to
gather insights on the applicability of the leading cloud service providers in
AGV operations. These services include Google Cloud, Microsoft Azure, Amazon
AWS, and IBM Cloud. The study begins with a brief review of AGV technical
requirements that are necessary to determine the rationale for identifying the
most suitable cloud service. The qualitative analysis studies and addresses the
applicability of the cloud service over the proposed generalized AGV's
architecture integration, performance, and manageability. Our findings conclude
that a generalized AGV architecture can be supported by state-of-the-art cloud
service, but there should be a clear line of separation between the primary and
secondary computing needs. Moreover, our results show significant lags while
using cloud services and preventing their use in real-time AGV operation.
"
2251,FLCD: A Flexible Low Complexity Design of Coded Distributed Computing,"  We propose a flexible low complexity design (FLCD) of coded distributed
computing (CDC) with empirical evaluation on Amazon Elastic Compute Cloud
(Amazon EC2). CDC can expedite MapReduce like computation by trading increased
map computations to reduce communication load and shuffle time. A main novelty
of FLCD is to utilize the design freedom in defining map and reduce functions
to develop asymptotic homogeneous systems to support varying intermediate
values (IV) sizes under a general MapReduce framework. Compared to existing
designs with constant IV sizes, FLCD offers greater flexibility in adapting to
network parameters and significantly reduces the implementation complexity by
requiring fewer input files and shuffle groups. The FLCD scheme is the first
proposed low-complexity CDC design that can operate on a network with an
arbitrary number of nodes and computation load. We perform empirical
evaluations of the FLCD by executing the TeraSort algorithm on an Amazon EC2
cluster. This is the first time that theoretical predictions of the CDC shuffle
time are validated by empirical evaluations. The evaluations demonstrate a 2.0
to 4.24x speedup compared to conventional uncoded MapReduce, a 12% to 52%
reduction in total time, and a wider range of operating network parameters
compared to existing CDC schemes.
"
2252,"Learnability and Robustness of Shallow Neural Networks Learned With a
  Performance-Driven BP and a Variant PSO For Edge Decision-Making","  In many cases, the computing resources are limited without the benefit from
GPU, especially in the edge devices of IoT enabled systems. It may not be easy
to implement complex AI models in edge devices. The Universal Approximation
Theorem states that a shallow neural network (SNN) can represent any nonlinear
function. However, how fat is an SNN enough to solve a nonlinear
decision-making problem in edge devices? In this paper, we focus on the
learnability and robustness of SNNs, obtained by a greedy tight force heuristic
algorithm (performance driven BP) and a loose force meta-heuristic algorithm (a
variant of PSO). Two groups of experiments are conducted to examine the
learnability and the robustness of SNNs with Sigmoid activation,
learned/optimised by KPI-PDBPs and KPI-VPSOs, where, KPIs (key performance
indicators: error (ERR), accuracy (ACC) and $F_1$ score) are the objectives,
driving the searching process. An incremental approach is applied to examine
the impact of hidden neuron numbers on the performance of SNNs,
learned/optimised by KPI-PDBPs and KPI-VPSOs. From the engineering prospective,
all sensors are well justified for a specific task. Hence, all sensor readings
should be strongly correlated to the target. Therefore, the structure of an SNN
should depend on the dimensions of a problem space. The experimental results
show that the number of hidden neurons up to the dimension number of a problem
space is enough; the learnability of SNNs, produced by KPI-PDBP, is better than
that of SNNs, optimized by KPI-VPSO, regarding the performance and learning
time on the training data sets; the robustness of SNNs learned by KPI-PDBPs and
KPI-VPSOs depends on the data sets; and comparing with other classic machine
learning models, ACC-PDBPs win for almost all tested data sets.
"
2253,"Consideration for effectively handling parallel workloads on public
  cloud system","  We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud
service to clarify how to build cost-effective hybrid storage systems. A hybrid
storage system consists of fast but low-capacity tier (first tier) and slow but
high-capacity tier (second tier). And, it typically consists of either SSDs and
HDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier
should be assigned only if a workload includes large number of IO accesses for
a whole day, 2) the regions that include a large number of IO accesses should
be dynamically chosen and moved from second tier to first tier for a short
interval, and 3) if a cache hit ratio is regularly low, use of the cache for
the workload should be cancelled, and the whole workload region should be
assigned to the region for first tier. These workloads already have been
released from the SNIA web site.
"
2254,Toward an End-to-End Auto-tuning Framework in HPC PowerStack,"  Efficiently utilizing procured power and optimizing performance of scientific
applications under power and energy constraints are challenging. The HPC
PowerStack defines a software stack to manage power and energy of
high-performance computing systems and standardizes the interfaces between
different components of the stack. This survey paper presents the findings of a
working group focused on the end-to-end tuning of the PowerStack. First, we
provide a background on the PowerStack layer-specific tuning efforts in terms
of their high-level objectives, the constraints and optimization goals,
layer-specific telemetry, and control parameters, and we list the existing
software solutions that address those challenges. Second, we propose the
PowerStack end-to-end auto-tuning framework, identify the opportunities in
co-tuning different layers in the PowerStack, and present specific use cases
and solutions. Third, we discuss the research opportunities and challenges for
collective auto-tuning of two or more management layers (or domains) in the
PowerStack. This paper takes the first steps in identifying and aggregating the
important R&D challenges in streamlining the optimization efforts across the
layers of the PowerStack.
"
2255,Erlang Redux: An Ansatz Method for Solving the M/M/m Queue,"  This exposition presents a novel approach to solving an M/M/m queue for the
waiting time and the residence time. The motivation comes from an algebraic
solution for the residence time of the M/M/1 queue. The key idea is the
introduction of an ansatz transformation, defined in terms of the Erlang B
function, that avoids the more opaque derivation based on applied probability
theory. The only prerequisite is an elementary knowledge of the Poisson
distribution, which is already necessary for understanding the M/M/1 queue. The
approach described here supersedes our earlier approximate morphing
transformation.
"
2256,"In-situ Workflow Auto-tuning via Combining Performance Models of
  Component Applications","  In-situ parallel workflows couple multiple component applications, such as
simulation and analysis, via streaming data transfer. in order to avoid data
exchange via shared file systems. Such workflows are challenging to configure
for optimal performance due to the large space of possible configurations.
Expert experience is rarely sufficient to identify optimal configurations, and
existing empirical auto-tuning approaches are inefficient due to the high cost
of obtaining training data for machine learning models. It is also infeasible
to optimize individual components independently, due to component interactions.
We propose here a new auto-tuning method, Component-based Ensemble Active
Learning (CEAL), that combines machine learning techniques with knowledge of
in-situ workflow structure to enable automated workflow configuration with a
limited number of performance measurements.
"
2257,Load Balancing Under Strict Compatibility Constraints,"  We study large-scale systems operating under the JSQ$(d)$ policy in the
presence of stringent task-server compatibility constraints. Consider a system
with $N$ identical single-server queues and $M(N)$ task types, where each
server is able to process only a small subset of possible task types. Each
arriving task selects $d\geq 2$ random servers compatible to its type, and
joins the shortest queue among them. The compatibility constraint is naturally
captured by a fixed bipartite graph $G_N$ between the servers and the task
types. When $G_N$ is complete bipartite, the meanfield approximation is proven
to be accurate. However, such dense compatibility graphs are infeasible due to
their overwhelming implementation cost and prohibitive storage capacity
requirement at the servers. Our goal in this paper is to characterize the class
of sparse compatibility graphs for which the meanfield approximation remains
valid.
  To achieve this, first, we introduce a novel graph expansion-based notion,
called proportional sparsity, and establish that systems with proportionally
sparse compatibility graphs match the performance of a fully flexible system,
asymptotically in the large-system limit. Furthermore, for any $c(N)$
satisfying $$\frac{Nc(N)}{M(N)\ln(N)}\to \infty\quad \text{and}\quad c(N)\to
\infty,$$ as $N\to\infty$, we show that proportionally sparse random
compatibility graphs can be designed, so that the degree of each server is at
most $c(N)$. This reduces the server-degree almost by a factor $N/\ln(N)$,
compared to the complete bipartite compatibility graph, while maintaining the
same asymptotic performance. Extensive simulation experiments are conducted to
corroborate the theoretical results.
"
2258,"A Microservices Architecture for Distributed Complex Event Processing in
  Smart Cities","  A considerable volume of data is collected from sensors today and needs to be
processed in real time. Complex Event Processing (CEP) is one of the most
important techniques developed for this purpose. In CEP, each new sensor
measurement is considered an event and new event types can be defined based on
other events occurrence. There exists several open-source CEP implementations
currently available, but all of them use orchestration to distribute event
processing. This kind of architectural organization may harm system resilience,
since it relies on a central core (i.e. the orchestrator). Any failures in the
core might impact the whole system. Moreover, the core can become a bottleneck
on system performance. In this work, a choreography-based microservices
architecture is proposed for distributed CEP, in order to benefit from the low
coupling and greater horizontal scalability this kind of architecture provides.
"
2259,"Benchmarking network fabrics for data distributed training of deep
  neural networks","  Artificial Intelligence/Machine Learning applications require the training of
complex models on large amounts of labelled data. The large computational
requirements for training deep models have necessitated the development of new
methods for faster training. One such approach is the data parallel approach,
where the training data is distributed across multiple compute nodes. This
approach is simple to implement and supported by most of the commonly used
machine learning frameworks. The data parallel approach leverages MPI for
communicating gradients across all nodes. In this paper, we examine the effects
of using different physical hardware interconnects and network-related software
primitives for enabling data distributed deep learning. We compare the effect
of using GPUDirect and NCCL on Ethernet and OmniPath fabrics. Our results show
that using Ethernet-based networking in shared HPC systems does not have a
significant effect on the training times for commonly used deep neural network
architectures or traditional HPC applications such as Computational Fluid
Dynamics.
"
2260,"Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear
  Algebra Computations","  GPU accelerators have become an important backbone for scientific high
performance computing, and the performance advances obtained from adopting new
GPU hardware are significant. In this paper we take a first look at NVIDIA's
newest server line GPU, the A100 architecture part of the Ampere generation.
Specifically, we assess its performance for sparse linear algebra operations
that form the backbone of many scientific applications and assess the
performance improvements over its predecessor.
"
2261,"FIRM: An Intelligent Fine-Grained Resource Management Framework for
  SLO-Oriented Microservices","  Modern user-facing latency-sensitive web services include numerous
distributed, intercommunicating microservices that promise to simplify software
development and operation. However, multiplexing of compute resources across
microservices is still challenging in production because contention for shared
resources can cause latency spikes that violate the service-level objectives
(SLOs) of user requests. This paper presents FIRM, an intelligent fine-grained
resource management framework for predictable sharing of resources across
microservices to drive up overall utilization. FIRM leverages online telemetry
data and machine-learning methods to adaptively (a) detect/localize
microservices that cause SLO violations, (b) identify low-level resources in
contention, and (c) take actions to mitigate SLO violations via dynamic
reprovisioning. Experiments across four microservice benchmarks demonstrate
that FIRM reduces SLO violations by up to 16x while reducing the overall
requested CPU limit by up to 62%. Moreover, FIRM improves performance
predictability by reducing tail latencies by up to 11x.
"
2262,Optimal Load Balancing in Bipartite Graphs,"  Applications in cloud platforms motivate the study of efficient load
balancing under job-server constraints and server heterogeneity. In this paper,
we study load balancing on a bipartite graph where left nodes correspond to job
types and right nodes correspond to servers, with each edge indicating that a
job type can be served by a server. Thus edges represent locality constraints,
i.e., each job can only be served at servers which contained certain data
and/or machine learning (ML) models. Servers in this system can have
heterogeneous service rates. In this setting, we investigate the performance of
two policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and
Join-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of
Join-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor
of the fastest servers. Under a ""well-connected"" graph condition, we show that
JFSQ and JFIQ are asymptotically optimal in the mean response time when the
number of servers goes to infinity. In addition to asymptotic optimality, we
also obtain upper bounds on the mean response time for finite-size systems. We
further show that the well-connectedness condition can be satisfied by a random
bipartite graph construction with relatively sparse connectivity.
"
2263,"High-Performance Simultaneous Multiprocessing for Heterogeneous
  System-on-Chip","  This paper presents a methodology for simultaneous heterogeneous computing,
named ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a
preprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes
the tasks optimally among all the resources and all compute units run
asynchronously, which allows for improved performance for irregular workloads.
ENEAC achieves up to 17\% performance improvement \ignore{and 14\% energy usage
reduction,} when using all platform resources compared to just using the FPGA
accelerators and up to 865\% performance increase \ignore{and up to 89\% energy
usage decrease} when using just the CPU. The workflow uses existing commercial
tools and C/C++ as a single programming language for both accelerator design
and CPU programming for improved productivity and ease of verification.
"
2264,An In-Depth Analysis of the Slingshot Interconnect,"  The interconnect is one of the most critical components in large scale
computing systems, and its impact on the performance of applications is going
to increase with the system size. In this paper, we will describe Slingshot, an
interconnection network for large scale computing systems. Slingshot is based
on high-radix switches, which allow building exascale and hyperscale
datacenters networks with at most three switch-to-switch hops. Moreover,
Slingshot provides efficient adaptive routing and congestion control
algorithms, and highly tunable traffic classes. Slingshot uses an optimized
Ethernet protocol, which allows it to be interoperable with standard Ethernet
devices while providing high performance to HPC applications. We analyze the
extent to which Slingshot provides these features, evaluating it on
microbenchmarks and on several applications from the datacenter and AI worlds,
as well as on HPC applications. We find that applications running on Slingshot
are less affected by congestion compared to previous generation networks.
"
2265,"Accuracy and Performance Comparison of Video Action Recognition
  Approaches","  Over the past few years, there has been significant interest in video action
recognition systems and models. However, direct comparison of accuracy and
computational performance results remain clouded by differing training
environments, hardware specifications, hyperparameters, pipelines, and
inference methods. This article provides a direct comparison between fourteen
off-the-shelf and state-of-the-art models by ensuring consistency in these
training characteristics in order to provide readers with a meaningful
comparison across different types of video action recognition algorithms.
Accuracy of the models is evaluated using standard Top-1 and Top-5 accuracy
metrics in addition to a proposed new accuracy metric. Additionally, we compare
computational performance of distributed training from two to sixty-four GPUs
on a state-of-the-art HPC system.
"
2266,"Reinforcement Learning-based Admission Control in Delay-sensitive
  Service Systems","  Ensuring quality of service (QoS) guarantees in service systems is a
challenging task, particularly when the system is composed of more fine-grained
services, such as service function chains. An important QoS metric in service
systems is the end-to-end delay, which becomes even more important in
delay-sensitive applications, where the jobs must be completed within a time
deadline. Admission control is one way of providing end-to-end delay guarantee,
where the controller accepts a job only if it has a high probability of meeting
the deadline. In this paper, we propose a reinforcement learning-based
admission controller that guarantees a probabilistic upper-bound on the
end-to-end delay of the service system, while minimizes the probability of
unnecessary rejections. Our controller only uses the queue length information
of the network and requires no knowledge about the network topology or system
parameters. Since long-term performance metrics are of great importance in
service systems, we take an average-reward reinforcement learning approach,
which is well suited to infinite horizon problems. Our evaluations verify that
the proposed RL-based admission controller is capable of providing
probabilistic bounds on the end-to-end delay of the network, without using
system model information.
"
2267,Tearing Down the Memory Wall,"  We present a vision for the Erudite architecture that redefines the compute
and memory abstractions such that memory bandwidth and capacity become
first-class citizens along with compute throughput. In this architecture, we
envision coupling a high-density, massively parallel memory technology like
Flash with programmable near-data accelerators, like the streaming
multiprocessors in modern GPUs. Each accelerator has a local pool of
storage-class memory that it can access at high throughput by initiating very
large numbers of overlapping requests that help to tolerate long access
latency. The accelerators can also communicate with each other and remote
memory through a high-throughput low-latency interconnect. As a result, systems
based on the Erudite architecture scale compute and memory bandwidth at the
same rate, tearing down the notorious memory wall that has plagued computer
architecture for generations. In this paper, we present the motivation,
rationale, design, benefit, and research challenges for Erudite.
"
2268,Optimized routines for event generators in QED-PIC codes,"  In recent years, the prospects of performing fundamental and applied studies
at the next-generation high-intensity laser facilities have greatly stimulated
the interest in performing large-scale simulations of laser interaction with
matter with the account for quantum electrodynamics (QED) processes such as
emission of high energy photons and decay of such photons into
electron-positron pairs. These processes can be modeled via probabilistic
routines that include frequent computation of synchrotron functions and can
constitute significant computational demands within accordingly extended
Particle-in-Cell (QED-PIC) algorithms. In this regard, the optimization of
these routines is of great interest. In this paper, we propose and describe two
modifications. First, we derive a more accurate upper-bound estimate for the
rate of QED events and use it to arrange local sub-stepping of the global time
step in a significantly more efficient way than done previously. Second, we
present a new high-performance implementation of synchrotron functions. Our
optimizations made it possible to speed up the computations by a factor of up
to 13.7 depending on the problem. Our implementation is integrated into the
PICADOR and Hi-Chi codes, the latter of which is distributed publicly
(https://github.com/hi-chi/pyHiChi).
"
2269,"High-Performance Parallel Graph Coloring with Strong Guarantees on Work,
  Depth, and Quality","  We develop the first parallel graph coloring heuristics with strong
theoretical guarantees on work and depth and coloring quality. The key idea is
to design a relaxation of the vertex degeneracy order, a well-known graph
theory concept, and to color vertices in the order dictated by this relaxation.
This introduces a tunable amount of parallelism into the degeneracy ordering
that is otherwise hard to parallelize. This simple idea enables significant
benefits in several key aspects of graph coloring. For example, one of our
algorithms ensures polylogarithmic depth and a bound on the number of used
colors that is superior to all other parallelizable schemes, while maintaining
work-efficiency. In addition to provable guarantees, the developed algorithms
have competitive run-times for several real-world graphs, while almost always
providing superior coloring quality. Our degeneracy ordering relaxation is of
separate interest for algorithms outside the context of coloring.
"
2270,"8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other
  Tricks","  Performance optimization can be a daunting task especially as the hardware
architecture becomes more and more complex. This paper takes a kernel from the
Materials Science code BerkeleyGW, and demonstrates a few performance analysis
and optimization techniques. Despite challenges such as high register usage,
low occupancy, complex data access patterns, and the existence of several
long-latency instructions, we have achieved 3.7 TFLOP/s of double-precision
performance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of
the theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of
the more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of
techniques used to analyze this OpenACC kernel and optimize its performance are
shown, including the use of hierarchical Roofline performance model and the
performance tool Nsight Compute. This kernel exhibits computational
characteristics that are commonly seen in many high-performance computing (HPC)
applications, and are expected to be very helpful to a general audience of HPC
developers and computational scientists, as they pursue more performance on
NVIDIA GPUs.
"
2271,Optimising AI Training Deployments using Graph Compilers and Containers,"  Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN)
or Deep Learning (DL) have become popular due to their success in solving
problems likeimage analysis and speech recognition. Training a DNN is
computationally intensive and High Performance Computing(HPC) has been a key
driver in AI growth. Virtualisation and container technology have led to the
convergence of cloud and HPC infrastructure. These infrastructures with diverse
hardware increase the complexity of deploying and optimising AI training
workloads. AI training deployments in HPC or cloud can be optimised with
target-specific libraries, graph compilers, andby improving data movement or
IO. Graph compilers aim to optimise the execution of a DNN graph by generating
an optimised code for a target hardware/backend. As part of SODALITE (a Horizon
2020 project), MODAK tool is developed to optimise application deployment in
software defined infrastructures. Using input from the data scientist and
performance modelling, MODAK maps optimal application parameters to a target
infrastructure and builds an optimised container. In this paper, we introduce
MODAK and review container technologies and graph compilers for AI. We
illustrate optimisation of AI training deployments using graph compilers and
Singularity containers. Evaluation using MNIST-CNN and ResNet50 training
workloads shows that custom built optimised containers outperform the official
images from DockerHub. We also found that the performance of graph compilers
depends on the target hardware and the complexity of the neural network.
"
2272,"Smart-PGSim: Using Neural Network to Accelerate AC-OPF Power Grid
  Simulation","  The optimal power flow (OPF) problem is one of the most important
optimization problems for the operation of the power grid. It calculates the
optimum scheduling of the committed generation units. In this paper, we develop
a neural network approach to the problem of accelerating the current optimal
power flow (AC-OPF) by generating an intelligent initial solution. The high
quality of the initial solution and guidance of other outputs generated by the
neural network enables faster convergence to the solution without losing
optimality of final solution as computed by traditional methods. Smart-PGSim
generates a novel multitask-learning neural network model to accelerate the
AC-OPF simulation. Smart-PGSim also imposes the physical constraints of the
simulation on the neural network automatically. Smart-PGSim brings an average
of 49.2% performance improvement (up to 91%), computed over 10,000 problem
simulations, with respect to the original AC-OPF implementation, without losing
the optimality of the final solution.
"
2273,Quality of Service (QoS): Measurements of Video Streaming,"  Nowadays video streaming is growing over the social clouds, where end-users
always want to share High Definition (HD) videos among friends. Mostly videos
were recorded via smartphones and other HD devices and short time videos have a
big file size. The big file size of videos required high bandwidth to upload
and download on the Internet and also required more time to load in a web page
for play. So avoiding this problem social cloud compress videos during the
upload for smooth play and fast loading in a web page. Compression decreases
the video quality which also decreases the quality of experience of end users.
In this paper we measure the QoS of different standard video file formats on
social clouds; they varied from each other in resolution, audio/video bitrate,
and storage size.
"
2274,"Analysis of Interference between RDMA and Local Access on Hybrid Memory
  System","  We can use a hybrid memory system consisting of DRAM and Intel Optane DC
Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially
available since April 2019. Even if the latency for DCPM is several times
higher than that for DRAM, the capacity for DCPM is several times higher than
that for DRAM and the cost of DCPM is also several times lower than that for
DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory
system could improve the performance for in-memory database systems and virtual
machine (VM) systems because these systems often consume a large amount of
memory. Moreover, a high-speed shared storage system can be implemented by
accessing DCPM via remote direct memory access (RDMA). I assume that some of
the DCPM is often assigned as a shared area among other remote servers because
applications executed on a server with a hybrid memory system often cannot use
the entire capacity of DCPM. This paper evaluates the interference between
local memory access and RDMA from a remote server. As a result, I indicate that
the interference on this hybrid memory system is significantly different from
that on a conventional DRAM-only memory system. I also believe that some kind
of throttling implementation is needed when this interference occures.
"
2275,"Performance portability through machine learning guided kernel selection
  in SYCL libraries","  Automatically tuning parallel compute kernels allows libraries and frameworks
to achieve performance on a wide range of hardware, however these techniques
are typically focused on finding optimal kernel parameters for particular input
sizes and parameters. General purpose compute libraries must be able to cater
to all inputs and parameters provided by a user, and so these techniques are of
limited use. Additionally, parallel programming frameworks such as SYCL require
that the kernels be deployed in a binary format embedded within the library. As
such it is impractical to deploy a large number of possible kernel
configurations without inflating the library size.
  Machine learning methods can be used to mitigate against both of these
problems and provide performance for general purpose routines with a limited
number of kernel configurations. We show that unsupervised clustering methods
can be used to select a subset of the possible kernels that should be deployed
and that simple classification methods can be trained to select from these
kernels at runtime to give good performance. As these techniques are fully
automated, relying only on benchmark data, the tuning process for new hardware
or problems does not require any developer effort or expertise.
"
2276,Power and Performance Analysis of Persistent Key-Value Stores,"  With the current rate of data growth, processing needs are becoming difficult
to fulfill due to CPU power and energy limitations. Data serving systems and
especially persistent key-value stores have become a substantial part of data
processing stacks in the data center, providing access to massive amounts of
data for applications and services. Key-value stores exhibit high CPU and I/O
overheads because of their constant need to reorganize data on the devices. In
this paper, we examine the efficiency of two key-value stores on four servers
of different generations and with different CPU architectures. We use RocksDB,
a key-value that is deployed widely, e.g. in Facebook, and Kreon, a research
key-value store that has been designed to reduce CPU overhead. We evaluate
their behavior and overheads on an ARM-based microserver and three different
generations of x86 servers. Our findings show that microservers have better
power efficiency in the range of 0.68-3.6x with a comparable tail latency.
"
2277,CoShare: An Efficient Approach for Redundancy Allocation in NFV,"  An appealing feature of Network Function Virtualization (NFV) is that in an
NFV-based network, a network function (NF) instance may be placed at any node.
This, on the one hand, offers great flexibility in redundancy allocation to
meet the availability requirements of flows; on the other hand, it makes the
challenge unique and difficult. One particular highlight is that there is
inherent correlation among nodes due to the structure of the network, implying
that special care is needed for redundancy allocation in NFV-based networks. To
this aim, a novel approach, called CoShare, is proposed. Originally, its design
takes into consideration the effect of network structural dependency. In
addition, to efficiently make use of resources, CoShare proposes the idea of
shared reservation, where multiple flows may be allowed to share the same
reserved backup capacity at an NF instance. Furthermore, CoShare factors in the
heterogeneity in nodes, NF instances and availability requirements of flows in
the design. The results from a number of experiments conducted using realistic
network topologies show that CoShare is able to meet diverse availability
requirements in a resource-efficient manner, requiring less resource overbuild
than using the idea of dedicated reservation commonly adopted for redundancy
allocation in NFV.
"
2278,Chimbuko: A Workflow-Level Scalable Performance Trace Analysis Tool,"  Because of the limits input/output systems currently impose on
high-performance computing systems, a new generation of workflows that include
online data reduction and analysis is emerging. Diagnosing their performance
requires sophisticated performance analysis capabilities due to the complexity
of execution patterns and underlying hardware, and no tool could handle the
voluminous performance trace data needed to detect potential problems. This
work introduces Chimbuko, a performance analysis framework that provides
real-time, distributed, in situ anomaly detection. Data volumes are reduced for
human-level processing without losing necessary details. Chimbuko supports
online performance monitoring via a visualization module that presents the
overall workflow anomaly distribution, call stacks, and timelines. Chimbuko
also supports the capture and reduction of performance provenance. To the best
of our knowledge, Chimbuko is the first online, distributed, and scalable
workflow-level performance trace analysis framework, and we demonstrate the
tool's usefulness on Oak Ridge National Laboratory's Summit system.
"
2279,"Theodolite: Scalability Benchmarking of Distributed Stream Processing
  Engines","  Distributed stream processing engines are designed with a focus on
scalability to process big data volumes in a continuous manner. We present the
Theodolite method for benchmarking the scalability of distributed stream
processing engines. Core of this method is the definition of use cases that
microservices implementing stream processing have to fulfill. For each use
case, our method identifies relevant workload dimensions that might affect the
scalability of a use case. We propose to design one benchmark per use case and
relevant workload dimension. We present a general benchmarking framework, which
can be applied to execute the individual benchmarks for a given use case and
workload dimension. Our framework executes an implementation of the use case's
dataflow architecture for different workloads of the given dimension and
various numbers of processing instances. This way, it identifies how resources
demand evolves with increasing workloads. Within the scope of this paper, we
present 4 identified use cases, derived from processing Industrial Internet of
Things data, and 7 corresponding workload dimensions. We provide
implementations of 4 benchmarks with Kafka Streams as well as an implementation
of our benchmarking framework to execute scalability benchmarks in cloud
environments. We use both for evaluating the Theodolite method and for
benchmarking Kafka Streams' scalability for different deployment options.
"
2280,Architectural Implications of Graph Neural Networks,"  Graph neural networks (GNN) represent an emerging line of deep learning
models that operate on graph structures. It is becoming more and more popular
due to its high accuracy achieved in many graph-related tasks. However, GNN is
not as well understood in the system and architecture community as its
counterparts such as multi-layer perceptrons and convolutional neural networks.
This work tries to introduce the GNN to our community. In contrast to prior
work that only presents characterizations of GCNs, our work covers a large
portion of the varieties for GNN workloads based on a general GNN description
framework. By constructing the models on top of two widely-used libraries, we
characterize the GNN computation at inference stage concerning general-purpose
and application-specific architectures and hope our work can foster more system
and architecture research for GNNs.
"
2281,Service Rate Region: A New Aspect of Coded Distributed System Design,"  Erasure coding has been recently employed as a powerful method to mitigate
delays due to slow or straggling nodes in distributed systems. In this work, we
show that erasure coding of data objects can flexibly handle skews in the
request rates. Coding can help boost the service rate region, that is, increase
the overall volume of data access requests that can be handled by the system.
The goal of this paper is to postulate the service rate region as an important
consideration in the design of erasure coded distributed systems. We highlight
several open problems that can be grouped into two broad threads: 1)
characterizing the service rate region of a given code and finding the optimal
request allocation, and 2) designing the underlying erasure code for a given
service rate region. As contributions along the first thread, we characterize
the rate regions of maximum-distance-separable, locally repairable, and Simplex
codes. In terms of code design, we show the effectiveness of hybrid codes that
combine replication and erasure coding, and also discover fundamental
connections between multi-set batch codes and the problem of maximizing the
service rate region.
"
2282,"Analysis of an M/G/1 system for the optimization of the RTG performances
  in the delivery of containers in Abidjan Terminal","  In front of the major challenges to increase its productivity while
satisfying its customer, it is today important to establish in advance the
operational performances of the RTG Abidjan Terminal. In this article, by using
an M/G/1 retrial queue system, we obtained the average number of parked
delivery trucks and as well as their waiting time. Finally, we used Matlab to
represent them graphically then analyze the RTG performances according to the
traffic rate.
"
2283,ScalAna: Automating Scaling Loss Detection with Graph Analysis,"  Scaling a parallel program to modern supercomputers is challenging due to
inter-process communication, Amdahl's law, and resource contention. Performance
analysis tools for finding such scaling bottlenecks either base on profiling or
tracing. Profiling incurs low overheads but does not capture detailed
dependencies needed for root-cause analysis. Tracing collects all information
at prohibitive overheads. In this work, we design ScalAna that uses static
analysis techniques to achieve the best of both worlds - it enables the
analyzability of traces at a cost similar to profiling. ScalAna first leverages
static compiler techniques to build a Program Structure Graph, which records
the main computation and communication patterns as well as the program's
control structures. At runtime, we adopt lightweight techniques to collect
performance data according to the graph structure and generate a Program
Performance Graph. With this graph, we propose a novel approach, called
backtracking root cause detection, which can automatically and efficiently
detect the root cause of scaling loss. We evaluate ScalAna with real
applications. Results show that our approach can effectively locate the root
cause of scaling loss for real applications and incurs 1.73% overhead on
average for up to 2,048 processes. We achieve up to 11.11% performance
improvement by fixing the root causes detected by ScalAna on 2,048 processes.
"
2284,"Hierarchical Roofline Analysis: How to Collect Data using Performance
  Tools on Intel CPUs and NVIDIA GPUs","  This paper surveys a range of methods to collect necessary performance data
on Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of
mid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight
Compute, have integrated Roofline analysis into their supported feature set.
This paper fills the gap for when these tools are not available, or when users
would like a more customized workflow for certain analysis. Specifically, we
will discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel
Amplifier on Intel architectures, and nvprof, Nsight Compute metrics, and
Nsight Compute section files on NVIDIA architectures. These tools will be used
to collect information for as many memory/cache levels in the memory hierarchy
as possible in order to provide insights into application's data reuse and
cache locality characteristics.
"
2285,Collaborative Management of Benchmark Instances and their Attributes,"  Experimental evaluation is an integral part in the design process of
algorithms. Publicly available benchmark instances are widely used to evaluate
methods in SAT solving. For the interpretation of results and the design of
algorithm portfolios their attributes are crucial. Capturing the interrelation
of benchmark instances and their attributes is considerably simplified through
our specification of a benchmark instance identifier. Thus, our tool increases
the availability of both by providing means to manage and retrieve benchmark
instances by their attributes and vice versa. Like this, it facilitates the
design and analysis of SAT experiments and the exchange of results.
"
2286,"Latency and Throughput Optimization in Modern Networks: A Comprehensive
  Survey","  Modern applications are highly sensitive to communication delays and
throughput. This paper surveys major attempts on reducing latency and
increasing the throughput. These methods are surveyed on different networks and
surroundings such as wired networks, wireless networks, application layer
transport control, Remote Direct Memory Access, and machine learning based
transport control.
"
2287,GPA: A GPU Performance Advisor Based on Instruction Sampling,"  Developing efficient GPU kernels can be difficult because of the complexity
of GPU architectures and programming models. Existing performance tools only
provide coarse-grained suggestions at the kernel level, if any. In this paper,
we describe GPA, a performance advisor for NVIDIA GPUs that suggests potential
code optimization opportunities at a hierarchy of levels, including individual
lines, loops, and functions. To relieve users of the burden of interpreting
performance counters and analyzing bottlenecks, GPA uses data flow analysis to
approximately attribute measured instruction stalls to their root causes and
uses information about a program's structure and the GPU to match inefficiency
patterns with suggestions for optimization. To quantify each suggestion's
potential benefits, we developed PC sampling-based performance models to
estimate its speedup. Our experiments with benchmarks and applications show
that GPA provides an insightful report to guide performance optimization. Using
GPA, we obtained speedups on a Volta V100 GPU ranging from 1.03$\times$ to
3.86$\times$, with a geometric mean of 1.22$\times$.
"
2288,Time-Based Roofline for Deep Learning Performance Analysis,"  Deep learning applications are usually very compute-intensive and require a
long run time for training and inference. This has been tackled by researchers
from both hardware and software sides, and in this paper, we propose a
Roofline-based approach to performance analysis to facilitate the optimization
of these applications. This approach is an extension of the Roofline model
widely used in traditional high-performance computing applications, and it
incorporates both compute/bandwidth complexity and run time in its formulae to
provide insights into deep learning-specific characteristics. We take two sets
of representative kernels, 2D convolution and long short-term memory, to
validate and demonstrate the use of this new approach, and investigate how
arithmetic intensity, cache locality, auto-tuning, kernel launch overhead, and
Tensor Core usage can affect performance. Compared to the common ad-hoc
approach, this study helps form a more systematic way to analyze code
performance and identify optimization opportunities for deep learning
applications.
"
2289,"Hierarchical Roofline Performance Analysis for Deep Learning
  Applications","  This paper presents a practical methodology for collecting performance data
necessary to conduct hierarchical Roofline analysis on NVIDIA GPUs. It
discusses the extension of the Empirical Roofline Toolkit for broader support
of a range of data precisions and Tensor Core support and introduces a Nsight
Compute based method to accurately collect application performance information.
This methodology allows for automated machine characterization and application
characterization for Roofline analysis across the entire memory hierarchy on
NVIDIA GPUs, and it is validated by a complex deep learning application used
for climate image segmentation. We use two versions of the code, in TensorFlow
and PyTorch respectively, to demonstrate the use and effectiveness of this
methodology. We highlight how the application utilizes the compute and memory
capabilities on the GPU and how the implementation and performance differ in
two deep learning frameworks.
"
2290,Repeated Recursion Unfolding for Super-Linear Speedup within Bounds,"  Repeated recursion unfolding is a new approach that repeatedly unfolds a
recursion with itself and simplifies it while keeping all unfolded rules. Each
unfolding doubles the number of recursive steps covered. This reduces the
number of recursive rule applications to its logarithm at the expense of
introducing a logarithmic number of unfolded rules to the program. Efficiency
crucially depends on the amount of simplification inside the unfolded rules. We
prove a super-linear speedup theorem in the best case, i.e. speedup by more
than a constant factor. Our optimization can lower the time complexity class of
a program. In this paper, the super-linear speedup is within bounds: it holds
up to an arbitrary but chosen upper bound on the number of recursive steps. We
also report on the first results with a prototype implementation of repeated
recursion unfolding. A simple program transformation completely removes
recursion up to the chosen bound. The actual runtime improvement quickly
reaches several orders of magnitude.
"
2291,"Artery-C -- An OMNeT++ Based Discrete Event Simulation Framework for
  Cellular V2X","  Cellular Vehicle-to-X (Cellular V2X) is a communication technology that aims
to facilitate the communication among vehicles and with the roadside
infrastructure. Introduced with LTE Release 14, Cellular V2X enables
device-to-device communication to support road safety and traffic efficiency
applications. We present Artery-C, a simulation framework for the performance
evaluation of Cellular V2X protocols and V2X applications. Our simulator relies
on the simulation framework SimuLTE and substantially extends it by
implementing control and user planes. Besides the vehicle-to-network
communication via the up-/downlink interface, it provides vehicle-to-vehicle
and vehicle-infrastructure communication via the sidelink interface using the
managed and the unmanaged mode of Cellular V2X (mode 3 and 4, respectively).
The simulator also implements advanced features of 5G mobile networks, such as
variable numerologies. For the transmission of of V2X messages, it adds a
non-IP interface. Artery-C integrates seamlessly into the simulation framework
Artery, which enables the simulation of standardized V2X messages at the
facilities layer as well as the coupling to the mobility simulator SUMO. A
specific feature of Artery-C is the support of dynamic switching between all
modes of Cellular V2X. In order to demonstrate the capabilities of Artery-C, we
evaluate V2X-based platooning as a representative use case and present results
for mode 3, mode 4 and mode switching in a highway scenario.
"
2292,"Efficiency Near the Edge: Increasing the Energy Efficiency of FFTs on
  GPUs for Real-time Edge Computing","  The Square Kilometre Array (SKA) is an international initiative for
developing the world's largest radio telescope with a total collecting area of
over a million square meters. The scale of the operation, combined with the
remote location of the telescope, requires the use of energy-efficient
computational algorithms. This, along with the extreme data rates that will be
produced by the SKA and the requirement for a real-time observing capability,
necessitates in-situ data processing in an edge style computing solution. More
generally, energy efficiency in the modern computing landscape is becoming of
paramount concern. Whether it be the power budget that can limit some of the
world's largest supercomputers, or the limited power available to the smallest
Internet-of-Things devices. In this paper, we study the impact of hardware
frequency scaling on the energy consumption and execution time of the Fast
Fourier Transform (FFT) on NVIDIA GPUs using the cuFFT library. The FFT is used
in many areas of science and it is one of the key algorithms used in radio
astronomy data processing pipelines. Through the use of frequency scaling, we
show that we can lower the power consumption of the NVIDIA V100 GPU when
computing the FFT by up to 60% compared to the boost clock frequency, with less
than a 10% increase in the execution time. Furthermore, using one common core
clock frequency for all tested FFT lengths, we show on average a 50% reduction
in power consumption compared to the boost core clock frequency with an
increase in the execution time still below 10%. We demonstrate how these
results can be used to lower the power consumption of existing data processing
pipelines. These savings, when considered over years of operation, can yield
significant financial savings, but can also lead to a significant reduction of
greenhouse gas emissions.
"
2293,"tinyMD: A Portable and Scalable Implementation for Pairwise Interactions
  Simulations","  This paper investigates the suitability of the AnyDSL partial evaluation
framework to implement tinyMD: an efficient, scalable, and portable simulation
of pairwise interactions among particles. We compare tinyMD with the miniMD
proxy application that scales very well on parallel supercomputers. We discuss
the differences between both implementations and contrast miniMD's performance
for single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG
and Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility
by coupling it with the waLBerla multi-physics framework. This allow us to
execute tinyMD simulations using the load-balancing mechanism implemented in
waLBerla.
"
2294,"Exploration of Fine-Grained Parallelism for Load Balancing Eager K-truss
  on GPU and CPU","  In this work we present a performance exploration on Eager K-truss, a
linear-algebraic formulation of the K-truss graph algorithm. We address
performance issues related to load imbalance of parallel tasks in symmetric,
triangular graphs by presenting a fine-grained parallel approach to executing
the support computation. This approach also increases available parallelism,
making it amenable to GPU execution. We demonstrate our fine-grained parallel
approach using implementations in Kokkos and evaluate them on an Intel Skylake
CPU and an Nvidia Tesla V100 GPU. Overall, we observe between a 1.261. 48x
improvement on the CPU and a 9.97-16.92x improvement on the GPU due to our
fine-grained parallel formulation.
"
2295,Towards an Objective Metric for the Performance of Exact Triangle Count,"  The performance of graph algorithms is often measured in terms of the number
of traversed edges per second (TEPS). However, this performance metric is
inadequate for a graph operation such as exact triangle counting. In triangle
counting, execution times on graphs with a similar number of edges can be
distinctly different as demonstrated by results from the past Graph Challenge
entries. We discuss the need for an objective performance metric for graph
operations and the desired characteristics of such a metric such that it more
accurately captures the interactions between the amount of work performed and
the capabilities of the hardware on which the code is executed. Using exact
triangle counting as an example, we derive a metric that captures how certain
techniques employed in many implementations improve performance. We demonstrate
that our proposed metric can be used to evaluate and compare multiple
approaches for triangle counting, using a SIMD approach as a case study against
a scalar baseline.
"
2296,Caching in Networks without Regret,"  We consider the online $\textsf{Bipartite Caching}$ problem where $n$ users
are connected to $m$ caches in the form of a bipartite network. Each of the $m$
caches has a file storage capacity of $C$. There is a library consisting of $N
>C$ distinct files. Each user can request any one of the files from the library
at each time slot. We allow the file request sequences to be chosen in an
adversarial fashion. A user's request at a time slot is satisfied if the
requested file is already hosted on at least one of the caches connected to the
user at that time slot. Our objective is to design an efficient online caching
policy with minimal regret. In this paper, we propose $\textsf{LeadCache,}$ an
online caching policy based on the $\textsf{Follow the Perturbed Leader}$
(FTPL) paradigm. We show that $\textsf{LeadCache}$ is regret optimal up to a
multiplicative factor of $\tilde{O}(n^{0.375}).$ As a byproduct of our
analysis, we design a new linear-time deterministic Pipage rounding procedure
for the LP relaxation of a well-known NP-hard combinatorial optimization
problem in this area. Our new rounding algorithm substantially improves upon
the currently best-known complexity for this problem. Moreover, we show the
surprising result that under mild Strong-Law-type assumptions on the file
request sequence, the rate of file fetches to the caches approaches to zero
under the $\textsf{LeadCache}$ policy. Finally, we derive a tight universal
regret lower bound for the $\textsf{Bipartite Caching}$ problem, which
critically makes use of results from graph coloring theory and certifies the
announced approximation ratio.
"
2297,On the Throughput Optimization in Large-Scale Batch-Processing Systems,"  We analyze a data-processing system with $n$ clients producing jobs which are
processed in \textit{batches} by $m$ parallel servers; the system throughput
critically depends on the batch size and a corresponding sub-additive speedup
function. In practice, throughput optimization relies on numerical searches for
the optimal batch size, a process that can take up to multiple days in existing
commercial systems. In this paper, we model the system in terms of a closed
queueing network; a standard Markovian analysis yields the optimal throughput
in $\omega\left(n^4\right)$ time. Our main contribution is a mean-field model
of the system for the regime where the system size is large. We show that the
mean-field model has a unique, globally attractive stationary point which can
be found in closed form and which characterizes the asymptotic throughput of
the system as a function of the batch size. Using this expression we find the
\textit{asymptotically} optimal throughput in $O(1)$ time. Numerical settings
from a large commercial system reveal that this asymptotic optimum is accurate
in practical finite regimes.
"
2298,"Inter-database validation of a deep learning approach for automatic
  sleep scoring","  In this work we describe a new deep learning approach for automatic sleep
staging, and carry out its validation by addressing its generalization
capabilities on a wide range of sleep staging databases. Prediction
capabilities are evaluated in the context of independent local and external
generalization scenarios. Effectively, by comparing both procedures it is
possible to better extrapolate the expected performance of the method on the
general reference task of sleep staging, regardless of data from a specific
database. In addition, we examine the suitability of a novel approach based on
the use of an ensemble of individual local models and evaluate its impact on
the resulting inter-database generalization performance. Validation results
show good general performance, as compared to the expected levels of human
expert agreement, as well as state-of-the-art automatic sleep staging
approaches
"
2299,"Correlation Coefficient Analysis of the Age of Information in
  Multi-Source Systems","  This paper studies the age of information (AoI) on an information updating
system such that multiple sources share one server to process packets of
updated information. In such systems, packets from different sources compete
for the server, and thus they may suffer from being interrupted, being
backlogged, and becoming stale. Therefore, in order to grasp structures of such
systems, it is crucially important to study a metric indicating a correlation
of different sources. In this paper, we aim to analyze the correlation of AoIs
on a single-server queueing system with multiple sources. As our contribution,
we provide the closed-form expression of the correlation coefficient of the
AoIs. To this end, we first derive the Laplace-Stieltjes transform of the
stationary distribution of each AoI for the multiple sources. Some nontrivial
properties on the systems are revealed from our analysis results.
"
2300,"ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization
  Of Ephemeral Cloud Resources","  Cloud data center capacities are over-provisioned to handle demand peaks and
hardware failures which leads to low resources' utilization. One way to improve
resource utilization and thus reduce the total cost of ownership is to offer
unused resources (referred to as ephemeral resources) at a lower price.
However, reselling resources needs to meet the expectations of its customers in
terms of Quality of Service. The goal is so to maximize the amount of reclaimed
resources while avoiding SLA penalties. To achieve that, cloud providers have
to estimate their future utilization to provide availability guarantees. The
prediction should consider a safety margin for resources to react to
unpredictable workloads. The challenge is to find the safety margin that
provides the best trade-off between the amount of resources to reclaim and the
risk of SLA violations. Most state-of-the-art solutions consider a fixed safety
margin for all types of metrics (e.g., CPU, RAM). However, a unique fixed
margin does not consider various workloads variations over time which may lead
to SLA violations or/and poor utilization. In order to tackle these challenges,
we propose ReLeaSER, a Reinforcement Learning strategy for optimizing the
ephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the
safety margin at the host-level for each resource metric. The strategy learns
from past prediction errors (that caused SLA violations). Our solution reduces
significantly the SLA violation penalties on average by 2.7x and up to 3.4x. It
also improves considerably the CPs' potential savings by 27.6% on average and
up to 43.6%.
"
2301,Applying the Roofline model for Deep Learning performance optimizations,"  In this paper We present a methodology for creating Roofline models
automatically for Non-Unified Memory Access (NUMA) using Intel Xeon as an
example. Finally, we present an evaluation of highly efficient deep learning
primitives as implemented in the Intel oneDNN Library.
"
2302,Investigating Applications on the A64FX,"  The A64FX processor from Fujitsu, being designed for computational simulation
and machine learning applications, has the potential for unprecedented
performance in HPC systems. In this paper, we evaluate the A64FX by
benchmarking against a range of production HPC platforms that cover a number of
processor technologies. We investigate the performance of complex scientific
applications across multiple nodes, as well as single node and mini-kernel
benchmarks. This paper finds that the performance of the A64FX processor across
our chosen benchmarks often significantly exceeds other platforms, even without
specific application optimisations for the processor instruction set or
hardware. However, this is not true for all the benchmarks we have undertaken.
Furthermore, the specific configuration of applications can have an impact on
the runtime and performance experienced.
"
2303,Flexible Performant GEMM Kernels on GPUs,"  General Matrix Multiplication or GEMM kernels take center place in high
performance computing and machine learning. Recent NVIDIA GPUs include GEMM
accelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by
the two-language problem: it requires either low-level programming which
implies low programmer productivity or using libraries that only offer a
limited set of components. Because rephrasing algorithms in terms of
established components often introduces overhead, the libraries' lack of
flexibility limits the freedom to explore new algorithms. Researchers using
GEMMs can hence not enjoy programming productivity, high performance, and
research flexibility at once.
  In this paper we solve this problem. We present three sets of abstractions
and interfaces to program GEMMs within the scientific Julia programming
language. The interfaces and abstractions are co-designed for researchers'
needs and Julia's features to achieve sufficient separation of concerns and
flexibility to easily extend basic GEMMs in many different ways without paying
a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS
and CUTLASS, we demonstrate that our performance is mostly on par with, and in
some cases even exceeds, the libraries, without having to write a single line
of code in CUDA C++ or assembly, and without facing flexibility limitations.
"
2304,Pass-and-swap queues,"  Order-independent (OI) queues, introduced by Berezner, Kriel, and Krzesinski
in 1995, expanded the family of multi-class queues that are known to have a
product-form stationary distribution by allowing for intricate class-dependent
service rates. This paper further broadens this family by introducing
pass-and-swap (P&S) queues, an extension of OI queues where any customer that
completes service is not necessarily the customer that leaves the system. More
precisely, we supplement the OI queue model with an undirected graph on the
customer classes, which we call a swapping graph, such that there is an edge
between two classes if customers of these classes can be swapped with one
another. When a customer completes service, it passes over customers in the
remainder of the queue until it finds a customer it can swap position with,
that is, a customer whose class is a neighbor in the graph. In its turn, the
customer that is ejected from its position takes the position of the next
customer it can swap with, and so on. This is repeated until a customer cannot
find another customer to be swapped with anymore; this customer is the one that
leaves the queue. After proving that P&S queues have a product-form stationary
distribution, we derive a necessary and sufficient stability condition for
(open networks of) P&S queues that also applies to OI queues. We then study
irreducibility properties of closed networks of P&S queues and derive the
corresponding product-form stationary distribution. Lastly, we demonstrate that
closed networks of P&S queues can be applied to describe the dynamics of new
and existing load-distribution and scheduling algorithms in machine pools.
"
2305,"An experimental evaluation and characterization of VoIP over an LTE-A
  network","  Mobile telecommunications are converging towards all-IP solutions. This is
the case of the Long Term Evolution (LTE) technology that, having no
circuit-switched bearer to support voice traffic, needs a dedicated VoIP
infrastructure, which often relies on the IP Multimedia Subsystem architecture.
Most telecom operators implement LTE-A, an advanced version of LTE often
marketed as 4G+, which achieves data rate peaks of 300 Mbps. Yet, although such
novel technology boosts the access to advanced multimedia contents and
services, telco operators continue to consider the VoIP market as the major
revenue for their business. In this work, the authors propose a detailed
performance assessment of VoIP traffic by carrying out experimental trials
across a real LTE-A environment. The experimental campaign consists of two
stages. First, we characterize VoIP calls between fixed and mobile terminals,
based on a dataset that includes more than 750,000 data-voice packets. We
analyze quality-of-service metrics such as round-trip time (RTT) and jitter, to
capture the influence of uncontrolled factors that typically appear in
real-world settings. In the second stage, we further consider VoIP flows across
a range of codecs, looking at the trade-offs between quality and bandwidth
consumption. Moreover, we propose a statistical characterization of jitter and
RTT (representing the most critical parameters), identifying the optimal
approximating distribution, namely the Generalized Extreme Value (GEV). The
estimation of parameters through the Maximum Likelihood criterion, leads us to
reveal both the short- and long-tail behaviour for jitter and RTT,
respectively.
"
2306,"Seagull: An Infrastructure for Load Prediction and Optimized Resource
  Allocation","  Microsoft Azure is dedicated to guarantee high quality of service to its
customers, in particular, during periods of high customer activity, while
controlling cost. We employ a Data Science (DS) driven solution to predict user
load and leverage these predictions to optimize resource allocation. To this
end, we built the Seagull infrastructure that processes per-server telemetry,
validates the data, trains and deploys ML models. The models are used to
predict customer load per server (24h into the future), and optimize service
operations. Seagull continually re-evaluates accuracy of predictions, fallback
to previously known good models and triggers alerts as appropriate. We deployed
this infrastructure in production for PostgreSQL and MySQL servers across all
Azure regions, and applied it to the problem of scheduling server backups
during low-load time. This minimizes interference with user-induced load and
improves customer experience.
"
2307,"Availability Evaluation of Multi-tenant Service Function Chaining
  Infrastructures by Multidimensional Universal Generating Function","  The Network Function Virtualization (NFV) paradigm has been devised as an
enabler of next generation network infrastructures by speeding up the
provisioning and the composition of novel network services. The latter are
implemented via a chain of virtualized network functions, a process known as
Service Function Chaining. In this paper, we evaluate the availability of
multi-tenant SFC infrastructures, where every network function is modeled as a
multi-state system and is shared among different and independent tenants. To
this aim, we propose a Universal Generating Function (UGF) approach, suitably
extended to handle performance vectors, that we call Multidimensional UGF. This
novel methodology is validated in a realistic multi-tenant telecommunication
network scenario, where the service chain is composed by the network elements
of an IP Multimedia Subsystem implemented via NFV. A steady-state availability
evaluation of such an exemplary system is presented and a redundancy
optimization problem is solved, so providing the SFC infrastructure which
minimizes deployment cost while respecting a given availability requirement.
"
2308,"New Accurate Approximation for Average Error Probability Under
  $\kappa-\mu$ Shadowed Fading Channel","  This paper proposes new accurate approximations for average error probability
(AEP) of a communication system employing either $M$-phase-shift keying (PSK)
or differential quaternary PSK with Gray coding (GC-DQPSK) modulation schemes
over $\kappa-\mu$ shadowed fading channel. Firstly, new accurate approximations
of error probability (EP) of both modulation schemes are derived over additive
white Gaussian noise (AWGN) channel. Leveraging the trapezoidal integral
method, a tight approximate expression of symbol error probability for $M$-PSK
modulation is presented, while new upper and lower bounds for Marcum
$Q$-function of the first order (MQF), and subsequently those for bit error
probability (BER) under DQPSK scheme, are proposed. Next, these bounds are
linearly combined to propose a highly refined and accurate BER's approximation.
The key idea manifested in the decrease property of modified Bessel function
$I_{v}$, strongly related to MQF, with its argument $v$. Finally, theses
approximations are used to tackle AEP's approximation under $\kappa-\mu$
shadowed fading. Numerical results show the accuracy of the presented
approximations compared to the exact ones.
"
2309,"On the sojourn time of a batch in the $M^{[X]}/M/1$ Processor Sharing
  Queue","  In this paper, we analyze the sojourn of an entire batch in a processor
sharing $M^{[X]}/M/1$ processor queue, where geometrically distributed batches
arrive according to a Poisson process and jobs require exponential service
times. By conditioning on the number of jobs in the systems and the number of
jobs in a tagged batch, we establish recurrence relations between conditional
sojourn times, which subsequently allow us to derive a partial differential
equation for an associated bivariate generating function. This equation
involves an unknown generating function, whose coefficients can be computed by
solving an infinite lower triangular linear system. Once this unknown function
is determined, we compute the Laplace transform and the mean value of the
sojourn time of a batch in the system.
"
2310,"Montage: A General System for Buffered Durably Linearizable Data
  Structures","  The recent emergence of fast, dense, nonvolatile main memory suggests that
certain long-lived data might remain in its natural pointer-rich format across
program runs and hardware reboots. Operations on such data must be instrumented
with explicit write-back and fence instructions to ensure consistency in the
wake of a crash. Techniques to minimize the cost of this instrumentation are an
active topic of research.
  We present what we believe to be the first general-purpose approach to
building buffered durably linearizable persistent data structures, and a
system, Montage, to support that approach. Montage is built on top of the
Ralloc nonblocking persistent allocator. It employs a slow-ticking epoch clock,
and ensures that no operation appears to span an epoch boundary. It also
arranges to persist only that data minimally required to reconstruct the
structure after a crash. If a crash occurs in epoch $e$, all work performed in
epochs $e$ and $e-1$ is lost, but work from prior epochs is preserved.
  We describe the implementation of Montage, argue its correctness, and report
unprecedented throughput for persistent queues, sets/mappings, and general
graphs.
"
2311,"Performance Modeling of Streaming Kernels and Sparse Matrix-Vector
  Multiplication on A64FX","  The A64FX CPU powers the current number one supercomputer on the Top500 list.
Although it is a traditional cache-based multicore processor, its peak
performance and memory bandwidth rival accelerator devices. Generating
efficient code for such a new architecture requires a good understanding of its
performance features. Using these features, we construct the
Execution-Cache-Memory (ECM) performance model for the A64FX processor in the
FX700 supercomputer and validate it using streaming loops. We also identify
architectural peculiarities and derive optimization hints. Applying the ECM
model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS
matrix storage format is inappropriate and how the SELL-C-sigma format with
suitable code optimizations can achieve bandwidth saturation for SpMV.
"
2312,"Sequential Algorithms and Independent Sets Discovering on Large Sparse
  Random Graphs","  Computing the size of maximum independent sets is a NP-hard problem for fixed
graphs. Characterizing and designing efficient algorithms to estimate this
independence number for random graphs are notoriously difficult and still
largely open issues. In a companion paper, we showed that a low complexity
degree-greedy exploration is actually asymptotically optimal on a large class
of sparse random graphs. Encouraged by this result, we present and study two
variants of sequential exploration algorithms: static and dynamic degree-aware
explorations. We derive hydrodynamic limits for both of them, which in turn
allow us to compute the size of the resulting independent set. Whereas the
former is simpler to compute, the latter may be used to arbitrarily approximate
the degree-greedy algorithm. Both can be implemented in a distributed manner.
The corresponding hydrodynamic limits constitute an efficient method to compute
or bound the independence number for a large class of sparse random graphs. As
an application, we then show how our method may be used to estimate the
capacity of a large 802.11-based wireless network. We finally consider further
indicators such as the fairness of the resulting configuration, and show how an
unexpected trade-off between fairness and capacity can be achieved.
"
2313,Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores,"  Sparse general matrix-matrix multiplication (spGEMM) is an essential
component in many scientific and data analytics applications. However, the
sparsity pattern of the input matrices and the interaction of their patterns
make spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which
specialize in dense matrix multiplication. Our aim is to re-purpose TCUs for
sparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply
sparse rectangular blocks using the mixed precision mode of TCUs. tSparse
partitions the input matrices into tiles and operates only on tiles which
contain one or more elements. It creates a task list of the tiles, and performs
matrix multiplication of these tiles using TCUs. To the best of our knowledge,
this is the first time that TCUs are used in the context of spGEMM. We show
that spGEMM, with our tiling approach, benefits from TCUs. Our approach
significantly improves the performance of spGEMM in comparison to cuSPARSE,
CUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.
"
2314,Stability for Two-class Multiserver-job Systems,"  Multiserver-job systems, where jobs require concurrent service at many
servers, occur widely in practice. Much is known in the dropping setting, where
jobs are immediately discarded if they require more servers than are currently
available. However, very little is known in the more practical setting where
jobs queue instead.
  In this paper, we derive a closed-form analytical expression for the
stability region of a two-class (non-dropping) multiserver-job system where
each class of jobs requires a distinct number of servers and requires a
distinct exponential distribution of service time, and jobs are served in
first-come-first-served (FCFS) order. This is the first result of any kind for
an FCFS multiserver-job system where the classes have distinct service
distributions. Our work is based on a technique that leverages the idea of a
""saturated"" system, in which an unlimited number of jobs are always available.
  Our analytical formula provides insight into the behavior of FCFS
multiserver-job systems, highlighting the huge wastage (idle servers while jobs
are in the queue) that can occur, as well as the nonmonotonic effects of the
service rates on wastage.
"
2315,P = FS: Parallel is Just Fast Serial,"  We prove that parallel processing with homogeneous processors is logically
equivalent to fast serial processing. The reverse proposition can also be used
to identify obscure opportunities for applying parallelism. To our knowledge,
this theorem has not been previously reported in the queueing theory
literature. A plausible explanation is offered for why this might be. The basic
homogeneous theorem is also extended to optimizing the latency of heterogenous
parallel arrays.
"
2316,"Stochastic Analysis of Satellite Broadband by Mega-Constellations with
  Inclined LEOs","  As emerging massive constellations are intended to provide seamless
connectivity for remote areas using hundreds of small low Earth orbit (LEO)
satellites, new methodologies have great importance to study the performance of
these networks. In this paper, we derive both downlink and uplink analytical
expressions for coverage probability and data rate of an inclined LEO
constellation under general fading, regardless of exact satellites' positions.
Our solution involves two phases as we, first, abstract the network into a
uniformly distributed network. Secondly, we obtain a new parameter, effective
number of satellites, for every user's latitude which compensates for the
performance mismatch between the actual and uniform constellations. In addition
to exact derivation of the network performance metrics, this study provides
insight into selecting the constellation parameters, e.g., the total number of
satellites, altitude, and inclination angle.
"
2317,"iPregel: Strategies to Deal with an Extreme Form of Irregularity in
  Vertex-Centric Graph Processing","  Over the last decade, the vertex-centric programming model has attracted
significant attention in the world of graph processing, resulting in the
emergence of a number of vertex-centric frameworks. Its simple programming
interface, where computation is expressed from a vertex point of view, offers
both ease of programming to the user and inherent parallelism for the
underlying framework to leverage. However, vertex-centric programs represent an
extreme form of irregularity, both inter and intra core. This is because they
exhibit a variety of challenges from a workload that may greatly vary across
supersteps, through fine-grain synchronisations, to memory accesses that are
unpredictable both in terms of quantity and location. In this paper, we explore
three optimisations which address these irregular challenges; a hybrid combiner
carefully coupling lock-free and lock-based combinations, the partial
externalisation of vertex structures to improve locality and the shift to an
edge-centric representation of the workload. The optimisations were integrated
into the iPregel vertex-centric framework, enabling the evaluation of each
optimisation in the context of graph processing across three general purpose
benchmarks common in the vertex-centric community, each run on four publicly
available graphs covering all orders of magnitude from a million to a billion
edges. The result of this work is a set of techniques which we believe not only
provide a significant performance improvement in vertex-centric graph
processing, but are also applicable more generally to irregular applications.
"
2318,"Instead of Rewriting Foreign Code for Machine Learning, Automatically
  Synthesize Fast Gradients","  Applying differentiable programming techniques and machine learning
algorithms to foreign programs requires developers to either rewrite their code
in a machine learning framework, or otherwise provide derivatives of the
foreign code. This paper presents Enzyme, a high-performance automatic
differentiation (AD) compiler plugin for the LLVM compiler framework capable of
synthesizing gradients of statically analyzable programs expressed in the LLVM
intermediate representation (IR). Enzyme synthesizes gradients for programs
written in any language whose compiler targets LLVM IR including C, C++,
Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD
capabilities in these languages. Unlike traditional source-to-source and
operator-overloading tools, Enzyme performs AD on optimized IR. On a
machine-learning focused benchmark suite including Microsoft's ADBench, AD on
optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before
optimization allowing Enzyme to achieve state-of-the-art performance. Packaging
Enzyme for PyTorch and TensorFlow provides convenient access to gradients of
foreign code with state-of-the art performance, enabling foreign code to be
directly incorporated into existing machine learning workflows.
"
2319,Diversity/Parallelism Trade-off in Distributed Systems with Redundancy,"  As numerous machine learning and other algorithms increase in complexity and
data requirements, distributed computing becomes necessary to satisfy the
growing computational and storage demands, because it enables parallel
execution of smaller tasks that make up a large computing job. However, random
fluctuations in task service times lead to straggling tasks with long execution
times. Redundancy, in the form of task replication and erasure coding, provides
diversity that allows a job to be completed when only a subset of redundant
tasks is executed, thus removing the dependency on the straggling tasks. In
situations of constrained resources (here a fixed number of parallel servers),
increasing redundancy reduces the available resources for parallelism. In this
paper, we characterize the diversity vs. parallelism trade-off and identify the
optimal strategy, among replication, coding and splitting, which minimizes the
expected job completion time. We consider three common service time
distributions and establish three models that describe scaling of these
distributions with the task size. We find that different distributions with
different scaling models operate optimally at different levels of redundancy,
and thus may require very different code rates.
"
2320,A Streaming Approach For Efficient Batched Beam Search,"  We propose an efficient batching strategy for variable-length decoding on GPU
architectures. During decoding, when candidates terminate or are pruned
according to heuristics, our streaming approach periodically ""refills"" the
batch before proceeding with a selected subset of candidates. We apply our
method to variable-width beam search on a state-of-the-art machine translation
model. Our method decreases runtime by up to 71% compared to a fixed-width beam
search baseline and 17% compared to a variable-width baseline, while matching
baselines' BLEU. Finally, experiments show that our method can speed up
decoding in other domains, such as semantic and syntactic parsing.
"
2321,"A Characterization of the COVID-19 Pandemic Impact on a Mobile Network
  Operator Traffic","  During early 2020, the SARS-CoV-2 virus rapidly spread worldwide, forcing
many governments to impose strict lockdown measures to tackle the pandemic.
This significantly changed people's mobility and habits, subsequently impacting
how they use telecommunication networks. In this paper, we investigate the
effects of the COVID-19 emergency on a UK Mobile Network Operator (MNO). We
quantify the changes in users' mobility and investigate how this impacted the
cellular network usage and performance. Our analysis spans from the entire
country to specific regions, and geodemographic area clusters. We also provide
a detailed analysis for London. Our findings bring insights at different
geo-temporal granularity on the status of the cellular network, from the
decrease in data traffic volume in the cellular network and lower load on the
radio network, counterposed to a surge in the conversational voice traffic
volume.
"
2322,"LETI: Latency Estimation Tool and Investigation of Neural Networks
  inference on Mobile GPU","  A lot of deep learning applications are desired to be run on mobile devices.
Both accuracy and inference time are meaningful for a lot of them. While the
number of FLOPs is usually used as a proxy for neural network latency, it may
be not the best choice. In order to obtain a better approximation of latency,
research community uses look-up tables of all possible layers for latency
calculation for the final prediction of the inference on mobile CPU. It
requires only a small number of experiments. Unfortunately, on mobile GPU this
method is not applicable in a straight-forward way and shows low precision. In
this work, we consider latency approximation on mobile GPU as a data and
hardware-specific problem. Our main goal is to construct a convenient latency
estimation tool for investigation(LETI) of neural network inference and
building robust and accurate latency prediction models for each specific task.
To achieve this goal, we build open-source tools which provide a convenient way
to conduct massive experiments on different target devices focusing on mobile
GPU. After evaluation of the dataset, we learn the regression model on
experimental data and use it for future latency prediction and analysis. We
experimentally demonstrate the applicability of such an approach on a subset of
popular NAS-Benchmark 101 dataset and also evaluate the most popular neural
network architectures for two mobile GPUs. As a result, we construct latency
prediction model with good precision on the target evaluation subset. We
consider LETI as a useful tool for neural architecture search or massive
latency evaluation. The project is available at https://github.com/leti-ai
"
2323,Event Trend Aggregation Under Rich Event Matching Semantics,"  Streaming applications from health care analytics to algorithmic trading
deploy Kleene queries to detect and aggregate event trends. Rich event matching
semantics determine how to compose events into trends. The expressive power of
state-of-the-art systems remains limited in that they do not support the rich
variety of these semantics. Worse yet, they suffer from long delays and high
memory costs because they opt to maintain aggregates at a fine granularity. To
overcome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra)
approach supports this rich diversity of event matching semantics within one
system. Better yet, Cogra incrementally maintains aggregates at the coarsest
granularity possible for each of these semantics. In this way, Cogra minimizes
the number of aggregates -- reducing both time and space complexity. Our
experiments demonstrate that Cogra achieves up to four orders of magnitude
speed-up and up to eight orders of magnitude memory reduction compared to
state-of-the-art approaches.
"
2324,GRETA: Graph-based Real-time Event Trend Aggregation,"  Streaming applications from algorithmic trading to traffic management deploy
Kleene patterns to detect and aggregate arbitrarily-long event sequences,
called event trends. State-of-the-art systems process such queries in two
steps. Namely, they first construct all trends and then aggregate them. Due to
the exponential costs of trend construction, this two-step approach suffers
from both a long delays and high memory costs. To overcome these limitations,
we propose the Graph-based Real-time Event Trend Aggregation (Greta) approach
that dynamically computes event trend aggregation without first constructing
these trends. We define the Greta graph to compactly encode all trends. Our
Greta runtime incrementally maintains the graph, while dynamically propagating
aggregates along its edges. Based on the graph, the final aggregate is
incrementally updated and instantaneously returned at the end of each query
window. Our Greta runtime represents a win-win solution, reducing both the time
complexity from exponential to quadratic and the space complexity from
exponential to linear in the number of events. Our experiments demonstrate that
Greta achieves up to four orders of magnitude speed-up and up to 50--fold
memory reduction compared to the state-of-the-art two-step approaches.
"
2325,Sharon: Shared Online Event Sequence Aggregation,"  Streaming systems evaluate massive workloads of event sequence aggregation
queries. State-of-the-art approaches suffer from long delays caused by not
sharing intermediate results of similar queries and by constructing event
sequences prior to their aggregation. To overcome these limitations, our Shared
Online Event Sequence Aggregation (Sharon) approach shares intermediate
aggregates among multiple queries while avoiding the expensive construction of
event sequences. Our Sharon optimizer faces two challenges. One, a sharing
decision is not always beneficial. Two, a sharing decision may exclude other
sharing opportunities. To guide our Sharon optimizer, we compactly encode
sharing candidates, their benefits, and conflicts among candidates into the
Sharon graph. Based on the graph, we map our problem of finding an optimal
sharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use
the guaranteed weight of a greedy algorithm for the MWIS problem to prune the
search of our sharing plan finder without sacrificing its optimality. The
Sharon optimizer is shown to produce sharing plans that achieve up to an
18-fold speed-up compared to state-of-the-art approaches.
"
2326,Rank and run-time aware compression of NLP Applications,"  Sequence model based NLP applications can be large. Yet, many applications
that benefit from them run on small devices with very limited compute and
storage capabilities, while still having run-time constraints. As a result,
there is a need for a compression technique that can achieve significant
compression without negatively impacting inference run-time and task accuracy.
This paper proposes a new compression technique called Hybrid Matrix
Factorization that achieves this dual objective. HMF improves low-rank matrix
factorization (LMF) techniques by doubling the rank of the matrix using an
intelligent hybrid-structure leading to better accuracy than LMF. Further, by
preserving dense matrices, it leads to faster inference run-time than pruning
or structure matrix based compression technique. We evaluate the impact of this
technique on 5 NLP benchmarks across multiple tasks (Translation, Intent
Detection, Language Modeling) and show that for similar accuracy values and
compression factors, HMF can achieve more than 2.32x faster inference run-time
than pruning and 16.77% better accuracy than LMF.
"
2327,"Machine Learning Enabled Scalable Performance Prediction of Scientific
  Codes","  We present the Analytical Memory Model with Pipelines (AMMP) of the
Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and
hardware architecture parameters as input, predicts runtime of that code on the
target hardware platform, which is defined in the input parameters. PPT-AMMP
transforms the code to an (architecture-independent) intermediate
representation, then (i) analyzes the basic block structure of the code, (ii)
processes architecture-independent virtual memory access patterns that it uses
to build memory reuse distance distribution models for each basic block, (iii)
runs detailed basic-block level simulations to determine hardware pipeline
usage.
  PPT-AMMP uses machine learning and regression techniques to build the
prediction models based on small instances of the input code, then integrates
into a higher-order discrete-event simulation model of PPT running on Simian
PDES engine. We validate PPT-AMMP on four standard computational physics
benchmarks, finally present a use case of hardware parameter sensitivity
analysis to identify bottleneck hardware resources on different code inputs. We
further extend PPT-AMMP to predict the performance of scientific application
(radiation transport), SNAP. We analyze the application of multi-variate
regression models that accurately predict the reuse profiles and the basic
block counts. The predicted runtimes of SNAP when compared to that of actual
times are accurate.
"
2328,Temporal Vectorization for Stencils,"  Stencil computations represent a very common class of nested loops in
scientific and engineering applications. Exploiting vector units in modern CPUs
is crucial to achieving peak performance. Previous vectorization approaches
often consider the data space, in particular the innermost unit-strided loop.
It leads to the well-known data alignment conflict problem that vector loads
are overlapped due to the data sharing between continuous stencil computations.
This paper proposes a novel temporal vectorization scheme for stencils. It
vectorizes the stencil computation in the iteration space and assembles points
with different time coordinates in one vector. The temporal vectorization leads
to a small fixed number of vector reorganizations that is irrelevant to the
vector length, stencil order, and dimension. Furthermore, it is also applicable
to Gauss-Seidel stencils, whose vectorization is not well-studied. The
effectiveness of the temporal vectorization is demonstrated by various Jacobi
and Gauss-Seidel stencils.
"
2329,Data Engineering for HPC with Python,"  Data engineering is becoming an increasingly important part of scientific
discoveries with the adoption of deep learning and machine learning. Data
engineering deals with a variety of data formats, storage, data extraction,
transformation, and data movements. One goal of data engineering is to
transform data from original data to vector/matrix/tensor formats accepted by
deep learning and machine learning applications. There are many structures such
as tables, graphs, and trees to represent data in these data engineering
phases. Among them, tables are a versatile and commonly used format to load and
process data. In this paper, we present a distributed Python API based on table
abstraction for representing and processing data. Unlike existing
state-of-the-art data engineering tools written purely in Python, our solution
adopts high performance compute kernels in C++, with an in-memory table
representation with Cython-based Python bindings. In the core system, we use
MPI for distributed memory computations with a data-parallel approach for
processing large datasets in HPC clusters.
"
2330,Robust Ranking of Linear Algebra Algorithms via Relative Performance,"  For a given linear algebra problem, we consider those solution algorithms
that are mathematically equivalent to one another, and that mostly consist of a
sequence of calls to kernels from optimized libraries such as BLAS and LAPACK.
Although equivalent (at least in exact precision), those algorithms typically
exhibit significant differences in terms of performance, and naturally, we are
interested in finding the fastest one(s). In practice, we often observe that
multiple algorithms yield comparable performance characteristics. Therefore, we
aim to identify the subset of algorithms that are reliably faster than the
rest. To this end, instead of quantifying the performance of an algorithm in
absolute terms, we present a measurement-based approach that assigns a relative
score to the algorithms in comparison to one another. The relative performance
is encoded by sorting the algorithms based on pair-wise comparisons and ranking
them into equivalence classes, where more than one algorithm can obtain the
same rank. We show that the relative performance leads to robust identification
of the fastest algorithms, that is, reliable identifications even with noisy
system conditions
"
2331,"Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization
  Pragmas Using Bayesian Optimization","  An autotuning is an approach that explores a search space of possible
implementations/configurations of a kernel or an application by selecting and
evaluating a subset of implementations/configurations on a target platform
and/or use models to identify a high performance implementation/configuration.
In this paper, we develop an autotuning framework that leverages Bayesian
optimization to explore the parameter space search. We select six of the most
complex benchmarks from the application domains of the PolyBench benchmarks
(syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly
developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to
optimize them. We then use the autotuning framework to optimize the pragma
parameters to improve their performance. The experimental results show that our
autotuning approach outperforms the other compiling methods to provide the
smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and
covariance with two large datasets in 200 code evaluations for effectively
searching the parameter spaces with up to 170,368 different configurations. We
compare four different supervised learning methods within Bayesian optimization
and evaluate their effectiveness. We find that the Floyd-Warshall benchmark did
not benefit from autotuning because Polly uses heuristics to optimize the
benchmark to make it run much slower. To cope with this issue, we provide some
compiler option solutions to improve the performance.
"
2332,HARQ in Full-Duplex Relay-Assisted Transmissions for URLLC,"  The Release 16 completion unlocks the road to an exciting phase pertain to
the sixth generation (6G) era. Meanwhile, to sustain far-reaching applications
with unprecedented challenges in terms of latency and reliability, much
interest is already getting intensified toward physical layer specifications of
6G. In support of this vision, this work exhibits the forward-looking
perception of full-duplex (FD) cooperative relaying in support of upcoming
generations and adopts as a mean concern the critical contribution of hybrid
automatic repeat request (HARQ) mechanism to ultra-reliable and low-latency
communication (URLLC). Indeed, the HARQ roundtrip time (RTT) is known to
include basic physical delays that may cause the HARQ abandonment for the 1 ms
latency use case of URLLC. Taking up these challenges, this article proposes a
hybrid FD amplify-and-forward (AF)-selective decode-and-forward (SDF)
relay-based system for URLLC. Over this build system, two HARQ procedures
within which the HARQ RTT is shortened, are suggested to face latency and
reliability issues, namely, the conventional and the enhanced HARQ procedures.
We develop then an analytical framework of this relay based HARQ system within
its different procedures. Finally, using Monte-Carlo simulations, we confirm
the theoretical results and compare the proposed relay-assisted HARQ procedures
to the source-to-destination (S2D) HARQ-based system where no relay assists the
communication between the source and the destination.
"
2333,"Enabling Fast Differentially Private SGD via Just-in-Time Compilation
  and Vectorization","  A common pain point in differentially private machine learning is the
significant runtime overhead incurred when executing Differentially Private
Stochastic Gradient Descent (DPSGD), which may be as large as two orders of
magnitude. We thoroughly demonstrate that by exploiting powerful language
primitives, including vectorization, just-in-time compilation, and static graph
optimization, one can dramatically reduce these overheads, in many cases nearly
matching the best non-private running times. These gains are realized in two
frameworks: JAX and TensorFlow. JAX provides rich support for these primitives
as core features of the language through the XLA compiler. We also rebuild core
parts of TensorFlow Privacy, integrating features from TensorFlow 2 as well as
XLA compilation, granting significant memory and runtime improvements over the
current release version. These approaches allow us to achieve up to 50x
speedups in comparison to the best alternatives. Our code is available at
https://github.com/TheSalon/fast-dpsgd.
"
2334,"Trade-off between accuracy and tractability of network calculus in FIFO
  networks","  Computing accurate deterministic performance bounds is a strong need for
communication technologies having strong requirements on latency and
reliability. Beyond new scheduling protocols such as TSN, the FIFO policy
remains at work within each class of communication.
  In this paper, we focus on computing deterministic performance bounds in FIFO
networks in the network calculus framework. We propose a new algorithm based on
linear programming that presents a trade-off between accuracy and tractability.
This algorithm is first presented for tree networks. In a second time, we
generalize our approach and present a linear program for computing performance
bounds for arbitrary topologies, including cyclic dependencies. Finally, we
provide numerical results, both of toy examples and real topologies, to assess
the interest of our approach.
"
2335,Performance Assessment of OpenMP Compilers Targeting NVIDIA V100 GPUs,"  Heterogeneous systems are becoming increasingly prevalent. In order to
exploit the rich compute resources of such systems, robust programming models
are needed for application developers to seamlessly migrate legacy code from
today's systems to tomorrow's. Over the past decade and more, directives have
been established as one of the promising paths to tackle programmatic
challenges on emerging systems. This work focuses on applying and demonstrating
OpenMP offloading directives on five proxy applications. We observe that the
performance varies widely from one compiler to the other; a crucial aspect of
our work is reporting best practices to application developers who use OpenMP
offloading compilers. While some issues can be worked around by the developer,
there are other issues that must be reported to the compiler vendors. By
restructuring OpenMP offloading directives, we gain an 18x speedup for the su3
proxy application on NERSC's Cori system when using the Clang compiler, and a
15.7x speedup by switching max reductions to add reductions in the laplace
mini-app when using the Cray-llvm compiler on Cori.
"
2336,Evaluating the Cost of Atomic Operations on Modern Architectures,"  Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add
(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs
between these operations and various characteristics of such systems, such as
the structure of caches, are unclear and have not been thoroughly analyzed. In
this paper we establish an evaluation methodology, develop a performance model,
and present a set of detailed benchmarks for latency and bandwidth of different
atomics. We consider various state-of-the-art x86 architectures: Intel Haswell,
Xeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising
performance relationships between the considered atomics and architectural
properties such as the coherence state of the accessed cache lines. One key
finding is that all the tested atomics have comparable latency and bandwidth
even if they are characterized by different consensus numbers. Another insight
is that the hardware implementation of atomics prevents any instruction-level
parallelism even if there are no dependencies between the issued operations.
Finally, we discuss solutions to the discovered performance issues in the
analyzed architectures. Our analysis enables simpler and more effective
parallel programming and accelerates data processing on various architectures
deployed in both off-the-shelf machines and large compute systems.
"
2337,"Temporal blocking of finite-difference stencil operators with sparse
  ""off-the-grid"" sources","  Stencil kernels dominate a range of scientific applications including seismic
and medical imaging, image processing, and neural networks. Temporal blocking
is a performance optimisation that aims to reduce the required memory bandwidth
of stencil computations by re-using data from the cache for multiple time
steps. It has already been shown to be beneficial for this class of algorithms.
However, optimising stencils for practical applications remains challenging.
These computations often include sparsely located operators, not aligned with
the computational grid (""off-the-grid""). For example, our work is motivated by
sources that inject a wavefield and measurements interpolating grid values. The
resulting data dependencies make the adoption of temporal blocking much more
challenging. We propose a methodology to inspect these data dependencies and
reorder the computation, leading to performance gains in stencil codes where
temporal blocking has not been applicable. We implement this novel scheme in
the Devito domain-specific compiler toolchain. Devito implements a
domain-specific language embedded in Python to generate optimised partial
differential equation solvers using the finite-difference method from
high-level symbolic problem definitions. We evaluate our scheme using isotropic
acoustic, anisotropic acoustic and isotropic elastic wave propagators of
industrial significance. Performance evaluation, after auto-tuning, shows that
this enables substantial performance improvement through temporal blocking,
over highly-optimised vectorized spatially-blocked code of up to 1.6x.
"
2338,"Resource Management Schemes for Cloud-Native Platforms with Computing
  Containers of Docker and Kubernetes","  Businesses have made increasing adoption and incorporation of cloud
technology into internal processes in the last decade. The cloud-based
deployment provides on-demand availability without active management. More
recently, the concept of cloud-native application has been proposed and
represents an invaluable step toward helping organizations develop software
faster and update it more frequently to achieve dramatic business outcomes.
Cloud-native is an approach to build and run applications that exploit the
cloud computing delivery model's advantages. It is more about how applications
are created and deployed than where. The container-based virtualization
technology, such as Docker and Kubernetes, serves as the foundation for
cloud-native applications. This paper investigates the performance of two
popular computational-intensive applications, big data, and deep learning, in a
cloud-native environment. We analyze the system overhead and resource usage for
these applications. Through extensive experiments, we show that the completion
time reduces by up to 79.4% by changing the default setting and increases by up
to 96.7% due to different resource management schemes on two platforms.
Additionally, the resource release is delayed by up to 116.7% across different
systems. Our work can guide developers, administrators, and researchers to
better design and deploy their applications by selecting and configuring a
hosting platform.
"
2339,"Optimising the Performance of Convolutional Neural Networks across
  Computing Systems using Transfer Learning","  The choice of convolutional routines (primitives) to implement neural
networks has a tremendous impact on their inference performance (execution
speed) on a given hardware platform. To optimise a neural network by primitive
selection, the optimal primitive is identified for each layer of the network.
This process requires a lengthy profiling stage, iterating over all the
available primitives for each layer configuration, to measure their execution
time on the target platform. Because each primitive exploits the hardware in
different ways, new profiling is needed to obtain the best performance when
moving to another platform. In this work, we propose to replace this
prohibitively expensive profiling stage with a machine learning based approach
of performance modeling. Our approach speeds up the optimisation time
drastically. After training, our performance model can estimate the performance
of convolutional primitives in any layer configuration. The time to optimise
the execution of large neural networks via primitive selection is reduced from
hours to just seconds. Our performance model is easily transferable to other
target platforms. We demonstrate this by training a performance model on an
Intel platform and performing transfer learning to AMD and ARM processor
devices with minimal profiled samples.
"
2340,"Speculative Container Scheduling for Deep Learning Applications in a
  Kubernetes Cluster","  In the past decade, we have witnessed a dramatically increasing volume of
data collected from varied sources. The explosion of data has transformed the
world as more information is available for collection and analysis than ever
before. To maximize the utilization, various machine and deep learning models
have been developed, e.g. CNN [1] and RNN [2], to study data and extract
valuable information from different perspectives. While data-driven
applications improve countless products, training models for hyperparameter
tuning is still a time-consuming and resource-intensive process. Cloud
computing provides infrastructure support for the training of deep learning
applications. The cloud service providers, such as Amazon Web Services [3],
create an isolated virtual environment (virtual machines and containers) for
clients, who share physical resources, e.g., CPU and memory. On the cloud,
resource management schemes are implemented to enable better sharing among
users and boost the system-wide performance. However, general scheduling
approaches, such as spread priority and balanced resource schedulers, do not
work well with deep learning workloads. In this project, we propose SpeCon, a
novel container scheduler that is optimized for shortlived deep learning
applications. Based on virtualized containers, such as Kubernetes [4] and
Docker [5], SpeCon analyzes the common characteristics of training processes.
We design a suite of algorithms to monitor the progress of the training and
speculatively migrate the slow-growing models to release resources for
fast-growing ones. Specifically, the extensive experiments demonstrate that
SpeCon improves the completion time of an individual job by up to 41.5%, 14.8%
system-wide and 24.7% in terms of makespan.
"
2341,"Analysis and Verification of Relation between Digitizer's Sampling
  Properties and Energy Resolution of HPGe Detectors","  The CDEX (China Dark matter Experiment) aims at detection of WIMPs (Weakly
Interacting Massive Particles) and 0vbb (Neutrinoless double beta decay) of
76Ge. It now uses ~10 kg HPGe (High Purity Germanium) detectors in CJPL (China
Jinping Underground Laboratory). The energy resolution of detectors is
calculated via height spectrum of waveforms with 6-us shaping time. It is
necessary to know how sampling properties of a digitizer effect the energy
resolution. This paper will present preliminary energy resolution results of
waveforms at different sampling properties. The preliminary results show that
the ENOB (effective number of bits) with 8.25-bit or better can meet the energy
resolution @122keV of CDEX HPGe detectors. Based on the ADC (Analog-to-Digital
Converter) quantized error theory, this paper will also make a quantitative
analysis on energy resolution in CDEX HPGe detectors. It will provide guidance
for ADC design in full-chain cryogenic readout electronics for HPGe detectors.
"
2342,"Towards Co-execution on Commodity Heterogeneous Systems: Optimizations
  for Time-Constrained Scenarios","  Heterogeneous systems are present from powerful supercomputers, to mobile
devices, including desktop computers, thanks to their excellent performance and
energy consumption. The ubiquity of these architectures in both desktop systems
and medium-sized service servers allow enough variability to exploit a wide
range of problems, such as multimedia workloads, video encoding, image
filtering and inference in machine learning. Due to the heterogeneity, some
efforts have been done to reduce the programming effort and preserve
performance portability, but these systems include a set of challenges. The
context in which applications offload the workload along with the management
overheads introduced when doing co-execution, penalize the performance gains
under time-constrained scenarios. Therefore, this paper proposes optimizations
for the EngineCL runtime to reduce the penalization when co-executing in
commodity systems, as well as algorithmic improvements when load balancing. An
exhaustive experimental evaluation is performed, showing optimization
improvements of 7.5\% and 17.4\% for binary and ROI-based offloading modes,
respectively. Thanks to all the optimizations, the new load balancing algorithm
is always the most efficient scheduling configuration, achieving an average
efficiency of 0.84 under a pessimistic scenario.
"
2343,"Not Half Bad: Exploring Half-Precision in Graph Convolutional Neural
  Networks","  With the growing significance of graphs as an effective representation of
data in numerous applications, efficient graph analysis using modern machine
learning is receiving a growing level of attention. Deep learning approaches
often operate over the entire adjacency matrix -- as the input and intermediate
network layers are all designed in proportion to the size of the adjacency
matrix -- leading to intensive computation and large memory requirements as the
graph size increases. It is therefore desirable to identify efficient measures
to reduce both run-time and memory requirements allowing for the analysis of
the largest graphs possible. The use of reduced precision operations within the
forward and backward passes of a deep neural network along with novel
specialised hardware in modern GPUs can offer promising avenues towards
efficiency. In this paper, we provide an in-depth exploration of the use of
reduced-precision operations, easily integrable into the highly popular PyTorch
framework, and an analysis of the effects of Tensor Cores on graph
convolutional neural networks. We perform an extensive experimental evaluation
of three GPU architectures and two widely-used graph analysis tasks (vertex
classification and link prediction) using well-known benchmark and
synthetically generated datasets. Thus allowing us to make important
observations on the effects of reduced-precision operations and Tensor Cores on
computational and memory usage of graph convolutional neural networks -- often
neglected in the literature.
"
2344,"Differentiate Quality of Experience Scheduling for Deep Learning
  Applications with Docker Containers in the Cloud","  With the prevalence of big-data-driven applications, such as face recognition
on smartphones and tailored recommendations from Google Ads, we are on the road
to a lifestyle with significantly more intelligence than ever before. For
example, Aipoly Vision [1] is an object and color recognizer that helps the
blind, visually impaired, and color blind understand their surroundings. At the
back end side of their intelligence, various neural networks powered models are
running to enable quick responses to users. Supporting those models requires
lots of cloud-based computational resources, e.g. CPUs and GPUs. The cloud
providers charge their clients by the amount of resources that they occupied.
From clients' perspective, they have to balance the budget and quality of
experiences (e.g. response time). The budget leans on individual business
owners and the required Quality of Experience (QoE) depends on usage scenarios
of different applications, for instance, an autonomous vehicle requires
realtime response, but, unlocking your smartphone can tolerate delays. However,
cloud providers fail to offer a QoE based option to their clients. In this
paper, we propose DQoES, a differentiate quality of experience scheduler for
deep learning applications. DQoES accepts client's specification on targeted
QoEs, and dynamically adjust resources to approach their targets. Through
extensive, cloud-based experiments, DQoES demonstrates that it can schedule
multiple concurrent jobs with respect to various QoEs and achieve up to 8x
times more satisfied models compared to the existing system.
"
2345,"ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network
  Design in FPGA-based Systems","  The recent advances in machine learning, in general, and Artificial Neural
Networks (ANN), in particular, has made smart embedded systems an attractive
option for a larger number of application areas. However, the high
computational complexity, memory footprints, and energy requirements of machine
learning models hinder their deployment on resource-constrained embedded
systems. Most state-of-the-art works have considered this problem by proposing
various low bit-width data representation schemes, optimized arithmetic
operators' implementations, and different complexity reduction techniques such
as network pruning. To further elevate the implementation gains offered by
these individual techniques, there is a need to cross-examine and combine these
techniques' unique features. This paper presents ExPAN(N)D, a framework to
analyze and ingather the efficacy of the Posit number representation scheme and
the efficiency of fixed-point arithmetic implementations for ANNs. The Posit
scheme offers a better dynamic range and higher precision for various
applications than IEEE $754$ single-precision floating-point format. However,
due to the dynamic nature of the various fields of the Posit scheme, the
corresponding arithmetic circuits have higher critical path delay and resource
requirements than the single-precision-based arithmetic units. Towards this
end, we propose a novel Posit to fixed-point converter for enabling
high-performance and energy-efficient hardware implementations for ANNs with
minimal drop in the output accuracy. We also propose a modified Posit-based
representation to store the trained parameters of a network. Compared to an
$8$-bit fixed-point-based inference accelerator, our proposed implementation
offers $\approx46\%$ and $\approx18\%$ reductions in the storage requirements
of the parameters and energy consumption of the MAC units, respectively.
"
2346,Enhancing Cloud Storage with Shareable Instances for Social Computing,"  Cloud storage plays an important role in social computing. This paper aims to
develop a cloud storage management system for mobile devices to support an
extended set of file operations. Because of the limit of storage, bandwidth,
power consumption, and other resource restrictions, most existing cloud storage
apps for smartphones do not keep local copies of files. This efficient design,
however, limits the application capacities. In this paper, we attempt to extend
the available file operations for cloud storage service to better serve
smartphone users. We develop an efficient and secure file management system,
Skyfiles, to support more advanced file operations. The basic idea of our
design is to utilize cloud instances to assist file operations. Particularly,
Skyfiles supports downloading, compressing, encrypting, and converting
operations, as well as file transfer between two smartphone users' cloud
storage spaces. In addition, we propose a protocol for users to share their
idle instances. All file operations supported by Skyfiles can be efficiently
and securely accomplished with either a self-created instance or shared
instance.
"
2347,Building a SDN Enterprise WLAN Based on Virtual APs,"  In this letter the development and testing of an open enterprise Wi-Fi
solution based on virtual APs, managed by a central WLAN controller is
presented. It allows seamless handovers between APs in different channels,
maintaining the QoS of real-time services. The potential scalability issues
associated to the beacon generation and channel assignment have been addressed.
A battery of tests has been run in a real environment, and the results are
reported in terms of packet loss and delay.
"
2348,EdgeBench: A Workflow-based Benchmark for Edge Computing,"  Edge computing has been developed to utilize multiple tiers of resources for
privacy, cost and Quality of Service (QoS) reasons. Edge workloads have the
characteristics of data-driven and latency-sensitive. Because of this, edge
systems have developed to be both heterogeneous and distributed. The unique
characteristics of edge workloads and edge systems have motivated EdgeBench, a
workflow-based benchmark aims to provide the ability to explore the full design
space of edge workloads and edge systems. EdgeBench is both customizable and
representative. It allows users to customize the workflow logic of edge
workloads, the data storage backends, and the distribution of the individual
workflow stages to different computing tiers. To illustrate the usability of
EdgeBench, we also implements two representative edge workflows, a video
analytics workflow and an IoT hub workflow that represents two distinct but
common edge workloads. Both workflows are evaluated using the workflow-level
and function-level metrics reported by EdgeBench to illustrate both the
performance bottlenecks of the edge systems and the edge workloads.
"
2349,"Measurement-based coexistence studies of LAA & Wi-Fi deployments in
  Chicago","  LTE-Licensed Assisted Access (LAA) networks are beginning to be deployed
widely in major metropolitan areas in the US in the unlicensed 5 GHz bands,
which have existing dense deployments of Wi-Fi as well. Various aspects of the
coexistence scenarios such deployments give rise to have been considered ina
vast body of academic and industry research. However, there is very little data
and research on how these coexisting networks will behave in practice. The
question of fair coexistence between Wi-Fi and LAA has moved from a theoretical
question to reality. The recent roll-out of LAA deployments provides an
opportunity to collect data on the operation of these networks as well as
studying coexistence issues on the ground. In this paper we describe the first
results of a measurement campaign conducted over many months, using custom apps
as well as off-the-shelf tools, in several areas of Chicago where the major
carriers have been expanding LAA deployments. The measurements reveal that
coexistence between LAA and Wi-Fi in dense, urban environments where both
systems aggregate multiple channels, continues to be a challenging problem that
requires further research.
"
2350,Advanced Python Performance Monitoring with Score-P,"  Within the last years, Python became more prominent in the scientific
community and is now used for simulations, machine learning, and data analysis.
All these tasks profit from additional compute power offered by parallelism and
offloading. In the domain of High Performance Computing (HPC), we can look back
to decades of experience exploiting different levels of parallelism on the
core, node or inter-node level, as well as utilising accelerators. By using
performance analysis tools to investigate all these levels of parallelism, we
can tune applications for unprecedented performance. Unfortunately, standard
Python performance analysis tools cannot cope with highly parallel programs.
Since the development of such software is complex and error-prone, we
demonstrate an easy-to-use solution based on an existing tool infrastructure
for performance analysis. In this paper, we describe how to apply the
established instrumentation framework \scorep to trace Python applications. We
finish with a study of the overhead that users can expect for instrumenting
their applications.
"
2351,Self-Learning Threshold-Based Load Balancing,"  We consider a large-scale service system where incoming tasks have to be
instantaneously dispatched to one out of many parallel server pools. The
dispatcher uses a threshold for balancing the load and keeping the maximum
number of concurrent tasks across server pools low. We demonstrate that such a
policy is optimal on the fluid and diffusion scales for a suitable threshold
value, while only involving a small communication overhead. In order to set the
threshold optimally, it is important, however, to learn the load of the system,
which may be uncertain or even time-varying. For that purpose, we design a
control rule for tuning the threshold in an online manner. We provide
conditions which guarantee that this adaptive threshold settles at the optimal
value, along with estimates for the time until this happens.
"
2352,Poster: Benchmarking Financial Data Feed Systems,"  Data-driven solutions for the investment industry require event-based backend
systems to process high-volume financial data feeds with low latency, high
throughput, and guaranteed delivery modes.
  At vwd we process an average of 18 billion incoming event notifications from
500+ data sources for 30 million symbols per day and peak rates of 1+ million
notifications per second using custom-built platforms that keep audit logs of
every event.
  We currently assess modern open source event-processing platforms such as
Kafka, NATS, Redis, Flink or Storm for the use in our ticker plant to reduce
the maintenance effort for cross-cutting concerns and leverage hybrid
deployment models. For comparability and repeatability we benchmark candidates
with a standardized workload we derived from our real data feeds.
  We have enhanced an existing light-weight open source benchmarking tool in
its processing, logging, and reporting capabilities to cope with our workloads.
The resulting tool wrench can simulate workloads or replay snapshots in volume
and dynamics like those we process in our ticker plant. We provide the tool as
open source.
  As part of ongoing work we contribute details on (a) our workload and
requirements for benchmarking candidate platforms for financial feed
processing; (b) the current state of the tool wrench.
"
2353,"Experimental Analysis of Communication Relaying Delay in Low-Energy
  Ad-hoc Networks","  In recent years, more and more applications use ad-hoc networks for local M2M
communications, but in some cases such as when using WSNs, the software
processing delay induced by packets relaying may not be negligible. In this
paper, we planned and carried out a delay measurement experiment using
Raspberry Pi Zero W. The results demonstrated that, in low-energy ad-hoc
networks, processing delay of the application is always too large to ignore; it
is at least ten times greater than the kernel routing and corresponds to 30% of
the transmission delay. Furthermore, if the task is CPU-intensive, such as
packet encryption, the processing delay can be greater than the transmission
delay and its behavior is represented by a simple linear model. Our findings
indicate that the key factor for achieving QoS in ad-hoc networks is an
appropriate node-to-node load balancing that takes into account the CPU
performance and the amount of traffic passing through each node.
"
2354,"Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent
  Edge Devices","  The increasing computational cost of deep neural network models limits the
applicability of intelligent applications on resource-constrained edge devices.
While a number of neural network pruning methods have been proposed to compress
the models, prevailing approaches focus only on parametric operators (e.g.,
convolution), which may miss optimization opportunities. In this paper, we
present a novel fusion-catalyzed pruning approach, called FuPruner, which
simultaneously optimizes the parametric and non-parametric operators for
accelerating neural networks. We introduce an aggressive fusion method to
equivalently transform a model, which extends the optimization space of pruning
and enables non-parametric operators to be pruned in a similar manner as
parametric operators, and a dynamic filter pruning method is applied to
decrease the computational cost of models while retaining the accuracy
requirement. Moreover, FuPruner provides configurable optimization options for
controlling fusion and pruning, allowing much more flexible
performance-accuracy trade-offs to be made. Evaluation with state-of-the-art
residual neural networks on five representative intelligent edge platforms,
Jetson TX2, Jetson Nano, Edge TPU, NCS, and NCS2, demonstrates the
effectiveness of our approach, which can accelerate the inference of models on
CIFAR-10 and ImageNet datasets.
"
2355,"Effects of round-to-nearest and stochastic rounding in the numerical
  solution of the heat equation in low precision","  Motivated by the advent of machine learning, the last few years saw the
return of hardware-supported low-precision computing. Computations with fewer
digits are faster and more memory and energy efficient, but can be extremely
susceptible to rounding errors. An application that can largely benefit from
the advantages of low-precision computing is the numerical solution of partial
differential equations (PDEs), but a careful implementation and rounding error
analysis are required to ensure that sensible results can still be obtained.
  In this paper we study the accumulation of rounding errors in the solution of
the heat equation, a proxy for parabolic PDEs, via Runge-Kutta finite
difference methods using round-to-nearest (RtN) and stochastic rounding (SR).
We demonstrate how to implement the scheme to reduce rounding errors and we
derive \emph{a priori} estimates for local and global rounding errors. Let $u$
be the roundoff unit. While the worst-case local errors are $O(u)$ with respect
to the discretization parameters, the RtN and SR error behavior is
substantially different. We prove that the RtN solution is discretization,
initial condition and precision dependent, and always stagnates for small
enough $\Delta t$. Until stagnation, the global error grows like $O(u\Delta
t^{-1})$. In contrast, we show that the leading order errors introduced by SR
are zero-mean, independent in space and mean-independent in time, making SR
resilient to stagnation and rounding error accumulation. In fact, we prove that
for SR the global rounding errors are only $O(u\Delta t^{-1/4})$ in 1D and are
essentially bounded (up to logarithmic factors) in higher dimensions.
"
2356,"An analytic performance model for overlapping execution of memory-bound
  loop kernels on multicore CPUs","  Complex applications running on multicore processors show a rich performance
phenomenology. The growing number of cores per ccNUMA domain complicates
performance analysis of memory-bound code since system noise, load imbalance,
or task-based programming models can lead to thread desynchronization. Hence,
the simplifying assumption that all cores execute the same loop can not be
upheld. Motivated by observations on plain and modified versions of the HPCG
benchmark, we construct a performance model of execution of memory-bound loop
kernels. It can predict the memory bandwidth share per kernel on a memory
contention domain depending on the number of active cores and which other
workload the kernel is paired with. The only code features required are the
single-thread cache line access frequency per kernel, which is directly related
to the single-thread memory bandwidth, and its saturated bandwidth. It can
either be measured directly or predicted using the Execution-Cache-Memory (ECM)
performance model. The computational intensity of the kernels and the detailed
structure of the code is of no significance. We validate our model on Intel
Broadwell, Intel Cascade Lake, and AMD Rome processors pairing various
streaming and stencil kernels. The error in predicting the bandwidth share per
kernel is less than 8%.
"
2357,10 Years Later: Cloud Computing is Closing the Performance Gap,"  Large scale modeling and simulation problems, from nanoscale materials to
universe-scale cosmology, have in the past used the massive computing resources
of High-Performance Computing (HPC) systems. Over the last decade, cloud
computing has gained popularity for business applications and increasingly for
computationally intensive machine learning problems. Despite the prolific
literature, the question remains open whether cloud computing can provide
HPC-competitive performance for a wide range of scientific applications. The
answer to this question is crucial in guiding the design of future systems and
providing access to high-performance resources to a broadened community. Here
we present a multi-level approach to identifying the performance gap between
HPC and cloud computing and to isolate several variables that contribute to
this gap by dividing our experiments into (i) hardware and system
microbenchmarks and (ii) user applications. Our results show that today's
high-end cloud computing can deliver HPC-like performance - at least at modest
scales - not only for computationally intensive applications, but also for
memory- and communication-intensive applications, thanks to the high-speed
memory systems and interconnects and dedicated batch scheduling now available
on some cloud platforms.
"
2358,"An End-to-End ML System for Personalized Conversational Voice Models in
  Walmart E-Commerce","  Searching for and making decisions about products is becoming increasingly
easier in the e-commerce space, thanks to the evolution of recommender systems.
Personalization and recommender systems have gone hand-in-hand to help
customers fulfill their shopping needs and improve their experiences in the
process. With the growing adoption of conversational platforms for shopping, it
has become important to build personalized models at scale to handle the large
influx of data and perform inference in real-time. In this work, we present an
end-to-end machine learning system for personalized conversational voice
commerce. We include components for implicit feedback to the model, model
training, evaluation on update, and a real-time inference engine. Our system
personalizes voice shopping for Walmart Grocery customers and is currently
available via Google Assistant, Siri and Google Home devices.
"
2359,Proximity Based Load Balancing Policies on Graphs: A Simulation Study,"  Distributed load balancing is the act of allocating jobs among a set of
servers as evenly as possible. There are mainly two versions of the load
balancing problem that have been studied in the literature: static and dynamic.
The static interpretation leads to formulating the load balancing problem as a
case with jobs (balls) never leaving the system and accumulating at the servers
(bins) whereas the dynamic setting deals with the case when jobs arrive and
leave the system after service completion. This paper designs and evaluates
server proximity aware job allocation policies for treating load balancing
problems with a goal to reduce the communication cost associated with the jobs.
We consider a class of proximity aware Power of Two (POT) choice based
assignment policies for allocating jobs to servers, where servers are
interconnected as an n-vertex graph G(V, E). For the static version, we assume
each job arrives at one of the servers, u. For the dynamic setting, we assume G
to be a circular graph and job arrival process at each server is described by a
Poisson point process with the job service time exponentially distributed. For
both settings, we then assign each job to the server with minimum load among
servers u and v where v is chosen according to one of the following two
policies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop
neighborhood of u (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood
of u with probability proportional to the inverse square of the distance
between u and v. Our simulation results show that both the policies
consistently produce a load distribution which is much similar to that of a
classical proximity oblivious POT policy.
"
2360,"Solving large number of non-stiff, low-dimensional ordinary differential
  equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT
  and DifferentialEquations.jl","  In this paper, the performance characteristics of different solution
techniques and program packages to solve a large number of independent ordinary
differential equation systems is examined. The employed hardware are an Intel
Core i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores
and an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak
double-precision performance. The tested systems (Lorenz equation,
Keller--Miksis equation and a pressure relief valve model) are non-stiff and
have low dimension. Thus, the performance of the codes are not limited by
memory bandwidth, and Runge--Kutta type solvers are efficient and suitable
choices. The tested program packages are MPGOS written in C++ and specialised
only for GPUs; ODEINT implemented in C++, which supports execution on both CPUs
and GPUs; finally, DifferentialEquations.jl written in Julia that also supports
execution on both CPUs and GPUs. Using GPUs, the program package MPGOS is
superior. For CPU computations, the ODEINT program package has the best
performance.
"
2361,"No more 996: Understanding Deep Learning Inference Serving with an
  Automatic Benchmarking System","  Deep learning (DL) models have become core modules for many applications.
However, deploying these models without careful performance benchmarking that
considers both hardware and software's impact often leads to poor service and
costly operational expenditure. To facilitate DL models' deployment, we
implement an automatic and comprehensive benchmark system for DL developers. To
accomplish benchmark-related tasks, the developers only need to prepare a
configuration file consisting of a few lines of code. Our system, deployed to a
leader server in DL clusters, will dispatch users' benchmark jobs to follower
workers. Next, the corresponding requests, workload, and even models can be
generated automatically by the system to conduct DL serving benchmarks.
Finally, developers can leverage many analysis tools and models in our system
to gain insights into the trade-offs of different system configurations. In
addition, a two-tier scheduler is incorporated to avoid unnecessary
interference and improve average job compilation time by up to 1.43x
(equivalent of 30\% reduction). Our system design follows the best practice in
DL clusters operations to expedite day-to-day DL service evaluation efforts by
the developers. We conduct many benchmark experiments to provide in-depth and
comprehensive evaluations. We believe these results are of great values as
guidelines for DL service configuration and resource allocation.
"
2362,"Simulation-Based Performance Prediction of HPC Applications: A Case
  Study of HPL","  We propose a simulation-based approach for performance modeling of parallel
applications on high-performance computing platforms. Our approach enables
full-system performance modeling: (1) the hardware platform is represented by
an abstract yet high-fidelity model; (2) the computation and communication
components are simulated at a functional level, where the simulator allows the
use of the components native interface; this results in a (3) fast and accurate
simulation of full HPC applications with minimal modifications to the
application source code. This hardware/software hybrid modeling methodology
allows for low overhead, fast, and accurate exascale simulation and can be
easily carried out on a standard client platform (desktop or laptop). We
demonstrate the capability and scalability of our approach with High
Performance LINPACK (HPL), the benchmark used to rank supercomputers in the
TOP500 list. Our results show that our modeling approach can accurately and
efficiently predict the performance of HPL at the scale of the TOP500 list
supercomputers. For instance, the simulation of HPL on Frontera takes less than
five hours with an error rate of four percent.
"
2363,On the Analysis of Spatially Constrained Power of Two Choice Policies,"  We consider a class of power of two choice based assignment policies for
allocating users to servers, where both users and servers are located on a
two-dimensional Euclidean plane. In this framework, we investigate the inherent
tradeoff between the communication cost, and load balancing performance of
different allocation policies. To this end, we first design and evaluate a
Spatial Power of two (sPOT) policy in which each user is allocated to the least
loaded server among its two geographically nearest servers sequentially. When
servers are placed on a two-dimensional square grid, sPOT maps to the classical
Power of two (POT) policy on the Delaunay graph associated with the Voronoi
tessellation of the set of servers. We show that the associated Delaunay graph
is 4-regular and provide expressions for asymptotic maximum load using results
from the literature. For uniform placement of servers, we map sPOT to a
classical balls and bins allocation policy with bins corresponding to the
Voronoi regions associated with the second order Voronoi diagram of the set of
servers. We provide expressions for the lower bound on the asymptotic expected
maximum load on the servers and prove that sPOT does not achieve POT load
balancing benefits. However, experimental results suggest the efficacy of sPOT
with respect to expected communication cost. Finally, we propose two
non-uniform server sampling based POT policies that achieve the best of both
the performance metrics. Experimental results validate the effctiveness of our
proposed policies.
"
2364,"Power-Aware Run-Time Scheduler for Mixed-Criticality Systems on
  Multi-Core Platform","  In modern multi-core Mixed-Criticality (MC) systems, a rise in peak power
consumption due to parallel execution of tasks with maximum frequency,
specially in the overload situation, may lead to thermal issues, which may
affect the reliability and timeliness of MC systems. Therefore, managing peak
power consumption has become imperative in multi-core MC systems. In this
regard, we propose an online peak power and thermal management heuristic for
multi-core MC systems. This heuristic reduces the peak power consumption of the
system as much as possible during runtime by exploiting dynamic slack and
per-cluster Dynamic Voltage and Frequency Scaling (DVFS). Specifically, our
approach examines multiple tasks ahead to determine the most appropriate one
for slack assignment, that has the most impact on the system peak power and
temperature. However, changing the frequency and selecting a proper task for
slack assignment and a proper core for task re-mapping at runtime can be
time-consuming and may cause deadline violation which is not admissible for
high-criticality tasks. Therefore, we analyze and then optimize our run-time
scheduler and evaluate it for various platforms. The proposed approach is
experimentally validated on the ODROID-XU3 (DVFS-enabled heterogeneous
multi-core platform) with various embedded real-time benchmarks. Results show
that our heuristic achieves up to 5.25% reduction in system peak power and
20.33\% reduction in maximum temperature compared to an existing method while
meeting deadline constraints in different criticality modes.
"
2365,"An approach to define Very High Capacity Networks with improved quality
  at an affordable cost","  This paper aims to propose one possible approach in the setting of VHCNs
(Very High Capacity Networks) performance targets that should be capable of
promoting efficient investments for operators and, at the same time, improving
the benefits for end-users. To this aim, we suggest relying on some specific
KPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth
as perceived by the customer - valid at the application layer, instead of the
physical layer data-rate. In this regard, the paper underlines that the
bandwidth perceived is strictly linked to the latency. The most important
implication is that some of the most demanding services envisaged for the
future (e.g., mobile virtual and augmented reality, tactile internet) cannot be
met by merely increasing the low-level protocol data-rate. Therefore, for the
VHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory
pre-requisite.
"
2366,"Towards Latency-aware DNN Optimization with GPU Runtime Analysis and
  Tail Effect Elimination","  Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the
increasing computational cost makes them very challenging to meet real-time
latency and accuracy requirements. Although DNN runtime latency is dictated by
model property (e.g., architecture, operations), hardware property (e.g.,
utilization, throughput), and more importantly, the effective mapping between
these two, many existing approaches focus only on optimizing model property
such as FLOPS reduction and overlook the mismatch between DNN model and
hardware properties. In this work, we show that the mismatch between the varied
DNN computation workloads and GPU capacity can cause the idle GPU tail effect,
leading to GPU under-utilization and low throughput. As a result, the FLOPs
reduction cannot bring effective latency reduction, which causes sub-optimal
accuracy versus latency trade-offs. Motivated by this, we propose a GPU
runtime-aware DNN optimization methodology to eliminate such GPU tail effect
adaptively on GPU platforms. Our methodology can be applied on top of existing
SOTA DNN optimization approaches to achieve better latency and accuracy
trade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy
improvement over several SOTA DNN pruning and NAS methods, respectively
"
2367,Runtime Performances Benchmark for Knowledge Graph Embedding Methods,"  This paper wants to focus on providing a characterization of the runtime
performances of state-of-the-art implementations of KGE alghoritms, in terms of
memory footprint and execution time. Despite the rapidly growing interest in
KGE methods, so far little attention has been devoted to their comparison and
evaluation; in particular, previous work mainly focused on performance in terms
of accuracy in specific tasks, such as link prediction. To this extent, a
framework is proposed for evaluating available KGE implementations against
graphs with different properties, with a particular focus on the effectiveness
of the adopted optimization strategies. Graphs and models have been trained
leveraging different architectures, in order to enlighten features and
properties of both models and the architectures they have been trained on. Some
results enlightened with experiments in this document are the fact that
multithreading is efficient, but benefit deacreases as the number of threads
grows in case of CPU. GPU proves to be the best architecture for the given
task, even if CPU with some vectorized instructions still behaves well.
Finally, RAM utilization for the loading of the graph never changes between
different architectures and depends only on the type of graph, not on the
model.
"
2368,"Assessing the Feasibility of Web-Request Prediction Models on Mobile
  Platforms","  Prefetching web pages is a well-studied solution to reduce network latency by
predicting users' future actions based on their past behaviors. However, such
techniques are largely unexplored on mobile platforms. Today's privacy
regulations make it infeasible to explore prefetching with the usual strategy
of amassing large amounts of data over long periods and constructing
conventional, ""large"" prediction models. Our work is based on the observation
that this may not be necessary: Given previously reported mobile-device usage
trends (e.g., repetitive behaviors in brief bursts), we hypothesized that
prefetching should work effectively with ""small"" models trained on mobile-user
requests collected during much shorter time periods. To test this hypothesis,
we constructed a framework for automatically assessing prediction models, and
used it to conduct an extensive empirical study based on over 15 million HTTP
requests collected from nearly 11,500 mobile users during a 24-hour period,
resulting in over 7 million models. Our results demonstrate the feasibility of
prefetching with small models on mobile platforms, directly motivating future
work in this area. We further introduce several strategies for improving
prediction models while reducing the model size. Finally, our framework
provides the foundation for future explorations of effective prediction models
across a range of usage scenarios.
"
2369,"Resource Allocation in One-dimensional Distributed Service Networks with
  Applications","  We consider assignment policies that allocate resources to users, where both
resources and users are located on a one-dimensional line. First, we consider
unidirectional assignment policies that allocate resources only to users
located to their left. We propose the Move to Right (MTR) policy, which scans
from left to right assigning nearest rightmost available resource to a user,
and contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While
both policies among all unidirectional policies, minimize the expected distance
traveled by a request (request distance), MTR is fairer. Moreover, we show that
when user and resource locations are modeled by statistical point processes,
and resources are allowed to satisfy more than one user, the spatial system
under unidirectional policies can be mapped into bulk service queueing systems,
thus allowing the application of many queueing theory results that yield closed
form expressions. As we consider a case where different resources can satisfy
different numbers of users, we also generate new results for bulk service
queues. We also consider bidirectional policies where there are no directional
restrictions on resource allocation and develop an algorithm for computing the
optimal assignment which is more efficient than known algorithms in the
literature when there are more resources than users. Numerical evaluation of
performance of unidirectional and bidirectional allocation schemes yields
design guidelines beneficial for resource placement. \np{Finally, we present a
heuristic algorithm, which leverages the optimal dynamic programming scheme for
one-dimensional inputs to obtain approximate solutions to the optimal
assignment problem for the two-dimensional scenario and empirically yields
request distances within a constant factor of the optimal solution.
"
2370,Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture,"  Stencils represent a class of computational patterns where an output grid
point depends on a fixed shape of neighboring points in an input grid. Stencil
computations are prevalent in scientific applications engaging a significant
portion of supercomputing resources. Therefore, it has been always important to
optimize stencil programs for the best performance. A rich body of research has
focused on optimizing stencil computations on almost all parallel
architectures. Stencil applications have regular dependency patterns, inherent
pipeline-parallelism, and plenty of data reuse. This makes these applications a
perfect match for a coarse-grained reconfigurable spatial architecture (CGRA).
A CGRA consists of many simple, small processing elements (PEs) connected with
an on-chip network. Each PE can be configured to execute part of a stencil
computation and all PEs run in parallel; the network can also be configured so
that data loaded can be passed from a PE to a neighbor PE directly and thus
reused by many PEs without register spilling and memory traffic. How to
efficiently map a stencil computation to a CGRA is the key to performance. In
this paper, we show a few unique and generalizable ways of mapping one- and
multidimensional stencil computations to a CGRA, fully exploiting the data
reuse opportunities and parallelism. Our simulation experiments demonstrate
that these mappings are efficient and enable the CGRA to outperform
state-of-the-art GPUs.
"
2371,"Optimizing the Age-of-Information for Mobile Users in Adversarial and
  Stochastic Environments","  We study a multi-user downlink scheduling problem for optimizing the
freshness of information available to users roaming across multiple cells. We
consider both adversarial and stochastic settings and design scheduling
policies that optimize two distinct information freshness metrics, namely the
average age-of-information and the peak age-of-information. We show that a
natural greedy scheduling policy is competitive with the optimal offline policy
in the adversarial setting. We also derive fundamental lower bounds to the
competitive ratio achievable by any online policy. In the stochastic
environment, we show that a Max-Weight scheduling policy that takes into
account the channel statistics achieves an approximation factor of $2$ for
minimizing the average age of information in two extreme mobility scenarios. We
conclude the paper by establishing a large-deviation optimality result achieved
by the greedy policy for minimizing the peak age of information for static
users situated at a single cell.
"
2372,"DLFusion: An Auto-Tuning Compiler for Layer Fusion on Deep Neural
  Network Accelerator","  Many hardware vendors have introduced specialized deep neural networks (DNN)
accelerators owing to their superior performance and efficiency. As such, how
to generate and optimize the code for the hardware accelerator becomes an
important yet less explored problem. In this paper, we perform the
compiler-stage optimization study using a novel and representative Cambricon
DNN accelerator and demonstrate that the code optimization knobs play an
important role in unleashing the potential of hardware computational
horsepower. However, even only two studied code optimization knobs, namely the
number of cores and layer fusion scheme, present an enormous search space that
prevents the naive brute-force search. This work introduces a joint,
auto-tuning optimization framework to address this challenge. We first use a
set of synthesized DNN layers to study the interplay between the hardware
performance and layer characteristics. Based on the insights, we extract the
operation count and feature map channel size as each layer's characteristics
and derive a joint optimization strategy to decide the performance-optimal core
number and fusion scheme. We evaluate the performance of the proposed approach
using a set of representative DNN models and show that it achieves the minimal
of 3.6x and the maximal of 7.9x performance speedup compared to no optimization
baseline. We also show that the achieved speedup is close to the oracle case
that is based on a reduced brute-force search but with much less search time.
"
2373,"Utilizing Ensemble Learning for Performance and Power Modeling and
  Improvement of Parallel Cancer Deep Learning CANDLE Benchmarks","  Machine learning (ML) continues to grow in importance across nearly all
domains and is a natural tool in modeling to learn from data. Often a tradeoff
exists between a model's ability to minimize bias and variance. In this paper,
we utilize ensemble learning to combine linear, nonlinear, and tree-/rule-based
ML methods to cope with the bias-variance tradeoff and result in more accurate
models. Hardware performance counter values are correlated with properties of
applications that impact performance and power on the underlying system. We use
the datasets collected for two parallel cancer deep learning CANDLE benchmarks,
NT3 (weak scaling) and P1B2 (strong scaling), to build performance and power
models based on hardware performance counters using single-object and
multiple-objects ensemble learning to identify the most important counters for
improvement. Based on the insights from these models, we improve the
performance and energy of P1B2 and NT3 by optimizing the deep learning
environments TensorFlow, Keras, Horovod, and Python under the huge page size of
8 MB on the Cray XC40 Theta at Argonne National Laboratory. Experimental
results show that ensemble learning not only produces more accurate models but
also provides more robust performance counter ranking. We achieve up to 61.15%
performance improvement and up to 62.58% energy saving for P1B2 and up to
55.81% performance improvement and up to 52.60% energy saving for NT3 on up to
24,576 cores.
"
2374,"Performance and Power Modeling and Prediction Using MuMMI and Ten
  Machine Learning Methods","  In this paper, we use modeling and prediction tool MuMMI (Multiple Metrics
Modeling Infrastructure) and ten machine learning methods to model and predict
performance and power and compare their prediction error rates. We use a
fault-tolerant linear algebra code and a fault-tolerant heat distribution code
to conduct our modeling and prediction study on the Cray XC40 Theta and IBM
BG/Q Mira at Argonne National Laboratory and the Intel Haswell cluster Shepard
at Sandia National Laboratories. Our experiment results show that the
prediction error rates in performance and power using MuMMI are less than 10%
for most cases. Based on the models for runtime, node power, CPU power, and
memory power, we identify the most significant performance counters for
potential optimization efforts associated with the application characteristics
and the target architectures, and we predict theoretical outcomes of the
potential optimizations. When we compare the prediction accuracy using MuMMI
with that using 10 machine learning methods, we observe that MuMMI not only
results in more accurate prediction in both performance and power but also
presents how performance counters impact the performance and power models. This
provides some insights about how to fine-tune the applications and/or systems
for energy efficiency.
"
2375,"Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for
  Emerging Storage Models","  With data durability, high access speed, low power efficiency and byte
addressability, NVMe and SSD, which are acknowledged representatives of
emerging storage technologies, have been applied broadly in many areas.
However, one key issue with high-performance adoption of these technologies is
how to properly define intelligent cache layers such that the performance gap
between emerging technologies and main memory can be well bridged. To this end,
we propose Phoebe, a reuse-aware reinforcement learning framework for the
optimal online caching that is applicable for a wide range of emerging storage
models. By continuous interacting with the cache environment and the data
stream, Phoebe is capable to extract critical temporal data dependency and
relative positional information from a single trace, becoming ever smarter over
time. To reduce training overhead during online learning, we utilize periodical
training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud
storage workloads. Experiment results show that Phoebe is able to close the gap
of cache miss rate from LRU and a state-of-the-art online learning based cache
policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.
"
2376,"RL-QN: A Reinforcement Learning Framework for Optimal Control of
  Queueing Systems","  With the rapid advance of information technology, network systems have become
increasingly complex and hence the underlying system dynamics are often unknown
or difficult to characterize. Finding a good network control policy is of
significant importance to achieve desirable network performance (e.g., high
throughput or low delay). In this work, we consider using model-based
reinforcement learning (RL) to learn the optimal control policy for queueing
networks so that the average job delay (or equivalently the average queue
backlog) is minimized. Traditional approaches in RL, however, cannot handle the
unbounded state spaces of the network control problem. To overcome this
difficulty, we propose a new algorithm, called Reinforcement Learning for
Queueing Networks (RL-QN), which applies model-based RL methods over a finite
subset of the state space, while applying a known stabilizing policy for the
rest of the states. We establish that the average queue backlog under RL-QN
with an appropriately constructed subset can be arbitrarily close to the
optimal result. We evaluate RL-QN in dynamic server allocation, routing and
switching problems. Simulation results show that RL-QN minimizes the average
queue backlog effectively.
"
2377,Performance Analysis of an Interference-Limited RIS-Aided Network,"  In this work, the performance of reconfigurable intelligent surface
(RIS)-aided communication systems corrupted by the co-channel interference
(CCI) at the destination is investigated. Assuming Rayleigh fading and
equal-power CCI, we present the analysis for the outage probability (OP),
average bit error rate (BER), and ergodic capacity. In addition, an asymptotic
outage analysis is carried in order to obtain further insights. Our analysis
shows that the number of reflecting elements as well as the number of
interferers have a great impact on the overall system performance.
"
2378,Tools for modelling and simulating the Smart Grid,"  The Smart Grid (SG) is a Cyber-Physical System (CPS) considered a critical
infrastructure divided into cyber (software) and physical (hardware)
counterparts that complement each other. It is responsible for timely power
provision wrapped by Information and Communication Technologies (ICT) for
handling bi-directional energy flows in electric power grids. Enacting control
and performance over the massive infrastructure of the SG requires convenient
analysis methods. Modelling and simulation (M&S) is a performance evaluation
technique used to study virtually any system by testing designs and
artificially creating 'what-if' scenarios for system reasoning and advanced
analysis. M&S avoids stressing the actual physical infrastructure and systems
in production by addressing the problem in a purely computational perspective.
Present work compiles a non-exhaustive list of tools for M&S of interest when
tackling SG capabilities. Our contribution is to delineate available options
for modellers when considering power systems in combination with ICT. We also
show the auxiliary tools and details of most relevant solutions pointing out
major features and combinations over the years.
"
2379,A Model of Polarization on Social Media Caused by Empathy and Repulsion,"  In recent years, the ease with which social media can be accessed has led to
the unexpected problem of a shrinkage in information sources. This phenomenon
is caused by a system that facilitates the connection of people with similar
ideas and recommendation systems. Bias in the selection of information sources
promotes polarization that divides people into multiple groups with opposing
views and creates conflicts between opposing groups. This paper elucidates the
mechanism of polarization by proposing a model of opinion formation in social
media that considers users' reactions of empathy and repulsion. Based on the
idea that opinion neutrality is only relative, this model offers a novel
technology for dealing with polarization.
"
2380,"Optimizing Graph Processing and Preprocessing with Hardware Assisted
  Propagation Blocking","  Extensive prior research has focused on alleviating the characteristic poor
cache locality of graph analytics workloads. However, graph pre-processing
tasks remain relatively unexplored. In many important scenarios, graph
pre-processing tasks can be as expensive as the downstream graph analytics
kernel. We observe that Propagation Blocking (PB), a software optimization
designed for SpMV kernels, generalizes to many graph analytics kernels as well
as common pre-processing tasks. In this work, we identify the lingering
inefficiencies of a PB execution on conventional multicores and propose
architecture support to eliminate PB's bottlenecks, further improving the
performance gains from PB. Our proposed architecture -- COBRA -- optimizes the
PB execution of both graph processing and pre-processing alike to provide
end-to-end speedups of up to 4.6x (3.5x on average).
"
2381,Automatic Microprocessor Performance Bug Detection,"  Processor design validation and debug is a difficult and complex task, which
consumes the lion's share of the design process. Design bugs that affect
processor performance rather than its functionality are especially difficult to
catch, particularly in new microarchitectures. This is because, unlike
functional bugs, the correct processor performance of new microarchitectures on
complex, long-running benchmarks is typically not deterministically known.
Thus, when performance benchmarking new microarchitectures, performance teams
may assume that the design is correct when the performance of the new
microarchitecture exceeds that of the previous generation, despite significant
performance regressions existing in the design. In this work, we present a
two-stage, machine learning-based methodology that is able to detect the
existence of performance bugs in microprocessors. Our results show that our
best technique detects 91.5% of microprocessor core performance bugs whose
average IPC impact across the studied applications is greater than 1% versus a
bug-free design with zero false positives. When evaluated on memory system
bugs, our technique achieves 100% detection with zero false positives.
Moreover, the detection is automatic, requiring very little performance
engineer time.
"
2382,Ginkgo -- A Math Library designed for Platform Portability,"  The first associations to software sustainability might be the existence of a
continuous integration (CI) framework; the existence of a testing framework
composed of unit tests, integration tests, and end-to-end tests; and also the
existence of software documentation. However, when asking what is a common
deathblow for a scientific software product, it is often the lack of platform
and performance portability. Against this background, we designed the Ginkgo
library with the primary focus on platform portability and the ability to not
only port to new hardware architectures, but also achieve good performance. In
this paper we present the Ginkgo library design, radically separating
algorithms from hardware-specific kernels forming the distinct hardware
executors, and report our experience when adding execution backends for NVIDIA,
AMD, and Intel GPUs. We also comment on the different levels of performance
portability, and the performance we achieved on the distinct hardware backends.
"
2383,Performance Analysis of Dual-Hop Mixed PLC/RF Communication Systems,"  In this paper, we study a dual-hop mixed power line communication and
radio-frequency communication (PLC/RF) system, where the connection between the
PLC link and the RF link is made by a decode-and-forward (DF) or
amplify-and-forward (AF) relay. Assume that the PLC channel is affected by both
additive background noise and impulsive noise suffers from Log-normal fading,
while the RF link undergoes Rician fading. Based on this model, analytical
expressions of the outage probability (OP), average bit error rate (BER), and
the average channel capacity (ACC) are derived. Furthermore, an asymptotic
analysis for the OP and average BER, as well as an upper bound expression for
the ACC are presented. At last, numerical results are developed to validate our
analytical results, and in-depth discussions are conducted.
"
2384,On the Performance of RIS-Assisted Dual-Hop Mixed RF-UWOC Systems,"  In this paper, we investigate the performance of a reconfigurable intelligent
surface (RIS)-assisted dual-hop mixed radio-frequency underwater wireless
optical communication (RF-UWOC) system. An RIS is an emerging and low-cost
technology that aims to enhance the strength of the received signal, thus
improving the system performance. In the considered system setup, a ground
source does not have a reliable direct link to a given marine buoy and
communicates with it through an RIS installed on a building. In particular, the
buoy acts as a relay that sends the signal to an underwater destination. In
this context, analytical expressions for the outage probability (OP), average
bit error rate (ABER), and average channel capacity (ACC) are derived assuming
fixed-gain amplify-and-forward (AF) and decode-and-forward (DF) relaying
protocols at the marine buoy. Moreover, asymptotic analyses of the OP and ABER
are carried out in order to gain further insights from the analytical
frameworks. In particular, the system diversity order is derived and it is
shown to depend on the RF link parameters and on the detection schemes of the
UWOC link. Finally, it is demonstrated that RIS-assisted systems can
effectively improve the performance of mixed dual-hop RF-UWOC systems.
"
2385,Performance Analysis of UAV-based Mixed RF-UWOC Transmission Systems,"  In this paper, we investigate the performance of a mixed
radio-frequency-underwater wireless optical communication (RF-UWOC) system
where an unmanned aerial vehicle (UAV), as a low-altitude mobile aerial base
station, transmits information to an autonomous underwater vehicle (AUV)
through a fixed-gain amplify-and-forward (AF) or decode-and-forward (DF) relay.
Our analysis accounts for the main factors that affect the system performance,
such as the UAV height, air bubbles, temperature gradient, water salinity
variations, and detection techniques. Employing fixed-gain AF relaying and DF
relaying, we derive closed-form expressions for some key performance metrics,
e.g., outage probability (OP), average bit error rate (ABER), and average
channel capacity (ACC). In addition, in order to get further insights,
asymptotic analyses for the OP and ABER are also carried out. Furthermore,
assuming DF relaying, we derive analytical expressions for the optimal UAV
altitude that minimizes the OP. Simulation results show that the UAV altitude
influences the system performance and there is an optimal altitude which
ensures a minimum OP. Moreover, based on the asymptotic results, it is
demonstrated that the diversity order of fixed-gain AF relaying and DF relaying
are respectively determined by the RF link and by the detection techniques of
the UWOC link.
"
2386,"FedEval: A Benchmark System with a Comprehensive Evaluation Model for
  Federated Learning","  As an innovative solution for privacy-preserving machine learning (ML),
federated learning (FL) is attracting much attention from research and industry
areas. While new technologies proposed in the past few years do evolve the FL
area, unfortunately, the evaluation results presented in these works fall short
in integrity and are hardly comparable because of the inconsistent evaluation
metrics and the lack of a common platform. In this paper, we propose a
comprehensive evaluation framework for FL systems. Specifically, we first
introduce the ACTPR model, which defines five metrics that cannot be excluded
in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy,
and Robustness. Then we design and implement a benchmarking system called
FedEval, which enables the systematic evaluation and comparison of existing
works under consistent experimental conditions. We then provide an in-depth
benchmarking study between the two most widely-used FL mechanisms, FedSGD and
FedAvg. The benchmarking results show that FedSGD and FedAvg both have
advantages and disadvantages under the ACTPR model. For example, FedSGD is
barely influenced by the none independent and identically distributed (non-IID)
data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our
experiments. On the other hand, FedAvg is more efficient than FedSGD regarding
time consumption and communication. Lastly, we excavate a set of take-away
conclusions, which are very helpful for researchers in the FL area.
"
2387,heSRPT: Parallel Scheduling to Minimize Mean Slowdown,"  Modern data centers serve workloads which are capable of exploiting
parallelism. When a job parallelizes across multiple servers it will complete
more quickly, but jobs receive diminishing returns from being allocated
additional servers. Because allocating multiple servers to a single job is
inefficient, it is unclear how best to allocate a fixed number of servers
between many parallelizable jobs. This paper provides the first optimal
allocation policy for minimizing the mean slowdown of parallelizable jobs of
known size when all jobs are present at time 0. Our policy provides a simple
closed form formula for the optimal allocations at every moment in time.
Minimizing mean slowdown usually requires favoring short jobs over long ones
(as in the SRPT policy). However, because parallelizable jobs have sublinear
speedup functions, system efficiency is also an issue. System efficiency is
maximized by giving equal allocations to all jobs and thus competes with the
goal of prioritizing small jobs. Our optimal policy, high-efficiency SRPT
(heSRPT), balances these competing goals. heSRPT completes jobs according to
their size order, but maintains overall system efficiency by allocating some
servers to each job at every moment in time. Our results generalize to also
provide the optimal allocation policy with respect to mean flow time. Finally,
we consider the online case where jobs arrive to the system over time. While
optimizing mean slowdown in the online setting is even more difficult, we find
that heSRPT provides an excellent heuristic policy for the online setting. In
fact, our simulations show that heSRPT significantly outperforms
state-of-the-art allocation policies for parallelizable jobs.
"
2388,"A Bounded Multi-Vacation Queue Model for Multi-stage Sleep Control 5G
  Base station","  Modelling and control of energy consumption is an important problem in
telecommunication systems.To model such systems, this paper publishes a bounded
multi-vacation queue model. The energy consumption predicted by the model shows
an average error rate of 0.0177 and the delay predicted by the model shows an
average error rate of 0.0655 over 99 test instances.Subsequently, an
optimization algorithm is proposed to minimize the energy consumption while not
violate the delay bound. Furthermore, given current state of art 5G base
station system configuration, numerical results shows that with the increase of
traffic load, energy saving rate becomes less.
"
2389,"Non-equilibrium Surface Growth and Scalability of Parallel Algorithms
  for Large Asynchronous Systems","  The scalability of massively parallel algorithms is a fundamental question in
computer science. We study the scalability and the efficiency of a conservative
massively parallel algorithm for discrete-event simulations where the discrete
events are Poisson arrivals. The parallel algorithm is applicable to a wide
range of problems, including dynamic Monte Carlo simulations for large
asynchronous systems with short-range interactions. The evolution of the
simulated time horizon is analogous to a growing and fluctuating surface, and
the efficiency of the algorithm corresponds to the density of local minima of
this surface. In one dimension we find that the steady state of the macroscopic
landscape is governed by the Edwards-Wilkinson Hamiltonian, which implies that
the algorithm is scalable. Preliminary results for higher-dimensional logical
topologies are discussed.
"
2390,"Going through Rough Times: from Non-Equilibrium Surface Growth to
  Algorithmic Scalability","  Efficient and faithful parallel simulation of large asynchronous systems is a
challenging computational problem. It requires using the concept of local
simulated times and a synchronization scheme. We study the scalability of
massively parallel algorithms for discrete-event simulations which employ
conservative synchronization to enforce causality. We do this by looking at the
simulated time horizon as a complex evolving system, and we identify its
universal characteristics. We find that the time horizon for the conservative
parallel discrete-event simulation scheme exhibits Kardar-Parisi-Zhang-like
kinetic roughening. This implies that the algorithm is asymptotically scalable
in the sense that the average progress rate of the simulation approaches a
non-zero constant. It also implies, however, that there are diverging memory
requirements associated with such schemes.
"
2391,"Solaris System Resource Manager: All I Ever Wanted Was My Unfair
  Advantage (And Why You Can't Have It!)","  Traditional UNIX time-share schedulers attempt to be fair to all users by
employing a round-robin style algorithm for allocating CPU time. Unfortunately,
a loophole exists whereby the scheduler can be biased in favor of a greedy user
running many short CPU-time processes. This loophole is not a defect but an
intrinsic property of the round-robin scheduler that ensures responsiveness to
the short CPU demands associated with multiple interactive users. A new
generation of UNIX system resource management software constrains the scheduler
to be equitable to all users regardless of the number of processes each may be
running. This ""fair-share"" scheduling draws on the concept of pro rating
resource ""shares"" across users and groups and then dynamically adjusting CPU
usage to meet those share proportions. The simple notion of statically
allocating these shares, however, belies the potential consequences for
performance as measured by user response time and service level targets. We
demonstrate this point by modeling several simple share allocation scenarios
and analyzing the corresponding performance effects. A brief comparison of
commercial system resource management implementations from HP, IBM, and SUN is
also given.
"
2392,UNIX Resource Managers: Capacity Planning and Resource Issues,"  The latest implementations of commercial UNIX to offer mainframe style
capacity management on enterprise servers include: AIX Workload Manager (WLM),
HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well
as SGI and Compaq. The ability to manage server capacity is achieved by making
significant modifications to the standard UNIX operating system so that
processes are inherently tied to specific users. Those users, in turn, are
granted only a certain fraction of system resources. Resource usage is
monitored and compared with each users grant to ensure that the assigned
entitlement constraints are met. In this paper, we begin by clearing up some of
the confusion that has surrounded the motivation and the terminology behind the
new technology. The common theme across each of the commercial implementations
is the introduction of the fair-share scheduler. After reviewing some potential
performance pitfalls, we present capacity planning guidelines for migrating to
automated UNIX resource management.
"
2393,The X-Files: Investigating Alien Performance in a Thin-client World,"  Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools.
"
2394,"Multicast-based Architecture for IP Mobility: Simulation Analysis and
  Comparison with Basic Mobile IP","  With the introduction of a newer generation of wireless devices and
technologies, the need for an efficient architecture for IP mobility is
becoming more apparent. Several architectures have been proposed to support IP
mobility. Most studies, however, show that current architectures, in general,
fall short from satisfying the performance requirements for wireless
applications, mainly audio. Other studies have shown performance improvement by
using multicast to reduce latency and packet loss during handoff. In this
study, we propose a multicast-based architecture to support IP mobility. We
evaluate our approach through simulation, and we compare it to mainstream
approaches for IP mobility, mainly, the Mobile IP protocol. Comparison is
performed according to the required performance criteria, such as smooth
handoff and efficient routing.
  Our simulation results show significant improvement for the proposed
architecture. On average, basic Mobile IP consumes almost twice as much network
bandwidth, and experiences more than twice as much end-to-end and handoff
delays, as does our proposed architecture. Furthermore, we propose an extension
to Mobile IP to support our architecture with minimal modification.
"
2395,"Efficient cache use for stencil operations on structured discretization
  grids","  We derive tight bounds on cache misses for evaluation of explicit stencil
operators on structured grids. Our lower bound is based on the isoperimetrical
property of the discrete octahedron. Our upper bound is based on good surface
to volume ratio of a parallelepiped spanned by a reduced basis of the inter-
ference lattice of a grid. Measurements show that our algorithm typically
reduces the number of cache misses by factor of three relative to a compiler
optimized code. We show that stencil calculations on grids whose interference
lattice have a short vector feature abnormally high numbers of cache misses. We
call such grids unfavorable and suggest to avoid these in computations by
appropriate padding. By direct measurements on MIPS R10000 we show a good
correlation of abnormally high cache misses and unfavorable three-dimensional
grids.
"
2396,The Role of Commutativity in Constraint Propagation Algorithms,"  Constraint propagation algorithms form an important part of most of the
constraint programming systems. We provide here a simple, yet very general
framework that allows us to explain several constraint propagation algorithms
in a systematic way. In this framework we proceed in two steps. First, we
introduce a generic iteration algorithm on partial orderings and prove its
correctness in an abstract setting. Then we instantiate this algorithm with
specific partial orderings and functions to obtain specific constraint
propagation algorithms.
  In particular, using the notions commutativity and semi-commutativity, we
show that the {\tt AC-3}, {\tt PC-2}, {\tt DAC} and {\tt DPC} algorithms for
achieving (directional) arc consistency and (directional) path consistency are
instances of a single generic algorithm. The work reported here extends and
simplifies that of Apt \citeyear{Apt99b}.
"
2397,Performance and Scalability Models for a Hypergrowth e-Commerce Web Site,"  The performance of successful Web-based e-commerce services has all the
allure of a roller-coaster ride: accelerated fiscal growth combined with the
ever-present danger of running out of server capacity. This chapter presents a
case study based on the author's own capacity planning engagement with one of
the hottest e-commerce Web sites in the world. Several spreadsheet techniques
are presented for forecasting both short-term and long-term trends in the
consumption of server capacity. Two new performance metrics are introduced for
site planning and procurement: the effective demand, and the doubling period.
"
2398,"Faster-than-light effects and negative group delays in optics and
  electronics, and their applications","  Recent manifestations of apparently faster-than-light effects confirmed our
predictions that the group velocity in transparent optical media can exceed c.
Special relativity is not violated by these phenomena. Moreover, in the
electronic domain, the causality principle does not forbid negative group
delays of analytic signals in electronic circuits, in which the peak of an
output pulse leaves the exit port of a circuit before the peak of the input
pulse enters the input port. Furthermore, pulse distortion for these
superluminal analytic signals can be negligible in both the optical and
electronic domains. Here we suggest an extension of these ideas to the
microelectronic domain. The underlying principle is that negative feedback can
be used to produce negative group delays. Such negative group delays can be
used to cancel out the positive group delays due to transistor latency (e.g.,
the finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),
as well as the propagation delays due to the interconnects between transistors.
Using this principle, it is possible to speed up computer systems.
"
2399,Search in Power-Law Networks,"  Many communication and social networks have power-law link distributions,
containing a few nodes which have a very high degree and many with low degree.
The high connectivity nodes play the important role of hubs in communication
and networking, a fact which can be exploited when designing efficient search
algorithms. We introduce a number of local search strategies which utilize high
degree nodes in power-law graphs and which have costs which scale sub-linearly
with the size of the graph. We also demonstrate the utility of these strategies
on the Gnutella peer-to-peer network.
"
2400,Analysis of Network Traffic in Switched Ethernet Systems,"  A 100 Mbps Ethernet link between a college campus and the outside world was
monitored with a dedicated PC and the measured data analysed for its
statistical properties. Similar measurements were taken at an internal node of
the network. The networks in both cases are a full-duplex switched Ethernet.
Inter-event interval histograms and power spectra of the throughput aggregated
for 10ms bins were used to analyse the measured traffic. For most investigated
cases both methods reveal that the traffic behaves according to a power law.
The results will be used in later studies to parameterise models for network
traffic.
"
2401,Experiences with advanced CORBA services,"  The Common Object Request Broker Architecture (CORBA) is successfully used in
many control systems (CS) for data transfer and device modeling. Communication
rates below 1 millisecond, high reliability, scalability, language independence
and other features make it very attractive. For common types of applications
like error logging, alarm messaging or slow monitoring, one can benefit from
standard CORBA services that are implemented by third parties and save
tremendous amount of developing time. We have started using few CORBA services
on our previous CORBA-based control system for the light source ANKA [1] and
use now several CORBA services for the ALMA Common Software (ACS) [2], the core
of the control system of the Atacama Large Millimeter Array. Our experiences
with the interface repository (IFR), the implementation repository, the naming
service, the property service, telecom log service and the notify service from
different vendors are presented. Performance and scalability benchmarks have
been performed.
"
2402,Hypernets -- Good (G)news for Gnutella,"  Criticism of Gnutella network scalability has rested on the bandwidth
attributes of the original interconnection topology: a Cayley tree. Trees, in
general, are known to have lower aggregate bandwidth than higher dimensional
topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support
thousands to millions of peers. Studies of interconnection topologies in the
literature, however, have focused on hardware implementations which are limited
by cost to a few thousand nodes. Since the Gnutella network is virtual,
hyper-topologies are relatively unfettered by such constraints. We present
performance models for several plausible hyper-topologies and compare their
query throughput up to millions of peers. The virtual hypercube and the virtual
hypertorus are shown to offer near linear scalability subject to the number of
peer TCP/IP connections that can be simultaneously kept open.
"
2403,"Weaves: A Novel Direct Code Execution Interface for Parallel High
  Performance Scientific Codes","  Scientific codes are increasingly being used in compositional settings,
especially problem solving environments (PSEs). Typical compositional modeling
frameworks require significant buy-in, in the form of commitment to a
particular style of programming (e.g., distributed object components). While
this solution is feasible for newer generations of component-based scientific
codes, large legacy code bases present a veritable software engineering
nightmare. We introduce Weaves a novel framework that enables modeling,
composition, direct code execution, performance characterization, adaptation,
and control of unmodified high performance scientific codes. Weaves is an
efficient generalized framework for parallel compositional modeling that is a
proper superset of the threads and processes models of programming. In this
paper, our focus is on the transparent code execution interface enabled by
Weaves. We identify design constraints, their impact on implementation
alternatives, configuration scenarios, and present results from a prototype
implementation on Intel x86 architectures.
"
2404,"Minimizing Cache Misses in Scientific Computing Using Isoperimetric
  Bodies","  A number of known techniques for improving cache performance in scientific
computations involve the reordering of the iteration space. Some of these
reorderings can be considered coverings of the iteration space with sets having
small surface-to-volume ratios. Use of such sets may reduce the number of cache
misses in computations of local operators having the iteration space as their
domain. First, we derive lower bounds on cache misses that any algorithm must
suffer while computing a local operator on a grid. Then, we explore coverings
of iteration spaces of structured and unstructured discretization grid
operators which allow us to approach these lower bounds. For structured grids
we introduce a covering by successive minima tiles based on the interference
lattice of the grid. We show that the covering has a small surface-to-volume
ratio and present a computer experiment showing actual reduction of the cache
misses achieved by using these tiles. For planar unstructured grids we show
existence of a covering which reduces the number of cache misses to the level
of that of structured grids. Next, we introduce a class of multidimensional
grids, called starry grids in this paper. These grids represent an abstraction
of unstructured grids used in, for example, molecular simulations and the
solution of partial differential equations. We show that starry grids can be
covered by sets having a low surface-to-volume ratio and, hence have the same
cache efficiency as structured grids. Finally, we present a triangulation of a
three-dimensional cube that has the property that any local operator on the
corresponding grid must incur a significantly larger number of cache misses
than a similar operator on a structured grid of the same size.
"
2405,A generalization of Amdahl's law and relative conditions of parallelism,"  In this work I present a generalization of Amdahl's law on the limits of a
parallel implementation with many processors. In particular I establish some
mathematical relations involving the number of processors and the dimension of
the treated problem, and with these conditions I define, on the ground of the
reachable speedup, some classes of parallelism for the implementations. I also
derive a condition for obtaining superlinear speedup. The used mathematical
technics are those of differential calculus. I describe some examples from
classical problems offered by the specialized literature on the subject.
"
2406,A New Interpretation of Amdahl's Law and Geometric Scalability,"  The multiprocessor effect refers to the loss of computing cycles due to
processing overhead. Amdahl's law and the Multiprocessing Factor (MPF) are two
scaling models used in industry and academia for estimating multiprocessor
capacity in the presence of this multiprocessor effect. Both models express
different laws of diminishing returns. Amdahl's law identifies diminishing
processor capacity with a fixed degree of serialization in the workload, while
the MPF model treats it as a constant geometric ratio. The utility of both
models for performance evaluation stems from the presence of a single parameter
that can be determined easily from a small set of benchmark measurements. This
utility, however, is marred by a dilemma. The two models produce different
results, especially for large processor configurations that are so important
for today's applications. The question naturally arises: Which of these two
models is the correct one to use? Ignoring this question merely reduces
capacity prediction to arbitrary curve-fitting. Removing the dilemma requires a
dynamical interpretation of these scaling models. We present a physical
interpretation based on queueing theory and show that Amdahl's law corresponds
to synchronous queueing in a bus model while the MPF model belongs to a Coxian
server model. The latter exhibits unphysical effects such as sublinear response
times hence, we caution against its use for large multiprocessor
configurations.
"
2407,Narses: A Scalable Flow-Based Network Simulator,"  Most popular, modern network simulators, such as ns, are targeted towards
simulating low-level protocol details. These existing simulators are not
intended for simulating large distributed applications with many hosts and many
concurrent connections over long periods of simulated time. We introduce a new
simulator, Narses, targeted towards large distributed applications. The goal of
Narses is to simulate and validate large applications efficiently using network
models of varying levels of detail. We introduce several simplifying
assumptions that allow our simulator to scale to the needs of large distributed
applications while maintaining a reasonable degree of accuracy. Initial results
show up to a 45 times speedup while consuming 28% of the memory used by ns.
Narses maintains a reasonable degree of accuracy -- within 8% on average.
"
2408,Modelling Delay Jitter in Voice over IP,"  It has been suggested in voice over IP that an appropriate choice of the
distribution used in modeling the delay jitters, can improve the play-out
algorithm. In this paper, we propose a tool using which, one can determine, at
a given instance, which distribution model best explains the jitter
distribution. This is done using Expectation Maximization, to choose amongst
possible distribution models which include, the i.i.d exponential distribution,
the gamma distribution etc.
"
2409,"A Performance Study of Monitoring and Information Services for
  Distributed Systems","  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems.
"
2410,A Monitoring System for the BaBar INFN Computing Cluster,"  Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines.
"
2411,"Performance comparison between iSCSI and other hardware and software
  solutions","  We report on our investigations on some technologies that can be used to
build disk servers and networks of disk servers using commodity hardware and
software solutions. It focuses on the performance that can be achieved by these
systems and gives measured figures for different configurations.
  It is divided into two parts : iSCSI and other technologies and hardware and
software RAID solutions.
  The first part studies different technologies that can be used by clients to
access disk servers using a gigabit ethernet network. It covers block access
technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for
different numbers of clients and servers.
  The second part compares a system based on 3ware hardware RAID controllers, a
system using linux software RAID and IDE cards and a system mixing both
hardware RAID and software RAID. Performance measurements for reading and
writing are given for different RAID levels.
"
2412,"GridMonitor: Integration of Large Scale Facility Fabric Monitoring with
  Meta Data Service in Grid Environment","  Grid computing consists of the coordinated use of large sets of diverse,
geographically distributed resources for high performance computation.
Effective monitoring of these computing resources is extremely important to
allow efficient use on the Grid. The large number of heterogeneous computing
entities available in Grids makes the task challenging. In this work, we
describe a Grid monitoring system, called GridMonitor, that captures and makes
available the most important information from a large computing facility. The
Grid monitoring system consists of four tiers: local monitoring, archiving,
publishing and harnessing. This architecture was applied on a large scale linux
farm and network infrastructure. It can be used by many higher-level Grid
services including scheduling services and resource brokering.
"
2413,Worldwide Fast File Replication on Grid Datafarm,"  The Grid Datafarm architecture is designed for global petascale
data-intensive computing. It provides a global parallel filesystem with online
petascale storage, scalable I/O bandwidth, and scalable parallel processing,
and it can exploit local I/O in a grid of clusters with tens of thousands of
nodes. One of features is that it manages file replicas in filesystem metadata
for fault tolerance and load balancing.
  This paper discusses and evaluates several techniques to support
long-distance fast file replication. The Grid Datafarm manages a ranked group
of files as a Gfarm file, each file, called a Gfarm file fragment, being stored
on a filesystem node, or replicated on several filesystem nodes. Each Gfarm
file fragment is replicated independently and in parallel using rate-controlled
HighSpeed TCP with network striping. On a US-Japan testbed with 10,000 km
distance, we achieve 419 Mbps using 2 nodes on each side, and 741 Mbps using 4
nodes out of 893 Mbps with two transpacific networks.
"
2414,Efficient Instrumentation for Performance Profiling,"  Performance profiling consists of tracing a software system during execution
and then analyzing the obtained traces. However, traces themselves affect the
performance of the system distorting its execution. Therefore, there is a need
to minimize the effect of the tracing on the underlying system's performance.
To achieve this, the trace set needs to be optimized according to the
performance profiling problem being solved. Our position is that such
minimization can be achieved only by adding the software trace design and
implementation to the overall software development process. In such a process,
the performance analyst supplies the knowledge of performance measurement
requirements, while the software developer supplies the knowledge of the
software. Both of these are needed for an optimal trace placement.
"
2415,Using Propagation for Solving Complex Arithmetic Constraints,"  Solving a system of nonlinear inequalities is an important problem for which
conventional numerical analysis has no satisfactory method. With a
box-consistency algorithm one can compute a cover for the solution set to
arbitrarily close approximation. Because of difficulties in the use of
propagation for complex arithmetic expressions, box consistency is computed
with interval arithmetic. In this paper we present theorems that support a
simple modification of propagation that allows complex arithmetic expressions
to be handled efficiently. The version of box consistency that is obtained in
this way is stronger than when interval arithmetic is used.
"
2416,A Performance Analysis Tool for Nokia Mobile Phone Software,"  Performance problems are often observed in embedded software systems. The
reasons for poor performance are frequently not obvious. Bottlenecks can occur
in any of the software components along the execution path. Therefore it is
important to instrument and monitor the different components contributing to
the runtime behavior of an embedded software system. Performance analysis tools
can help locate performance bottlenecks in embedded software systems by
monitoring the software's execution and producing easily understandable
performance data. We maintain and further develop a tool for analyzing the
performance of Nokia mobile phone software. The user can select among four
performance analysis reports to be generated: average processor load, processor
utilization, task execution time statistics, and task execution timeline. Each
of these reports provides important information about where execution time is
being spent. The demo will show how the tool helps to identify performance
bottlenecks in Nokia mobile phone software and better understand areas of poor
performance.
"
2417,Improving TCP/IP Performance over Wireless IEEE 802.11 Link,"  Cellular phones, wireless laptops, personal portable devices that supports
both voice and data access are all examples of communicating devices that uses
wireless communication. Sine TCP/IP (and UDP) is the dominant technology in use
in the internet, it is expected that they will be used (and they are currently)
over wireless connections. In this paper, we investigate the performance of the
TCP (and UDP) over IEEE802.11 wireless MAC protocol. We investigate the
performance of the TCP and UDP assuming three different traffic patterns. First
bulk transmission where the main concern is the throughput. Second real-time
audio (using UDP) in the existence of bulk TCP transmission where the main
concern is the packet loss for audio traffic. Finally web traffic where the
main concern is the response time. We also investigate the effect of using
forward Error Correction (FEC) technique and the MAC sublayer parameters on the
throughput and response time.
"
2418,Performance of TCP/UDP under Ad Hoc IEEE802.11,"  TCP is the De facto standard for connection oriented transport layer
protocol, while UDP is the De facto standard for transport layer protocol,
which is used with real time traffic for audio and video. Although there have
been many attempts to measure and analyze the performance of the TCP protocol
in wireless networks, very few research was done on the UDP or the interaction
between TCP and UDP traffic over the wireless link. In this paper, we tudy the
performance of TCP and UDP over IEEE802.11 ad hoc network. We used two
topologies, a string and a mesh topology. Our work indicates that IEEE802.11 as
a ad-hoc network is not very suitable for bulk transfer using TCP. It also
indicates that it is much better for real-time audio. Although one has to be
careful here since real-time audio does require much less bandwidth than the
wireless link bandwidth. Careful and detailed studies are needed to further
clarify that issue.
"
2419,"Benchmarking and Implementation of Probability-Based Simulations on
  Programmable Graphics Cards","  The latest Graphics Processing Units (GPUs) are reported to reach up to
  200 billion floating point operations per second (200 Gflops) and to have
price performance of 0.1 cents per M flop. These facts raise great interest in
the plausibility of extending the GPUs' use to non-graphics applications, in
particular numerical simulations on structured grids (lattice).
  We review previous work on using GPUs for non-graphics applications,
implement probability-based simulations on the GPU, namely the
  Ising and percolation models, implement vector operation benchmarks for the
GPU, and finally compare the CPU's and GPU's performance.
  A general conclusion from the results obtained is that moving computations
from the CPU to the GPU is feasible, yielding good time and price performance,
for certain lattice computations.
  Preliminary results also show that it is feasible to use them in parallel
"
2420,"Analysis of Implementation Hierocrypt-3 algorithm (and its comparison to
  Camellia algorithm) using ALTERA devices","  Alghoritms: HIEROCRYPT-3, CAMELLIA and ANUBIS, GRAND CRU, NOEKEON, NUSH, Q,
RC6, SAFER++128, SC2000, SHACAL were requested for the submission of block
ciphers (high level block cipher) to NESSIE (New European Schemes for
Signatures, Integrity, and Encryption) project. The main purpose of this
project was to put forward a portfolio of strong cryptographic primitives of
various types. The NESSIE project was a three year long project and has been
divided into two phases. The first was finished in June 2001r. CAMELLIA, RC6,
SAFER++128 and SHACAL were accepted for the second phase of the evaluation
process. HIEROCRYPT-3 had key schedule problems, and there were attacks for up
to 3,5 rounds out of 6, at least hardware implementations of this cipher were
extremely slow [12]. HIEROCRYPT-3 was not selected to Phase II. CAMELLIA was
selected as an algorithm suggested for future standard. In the paper we present
the hardware implementations these two algorithms with 128-bit blocks and
128-bit keys, using ALTERA devices and their comparisons.
"
2421,On the Security of the Yi-Tan-Siew Chaos-Based Cipher,"  This paper presents a comprehensive analysis on the security of the
Yi-Tan-Siew chaotic cipher proposed in [IEEE TCAS-I 49(12):1826-1829 (2002)]. A
differential chosen-plaintext attack and a differential chosen-ciphertext
attack are suggested to break the sub-key K, under the assumption that the time
stamp can be altered by the attacker, which is reasonable in such attacks.
Also, some security Problems about the sub-keys $\alpha$ and $\beta$ are
clarified, from both theoretical and experimental points of view. Further
analysis shows that the security of this cipher is independent of the use of
the chaotic tent map, once the sub-key $K$ is removed via the proposed
suggested differential chosen-plaintext attack.
"
2422,A Quick Look at SATA Disk Performance,"  We have been investigating the use of low-cost, commodity components for
multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are
white box PCs containing the largest ATA drives, value-priced AMD or Intel
processors, and inexpensive ECC memory. One issue has been the wiring mess, air
flow problems, length restrictions, and connector failures created by seven or
more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount
chassis. Large capacity Serial ATA (SATA) drives have recently become widely
available for the PC environment at a reasonable price. In addition to being
faster, the SATA connectors seem more reliable, have a more reasonable length
restriction (1m) and allow better airflow. We tested two drive brands along
with two RAID controllers to evaluate SATA drive performance and reliablility.
This paper documents our results so far.
"
2423,"Characterization of the Burst Stabilization Protocol for the RR/RR CICQ
  Switch","  Input buffered switches with Virtual Output Queueing (VOQ) can be unstable
when presented with unbalanced loads. Existing scheduling algorithms, including
iSLIP for Input Queued (IQ) switches and Round Robin (RR) for Combined Input
and Crossbar Queued (CICQ) switches, exhibit instability for some schedulable
loads. We investigate the use of a queue length threshold and bursting
mechanism to achieve stability without requiring internal speed-up. An
analytical model is developed to prove that the burst stabilization protocol
achieves stability and to predict the minimum burst value needed as a function
of offered load. The analytical model is shown to have very good agreement with
simulation results. These results show the advantage of the RR/RR CICQ switch
as a contender for the next generation of high-speed switches.
"
2424,"Performance Evaluation of Packet-to-Cell Segmentation Schemes in Input
  Buffered Packet Switches","  Most input buffered packet switches internally segment variable-length
packets into fixed-length cells. The last cell in a segmented packet will
contain overhead bytes if the packet length is not evenly divisible by the cell
length. Switch speed-up is used to compensate for this overhead. In this paper,
we develop an analytical model of a single-server queue where an input stream
of packets is segmented into cells for service. Analytical models are developed
for M/M/1, M/H2/1, and M/E2/1 queues with a discretized (or quantized) service
time. These models and simulation using real packet traces are used to evaluate
the effect of speed-up on mean queue length. We propose and evaluate a new
method of segmenting a packet trailer and subsequent packet header into a
single cell. This cell merging method reduces the required speed-up. No changes
to switch-matrix scheduling algorithms are needed. Simulation with a packet
trace shows a reduction in the needed speed-up for an iSLIP scheduled input
buffered switch.
"
2425,"On the Practicality of Intrinsic Reconfiguration As a Fault Recovery
  Method in Analog Systems","  Evolvable hardware combines the powerful search capability of evolutionary
algorithms with the flexibility of reprogrammable devices, thereby providing a
natural framework for reconfiguration. This framework has generated an interest
in using evolvable hardware for fault-tolerant systems because reconfiguration
can effectively deal with hardware faults whenever it is impossible to provide
spares. But systems cannot tolerate faults indefinitely, which means
reconfiguration does have a deadline. The focus of previous evolvable hardware
research relating to fault-tolerance has been primarily restricted to restoring
functionality, with no real consideration of time constraints. In this paper we
are concerned with evolvable hardware performing reconfiguration under deadline
constraints. In particular, we investigate reconfigurable hardware that
undergoes intrinsic evolution. We show that fault recovery done by intrinsic
reconfiguration has some restrictions, which designers cannot ignore.
"
2426,Elements for Response Time Statistics in ERP Transaction Systems,"  We present some measurements and ideas for response time statistics in ERP
systems. It is shown that the response time distribution of a given transaction
in a given system is generically a log-normal distribution or, in some
situations, a sum of two or more log-normal distributions. We present some
arguments for this form of the distribution based on heuristic rules for
response times, and we show data from performance measurements in actual
systems to support the log-normal form. Deviations of the log-normal form can
often be traced back to performance problems in the system. Consequences for
the interpretation of response time data and for service level agreements are
discussed.
"
2427,Benchmarking Blunders and Things That Go Bump in the Night,"  Benchmarking; by which I mean any computer system that is driven by a
controlled workload, is the ultimate in performance testing and simulation.
Aside from being a form of institutionalized cheating, it also offer countless
opportunities for systematic mistakes in the way the workloads are applied and
the resulting measurements interpreted. Right test, wrong conclusion is a
ubiquitous mistake that happens because test engineers tend to treat data as
divine. Such reverence is not only misplaced, it's also a sure ticket to
production hell when the application finally goes live. I demonstrate how such
mistakes can be avoided by means of two war stories that are real WOPRs. (a)
How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks
can go flat with too much Java juice. In each case I present simple performance
models and show how they can be applied to correctly assess benchmark data.
"
2428,Business Process Measures,"  The paper proposes a new methodology for defining business process measures
and their computation. The approach is based on metamodeling according to MOF.
Especially, a metamodel providing precise definitions of typical process
measures for UML activity diagram-like notation is proposed, including precise
definitions how measures should be aggregated for composite process elements.
The proposed approach allows defining values in a natural way, and measurement
of data, which are of interest to business, without deep investigation into
specific technical solutions. This provides new possibilities for business
process measurement, decreasing the gap between technical solutions and asset
management methodologies.
"
2429,"Performance Analysis of the Globus Toolkit Monitoring and Discovery
  Service, MDS2","  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the monitoring system,
and help evaluate future development work. To this end, we examined the
performance of the Globus Toolkit(reg. trdmrk) Monitoring and Discovery Service
(MDS2) by instrumenting its main services using NetLogger. Our study shows a
strong advantage to caching or prefetching the data, as well as the need to
have primary components at well-connected sites.
"
2430,Roaming Real-Time Applications - Mobility Services in IPv6 Networks,"  Emerging mobility standards within the next generation Internet Protocol,
IPv6, promise to continuously operate devices roaming between IP networks.
Associated with the paradigm of ubiquitous computing and communication, network
technology is on the spot to deliver voice and videoconferencing as a standard
internet solution. However, current roaming procedures are too slow, to remain
seamless for real-time applications. Multicast mobility still waits for a
convincing design. This paper investigates the temporal behaviour of mobile
IPv6 with dedicated focus on topological impacts. Extending the hierarchical
mobile IPv6 approach we suggest protocol improvements for a continuous
handover, which may serve bidirectional multicast communication, as well. Along
this line a multicast mobility concept is introduced as a service for clients
and sources, as they are of dedicated importance in multipoint conferencing
applications. The mechanisms introduced do not rely on assumptions of any
specific multicast routing protocol in use.
"
2431,"Performance Analysis of Multicast Mobility in a Hierarchical Mobile IP
  Proxy Environment","  Mobility support in IPv6 networks is ready for release as an RFC, stimulating
major discussions on improvements to meet real-time communication requirements.
Sprawling hot spots of IP-only wireless networks at the same time await voice
and videoconferencing as standard mobile Internet services, thereby adding the
request for multicast support to real-time mobility. This paper briefly
introduces current approaches for seamless multicast extensions to Mobile IPv6.
Key issues of multicast mobility are discussed. Both analytically and in
simulations comparisons are drawn between handover performance characteristics,
dedicating special focus on the M-HMIPv6 approach.
"
2432,"Parallel Computing Environments and Methods for Power Distribution
  System Simulation","  The development of cost-effective highperformance parallel computing on
multi-processor supercomputers makes it attractive to port excessively time
consuming simulation software from personal computers (PC) to super computes.
The power distribution system simulator (PDSS) takes a bottom-up approach and
simulates load at the appliance level, where detailed thermal models for
appliances are used. This approach works well for a small power distribution
system consisting of a few thousand appliances. When the number of appliances
increases, the simulation uses up the PC memory and its runtime increases to a
point where the approach is no longer feasible to model a practical large power
distribution system. This paper presents an effort made to port a PC-based
power distribution system simulator to a 128-processor shared-memory
supercomputer. The paper offers an overview of the parallel computing
environment and a description of the modification made to the PDSS model. The
performance of the PDSS running on a standalone PC and on the supercomputer is
compared. Future research direction of utilizing parallel computing in the
power distribution system simulation is also addressed.
"
2433,DiPerF: an automated DIstributed PERformance testing Framework,"  We present DiPerF, a distributed performance testing framework, aimed at
simplifying and automating service performance evaluation. DiPerF coordinates a
pool of machines that test a target service, collects and aggregates
performance metrics, and generates performance statistics. The aggregate data
collected provide information on service throughput, on service ""fairness"" when
serving multiple clients concurrently, and on the impact of network latency on
service performance. Furthermore, using this data, it is possible to build
predictive models that estimate a service performance given the service load.
We have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab,
and explored the performance of job submission services (pre WS GRAM and WS
GRAM) included with Globus Toolkit 3.2.
"
2434,Fast Query Processing by Distributing an Index over CPU Caches,"  Data intensive applications on clusters often require requests quickly be
sent to the node managing the desired data. In many applications, one must look
through a sorted tree structure to determine the responsible node for accessing
or storing the data.
  Examples include object tracking in sensor networks, packet routing over the
internet, request processing in publish-subscribe middleware, and query
processing in database systems. When the tree structure is larger than the CPU
cache, the standard implementation potentially incurs many cache misses for
each lookup; one cache miss at each successive level of the tree. As the
CPU-RAM gap grows, this performance degradation will only become worse in the
future.
  We propose a solution that takes advantage of the growing speed of local area
networks for clusters. We split the sorted tree structure among the nodes of
the cluster. We assume that the structure will fit inside the aggregation of
the CPU caches of the entire cluster. We then send a word over the network (as
part of a larger packet containing other words) in order to examine the tree
structure in another node's CPU cache. We show that this is often faster than
the standard solution, which locally incurs multiple cache misses while
accessing each successive level of the tree.
"
2435,Programmable Ethernet Switches and Their Applications,"  Modern Ethernet switches support many advanced features beyond route learning
and packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, and
status monitoring, which can be controlled through a programmatic interface.
Traditionally, these features are mostly used to statically configure a
network. This paper proposes to apply them as dynamic control mechanisms to
maximize physical network link resources, to minimize failure recovery time, to
enforce QoS requirements, and to support link-layer multicast without
broadcasting. With these advanced programmable control mechanisms, standard
Ethernet switches can be used as effective building blocks for
metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and
computation cluster interconnects. We demonstrate the usefulness of this new
level of control over Ethernet switches with a MEN architecture that features
multi-fold throughput gains and sub-second failure recovery time.
"
2436,An Empirical Analysis of Internet Protocol Version 6 (IPv6),"  Although the current Internet Protocol known as IPv4 has served its purpose
for over 20 years, its days are numbered. With IPv6 reaching a mature enough
level, there is a need to evaluate the performance benefits or drawbacks that
the new IPv6 protocol will have in comparison to the well established IPv4
protocol. Theoretically, the overhead between the two different protocols
should be directly proportional to the difference in the packet's header size,
however according to our findings, the empirical performance difference between
IPv4 and IPv6, especially when the transition mechanisms are taken into
consideration, is much larger than anticipated. We first examine the
performance of each protocol independently. We then examined two transition
mechanisms which perform the encapsulation at various points in the network:
host-to-host and router-to-router (tunneling). Our experiments were conducted
using two dual stack (IPv4/IPv6) routers using end nodes running both Windows
2000 and Solaris 8.0 in order to compare two different IPv6 implementations
side by side. Our tests were written in C++ and utilized metrics such as
latency, throughput, CPU utilization, socket creation time, socket connection
time, web server simulation, and a video client/server application for TCP/UDP
in IPv4/IPv6 under both Windows 2000 and Solaris 8.0. Our empirical evaluation
proved that IPv6 is not yet a mature enough technology and that it is still
years away from having consistent and good enough implementations, as the
performance of IPv6 in many cases proved to be significantly worse than IPv4.
"
2437,Correlated dynamics in human printing behavior,"  Arrival times of requests to print in a student laboratory were analyzed.
Inter-arrival times between subsequent requests follow a universal scaling law
relating time intervals and the size of the request, indicating a scale
invariant dynamics with respect to the size. The cumulative distribution of
file sizes is well-described by a modified power law often seen in
non-equilibrium critical systems. For each user, waiting times between their
individual requests show long range dependence and are broadly distributed from
seconds to weeks. All results are incompatible with Poisson models, and may
provide evidence of critical dynamics associated with voluntary thought
processes in the brain.
"
2438,Optimal Union-Find in Constraint Handling Rules,"  Constraint Handling Rules (CHR) is a committed-choice rule-based language
that was originally intended for writing constraint solvers. In this paper we
show that it is also possible to write the classic union-find algorithm and
variants in CHR. The programs neither compromise in declarativeness nor
efficiency. We study the time complexity of our programs: they match the
almost-linear complexity of the best known imperative implementations. This
fact is illustrated with experimental results.
"
2439,Shawn: A new approach to simulating wireless sensor networks,"  We consider the simulation of wireless sensor networks (WSN) using a new
approach. We present Shawn, an open-source discrete-event simulator that has
considerable differences to all other existing simulators. Shawn is very
powerful in simulating large scale networks with an abstract point of view. It
is, to the best of our knowledge, the first simulator to support generic
high-level algorithms as well as distributed protocols on exactly the same
underlying networks.
"
2440,"Performance Considerations for Gigabyte per Second Transcontinental
  Disk-to-Disk File Transfers","  Moving data from CERN to Pasadena at a gigabyte per second using the next
generation Internet requires good networking and good disk IO. Ten Gbps
Ethernet and OC192 links are in place, so now it is simply a matter of
programming. This report describes our preliminary work and measurements in
configuring the disk subsystem for this effort. Using 24 SATA disks at each
endpoint we are able to locally read and write an NTFS volume is striped across
24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher
performance and higher-capacity systems deliver up to 3.5 GBps.
"
2441,Sequential File Programming Patterns and Performance with .NET,"  Programming patterns for sequential file access in the .NET Framework are
described and the performance is measured. The default behavior provides
excellent performance on a single disk - 50 MBps both reading and writing.
Using large request sizes and doing file pre-allocation when possible have
quantifiable benefits. When one considers disk arrays, .NET unbuffered IO
delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of
that performance. Consequently, high-performance file and database utilities
are still forced to use unbuffered IO for maximum sequential performance. The
report is accompanied by downloadable source code that demonstrates the
concepts and code that was used to obtain these measurements.
"
2442,Efficient Parallel Simulations of Asynchronous Cellular Arrays,"  A definition for a class of asynchronous cellular arrays is proposed. An
example of such asynchrony would be independent Poisson arrivals of cell
iterations. The Ising model in the continuous time formulation of Glauber falls
into this class. Also proposed are efficient parallel algorithms for simulating
these asynchronous cellular arrays. In the algorithms, one or several cells are
assigned to a processing element (PE), local times for different PEs can be
different. Although the standard serial algorithm by Metropolis, Rosenbluth,
Rosenbluth, Teller, and Teller can simulate such arrays, it is usually believed
to be without an efficient parallel counterpart. However, the proposed parallel
algorithms contradict this belief proving to be both efficient and able to
perform the same task as the standard algorithm. The results of experiments
with the new algorithms are encouraging: the speed-up is greater than 16 using
25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2**14
PEs on a SIMD computer. The algorithm by Bortz, Kalos, and Lebowitz can be
incorporated in the proposed parallel algorithms, further contributing to
speed-up. [In this paper I invented the update-cites-of-local-time-minima
parallel simulation scheme. Now the scheme is becoming popular. Many misprints
of the original 1987 Complex Systems publication are corrected here.-B.L.]
"
2443,Improving PARMA Trailing,"  Taylor introduced a variable binding scheme for logic variables in his PARMA
system, that uses cycles of bindings rather than the linear chains of bindings
used in the standard WAM representation. Both the HAL and dProlog languages
make use of the PARMA representation in their Herbrand constraint solvers.
Unfortunately, PARMA's trailing scheme is considerably more expensive in both
time and space consumption. The aim of this paper is to present several
techniques that lower the cost.
  First, we introduce a trailing analysis for HAL using the classic PARMA
trailing scheme that detects and eliminates unnecessary trailings. The
analysis, whose accuracy comes from HAL's determinism and mode declarations,
has been integrated in the HAL compiler and is shown to produce space
improvements as well as speed improvements. Second, we explain how to modify
the classic PARMA trailing scheme to halve its trailing cost. This technique is
illustrated and evaluated both in the context of dProlog and HAL. Finally, we
explain the modifications needed by the trailing analysis in order to be
combined with our modified PARMA trailing scheme. Empirical evidence shows that
the combination is more effective than any of the techniques when used in
isolation.
  To appear in Theory and Practice of Logic Programming.
"
2444,"An End-to-End Probabilistic Network Calculus with Moment Generating
  Functions","  Network calculus is a min-plus system theory for performance evaluation of
queuing networks. Its elegance stems from intuitive convolution formulas for
concatenation of deterministic servers. Recent research dispenses with the
worst-case assumptions of network calculus to develop a probabilistic
equivalent that benefits from statistical multiplexing. Significant
achievements have been made, owing for example to the theory of effective
bandwidths, however, the outstanding scalability set up by concatenation of
deterministic servers has not been shown.
  This paper establishes a concise, probabilistic network calculus with moment
generating functions. The presented work features closed-form, end-to-end,
probabilistic performance bounds that achieve the objective of scaling linearly
in the number of servers in series. The consistent application of moment
generating functions put forth in this paper utilizes independence beyond the
scope of current statistical multiplexing of flows. A relevant additional gain
is demonstrated for tandem servers with independent cross-traffic.
"
2445,Software Performance Analysis,"  The key to speeding up applications is often understanding where the elapsed
time is spent, and why. This document reviews in depth the full array of
performance analysis tools and techniques available on Linux for this task,
from the traditional tools like gcov and gprof, to the more advanced tools
still under development like oprofile and the Linux Trace Toolkit. The focus is
more on the underlying data collection and processing algorithms, and their
overhead and precision, than on the cosmetic details of the graphical user
interface frontends.
"
2446,"Disks, Partitions, Volumes and RAID Performance with the Linux Operating
  System","  Block devices in computer operating systems typically correspond to disks or
disk partitions, and are used to store files in a filesystem. Disks are not the
only real or virtual device which adhere to the block accessible stream of
bytes block device model. Files, remote devices, or even RAM may be used as a
virtual disks. This article examines several common combinations of block
device layers used as virtual disks in the Linux operating system: disk
partitions, loopback files, software RAID, Logical Volume Manager, and Network
Block Devices. It measures their relative performance using different
filesystems: Ext2, Ext3, ReiserFS, JFS, XFS,NFS.
"
2447,"Analysis of Stochastic Service Guarantees in Communication Networks: A
  Basic Calculus","  A basic calculus is presented for stochastic service guarantee analysis in
communication networks. Central to the calculus are two definitions,
maximum-(virtual)-backlog-centric (m.b.c) stochastic arrival curve and
stochastic service curve, which respectively generalize arrival curve and
service curve in the deterministic network calculus framework. With m.b.c
stochastic arrival curve and stochastic service curve, various basic results
are derived under the (min, +) algebra for the general case analysis, which are
crucial to the development of stochastic network calculus. These results
include (i) superposition of flows, (ii) concatenation of servers, (iii) output
characterization, (iv) per-flow service under aggregation, and (v) stochastic
backlog and delay guarantees. In addition, to perform independent case
analysis, stochastic strict server is defined, which uses an ideal service
process and an impairment process to characterize a server. The concept of
stochastic strict server not only allows us to improve the basic results (i) --
(v) under the independent case, but also provides a convenient way to find the
stochastic service curve of a serve. Moreover, an approach is introduced to
find the m.b.c stochastic arrival curve of a flow and the stochastic service
curve of a server.
"
2448,Random Walks with Anti-Correlated Steps,"  We conjecture the expected value of random walks with anti-correlated steps
to be exactly 1. We support this conjecture with 2 plausibility arguments and
experimental data. The experimental analysis includes the computation of the
expected values of random walks for steps up to 22. The result shows the
expected value asymptotically converging to 1.
"
2449,Compression Scheme for Faster and Secure Data Transmission Over Internet,"  Compression algorithms reduce the redundancy in data representation to
decrease the storage required for that data. Data compression offers an
attractive approach to reducing communication costs by using available
bandwidth effectively. Over the last decade there has been an unprecedented
explosion in the amount of digital data transmitted via the Internet,
representing text, images, video, sound, computer programs, etc. With this
trend expected to continue, it makes sense to pursue research on developing
algorithms that can most effectively use available network bandwidth by
maximally compressing data. It is also important to consider the security
aspects of the data being transmitted while compressing it, as most of the text
data transmitted over the Internet is very much vulnerable to a multitude of
attacks. This paper is focused on addressing this problem of lossless
compression of text files with an added security.
"
2450,The Computational and Storage Potential of Volunteer Computing,"  ""Volunteer computing"" uses Internet-connected computers, volunteered by their
owners, as a source of computing power and storage. This paper studies the
potential capacity of volunteer computing. We analyzed measurements of over
330,000 hosts participating in a volunteer computing project. These
measurements include processing power, memory, disk space, network throughput,
host availability, user-specified limits on resource usage, and host churn. We
show that volunteer computing can support applications that are significantly
more data-intensive, or have larger memory and storage requirements, than those
in current projects.
"
2451,Benchmark Problems for Constraint Solving,"  Constraint Programming is roughly a new software technology introduced by
Jaffar and Lassez in 1987 for description and effective solving of large,
particularly combinatorial, problems especially in areas of planning and
scheduling. In the following we define three problems for constraint solving
from the domain of electrical networks; based on them we define 43 related
problems. For the defined set of problems we benchmarked five systems: ILOG
OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems
performed very well for some problems while others performed very well on
others.
"
2452,On Conditional Branches in Optimal Search Trees,"  Algorithms for efficiently finding optimal alphabetic decision trees -- such
as the Hu-Tucker algorithm -- are well established and commonly used. However,
such algorithms generally assume that the cost per decision is uniform and thus
independent of the outcome of the decision. The few algorithms without this
assumption instead use one cost if the decision outcome is ``less than'' and
another cost otherwise. In practice, neither assumption is accurate for
software optimized for today's microprocessors. Such software generally has one
cost for the more likely decision outcome and a greater cost -- often far
greater -- for the less likely decision outcome. This problem and
generalizations thereof are thus applicable to hard coding static decision tree
instances in software, e.g., for optimizing program bottlenecks or for
compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming
algorithm can solve this optimal binary decision tree problem, and this
approach has many generalizations that optimize for the behavior of processors
with predictive branch capabilities, both static and dynamic. Solutions to this
formulation are often faster in practice than ``optimal'' decision trees as
formulated in the literature. Different search paradigms can sometimes yield
even better performance.
"
2453,A Delay Analysis of Maximal Matching Switching with Speedup,"  In this paper we analyze the average queue backlog in a combined input-output
queued switch using a maximal size matching scheduling algorithm. We compare
this average backlog to the average backlog achieved by an optimal switch. We
model the cell arrival process as independent and identically distributed
between time slots and uniformly distributed among input and output ports. For
switches with many input and output ports, the backlog associated with maximal
size matching with speedup 3 is no more than 10/3 times the backlog associated
with an optimal switch. Moreover, this performance ratio rapidly approaches 2
as speedup increases.
"
2454,Fast and Generalized Polynomial Time Memory Consistency Verification,"  The problem of verifying multi-threaded execution against the memory
consistency model of a processor is known to be an NP hard problem. However
polynomial time algorithms exist that detect almost all failures in such
execution. These are often used in practice for microprocessor verification. We
present a low complexity and fully parallelized algorithm to check program
execution against the processor consistency model. In addition our algorithm is
general enough to support a number of consistency models without any
degradation in performance. An implementation of this algorithm is currently
used in practice to verify processors in the post silicon stage for multiple
architectures.
"
2455,On symmetric sandpiles,"  A symmetric version of the well-known SPM model for sandpiles is introduced.
We prove that the new model has fixed point dynamics. Although there might be
several fixed points, a precise description of the fixed points is given.
Moreover, we provide a simple closed formula for counting the number of fixed
points originated by initial conditions made of a single column of grains.
"
2456,"Methods for Partitioning Data to Improve Parallel Execution Time for
  Sorting on Heterogeneous Clusters","  The aim of the paper is to introduce general techniques in order to optimize
the parallel execution time of sorting on a distributed architectures with
processors of various speeds. Such an application requires a partitioning step.
For uniformly related processors (processors speeds are related by a constant
factor), we develop a constant time technique for mastering processor load and
execution time in an heterogeneous environment and also a technique to deal
with unknown cost functions. For non uniformly related processors, we use a
technique based on dynamic programming. Most of the time, the solutions are in
O(p) (p is the number of processors), independent of the problem size n.
Consequently, there is a small overhead regarding the problem we deal with but
it is inherently limited by the knowing of time complexity of the portion of
code following the partitioning.
"
2457,Concurrent Processing Memory,"  A theoretical memory with limited processing power and internal connectivity
at each element is proposed. This memory carries out parallel processing within
itself to solve generic array problems. The applicability of this in-memory
finest-grain massive SIMD approach is studied in some details. For an array of
N items, it reduces the total instruction cycle count of universal operations
such as insertion/deletion and match finding to ~ 1, local operations such as
filtering and template matching to ~ local operation size, and global
operations such as sum, finding global limit and sorting to ~\sqroot{N}
instruction cycles. It eliminates most streaming activities for data processing
purpose on the system bus. Yet it remains general-purposed, easy to use, pin
compatible with conventional memory, and practical for implementation.
"
2458,A Generic Lazy Evaluation Scheme for Exact Geometric Computations,"  We present a generic C++ design to perform efficient and exact geometric
computations using lazy evaluations. Exact geometric computations are critical
for the robustness of geometric algorithms. Their efficiency is also critical
for most applications, hence the need for delaying the exact computations at
run time until they are actually needed. Our approach is generic and extensible
in the sense that it is possible to make it a library which users can extend to
their own geometric objects or primitives. It involves techniques such as
generic functor adaptors, dynamic polymorphism, reference counting for the
management of directed acyclic graphs and exception handling for detecting
cases where exact computations are needed. It also relies on multiple precision
arithmetic as well as interval arithmetic. We apply our approach to the whole
geometric kernel of CGAL.
"
2459,JiTS: Just-in-Time Scheduling for Real-Time Sensor Data Dissemination,"  We consider the problem of real-time data dissemination in wireless sensor
networks, in which data are associated with deadlines and it is desired for
data to reach the sink(s) by their deadlines. To this end, existing real-time
data dissemination work have developed packet scheduling schemes that
prioritize packets according to their deadlines. In this paper, we first
demonstrate that not only the scheduling discipline but also the routing
protocol has a significant impact on the success of real-time sensor data
dissemination. We show that the shortest path routing using the minimum number
of hops leads to considerably better performance than Geographical Forwarding,
which has often been used in existing real-time data dissemination work. We
also observe that packet prioritization by itself is not enough for real-time
data dissemination, since many high priority packets may simultaneously contend
for network resources, deteriorating the network performance. Instead,
real-time packets could be judiciously delayed to avoid severe contention as
long as their deadlines can be met. Based on this observation, we propose a
Just-in-Time Scheduling (JiTS) algorithm for scheduling data transmissions to
alleviate the shortcomings of the existing solutions. We explore several
policies for non-uniformly delaying data at different intermediate nodes to
account for the higher expected contention as the packet gets closer to the
sink(s). By an extensive simulation study, we demonstrate that JiTS can
significantly improve the deadline miss ratio and packet drop ratio compared to
existing approaches in various situations. Notably, JiTS improves the
performance requiring neither lower layer support nor synchronization among the
sensor nodes.
"
2460,The Effect of Scheduling on Link Capacity in Multi-hopWireless Networks,"  Existing models of Multi-Hop Wireless Networks (MHWNs) assume that
interference estimators of link quality such as observed busy time predict the
capacity of the links. We show that these estimators do not capture the
intricate interactions that occur at the scheduling level, which have a large
impact on effective link capacity under contention based MAC protocols. We
observe that scheduling problems arise only among those interfering sources
whose concurrent transmissions cannot be prevented by the MAC protocol's
collision management mechanisms; other interfering sources can arbitrate the
medium and coexist successfully. Based on this observation, we propose a
methodology for rating links and show that it achieves high correlation with
observed behavior in simulation. We then use this rating as part of a
branch-and-bound framework based on a linear programming formulation for
traffic engineering in static MHWNs and show that it achieves considerable
improvement in performance relative to interference based models.
"
2461,Analysis of a Reputation System for Mobile Ad-Hoc Networks with Liars,"  The application of decentralized reputation systems is a promising approach
to ensure cooperation and fairness, as well as to address random failures and
malicious attacks in Mobile Ad-Hoc Networks. However, they are potentially
vulnerable to liars. With our work, we provide a first step to analyzing
robustness of a reputation system based on a deviation test. Using a mean-field
approach to our stochastic process model, we show that liars have no impact
unless their number exceeds a certain threshold (phase transition). We give
precise formulae for the critical values and thus provide guidelines for an
optimal choice of parameters.
"
2462,Memory and compiler optimizations for low-power and -energy,"  Embedded systems become more and more widespread, especially autonomous ones,
and clearly tend to be ubiquitous. In such systems, low-power and low-energy
usage get ever more crucial. Furthermore, these issues also become paramount in
(massively) multi-processors systems, either in one machine or more widely in a
grid. The various problems faced pertain to autonomy, power supply
possibilities, thermal dissipation, or even sheer energy cost. Although it has
since long been studied in harware, energy optimization is more recent in
software. In this paper, we thus aim at raising awareness to low-power and
low-energy issues in the language and compilation community. We thus broadly
but briefly survey techniques and solutions to this energy issue, focusing on a
few specific aspects in the context of compiler optimizations and memory
management.
"
2463,A Markov Chain based method for generating long-range dependence,"  This paper describes a model for generating time series which exhibit the
statistical phenomenon known as long-range dependence (LRD). A Markov Modulated
Process based upon an infinite Markov chain is described. The work described is
motivated by applications in telecommunications where LRD is a known property
of time-series measured on the internet. The process can generate a time series
exhibiting LRD with known parameters and is particularly suitable for modelling
internet traffic since the time series is in terms of ones and zeros which can
be interpreted as data packets and inter-packet gaps. The method is extremely
simple computationally and analytically and could prove more tractable than
other methods described in the literature
"
2464,"Markov-modulated on/off processes for long-range dependent internet
  traffic","  The aim of this paper is to use a very simple queuing model to compare a
number of models from the literature which have been used to replicate the
statistical nature of internet traffic and, in particular, the long-range
dependence of this traffic. The four models all have the form of discrete time
Markov-modulated processes (two other models are introduced for comparison
purposes).
  While it is often stated that long-range dependence has a critical effect on
queuing performance, it appears that the models used here do not well
replicated the queuing performance of real internet traffic. In particular,
they fail to replicate the mean queue length (and hence the mean delay) and the
probability of the queue length exceeding a given level.
"
2465,On Degree-Based Decentralized Search in Complex Networks,"  Decentralized search aims to find the target node in a large network by using
only local information. The applications of it include peer-to-peer file
sharing, web search and anything else that requires locating a specific target
in a complex system. In this paper, we examine the degree-based decentralized
search method. Specifically, we evaluate the efficiency of the method in
different cases with different amounts of available local information. In
addition, we propose a simple refinement algorithm for significantly shortening
the length of the route that has been found. Some insights useful for the
future developments of efficient decentralized search schemes have been
achieved.
"
2466,On Conditional Branches in Optimal Decision Trees,"  The decision tree is one of the most fundamental programming abstractions. A
commonly used type of decision tree is the alphabetic binary tree, which uses
(without loss of generality) ``less than'' versus ''greater than or equal to''
tests in order to determine one of $n$ outcome events. The process of finding
an optimal alphabetic binary tree for a known probability distribution on
outcome events usually has the underlying assumption that the cost (time) per
decision is uniform and thus independent of the outcome of the decision. This
assumption, however, is incorrect in the case of software to be optimized for a
given microprocessor, e.g., in compiling switch statements or in fine-tuning
program bottlenecks. The operation of the microprocessor generally means that
the cost for the more likely decision outcome can or will be less -- often far
less -- than the less likely decision outcome. Here we formulate a variety of
$O(n^3)$-time $O(n^2)$-space dynamic programming algorithms to solve such
optimal binary decision tree problems, optimizing for the behavior of
processors with predictive branch capabilities, both static and dynamic. In the
static case, we use existing results to arrive at entropy-based performance
bounds. Solutions to this formulation are often faster in practice than
``optimal'' decision trees as formulated in the literature, and, for small
problems, are easily worth the extra complexity in finding the better solution.
This can be applied in fast implementation of decoding Huffman codes.
"
2467,Distributed Link Scheduling with Constant Overhead,"  This paper proposes a new class of simple, distributed algorithms for
scheduling in wireless networks. The algorithms generate new schedules in a
distributed manner via simple local changes to existing schedules. The class is
parameterized by integers $k\geq 1$. We show that algorithm $k$ of our class
achieves $k/(k+2)$ of the capacity region, for every $k\geq 1$. The algorithms
have small and constant worst-case overheads: in particular, algorithm $k$
generates a new schedule using {\em (a)} time less than $4k+2$ round-trip times
between neighboring nodes in the network, and {\em (b)} at most three control
transmissions by any given node, for any $k$. The control signals are
explicitly specified, and face the same interference effects as normal data
transmissions. Our class of distributed wireless scheduling algorithms are the
first ones guaranteed to achieve any fixed fraction of the capacity region
while using small and constant overheads that do not scale with network size.
The parameter $k$ explicitly captures the tradeoff between control overhead and
scheduler throughput performance and provides a tuning knob protocol designers
can use to harness this trade-off in practice.
"
2468,"Proportional Fairness in Multi-channel Multi-rate Wireless Networks-Part
  I: The Case of Deterministic Channels","  This is Part I of a two-part paper series that studies the use of the
proportional fairness (PF) utility function as the basis for capacity
allocation and scheduling in multi-channel multi-rate wireless networks. The
contributions of Part I are threefold. (i) First, we lay down the theoretical
foundation for PF. Specifically, we present the fundamental properties and
physical/economic interpretation of PF. We show by general mathematical
arguments that PF leads to equal airtime allocation to users for the
single-channel case; and equal equivalent airtime allocation to users for the
multi-channel case, where the equivalent airtime enjoyed by a user is a
weighted sum of the airtimes enjoyed by the user on all channels, with the
weight of a channel being the price or value of that channel. We also establish
the Pareto efficiency of PF solutions. (ii) Second, we derive characteristics
of PF solutions that are useful for the construction of PF-optimization
algorithms. We present several PF-optimization algorithms, including a fast
algorithm that is amenable to parallel implementation. (iii) Third, we study
the use of PF utility for capacity allocation in large-scale WiFi networks
consisting of many adjacent wireless LANs. We find that the PF solution
simultaneously achieves higher system throughput, better fairness, and lower
outage probability with respect to the default solution given by today's 802.11
commercial products. Part II of this paper series extends our investigation to
the time-varying-channel case in which the data rates enjoyed by users over the
channels vary dynamically over time
"
2469,"Proportional Fairness in Multi-channel Multi-rate Wireless Networks-Part
  II: The Case of Time-Varying Channels","  This is Part II of a two-part paper series that studies the use of the
proportional fairness (PF) utility function as the basis for capacity
allocation and scheduling in multi-channel multi-rate wireless networks. The
contributions of Part II are twofold. (i) First, we extend the problem
formulation, theoretical results, and algorithms to the case of time-varying
channels, where opportunistic capacity allocation and scheduling can be
exploited to improve system performance. We lay down the theoretical foundation
for optimization that ""couples"" the time-varying characteristic of channels
with the requirements of the underlying applications into one consideration. In
particular, the extent to which opportunistic optimization is possible is not
just a function of how fast the channel characteristics vary, but also a
function of the elasticity of the underlying applications for delayed capacity
allocation. (ii) Second, building upon our theoretical framework and results,
we study subcarrier allocation and scheduling in orthogonal frequency division
multiplexing (OFDM) cellular wireless networks. We introduce the concept of a
W-normalized Doppler frequency to capture the extent to which opportunistic
scheduling can be exploited to achieve throughput-fairness performance gain. We
show that a ""look-back PF"" scheduling can strike a good balance between system
throughput and fairness while taking the underlying application requirements
into account.
"
2470,"A Multi-server Scheduling Framework for Resource Allocation in Wireless
  Multi-carrier Networks","  Multiuser resource allocation has recently been recognized as an effective
methodology for enhancing the power and spectrum efficiency in OFDM (orthogonal
frequency division multiplexing) systems. It is, however, not directly
applicable to current packet-switched networks, because (i) most existing
packet-scheduling schemes are based on a single-server model and do not serve
multiple users at the same time; and (ii) the conventional separate design of
MAC (medium access control) packet scheduling and PHY (physical) resource
allocation yields inefficient resource utilization. In this paper, we propose a
cross-layer resource allocation algorithm based on a novel multi-server
scheduling framework to achieve overall high system power efficiency in
packet-switched OFDM networks. Our contribution is four fold: (i) we propose
and analyze a MPGPS (multi-server packetized general processor sharing) service
discipline that serves multiple users at the same time and facilitates
multiuser resource allocation; (ii) we present a MPGPS-based joint MAC-PHY
resource allocation scheme that incorporates packet scheduling, subcarrier
allocation, and power allocation in an integrated framework; (iii) by
investigating the fundamental tradeoff between multiuser-diversity and queueing
performance, we present an A-MPGPS (adaptive MPGPS) service discipline that
strikes balance between power efficiency and queueing performance; and (iv) we
extend MPGPS to an O-MPGPS (opportunistic MPGPS) service discipline to further
enhance the resource utilization efficiency.
"
2471,"A Combined LIFO-Priority Scheme for Overload Control of E-commerce Web
  Servers","  E-commerce Web-servers often face overload conditions during which
revenue-generating requests may be dropped or abandoned due to an increase in
the browsing requests. In this paper we present a simple, yet effective,
mechanism for overload control of E-commerce Web-servers. We develop an
E-commerce workload model that separates the browsing requests from
revenue-generating transaction requests. During overload, we apply LIFO
discipline in the browsing queues and use a dynamic priority model to service
them. The transaction queues are given absolute priority over the browsing
queues. This is called the LIFO-Pri scheduling discipline. Experimental results
show that LIFO-Pri dramatically improves the overall Web-server throughput
while also increasing the completion rate of revenue-generating requests. The
Web-server was able to operate at nearly 60% of its maximum capacity even when
offered load was 1.5 times its capacity. Further, when compared to a single
queue FIFO system, there was a seven-fold increase in the number of completed
revenue-generating requests during overload.
"
2472,"Lossy Bulk Synchronous Parallel Processing Model for Very Large Scale
  Grids","  The performance of a parallel algorithm in a very large scale grid is
significantly influenced by the underlying Internet protocols and
inter-connectivity. Many grid programming platforms use TCP due to its
reliability, usually with some optimizations to reduce its costs. However, TCP
does not perform well in a high bandwidth and high delay network environment.
On the other hand, UDP is the fastest protocol available because it omits
connection setup process, acknowledgments and retransmissions sacrificing
reliable transfer. Many new bulk data transfer schemes using UDP for data
transmission such as RBUDP, Tsunami, and SABUL have been introduced and shown
to have better performance compared to TCP. In this paper, we consider the use
of UDP and examine the relationship between packet loss and speedup with
respect to the number of grid nodes. Our measurement suggests that packet loss
rates between 5%-15% on average are not uncommon between PlanetLab nodes that
are widely distributed over the Internet. We show that transmitting multiple
copies of same packet produces higher speedup. We show the minimum number of
packet duplication required to maximize the possible speedup for a given number
of nodes using a BSP based model. Our work demonstrates that by using an
appropriate number of packet copies, we can increase performance of parallel
program.
"
2473,Effectiveness of Garbage Collection in MIT/GNU Scheme,"  Scheme uses garbage collection for heap memory management. Ideally, garbage
collectors should be able to reclaim all dead objects, i.e. objects that will
not be used in future. However, garbage collectors collect only those dead
objects that are not reachable from any program variable. Dead objects that are
reachable from program variables are not reclaimed.
  In this paper we describe our experiments to measure the effectiveness of
garbage collection in MIT/GNU Scheme. We compute the drag time of objects, i.e.
the time for which an object remains in heap memory after its last use. The
number of dead objects and the drag time together indicate opportunities for
improving garbage collection. Our experiments reveal that up to 26% of dead
objects remain in memory. The average drag time is up to 37% of execution time.
Overall, we observe memory saving potential ranging from 9% to 65%.
"
2474,Queue Model of Leaf Degree Keeping Process in Gnutella Network,"  Leaf degree keeping process of Gnutella is discussed in this paper. Queue
system based on rules of Gnutella protocol are introduced to modeling this
process. The leaf degree distributions resulted from the queue system and from
our real measurement are compared. The well match of those distributions reveal
that the leaf degree distribution in Gnutella network should not be power law
or power law like as reported before. It is more likely a distribution driven
by certain queue process specified by the protocol.
"
2475,"Towards Parallel Computing on the Internet: Applications, Architectures,
  Models and Programming Tools","  The development of Internet wide resources for general purpose parallel
computing poses the challenging task of matching computation and communication
complexity. A number of parallel computing models exist that address this for
traditional parallel architectures, and there are a number of emerging models
that attempt to do this for large scale Internet-based systems like
computational grids. In this survey we cover the three fundamental aspects --
application, architecture and model, and we show how they have been developed
over the last decade. We also cover programming tools that are currently being
used for parallel programming in computational grids. The trend in conventional
computational models are to put emphasis on efficient communication between
participating nodes by adapting different types of communication to network
conditions. Effects of dynamism and uncertainties that arise in large scale
systems are evidently important to understand and yet there is currently little
work that addresses this from a parallel computing perspective.
"
2476,Exact Failure Frequency Calculations for Extended Systems,"  This paper shows how the steady-state availability and failure frequency can
be calculated in a single pass for very large systems, when the availability is
expressed as a product of matrices. We apply the general procedure to
$k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a
simple ladder network in which each edge and node may fail. We also give the
associated generating functions when the components have identical
availabilities and failure rates. For large systems, the failure rate of the
whole system is asymptotically proportional to its size. This paves the way to
ready-to-use formulae for various architectures, as well as proof that the
differential operator approach to failure frequency calculations is very useful
and straightforward.
"
2477,"Exact solutions for the two- and all-terminal reliabilities of a simple
  ladder network","  The exact calculation of network reliability in a probabilistic context has
been a long-standing issue of practical importance, but a difficult one, even
for planar graphs, with perfect nodes and with edges of identical reliability
p. Many approaches (determination of bounds, sums of disjoint products
algorithms, Monte Carlo evaluations, studies of the reliability polynomials,
etc.) can only provide approximations when the network's size increases. We
consider here a ladder graph of arbitrary size corresponding to real-life
network configurations, and give the exact, analytical solutions for the all-
and two-terminal reliabilities. These solutions use transfer matrices, in which
individual reliabilities of edges and nodes are taken into account. The special
case of identical edge and node reliabilities -- p and rho, respectively -- is
solved. We show that the zeros of the two-terminal reliability polynomial
exhibit structures which differ substantially for seemingly similar networks,
and we compare the sensitivity of various edges. We discuss how the present
work may be further extended to lead to a catalog of exactly solvable networks
in terms of reliability, which could be useful as elementary bricks for a new
and improved set of bounds or benchmarks in the general case.
"
2478,On High Spatial Reuse Link Scheduling in STDMA Wireless Ad Hoc Networks,"  Graph-based algorithms for point-to-point link scheduling in Spatial reuse
Time Division Multiple Access (STDMA) wireless ad hoc networks often result in
a significant number of transmissions having low Signal to Interference and
Noise density Ratio (SINR) at intended receivers, leading to low throughput. To
overcome this problem, we propose a new algorithm for STDMA link scheduling
based on a graph model of the network as well as SINR computations. The
performance of our algorithm is evaluated in terms of spatial reuse and
computational complexity. Simulation results demonstrate that our algorithm
achieves better performance than existing algorithms.
"
2479,"Exact solutions for the two- and all-terminal reliabilities of the
  Brecht-Colbourn ladder and the generalized fan","  The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the
generalized fan have been calculated exactly for arbitrary size as well as
arbitrary individual edge and node reliabilities, using transfer matrices of
dimension four at most. While the all-terminal reliabilities of these graphs
are identical, the special case of identical edge ($p$) and node ($\rho$)
reliabilities shows that their two-terminal reliabilities are quite distinct,
as demonstrated by their generating functions and the locations of the zeros of
the reliability polynomials, which undergo structural transitions at $\rho =
\displaystyle {1/2}$.
"
2480,"On the Correlation of Geographic and Network Proximity at Internet Edges
  and its Implications for Mobile Unicast and Multicast Routing","  Significant effort has been invested recently to accelerate handover
operations in a next generation mobile Internet. Corresponding works for
developing efficient mobile multicast management are emergent. Both problems
simultaneously expose routing complexity between subsequent points of
attachment as a characteristic parameter for handover performance in access
networks.
  As continuous mobility handovers necessarily occur between access routers
located in geographic vicinity, this paper investigates on the hypothesis that
geographically adjacent edge networks attain a reduced network distances as
compared to arbitrary Internet nodes. We therefore evaluate and analyze edge
distance distributions in various regions for clustered IP ranges on their
geographic location such as a city. We use traceroute to collect packet
forwarding path and round-trip-time of each intermediate node to scan-wise
derive an upper bound of the node distances. Results of different scanning
origins are compared to obtain the best estimation of network distance of each
pair. Our results are compared with corresponding analysis of CAIDA Skitter
data, overall leading to fairly stable, reproducible edge distance
distributions. As a first conclusion on expected impact on handover performance
measures, our results indicate a general optimum for handover anticipation time
in 802.11 networks of 25 ms.
"
2481,Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive,"  A $2k computer can execute about 8k transactions per second. This is 80x more
than one of the largest US bank's 1970's traffic - it approximates the total US
1970's financial transaction volume. Very modest modern computers can easily
solve yesterday's problems.
"
2482,A Measure of Transaction Processing 20 Years Later,"  This provides a retrospective of the paper ""A Measure of Transaction
Processing"" published in 1985. It shows that transaction processing peak
performance and price-peformance have improved about 100,000x respectively and
that sort/sequential performance has approximately doubled each year (so a
million fold improvement) even though processor performance plateaued in 1995.
"
2483,The parallel implementation of the Astr\'{e}e static analyzer,"  The Astr\'{e}e static analyzer is a specialized tool that can prove the
absence of runtime errors, including arithmetic overflows, in large critical
programs. Keeping analysis times reasonable for industrial use is one of the
design objectives. In this paper, we discuss the parallel implementation of the
analysis.
"
2484,A Static Analyzer for Large Safety-Critical Software,"  We show that abstract interpretation-based static program analysis can be
made efficient and precise enough to formally verify a class of properties for
a family of large programs with few or no false alarms. This is achieved by
refinement of a general purpose static analyzer and later adaptation to
particular programs of the family by the end-user through parametrization. This
is applied to the proof of soundness of data manipulation operations at the
machine level for periodic synchronous safety critical embedded software. The
main novelties are the design principle of static analyzers by refinement and
adaptation through parametrization, the symbolic manipulation of expressions to
improve the precision of abstract transfer functions, the octagon, ellipsoid,
and decision tree abstract domains, all with sound handling of rounding errors
in floating point computations, widening strategies (with thresholds, delayed)
and the automatic determination of the parameters (parametrized packing).
"
2485,"An Abstract Monte-Carlo Method for the Analysis of Probabilistic
  Programs","  We introduce a new method, combination of random testing and abstract
interpretation, for the analysis of programs featuring both probabilistic and
non-probabilistic nondeterminism. After introducing ""ordinary"" testing, we show
how to combine testing and abstract interpretation and give formulas linking
the precision of the results to the number of iterations. We then discuss
complexity and optimization issues and end with some experimental results.
"
2486,Noise Limited Computational Speed,"  In modern transistor based logic gates, the impact of noise on computation
has become increasingly relevant since the voltage scaling strategy, aimed at
decreasing the dissipated power, has increased the probability of error due to
the reduced switching threshold voltages. In this paper we discuss the role of
noise in a two state model that mimic the dynamics of standard logic gates and
show that the presence of the noise sets a fundamental limit to the computing
speed. An optimal idle time interval that minimizes the error probability, is
derived.
"
2487,"High Performance Direct Gravitational N-body Simulations on Graphics
  Processing Units","  We present the results of gravitational direct $N$-body simulations using the
commercial graphics processing units (GPU) NVIDIA Quadro FX1400 and GeForce
8800GTX, and compare the results with GRAPE-6Af special purpose hardware. The
force evaluation of the $N$-body problem was implemented in Cg using the GPU
directly to speed-up the calculations. The integration of the equations of
motions were, running on the host computer, implemented in C using the 4th
order predictor-corrector Hermite integrator with block time steps. We find
that for a large number of particles ($N \apgt 10^4$) modern graphics
processing units offer an attractive low cost alternative to GRAPE special
purpose hardware. A modern GPU continues to give a relatively flat scaling with
the number of particles, comparable to that of the GRAPE. Using the same time
step criterion the total energy of the $N$-body system was conserved better
than to one in $10^6$ on the GPU, which is only about an order of magnitude
worse than obtained with GRAPE. For $N\apgt 10^6$ the GeForce 8800GTX was about
20 times faster than the host computer. Though still about an order of
magnitude slower than GRAPE, modern GPU's outperform GRAPE in their low cost,
long mean time between failure and the much larger onboard memory; the
GRAPE-6Af holds at most 256k particles whereas the GeForce 8800GTF can hold 9
million particles in memory.
"
2488,"In Search of Simplicity: A Self-Organizing Multi-Source Multicast
  Overlay","  Multicast communication primitives have broad utility as building blocks for
distributed applications. The challenge is to create and maintain the
distributed structures that support these primitives while accounting for
volatile end nodes and variable network characteristics. Most solutions
proposed to date rely on complex algorithms or global information, thus
limiting the scale of deployments and acceptance outside the academic realm.
This article introduces a low-complexity, self organizing solution for
maintaining multicast trees, that we refer to as UMM (Unstructured Multi-source
Multicast). UMM uses traditional distributed systems techniques: layering,
soft-state, and passive data collection to adapt to the dynamics of the
physical network and maintain data dissemination trees. The result is a simple,
adaptive system with lower overheads than more complex alternatives. We have
implemented UMM and evaluated it on a 100-node PlanetLab testbed and on up to
1024-node emulated ModelNet networks Extensive experimental evaluations
demonstrate UMM's low overhead, efficient network usage compared to alternative
solutions, and ability to quickly adapt to network changes and to recover from
failures.
"
2489,Unasssuming View-Size Estimation Techniques in OLAP,"  Even if storage was infinite, a data warehouse could not materialize all
possible views due to the running time and update requirements. Therefore, it
is necessary to estimate quickly, accurately, and reliably the size of views.
Many available techniques make particular statistical assumptions and their
error can be quite large. Unassuming techniques exist, but typically assume we
have independent hashing for which there is no known practical implementation.
We adapt an unassuming estimator due to Gibbons and Tirthapura: its theoretical
bounds do not make unpractical assumptions. We compare this technique
experimentally with stochastic probabilistic counting, LogLog probabilistic
counting, and multifractal statistical models. Our experiments show that we can
reliably and accurately (within 10%, 19 times out 20) estimate view sizes over
large data sets (1.5 GB) within minutes, using almost no memory. However, only
Gibbons-Tirthapura provides universally tight estimates irrespective of the
size of the view. For large views, probabilistic counting has a small edge in
accuracy, whereas the competitive sampling-based method (multifractal) we
tested is an order of magnitude faster but can sometimes provide poor estimates
(relative error of 100%). In our tests, LogLog probabilistic counting is not
competitive. Experimental validation on the US Census 1990 data set and on the
Transaction Processing Performance (TPC H) data set is provided.
"
2490,"A Comparison of Five Probabilistic View-Size Estimation Techniques in
  OLAP","  A data warehouse cannot materialize all possible views, hence we must
estimate quickly, accurately, and reliably the size of views to determine the
best candidates for materialization. Many available techniques for view-size
estimation make particular statistical assumptions and their error can be
large. Comparatively, unassuming probabilistic techniques are slower, but they
estimate accurately and reliability very large view sizes using little memory.
We compare five unassuming hashing-based view-size estimation techniques
including Stochastic Probabilistic Counting and LogLog Probabilistic Counting.
Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and
Adaptive Counting provide universally tight estimates irrespective of the size
of the view; of those, only Adaptive Counting remains constantly fast as we
increase the memory budget.
"
2491,A Technical Report On Grid Benchmarking using SEE V.O,"  Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.
"
2492,"Differentiated End-to-End Internet Services using a Weighted
  Proportional Fair Sharing TCP","  In this document we study the application of weighted proportional fairness
to data flows in the Internet. We let the users set the weights of their
connections in order to maximise the utility they get from the network. When
combined with a pricing scheme where connections are billed by weight and time,
such a system is known to maximise the total utility of the network. Our study
case is a national Web cache server connected to long distance links. We
propose two ways of weighting TCP connections by manipulating some parameters
of the protocol and present results from simulations and prototypes. We finally
discuss how proportional fairness could be used to implement an Internet with
differentiated services.
"
2493,Performance / Price Sort,"  NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but
excellent price performance. In particular, running on mail-order hardware it
can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort
from Robert Ramey Software Development has elapsed time performance comparable
to NTsort, while using less processor time. It can sort 1.27 GB for a penny
(12.7 million records.) These sorts set new price-performance records. This
paper documents this and proposes that the PennySort benchmark be revised to
Performance/Price sort: a simple GB/$ sort metric based on a two-pass external
sort.
"
2494,C++ Templates as Partial Evaluation,"  This paper explores the relationship between C++ templates and partial
evaluation. Templates were designed to support generic programming, but
unintentionally provided the ability to perform compile-time computations and
code generation. These features are completely accidental, and as a result
their syntax is awkward. By recasting these features in terms of partial
evaluation, a much simpler syntax can be achieved. C++ may be regarded as a
two-level language in which types are first-class values. Template
instantiation resembles an offline partial evaluator. This paper describes
preliminary work toward a single mechanism based on Partial Evaluation which
unifies generic programming, compile-time computation and code generation. The
language Catat is introduced to illustrate these ideas.
"
2495,"Algorithms of Two-Level Parallelization for DSMC of Unsteady Flows in
  Molecular Gasdynamics","  The general scheme of two-level parallelization (TLP) for direct simulation
Monte Carlo of unsteady gas flows on shared memory multiprocessor computers has
been described. The high efficient algorithm of parallel independent runs is
used on the first level. The data parallelization is employed for the second
one. Two versions of TLP algorithm are elaborated with static and dynamic load
balancing. The method of dynamic processor reallocation is used for dynamic
load balancing. Two gasdynamic unsteady problems were used to study speedup and
efficiency of the algorithms. The conditions of efficient application field for
the algorithms have been determined.
"
2496,Brittle System Analysis,"  The goal of this paper is to define and analyze systems which exhibit brittle
behavior. This behavior is characterized by a sudden and steep decline in
performance as the system approaches the limits of tolerance. This can be due
to input parameters which exceed a specified input, or environmental conditions
which exceed specified operating boundaries. An analogy is made between brittle
commmunication systems in particular and materials science.
"
2497,The ATLAS Tile Calorimeter Test Beam Monitoring Program,"  During 2003 test beam session for ATLAS Tile Calorimeter a monitoring program
has been developed to ease the setup of correct running condition and the
assessment of data quality. The program has been built using the Online
Software services provided by the ATLAS Online Software group. The first part
of this note contains a brief overview of these services followed by the full
description of Tile Calorimeter monitoring program architecture and features.
Performances and future upgrades are discussed in the final part of this note.
"
