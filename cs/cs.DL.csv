,title,abstract
0,"Supporting Knowledge and Expertise Finding within Australia's Defence
  Science and Technology Organisation","  This paper reports on work aimed at supporting knowledge and expertise
finding within a large Research and Development (R&D) organisation. The paper
first discusses the nature of knowledge important to R&D organisations and
presents a prototype information system developed to support knowledge and
expertise finding. The paper then discusses a trial of the system within an R&D
organisation, the implications and limitations of the trial, and discusses
future research questions.
"
1,Exploiting Social Annotation for Automatic Resource Discovery,"  Information integration applications, such as mediators or mashups, that
require access to information resources currently rely on users manually
discovering and integrating them in the application. Manual resource discovery
is a slow process, requiring the user to sift through results obtained via
keyword-based search. Although search methods have advanced to include evidence
from document contents, its metadata and the contents and link structure of the
referring pages, they still do not adequately cover information sources --
often called ``the hidden Web''-- that dynamically generate documents in
response to a query. The recently popular social bookmarking sites, which allow
users to annotate and share metadata about various information sources, provide
rich evidence for resource discovery. In this paper, we describe a
probabilistic model of the user annotation process in a social bookmarking
system del.icio.us. We then use the model to automatically find resources
relevant to a particular information domain. Our experimental results on data
obtained from \emph{del.icio.us} show this approach as a promising method for
helping automate the resource discovery task.
"
2,Personalizing Image Search Results on Flickr,"  The social media site Flickr allows users to upload their photos, annotate
them with tags, submit them to groups, and also to form social networks by
adding other users as contacts. Flickr offers multiple ways of browsing or
searching it. One option is tag search, which returns all images tagged with a
specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an
insect or a car, tag search results will include many images that are not
relevant to the sense the user had in mind when executing the query. We claim
that users express their photography interests through the metadata they add in
the form of contacts and image annotations. We show how to exploit this
metadata to personalize search results for the user, thereby improving search
performance. First, we show that we can significantly improve search precision
by filtering tag search results by user's contacts or a larger social network
that includes those contact's contacts. Secondly, we describe a probabilistic
model that takes advantage of tag information to discover latent topics
contained in the search results. The users' interests can similarly be
described by the tags they used for annotating their images. The latent topics
found by the model are then used to personalize search results by finding
images on topics that are of interest to the user.
"
3,Recommending Related Papers Based on Digital Library Access Records,"  An important goal for digital libraries is to enable researchers to more
easily explore related work. While citation data is often used as an indicator
of relatedness, in this paper we demonstrate that digital access records (e.g.
http-server logs) can be used as indicators as well. In particular, we show
that measures based on co-access provide better coverage than co-citation, that
they are available much sooner, and that they are more accurate for recent
papers.
"
4,Using Access Data for Paper Recommendations on ArXiv.org,"  This thesis investigates in the use of access log data as a source of
information for identifying related scientific papers. This is done for
arXiv.org, the authority for publication of e-prints in several fields of
physics.
  Compared to citation information, access logs have the advantage of being
immediately available, without manual or automatic extraction of the citation
graph. Because of that, a main focus is on the question, how far user behavior
can serve as a replacement for explicit meta-data, which potentially might be
expensive or completely unavailable. Therefore, we compare access, content, and
citation-based measures of relatedness on different recommendation tasks. As a
final result, an online recommendation system has been built that can help
scientists to find further relevant literature, without having to search for
them actively.
"
5,Evaluating Personal Archiving Strategies for Internet-based Information,"  Internet-based personal digital belongings present different vulnerabilities
than locally stored materials. We use responses to a survey of people who have
recovered lost websites, in combination with supplementary interviews, to paint
a fuller picture of current curatorial strategies and practices. We examine the
types of personal, topical, and commercial websites that respondents have lost
and the reasons they have lost this potentially valuable material. We further
explore what they have tried to recover and how the loss influences their
subsequent practices. We found that curation of personal digital materials in
online stores bears some striking similarities to the curation of similar
materials stored locally in that study participants continue to archive
personal assets by relying on a combination of benign neglect, sporadic
backups, and unsystematic file replication. However, we have also identified
issues specific to Internet-based material: how risk is spread by distributing
the files among multiple servers and services; the circular reasoning
participants use when they discuss the safety of their digital assets; and the
types of online material that are particularly vulnerable to loss. The study
reveals ways in which expectations of permanence and notification are violated
and situations in which benign neglect has far greater consequences for the
long-term fate of important digital assets.
"
6,"The Long Term Fate of Our Digital Belongings: Toward a Service Model for
  Personal Archives","  We conducted a preliminary field study to understand the current state of
personal digital archiving in practice. Our aim is to design a service for the
long-term storage, preservation, and access of digital belongings by examining
how personal archiving needs intersect with existing and emerging archiving
technologies, best practices, and policies. Our findings not only confirmed
that experienced home computer users are creating, receiving, and finding an
increasing number of digital belongings, but also that they have already lost
irreplaceable digital artifacts such as photos, creative efforts, and records.
Although participants reported strategies such as backup and file replication
for digital safekeeping, they were seldom able to implement them consistently.
Four central archiving themes emerged from the data: (1) people find it
difficult to evaluate the worth of accumulated materials; (2) personal storage
is highly distributed both on- and offline; (3) people are experiencing
magnified curatorial problems associated with managing files in the aggregate,
creating appropriate metadata, and migrating materials to maintainable formats;
and (4) facilities for long-term access are not supported by the current
desktop metaphor. Four environmental factors further complicate archiving in
consumer settings: the pervasive influence of malware; consumer reliance on ad
hoc IT providers; an accretion of minor system and registry inconsistencies;
and strong consumer beliefs about the incorruptibility of digital forms, the
reliability of digital technologies, and the social vulnerability of networked
storage.
"
7,Approximate textual retrieval,"  An approximate textual retrieval algorithm for searching sources with high
levels of defects is presented. It considers splitting the words in a query
into two overlapping segments and subsequently building composite regular
expressions from interlacing subsets of the segments. This procedure reduces
the probability of missed occurrences due to source defects, yet diminishes the
retrieval of irrelevant, non-contextual occurrences.
"
8,Tracking User Attention in Collaborative Tagging Communities,"  Collaborative tagging has recently attracted the attention of both industry
and academia due to the popularity of content-sharing systems such as
CiteULike, del.icio.us, and Flickr. These systems give users the opportunity to
add data items and to attach their own metadata (or tags) to stored data. The
result is an effective content management tool for individual users. Recent
studies, however, suggest that, as tagging communities grow, the added content
and the metadata become harder to manage due to an ease in content diversity.
Thus, mechanisms that cope with increase of diversity are fundamental to
improve the scalability and usability of collaborative tagging systems. This
paper analyzes whether usage patterns can be harnessed to improve navigability
in a growing knowledge space. To this end, it presents a characterization of
two collaborative tagging communities that target scientific literature:
CiteULike and Bibsonomy. We explore three main directions: First, we analyze
the tagging activity distribution across the user population. Second, we define
new metrics for similarity in user interest and use these metrics to uncover
the structure of the tagging communities we study. The structure we uncover
suggests a clear segmentation of interests into a large number of individuals
with unique preferences and a core set of users with interspersed interests.
Finally, we offer preliminary results that demonstrate that the interest-based
structure of the tagging community can be used to facilitate content usage as
communities scale.
"
9,Scientific citations in Wikipedia,"  The Internet-based encyclopaedia Wikipedia has grown to become one of the
most visited web-sites on the Internet. However, critics have questioned the
quality of entries, and an empirical study has shown Wikipedia to contain
errors in a 2005 sample of science entries. Biased coverage and lack of sources
are among the ""Wikipedia risks"". The present work describes a simple assessment
of these aspects by examining the outbound links from Wikipedia articles to
articles in scientific journals with a comparison against journal statistics
from Journal Citation Reports such as impact factors. The results show an
increasing use of structured citation markup and good agreement with the
citation pattern seen in the scientific literature though with a slight
tendency to cite articles in high-impact journals such as Nature and Science.
These results increase confidence in Wikipedia as an good information organizer
for science in general.
"
10,"Open Access Publishing in Particle Physics: A Brief Introduction for the
  non-Expert","  Open Access to particle physics literature does not sound particularly new or
exciting, since particle physicists have been reading preprints for decades,
and arXiv.org for 15 years. However new movements in Europe are attempting to
make the peer-reviewed literature of the field fully Open Access. This is not a
new movement, nor is it restricted to this field. However, given the field's
history of preprints and eprints, it is well suited to a change to a fully Open
Access publishing model. Data shows that 90% of HEP published literature is
freely available online, meaning that HEP libraries have little need for
expensive journal subscriptions. As libraries begin to cancel journal
subscriptions, the peer review process will lose its primary source of funding.
Open Access publishing models can potentially address this issue. European
physicists and funding agencies are proposing a consortium, SCOAP3, that might
solve many of the objections to traditional Open Access publishing models in
Particle Physics. These proposed changes should be viewed as a starting point
for a serious look at the field's publication model, and are at least worthy of
attention, if not adoption.
"
11,"Submission of content to a digital object repository using a
  configurable workflow system","  The prototype of a workflow system for the submission of content to a digital
object repository is here presented. It is based entirely on open-source
standard components and features a service-oriented architecture. The front-end
consists of Java Business Process Management (jBPM), Java Server Faces (JSF),
and Java Server Pages (JSP). A Fedora Repository and a mySQL data base
management system serve as a back-end. The communication between front-end and
back-end uses a SOAP minimal binding stub. We describe the design principles
and the construction of the prototype and discuss the possibilities and
limitations of work ow creation by administrators. The code of the prototype is
open-source and can be retrieved in the project escipub at
http://sourceforge.net
"
12,"Removing Manually-Generated Boilerplate from Electronic Texts:
  Experiments with Project Gutenberg e-Books","  Collaborative work on unstructured or semi-structured documents, such as in
literature corpora or source code, often involves agreed upon templates
containing metadata. These templates are not consistent across users and over
time. Rule-based parsing of these templates is expensive to maintain and tends
to fail as new documents are added. Statistical techniques based on frequent
occurrences have the potential to identify automatically a large fraction of
the templates, thus reducing the burden on the programmers. We investigate the
case of the Project Gutenberg corpus, where most documents are in ASCII format
with preambles and epilogues that are often copied and pasted or manually
typed. We show that a statistical approach can solve most cases though some
documents require knowledge of English. We also survey various technical
solutions that make our approach applicable to large data sets.
"
13,OA@MPS - a colourful view,"  The open access agenda of the Max Planck Society, initiator of the Berlin
Declaration, envisions the support of both the green way and the golden way to
open access. For the implementation of the green way the Max Planck Society
through its newly established unit (Max Planck Digital Library) follows the
idea of providing a centralized technical platform for publications and a local
support for editorial issues. With regard to the golden way, the Max Planck
Society fosters the development of open access publication models and
experiments new publishing concepts like the Living Reviews journals.
"
14,An exploratory study of Google Scholar,"  The paper discusses and analyzes the scientific search service Google Scholar
(GS). The focus is on an exploratory study which investigates the coverage of
scientific serials in GS. The study shows deficiencies in the coverage and
up-to-dateness of the GS index. Furthermore, the study points up which Web
servers are the most important data providers for this search service and which
information sources are highly represented. We can show that there is a
relatively large gap in Google Scholars coverage of German literature as well
as weaknesses in the accessibility of Open Access content.
  Keywords: Search engines, Digital libraries, Worldwide Web, Serials,
Electronic journals
"
15,"Reconstruction of Protein-Protein Interaction Pathways by Mining
  Subject-Verb-Objects Intermediates","  The exponential increase in publication rate of new articles is limiting
access of researchers to relevant literature. This has prompted the use of text
mining tools to extract key biological information. Previous studies have
reported extensive modification of existing generic text processors to process
biological text. However, this requirement for modification had not been
examined. In this study, we have constructed Muscorian, using MontyLingua, a
generic text processor. It uses a two-layered generalization-specialization
paradigm previously proposed where text was generically processed to a suitable
intermediate format before domain-specific data extraction techniques are
applied at the specialization layer. Evaluation using a corpus and experts
indicated 86-90% precision and approximately 30% recall in extracting
protein-protein interactions, which was comparable to previous studies using
either specialized biological text processing tools or modified existing tools.
Our study had also demonstrated the flexibility of the two-layered
generalization-specialization paradigm by using the same generalization layer
for two specialized information extraction tasks.
"
16,"A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts
  and their Usage","  The large-scale analysis of scholarly artifact usage is constrained primarily
by current practices in usage data archiving, privacy issues concerned with the
dissemination of usage data, and the lack of a practical ontology for modeling
the usage domain. As a remedy to the third constraint, this article presents a
scholarly ontology that was engineered to represent those classes for which
large-scale bibliographic and usage data exists, supports usage research, and
whose instantiation is scalable to the order of 50 million articles along with
their associated artifacts (e.g. authors and journals) and an accompanying 1
billion usage events. The real world instantiation of the presented abstract
ontology is a semantic network model of the scholarly community which lends the
scholarly process to statistical analysis and computational support. We present
the ontology, discuss its instantiation, and provide some example inference
rules for calculating various scholarly artifact metrics.
"
17,"Open Access does not increase citations for research articles from The
  Astrophysical Journal","  We demonstrate conclusively that there is no ""Open Access Advantage"" for
papers from the Astrophysical Journal. The two to one citation advantage
enjoyed by papers deposited in the arXiv e-print server is due entirely to the
nature and timing of the deposited papers. This may have implications for other
disciplines.
"
18,When are recommender systems useful?,"  Recommender systems are crucial tools to overcome the information overload
brought about by the Internet. Rigorous tests are needed to establish to what
extent sophisticated methods can improve the quality of the predictions. Here
we analyse a refined correlation-based collaborative filtering algorithm and
compare it with a novel spectral method for recommending. We test them on two
databases that bear different statistical properties (MovieLens and Jester)
without filtering out the less active users and ordering the opinions in time,
whenever possible. We find that, when the distribution of user-user
correlations is narrow, simple averages work nearly as well as advanced
methods. Recommender systems can, on the other hand, exploit a great deal of
additional information in systems where external influence is negligible and
peoples' tastes emerge entirely. These findings are validated by simulations
with artificially generated data.
"
19,Content Reuse and Interest Sharing in Tagging Communities,"  Tagging communities represent a subclass of a broader class of user-generated
content-sharing online communities. In such communities users introduce and tag
content for later use. Although recent studies advocate and attempt to harness
social knowledge in this context by exploiting collaboration among users,
little research has been done to quantify the current level of user
collaboration in these communities. This paper introduces two metrics to
quantify the level of collaboration: content reuse and shared interest. Using
these two metrics, this paper shows that the current level of collaboration in
CiteULike and Connotea is consistently low, which significantly limits the
potential of harnessing the social knowledge in communities. This study also
discusses implications of these findings in the context of recommendation and
reputation systems.
"
20,Simrank++: Query rewriting through link analysis of the click graph,"  We focus on the problem of query rewriting for sponsored search. We base
rewrites on a historical click graph that records the ads that have been
clicked on in response to past user queries. Given a query q, we first consider
Simrank as a way to identify queries similar to q, i.e., queries whose ads a
user may be interested in. We argue that Simrank fails to properly identify
query similarities in our application, and we present two enhanced version of
Simrank: one that exploits weights on click graph edges and another that
exploits ``evidence.'' We experimentally evaluate our new schemes against
Simrank, using actual click graphs and queries form Yahoo!, and using a variety
of metrics. Our results show that the enhanced methods can yield more and
better query rewrites.
"
21,"The Importance of Being First: Position Dependent Citation Rates on
  arXiv:astro-ph","  We study the dependence of citation counts of e-prints published on the
arXiv:astro-ph server on their position in the daily astro-ph listing. Using
the SPIRES literature database we reconstruct the astro-ph listings from July
2002 to December 2005 and determine citation counts for e-prints from their ADS
entry. We use Zipf plots to analyze the citation distributions for each
astro-ph position. We find that e-prints appearing at or near the top of the
astro-ph mailings receive significantly more citations than those further down
the list. This difference is significant at the 7 sigma level and on average
amounts to two times more citations for papers at the top than those further
down the listing. We propose three possible non-exclusive explanations for this
positional citation effect and try to test them. We conclude that
self-promotion by authors plays a role in the observed effect but cannot
exclude that increased visibility at the top of the daily listings contributes
to higher citation counts as well. We can rule out that the positional
dependence of citations is caused by the coincidence of the submission deadline
with the working hours of a geographically constrained set of intrinsically
higher cited authors. We discuss several ways of mitigating the observed
effect, including splitting astro-ph into several subject classes, randomizing
the order of e-prints, and a novel approach to sorting entries by relevance to
individual readers.
"
22,Ranking forestry journals using the h-index,"  An expert ranking of forestry journals was compared with journal impact
factors and h-indices computed from the ISI Web of Science and internet-based
data. Citations reported by Google Scholar appear to offer the most efficient
way to rank all journals objectively, in a manner consistent with other
indicators. This h-index exhibited a high correlation with the journal impact
factor (r=0.92), but is not confined to journals selected by any particular
commercial provider. A ranking of 180 forestry journals is presented, on the
basis of this index.
"
23,"Reducing semantic complexity in distributed Digital Libraries: treatment
  of term vagueness and document re-ranking","  The purpose of the paper is to propose models to reduce the semantic
complexity in heterogeneous DLs. The aim is to introduce value-added services
(treatment of term vagueness and document re-ranking) that gain a certain
quality in DLs if they are combined with heterogeneity components established
in the project ""Competence Center Modeling and Treatment of Semantic
Heterogeneity"". Empirical observations show that freely formulated user terms
and terms from controlled vocabularies are often not the same or match just by
coincidence. Therefore, a value-added service will be developed which rephrases
the natural language searcher terms into suggestions from the controlled
vocabulary, the Search Term Recommender (STR). Two methods, which are derived
from scientometrics and network analysis, will be implemented with the
objective to re-rank result sets by the following structural properties: the
ranking of the results by core journals (so-called Bradfordizing) and ranking
by centrality of authors in co-authorship networks.
"
24,"Spam: It's Not Just for Inboxes and Search Engines! Making Hirsch
  h-index Robust to Scientospam","  What is the 'level of excellence' of a scientist and the real impact of
his/her work upon the scientific thinking and practising? How can we design a
fair, an unbiased metric -- and most importantly -- a metric robust to
manipulation?
"
25,"On the relationship between the structural and socioacademic communities
  of a coauthorship network","  This article presents a study that compares detected structural communities
in a coauthorship network to the socioacademic characteristics of the scholars
that compose the network. The coauthorship network was created from the
bibliographic record of a multi-institution, interdisciplinary research group
focused on the study of sensor networks and wireless communication. Four
different community detection algorithms were employed to assign a structural
community to each scholar in the network: leading eigenvector, walktrap, edge
betweenness and spinglass. Socioacademic characteristics were gathered from the
scholars and include such information as their academic department, academic
affiliation, country of origin, and academic position. A Pearson's $\chi^2$
test, with a simulated Monte Carlo, revealed that structural communities best
represent groupings of individuals working in the same academic department and
at the same institution. A generalization of this result suggests that, even in
interdisciplinary, multi-institutional research groups, coauthorship is
primarily driven by departmental and institutional affiliation.
"
26,"Online-concordance ""Perekhresni stezhky"" (""The Cross-Paths""), a novel by
  Ivan Franko","  In the article, theoretical principles and practical realization for the
compilation of the concordance to ""Perekhresni stezhky"" (""The Cross-Paths""), a
novel by Ivan Franko, are described. Two forms for the context presentation are
proposed. The electronic version of this lexicographic work is available
online.
"
27,Knowledge management by wikis,"  Wikis provide a new way of collaboration and knowledge sharing. Wikis are
software that allows users to work collectively on a web-based knowledge base.
Wikis are characterised by a sense of anarchism, collaboration, connectivity,
organic development and self-healing, and they rely on trust. We list several
concerns about applying wikis in professional organisation. After these
concerns are met, wikis can provide a progessive, new knowledge sharing and
collaboration tool.
"
28,"NCore: Architecture and Implementation of a Flexible, Collaborative
  Digital Library","  NCore is an open source architecture and software platform for creating
flexible, collaborative digital libraries. NCore was developed by the National
Science Digital Library (NSDL) project, and it serves as the central technical
infrastructure for NSDL. NCore consists of a central Fedora-based digital
repository, a specific data model, an API, and a set of backend services and
frontend tools that create a new model for collaborative, contributory digital
libraries. This paper describes NCore, presents and analyzes its architecture,
tools and services; and reports on the experience of NSDL in building and
operating a major digital library on it over the past year and the experience
of the Digital Library for Earth Systems Education in porting their existing
digital library and tools to the NCore platform.
"
29,The aDORe Federation Architecture,"  The need to federate repositories emerges in two distinctive scenarios. In
one scenario, scalability-related problems in the operation of a repository
reach a point beyond which continued service requires parallelization and hence
federation of the repository infrastructure. In the other scenario, multiple
distributed repositories manage collections of interest to certain communities
or applications, and federation is an approach to present a unified perspective
across these repositories. The high-level, 3-Tier aDORe federation architecture
can be used as a guideline to federate repositories in both cases. This paper
describes the architecture, consisting of core interfaces for federated
repositories in Tier-1, two shared infrastructure components in Tier-2, and a
single-point of access to the federation in Tier-3. The paper also illustrates
two large-scale deployments of the aDORe federation architecture: the aDORe
Archive repository (over 100,000,000 digital objects) at the Los Alamos
National Laboratory and the Ghent University Image Repository federation
(multiple terabytes of image files).
"
30,Object Re-Use & Exchange: A Resource-Centric Approach,"  The OAI Object Reuse and Exchange (OAI-ORE) framework recasts the
repository-centric notion of digital object to a bounded aggregation of Web
resources. In this manner, digital library content is more integrated with the
Web architecture, and thereby more accessible to Web applications and clients.
This generalized notion of an aggregation that is independent of repository
containment conforms more closely with notions in eScience and eScholarship,
where content is distributed across multiple services and databases. We provide
a motivation for the OAI-ORE project, review previous interoperability efforts,
describe draft ORE specifications and report on promising results from early
experimentation that illustrate improved interoperability and reuse of digital
objects.
"
31,"Information Resources in High-Energy Physics: Surveying the Present
  Landscape and Charting the Future Course","  Access to previous results is of paramount importance in the scientific
process. Recent progress in information management focuses on building
e-infrastructures for the optimization of the research workflow, through both
policy-driven and user-pulled dynamics. For decades, High-Energy Physics (HEP)
has pioneered innovative solutions in the field of information management and
dissemination. In light of a transforming information environment, it is
important to assess the current usage of information resources by researchers
and HEP provides a unique test-bed for this assessment. A survey of about 10%
of practitioners in the field reveals usage trends and information needs.
Community-based services, such as the pioneering arXiv and SPIRES systems,
largely answer the need of the scientists, with a limited but increasing
fraction of younger users relying on Google. Commercial services offered by
publishers or database vendors are essentially unused in the field. The survey
offers an insight into the most important features that users require to
optimize their research workflow. These results inform the future evolution of
information management in HEP and, as these researchers are traditionally
``early adopters'' of innovation in scholarly communication, can inspire
developments of disciplinary repositories serving other communities.
"
32,"Towards Usage-based Impact Metrics: - First Results from the MESUR
  Project","  Scholarly usage data holds the potential to be used as a tool to study the
dynamics of scholarship in real time, and to form the basis for the definition
of novel metrics of scholarly impact. However, the formal groundwork to
reliably and validly exploit usage data is lacking, and the exact nature,
meaning and applicability of usage-based metrics is poorly understood. The
MESUR project funded by the Andrew W. Mellon Foundation constitutes a
systematic effort to define, validate and cross-validate a range of usage-based
metrics of scholarly impact. MESUR has collected nearly 1 billion usage events
as well as all associated bibliographic and citation data from significant
publishers, aggregators and institutional consortia to construct a large-scale
usage data reference set. This paper describes some major challenges related to
aggregating and processing usage data, and discusses preliminary results
obtained from analyzing the MESUR reference data set. The results confirm the
intrinsic value of scholarly usage data, and support the feasibility of
reliable and valid usage-based metrics of scholarly impact.
"
33,"Specification of an extensible and portable file format for electronic
  structure and crystallographic data","  In order to allow different software applications, in constant evolution, to
interact and exchange data, flexible file formats are needed. A file format
specification for different types of content has been elaborated to allow
communication of data for the software developed within the European Network of
Excellence ""NANOQUANTA"", focusing on first-principles calculations of materials
and nanosystems. It might be used by other software as well, and is described
here in detail. The format relies on the NetCDF binary input/output library,
already used in many different scientific communities, that provides
flexibility as well as portability accross languages and platforms. Thanks to
NetCDF, the content can be accessed by keywords, ensuring the file format is
extensible and backward compatible.
"
34,"Disentangling Visibility and Self-Promotion Bias in the arXiv:astro-ph
  Positional Citation Effect","  We established in an earlier study that articles listed at or near the top of
the daily arXiv:astro-ph mailings receive on average significantly more
citations than articles further down the list. In our earlier work we were not
able to decide whether this positional citation effect was due to author
self-promotion of intrinsically more citable papers or whether papers are cited
more often simply because they are at the top of the astro-ph listing. Using
new data we can now disentangle both effects. Based on their submission times
we separate articles into a self-promoted sample and a sample of articles that
achieved a high rank on astro-ph by chance and compare their citation
distributions with those of articles on lower astro-ph positions. We find that
the positional citation effect is a superposition of self-promotion and
visibility bias.
"
35,Clustering of scientific citations in Wikipedia,"  The instances of templates in Wikipedia form an interesting data set of
structured information. Here I focus on the cite journal template that is
primarily used for citation to articles in scientific journals. These citations
can be extracted and analyzed: Non-negative matrix factorization is performed
on a (article x journal) matrix resulting in a soft clustering of Wikipedia
articles and scientific journals, each cluster more or less representing a
scientific topic.
"
36,"Semantic Analysis of Tag Similarity Measures in Collaborative Tagging
  Systems","  Social bookmarking systems allow users to organise collections of resources
on the Web in a collaborative fashion. The increasing popularity of these
systems as well as first insights into their emergent semantics have made them
relevant to disciplines like knowledge extraction and ontology learning. The
problem of devising methods to measure the semantic relatedness between tags
and characterizing it semantically is still largely open. Here we analyze three
measures of tag relatedness: tag co-occurrence, cosine similarity of
co-occurrence distributions, and FolkRank, an adaptation of the PageRank
algorithm to folksonomies. Each measure is computed on tags from a large-scale
dataset crawled from the social bookmarking system del.icio.us. To provide a
semantic grounding of our findings, a connection to WordNet (a semantic lexicon
for the English language) is established by mapping tags into synonym sets of
WordNet, and applying there well-known metrics of semantic similarity. Our
results clearly expose different characteristics of the selected measures of
relatedness, making them applicable to different subtasks of knowledge
extraction such as synonym detection or discovery of concept hierarchies.
"
37,"Innovation in Scholarly Communication: Vision and Projects from
  High-Energy Physics","  Having always been at the forefront of information management and open
access, High-Energy Physics (HEP) proves to be an ideal test-bed for
innovations in scholarly communication including new information and
communication technologies. Three selected topics of scholarly communication in
High-Energy Physics are presented here: A new open access business model,
SCOAP3, a world-wide sponsoring consortium for peer-reviewed HEP literature;
the design, development and deployment of an e-infrastructure for information
management; and the emerging debate on long-term preservation, re-use and
(open) access to HEP data.
"
38,"LCSH, SKOS and Linked Data","  A technique for converting Library of Congress Subject Headings MARCXML to
Simple Knowledge Organization System (SKOS) RDF is described. Strengths of the
SKOS vocabulary are highlighted, as well as possible points for extension, and
the integration of other semantic web vocabularies such as Dublin Core. An
application for making the vocabulary available as linked-data on the Web is
also described.
"
39,"Universality of citation distributions: towards an objective measure of
  scientific impact","  We study the distributions of citations received by a single publication
within several disciplines, spanning broad areas of science. We show that the
probability that an article is cited $c$ times has large variations between
different disciplines, but all distributions are rescaled on a universal curve
when the relative indicator $c_f=c/c_0$ is considered, where $c_0$ is the
average number of citations per article for the discipline. In addition we show
that the same universal behavior occurs when citation distributions of articles
published in the same field, but in different years, are compared. These
findings provide a strong validation of $c_f$ as an unbiased indicator for
citation performance across disciplines and years. Based on this indicator, we
introduce a generalization of the h-index suitable for comparing scientists
working in different fields.
"
40,"ICT, Community Memory and Technological Appropriation","  The core mission of universities and higher education institutions is to make
public the results of their work and to preserve the collective memory of the
institution. This includes the effective use of information and communication
technologies (ICT) to systematically compile academic and research achievements
as well as disseminate effectively this accumulated knowledge to society at
large. Current efforts in Latin America and Venezuela in particular, are
limited but provide some valuable insights to pave the road to this important
goal. The institutional repository of Universidad de Los Andes (ULA) in
Venezuela (www.saber.ula.ve) is such an example of ICT usage to store, manage
and disseminate digital material produced by our University. In this paper we
elaborate on the overall process of promoting a culture of content creation,
publishing and preservation within ULA.
"
41,"Cross-concordances: terminology mapping and its effectiveness for
  information retrieval","  The German Federal Ministry for Education and Research funded a major
terminology mapping initiative, which found its conclusion in 2007. The task of
this terminology mapping initiative was to organize, create and manage
'cross-concordances' between controlled vocabularies (thesauri, classification
systems, subject heading lists) centred around the social sciences but quickly
extending to other subject areas. 64 crosswalks with more than 500,000
relations were established. In the final phase of the project, a major
evaluation effort to test and measure the effectiveness of the vocabulary
mappings in an information system environment was conducted. The paper reports
on the cross-concordance work and evaluation results.
"
42,Automatic Metadata Generation using Associative Networks,"  In spite of its tremendous value, metadata is generally sparse and
incomplete, thereby hampering the effectiveness of digital information
services. Many of the existing mechanisms for the automated creation of
metadata rely primarily on content analysis which can be costly and
inefficient. The automatic metadata generation system proposed in this article
leverages resource relationships generated from existing metadata as a medium
for propagation from metadata-rich to metadata-poor resources. Because of its
independence from content analysis, it can be applied to a wide variety of
resource media types and is shown to be computationally inexpensive. The
proposed method operates through two distinct phases. Occurrence and
co-occurrence algorithms first generate an associative network of repository
resources leveraging existing repository metadata. Second, using the
associative network as a substrate, metadata associated with metadata-rich
resources is propagated to metadata-poor resources by means of a discrete-form
spreading activation algorithm. This article discusses the general framework
for building associative networks, an algorithm for disseminating metadata
through such networks, and the results of an experiment and validation of the
proposed method using a standard bibliographic dataset.
"
43,Astrophysics in S.Co.P.E,"  S.Co.P.E. is one of the four projects funded by the Italian Government in
order to provide Southern Italy with a distributed computing infrastructure for
fundamental science. Beside being aimed at building the infrastructure,
S.Co.P.E. is also actively pursuing research in several areas among which
astrophysics and observational cosmology. We shortly summarize the most
significant results obtained in the first two years of the project and related
to the development of middleware and Data Mining tools for the Virtual
Observatory.
"
44,"Random drift versus selection in academic vocabulary: an evolutionary
  analysis of published keywords","  The evolution of vocabulary in academic publishing is characterized via
keyword frequencies recorded the ISI Web of Science citations database. In four
distinct case-studies, evolutionary analysis of keyword frequency change
through time is compared to a model of random copying used as the null
hypothesis, such that selection may be identified against it. The case studies
from the physical sciences indicate greater selection in keyword choice than in
the social sciences. Similar evolutionary analyses can be applied to a wide
range of phenomena; wherever the popularity of multiple items through time has
been recorded, as with web searches, or sales of popular music and books, for
example.
"
45,"Eigenfactor : Does the Principle of Repeated Improvement Result in
  Better Journal Impact Estimates than Raw Citation Counts?","  Eigenfactor.org, a journal evaluation tool which uses an iterative algorithm
to weight citations (similar to the PageRank algorithm used for Google) has
been proposed as a more valid method for calculating the impact of journals.
The purpose of this brief communication is to investigate whether the principle
of repeated improvement provides different rankings of journals than does a
simple unweighted citation count (the method used by ISI).
"
46,Approximating Document Frequency with Term Count Values,"  For bounded datasets such as the TREC Web Track (WT10g) the computation of
term frequency (TF) and inverse document frequency (IDF) is not difficult.
However, when the corpus is the entire web, direct IDF calculation is
impossible and values must instead be estimated. Most available datasets
provide values for term count (TC) meaning the number of times a certain term
occurs in the entire corpus. Intuitively this value is different from document
frequency (DF), the number of documents (e.g., web pages) a certain term occurs
in. We conduct a comparison study between TC and DF values within the Web as
Corpus (WaC). We found a very strong correlation with Spearman's rho >0.8
(p<0.005) which makes us confident in claiming that for such recently created
corpora the TC and DF values can be used interchangeably to compute IDF values.
These results are useful for the generation of accurate lexical signatures
based on the TF-IDF scheme.
"
47,A Distributed Process Infrastructure for a Distributed Data Structure,"  The Resource Description Framework (RDF) is continuing to grow outside the
bounds of its initial function as a metadata framework and into the domain of
general-purpose data modeling. This expansion has been facilitated by the
continued increase in the capacity and speed of RDF database repositories known
as triple-stores. High-end RDF triple-stores can hold and process on the order
of 10 billion triples. In an effort to provide a seamless integration of the
data contained in RDF repositories, the Linked Data community is providing
specifications for linking RDF data sets into a universal distributed graph
that can be traversed by both man and machine. While the seamless integration
of RDF data sets is important, at the scale of the data sets that currently
exist and will ultimately grow to become, the ""download and index"" philosophy
of the World Wide Web will not so easily map over to the Semantic Web. This
essay discusses the importance of adding a distributed RDF process
infrastructure to the current distributed RDF data structure.
"
48,Use of Astronomical Literature - A Report on Usage Patterns,"  In this paper we present a number of metrics for usage of the SAO/NASA
Astrophysics Data System (ADS). Since the ADS is used by the entire
astronomical community, these are indicative of how the astronomical literature
is used. We will show how the use of the ADS has changed both quantitatively
and qualitatively. We will also show that different types of users access the
system in different ways. Finally, we show how use of the ADS has evolved over
the years in various regions of the world.
  The ADS is funded by NASA Grant NNG06GG68G.
"
49,Building a terminology network for search: the KoMoHe project,"  The paper reports about results on the GESIS-IZ project ""Competence Center
Modeling and Treatment of Semantic Heterogeneity"" (KoMoHe). KoMoHe supervised a
terminology mapping effort, in which 'cross-concordances' between major
controlled vocabularies were organized, created and managed. In this paper we
describe the establishment and implementation of cross-concordances for search
in a digital library (DL).
"
50,"Comparing human and automatic thesaurus mapping approaches in the
  agricultural domain","  Knowledge organization systems (KOS), like thesauri and other controlled
vocabularies, are used to provide subject access to information systems across
the web. Due to the heterogeneity of these systems, mapping between
vocabularies becomes crucial for retrieving relevant information. However,
mapping thesauri is a laborious task, and thus big efforts are being made to
automate the mapping process. This paper examines two mapping approaches
involving the agricultural thesaurus AGROVOC, one machine-created and one human
created. We are addressing the basic question ""What are the pros and cons of
human and automatic mapping and how can they complement each other?"" By
pointing out the difficulties in specific cases or groups of cases and grouping
the sample into simple and difficult types of mappings, we show the limitations
of current automatic methods and come up with some basic recommendations on
what approach to use when.
"
51,"Author-choice open access publishing in the biological and medical
  literature: a citation analysis","  In this article, we analyze the citations to articles published in 11
biological and medical journals from 2003 to 2007 that employ author-choice
open access models. Controlling for known explanatory predictors of citations,
only 2 of the 11 journals show positive and significant open access effects.
Analyzing all journals together, we report a small but significant increase in
article citations of 17%. In addition, there is strong evidence to suggest that
the open access advantage is declining by about 7% per year, from 32% in 2004
to 11% in 2007.
"
52,"Confirmation Bias and the Open Access Advantage: Some Methodological
  Suggestions for the Davis Citation Study","  Davis (2008) analyzes citations from 2004-2007 in 11 biomedical journals. 15%
of authors paid to make them Open Access (OA). The outcome is a significant OA
citation Advantage, but a small one (21%). The author infers that the OA
advantage has been shrinking yearly, but the data suggest the opposite. Further
analyses are necessary:
  (1) Not just author-choice (paid) OA but Free OA self-archiving needs to be
taken into account rather than being counted as non-OA.
  (2) proportion of OA articles per journal per year needs to be reported and
taken into account.
  (3) The Journal Impact Factor and the relation between the size of the OA
Advantage article 'citation-bracket' need to be taken into account.
  (4) The sample-size for the highest-impact, largest-sample journal analyzed,
PNAS, is restricted and excluded from some of the analyses. The full PNAS
dataset is needed.
  (5) The interaction between OA and time, 2004-2007, is based on retrospective
data from a June 2008 total cumulative citation count. The dates of both the
cited articles and the citing articles need to be taken into account.
  The author proposes that author self-selection bias for is the primary cause
of the observed OA Advantage, but this study does not test this or of any of
the other potential causal factors. The author suggests that paid OA is not
worth the cost, per extra citation. But with OA self-archiving both the OA and
the extra citations are free.
"
53,The first-mover advantage in scientific publication,"  Mathematical models of the scientific citation process predict a strong
""first-mover"" effect under which the first papers in a field will, essentially
regardless of content, receive citations at a rate enormously higher than
papers published later. Moreover papers are expected to retain this advantage
in perpetuity -- they should receive more citations indefinitely, no matter how
many other papers are published after them. We test this conjecture against
data from a selection of fields and in several cases find a first-mover effect
of a magnitude similar to that predicted by the theory. Were we wearing our
cynical hat today, we might say that the scientist who wants to become famous
is better off -- by a wide margin -- writing a modest paper in next year's
hottest field than an outstanding paper in this year's. On the other hand,
there are some papers, albeit only a small fraction, that buck the trend and
attract significantly more citations than theory predicts despite having
relatively late publication dates. We suggest that papers of this kind, though
they often receive comparatively few citations overall, are probably worthy of
our attention.
"
54,How long should an astronomical paper be to increase its Impact?,"  Naively, one would expect longer papers to have larger impact (i.e., to be
cited more). I tested this expectation by selecting all (~30,000) refereed
papers from A&A, AJ, ApJ and MNRAS published between 2000 and 2004. These
particular years were chosen so papers analyzed would not be too ""fresh"", but
at the same time length of each article could be obtained via ADS. I find that
indeed longer papers published in these four major astronomy journals are on
average cited more, with a median number of citations increasing from 6 for
articles 2-3 pages long to about 50 for articles ~50 pages long. I do however
observe a significant ""Letters effect"", i.e. ApJ and A&A articles 4 pages long
are cited more than articles 5-10 pages long. Also, the very few longest (>80
pages) papers are actually cited less than somewhat shorter papers. For
individual journals, median citations per paper increase from 11 for ~9,300 A&A
papers to 14 for ~5,300 MNRAS papers, 16 for ~2,550 AJ papers, and 20 for
~12,850 ApJ papers (including ApJ Letters and Supplement). I conclude with some
semi-humorous career advice, directed especially at first-year graduate
students.
"
55,A Simple Framework to Typify Social Bibliographic Communities,"  Social Communities in bibliographic databases exist since many years,
researchers share common research interests, and work and publish together. A
social community may vary in type and size, being fully connected between
participating members or even more expressed by a consortium of small and
individual members who play individual roles in it. In this work, we focus on
social communities inside the bibliographic database DBLP and characterize
communities through a simple typifying description model. Generally, we
understand a publication as a transaction between the associated authors. The
idea therefore is to concern with directed associative relationships among
them, to decompose each pattern to its fundamental structure, and to describe
the communities by expressive attributes. Finally, we argue that the
decomposition supports the management of discovered structures towards the use
of adaptive-incremental mind-maps.
"
56,Correlation of Expert and Search Engine Rankings,"  In previous research it has been shown that link-based web page metrics can
be used to predict experts' assessment of quality. We are interested in a
related question: do expert rankings of real-world entities correlate with
search engine rankings of corresponding web resources? For example, each year
US News & World Report publishes a list of (among others) top 50 graduate
business schools. Does their expert ranking correlate with the search engine
ranking of the URLs of those business schools? To answer this question we
conducted 9 experiments using 8 expert rankings on a range of academic,
athletic, financial and popular culture topics. We compared the expert rankings
with the rankings in Google, Live Search (formerly MSN) and Yahoo (with list
lengths of 10, 25, and 50). In 57 search engine vs. expert comparisons, only 1
strong and 4 moderate correlations were statistically significant. In 42
inter-search engine comparisons, only 2 strong and 4 moderate correlations were
statistically significant. The correlations appeared to decrease with the size
of the lists: the 3 strong correlations were for lists of 10, the 8 moderate
correlations were for lists of 25, and no correlations were found for lists of
50.
"
57,"The decline in the concentration of citations, 1900-2007","  This paper challenges recent research (Evans, 2008) reporting that the
concentration of cited scientific literature increases with the online
availability of articles and journals. Using Thomson Reuters' Web of Science,
the present paper analyses changes in the concentration of citations received
(two- and five-year citation windows) by papers published between 1900 and
2005. Three measures of concentration are used: the percentage of papers that
received at least one citation (cited papers); the percentage of papers needed
to account for 20, 50 and 80 percent of the citations; and, the
Herfindahl-Hirschman index. These measures are used for four broad disciplines:
natural sciences and engineering, medical fields, social sciences, and the
humanities. All these measures converge and show that, contrary to what was
reported by Evans, the dispersion of citations is actually increasing.
"
58,Peer-review in the Internet age,"  The importance of peer-review in the scientific process can not be
overestimated. Yet, due to increasing pressures of research and exponentially
growing number of publications the task faced by the referees becomes ever more
difficult. We discuss here a few possible improvements that would enable more
efficient review of the scientific literature, using the growing Internet
connectivity. In particular, a practical automated model for providing the
referees with references to papers that might have strong relationship with the
work under review, based on general network properties of citations is
proposed.
"
59,Assembling Actor-based Mind-Maps from Text Stream,"  For human beings, the processing of text streams of unknown size leads
generally to problems because e.g. noise must be selected out, information be
tested for its relevance or redundancy, and linguistic phenomenon like
ambiguity or the resolution of pronouns be advanced. Putting this into
simulation by using an artificial mind-map is a challenge, which offers the
gate for a wide field of applications like automatic text summarization or
punctual retrieval. In this work we present a framework that is a first step
towards an automatic intellect. It aims at assembling a mind-map based on
incoming text streams and on a subject-verb-object strategy, having the verb as
an interconnection between the adjacent nouns. The mind-map's performance is
enriched by a pronoun resolution engine that bases on the work of D. Klein, and
C. D. Manning.
"
60,On Granular Knowledge Structures,"  Knowledge plays a central role in human and artificial intelligence. One of
the key characteristics of knowledge is its structured organization. Knowledge
can be and should be presented in multiple levels and multiple views to meet
people's needs in different levels of granularities and from different
perspectives. In this paper, we stand on the view point of granular computing
and provide our understanding on multi-level and multi-view of knowledge
through granular knowledge structures (GKS). Representation of granular
knowledge structures, operations for building granular knowledge structures and
how to use them are investigated. As an illustration, we provide some examples
through results from an analysis of proceeding papers. Results show that
granular knowledge structures could help users get better understanding of the
knowledge source from set theoretical, logical and visual point of views. One
may consider using them to meet specific needs or solve certain kinds of
problems.
"
61,"Combining Advanced Visualization and Automatized Reasoning for
  Webometrics: A Test Study","  This paper presents a first attempt at performing a precise and automatic
identification of the linking behaviour in a scientific domain through the
analysis of the communication of the related academic institutions on the web.
The proposed approach is based on the paradigm of multiple viewpoint data
analysis (MVDA) than can be fruitfully exploited to highlight relationships
between data, like websites, carrying several kinds of description. It uses the
MultiSOM clustering and mapping method. The domain that has been chosen for
this study is the domain of Computer Science in Germany. The analysis is
conduced on a set of 438 websites of this domain using all together, thematic,
geographic and linking information. It highlights interesting results
concerning both global and local linking behaviour.
"
62,A Web-Based Resource Model for eScience: Object Reuse & Exchange,"  Work in the Open Archives Initiative - Object Reuse and Exchange (OAI-ORE)
focuses on an important aspect of infrastructure for eScience: the
specification of the data model and a suite of implementation standards to
identify and describe compound objects. These are objects that aggregate
multiple sources of content including text, images, data, visualization tools,
and the like. These aggregations are an essential product of eScience, and will
become increasingly common in the age of data-driven scholarship. The OAI-ORE
specifications conform to the core concepts of the Web architecture and the
semantic Web, ensuring that applications that use them will integrate well into
the general Web environment.
"
63,"Visualization of association graphs for assisting the interpretation of
  classifications","  Given a query on the PASCAL database maintained by the INIST, we design user
interfaces to visualize and browse two types of graphs extracted from
abstracts: 1) the graph of all associations between authors (co-author graph),
2) the graph of strong associations between authors and terms automatically
extracted from abstracts and grouped using linguistic variations. We adapt for
this purpose the TermWatch system that comprises a term extractor, a relation
identifier which yields the terminological network and a clustering module. The
results are output on two interfaces: a graphic one mapping the clusters in a
2D space and a terminological hypertext network allowing the user to
interactively explore results and return to source texts.
"
64,"Origins of Modern Data Analysis Linked to the Beginnings and Early
  Development of Computer Science and Information Engineering","  The history of data analysis that is addressed here is underpinned by two
themes, -- those of tabular data analysis, and the analysis of collected
heterogeneous data. ""Exploratory data analysis"" is taken as the heuristic
approach that begins with data and information and seeks underlying explanation
for what is observed or measured. I also cover some of the evolving context of
research and applications, including scholarly publishing, technology transfer
and the economic relationship of the university to society.
"
65,"Collecting and Preserving Videogames and Their Related Materials: A
  Review of Current Practice, Game-Related Archives and Research Projects","  This paper reviews the major methods and theories regarding the preservation
of new media artifacts such as videogames, and argues for the importance of
collecting and coming to a better understanding of videogame artifacts of
creation, which will help build a more detailed understanding of the essential
qualities of these culturally significant artifacts. We will also review the
major videogame collections in the United States, Europe and Japan to give an
idea of the current state of videogame archives, and argue for a fuller, more
comprehensive coverage of these materials in institutional repositories.
"
66,"Anti Plagiarism Application with Algorithm Karp-Rabin at Thesis in
  Gunadarma University","  Plagiarism that is plagiarizing or composition retrieval, opinion, etcetera
from other people and makes it is likely composition and opinion him-self.
Plagiarism can be considered to be crime because stealing others copyrights.
Like action a student copying some part of writings without valid permission
from the original writer. In education world, plagiarism perpetrator can get
the devil to pay from school/university. Plagiarism perpetrator conceived of
plagiator. This thing is possible unable to be paid attention by the side of
campus because of limitation from some interconnected factors for example
student amounts Gunadarma University reaching thousands and incommensurate to
tester amounts or lecturer the side of campus in charge directs problem thesis.
In this paper, an application have been developed in order to check and look
for 5 type percentage similarity from a thesis with other one at certain part
or chapters. Percentage got that is 0%, under 15%, between 15-50%, up to 50%
and 100%. So it should be expected that the results could be used by thesis
advisor and also thesis examiner from the Student at Gunadarma University.
"
67,Frozen Footprints,"  Bibliometrics has the ambitious goal of measuring science. To this end, it
exploits the way science is disseminated trough scientific publications and the
resulting citation network of scientific papers. We survey the main historical
contributions to the field, the most interesting bibliometric indicators, and
the most popular bibliometric data sources. Moreover, we discuss distributions
commonly used to model bibliometric phenomena and give an overview of methods
to build bibliometric maps of science.
"
68,An evaluation of Bradfordizing effects,"  The purpose of this paper is to apply and evaluate the bibliometric method
Bradfordizing for information retrieval (IR) experiments. Bradfordizing is used
for generating core document sets for subject-specific questions and to reorder
result sets from distributed searches. The method will be applied and tested in
a controlled scenario of scientific literature databases from social and
political sciences, economics, psychology and medical science (SOLIS, SoLit,
USB Koeln Opac, CSA Sociological Abstracts, World Affairs Online, Psyndex and
Medline) and 164 standardized topics. An evaluation of the method and its
effects is carried out in two laboratory-based information retrieval
experiments (CLEF and KoMoHe) using a controlled document corpus and human
relevance assessments. The results show that Bradfordizing is a very robust
method for re-ranking the main document types (journal articles and monographs)
in today's digital libraries (DL). The IR tests show that relevance
distributions after re-ranking improve at a significant level if articles in
the core are compared with articles in the succeeding zones. The items in the
core are significantly more often assessed as relevant, than items in zone 2
(z2) or zone 3 (z3). The improvements between the zones are statistically
significant based on the Wilcoxon signed-rank test and the paired T-Test.
"
69,Questions & Answers for TEI Newcomers,"  This paper provides an introduction to the Text Encoding Initia-tive (TEI),
focused at bringing in newcomers who have to deal with a digital document
project and are looking at the capacity that the TEI environment may have to
fulfil his needs. To this end, we avoid a strictly technical presentation of
the TEI and concentrate on the actual issues that such projects face, with
parallel made on the situation within two institutions. While a quick
walkthrough the TEI technical framework is provided, the papers ends up by
showing the essential role of the community in the actual technical
contributions that are being brought to the TEI.
"
70,"Tsallis $q$-exponential describes the distribution of scientific
  citations - A new characterization of the impact","  In this work we have studied the research activity for countries of Europe,
Latin America and Africa for all sciences between 1945 and November 2008. All
the data are captured from the Web of Science database during this period. The
analysis of the experimental data shows that, within a nonextensive
thermostatistical formalism, the Tsallis \emph{q}-exponential distribution
$N(c)$ satisfactorily describes Institute of Scientific Information citations.
The data which are examined in the present survey can be fitted successfully as
a first approach by applying a {\it single} curve (namely, $N(c) \propto
1/[1+(q-1) c/T]^{\frac{1}{q-1}}$ with $q\simeq 4/3$ for {\it all} the available
citations $c$, $T$ being an ""effective temperature"". The present analysis
ultimately suggests that the phenomenon might essentially be {\it one and the
same} along the {\it entire} range of the citation number. Finally, this
manuscript provides a new ranking index, via the ""effective temperature"" $T$,
for the impact level of the research activity in these countries, taking into
account the number of the publications and their citations.
"
71,"Content-based and Algorithmic Classifications of Journals: Perspectives
  on the Dynamics of Scientific Communication and Indexer Effects","  The aggregated journal-journal citation matrix -based on the Journal Citation
Reports (JCR) of the Science Citation Index- can be decomposed by indexers
and/or algorithmically. In this study, we test the results of two recently
available algorithms for the decomposition of large matrices against two
content-based classifications of journals: the ISI Subject Categories and the
field/subfield classification of Glaenzel & Schubert (2003). The content-based
schemes allow for the attribution of more than a single category to a journal,
whereas the algorithms maximize the ratio of within-category citations over
between-category citations in the aggregated category-category citation matrix.
By adding categories, indexers generate between-category citations, which may
enrich the database, for example, in the case of inter-disciplinary
developments. The consequent indexer effects are significant in sparse areas of
the matrix more than in denser ones. Algorithmic decompositions, on the other
hand, are more heavily skewed towards a relatively small number of categories,
while this is deliberately counter-acted upon in the case of content-based
classifications. Because of the indexer effects, science policy studies and the
sociology of science should be careful when using content-based
classifications, which are made for bibliographic disclosure, and not for the
purpose of analyzing latent structures in scientific communications. Despite
the large differences among them, the four classification schemes enable us to
generate surprisingly similar maps of science at the global level. Erroneous
classifications are cancelled as noise at the aggregate level, but may disturb
the evaluation locally.
"
72,"Filtering Microarray Correlations by Statistical Literature Analysis
  Yields Potential Hypotheses for Lactation Research","  Our results demonstrated that a previously reported protein name
co-occurrence method (5-mention PubGene) which was not based on a hypothesis
testing framework, it is generally statistically more significant than the 99th
percentile of Poisson distribution-based method of calculating co-occurrence.
It agrees with previous methods using natural language processing to extract
protein-protein interaction from text as more than 96% of the interactions
found by natural language processing methods to overlap with the results from
5-mention PubGene method. However, less than 2% of the gene co-expressions
analyzed by microarray were found from direct co-occurrence or interaction
information extraction from the literature. At the same time, combining
microarray and literature analyses, we derive a novel set of 7 potential
functional protein-protein interactions that had not been previously described
in the literature.
"
73,"Revisiting the Age of Enlightenment from a Collective Decision Making
  Systems Perspective","  The ideals of the eighteenth century's Age of Enlightenment are the
foundation of modern democracies. The era was characterized by thinkers who
promoted progressive social reforms that opposed the long-established
aristocracies and monarchies of the time. Prominent examples of such reforms
include the establishment of inalienable human rights, self-governing
republics, and market capitalism. Twenty-first century democratic nations can
benefit from revisiting the systems developed during the Enlightenment and
reframing them within the techno-social context of the Information Age. This
article explores the application of social algorithms that make use of Thomas
Paine's (English: 1737--1809) representatives, Adam Smith's (Scottish:
1723--1790) self-interested actors, and Marquis de Condorcet's (French:
1743--1794) optimal decision making groups. It is posited that
technology-enabled social algorithms can better realize the ideals articulated
during the Enlightenment.
"
74,Effectively Searching Maps in Web Documents,"  Maps are an important source of information in archaeology and other
sciences. Users want to search for historical maps to determine recorded
history of the political geography of regions at different eras, to find out
where exactly archaeological artifacts were discovered, etc. Currently, they
have to use a generic search engine and add the term map along with other
keywords to search for maps. This crude method will generate a significant
number of false positives that the user will need to cull through to get the
desired results. To reduce their manual effort, we propose an automatic map
identification, indexing, and retrieval system that enables users to search and
retrieve maps appearing in a large corpus of digital documents using simple
keyword queries. We identify features that can help in distinguishing maps from
other figures in digital documents and show how a Support-Vector-Machine-based
classifier can be used to identify maps. We propose map-level-metadata e.g.,
captions, references to the maps in text, etc. and document-level metadata,
e.g., title, abstract, citations, how recent the publication is, etc. and show
how they can be automatically extracted and indexed. Our novel ranking
algorithm weights different metadata fields differently and also uses the
document-level metadata to help rank retrieved maps. Empirical evaluations show
which features should be selected and which metadata fields should be weighted
more. We also demonstrate improved retrieval results in comparison to
adaptations of existing methods for map retrieval. Our map search engine has
been deployed in an online map-search system that is part of the Blind-Review
digital library system.
"
75,Everyone is a Curator: Human-Assisted Preservation for ORE Aggregations,"  The Open Archives Initiative (OAI) has recently created the Object Reuse and
Exchange (ORE) project that defines Resource Maps (ReMs) for describing
aggregations of web resources. These aggregations are susceptible to many of
the same preservation challenges that face other web resources. In this paper,
we investigate how the aggregations of web resources can be preserved outside
of the typical repository environment and instead rely on the thousands of
interactive users in the web community and the Web Infrastructure (the
collection of web archives, search engines, and personal archiving services) to
facilitate preservation. Inspired by Web 2.0 services such as digg,
deli.cio.us, and Yahoo! Buzz, we have developed a lightweight system called
ReMember that attempts to harness the collective abilities of the web community
for preservation purposes instead of solely placing the burden of curatorial
responsibilities on a small number of experts.
"
76,A Simple Extraction Procedure for Bibliographical Author Field,"  A procedure for bibliographic author metadata extraction from scholarly texts
is presented. The author segments are identified based on capitalization and
line break patterns. Two main author layout templates, which can retrieve from
a varied set of title pages, are provided. Additionally, several disambiguating
rules are described.
"
77,A principal component analysis of 39 scientific impact measures,"  The impact of scientific publications has traditionally been expressed in
terms of citation counts. However, scientific activity has moved online over
the past decade. To better capture scientific impact in the digital era, a
variety of new impact measures has been proposed on the basis of social network
analysis and usage log data. Here we investigate how these new measures relate
to each other, and how accurately and completely they express scientific
impact. We performed a principal component analysis of the rankings produced by
39 existing and proposed measures of scholarly impact that were calculated on
the basis of both citation and usage log data. Our results indicate that the
notion of scientific impact is a multi-dimensional construct that can not be
adequately measured by any single indicator, although some measures are more
suitable than others. The commonly used citation Impact Factor is not
positioned at the core of this construct, but at its periphery, and should thus
be used with caution.
"
78,ImageSpace: An Environment for Image Ontology Management,"  More and more researchers have realized that ontologies will play a critical
role in the development of the Semantic Web, the next generation Web in which
content is not only consumable by humans, but also by software agents. The
development of tools to support ontology management including creation,
visualization, annotation, database storage, and retrieval is thus extremely
important. We have developed ImageSpace, an image ontology creation and
annotation tool that features (1) full support for the standard web ontology
language DAML+OIL; (2) image ontology creation, visualization, image annotation
and display in one integrated framework; (3) ontology consistency assurance;
and (4) storing ontologies and annotations in relational databases. It is
expected that the availability of such a tool will greatly facilitate the
creation of image repositories as islands of the Semantic Web.
"
79,OntoELAN: An Ontology-based Linguistic Multimedia Annotator,"  Despite its scientific, political, and practical value, comprehensive
information about human languages, in all their variety and complexity, is not
readily obtainable and searchable. One reason is that many language data are
collected as audio and video recordings which imposes a challenge to document
indexing and retrieval. Annotation of multimedia data provides an opportunity
for making the semantics explicit and facilitates the searching of multimedia
documents. We have developed OntoELAN, an ontology-based linguistic multimedia
annotator that features: (1) support for loading and displaying ontologies
specified in OWL; (2) creation of a language profile, which allows a user to
choose a subset of terms from an ontology and conveniently rename them if
needed; (3) creation of ontological tiers, which can be annotated with profile
terms and, therefore, corresponding ontological terms; and (4) saving
annotations in the XML format as Multimedia Ontology class instances and,
linked to them, class instances of other ontologies used in ontological tiers.
To our best knowledge, OntoELAN is the first audio/video annotation tool in
linguistic domain that provides support for ontology-based annotation.
"
80,"Ontology-Based Annotation of Multimedia Language Data for the Semantic
  Web","  There is an increasing interest and effort in preserving and documenting
endangered languages. Language data are valuable only when they are
well-cataloged, indexed and searchable. Many language data, particularly those
of lesser-spoken languages, are collected as audio and video recordings. While
multimedia data provide more channels and dimensions to describe a language's
function, and gives a better presentation of the cultural system associated
with the language of that community, they are not text-based or structured (in
binary format), and their semantics is implicit in their content. The content
is thus easy for a human being to understand, but difficult for computers to
interpret. Hence, there is a great need for a powerful and user-friendly system
to annotate multimedia data with text-based, well-structured and searchable
metadata. This chapter describes an ontology-based multimedia annotation tool,
OntoELAN, that enables annotation of language multimedia data with a linguistic
ontology.
"
81,Random hypergraphs and their applications,"  In the last few years we have witnessed the emergence, primarily in on-line
communities, of new types of social networks that require for their
representation more complex graph structures than have been employed in the
past. One example is the folksonomy, a tripartite structure of users,
resources, and tags -- labels collaboratively applied by the users to the
resources in order to impart meaningful structure on an otherwise
undifferentiated database. Here we propose a mathematical model of such
tripartite structures which represents them as random hypergraphs. We show that
it is possible to calculate many properties of this model exactly in the limit
of large network size and we compare the results against observations of a real
folksonomy, that of the on-line photography web site Flickr. We show that in
some cases the model matches the properties of the observed network well, while
in others there are significant differences, which we find to be attributable
to the practice of multiple tagging, i.e., the application by a single user of
many tags to one resource, or one tag to many resources.
"
82,Faceted Exploration of Emerging Resource Spaces,"  Humans have the ability to regcognize the real world from different facets.
Faceted exploration is a mechanism for browsing and understanding large-scale
resources in information network by multiple facets. This paper proposes an
Emerging Resource Space Model, whose schema is a partially ordered set of
concepts with subclassOf relation and each resource is categorized by multiple
concepts. Emering Resource Space (ERS) is a class of resources characterized by
a concept set. ERSes compose a lattice (ERSL) via concept association. A series
of exploration operations is proposed to guide users to explore through ERSL
with more demanding and richer semantics than current faceted navigation. To
fulfill instant response during faceted exploration, we devise an efficient
algorithm for mining and indexing ERSL. The proposed model can effectively
support faceted exploration in various applications from personal information
management to large-scale information sharing.
"
83,The Smithsonian/NASA Astrophysics Data System (ADS) Decennial Report,"  Eight years after the ADS first appeared the last decadal survey wrote:
""NASA's initiative for the Astrophysics Data System has vastly increased the
accessibility of the scientific literature for astronomers. NASA deserves
credit for this valuable initiative and is urged to continue it."" Here we
summarize some of the changes concerning the ADS which have occurred in the
past ten years, and we describe the current status of the ADS. We then point
out two areas where the ADS is building an improved capability which could
benefit from a policy statement of support in the ASTRO2010 report. These are:
The Semantic Interlinking of Astronomy Observations and Datasets and The
Indexing of the Full Text of Astronomy Research Publications.
"
84,Visual Conceptualizations and Models of Science,"  This Journal of Informetrics special issue aims to improve our understanding
of the structure and dynamics of science by reviewing and advancing existing
conceptualizations and models of scholarly activity. Several of these
conceptualizations and models have visual manifestations supporting the
combination and comparison of theories and approaches developed in different
disciplines of science. Subsequently, we discuss challenges towards a
theoretically grounded and practically useful science of science and provide a
brief chronological review of relevant work. Then, we exemplarily present three
conceptualizations of science that attempt to provide frameworks for the
comparison and combination of existing approaches, theories, laws, and
measurements. Finally, we discuss the contributions of and interlinkages among
the eight papers included in this issue. Each paper makes a unique contribution
towards conceptualizations and models of science and roots this contribution in
a review and comparison with existing work.
"
85,"Comparing Bibliometric Statistics Obtained from the Web of Science and
  Scopus","  For more than 40 years, the Institute for Scientific Information (ISI, now
part of Thomson Reuters) produced the only available bibliographic databases
from which bibliometricians could compile large-scale bibliometric indicators.
ISI's citation indexes, now regrouped under the Web of Science (WoS), were the
major sources of bibliometric data until 2004, when Scopus was launched by the
publisher Reed Elsevier. For those who perform bibliometric analyses and
comparisons of countries or institutions, the existence of these two major
databases raises the important question of the comparability and stability of
statistics obtained from different data sources. This paper uses macro-level
bibliometric indicators to compare results obtained from the WoS and Scopus. It
shows that the correlations between the measures obtained with both databases
for the number of papers and the number of citations received by countries, as
well as for their ranks, are extremely high (R2 > .99). There is also a very
high correlation when countries' papers are broken down by field. The paper
thus provides evidence that indicators of scientific production and citations
at the country level are stable and largely independent of the database.
"
86,Mapping the evolution of scientific fields,"  Despite the apparent cross-disciplinary interactions among scientific fields,
a formal description of their evolution is lacking. Here we describe a novel
approach to study the dynamics and evolution of scientific fields using a
network-based analysis. We build an idea network consisting of American
Physical Society Physics and Astronomy Classification Scheme (PACS) numbers as
nodes representing scientific concepts. Two PACS numbers are linked if there
exist publications that reference them simultaneously. We locate scientific
fields using a community finding algorithm, and describe the time evolution of
these fields over the course of 1985-2006. The communities we identify map to
known scientific fields, and their age depends on their size and activity. We
expect our approach to quantifying the evolution of ideas to be relevant for
making predictions about the future of science and thus help to guide its
development.
"
87,On the Communication of Scientific Results: The Full-Metadata Format,"  In this paper, we introduce a scientific format for text-based data files,
which facilitates storing and communicating tabular data sets. The so-called
Full-Metadata Format builds on the widely used INI-standard and is based on
four principles: readable self-documentation, flexible structure, fail-safe
compatibility, and searchability. As a consequence, all metadata required to
interpret the tabular data are stored in the same file, allowing for the
automated generation of publication-ready tables and graphs and the semantic
searchability of data file collections. The Full-Metadata Format is introduced
on the basis of three comprehensive examples. The complete format and syntax is
given in the appendix.
"
88,Citation entropy and research impact estimation,"  A new indicator, a real valued $s$-index, is suggested to characterize a
quality and impact of the scientific research output. It is expected to be at
least as useful as the notorious $h$-index, at the same time avoiding some its
obvious drawbacks. However, surprisingly, the $h$-index is found to be quite a
good indicator for majority of real-life citation data with their alleged
Zipfian behaviour for which these drawbacks do not show up. The style of the
paper was chosen deliberately somewhat frivolous to indicate that any attempt
to characterize the scientific output of a researcher by just one number always
has an element of a grotesque game in it and should not be taken too seriously.
I hope this frivolous style will be perceived as a funny decoration only.
"
89,A Recommender System to Support the Scholarly Communication Process,"  The number of researchers, articles, journals, conferences, funding
opportunities, and other such scholarly resources continues to grow every year
and at an increasing rate. Many services have emerged to support scholars in
navigating particular aspects of this resource-rich environment. Some
commercial publishers provide recommender and alert services for the articles
and journals in their digital libraries. Similarly, numerous noncommercial
social bookmarking services have emerged for citation sharing. While these
services do provide some support, they lack an understanding of the various
problem-solving scenarios that researchers face daily. Example scenarios, to
name a few, include when a scholar is in search of an article related to
another article of interest, when a scholar is in search of a potential
collaborator for a funding opportunity, when a scholar is in search of an
optimal venue to which to submit their article, and when a scholar, in the role
of an editor, is in search of referees to review an article. All of these
example scenarios can be represented as a problem in information filtering by
means of context-sensitive recommendation. This article presents an overview of
a context-sensitive recommender system to support the scholarly communication
process that is based on the standards and technology set forth by the Semantic
Web initiative.
"
90,Identifying Influential Bloggers: Time Does Matter,"  Blogs have recently become one of the most favored services on the Web. Many
users maintain a blog and write posts to express their opinion, experience and
knowledge about a product, an event and every subject of general or specific
interest. More users visit blogs to read these posts and comment them. This
""participatory journalism"" of blogs has such an impact upon the masses that
Keller and Berry argued that through blogging ""one American in tens tells the
other nine how to vote, where to eat and what to buy"" \cite{keller1}.
Therefore, a significant issue is how to identify such influential bloggers.
This problem is very new and the relevant literature lacks sophisticated
solutions, but most importantly these solutions have not taken into account
temporal aspects for identifying influential bloggers, even though the time is
the most critical aspect of the Blogosphere. This article investigates the
issue of identifying influential bloggers by proposing two easily computed
blogger ranking methods, which incorporate temporal aspects of the blogging
activity. Each method is based on a specific metric to score the blogger's
posts. The first metric, termed MEIBI, takes into consideration the number of
the blog post's inlinks and its comments, along with the publication date of
the post. The second metric, MEIBIX, is used to score a blog post according to
the number and age of the blog post's inlinks and its comments. These methods
are evaluated against the state-of-the-art influential blogger identification
method utilizing data collected from a real-world community blog site. The
obtained results attest that the new methods are able to better identify
significant temporal patterns in the blogging behaviour.
"
91,Information Diffusion in Computer Science Citation Networks,"  The paper citation network is a traditional social medium for the exchange of
ideas and knowledge. In this paper we view citation networks from the
perspective of information diffusion. We study the structural features of the
information paths through the citation networks of publications in computer
science, and analyze the impact of various citation choices on the subsequent
impact of the article. We find that citing recent papers and papers within the
same scholarly community garners a slightly larger number of citations on
average. However, this correlation is weaker among well-cited papers implying
that for high impact work citing within one's field is of lesser importance. We
also study differences in information flow for specific subsets of citation
networks: books versus conference and journal articles, different areas of
computer science, and different time periods.
"
92,The Game of Cipher Beads,"  Comparison between the various impact factors of a few Russian journals
demonstrates the deficiencies of the popular citation indices.
"
93,Interpretations of the Web of Data,"  The emerging Web of Data utilizes the web infrastructure to represent and
interrelate data. The foundational standards of the Web of Data include the
Uniform Resource Identifier (URI) and the Resource Description Framework (RDF).
URIs are used to identify resources and RDF is used to relate resources. While
RDF has been posited as a logic language designed specifically for knowledge
representation and reasoning, it is more generally useful if it can
conveniently support other models of computing. In order to realize the Web of
Data as a general-purpose medium for storing and processing the world's data,
it is necessary to separate RDF from its logic language legacy and frame it
simply as a data model. Moreover, there is significant advantage in seeing the
Semantic Web as a particular interpretation of the Web of Data that is focused
specifically on knowledge representation and reasoning. By doing so, other
interpretations of the Web of Data are exposed that realize RDF in different
capacities and in support of different computing models.
"
94,Indexing Research Papers in Open Access Databases,"  This paper synthesizes the actions performed in order to transform a classic
scientific research journal - 'Annals. Computer Science Series' - available
only in printed form until 2008, into a modern e-journal with free access to
the full text of the articles. For achieving this goal, the research papers
have been included in various article databases, portals and library catalogs
which offered a high visibility to the journal.
"
95,"Reengineering PDF-Based Documents Targeting Complex Software
  Specifications","  This article aims at reengineering of PDF-based complex documents, where
specifications of the Object Management Group (OMG) are our initial targets.
Our motivation is that such specifications are dense and intricate to use, and
tend to have complicated structures. Our objective is therefore to create an
approach that allows us to reengineer PDF-based documents, and to illustrate
how to make more usable versions of electronic documents (such as
specifications, technical books, etc) so that end users to have a better
experience with them. The first step was to extract the logical structure of
the document in a meaningful XML format for subsequent processing. Our initial
assumption was that, many key concepts of a document are expressed in this
structure. In the next phase, we created a multilayer hypertext version of the
document to facilitate browsing and navigating. Although we initially focused
on OMG software specifications, we chose a general approach for different
phases of our work including format conversions, logical structure extraction,
text extraction, multilayer hypertext generation, and concept exploration. As a
consequence, we can process other complex documents to achieve our goals.
"
96,"First results from the PARSE.Insight project: HEP survey on data
  preservation, re-use and (open) access","  There is growing interest in the issues of preservation and re-use of the
records of science, in the ""digital era"". The aim of the PARSE.Insight project,
partly financed by the European Commission under the Seventh Framework Program,
is twofold: to provide an assessment of the current activities, trends and
risks in the field of digital preservation of scientific results, from primary
data to published articles; to inform the design of the preservation layer of
an emerging e-Infrastructure for e-Science. CERN, as a partner of the
PARSE.Insight consortium, is performing an in-depth case study on data
preservation, re-use and (open) access within the High-Energy Physics (HEP)
community. The first results of this large-scale survey of the attitudes and
concerns of HEP scientists are presented. The survey reveals the widespread
opinion that data preservation is ""very important"" to ""crucial"". At the same
time, it also highlights the chronic lack of resources and infrastructure to
tackle this issue, as well as deeply-rooted concerns on the access to, and the
understanding of, preserved data in future analyses.
"
97,PDF/A standard for long term archiving,"  PDF/A is defined by ISO 19005-1 as a file format based on PDF format. The
standard provides a mechanism for representing electronic documents in a way
that preserves their visual appearance over time, independent of the tools and
systems used for creating or storing the files.
"
98,Report on the current state of the French DMLs,"  This is a survey of the existing digital collections of French mathematical
literature, run by non-profit organizations. This includes research monographs,
serials, proceedings, Ph. D. theses, collected works, books and personal
websites.
"
99,Adding eScience Assets to the Data Web,"  Aggregations of Web resources are increasingly important in scholarship as it
adopts new methods that are data-centric, collaborative, and networked-based.
The same notion of aggregations of resources is common to the mashed-up,
socially networked information environment of Web 2.0. We present a mechanism
to identify and describe aggregations of Web resources that has resulted from
the Open Archives Initiative - Object Reuse and Exchange (OAI-ORE) project. The
OAI-ORE specifications are based on the principles of the Architecture of the
World Wide Web, the Semantic Web, and the Linked Data effort. Therefore, their
incorporation into the cyberinfrastructure that supports eScholarship will
ensure the integration of the products of scholarly research into the Data Web.
"
100,A standard transformation from XML to RDF via XSLT,"  A generic transformation of XML data into the Resource Description Framework
(RDF) and its implementation by XSLT transformations is presented. It was
developed by the grid integration project for robotic telescopes of AstroGrid-D
to provide network communication through the Remote Telescope Markup Language
(RTML) to its RDF based information service. The transformation's generality is
explained by this example. It automates the transformation of XML data into RDF
and thus solves this problem of semantic computing. Its design also permits the
inverse transformation but this is not yet implemented.
"
101,"From Artifacts to Aggregations: Modeling Scientific Life Cycles on the
  Semantic Web","  In the process of scientific research, many information objects are
generated, all of which may remain valuable indefinitely. However, artifacts
such as instrument data and associated calibration information may have little
value in isolation; their meaning is derived from their relationships to each
other. Individual artifacts are best represented as components of a life cycle
that is specific to a scientific research domain or project. Current cataloging
practices do not describe objects at a sufficient level of granularity nor do
they offer the globally persistent identifiers necessary to discover and manage
scholarly products with World Wide Web standards. The Open Archives
Initiative's Object Reuse and Exchange data model (OAI-ORE) meets these
requirements. We demonstrate a conceptual implementation of OAI-ORE to
represent the scientific life cycles of embedded networked sensor applications
in seismology and environmental sciences. By establishing relationships between
publications, data, and contextual research information, we illustrate how to
obtain a richer and more realistic view of scientific practices. That view can
facilitate new forms of scientific research and learning. Our analysis is
framed by studies of scientific practices in a large, multi-disciplinary,
multi-university science and engineering research center, the Center for
Embedded Networked Sensing (CENS).
"
102,"On the prevalence and scientific impact of duplicate publications in
  different scientific fields (1980-2007)","  The issue of duplicate publications has received a lot of attention in the
medical literature, but much less in the information science community. This
paper aims at analyzing the prevalence and scientific impact of duplicate
publications across all fields of research between 1980 and 2007, using a
definition of duplicate papers based on their metadata. It shows that in all
fields combined, the prevalence of duplicates is one out of two-thousand
papers, but is higher in the natural and medical sciences than in the social
sciences and humanities. A very high proportion (>85%) of these papers are
published the same year or one year apart, which suggest that most duplicate
papers were submitted simultaneously. Furthermore, duplicate papers are
generally published in journals with impact factors below the average of their
field and obtain a lower number of citations. This paper provides clear
evidence that the prevalence of duplicate papers is low and, more importantly,
that the scientific impact of such papers is below average.
"
103,"A Quantum-based Model for Interactive Information Retrieval (extended
  version)","  Even the best information retrieval model cannot always identify the most
useful answers to a user query. This is in particular the case with web search
systems, where it is known that users tend to minimise their effort to access
relevant information. It is, however, believed that the interaction between
users and a retrieval system, such as a web search engine, can be exploited to
provide better answers to users. Interactive Information Retrieval (IR)
systems, in which users access information through a series of interactions
with the search system, are concerned with building models for IR, where
interaction plays a central role. There are many possible interactions between
a user and a search system, ranging from query (re)formulation to relevance
feedback. However, capturing them within a single framework is difficult and
previously proposed approaches have mostly focused on relevance feedback. In
this paper, we propose a general framework for interactive IR that is able to
capture the full interaction process in a principled way. Our approach relies
upon a generalisation of the probability framework of quantum physics, whose
strong geometric component can be a key towards a successful interactive IR
model.
"
104,"Citing and Reading Behaviours in High-Energy Physics. How a Community
  Stopped Worrying about Journals and Learned to Love Repositories","  Contemporary scholarly discourse follows many alternative routes in addition
to the three-century old tradition of publication in peer-reviewed journals.
The field of High- Energy Physics (HEP) has explored alternative communication
strategies for decades, initially via the mass mailing of paper copies of
preliminary manuscripts, then via the inception of the first online
repositories and digital libraries.
  This field is uniquely placed to answer recurrent questions raised by the
current trends in scholarly communication: is there an advantage for scientists
to make their work available through repositories, often in preliminary form?
Is there an advantage to publishing in Open Access journals? Do scientists
still read journals or do they use digital repositories?
  The analysis of citation data demonstrates that free and immediate online
dissemination of preprints creates an immense citation advantage in HEP,
whereas publication in Open Access journals presents no discernible advantage.
In addition, the analysis of clickstreams in the leading digital library of the
field shows that HEP scientists seldom read journals, preferring preprints
instead.
"
105,Diffusion of scientific credits and the ranking of scientists,"  Recently, the abundance of digital data enabled the implementation of graph
based ranking algorithms that provide system level analysis for ranking
publications and authors. Here we take advantage of the entire Physical Review
publication archive (1893-2006) to construct authors' networks where weighted
edges, as measured from opportunely normalized citation counts, define a proxy
for the mechanism of scientific credit transfer. On this network we define a
ranking method based on a diffusion algorithm that mimics the spreading of
scientific credits on the network. We compare the results obtained with our
algorithm with those obtained by local measures such as the citation count and
provide a statistical analysis of the assignment of major career awards in the
area of Physics. A web site where the algorithm is made available to perform
customized rank analysis can be found at the address
http://www.physauthorsrank.org
"
106,"Is scientific literature subject to a sell-by-date? A general
  methodology to analyze the durability of scientific documents","  The study of the citation histories and ageing of documents are topics that
have been addressed from several perspectives, especially in the analysis of
documents with delayed recognition or sleeping beauties. However, there is no
general methodology that can be extensively applied for different time periods
and/or research fields. In this paper a new methodology for the general
analysis of the ageing and durability of scientific papers is presented. This
methodology classifies documents into three general types: Delayed documents,
which receive the main part of their citations later than normal documents;
Flash in the pans, which receive citations immediately after their publication
but they are not cited in the long term; and Normal documents, documents with a
typical distribution of citations over time. These three types of durability
have been analyzed considering the whole population of documents in the Web of
Science with at least 5 external citations (i.e. not considering
self-citations). Several patterns related to the three types of durability have
been found and the potential for further research of the developed methodology
is discussed.
"
107,Data Preservation and Long Term Analysis in High Energy Physics,"  High energy physics data is a long term investment and contains the potential
for physics results beyond the lifetime of a collaboration. Many existing
experiments are concluding their physics programs, and looking at ways to
preserve their data heritage. Preservation of high-energy physics data and data
analysis structures is a challenge, and past experience has shown it can be
difficult if adequate planning and resources are not provided. A study group
has been formed to provide guidelines for such data preservation efforts in the
future. Key areas to be investigated were identified at a workshop at DESY in
January 2009, to be followed by a workshop at SLAC in May 2009. More
information can be found at http://dphep.org
"
108,"Evaluating Methods to Rediscover Missing Web Pages from the Web
  Infrastructure","  Missing web pages (pages that return the 404 ""Page Not Found"" error) are part
of the browsing experience. The manual use of search engines to rediscover
missing pages can be frustrating and unsuccessful. We compare four automated
methods for rediscovering web pages. We extract the page's title, generate the
page's lexical signature (LS), obtain the page's tags from the bookmarking
website delicious.com and generate a LS from the page's link neighborhood. We
use the output of all methods to query Internet search engines and analyze
their retrieval performance. Our results show that both LSs and titles perform
fairly well with over 60% URIs returned top ranked from Yahoo!. However, the
combination of methods improves the retrieval performance. Considering the
complexity of the LS generation, querying the title first and in case of
insufficient results querying the LSs second is the preferable setup. This
combination accounts for more than 75% top ranked URIs.
"
109,"Managing Information for Sparsely Distributed Articles and Readers: The
  Virtual Journals of the Joint Institute for Nuclear Astrophysics (JINA)","  The research area of nuclear astrophysics is characterized by a need for
information published in tens of journals in several fields and an extremely
dilute distribution of researchers. For these reasons it is difficult for
researchers, especially students, to be adequately informed of the relevant
published research. For example, the commonly employed journal club is
inefficient for a group consisting of a professor and his two students. In an
attempt to address this problem, we have developed a virtual journal (VJ), a
process for collecting and distributing a weekly compendium of articles of
interest to researchers in nuclear astrophysics. Subscribers are notified of
each VJ issue using an email-list server or an RSS feed. The VJ data base is
searchable by topics assigned by the editors, or by keywords. There are two
related VJs: the Virtual Journal of Nuclear Astrophysics (JINA VJ), and the
SEGUE Virtual Journal (SEGUE VJ). The JINA VJ also serves as a source of new
experimental and theoretical information for the JINA REACLIB reaction rate
database. References to review articles and popular level articles provide an
introduction to the literature for students. The VJs and support information
are available at http://groups.nscl.msu.edu/jina/journals
"
110,"COMMENTARY ON: Citing and Reading Behavours in High-Energy Physics
  (arXiv:0906.5418)","  Evidence confirming that OA increases impact will not be sufficient to induce
enough researchers to provide OA; only mandates from their institutions and
funders can ensure that. HEP researchers continue to submit their papers to
peer-reviewed journals, as they always did, depositing both their unrefereed
preprints and their refereed postprints. None of that has changed. In fields
like HEP and astrophysics, the journal affordability/accessibility problem is
not as great as in many other fields, where it the HEP Early Access impact
advantage translates into the OA impact advantage itself. Almost no one has
ever argued that Gold OA provides a greater OA advantage than Green OA. The OA
advantage is the OA advantage, whether Green or Gold.
"
111,Investigating the Change of Web Pages' Titles Over Time,"  Inaccessible web pages are part of the browsing experience. The content of
these pages however is often not completely lost but rather missing. Lexical
signatures (LS) generated from the web pages' textual content have been shown
to be suitable as search engine queries when trying to discover a (missing) web
page. Since LSs are expensive to generate, we investigate the potential of web
pages' titles as they are available at a lower cost. We present the results
from studying the change of titles over time. We take titles from copies
provided by the Internet Archive of randomly sampled web pages and show the
frequency of change as well as the degree of change in terms of the Levenshtein
score. We found very low frequencies of change and high Levenshtein scores
indicating that titles, on average, change little from their original, first
observed values (rooted comparison) and even less from the values of their
previous observation (sliding).
"
112,Experimental DML over digital repositories in Japan,"  In this paper the authors show an overview of Virtual Digital Mathematics
Library in Japan (DML-JP), contents of which consist of metadata harvested from
institutional repositories in Japan and digital repositories in the world.
DML-JP is, in a sense, a subject specific repository which collaborate with
various digital repositories. Beyond portal website, DML-JP provides
subject-specific metadata through OAI-ORE. By the schema it is enabled that
digital repositories can load the rich metadata which were added by
mathematicians.
"
113,Positional Effects on Citation and Readership in arXiv,"  arXiv.org mediates contact with the literature for entire scholarly
communities, both through provision of archival access and through daily email
and web announcements of new materials, potentially many screenlengths long. We
confirm and extend a surprising correlation between article position in these
initial announcements, ordered by submission time, and later citation impact,
due primarily to intentional ""self-promotion"" on the part of authors. A pure
""visibility"" effect was also present: the subset of articles accidentally in
early positions fared measurably better in the long-term citation record than
those lower down. Astrophysics articles announced in position 1, for example,
overall received a median number of citations 83\% higher, while those there
accidentally had a 44\% visibility boost. For two large subcommunities of
theoretical high energy physics, hep-th and hep-ph articles announced in
position 1 had median numbers of citations 50\% and 100\% larger than for
positions 5--15, and the subsets there accidentally had visibility boosts of
38\% and 71\%.
  We also consider the positional effects on early readership. The median
numbers of early full text downloads for astro-ph, hep-th, and hep-ph articles
announced in position 1 were 82\%, 61\%, and 58\% higher than for lower
positions, respectively, and those there accidentally had medians
visibility-boosted by 53\%, 44\%, and 46\%. Finally, we correlate a variety of
readership features with long-term citations, using machine learning methods,
thereby extending previous results on the predictive power of early readership
in a broader context. We conclude with some observations on impact metrics and
dangers of recommender mechanisms.
"
114,"A population-modulated bibliometric measure with an application in the
  field of statistics","  We use confirmatory factor analysis to derive a unifying measure of
comparison of scientists based on bibliometric measurements, by utilizing the
h-index, some similar h-type indices as well as other common measures of
scientific performance. We use a real data example from nine well-known
departments of statistics to demonstrate our approach and argue that our
combined measure results in a better overall evaluation of a researchers'
scientific work.
"
115,A Reflection on the Structure and Process of the Web of Data,"  The Web community has introduced a set of standards and technologies for
representing, querying, and manipulating a globally distributed data structure
known as the Web of Data. The proponents of the Web of Data envision much of
the world's data being interrelated and openly accessible to the general
public. This vision is analogous in many ways to the Web of Documents of common
knowledge, but instead of making documents and media openly accessible, the
focus is on making data openly accessible. In providing data for public use,
there has been a stimulated interest in a movement dubbed Open Data. Open Data
is analogous in many ways to the Open Source movement. However, instead of
focusing on software, Open Data is focused on the legal and licensing issues
around publicly exposed data. Together, various technological and legal tools
are laying the groundwork for the future of global-scale data management on the
Web. As of today, in its early form, the Web of Data hosts a variety of data
sets that include encyclopedic facts, drug and protein data, metadata on music,
books and scholarly articles, social network representations, geospatial
information, and many other types of information. The size and diversity of the
Web of Data is a demonstration of the flexibility of the underlying standards
and the overall feasibility of the project as a whole. The purpose of this
article is to provide a review of the technological underpinnings of the Web of
Data as well as some of the hurdles that need to be overcome if the Web of Data
is to emerge as the defacto medium for data representation, distribution, and
ultimately, processing.
"
116,On the relationship between interdisciplinarity and scientific impact,"  This paper analyzes the effect of interdisciplinarity on the scientific
impact of individual papers. Using all the papers published in Web of Science
in 2000, we define the degree of interdisciplinarity of a given paper as the
percentage of its cited references made to journals of other disciplines. We
show that, although for all disciplines combined there is no clear correlation
between the level of interdisciplinarity of papers and their citation rates,
there are nonetheless some disciplines in which a higher level of
interdisciplinarity is related to a higher citation rates. For other
disciplines, citations decline as interdisciplinarity grows. One characteristic
is visible in all disciplines: highly disciplinary and highly interdisciplinary
papers have a low scientific impact. This suggests that there might be an
optimum of interdisciplinarity beyond which the research is too dispersed to
find its niche and under which it is too mainstream to have high impact.
Finally, the relationship between interdisciplinarity and scientific impact is
highly determined by the citation characteristics of the disciplines involved:
papers citing citation intensive disciplines are more likely to be cited by
those disciplines and, hence, obtain higher citation scores than papers citing
non citation intensive disciplines.
"
117,"Modeling scientific-citation patterns and other triangle-rich acyclic
  networks","  We propose a model of the evolution of the networks of scientific citations.
The model takes an out-degree distribution (distribution of number of
citations) and two parameters as input. The parameters capture the two main
ingredients of the model, the aging of the relevance of papers and the
formation of triangles when new papers cite old. We compare our model with
three network structural quantities of an empirical citation network. We find
that an unique point in parameter space optimizing the match between the real
and model data for all quantities. The optimal parameter values suggest that
the impact of scientific papers, at least in the empirical data set we model is
proportional to the inverse of the number of papers since they were published.
"
118,"The impact factor's Matthew effect: a natural experiment in
  bibliometrics","  Since the publication of Robert K. Merton's theory of cumulative advantage in
science (Matthew Effect), several empirical studies have tried to measure its
presence at the level of papers, individual researchers, institutions or
countries. However, these studies seldom control for the intrinsic ""quality"" of
papers or of researchers--""better"" (however defined) papers or researchers
could receive higher citation rates because they are indeed of better quality.
Using an original method for controlling the intrinsic value of
papers--identical duplicate papers published in different journals with
different impact factors--this paper shows that the journal in which papers are
published have a strong influence on their citation rates, as duplicate papers
published in high impact journals obtain, on average, twice as much citations
as their identical counterparts published in journals with lower impact
factors. The intrinsic value of a paper is thus not the only reason a given
paper gets cited or not; there is a specific Matthew effect attached to
journals and this gives to paper published there an added value over and above
their intrinsic quality.
"
119,Astroinformatics: A 21st Century Approach to Astronomy,"  Data volumes from multiple sky surveys have grown from gigabytes into
terabytes during the past decade, and will grow from terabytes into tens (or
hundreds) of petabytes in the next decade. This exponential growth of new data
both enables and challenges effective astronomical research, requiring new
approaches. Thus far, astronomy has tended to address these challenges in an
informal and ad hoc manner, with the necessary special expertise being assigned
to e-Science or survey science. However, we see an even wider scope and
therefore promote a broader vision of this data-driven revolution in
astronomical research. For astronomy to effectively cope with and reap the
maximum scientific return from existing and future large sky surveys,
facilities, and data-producing projects, we need our own information science
specialists. We therefore recommend the formal creation, recognition, and
support of a major new discipline, which we call Astroinformatics.
Astroinformatics includes a set of naturally-related specialties including data
organization, data description, astronomical classification taxonomies,
astronomical concept ontologies, data mining, machine learning, visualization,
and astrostatistics. By virtue of its new stature, we propose that astronomy
now needs to integrate Astroinformatics as a formal sub-discipline within
agency funding plans, university departments, research programs, graduate
training, and undergraduate education. Now is the time for the recognition of
Astroinformatics as an essential methodology of astronomical research. The
future of astronomy depends on it.
"
120,The Revolution in Astronomy Education: Data Science for the Masses,"  As our capacity to study ever-expanding domains of our science has increased
(including the time domain, non-electromagnetic phenomena, magnetized plasmas,
and numerous sky surveys in multiple wavebands with broad spatial coverage and
unprecedented depths), so have the horizons of our understanding of the
Universe been similarly expanding. This expansion is coupled to the exponential
data deluge from multiple sky surveys, which have grown from gigabytes into
terabytes during the past decade, and will grow from terabytes into Petabytes
(even hundreds of Petabytes) in the next decade. With this increased vastness
of information, there is a growing gap between our awareness of that
information and our understanding of it. Training the next generation in the
fine art of deriving intelligent understanding from data is needed for the
success of sciences, communities, projects, agencies, businesses, and
economies. This is true for both specialists (scientists) and non-specialists
(everyone else: the public, educators and students, workforce). Specialists
must learn and apply new data science research techniques in order to advance
our understanding of the Universe. Non-specialists require information literacy
skills as productive members of the 21st century workforce, integrating
foundational skills for lifelong learning in a world increasingly dominated by
data. We address the impact of the emerging discipline of data science on
astronomy education within two contexts: formal education and lifelong
learners.
"
121,"How are new citation-based journal indicators adding to the bibliometric
  toolbox?","  The launching of Scopus and Google Scholar, and methodological developments
in Social Network Analysis have made many more indicators for evaluating
journals available than the traditional Impact Factor, Cited Half-life, and
Immediacy Index of the ISI. In this study, these new indicators are compared
with one another and with the older ones. Do the various indicators measure new
dimensions of the citation networks, or are they highly correlated among them?
Are they robust and relatively stable over time? Two main dimensions are
distinguished -- size and impact -- which together shape influence. The H-index
combines the two dimensions and can also be considered as an indicator of reach
(like Indegree). PageRank is mainly an indicator of size, but has important
interactions with centrality measures. The Scimago Journal Ranking (SJR)
indicator provides an alternative to the Journal Impact Factor, but the
computation is less easy.
"
122,"Worldwide Use and Impact of the NASA Astrophysics Data System Digital
  Library","  By combining data from the text, citation, and reference databases with data
from the ADS readership logs we have been able to create Second Order
Bibliometric Operators, a customizable class of collaborative filters which
permits substantially improved accuracy in literature queries.
  Using the ADS usage logs along with membership statistics from the
International Astronomical Union and data on the population and gross domestic
product (GDP) we develop an accurate model for world-wide basic research where
the number of scientists in a country is proportional to the GDP of that
country, and the amount of basic research done by a country is proportional to
the number of scientists in that country times that country's per capita GDP.
  We introduce the concept of utility time to measure the impact of the
ADS/URANIA and the electronic astronomical library on astronomical research. We
find that in 2002 it amounted to the equivalent of 736 FTE researchers, or $250
Million, or the astronomical research done in France.
  Subject headings: digital libraries; bibliometrics; sociology of science;
information retrieval
"
123,The Bibliometric Properties of Article Readership Information,"  The NASA Astrophysics Data System (ADS), along with astronomy's journals and
data centers (a collaboration dubbed URANIA), has developed a distributed
on-line digital library which has become the dominant means by which
astronomers search, access and read their technical literature. Digital
libraries such as the NASA Astrophysics Data System permit the easy
accumulation of a new type of bibliometric measure, the number of electronic
accesses (``reads'') of individual articles. We explore various aspects of this
new measure. We examine the obsolescence function as measured by actual reads,
and show that it can be well fit by the sum of four exponentials with very
different time constants. We compare the obsolescence function as measured by
readership with the obsolescence function as measured by citations. We find
that the citation function is proportional to the sum of two of the components
of the readership function. This proves that the normative theory of citation
is true in the mean. We further examine in detail the similarities and
differences between the citation rate, the readership rate and the total
citations for individual articles, and discuss some of the causes. Using the
number of reads as a bibliometric measure for individuals, we introduce the
read-cite diagram to provide a two-dimensional view of an individual's
scientific productivity. We develop a simple model to account for an
individual's reads and cites and use it to show that the position of a person
in the read-cite diagram is a function of age, innate productivity, and work
history. We show the age biases of both reads and cites, and develop two new
bibliometric measures which have substantially less age bias than citations
"
124,On Metric Skyline Processing by PM-tree,"  The task of similarity search in multimedia databases is usually accomplished
by range or k nearest neighbor queries. However, the expressing power of these
""single-example"" queries fails when the user's delicate query intent is not
available as a single example. Recently, the well-known skyline operator was
reused in metric similarity search as a ""multi-example"" query type. When
applied on a multi-dimensional database (i.e., on a multi-attribute table), the
traditional skyline operator selects all database objects that are not
dominated by other objects. The metric skyline query adopts the skyline
operator such that the multiple attributes are represented by distances
(similarities) to multiple query examples. Hence, we can view the metric
skyline as a set of representative database objects which are as similar to all
the examples as possible and, simultaneously, are semantically distinct. In
this paper we propose a technique of processing the metric skyline query by use
of PM-tree, while we show that our technique significantly outperforms the
original M-tree based implementation in both time and space costs. In
experiments we also evaluate the partial metric skyline processing, where only
a controlled number of skyline objects is retrieved.
"
125,"On challenges and opportunities of designing integrated IT platforms for
  supporting knowledge works in organizations","  Designing and implementing comprehensive IT-based support environments for KM
in organizations is fraught with many problems. Solving them requires intimate
knowledge about the information usage in knowledge works and the scopes of
technology intervention. In this paper, the Task-oriented Organizational
Knowledge Management or TOKM, a design theory for building integrated IT
platforms for supporting organizational KM, is proposed. TOKM brings together
two apparently mutually exclusive practices of building KM systems, the
task-based approach and the generic or universalistic approach. In developing
the design, the information requirements of knowledge workers in light of an
information usage model of knowledge works is studied. Then the model is
extended to study possibilities of more advanced IT support and formulate them
in form of a set of meta-requirements. Following the IS design theory paradigm,
a set of artifacts are hypothesized to meet the requirements. Finally, a design
method, as a possible approach of building an IT-based integrated platform, the
Knowledge Work Support Platform (KWSP) to realize the artifacts in order to
meet the requirements, is outlined. The KWSP is a powerful platform for
building and maintaining a number of task-type specific Knowledge Work Support
Systems (KWSS) on a common sharable platform. Each KWSS, for the task-type
supported by it, can be easily designed to provide extensive and sophisticated
support to individual as well as group of knowledge workers in performing their
respective knowledge work instances
"
126,Communication scientifique : Pour le meilleur et pour le PEER,"  This paper provides an overview (in French) of the European PEER project,
focusing on its origins, the actual objectives and the technical deployment.
"
127,On building Information Warehouses,"  One of the most important goals of information management (IM) is supporting
the knowledge workers in performing their works. In this paper we examine
issues of relevance, linkage and provenance of information, as accessed and
used by the knowledge workers. These are usually not adequately addressed in
most of the IT based solutions for IM. Here we propose a non-conventional
approach for building information systems for supporting the knowledge workers
which addresses these issues. The approach leads to the ideas of building
Information Warehouses (IW) and Knowledge work Support Systems (KwSS). Such
systems can open up potential for building innovative applications of
significant impact, including those capable of helping organizations in
implementing processes for double-loop learning.
"
128,Towards a Semantic Preservation System,"  Preserving access to file content requires preserving not just bits but also
meaningful logical structures. The ongoing development of the Data Format
Description Language (DFDL) is a completely general standard that addresses
this need. The Defuddle parser is a generic parser that can use DFDL-style
format descriptions to extract logical structures from ASCII or binary files
written in those formats. DFDL and Defuddle provide a preservation capability
that has minimal format-specific software and cleanly separates issues related
to bits, formats, and logical content. Such a system has the potential to
greatly reduce overall system development and maintenance costs as well as the
per-file-format costs for long term preservation.
"
129,Adaptive model for recommendation of news,"  Most news recommender systems try to identify users' interests and news'
attributes and use them to obtain recommendations. Here we propose an adaptive
model which combines similarities in users' rating patterns with epidemic-like
spreading of news on an evolving network. We study the model by computer
agent-based simulations, measure its performance and discuss its robustness
against bias and malicious behavior. Subject to the approval fraction of news
recommended, the proposed model outperforms the widely adopted recommendation
of news according to their absolute or relative popularity. This model provides
a general social mechanism for recommender systems and may find its
applications also in other types of recommendation.
"
130,Citation Statistics,"  This is a report about the use and misuse of citation data in the assessment
of scientific research. The idea that research assessment must be done using
``simple and objective'' methods is increasingly prevalent today. The ``simple
and objective'' methods are broadly interpreted as bibliometrics, that is,
citation data and the statistics derived from them. There is a belief that
citation statistics are inherently more accurate because they substitute simple
numbers for complex judgments, and hence overcome the possible subjectivity of
peer review. But this belief is unfounded.
"
131,"Comment: Bibliometrics in the Context of the UK Research Assessment
  Exercise","  Research funding and reputation in the UK have, for over two decades, been
increasingly dependent on a regular peer-review of all UK departments. This is
to move to a system more based on bibliometrics. Assessment exercises of this
kind influence the behavior of institutions, departments and individuals, and
therefore bibliometrics will have effects beyond simple measurement.
[arXiv:0910.3529]
"
132,Comment: Citation Statistics,"  We discuss the paper ""Citation Statistics"" by the Joint Committee on
Quantitative Assessment of Research [arXiv:0910.3529]. In particular, we focus
on a necessary feature of ""good"" measures for ranking scientific authors: that
good measures must able to accurately distinguish between authors.
"
133,Comment: Citation Statistics,"  Comment on ""Citation Statistics"" [arXiv:0910.3529]
"
134,Comment: Citation Statistics,"  Comment on ""Citation Statistics"" [arXiv:0910.3529]
"
135,Rejoinder: Citation Statistics,"  Rejoinder to ""Citation Statistics"" [arXiv:0910.3529]
"
136,A theoretical foundation for building Knowledge-work Support Systems,"  In this paper we propose a novel approach aimed at building a new class of
information system platforms which we call the ""Knowledge-work Support Systems""
or KwSS. KwSS can play a significant role in enhancing the IS support for
knowledge management processes, including those customarily identified as less
amenable to IS support. In our approach we try to enhance basic functionalities
provided by the computer-based information systems, namely, that of improving
the efficiency of the knowledge workers in accessing, processing and creating
useful information. The improvement, along with proper focus on cultural,
social and other aspects of the knowledge management processes, can enhance the
workers' efficiency significantly in performing high quality knowledge works.
In order to build the proposed approach, we develop several new concepts. The
approach analyzes the information availability and usage from the knowledge
workers and their works' perspectives and consequently brings forth more
transparency in various aspects of information life-cycle with respect to
knowledge management. KsSSes are technology platforms, which can be implemented
independently as well as in conjunction with other knowledge management and
data management technology platforms, to provide significant boost in the
knowledge capabilities of organizations.
"
137,Accelerating the pace of discovery by changing the peer review algorithm,"  The number of scientific publications is constantly rising, increasing the
strain on the review process. The number of submissions is actually higher, as
each manuscript is often reviewed several times before publication. To face the
deluge of submissions, top journals reject a considerable fraction of
manuscripts without review, potentially declining manuscripts with merit. The
situation is frustrating for authors, reviewers and editors alike. Recently,
several editors wrote about the ``tragedy of the reviewer commons', advocating
for urgent corrections to the system. Almost every scientist has ideas on how
to improve the system, but it is very difficult, if not impossible, to perform
experiments to test which measures would be most effective. Surprisingly,
relatively few attempts have been made to model peer review. Here I implement a
simulation framework in which ideas on peer review can be quantitatively
tested. I incorporate authors, reviewers, manuscripts and journals into an
agent-based model and a peer review system emerges from their interactions. As
a proof-of-concept, I contrast an implementation of the current system, in
which authors decide the journal for their submissions, with a system in which
journals bid on manuscripts for publication. I show that, all other things
being equal, this latter system solves most of the problems currently
associated with the peer review process. Manuscripts' evaluation is faster,
authors publish more and in better journals, and reviewers' effort is optimally
utilized. However, more work is required from editors. This modeling framework
can be used to test other solutions for peer review, leading the way for an
improvement of how science is disseminated.
"
138,"Building a Vietnamese Language Query Processing Framework for ELibrary
  Searching Systems","  In the objective of building intelligent searching systems for Elibraries or
online bookstores, we have proposed a searching system model based on a
Vietnamese language query processing component. Such document searching systems
based on this model can allow users to use Vietnamese queries that represent
content information as input, instead of entering keywords for searching in
specific fields in database. To simplify the realization process of system
based on this searching system model, we set a target of building a framework
to support the rapid development of Vietnamese language query processing
components. Such framework let the implementation of Vietnamese language query
processing component in similar systems in this domain to be done more easily.
"
139,Memento: Time Travel for the Web,"  The Web is ephemeral. Many resources have representations that change over
time, and many of those representations are lost forever. A lucky few manage to
reappear as archived resources that carry their own URIs. For example, some
content management systems maintain version pages that reflect a frozen prior
state of their changing resources. Archives recurrently crawl the web to obtain
the actual representation of resources, and subsequently make those available
via special-purpose archived resources. In both cases, the archival copies have
URIs that are protocol-wise disconnected from the URI of the resource of which
they represent a prior state. Indeed, the lack of temporal capabilities in the
most common Web protocol, HTTP, prevents getting to an archived resource on the
basis of the URI of its original. This turns accessing archived resources into
a significant discovery challenge for both human and software agents, which
typically involves following a multitude of links from the original to the
archival resource, or of searching archives for the original URI. This paper
proposes the protocol-based Memento solution to address this problem, and
describes a proof-of-concept experiment that includes major servers of archival
content, including Wikipedia and the Internet Archive. The Memento solution is
based on existing HTTP capabilities applied in a novel way to add the temporal
dimension. The result is a framework in which archived resources can seamlessly
be reached via the URI of their original: protocol-based time travel for the
Web.
"
140,"Retrieval of very large numbers of items in the Web of Science: an
  exercise to develop accurate search strategies","  The current communication presents a simple exercise with the aim of solving
a singular problem: the retrieval of extremely large amounts of items in the
Web of Science interface. As it is known, Web of Science interface allows a
user to obtain at most 100,000 items from a single query. But what about
queries that achieve a result of more than 100,000 items? The exercise
developed one possible way to achieve this objective. The case study is the
retrieval of the entire scientific production from the United States in a
specific year. Different sections of items were retrieved using the field
Source of the database. Then, a simple Boolean statement was created with the
aim of eliminating overlapping and to improve the accuracy of the search
strategy. The importance of team work in the development of advanced search
strategies was noted.
"
141,"The Dynamics of Exchanges and References among Scientific Texts, and the
  Autopoiesis of Discursive Knowledge","  Discursive knowledge emerges as codification in flows of communication. The
flows of communication are constrained and enabled by networks of
communications as their historical manifestations at each moment of time. New
publications modify the existing networks by changing the distributions of
attributes and relations in document sets, while the networks are
self-referentially updated along trajectories. Codification operates
reflexively: the network structures are reconstructed from the perspective of
hindsight. Codification along different axes differentiates discursive
knowledge into specialties. These intellectual control structures are
constructed bottom-up, but feed top-down back upon the production of new
knowledge. However, the forward dynamics of diffusion in the development of the
communication networks along trajectories differs from the feedback mechanisms
of control. Analysis of the development of scientific communication in terms of
evolving scientific literatures provides us with a model which makes these
evolutionary processes amenable to measurement.
"
142,"Knowledge linkage structures in communication studies using citation
  analysis among communication journals","  This research analyzes a ""who cites whom"" matrix in terms of aggregated,
journal-journal citations to determine the location of communication studies on
the academic spectrum. Using the Journal of Communication as the seed journal,
the 2006 data in the Journal Citation Reports are used to map communication
studies. The results show that social and experimental psychology journals are
the most frequently used sources of information in this field. In addition,
several journals devoted to the use and effects of media and advertising are
weakly integrated into the larger communication research community, whereas
communication studies are dominated by American journals.
"
143,"Caveats for the Use of Citation Indicators in Research and Journal
  Evaluations","  Ageing of publications, percentage of self-citations, and impact vary from
journal to journal within fields of science. The assumption that citation and
publication practices are homogenous within specialties and fields of science
is invalid. Furthermore, the delineation of fields and among specialties is
fuzzy. Institutional units of analysis and persons may move between fields or
span different specialties. The match between the citation index and
institutional profiles varies among institutional units and nations. The
respective matches may heavily affect the representation of the units. Non-ISI
journals are increasingly cornered into ""transdisciplinary"" Mode-2 functions
with the exception of specialist journals publishing in languages other than
English. An ""externally cited impact factor"" can be calculated for these
journals. The citation impact of non-ISI journals will be demonstrated using
Science and Public Policy as the example.
"
144,"The delineation of nanoscience and nanotechnology in terms of journals
  and patents: a most recent update","  The journal set which provides a representation of nanoscience and
nanotechnology at the interfaces among applied physics, chemistry, and the life
sciences is developing rapidly because of the introduction of new journals. The
relevant contributions of nations can be expected to change according to the
representations of the relevant interfaces among journal sets. In the 2005 set
the position of the USA decreased more than in the 2004-set, while the EU-27
gained in terms of its percentage of world share of citations. The tag ""Y01N""
which was newly added to the EU classification system for patents, allows for
the visualization of national profiles of nanotechnology in terms of relevant
patents and patent classes.
"
145,"On the Normalization and Visualization of Author Co-Citation Data
  Salton's Cosine versus the Jaccard Index","  The debate about which similarity measure one should use for the
normalization in the case of Author Co-citation Analysis (ACA) is further
complicated when one distinguishes between the symmetrical co-citation--or,
more generally, co-occurrence--matrix and the underlying asymmetrical
citation--occurrence--matrix. In the Web environment, the approach of
retrieving original citation data is often not feasible. In that case, one
should use the Jaccard index, but preferentially after adding the number of
total citations (occurrences) on the main diagonal. Unlike Salton's cosine and
the Pearson correlation, the Jaccard index abstracts from the shape of the
distributions and focuses only on the intersection and the sum of the two sets.
Since the correlations in the co-occurrence matrix may partially be spurious,
this property of the Jaccard index can be considered as an advantage in this
case.
"
146,Co-word Analysis using the Chinese Character Set,"  Until recently, Chinese texts could not be studied using co-word analysis
because the words are not separated by spaces in Chinese (and Japanese). A word
can be composed of one or more characters. The online availability of programs
that separate Chinese texts makes it possible to analyze them using semantic
maps. Chinese characters contain not only information, but also meaning. This
may enhance the readability of semantic maps. In this study, we analyze 58
words which occur ten or more times in the 1652 journal titles of the China
Scientific and Technical Papers and Citations Database. The word occurrence
matrix is visualized and factor-analyzed.
"
147,"Main-path analysis and path-dependent transitions in HistCite(TM)-based
  historiograms","  With the program HistCite(TM) it is possible to generate and visualize the
most relevant papers in a set of documents retrieved from the Science Citation
Index. Historical reconstructions of scientific developments can be represented
chronologically as developments in networks of citation relations extracted
from scientific literature. This study aims to go beyond the historical
reconstruction of scientific knowledge, enriching the output of HistCite(TM)
with algorithms from social network analysis and information theory.
"
148,"Korean journals in the Science Citation Index: What do they reveal about
  the intellectual structure of S&T in Korea?","  During the last decade, we have witnessed a sustained growth of South Korea's
research output in terms of the world share of publications in the Science
Citation Index database. However, Korea's citation performance is not yet as
competitive as publication performance. In this study, the authors examine the
intellectual structure of Korean S&T field based on social network analysis of
journal-journal citation data using the ten Korean SCI journals as seed
journals. The results reveal that Korean SCI journals function more like
publication places, neither research channels nor information sources among
national scientists. Thus, these journals may provide Korean scholars with
access to international scientific communities by facilitating the respective
entry barriers. However, there are no citation relations based on their Korean
background. Furthermore, we intend to draw some policy implications which may
be helpful to increase Korea's research potential.
"
149,"Big Macs and Eigenfactor Scores: Don't Let Correlation Coefficients Fool
  You","  The Eigenfactor Metrics provide an alternative way of evaluating scholarly
journals based on an iterative ranking procedure analogous to Google's PageRank
algorithm. These metrics have recently been adopted by Thomson-Reuters and are
listed alongside the Impact Factor in the Journal Citation Reports. But do
these metrics differ sufficiently so as to be a useful addition to the
bibliometric toolbox? Davis (2008) has argued otherwise, based on his finding
of a 0.95 correlation coefficient between Eigenfactor score and Total Citations
for a sample of journals in the field of medicine. This conclusion is mistaken;
here we illustrate the basic statistical fallacy to which Davis succumbed. We
provide a complete analysis of the 2006 Journal Citation Reports and
demonstrate that there are statistically and economically significant
differences between the information provided by the Eigenfactor Metrics and
that provided by Impact Factor and Total Citations.
"
150,Journals as constituents of scientific discourse: economic heterodoxy,"  Purpose: to provide a view and analysis of the immediate field of journals
which surround a number of key heterodox economics journals.
Design/methodology/approach: Using citation data from the Science and Social
Science Citation Index, the individual and collective networks of a number of
journals in this field are analyzed. Findings: The size and shape of the
citation networks of journals can differ substantially, even if in a broadly
similar category. Heterodox economics cannot (yet) be considered as an
integrated specialty: authors in several journals in heterodox economics cite
more from mainstream economics than from other heterodox journals. There are
also strong links with other disciplinary fields such as geography, development
studies, women studies, etc.
"
151,Measuring contextual citation impact of scientific journals,"  This paper explores a new indicator of journal citation impact, denoted as
source normalized impact per paper (SNIP). It measures a journal's contextual
citation impact, taking into account characteristics of its properly defined
subject field, especially the frequency at which authors cite other papers in
their reference lists, the rapidity of maturing of citation impact, and the
extent to which a database used for the assessment covers the field's
literature. It further develops Eugene Garfield's notions of a field's
'citation potential' defined as the average length of references lists in a
field and determining the probability of being cited, and the need in fair
performance assessments to correct for differences between subject fields. A
journal's subject field is defined as the set of papers citing that journal.
SNIP is defined as the ratio of the journal's citation count per paper and the
citation potential in its subject field. It aims to allow direct comparison of
sources in different subject fields. Citation potential is shown to vary not
only between journal subject categories - groupings of journals sharing a
research field - or disciplines (e.g., journals in mathematics, engineering and
social sciences tend to have lower values than titles in life sciences), but
also between journals within the same subject category. For instance, basic
journals tend to show higher citation potentials than applied or clinical
journals, and journals covering emerging topics higher than periodicals in
classical subjects or more general journals. SNIP corrects for such
differences. Its strengths and limitations are critically discussed, and
suggestions are made for further research. All empirical results are derived
from Elsevier's Scopus.
"
152,"Visualization of the Citation Impact Environments of Scientific
  Journals: An online mapping exercise","  Aggregated journal-journal citation networks based on the Journal Citation
Reports 2004 of the Science Citation Index (5968 journals) and the Social
Science Citation Index (1712 journals) are made accessible from the perspective
of any of these journals. The user is thus able to analyze the citation
environment in terms of links and graphs. Furthermore, the local impact of a
journal is defined as its share of the total citations in the specific
journal's citation environments; the vertical size of the nodes is varied
proportionally to this citation impact. The horizontal size of each node can be
used to provide the same information after correction for within-journal
(self)-citations. In the ""citing"" environment, the equivalents of this measure
can be considered as a citation activity index which maps how the relevant
journal environment is perceived by the collective of authors of a given
journal. As a policy application, the mechanism of interdisciplinary
developments among the sciences is elaborated for the case of nanotechnology
journals.
"
153,Citation Environment of Angewandte Chemie,"  Recently, aggregated journal-journal citation networks were made accessible
from the perspective of each journal included in the Science Citation Index see
(http://www.leydesdorff.net/). The local matrices can be used to inspect the
relevant citation environment of a journal using statistical analysis and
visualization techniques from social network analysis. The inspection gives an
answer to the question what the local impact of this and other journals in the
environment is. In this study the citation environment of Angewandte Chemie was
analysed. Angewandte Chemie is one of the prime chemistry journals in the
world. Its environment was compared with that of the Journal of the American
Chemical Society. The results of the environment analyses give a detailed
insight into the field-embeddedness of Angewandte Chemie. The impacts of the
German and international editions of this journal are compared.
"
154,"A Comparison between the China Scientific and Technical Papers and
  Citations Database and the Science Citation Index in terms of journal
  hierarchies and inter-journal citation relations","  The journal structure in the China Scientific and Technical Papers and
Citations Database (CSTPCD) is analysed from three perspectives: the database
level, the specialty level and the institutional level (i.e., university
journals versus journals issued by the Chinese Academy of Sciences). The
results are compared with those for (Chinese) journals included in the Science
Citation Index. The frequency of journal-journal citation relations in the
CSTPCD is an order of magnitude lower than in the SCI. Chinese journals,
especially high-quality journals, prefer to cite international journals rather
than domestic ones. However, Chinese journals do not get an equivalent
reception from their international counterparts. The international visibility
of Chinese journals is low, but varies among fields of science. Journals of the
Chinese Academy of Sciences (CAS) have a better reception in the international
scientific community than university journals.
"
155,"The Citation Impacts and Citation Environments of Chinese Journals in
  Mathematics","  Based on the citation data of journals covered by the China Scientific and
Technical Papers and Citations Database (CSTPCD), we obtained aggregated
journal-journal citation environments by applying routines developed
specifically for this purpose. Local citation impact of journals is defined as
the share of the total citations in a local citation environment, which is
expressed as a ratio and can be visualized by the size of the nodes. The
vertical size of the nodes varies proportionally to a journal's total citation
share, while the horizontal size of the nodes is used to provide citation
information after correction for the within-journal (self-) citations. In this
study, we analyze citation impacts of three Chinese journals in mathematics and
compare local citation impacts with impact factors. Local citation impacts
reflect a journal's status and function better than (global) impact factors. We
also found that authors in Chinese journals prefer international instead of
domestic ones as sources for their citations.
"
156,"Indicators of Structural Change in the Dynamics of Science: Entropy
  Statistics of the SCI Journal Citation Reports","  Can change in citation patterns among journals be used as an indicator of
structural change in the organization of the sciences? Aggregated
journal-journal citations for 1999 are compared with similar data in the
Journal Citation Reports 1998 of the Science Citation Index. In addition to
indicating local change, probabilistic entropy measures enable us to analyze
changes in distributions at different levels of aggregation. The results of
various statistics are discussed and compared by elaborating the
journal-journal mappings. The relevance of this indicator for science and
technology policies is further specified.
"
157,"Environment and Planning B as a Journal: The interdisciplinarity of its
  environment and the citation impact","  The citation impact of Environment and Planning B can be visualized using its
citation relations with journals in its environment as the links of a network.
The size of the nodes is varied in correspondence to the relative citation
impact in this environment. Additionally, one can correct for the effect of
within-journal ""self""-citations. The network can be partitioned and clustered
using algorithms from social network analysis. After transposing the matrix in
terms of rows and columns, the citing patterns can be mapped analogously.
Citing patterns reflect the activity of the community of authors who publish in
the journal, while being cited indicates reception. Environment and Planning B
is cited across the interface between the social sciences and the natural
sciences, but its authors cite almost exclusively from the domain of the Social
Science Citation Index.
"
158,"Can Scientific Journals be Classified in terms of Aggregated
  Journal-Journal Citation Relations using the Journal Citation Reports?","  The aggregated citation relations among journals included in the Science
Citation Index provide us with a huge matrix which can be analyzed in various
ways. Using principal component analysis or factor analysis, the factor scores
can be used as indicators of the position of the cited journals in the citing
dimensions of the database. Unrotated factor scores are exact, and the
extraction of principal components can be made stepwise since the principal
components are independent. Rotation may be needed for the designation, but in
the rotated solution a model is assumed. This assumption can be legitimated on
pragmatic or theoretical grounds. Since the resulting outcomes remain sensitive
to the assumptions in the model, an unambiguous classification is no longer
possible in this case. However, the factor-analytic solutions allow us to test
classifications against the structures contained in the database. This will be
demonstrated for the delineation of a set of biochemistry journals.
"
159,Classification and Powerlaws: The Logarithmic Transformation,"  Logarithmic transformation of the data has been recommended by the literature
in the case of highly skewed distributions such as those commonly found in
information science. The purpose of the transformation is to make the data
conform to the lognormal law of error for inferential purposes. How does this
transformation affect the analysis? We factor analyze and visualize the
citation environment of the Journal of the American Chemical Society (JACS)
before and after a logarithmic transformation. The transformation strongly
reduces the variance necessary for classificatory purposes and therefore is
counterproductive to the purposes of the descriptive statistics. We recommend
against the logarithmic transformation when sets cannot be defined
unambiguously. The intellectual organization of the sciences is reflected in
the curvilinear parts of the citation distributions, while negative powerlaws
fit excellently to the tails of the distributions.
"
160,"Co-occurrence Matrices and their Applications in Information Science:
  Extending ACA to the Web Environment","  Co-occurrence matrices, such as co-citation, co-word, and co-link matrices,
have been used widely in the information sciences. However, confusion and
controversy have hindered the proper statistical analysis of this data. The
underlying problem, in our opinion, involved understanding the nature of
various types of matrices. This paper discusses the difference between a
symmetrical co-citation matrix and an asymmetrical citation matrix as well as
the appropriate statistical techniques that can be applied to each of these
matrices, respectively. Similarity measures (like the Pearson correlation
coefficient or the cosine) should not be applied to the symmetrical co-citation
matrix, but can be applied to the asymmetrical citation matrix to derive the
proximity matrix. The argument is illustrated with examples. The study then
extends the application of co-occurrence matrices to the Web environment where
the nature of the available data and thus data collection methods are different
from those of traditional databases such as the Science Citation Index. A set
of data collected with the Google Scholar search engine is analyzed using both
the traditional methods of multivariate analysis and the new visualization
software Pajek that is based on social network analysis and graph theory.
"
161,The Import and Export of Cognitive Science,"  From its inception, a large part of the motivation for Cognitive Science has
been the need for an interdisciplinary journal for the study of minds and
intelligent systems. One threat to the interdisciplinarity of Cognitive
Science, both the field and journal, is that it may become, or already be, too
dominated by psychologists. In 2005, psychology was a keyword for 51% of
submissions, followed distantly by linguistics (17%), artificial intelligence
(13%), neuroscience (10%), computer science (9%), and philosophy (8%). The
Institute for Scientific Information (ISI) gathers data not only on how
individual articles cite one another, but also on macroscopic citation patterns
among journals. Journals or sets of journals can be considered as proxies for
fields. As fields become established, they often create journals. By studying
the patterns of citations among journals that cite and are cited by Cognitive
Science, we can better: 1) appreciate the scholarly ecology surrounding the
journal and the journals role within this ecology, 2) establish competitor and
alternate journals, and 3) determine the natural clustering of fields related
to cognitive science.
"
162,"Collaboration in sensor network research: an in-depth longitudinal
  analysis of assortative mixing patterns","  Many investigations of scientific collaboration are based on statistical
analyses of large networks constructed from bibliographic repositories. These
investigations often rely on a wealth of bibliographic data, but very little or
no other information about the individuals in the network, and thus, fail to
illustrate the broader social and academic landscape in which collaboration
takes place. In this article, we perform an in-depth longitudinal analysis of a
relatively small network of scientific collaboration (N = 291) constructed from
the bibliographic record of a research center involved in the development and
application of sensor network and wireless technologies. We perform a
preliminary analysis of selected structural properties of the network,
computing its range, configuration and topology. We then support our
preliminary statistical analysis with an in-depth temporal investigation of the
assortative mixing of selected node characteristics, unveiling the researchers'
propensity to collaborate preferentially with others with a similar academic
profile. Our qualitative analysis of mixing patterns offers clues as to the
nature of the scientific community being modeled in relation to its
organizational, disciplinary, institutional, and international arrangements of
collaboration.
"
163,"An Indicator of Research Front Activity: Measuring Intellectual
  Organization as Uncertainty Reduction in Document Sets","  When using scientific literature to model scholarly discourse, a research
specialty can be operationalized as an evolving set of related documents. Each
publication can be expected to contribute to the further development of the
specialty at the research front. The specific combinations of title words and
cited references in a paper can then be considered as a signature of the
knowledge claim in the paper: new words and combinations of words can be
expected to represent variation, while each paper is at the same time
selectively positioned into the intellectual organization of a field using
context-relevant references. Can the mutual information among these three
dimensions--title words, cited references, and sequence numbers--be used as an
indicator of the extent to which intellectual organization structures the
uncertainty prevailing at a research front? The effect of the discovery of
nanotubes (1991) on the previously existing field of fullerenes is used as a
test case. Thereafter, this method is applied to science studies with a focus
on scientometrics using various sample delineations. An emerging research front
about citation analysis can be indicated.
"
164,"A New Approach to Analyzing Patterns of Collaboration in Co-authorship
  Networks - Mesoscopic Analysis and Interpretation","  This paper focuses on methods to study patterns of collaboration in
co-authorship networks at the mesoscopic level. We combine qualitative methods
(participant interviews) with quantitative methods (network analysis) and
demonstrate the application and value of our approach in a case study comparing
three research fields in chemistry. A mesoscopic level of analysis means that
in addition to the basic analytic unit of the individual researcher as node in
a co-author network, we base our analysis on the observed modular structure of
co-author networks. We interpret the clustering of authors into groups as
bibliometric footprints of the basic collective units of knowledge production
in a research specialty. We find two types of coauthor-linking patterns between
author clusters that we interpret as representing two different forms of
cooperative behavior, transfer-type connections due to career migrations or
one-off services rendered, and stronger, dedicated inter-group collaboration.
Hence the generic coauthor network of a research specialty can be understood as
the overlay of two distinct types of cooperative networks between groups of
authors publishing in a research specialty. We show how our analytic approach
exposes field specific differences in the social organization of research.
"
165,Data Preservation in High Energy Physics,"  Data from high-energy physics (HEP) experiments are collected with
significant financial and human effort and are mostly unique. At the same time,
HEP has no coherent strategy for data preservation and re-use. An
inter-experimental Study Group on HEP data preservation and long-term analysis
was convened at the end of 2008 and held two workshops, at DESY (January 2009)
and SLAC (May 2009). This document is an intermediate report to the
International Committee for Future Accelerators (ICFA) of the reflections of
this Study Group.
"
166,Geant4 in Scientific Literature,"  The Geant4 reference paper published in Nuclear Instruments and Methods A in
2003 has become the most cited publication in the whole Nuclear Science and
Technology category of Thomson-Reuter's Journal Citation Reports. It is
currently the second most cited article among the publications authored by two
major research institutes, CERN and INFN. An overview of Geant4 presence (and
absence) in scholarly literature is presented; the patterns of Geant4 citations
are quantitatively examined and discussed.
"
167,"Clusters and Maps of Science Journals Based on Bi-connected Graphs in
  the Journal Citation Reports","  The aggregated journal-journal citation matrix derived from the Journal
Citation Reports 2001 can be decomposed into a unique subject classification by
using the graph-analytical algorithm of bi-connected components. This technique
was recently incorporated in software tools for social network analysis. The
matrix can be assessed in terms of its decomposability using articulation
points which indicate overlap between the components. The articulation points
of this set did not exhibit a next-order network of 'general science' journals.
However, the clusters differ in size and in terms of the internal density of
their relations. A full classification of the journals is provided in an
Appendix. The clusters can also be extracted and mapped for the visualization.
"
168,"The university-industry knowledge relationship: Analyzing patents and
  the science base of technologies","  Via the Internet, information scientists can obtain cost-free access to large
databases in the hidden or deep web. These databases are often structured far
more than the Internet domains themselves. The patent database of the U.S.
Patent and Trade Office is used in this study to examine the science base of
patents in terms of the literature references in these patents.
University-based patents at the global level are compared with results when
using the national economy of the Netherlands as a system of reference. Methods
for accessing the on-line databases and for the visualization of the results
are specified. The conclusion is that 'biotechnology' has historically
generated a model for theorizing about university-industry relations that
cannot easily be generalized to other sectors and disciplines.
"
169,Mapping the Chinese Science Citation Database,"  Methods developed for mapping the journal structures contained in aggregated
journal-journal citations in the Science Citation Index are applied to the
Chinese Science Citation Database of the Chinese Academy of Sciences. This
database covers 991 journals, of which only 37 had originally English titles.
Using factor-analytical and graph-analytical techniques we show that this data
is dually structured. The main structure is the intellectual organization of
the journals in journal groups (as in the international SCI), but the
university-based journals provide an institutional layer that orients this
structure towards practical ends (e.g., agriculture). The Chinese Science
Citation Database exhibits the characteristics of Mode 2 in the production of
scientific knowledge more than its western counterparts. The contexts of
application lead to correlation (interfactorial complexity) among the
components.
"
170,Scientometrics and the evaluation of European integration,"  In this chapter, we elaborate on the topic of European integration in
science. We will not deal with questions related to the effects of European
integration, but only with the scientometric question how one can
quantitatively indicate integration of the European science system. Our study
is intended to facilitate and supplement debates rather than to provide a final
answer to the questions whether European integration 'exists'. In this chapter,
we first discuss the use of scientometric indicators in research evaluation
from a historical perspective in (section 2). A discussion of European science
policy follows (section 3). Then, we introduce a number of indicators of
integration and discuss our empirical results concerning the evolution of the
European science system in the 1980s and 1990s (section 4). We close the
chapter with a discussion of possible avenues of future research for enhancing
research evaluation (section 5).
"
171,"Communication of Science Shop Mediation: A Kaleidoscope of
  University-Society Relations","  The Science Shop model was initiated in the Netherlands in the 1970s. Part of
the model is the modest scale of the operation. The crucial idea behind the
Science Shops involves a working relationship between knowledge-producing
institutions like universities and citizen groups that need relevant questions
answered. In providing this link, the relations between science and the public
can be stimulated by providing such groups with access to the university and by
offering active mediation of these questions. This research addresses the
question of the external visibility of Science Shop work in terms of
communications which reach beyond the local context of the participants. In
addition to the question of the effects of this specific type of communication
in terms of publications, institutional development, and curriculum
development, we study the communication of the results in the press, the
popular and grey literature, and other means of communication insofar as
retrievable on distance through the Internet.
"
172,"The Mutual Information of University-Industry-Government Relations: An
  Indicator of the Triple Helix Dynamics","  University-industry-government relations provide a networked infrastructure
for knowledge-based innovation systems. This infrastructure organizes the
dynamic fluxes locally and the knowledge base remains emergent given these
conditions. Whereas the relations between the institutions can be measured as
variables, the interacting fluxes generate a probabilistic entropy. The mutual
information among the three institutional dimensions provides us with an
indicator of this entropy. When this indicator is negative, self-organization
can be expected. The self-organizing dynamic may temporarily be stabilized in
the overlay of communications among the carrying agencies. The various dynamics
of Triple Helix relations at the global and national levels, in different
databases, and in different regions of the world, are distinguished by applying
this indicator to scientometric and webometric data.
"
173,"A study of seismology as a dynamic, distributed area of scientific
  research","  Seismology has several features that suggest it is a highly internationalized
field: the subject matter is global, the tools used to analyse seismic waves
are dependent upon information technologies, and governments are interested in
funding cooperative research. We explore whether an emerging field like
seismology has a more internationalised structure than the older, related field
of geophysics. Using aggregated journal-journal citations, we first show that,
within the citing environment, seismology emerged from within geophysics as its
own field in the 1990s. The bibliographic analysis, however, does not show that
seismology is more internationalised than geophysics: in 2000, seismology had a
lower percentage of all articles co-authored on an international basis.
Nevertheless, social network analysis shows that the core group of cooperating
countries within seismology is proportionately larger and more distributed than
that within geophysics. While the latter exhibits an established network with a
hierarchy, the formation of a field in terms of new partnership relations is
ongoing in seismology.
"
174,"An evaluation of Flickrs distributed classification system, from the
  perspective of its members, and as an image retrieval tool in comparison with
  a controlled vocabulary","  The profusion of online digital images presents new challenges for image
indexing. Images have always been problematic to describe and catalogue due to
lack of inherent textual data and ambiguity of meaning. An alternative to
time-consuming professionally-applied metadata has been sought in the form of
tags, simple keywords that form a flat structure known as distributed
classification, or more popularly as a folksonomy.
  This research aims to increase understanding of why people tag and how
effective they find it for searching, using as the focus. Open-ended
questionnaires were sent out to members of the photo-sharing website Flickr,
with the opportunity to post comments to an online discussion space. There is
also a systematic comparison between a tag-based system and a more traditional
controlled vocabulary, to test out the claims made regarding the suitability of
tagging for searching and browsing. For this purpose Flickr has been compared
with Getty Images using a series of test themes.
  The small number of people who replied to the questionnaire gave detailed
answers that confirmed several assertions made about tags: they are accepted
despite their flaws (sloppiness and potential for inaccuracy) because they
serve their purpose to a satisfactory level. Some answers challenged the
assumption that tagging is only done for personal benefit. The search
comparison found that while Getty allows highly specific queries and logical
semantic links, Flickr is more flexible and better placed to deal with subtle
concepts. The overall conclusion is that tags achieve most when used in
conjunction with groupings of people with a shared interest.
"
175,Data Preservation at LEP,"  The four LEP experiments ALEPH, DELPHI, L3 and OPAL successfully recorded
e+e- collision data during the years 1989 to 2000. As part of the ordinary
evolution in High Energy Physics, these experiments can not be repeated and
their data is therefore unique. This article briefly reviews the data
preservation efforts undertaken by the four experiments beyond the end of data
taking. The current status of the preserved data and associated tools is
summarised.
"
176,Are astronomical papers with more authors cited more?,"  Following my previous study of paper length vs. number of citations in
astronomy (Stanek 2008), some colleagues expressed an interest in knowing if
any correlation exists between citations and the number of authors on an
astronomical paper. At least naively, one would expect papers with more authors
to be cited more. I test this expectation with the same sample of papers as
analyzed in Stanek (2008), selecting all (~30,000) refereed papers from A&A,
AJ, ApJ and MNRAS published between 2000 and 2004. (...) I find that indeed
papers with more authors are on average cited more, but only weakly so:
roughly, the number of citations doubles with ten-fold increase in the number
of authors. While the median number of citations to a 2 author paper is 17, the
median number of citations to a paper with 10 to 20 authors is 32. I find that
most of the papers are written by a small number of authors, with a mode of 2
authors and a median of 3 authors. I also find that papers with more authors
are not longer than papers with fewer authors, in fact a median number of 8 to
10 pages per paper holds for any number of authors. For the same sample of
papers, a median number of citations per paper grew from 15 in June 2008
(Stanek 2008) to 19 in November 2009. Unlike Stanek (2008), I do not conclude
with any career advice, semi-humorous or otherwise.
"
177,Institutional Repository saber.ula.ve: A testimonial perspective,"  In this paper, we describe our decade-long experience of building and
operating one of the most active Institutional Repository in the world:
www.saber.ula.ve <http://www.saber.ula.ve> (University of the Andes,
Merida-Venezuela). In order to share our experience with other institutions, we
firstly explain the steps we followed to preserve and disseminate the
scientific production of the University of Los Andes' researchers. We then
present some recent quantitative results about our repository activities and we
outline some methodological guidelines that could be applied in order to
replicate similar experiences. These guidelines list the ingredients or
building blocks as well as the processes followed for developing and
maintaining the services of an Institutional Repository. These include
technological infrastructure; institutional policies on preservation,
publication and dissemination of knowledge; recommendations on incentives for
open access publication; the process of selection, testing and adaptation of
technological tools; the planning and organization of services, and the
dissemination and support within the scientific community that will eventually
lead to the adoption of the ideas that lie behind the open access movement. We
summarize the results obtained regarding the acceptance, adoption and use of
the technological tools used for the publication of our institution's
intellectual production, and we present the main obstacles encountered on the
way.
"
178,"The first Italian research assessment exercise: a bibliometric
  perspective","  In December 2003, seventeen years after the first UK research assessment
exercise, Italy started up its first-ever national research evaluation, with
the aim to evaluate, using the peer review method, the excellence of the
national research production. The evaluation involved 20 disciplinary areas,
102 research structures, 18,500 research products and 6,661 peer reviewers
(1,465 from abroad); it had a direct cost of 3.55 millions Euros and a time
length spanning over 18 months. The introduction of ratings based on ex post
quality of output and not on ex ante respect for parameters and compliance is
an important leap forward of the national research evaluation system toward
meritocracy. From the bibliometric perspective, the national assessment offered
the unprecedented opportunity to perform a large-scale comparison of peer
review and bibliometric indicators for an important share of the Italian
research production. The present investigation takes full advantage of this
opportunity to test whether peer review judgements and (article and journal)
bibliometric indicators are independent variables and, in the negative case, to
measure the sign and strength of the association. Outcomes allow us to advocate
the use of bibliometric evaluation, suitably integrated with expert review, for
the forthcoming national assessment exercises, with the goal of shifting from
the assessment of research excellence to the evaluation of average research
performance without significant increase of expenses.
"
179,"Maps on the basis of the Arts & Humanities Citation Index: The journals
  Leonardo and Art Journal versus ""Digital Humanities"" as a topic","  The possibilities of using the Arts & Humanities Citation Index (A&HCI) for
journal mapping have not been sufficiently recognized because of the absence of
a Journal Citations Report (JCR) for this database. A quasi-JCR for the A&HCI
(2008) was constructed from the data contained in the Web-of-Science and is
used for the evaluation of two journals as examples: Leonardo and Art Journal.
The maps on the basis of the aggregated journal-journal citations within this
domain can be compared with maps including references to journals in the
Science Citation Index and Social Science Citation Index. Art journals are
cited by (social) science journals more than by other art journals, but these
journals draw upon one another in terms of their own references. This cultural
impact in terms of being cited is not found when documents with a topic such as
""digital humanities"" are analyzed. This community of practice functions more as
an intellectual organizer than a journal.
"
180,An index to link scientific productivity with visibility,"  I here propose an index that links the number of papers a researcher has
published with impact factors (IFs) of the journals that publish these papers.
A researcher is said to have an index z if totally z of his/her papers are
published in journals with IFs of at least z/2. The z-index, not meant to
evaluate, compare and rank scientists, is a number that hopefully conveniently
summarizes the number of publications in journals with high IFs.
"
181,"Science overlay maps: a new tool for research policy and library
  management","  We present a novel approach to visually locate bodies of research within the
sciences, both at each moment of time and dynamically. This article describes
how this approach fits with other efforts to locally and globally map
scientific outputs. We then show how these science overlay maps help benchmark,
explore collaborations, and track temporal changes, using examples of
universities, corporations, funding agencies, and research topics. We address
conditions of application, with their advantages, downsides and limitations.
Overlay maps especially help investigate the increasing number of scientific
developments and organisations that do not fit within traditional disciplinary
categories. We make these tools accessible to help researchers explore the
ongoing socio-cognitive transformation of science and technology systems.
"
182,Studies on access: a review,"  A review of the empirical literature on access to scholarly information. This
review focuses on surveys of authors, article download and citation analysis.
"
183,The SJR indicator: A new indicator of journals' scientific prestige,"  This paper proposes an indicator of journals' scientific prestige, the SJR
indicator, for ranking scholarly journals based on citation weighting schemes
and eigenvector centrality to be used in complex and heterogeneous citation
networks such Scopus. Its computation methodology is described and the results
after implementing the indicator over Scopus 2007 dataset are compared to an
ad-hoc Journal Impact Factor both generally and inside specific scientific
areas. The results showed that SJR indicator and JIF distributions fitted well
to a power law distribution and that both metrics were strongly correlated,
although there were also major changes in rank. There was an observable general
trend that might indicate that SJR indicator values decreased certain JIF
values whose citedeness was greater than would correspond to their scientific
influence.
"
184,The skewness of computer science,"  Computer science is a relatively young discipline combining science,
engineering, and mathematics. The main flavors of computer science research
involve the theoretical development of conceptual models for the different
aspects of computing and the more applicative building of software artifacts
and assessment of their properties. In the computer science publication
culture, conferences are an important vehicle to quickly move ideas, and
journals often publish deeper versions of papers already presented at
conferences. These peculiarities of the discipline make computer science an
original research field within the sciences, and, therefore, the assessment of
classical bibliometric laws is particularly important for this field. In this
paper, we study the skewness of the distribution of citations to papers
published in computer science publication venues (journals and conferences). We
find that the skewness in the distribution of mean citedness of different
venues combines with the asymmetry in citedness of articles in each venue,
resulting in a highly asymmetric citation distribution with a power law tail.
Furthermore, the skewness of conference publications is more pronounced than
the asymmetry of journal papers. Finally, the impact of journal papers, as
measured with bibliometric indicators, largely dominates that of proceeding
papers.
"
185,Using Multipartite Graphs for Recommendation and Discovery,"  The Smithsonian/NASA Astrophysics Data System exists at the nexus of a dense
system of interacting and interlinked information networks. The syntactic and
the semantic content of this multipartite graph structure can be combined to
provide very specific research recommendations to the scientist/user.
"
186,"TGCat, The Chandra Transmission Grating Catalog and Archive: Systems,
  Design and Accessibility","  The recently released Chandra Transmission Grating Catalog and Archive,
TGCat, presents a fully dynamic on-line catalog allowing users to browse and
categorize Chandra gratings observations quickly and easily, generate custom
plots of resulting response corrected spectra on-line without the need for
special software and to download analysis ready products from multiple
observations in one convenient operation. TGCat has been registered as a VO
resource with the NVO providing direct access to the catalogs interface. The
catalog is supported by a back-end designed to automatically fetch newly public
data, process, archive and catalog them, At the same time utilizing an advanced
queue system integrated into the archive's MySQL database allowing large
processing projects to take advantage of an unlimited number of CPUs across a
network for rapid completion. A unique feature of the catalog is that all of
the high level functions used to retrieve inputs from the Chandra archive and
to generate the final data products are available to the user in an ISIS
written library with detailed documentation. Here we present a structural
overview of the Systems, Design, and Accessibility features of the catalog and
archive.
"
187,"Self-Selected or Mandated, Open Access Increases Citation Impact for
  Higher Quality Research","  Articles whose authors make them Open Access (OA) by self-archiving them
online are cited significantly more than articles accessible only to
subscribers. Some have suggested that this ""OA Advantage"" may not be causal but
just a self-selection bias, because authors preferentially make higher-quality
articles OA. To test this we compared self-selective self-archiving with
mandatory self-archiving for a sample of 27,197 articles published 2002-2006 in
1,984 journals. The OA Advantage proved just as high for both. Logistic
regression showed that the advantage is independent of other correlates of
citations (article age; journal impact factor; number of co-authors, references
or pages; field; article type; or country) and greatest for the most highly
cited articles. The OA Advantage is real, independent and causal, but skewed.
Its size is indeed correlated with quality, just as citations themselves are
(the top 20% of articles receive about 80% of all citations). The advantage is
greater for the more citeable articles, not because of a quality bias from
authors self-selecting what to make OA, but because of a quality advantage,
from users self-selecting what to use and cite, freed by OA from the
constraints of selective accessibility to subscribers only.
"
188,"New ways of scientific publishing and accessing human knowledge inspired
  by transdisciplinary approaches","  Inspired by interdisciplinary work touching biology and microtribology, the
authors propose a new, dynamic way of publishing research results, the
establishment of a tree of knowledge and the localisation of scientific
articles on this tree. 'Technomimetics' is proposed as a new method of
knowledge management in science and technology: it shall help find and organise
information in an era of over-information. Such ways of presenting and managing
research results would be accessible by people with different kinds of
backgrounds and levels of education, and allow for full use of the ever-
increasing number of scientific and technical publications. This approach would
dramatically change and revolutionize the way we are doing science, and
contribute to overcoming the three gaps between the world of ideas, inventors,
innovators and investors as introduced by Gebeshuber, Gruber and Drack in 2009
for accelerated scientific and technological breakthroughs to improve the human
condition. Inspiration for the development of above methods was the fact that -
generally - tribologists and biologists do not see many overlaps of their
professions. However, both deal with materials, structures and processes.
Tribology is omnipresent in biology and many biological systems have impressive
tribological properties. Tribologists can therefore get valuable input and
inspiration from living systems. The aim of biomimetics is knowledge transfer
from biology to technology and successful biomimetics in tribology needs
collaboration between biologists and tribologists. Literature search shows that
the number of papers regarding biotribology is steadily increasing. However, at
the moment, most scientific papers of the other respective field are hard to
access and hard to understand, in terms of concepts and specific wording.
"
189,"Quality Control and Validation Boundaries in a Triple Helix of
  University-Industry-Government: 'Mode 2' and the Future of University
  Research","  How is quality control organized in the new ""Mode 2"" of the production of
scientific knowledge? When institutional boundaries are increasingly blurred in
a Triple Helix of University-Industry-Government relations, criteria for
quality control in the production of scientific knowledge can be expected to
change at the interfaces. The categorization in terms of two modes of knowledge
production was introduced by Gibbons et al. (1994) in order to describe changes
in the networks of scientific communications (funding patterns, research
configurations, styles of knowledge management, etc.). These changes were
mainly specified as institutional parameters in order to deal with the subjects
of R&D management and S&T policies, that is, ex ante (Spiegel-Ring 1973 Van den
Daele et al. 1979). We focus on the 'validation boundaries' emerging from the
differences between Mode 1 and Mode 2 that is, on the criteria for quality
control that can analytically and reflexively be brought to the fore ex post.
The shift from an institutional frame of reference to a focus on the dynamics
of communications enables us to clarify several problems in the discussion of
the future of university research.
"
190,"Distributed scientific communication in the European information
  society: Some cases of ""Mode 2"" fields of research","  Can self-organization of scientific communication be specified by using
literature-based indicators? In this study, we explore this question by
applying entropy measures to typical ""Mode-2"" fields of knowledge production.
We hypothesized these scientific systems to be developing from a
self-organization of the interaction between cognitive and institutional
levels: European subsidized research programs aim at creating an institutional
network, while a cognitive reorganization is continuously ongoing at the
scientific field level. The results indicate that the European system develops
towards a stable level of distribution of cited references and title-words
among the European member states. We suggested that this distribution could be
a property of the emerging European system. In order to measure to degree of
specialization with respect to the respective distributions of countries, cited
references and title words, the mutual information among the three frequency
distributions was calculated. The so-called transmission values informed us
that the European system shows increasing levels of differentiation.
"
191,"HepML, an XML-based format for describing simulated data in high energy
  physics","  In this paper we describe a HepML format and a corresponding C++ library
developed for keeping complete description of parton level events in a unified
and flexible form. HepML tags contain enough information to understand what
kind of physics the simulated events describe and how the events have been
prepared. A HepML block can be included into event files in the LHEF format.
The structure of the HepML block is described by means of several XML Schemas.
The Schemas define necessary information for the HepML block and how this
information should be located within the block. The library libhepml is a C++
library intended for parsing and serialization of HepML tags, and representing
the HepML block in computer memory. The library is an API for external
software. For example, Matrix Element Monte Carlo event generators can use the
library for preparing and writing a header of a LHEF file in the form of HepML
tags. In turn, Showering and Hadronization event generators can parse the HepML
header and get the information in the form of C++ classes. libhepml can be used
in C++, C, and Fortran programs. All necessary parts of HepML have been
prepared and we present the project to the HEP community.
"
192,"The long-term dynamics of co-authorship scientific networks,
  Iberoamerican Countries (1973-2006)","  We study the national production of academic knowledge in all Iberoamerican
countries (IAC) between 1973 and 2007. We show that the total number of
mainstream scientific publications listed in SCI,SSCI and A&HCI follows an
exponential growth, the same as the national productivity expressed in the
number of publications per capita. We also explore the temporal evolution of
the co-authorship patterns between a sample of 12 IAC responsible for 98% of
the total regional publications, with a group of other 45 nations. We show that
the scientific co-authorship among countries follows a power-law and behaves as
a self-organizing scale-free network, where each country appears as a node and
each co-publication as a link. We develop a mathematical model to study the
temporal evolution of co-authorship networks, based on a preferential
attachment strategy and we show that the number of co-publications among
countries growths quadraticly against time. We empirically determine the
quadratic growth constants for 352 different networks within. We corroborate
that the connectivity of regional countries with larger scientific networks is
growing faster than with other less connected countries. We determine the
dates, at which the co-authorship connectivities trigger the self-organizing
scale free network for each of the 352 cases. We find that the last follows a
normal distribution around year 1981.4 +/-2.2 and we connect the last effect
with a brain-drainage process generated in the region. We show how the number
of co-publications Pki (t) between country k and country i, is related with a
power-law against the coupling growth coefficients aki.
"
193,"Collaboration in an Open Data eScience: A Case Study of Sloan Digital
  Sky Survey","  Current science and technology has produced more and more publically
accessible scientific data. However, little is known about how the open data
trend impacts a scientific community, specifically in terms of its
collaboration behaviors. This paper aims to enhance our understanding of the
dynamics of scientific collaboration in the open data eScience environment via
a case study of co-author networks of an active and highly cited open data
project, called Sloan Digital Sky Survey. We visualized the co-authoring
networks and measured their properties over time at three levels: author,
institution, and country levels. We compared these measurements to a random
network model and also compared results across the three levels. The study
found that 1) the collaboration networks of the SDSS community transformed from
random networks to small-world networks; 2) the number of author-level
collaboration instances has not changed much over time, while the number of
collaboration instances at the other two levels has increased over time; 3)
pairwise institutional collaboration become common in recent years. The open
data trend may have both positive and negative impacts on scientific
collaboration.
"
194,"Digital Mathematics Libraries: The Good, the Bad, the Ugly","  The idea of a World digital mathematics library (DML) has been around since
the turn of the 21th century. We feel that it is time to make it a reality,
starting in a modest way from successful bricks that have already been built,
but with an ambitious goal in mind. After a brief historical overview of
publishing mathematics, an estimate of the size and a characterisation of the
bulk of documents to be included in the DML, we turn to proposing a model for a
Reference Digital Mathematics Library--a network of institutions where the
digital documents would be physically archived. This pattern based rather on
the bottom-up strategy seems to be more practicable and consistent with the
digital nature of the DML. After describing the model we summarise what can and
should be done in order to accomplish the vision. The current state of some of
the local libraries that could contribute to the global views are described
with more details.
"
195,"ONER: Tool for Organization Named Entity Recognition from Affiliation
  Strings in PubMed Abstracts","  Automatically extracting organization names from the affiliation sentences of
articles related to biomedicine is of great interest to the pharmaceutical
marketing industry, health care funding agencies and public health officials.
It will also be useful for other scientists in normalizing author names,
automatically creating citations, indexing articles and identifying potential
resources or collaborators. Today there are more than 18 million articles
related to biomedical research indexed in PubMed, and information derived from
them could be used effectively to save the great amount of time and resources
spent by government agencies in understanding the scientific landscape,
including key opinion leaders and centers of excellence. Our process for
extracting organization names involves multi-layered rule matching with
multiple dictionaries. The system achieves 99.6% f-measure in extracting
organization names.
"
196,"Towards Automatic Extraction of Social Networks of Organizations in
  PubMed Abstracts","  Social Network Analysis (SNA) of organizations can attract great interest
from government agencies and scientists for its ability to boost translational
research and accelerate the process of converting research to care. For SNA of
a particular disease area, we need to identify the key research groups in that
area by mining the affiliation information from PubMed. This not only involves
recognizing the organization names in the affiliation string, but also
resolving ambiguities to identify the article with a unique organization. We
present here a process of normalization that involves clustering based on local
sequence alignment metrics and local learning based on finding connected
components. We demonstrate the application of the method by analyzing
organizations involved in angiogenensis treatment, and demonstrating the
utility of the results for researchers in the pharmaceutical and biotechnology
industries or national funding agencies.
"
197,"Scientometrics and Communication Theory: Towards Theoretically Informed
  Indicators","  A theory of citations should not consider cited and/or citing agents as its
sole subject of study. One is able to study also the dynamics in the networks
of communications. While communicating agents (e.g., authors, laboratories,
journals) can be made comparable in terms of their publication and citation
counts, one would expect the communication networks not to be homogeneous. The
latent structures of the network indicate different codifications that span a
space of possible 'translations'. The various subdynamics can be hypothesized
from an evolutionary perspective. Using the network of aggregated
journal-journal citations in Science & Technology Studies as an empirical case,
the operation of such subdynamics can be demonstrated. Policy implications and
the consequences for a theory-driven type of scientometrics will be elaborated.
"
198,"Mapping the Geography of Science: Distribution Patterns and Networks of
  Relations among Cities and Institutes","  Using Google Earth, Google Maps and/or network visualization programs such as
Pajek, one can overlay the network of relations among addresses in scientific
publications on the geographic map. We discuss the pros en cons of the various
options, and provide software (freeware) for bridging existing gaps between the
Science Citation Indices and Scopus, on the one side, and these various
visualization tools, on the other. At the level of city names, the global map
can be drawn reliably on the basis of the available address information. At the
level of the names of organizations and institutes, there are problems of
unification both in the ISI-databases and Scopus. Pajek enables us to combine
the visualization with statistical analysis, whereas the Google Maps and its
derivates provide superior tools at the Internet.
"
199,The Citation Field of Evolutionary Economics,"  Evolutionary economics has developed into an academic field of its own,
institutionalized around, amongst others, the Journal of Evolutionary Economics
(JEE). This paper analyzes the way and extent to which evolutionary economics
has become an interdisciplinary journal, as its aim was: a journal that is
indispensable in the exchange of expert knowledge on topics and using
approaches that relate naturally with it. Analyzing citation data for the
relevant academic field for the Journal of Evolutionary Economics, we use
insights from scientometrics and social network analysis to find that, indeed,
the JEE is a central player in this interdisciplinary field aiming mostly at
understanding technological and regional dynamics. It does not, however, link
firmly with the natural sciences (including biology) nor to management
sciences, entrepreneurship, and organization studies. Another journal that
could be perceived to have evolutionary acumen, the Journal of Economic Issues,
does relate to heterodox economics journals and is relatively more involved in
discussing issues of firm and industry organization. The JEE seems most keen to
develop theoretical insights.
"
200,"International Lattice Data Grid: Turn on, plug in,and download","  In the beginning there was the internet, then came the world wide web, and
now there is the grid. In the future perhaps there will be the cloud. In the
age of persistent, pervasive, and pandemic networks I review how the lattice
QCD community embraced the open source paradigm for both code and data whilst
adopting the emerging grid technologies, and why having your data persistently
accessible via standardized protocols and services might be a good idea.
"
201,"What Can Heterogeneity Add to the Scientometric Map? Steps towards
  algorithmic historiography","  The Actor Network represents heterogeneous entities as actants (Callon et
al., 1983; 1986). Although computer programs for the visualization of social
networks increasingly allow us to represent heterogeneity in a network using
different shapes and colors for the visualization, hitherto this possibility
has scarcely been exploited (Mogoutov et al., 2008). In this contribution to
the Festschrift, I study the question of what heterogeneity can add
specifically to the visualization of a network. How does an integrated network
improve on the one-dimensional ones (such as co-word and co-author maps)? The
oeuvre of Michel Callon is used as the case materials, that is, his 65 papers
which can be retrieved from the (Social) Science Citation Index since 1975.
"
202,On the meaning of the h-index,"  The h-index -- the value for which an individual has published at least h
papers with at least h citations -- has become a popular metric to assess the
citation impact of scientists. As already noted in the original work of Hirsch
and as evidenced from data of a representative sample of physicists, sqrt{c}
scales as h, where c is the total number citations to an individual. Thus
sqrt{c} appears to be equivalent to the h index. As a further check of this
equivalence, the distribution of the ratio s=sqrt{c}/2h for this sample is
sharply peaked about 1. The outliers in this distribution reveal fundamentally
different types of individual publication records.
"
203,"Caveats for the journal and field normalizations in the CWTS (""Leiden"")
  evaluations of research performance","  The Center for Science and Technology Studies at Leiden University advocates
the use of specific normalizations for assessing research performance with
reference to a world average. The Journal Citation Score (JCS) and Field
Citation Score (FCS) are averaged for the research group or individual
researcher under study, and then these values are used as denominators of the
(mean) Citations per publication (CPP). Thus, this normalization is based on
dividing two averages. This procedure only generates a legitimate indicator in
the case of underlying normal distributions. Given the skewed distributions
under study, one should average the observed versus expected values which are
to be divided first for each publication. We show the effects of the Leiden
normalization for a recent evaluation where we happened to have access to the
underlying data.
"
204,PageRank: Standing on the shoulders of giants,"  PageRank is a Web page ranking technique that has been a fundamental
ingredient in the development and success of the Google search engine. The
method is still one of the many signals that Google uses to determine which
pages are most important. The main idea behind PageRank is to determine the
importance of a Web page in terms of the importance assigned to the pages
hyperlinking to it. In fact, this thesis is not new, and has been previously
successfully exploited in different contexts. We review the PageRank method and
link it to some renowned previous techniques that we have found in the fields
of Web information retrieval, bibliometrics, sociometry, and econometrics.
"
205,"Open Access Mandates and the ""Fair Dealing"" Button","  We describe the ""Fair Dealing Button,"" a feature designed for authors who
have deposited their papers in an Open Access Institutional Repository but have
deposited them as ""Closed Access"" (meaning only the metadata are visible and
retrievable, not the full eprint) rather than Open Access. The Button allows
individual users to request and authors to provide a single eprint via
semi-automated email. The purpose of the Button is to tide over research usage
needs during any publisher embargo on Open Access and, more importantly, to
make it possible for institutions to adopt the
""Immediate-Deposit/Optional-Access"" Mandate, without exceptions or opt-outs,
instead of a mandate that allows delayed deposit or deposit waivers, depending
on publisher permissions or embargoes (or no mandate at all). This is only
""Almost-Open Access,"" but in facilitating exception-free immediate-deposit
mandates it will accelerate the advent of universal Open Access.
"
206,A student's guide to searching the literature using online databases,"  A method is described to empower students to efficiently perform general and
literature searches using online resources. The method was tested on
undergraduate and graduate students with varying backgrounds with scientific
literature. Students involved in this study showed marked improvement in their
awareness of how and where to find accurate scientific information.
"
207,Author Identifiers in Scholarly Repositories,"  Bibliometric and usage-based analyses and tools highlight the value of
information about scholarship contained within the network of authors, articles
and usage data. Less progress has been made on populating and using the author
side of this network than the article side, in part because of the difficulty
of unambiguously identifying authors. I briefly review a sample of author
identifier schemes, and consider use in scholarly repositories. I then describe
preliminary work at arXiv to implement public author identifiers, services
based on them, and plans to make this information useful beyond the boundaries
of arXiv.
"
208,Rivals for the crown: Reply to Opthof and Leydesdorff,"  We reply to the criticism of Opthof and Leydesdorff [arXiv:1002.2769] on the
way in which our institute applies journal and field normalizations to citation
counts. We point out why we believe most of the criticism is unjustified, but
we also indicate where we think Opthof and Leydesdorff raise a valid point.
"
209,Towards a new crown indicator: Some theoretical considerations,"  The crown indicator is a well-known bibliometric indicator of research
performance developed by our institute. The indicator aims to normalize
citation counts for differences among fields. We critically examine the
theoretical basis of the normalization mechanism applied in the crown
indicator. We also make a comparison with an alternative normalization
mechanism. The alternative mechanism turns out to have more satisfactory
properties than the mechanism applied in the crown indicator. In particular,
the alternative mechanism has a so-called consistency property. The mechanism
applied in the crown indicator lacks this important property. As a consequence
of our findings, we are currently moving towards a new crown indicator, which
relies on the alternative normalization mechanism.
"
210,"The relation between Eigenfactor, audience factor, and influence weight","  We present a theoretical and empirical analysis of a number of bibliometric
indicators of journal performance. We focus on three indicators in particular,
namely the Eigenfactor indicator, the audience factor, and the influence weight
indicator. Our main finding is that the last two indicators can be regarded as
a kind of special cases of the first indicator. We also find that the three
indicators can be nicely characterized in terms of two properties. We refer to
these properties as the property of insensitivity to field differences and the
property of insensitivity to insignificant journals. The empirical results that
we present illustrate our theoretical findings. We also show empirically that
the differences between various indicators of journal performance are quite
substantial.
"
211,"A comparison of two techniques for bibliometric mapping:
  Multidimensional scaling and VOS","  VOS is a new mapping technique that can serve as an alternative to the
well-known technique of multidimensional scaling. We present an extensive
comparison between the use of multidimensional scaling and the use of VOS for
constructing bibliometric maps. In our theoretical analysis, we show the
mathematical relation between the two techniques. In our experimental analysis,
we use the techniques for constructing maps of authors, journals, and keywords.
Two commonly used approaches to bibliometric mapping, both based on
multidimensional scaling, turn out to produce maps that suffer from artifacts.
Maps constructed using VOS turn out not to have this problem. We conclude that
in general maps constructed using VOS provide a more satisfactory
representation of a data set than maps constructed using well-known
multidimensional scaling approaches.
"
212,Making Web Annotations Persistent over Time,"  As Digital Libraries (DL) become more aligned with the web architecture,
their functional components need to be fundamentally rethought in terms of URIs
and HTTP. Annotation, a core scholarly activity enabled by many DL solutions,
exhibits a clearly unacceptable characteristic when existing models are applied
to the web: due to the representations of web resources changing over time, an
annotation made about a web resource today may no longer be relevant to the
representation that is served from that same resource tomorrow. We assume the
existence of archived versions of resources, and combine the temporal features
of the emerging Open Annotation data model with the capability offered by the
Memento framework that allows seamless navigation from the URI of a resource to
archived versions of that resource, and arrive at a solution that provides
guarantees regarding the persistence of web annotations over time. More
specifically, we provide theoretical solutions and proof-of-concept
experimental evaluations for two problems: reconstructing an existing
annotation so that the correct archived version is displayed for all resources
involved in the annotation, and retrieving all annotations that involve a given
archived version of a web resource.
"
213,Axiomatic Quantification of Co-authors' Relative Contributions,"  Over the past decades, the competition for academic resources has gradually
intensified, and worsened with the current financial crisis. To optimize the
resource allocation, individualized assessment of research results is being
actively studied but the current indices, such as the number of papers, the
number of citations, the h-factor and its variants have limitations, especially
their inability of determining co-authors' credit shares fairly. Here we
establish an axiomatic system and quantify co-authors' relative contributions.
Our methodology avoids subjective assignment of co-authors' credits using the
inflated, fractional or harmonic methods, and provides a quantitative tool for
scientific management such as funding and tenure decisions.
"
214,Topic Map: An Ontology Framework for Information Retrieval,"  The basic classification techniques for organizing information are thesauri,
taxonomy and faceted classification. Topic map is relatively a new entrant to
this information space. Topic map standard describes how complex relationships
between abstract concepts and real world resources can be represented using XML
syntax. This paper explores how topic map incorporates the traditional
techniques and what are its advantages and disadvantages in several dimensions
such as content management, indexing, knowledge representation, constraint
specification and query languages in the context of information retrieval. The
constructs of topic maps are illustrated with a use-case implemented in XTM
"
215,"Indicators of the Interdisciplinarity of Journals: Diversity,
  Centrality, and Citations","  A citation-based indicator for interdisciplinarity has been missing hitherto
among the set of available journal indicators. In this study, we investigate
network indicators (betweenness centrality), journal indicators (Shannon
entropy, the Gini coefficient), and more recently proposed Rao-Stirling
measures for ""interdisciplinarity."" The latter index combines the statistics of
both citation distributions of journals (vector-based) and distances in
citation networks among journals (matrix-based). The effects of various
normalizations are specified and measured using the matrix of 8,207 journals
contained in the Journal Citation Reports of the (Social) Science Citation
Index 2008. Betweenness centrality in symmetrical (1-mode) cosine-normalized
networks provides an indicator outperforming betweenness in the asymmetrical
(2-mode) citation network. Among the vector-based indicators, Shannon entropy
performs better than the Gini coefficient, but is sensitive to size. Science
and Nature, for example, are indicated at the top of the list. The new
diversity measure provides reasonable results when (1 - cosine) is assumed as a
measure for the distance, but results using Euclidean distances were difficult
to interpret.
"
216,An HTTP-Based Versioning Mechanism for Linked Data,"  Dereferencing a URI returns a representation of the current state of the
resource identified by that URI. But, on the Web representations of prior
states of a resource are also available, for example, as resource versions in
Content Management Systems or archival resources in Web Archives such as the
Internet Archive. This paper introduces a resource versioning mechanism that is
fully based on HTTP and uses datetime as a global version indicator. The
approach allows ""follow your nose"" style navigation both from the current
time-generic resource to associated time-specific version resources as well as
among version resources. The proposed versioning mechanism is congruent with
the Architecture of the World Wide Web, and is based on the Memento framework
that extends HTTP with transparent content negotiation in the datetime
dimension. The paper shows how the versioning approach applies to Linked Data,
and by means of a demonstrator built for DBpedia, it also illustrates how it
can be used to conduct a time-series analysis across versions of Linked Data
descriptions.
"
217,A Mathematical Approach to the Study of the United States Code,"  The United States Code (Code) is a document containing over 22 million words
that represents a large and important source of Federal statutory law. Scholars
and policy advocates often discuss the direction and magnitude of changes in
various aspects of the Code. However, few have mathematically formalized the
notions behind these discussions or directly measured the resulting
representations. This paper addresses the current state of the literature in
two ways. First, we formalize a representation of the United States Code as the
union of a hierarchical network and a citation network over vertices containing
the language of the Code. This representation reflects the fact that the Code
is a hierarchically organized document containing language and explicit
citations between provisions. Second, we use this formalization to measure
aspects of the Code as codified in October 2008, November 2009, and March 2010.
These measurements allow for a characterization of the actual changes in the
Code over time. Our findings indicate that in the recent past, the Code has
grown in its amount of structure, interdependence, and language.
"
218,"Comparing Repository Types - Challenges and barriers for subject-based
  repositories, research repositories, national repository systems and
  institutional repositories in serving scholarly communication","  After two decades of repository development, some conclusions may be drawn as
to which type of repository and what kind of service best supports digital
scholarly communication, and thus the production of new knowledge. Four types
of publication repository may be distinguished, namely the subject-based
repository, research repository, national repository system and institutional
repository. Two important shifts in the role of repositories may be noted. With
regard to content, a well-defined and high quality corpus is essential. This
implies that repository services are likely to be most successful when
constructed with the user and reader uppermost in mind. With regard to service,
high value to specific scholarly communities is essential. This implies that
repositories are likely to be most useful to scholars when they offer dedicated
services supporting the production of new knowledge. Along these lines,
challenges and barriers to repository development may be identified in three
key dimensions: a) identification and deposit of content; b) access and use of
services; and c) preservation of content and sustainability of service. An
indicative comparison of challenges and barriers in some major world regions
such as Europe, North America and East Asia plus Australia is offered in
conclusion.
"
219,"wiki.openmath.org - how it works, how you can participate","  At http://wiki.openmath.org, the OpenMath 2 and 3 Content Dictionaries are
accessible via a semantic wiki interface, powered by the SWiM system. We
shortly introduce the inner workings of the system, then describe how to use
it, and conclude with first experiences gained from OpenMath society members
working with the system and an outlook to further development plans.
"
220,SWiM -- A Semantic Wiki for Mathematical Knowledge Management,"  SWiM is a semantic wiki for collaboratively building, editing and browsing
mathematical knowledge represented in the domain-specific structural semantic
markup language OMDoc. It motivates users to contribute to collections of
mathematical knowledge by instantly sharing the benefits of knowledge-powered
services with them. SWiM is currently being used for authoring content
dictionaries, i. e. collections of uniquely identified mathematical symbols,
and prepared for managing a large-scale proof formalisation effort.
"
221,"CWTS crown indicator measures citation impact of a research group's
  publication oeuvre","  The article ""Caveats for the journal and field normalizations in the CWTS
(`Leiden') evaluations of research performance"", published by Tobias Opthof and
Loet Leydesdorff (arXiv:1002.2769) deals with a subject as important as the
application of so called field normalized indicators of citation impact in the
assessment of research performance of individual researchers and research
groups. Field normalization aims to account for differences in citation
practices across scientific-scholarly subject fields. As the primary author of
the papers presenting the ""Leiden"" indicators and of many reports and articles
reporting on the outcomes of assessments actually using these measures, I
comment on the 3 main issues addressed in the paper by Opthof and Leydesdorff.
"
222,Towards a new crown indicator: An empirical analysis,"  We present an empirical comparison between two normalization mechanisms for
citation-based indicators of research performance. These mechanisms aim to
normalize citation counts for the field and the year in which a publication was
published. One mechanism is applied in the current so-called crown indicator of
our institute. The other mechanism is applied in the new crown indicator that
our institute is planning to adopt. We find that at high aggregation levels,
such as at the level of large research institutions or at the level of
countries, the differences between the two mechanisms are very small. At lower
aggregation levels, such as at the level of research groups or at the level of
journals, the differences between the two mechanisms are somewhat larger. We
pay special attention to the way in which recent publications are handled.
These publications typically have very low citation counts and should therefore
be handled with special care.
"
223,The danger of pseudo science in Informetrics,"  Two papers have been archived to which this letter is complementary: 1)
Opthof and Leydesdorff arxiv:1002.2769 2) Van Raan et al. arxiv:1003.2113 Van
Raan at all claims that the order of operations (first dividing then adding)
does not apply to citation analysis. In my contribution I discuss a few
analogues in Physics and Medicine and argue that in no other field of science
where quantities have physical or financial meaning, implying that that numbers
have a real unit of measure, it would be allowed to ignore the rule of
operations. Hence, the claim of CWTS that the order of operations is not
relevant brings studies ignoring this rule as done by CWTS in the category
'Pseudo Science'.
"
224,Citing for High Impact,"  The question of citation behavior has always intrigued scientists from
various disciplines. While general citation patterns have been widely studied
in the literature we develop the notion of citation projection graphs by
investigating the citations among the publications that a given paper cites. We
investigate how patterns of citations vary between various scientific
disciplines and how such patterns reflect the scientific impact of the paper.
We find that idiosyncratic citation patterns are characteristic for low impact
papers; while narrow, discipline-focused citation patterns are common for
medium impact papers. Our results show that crossing-community, or bridging
citation patters are high risk and high reward since such patterns are
characteristic for both low and high impact papers. Last, we observe that
recently citation networks are trending toward more bridging and
interdisciplinary forms.
"
225,Publishing Math Lecture Notes as Linked Data,"  We mark up a corpus of LaTeX lecture notes semantically and expose them as
Linked Data in XHTML+MathML+RDFa. Our application makes the resulting documents
interactively browsable for students. Our ontology helps to answer queries from
students and lecturers, and paves the path towards an integration of our corpus
with external sites.
"
226,"Scopus's Source Normalized Impact per Paper (SNIP) versus a Journal
  Impact Factor based on Fractional Counting of Citations","  Impact factors (and similar measures such as the Scimago Journal Rankings)
suffer from two problems: (i) citation behavior varies among fields of science
and therefore leads to systematic differences, and (ii) there are no statistics
to inform us whether differences are significant. The recently introduced SNIP
indicator of Scopus tries to remedy the first of these two problems, but a
number of normalization decisions are involved which makes it impossible to
test for significance. Using fractional counting of citations-based on the
assumption that impact is proportionate to the number of references in the
citing documents-citations can be contextualized at the paper level and
aggregated impacts of sets can be tested for their significance. It can be
shown that the weighted impact of Annals of Mathematics (0.247) is not so much
lower than that of Molecular Cell (0.386) despite a five-fold difference
between their impact factors (2.793 and 13.156, respectively).
"
227,Analysis of Graphs for Digital Preservation Suitability,"  We investigate the use of autonomically created small-world graphs as a
framework for the long term storage of digital objects on the Web in a
potentially hostile environment. We attack the classic Erdos - Renyi random,
Barab'asi and Albert power law, Watts - Strogatz small world and our
Unsupervised Small-World (USW) graphs using different attacker strategies and
report their respective robustness. Using different attacker profiles, we
construct a game where the attacker is allowed to use a strategy of his choice
to remove a percentage of each graph's elements. The graph is then allowed to
repair some portion of its self. We report on the number of alternating attack
and repair turns until either the graph is disconnected, or the game exceeds
the number of permitted turns. Based on our analysis, an attack strategy that
focuses on removing the vertices with the highest betweenness value is most
advantageous to the attacker. Power law graphs can become disconnected with the
removal of a single edge; random graphs with the removal of as few as 1% of
their vertices, small-world graphs with the removal of 14% vertices, and USW
with the removal of 17% vertices. Watts - Strogatz small-world graphs are more
robust and resilient than random or power law graphs. USW graphs are more
robust and resilient than small world graphs. A graph of USW connected web
objects (WOs) filled with data could outlive the individuals and institutions
that created the data in an environment where WOs are lost due to random
failures or directed attacks.
"
228,Dimensions of Formality: A Case Study for MKM in Software Engineering,"  We study the formalization of a collection of documents created for a
Software Engineering project from an MKM perspective. We analyze how document
and collection markup formats can cope with an open-ended, multi-dimensional
space of primary and secondary classifications and relationships. We show that
RDFa-based extensions of MKM formats, employing flexible ""metadata""
relationships referencing specific vocabularies for distinct dimensions, are
well-suited to encode this and to put it into service. This formalized
knowledge can be used for enriching interactive document browsing, for enabling
multi-dimensional metadata queries over documents and collections, and for
exporting Linked Data to the Semantic Web and thus enabling further reuse.
"
229,Notations Around the World: Census and Exploitation,"  Mathematical notations around the world are diverse. Not as much as requiring
computing machines' makers to adapt to each culture, but as much as to
disorient a person landing on a web-page with a text in mathematics. In order
to understand better this diversity, we are building a census of notations: it
should allow any content creator or mathematician to grasp which mathematical
notation is used in which language and culture. The census is built
collaboratively, collected in pages with a given semantic and presenting
observations of the widespread notations being used in existing materials by a
graphical extract. We contend that our approach should dissipate the fallacies
found here and there about the notations in ""other cultures"" so that a better
understanding of the cultures can be realized. The exploitation of the census
in the math-bridge project is also presented: this project aims at taking
learners ""where they are in their math-knowledge"" and bring them to a level
ready to start engineering studies. The census serves as definitive reference
for the transformation elements that generate the rendering of formul{\ae} in
web-browsers.
"
230,"Modes of Collaboration in Modern Science - Beyond Power Laws and
  Preferential Attachment","  The goal of the study is to determine the underlying processes leading to the
observed collaborator distribution in modern scientific fields, with special
attention to non-power law behavior. Nanoscience is used as a case study of a
modern interdisciplinary field, and its coauthorship network for 2000-04 period
is constructed from NanoBank database. We find three collaboration modes that
correspond to three distinct ranges in the distribution of collaborators: (1)
for authors with fewer than 20 collaborators (the majority) preferential
attachment does not hold and they form a log-normal ""hook"" instead of a power
law, (2) authors with more than 20 collaborators benefit from preferential
attachment and form a power law tail, and (3) authors with between 250 and 800
collaborators are more frequent than expected because of the hyperauthorship
practices in certain subfields.
"
231,"The Formulator MathML Editor Project: User-Friendly Authoring of Content
  Markup Documents","  Implementation of an editing process for Content MathML formulas in common
visual style is a real challenge for a software developer who does not really
want the user to have to understand the structure of Content MathML in order to
edit an expression, since it is expected that users are often not that
technically minded. In this paper, we demonstrate how this aim is achieved in
the context of the Formulator project and discuss features of this MathML
editor, which provides a user with a WYSIWYG editing style while authoring
MathML documents with Content or mixed markup. We also present the approach
taken to enhance availability of the MathML editor to end-users, demonstrating
an online version of the editor that runs inside a Web browser.
"
232,"Comparing Repository Types - Challenges and barriers for subject-based
  repositories, research repositories, national repository systems and
  institutional repositories in serving scholarly communication","  After two decades of repository development, some conclusions may be drawn as
to which type of repository and what kind of service best supports digital
scholarly communication, and thus the production of new knowledge. Four types
of publication repository may be distinguished, namely the subject-based
repository, research repository, national repository system and institutional
repository. Two important shifts in the role of repositories may be noted. With
regard to content, a well-defined and high quality corpus is essential. This
implies that repository services are likely to be most successful when
constructed with the user and reader uppermost in mind. With regard to service,
high value to specific scholarly communities is essential. This implies that
repositories are likely to be most useful to scholars when they offer dedicated
services supporting the production of new knowledge. Along these lines,
challenges and barriers to repository development may be identified in three
key dimensions: a) identification and deposit of content; b) access and use of
services; and c) preservation of content and sustainability of service. An
indicative comparison of challenges and barriers in some major world regions
such as Europe, North America and East Asia plus Australia is offered in
conclusion.
"
233,On Duplication in Mathematical Repositories,"  Building a repository of proof-checked mathematical knowledge is without any
doubt a lot of work, and besides the actual formalization process there also is
the task of maintaining the repository. Thus it seems obvious to keep a
repsoitory as small as possible, in particular each piece of mathematical
knowledge should be formalized only once. In this paper, however, we claim that
it might be reasonable or even necessary to duplicate knowledge in a
mathematical repository. We analyze different situations and reasons for doing
so and provide a number of examples supporting our thesis.
"
234,"Worldwide topology of the scientific subject profile: a macro approach
  on the country level","  Models for the production of knowledge and systems of innovation and science
are key elements for characterizing a country in view of its scientific
thematic profile. With regard to scientific output and publication in journals
of international visibility, the countries of the world may be classified into
three main groups according to their thematic bias. This paper aims to classify
the countries of the world in several broad groups, described in terms of
behavioural models that attempt to sum up the characteristics of their systems
of knowledge and innovation. We perceive three clusters in our analysis: 1) the
biomedical cluster, 2) the basic science & engineering cluster, and 3) the
agricultural cluster. The countries are conceptually associated with the
clusters via Principal Component Analysis (PCA), and a Multidimensional Scaling
(MDS) map with all the countries is presented.
"
235,Finding Your Literature Match -- A Recommender System,"  The universe of potentially interesting, searchable literature is expanding
continuously. Besides the normal expansion, there is an additional influx of
literature because of interdisciplinary boundaries becoming more and more
diffuse. Hence, the need for accurate, efficient and intelligent search tools
is bigger than ever. Even with a sophisticated search engine, looking for
information can still result in overwhelming results. An overload of
information has the intrinsic danger of scaring visitors away, and any
organization, for-profit or not-for-profit, in the business of providing
scholarly information wants to capture and keep the attention of its target
audience. Publishers and search engine engineers alike will benefit from a
service that is able to provide visitors with recommendations that closely meet
their interests. Providing visitors with special deals, new options and
highlights may be interesting to a certain degree, but what makes more sense
(especially from a commercial point of view) than to let visitors do most of
the work by the mere action of making choices? Hiring psychics is not an
option, so a technological solution is needed to recommend items that a visitor
is likely to be looking for. In this presentation we will introduce such a
solution and argue that it is practically feasible to incorporate this approach
into a useful addition to any information retrieval system with enough usage.
"
236,Metadata and provenance management,"  Scientists today collect, analyze, and generate TeraBytes and PetaBytes of
data. These data are often shared and further processed and analyzed among
collaborators. In order to facilitate sharing and data interpretations, data
need to carry with it metadata about how the data was collected or generated,
and provenance information about how the data was processed. This chapter
describes metadata and provenance in the context of the data lifecycle. It also
gives an overview of the approaches to metadata and provenance management,
followed by examples of how applications use metadata and provenance in their
scientific processes.
"
237,Proviola: A Tool for Proof Re-animation,"  To improve on existing models of interaction with a proof assistant (PA), in
particular for storage and replay of proofs, we in- troduce three related
concepts, those of: a proof movie, consisting of frames which record both user
input and the corresponding PA response; a camera, which films a user's
interactive session with a PA as a movie; and a proviola, which replays a movie
frame-by-frame to a third party. In this paper we describe the movie data
structure and we discuss a proto- type implementation of the camera and
proviola based on the ProofWeb system. ProofWeb uncouples the interaction with
a PA via a web- interface (the client) from the actual PA that resides on the
server. Our camera films a movie by ""listening"" to the ProofWeb communication.
The first reason for developing movies is to uncouple the reviewing of a formal
proof from the PA used to develop it: the movie concept enables users to
discuss small code fragments without the need to install the PA or to load a
whole library into it. Other advantages include the possibility to develop a
separate com- mentary track to discuss or explain the PA interaction. We assert
that a combined camera+proviola provides a generic layer between a client
(user) and a server (PA). Finally we claim that movies are the right type of
data to be stored in an encyclopedia of formalized mathematics, based on our
experience in filming the Coq standard library.
"
238,"Good practices for a literature survey are not followed by authors while
  preparing scientific manuscripts","  The number of citations received by authors in scientific journals has become
a major parameter to assess individual researchers and the journals themselves
through the impact factor. A fair assessment therefore requires that the
criteria for selecting references in a given manuscript should be unbiased with
respect to the authors or the journals cited. In this paper, we advocate that
authors should follow two mandatory principles to select papers (later
reflected in the list of references) while studying the literature for a given
research: i) consider similarity of content with the topics investigated, lest
very related work should be reproduced or ignored; ii) perform a systematic
search over the network of citations including seminal or very related papers.
We use formalisms of complex networks for two datasets of papers from the arXiv
repository to show that neither of these two criteria is fulfilled in practice.
"
239,"An Ontology-based Context Aware System for Selective Dissemination of
  Information in a Digital Library","  Users of Institutional Repositories and Digital Libraries are known by their
needs for very specific information about one or more subjects. To characterize
users profiles and offer them new documents and resources is one of the main
challenges of today's libraries. In this paper, a Selective Dissemination of
Information service is described, which proposes an Ontology-based Context
Aware system for identifying user's context (research subjects, work team,
areas of interest). This system enables librarians to broaden users profiles
beyond the information that users have introduced by hand (such as institution,
age and language). The system requires a context retrieval layer to capture
user information and behavior, and an inference engine to support context
inference from many information sources (selected documents and users'
queries).
"
240,Dissertations Repository System Using Context Module,"  Without a doubt, the electronic learning makes education quite flexible.
Nowadays, all organizations and institutions are trying to avoid Monotony and
the delay and inertia. As well the universities should be improving their
systems continually to achieve success. Whereas, the students need to access
the dissertations in the library. In this paper we will present Dissertations
Repository System Using Context Module to allow the students to benefit the
dissertations which is in the library flexibly.
"
241,"A Wiki for Mizar: Motivation, Considerations, and Initial Prototype","  Formal mathematics has so far not taken full advantage of ideas from
collaborative tools such as wikis and distributed version control systems
(DVCS). We argue that the field could profit from such tools, serving both
newcomers and experts alike. We describe a preliminary system for such
collaborative development based on the Git DVCS. We focus, initially, on the
Mizar system and its library of formalized mathematics.
"
242,"The Source-Normalized Impact per Paper (SNIP) is a valid and
  sophisticated indicator of journal citation impact","  This paper is a reply to the article ""Scopus's Source Normalized Impact per
Paper (SNIP) versus a Journal Impact Factor based on Fractional Counting of
Citations"", published by Loet Leydesdorff and Tobias Opthof (arXiv:1004.3580v2
[cs.DL]). It clarifies the relationship between SNIP and Elsevier's Scopus.
Since Leydesdorff and Opthof's description of SNIP is not complete, it
indicates four key differences between SNIP and the indicator proposed by the
two authors, and argues why the former is more valid than the latter.
Nevertheless, the idea of fractional citation counting deserves further
exploration. The paper discusses difficulties that arise if one attempts to
apply this principle at the level of individual (citing) papers.
"
243,"Twenty Hirsch index variants and other indicators giving more or less
  preference to highly cited papers","  The Hirsch index or h-index is widely used to quantify the impact of an
individual's scientific research output, determining the highest number h of a
scientist's papers that received at least h citations. Several variants of the
index have been proposed in order to give more or less preference to highly
cited papers. I analyse the citation records of 26 physicists discussing
various suggestions, in particular A, e, f, g, h(2), h_w, h_T, \hbar, m, {\pi},
R, s, t, w, and maxprod. The total number of all and of all cited publications
as well as the highest and the average number of citations are also compared.
Advantages and disadvantages of these indices and indicators are discussed.
Correlation coefficients are determined quantifying which indices and
indicators yield similar and which yield more deviating rankings of the 26
datasets. For 6 datasets the determination of the indices and indicators is
visualized.
"
244,"Eugene Garfield and Algorithmic Historiography: Co-Words, Co-Authors,
  and Journal Names","  Algorithmic historiography was proposed by Eugene Garfield in collaboration
with Irving Sher in the 1960s, but further developed only recently into
HistCite^{TM} with Alexander Pudovkin. As in history writing, HistCite^{TM}
reconstructs by drawing intellectual lineages. In addition to cited references,
however, documents can be attributed a multitude of other variables such as
title words, keywords, journal names, author names, and even full texts. New
developments in multidimensional scaling (MDS) enable us not only to visualize
these patterns at each moment of time, but also to animate them over time.
Using title words, co-authors, and journal names in Garfield's oeuvre, the
method is demonstrated and further developed in this paper (and in the
animation at http://www.leydesdorff.net/garfield/animation). The variety and
substantive content of the animation enables us to write, visualize, and
animate the author's intellectual history.
"
245,HepData reloaded: reinventing the HEP data archive,"  We describe the status of the HepData database system, following a major
re-development in time for the advent of LHC data. The new HepData system
benefits from use of modern database and programming language technologies, as
well as a variety of high-quality tools for interfacing the data sources and
their presentation, primarily via the Web. The new back-end provides much more
flexible and semantic data representations than before, on which new external
applications can be built to respond to the data demands of the LHC
experimental era. The HepData re-development was largely motivated by a desire
to have a single source of reference data for Monte Carlo validation and tuning
tools, whose status and connection to HepData we also briefly review.
"
246,Astronomy 3.0 Style,"  Over the next decade we will witness the development of a new infrastructure
in support of data-intensive scientific research, which includes Astronomy.
This new networked environment will offer both challenges and opportunities to
our community and has the potential to transform the way data are described,
curated and preserved. Based on the lessons learned during the development and
management of the ADS, a case is made for adopting the emerging technologies
and practices of the Semantic Web to support the way Astronomy research will be
conducted. Examples of how small, incremental steps can, in the aggregate, make
a significant difference in the provision and repurposing of astronomical data
are provided.
"
247,A unified approach to mapping and clustering of bibliometric networks,"  In the analysis of bibliometric networks, researchers often use mapping and
clustering techniques in a combined fashion. Typically, however, mapping and
clustering techniques that are used together rely on very different ideas and
assumptions. We propose a unified approach to mapping and clustering of
bibliometric networks. We show that the VOS mapping technique and a weighted
and parameterized variant of modularity-based clustering can both be derived
from the same underlying principle. We illustrate our proposed approach by
producing a combined mapping and clustering of the most frequently cited
publications that appeared in the field of information science in the period
1999-2008.
"
248,Bibliometric Evaluation of the Changing Finnish Astronomy,"  This is a follow-up on the bibliometric evaluation of Finnish astronomy
presented by the author at the LISA V conference in 2006. The data from the
previous study are revisited to determine how a wider institutional base and
mergers affect comparisons between research units.
"
249,Communities and Patterns of Scientific collaboration,"  This paper investigates the role of homophily and focus constraint in shaping
collaborative scientific research. First, homophily structures collaboration
when scientists adhere to a norm of exclusivity in selecting similar partners
at a higher rate than dissimilar ones. Two dimensions on which similarity
between scientists can be assessed are their research specialties and status
positions. Second, focus constraint shapes collaboration when connections among
scientists depend on opportunities for social contact. Constraint comes in two
forms, depending on whether it originates in institutional or geographic space.
Institutional constraint refers to the tendency of scientists to select
collaborators within rather than across institutional boundaries. Geographic
constraint is the principle that, when collaborations span different
institutions, they are more likely to involve scientists that are
geographically co-located than dispersed. To study homophily and focus
constraint, the paper will argue in favour of an idea of collaboration that
moves beyond formal co-authorship to include also other forms of informal
intellectual exchange that do not translate into the publication of joint work.
A community-detection algorithm is applied to the co-authorship network of the
scientists that submitted in Business and Management in the 2001 UK RAE. While
results only partially support research-based homophily, they indicate that
scientists use status positions for discriminating between potential partners
by selecting collaborators from institutions with a rating similar to their
own. Strong support is provided in favour of institutional and geographic
constraints. Scientists tend to forge intra-institutional collaborations; yet,
when they seek collaborators outside their own institutions, they tend to
select those who are in geographic proximity.
"
250,"Rozw\'oj bibliotek cyfrowych i repozytori\'ow elektronicznych na Dolnym
  Slasku w latach 2004-2008 / Development of Digital Libraries and Electronic
  Repositories in Lower Silesia in Years 2004-2008","  In following elaboration were presented digital libraries and electronic
repositories operating in Lower Silesia region (of Poland) in years 2004-2008.
General description of character and size of their collections was presented,
as well as standards and methods of digital collections management and
juridical aspects of this management. Potential of usage of digital collections
in regional scientific researches was described.
  -----
  W referacie przedstawiono biblioteki cyfrowe i repozytoria elektroniczne
funkcjonujace na Dolnym Slasku w latach 2004-2008. Scharakteryzowano og\'olnie
ich zawarto\'s\'c i wielko\'s\'c, zaprezentowano standardy i systemy
zarzadzania kolekcjami cyfrowymi oraz om\'owiono uwarunkowania prawne
towarzyszace zarzadzaniu zasobami cyfrowymi. Wskazano mo\.zliwo\'sci
wykorzystania kolekcji cyfrowych w badaniach naukowych realizowanych w
regionie.
"
251,From RESTful Services to RDF: Connecting the Web and the Semantic Web,"  RESTful services on the Web expose information through retrievable resource
representations that represent self-describing descriptions of resources, and
through the way how these resources are interlinked through the hyperlinks that
can be found in those representations. This basic design of RESTful services
means that for extracting the most useful information from a service, it is
necessary to understand a service's representations, which means both the
semantics in terms of describing a resource, and also its semantics in terms of
describing its linkage with other resources. Based on the Resource Linking
Language (ReLL), this paper describes a framework for how RESTful services can
be described, and how these descriptions can then be used to harvest
information from these services. Building on this framework, a layered model of
RESTful service semantics allows to represent a service's information in
RDF/OWL. Because REST is based on the linkage between resources, the same model
can be used for aggregating and interlinking multiple services for extracting
RDF data from sets of RESTful services.
"
252,Scopus' SNIP Indicator,"  Rejoinder to Moed [arXiv:1005.4906]: Our main objection is against developing
new indicators which, like some of the older ones (for example, the ""crown
indicator"" of CWTS), do not allow for indicating error because they do not
provide a statistics, but are based, in our opinion, on a violation of the
order of operations. The claim of validity for the SNIP indicator is hollow
because the normalizations are based on field classifications which are not
valid. Both problems can perhaps be solved by using fractional counting.
"
253,Normalization at the field level: fractional counting of citations,"  Van Raan et al. (2010; arXiv:1003.2113) have proposed a new indicator (MNCS)
for field normalization. Since field normalization is also used in the Leiden
Rankings of universities, we elaborate our critique of journal normalization in
Opthof & Leydesdorff (2010; arXiv:1002.2769) in this rejoinder concerning field
normalization. Fractional citation counting thoroughly solves the issue of
normalization for differences in citation behavior among fields. This indicator
can also be used to obtain a normalized impact factor.
"
254,"Concentration versus dispersion of research resources: a contribution to
  the debate","  Using the results of the UK's research assessment exercise, we show that the
size or mass of research groups, rather than individual caliber or prestige of
the institution, is the dominant factor which drives the quality of research
teams. There are two critical masses in research: a lower one, below which
teams are vulnerable and an upper one, above which average dependency of
research quality on team size reduces. This leveling off refutes arguments
which advocate ever increasing concentration of research support into a few
large institutions. We also show that to increase research quality, policies
which nourish two-way communication links between researchers are paramount.
"
255,"Normalization of peer-evaluation measures of group research quality
  across academic disciplines","  Peer-evaluation based measures of group research quality such as the UK's
Research Assessment Exercise (RAE), which do not employ bibliometric analyses,
cannot directly avail of such methods to normalize research impact across
disciplines. This is seen as a conspicuous flaw of such exercises and calls
have been made to find a remedy. Here a simple, systematic solution is proposed
based upon a mathematical model for the relationship between research quality
and group quantity. This model manifests both the Matthew effect and a
phenomenon akin to the Ringelmann effect and reveals the existence of two
critical masses for each academic discipline: a lower value, below which groups
are vulnerable, and an upper value beyond which the dependency of quality on
quantity reduces and plateaus appear when the critical masses are large. A
possible normalization procedure is then to pitch these plateaus at similar
levels. We examine the consequences of this procedure at RAE for a multitude of
academic disciplines, corresponding to a range of critical masses.
"
256,Towards OpenMath Content Dictionaries as Linked Data,"  ""The term 'Linked Data' refers to a set of best practices for publishing and
connecting structured data on the web"". Linked Data make the Semantic Web work
practically, which means that information can be retrieved without complicated
lookup mechanisms, that a lightweight semantics enables scalable reasoning, and
that the decentral nature of the Web is respected. OpenMath Content
Dictionaries (CDs) have the same characteristics - in principle, but not yet in
practice. The Linking Open Data movement has made a considerable practical
impact: Governments, broadcasting stations, scientific publishers, and many
more actors are already contributing to the ""Web of Data"". Queries can be
answered in a distributed way, and services aggregating data from different
sources are replacing hard-coded mashups. However, these services are currently
entirely lacking mathematical functionality. I will discuss real-world
scenarios, where today's RDF-based Linked Data do not quite get their job done,
but where an integration of OpenMath would help - were it not for certain
conceptual and practical restrictions. I will point out conceptual shortcomings
in the OpenMath 2 specification and common bad practices in publishing CDs and
then propose concrete steps to overcome them and to contribute OpenMath CDs to
the Web of Data.
"
257,New Quantitative Study for Dissertations Repository System,"  In the age of technology, the information communication technology becomes
very important especially in education field. Students must be allowed to learn
anytime, anywhere and at their own place. The facility of library in the
university should be developed. In this paper we are going to present new
Quantitative Study for Dissertations Repository System and also recommend
future application of the approach.
"
258,"Limits of responsiveness concerning human-readable knowledge bases: an
  operational analysis","  Introduction. The purpose of this work is the evaluation of responsiveness
when remote users communicate with a human-readable knowledge base (KB).
Responsiveness [R(s)] is considered here as a measure of service quality.
Method. The preferred method is operational analysis, a variation of classical
stochastic theory, which allows for the study of user-system interaction with
minimal computational effort. Analysis. The analysis is based on well-known
performance metrics, such as service ability, elapsed time, and throughput:
from these metrics estimates of R(s) are derived analytically. Results.
Critical points indicating congestion are obtained: these are limits on the
number of admissible requests and the number of connected users. Also obtained
is a sufficient condition for achieving flow balance between the KB host and
the request-relaying servers. Conclusions. When R(s) is within normal limits,
users should appreciate the benefits from using the services offered by their
KB host. When bottlenecks are formed, R(s) declines, and the whole
communication system heads for saturation. Flow balancing procedures are
necessary for the elimination of bottlenecks, which leads to a better resource
management.
"
259,"Tailor Based Allocations for Multiple Authorship: a fractional
  $gh$-index","  A quantitative modification to keep the number of published papers invariant
under multiple authorship is suggested. In those cases, fractional allocations
are attributed to each co-author with a summation equal to one. These
allocations are tailored on the basis of each author contribution. It is
denoted ""Tailor Based Allocations (TBA)"" for multiple authorship. Several
protocols to TBA are suggested. The choice of a specific TBA may vary from one
discipline to another. In addition, TBA is applied to the number of citations
of a multiple author paper to have also this number conserved. Each author gets
only a specific fraction of the total number of citations according to its
fractional paper allocation. The equivalent of the h-index obtained by using
TBA is denoted the gh-index. It yields values which differ drastically from
those given by the h-index. The gh-index departs also from the `h-index
recently proposed by Hirsh to account for multiple authorship. Contrary to the
h-index, the gh-index is a function of the total number of citations of each
paper. A highly cited paper allows a better allocation for all co-authors while
a less cited paper contributes essentially to one or two of the co-authors. The
scheme produces a substantial redistribution of the ranking of scientists in
terms of quantitative records. A few illustrations are provided.
"
260,The Use of Scientific Data: A Content Analysis,"  Nowadays, science has been coming into a new paradigm, called data-intensive
science. While current studies of the new phenomenon focused on building up
infrastructure for this new paradigm, yet a few studies concern users of
scientific data, particularly their usage practices in the newly emerging
paradigm, even though the importance of understanding users' work flow and
practices has been summoned. This study endeavors to improve our understanding
of users' data usage behavior through a content analysis of publications in a
frequently cited new paradigm-related project, Sloan Digital Sky Survey (SDSS).
We found that (1) nearly half studies used one data source only. A few studies
exploited three or more data sources; (2) the number of objects that were
analyzed in SDSS publications is in all scales from one digit to millions; (3)
different paper types may affect the data usage patterns; (4) Users are not
only consumers of scientific data. They are producers too; (5) studies that can
use multiple large scale data sources are relative rare. Issues of data
provenance, trust, and usability may prevent researchers from doing this kind
of research.
"
261,"How fractional counting affects the Impact Factor: Normalization in
  terms of differences in citation potentials among fields of science","  The ISI-Impact Factors suffer from a number of drawbacks, among them the
statistics-why should one use the mean and not the median?-and the
incomparability among fields of science because of systematic differences in
citation behavior among fields. Can these drawbacks be counteracted by counting
citation weights fractionally instead of using whole numbers in the numerators?
(i) Fractional citation counts are normalized in terms of the citing sources
and thus would take into account differences in citation behavior among fields
of science. (ii) Differences in the resulting distributions can be tested
statistically for their significance at different levels of aggregation. (iii)
Fractional counting can be generalized to any document set including journals
or groups of journals, and thus the significance of differences among both
small and large sets can be tested. A list of fractionally counted Impact
Factors for 2008 is available online at
http://www.leydesdorff.net/weighted_if/weighted_if.xls. The in-between group
variance among the thirteen fields of science identified in the U.S. Science
and Engineering Indicators is not statistically significant after this
normalization. Although citation behavior differs largely between disciplines,
the reflection of these differences in fractionally counted citation
distributions could not be used as a reliable instrument for the
classification.
"
262,The Emerging Scholarly Brain,"  It is now a commonplace observation that human society is becoming a coherent
super-organism, and that the information infrastructure forms its emerging
brain. Perhaps, as the underlying technologies are likely to become billions of
times more powerful than those we have today, we could say that we are now
building the lizard brain for the future organism.
"
263,Collaborative Structuring of Knowledge by Experts and the Public,"  There is much debate on how public participation and expertise can be brought
together in collaborative knowledge environments. One of the experiments
addressing the issue directly is Citizendium. In seeking to harvest the
strengths (and avoiding the major pitfalls) of both user-generated wiki
projects and traditional expert-approved reference works, it is a wiki to which
anybody can contribute using their real names, while those with specific
expertise are given a special role in assessing the quality of content. Upon
fulfillment of a set of criteria like factual and linguistic accuracy, lack of
bias, and readability by non-specialists, these entries are forked into two
versions: a stable (and thus citable) approved ""cluster"" (an article with
subpages providing supplementary information) and a draft version, the latter
to allow for further development and updates. We provide an overview of how
Citizendium is structured and what it offers to the open knowledge communities,
particularly to those engaged in education and research. Special attention will
be paid to the structures and processes put in place to provide for transparent
governance, to encourage collaboration, to resolve disputes in a civil manner
and by taking into account expert opinions, and to facilitate navigation of the
site and contextualization of its contents.
"
264,"Nonuniversal power law scaling in the probability distribution of
  scientific citations","  We develop a model for the distribution of scientific citations. The model
involves a dual mechanism: in the direct mechanism, the author of a new paper
finds an old paper A and cites it. In the indirect mechanism, the author of a
new paper finds an old paper A only via the reference list of a newer
intermediary paper B, which has previously cited A. By comparison to citation
databases, we find that papers having few citations are cited mainly by the
direct mechanism. Papers already having many citations ('classics') are cited
mainly by the indirect mechanism. The indirect mechanism gives a power-law
tail. The 'tipping point' at which a paper becomes a classic is about 21
citations for papers published in the Institute for Scientific Information
(ISI) Web of Science database in 1981, 29 for Physical Review D papers
published from 1975-1994, and 39 for all publications from a list of high
h-index chemists assembled in 2007. The power-law exponent is not universal.
Individuals who are highly cited have a systematically smaller exponent than
individuals who are less cited.
"
265,The Development of the Journal Environment of Leonardo,"  We present animations based on the aggregated journal-journal citations of
Leonardo during the period 1974-2008. Leonardo is mainly cited by journals
outside the arts domain for cultural reasons, for example, in neuropsychology
and physics. Articles in Leonardo itself cite a large number of journals, but
with a focus on the arts. Animations at this level of aggregation enable us to
show the history of the journal from a network perspective.
"
266,An evaluation of the Australian Research Council's journal ranking,"  As part of its program of 'Excellence in Research for Australia' (ERA), the
Australian Research Council ranked journals into four categories (A*, A, B, C)
in preparation for their performance evaluation of Australian universities. The
ranking is important because it likely to have a major impact on publication
choices and research dissemination in Australia. The ranking is problematic
because it is evident that some disciplines have been treated very differently
than others. This paper reveals weaknesses in the ERA journal ranking and
highlights the poor correlation between ERA rankings and other acknowledged
metrics of journal standing. It highlights the need for a reasonable
representation of journals ranked as A* in each scientific discipline.
"
267,"Data Preservation in High Energy Physics - why, how and when?","  Long-term preservation of data and software of large experiments and
detectors in high energy physics is of utmost importance to secure the heritage
of (mostly unique) data and to allow advanced physics (re-)analyses at later
times. Summarising the work of an international study group, motivation, use
cases and technical details are given for an organised effort to secure and
enable future use of past, present and future experimental data. As a practical
use case and motivation, the revival of JADE data and the corresponding latest
results on measuring $\alpha_s$ in NNLO QCD are reviewed.
"
268,Demonstrating a Service-Enhanced Retrieval System,"  This paper is a short description of an information retrieval system enhanced
by three model driven retrieval services: (1) co-word analysis based query
expansion, re-ranking via (2) Bradfordizing and (3) author centrality. The
different services each favor quite other - but still relevant - documents than
pure term-frequency based rankings. Each service can be interactively combined
with each other to allow an iterative retrieval refinement.
"
269,"A Framework for an Ego-centered and Time-aware Visualization of
  Relations in Arbitrary Data Repositories","  Understanding constellations in large data collections has become a common
task. One obstacle a user has to overcome is the internal complexity of these
repositories. For example, extracting connected data from a normalized
relational database requires knowledge of the table structure which might not
be available for the casual user. In this paper we present a visualization
framework which presents the collection as a set of entities and relations (on
the data level). Using rating functions, we divide large relation networks into
small graphs which resemble ego-centered networks. These graphs are connected
so the user can browse from one to another. To further assist the user, we
present two views which embed information on the evolution of the relations
into the graphs. Each view emphasizes another aspect of temporal development.
The framework can be adapted to any repository by a flexible data interface and
a graph configuration file. We present some first web-based applications
including a visualization of the DBLP data set. We use the DBLP visualization
to evaluate our approach.
"
270,A Simple Abstraction for Data Modeling,"  The problems that scientists face in creating well designed databases
intersect with the concerns of data curation. Entity-relationship modeling and
its variants have been the basis of most relational data modeling for decades.
However, these abstractions and the relational model itself are intricate and
have proved not to be very accessible among scientists with limited resources
for data management. This paper explores one aspect of relational data models,
the meaning of foreign key relationships. We observe that a foreign key
produces a table relationship that generally references either an entity or
repeating attributes. This paper proposes constructing foreign keys based on
these two cases, and suggests that the method promotes intuitive data modeling
and normalization.
"
271,"Establishing a Multi-Thesauri-Scenario based on SKOS and
  Cross-Concordances","  This case study proposes a scenario with three topic-related thesauri, which
have been connected with bilateral cross-concordances as part of a major
terminology mapping initiative in the project KoMoHe (Mayr & Petras, 2008). The
thesauri have already been or will be converted to SKOS and in order to not
omit the relevant crosswalks, the mapping properties of SKOS will be used for
modeling them adequately.
"
272,Nefarious Numbers,"  We investigate the journal impact factor, focusing on the applied mathematics
category. We discuss impact factor manipulation and demonstrate that the impact
factor gives an inaccurate view of journal quality, which is poorly correlated
with expert opinion.
"
273,First results of the SOAP project. Open access publishing in 2010,"  The SOAP (Study of Open Access Publishing) project has compiled data on the
present offer for open access publishing in online peer-reviewed journals.
Starting from the Directory of Open Access Journals, several sources of data
are considered, including inspection of journal web site and direct inquiries
within the publishing industry. Several results are derived and discussed,
together with their correlations: the number of open access journals and
articles; their subject area; the starting date of open access journals; the
size and business models of open access publishers; the licensing models; the
presence of an impact factor; the uptake of hybrid open access.
"
274,"Weighted Indices for Evaluating the Quality of Research with Multiple
  Authorship","  Devising an index to measure the quality of research is a challenging task.
In this paper, we propose a set of indices to evaluate the quality of research
produced by an author. Our indices utilize a policy that assigns the weights to
multiple authors of a paper. We have considered two weight assignment policies:
positionally weighted and equally weighted. We propose two classes of weighted
indices: weighted h-indices and weighted citation h-cuts. Further, we compare
our weighted h-indices with the original h-index for a selected set of authors.
As opposed to h-index, our weighted h-indices take into account the weighted
contributions of individual authors in multi-authored papers, and may serve as
an improvement over h-index. The other class of weighted indices that we call
weighted citation h-cuts take into account the number of citations that are in
excess of those required to compute the index, and may serve as a supplement to
h-index or its variants.
"
275,"Using Context Dependent Semantic Similarity to Browse Information
  Resources: an Application for the Industrial Design","  This paper deals with the semantic interpretation of information resources
(e.g., images, videos, 3D models). We present a case study of an approach based
on semantic and context dependent similarity applied to the industrial design.
Different application contexts are considered and modelled to browse a
repository of 3D digital objects according to different perspectives. The paper
briefly summarises the basic concepts behind the semantic similarity approach
and illustrates its application and results.
"
276,"Remaining problems with the ""New Crown Indicator"" (MNCS) of the CWTS","  In their article, entitled ""Towards a new crown indicator: some theoretical
considerations,"" Waltman et al. (2010; at arXiv:1003.2167) show that the ""old
crown indicator"" of CWTS in Leiden was mathematically inconsistent and that one
should move to the normalization as applied in the ""new crown indicator.""
Although we now agree about the statistical normalization, the ""new crown
indicator"" inherits the scientometric problems of the ""old"" one in treating
subject categories of journals as a standard for normalizing differences in
citation behavior among fields of science.
  We further note that the ""mean"" is not a proper statistics for measuring
differences among skewed distributions. Without changing the acronym of ""MNCS,""
one could define the ""Median Normalized Citation Score."" This would relate the
new crown indicator directly to the percentile approach that is, for example,
used in the Science and Engineering Indicators of US National Science Board
(2010). The median is by definition equal to the 50th percentile. The indicator
can thus easily be extended with the 1% (= 99th percentile) most highly-cited
papers (Bornmann et al., in press). The seeming disadvantage of having to use
non-parametric statistics is more than compensated by possible gains in the
precision.
"
277,Enabling Data Discovery through Virtual Internet Repositories,"  Mercury is a federated metadata harvesting, search and retrieval tool based
on both open source and software developed at Oak Ridge National Laboratory. It
was originally developed for NASA, and the Mercury development consortium now
includes funding from NASA, USGS, and DOE. A major new version of Mercury was
developed during 2007. This new version provides orders of magnitude
improvements in search speed, support for additional metadata formats,
integration with Google Maps for spatial queries, support for RSS delivery of
search results, among other features. Mercury provides a single portal to
information contained in disparate data management systems. It collects
metadata and key data from contributing project servers distributed around the
world and builds a centralized index. The Mercury search interfaces then allow
the users to perform simple, fielded, spatial and temporal searches across
these metadata sources. This centralized repository of metadata with
distributed data sources provides extremely fast search results to the user,
while allowing data providers to advertise the availability of their data and
maintain complete control and ownership of that data.
"
278,"How to evaluate universities in terms of their relative citation
  impacts: Fractional counting of citations and the normalization of
  differences among disciplines","  Fractional counting of citations can improve on ranking of multi-disciplinary
research units (such as universities) by normalizing the differences among
fields of science in terms of differences in citation behavior. Furthermore,
normalization in terms of citing papers abolishes the unsolved questions in
scientometrics about the delineation of fields of science in terms of journals
and normalization when comparing among different journals. Using publication
and citation data of seven Korean research universities, we demonstrate the
advantages and the differences in the rankings, explain the possible
statistics, and suggest ways to visualize the differences in (citing) audiences
in terms of a network.
"
279,"Last but not Least: Additional Positional Effects on Citation and
  Readership in arXiv","  We continue investigation of the effect of position in announcements of newly
received articles, a single day artifact, with citations received over the
course of ensuing years. Earlier work [arXiv:0907.4740, arXiv:0805.0307]
focused on the ""visibility"" effect for positions near the beginnings of
announcements, and on the ""self-promotion"" effect associated to authors
intentionally aiming for these positions, with both found correlated to a later
enhanced citation rate. Here we consider a ""reverse-visibility"" effect for
positions near the ends of announcements, and on a ""procrastination"" effect
associated to submissions made within the 20 minute period just before the
daily deadline. For two large subcommunities of theoretical high energy
physics, we find a clear ""reverse-visibility"" effect, in which articles near
the ends of the lists receive a boost in both short-term readership and
long-term citations, almost comparable in size to the ""visibility"" effect
documented earlier. For one of those subcommunities, we find an additional
""procrastination"" effect, in which last position articles submitted shortly
before the deadline have an even higher citation rate than those that land more
accidentally in that position. We consider and eliminate geographic effects as
responsible for the above, and speculate on other possible causes, including
""oblivious"" and ""nightowl"" effects.
"
280,Tracing scientific influence,"  Scientometrics is the field of quantitative studies of scholarly activity. It
has been used for systematic studies of the fundamentals of scholarly practice
as well as for evaluation purposes. Although advocated from the very beginning
the use of scientometrics as an additional method for science history is still
under explored. In this paper we show how a scientometric analysis can be used
to shed light on the reception history of certain outstanding scholars. As a
case, we look into citation patterns of a specific paper by the American
sociologist Robert K. Merton.
"
281,Collaboration in computer science: a network science approach. Part I,"  Co-authorship in publications within a discipline uncovers interesting
properties of the analysed field. We represent collaboration in academic papers
of computer science in terms of differently grained networks, including those
sub-networks that emerge from conference and journal co-authorship only. We
take advantage of the network science paraphernalia to take a picture of
computer science collaboration including all papers published in the field
since 1936. We investigate typical bibliometric properties like scientific
productivity of authors and collaboration level in papers, as well as
large-scale network properties like reachability and average separation
distance among scholars, distribution of the number of scholar collaborators,
network resilience and dependence on star collaborators, network clustering,
and network assortativity by number of collaborators.
"
282,"Editing Knowledge in Large Mathematical Corpora. A case study with
  Semantic LaTeX (sTeX)","  Before we can get the whole potential of employing computers in the process
of managing mathematical `knowledge', we have to convert informal knowledge
into machine-oriented representations. How exactly to support this process so
that it becomes as effortless as possible is one of the main unsolved problems
of Mathematical Knowledge Management.
  Two independent projects in formalization of mathematical content showed that
many of the time consuming tasks could be significantly reduced if adequate
tool support were available. It was also established that similar tasks are
typical for object oriented languages and that they are to a large extent
solved by Integrated Development Environments (IDE).
  This thesis starts by analyzing the opportunities where formalization process
can benefit from software support. A list of research questions is compiled
along with a set of software requirements which are then used for developing a
new IDE for the semantic \TeX{} (\stex{}) format. The result of the current
research is that, indeed, IDEs can be very useful in the process of
formalization and presents a set of best practices for implementing such IDEs.
"
283,"Analysis of temporal characteristics of the editorial processing in
  scientific periodicals","  The first part of our work is connected with the analysis of typical random
variables for the specific human-initiated process. We study the data
characterizing editorial work with received manuscripts in several scientific
journals. In such a way we found the waiting time distributions that could be
called the typical for an ordinary peer-review scientific journal. In the
second part of this study a model of editorial processing of received
manuscripts is developed. Within the model, different scenarios of the
manuscript editorial processing are examined. Combining the results of the
quantitative experiment and model simulations we arrive to the set of
conclusions about time characteristics of editorial process in scientific
journals and a peer-review contribution.
"
284,"Make Research Data Public? -- Not Always so Simple: A Dialogue for
  Statisticians and Science Editors","  Putting data into the public domain is not the same thing as making those
data accessible for intelligent analysis. A distinguished group of editors and
experts who were already engaged in one way or another with the issues inherent
in making research data public came together with statisticians to initiate a
dialogue about policies and practicalities of requiring published research to
be accompanied by publication of the research data. This dialogue carried
beyond the broad issues of the advisability, the intellectual integrity, the
scientific exigencies to the relevance of these issues to statistics as a
discipline and the relevance of statistics, from inference to modeling to data
exploration, to science and social science policies on these issues.
"
285,"Power-law Distributions in Information Science - Making the Case for
  Logarithmic Binning","  We suggest partial logarithmic binning as the method of choice for uncovering
the nature of many distributions encountered in information science (IS).
Logarithmic binning retrieves information and trends ""not visible"" in noisy
power-law tails. We also argue that obtaining the exponent from logarithmically
binned data using a simple least square method is in some cases warranted in
addition to methods such as the maximum likelihood. We also show why often used
cumulative distributions can make it difficult to distinguish noise from
genuine features, and make it difficult to obtain an accurate power-law
exponent of the underlying distribution. The treatment is non-technical, aimed
at IS researchers with little or no background in mathematics.
"
286,Can Knowledge be preserved in the long run?,"  Can (scientific) knowledge be reliably preserved over the long term? We have
today very efficient and reliable methods to encode, store and retrieve data in
a storage medium that is fault tolerant against many types of failures. But
does this guarantee -- or does it even seem likely -- that all knowledge can be
preserved over thousands of years and beyond? History shows that many types of
knowledge that were known before have been lost. We observe that the nature of
stored and communicated information and the way it is interpreted is such that
it always tends to decay and therefore must lost eventually in the long term.
The likely fundamental conclusion is that knowledge cannot be reliably
preserved indefinitely.
"
287,"The Local Emergence and Global Diffusion of Research Technologies: An
  Exploration of Patterns of Network Formation","  Grasping the fruits of ""emerging technologies"" is an objective of many
government priority programs in a knowledge-based and globalizing economy. We
use the publication records (in the Science Citation Index) of two emerging
technologies to study the mechanisms of diffusion in the case of two innovation
trajectories: small interference RNA (siRNA) and nano-crystalline solar cells
(NCSC). Methods for analyzing and visualizing geographical and cognitive
diffusion are specified as indicators of different dynamics. Geographical
diffusion is illustrated with overlays to Google Maps; cognitive diffusion is
mapped using an overlay to a map based on the ISI Subject Categories. The
evolving geographical networks show both preferential attachment and
small-world characteristics. The strength of preferential attachment decreases
over time, while the network evolves into an oligopolistic control structure
with small-world characteristics. The transition from disciplinary-oriented
(""mode-1"") to transfer-oriented (""mode-2"") research is suggested as the crucial
difference in explaining the different rates of diffusion between siRNA and
NCSC.
"
288,How to Create an Innovation Accelerator,"  Too many policy failures are fundamentally failures of knowledge. This has
become particularly apparent during the recent financial and economic crisis,
which is questioning the validity of mainstream scholarly paradigms. We propose
to pursue a multi-disciplinary approach and to establish new institutional
settings which remove or reduce obstacles impeding efficient knowledge
creation. We provided suggestions on (i) how to modernize and improve the
academic publication system, and (ii) how to support scientific coordination,
communication, and co-creation in large-scale multi-disciplinary projects. Both
constitute important elements of what we envision to be a novel ICT
infrastructure called ""Innovation Accelerator"" or ""Knowledge Accelerator"".
"
289,Analysis of Generalized Impact Factor and Indices of Journals,"  Analyzing the relationships among the parameters for quantifying the quality
of research published in journals is a challenging task. In this paper, we
analyze the relationships between impact factor, h-index, and g-index of a
journal. To keep our analysis simple and easy to understand, we consider a
generalized version of the impact factor where there is no time window. In the
absence of the time window, the impact factor converges to the number of
citations received per paper. This is not only justified for the impact factor,
it simplifies the analysis of h-index and g-index as well because addition of a
time window in the form of years complicates the computation of indices too. We
derive the expressions for the relationships among impact factor, h-index, and
g-index and validate them using a given set of publication-citation data.
"
290,Citations and impact of Dutch astronomy,"  The aim of this study is to make a bibliometric comparison of the performance
of research astronomers in the Netherlands Research School for Astronomy (NOVA)
with astronomers elsewhere by using the NASA Astrophysics Data System (ADS). We
use various indices for bibliometric performance for a sample of NOVA
astronomers to compare to samples of astronomers worldwide, and from the United
States. We give much weight to normalising bibliometric measures by number of
authors, and number of years since first publication. In particular we
calculate the `Hirsh-index' normalized to number of authors and for
first-author papers. Secondly, we consider the results of the 'Nederlands
Observatorium van Wetenschap en Technologie' (NOWT; Netherlands Observatory of
Science and Technology), which regularly publishes a report 'Science and
Technology Indicators'. We reproduce those results using publication lists from
institutions in the Netherlands, again using ADS, and examine and discuss the
conclusions and indications in these reports. We find that the NOVA researchers
perform much better in bibliometric measures than samples drawn from IAU or AAS
membership lists. A more suitable comparison is one with the (tenured) staff of
the top-15 US institutions and there the NOVA staff performs in these respects
as good or almost as good as that of American top institutes. From a citation
analysis through the use of ADS we conclude that the impact ratio of Dutch
astronomical publications is rising which is opposite to what is reported by
NOWT. This difference is most likely caused by a better separation of astronomy
and physics in ADS than in World of Knowledge. ADS probably finds more
citations in conference proceedings, while the inclusion of citations to
articles with their pre-print identifier could also help explain the difference
(especially since the citation windows in the reports are short).
"
291,"A Joint Initiative to Support the Semantic Interoperability within the
  GIIDA Project","  The GIIDA project aims to develop a digital infrastructure for the spatial
information within CNR. It is foreseen to use semantic-oriented technologies to
ease information modeling and connecting, according to international standards
like the ISO/IEC 11179. Complex information management systems, like GIIDA,
will take benefit from the use of terminological tools like thesauri that make
available a reference lexicon for the indexing and retrieval of information.
Within GIIDA the goal is to make available the EARTh thesaurus (Environmental
Applications Reference Thesaurus), developed by the CNR-IIA-EKOLab. A web-based
software, developed by the CNR-Water Research Institute (IRSA) was implemented
to allow consultation and utilization of thesaurus through the web. This
service is a useful tool to ensure interoperability between thesaurus and other
systems of the indexing, with, the idea of cooperating to develop a
comprehensive system of knowledge organization, that could be defined
integrated, open, multi-functional and multilingual. Currently the system is
available in multiple languages mode (Italian - English) and navigation can be
done in the following ways: Alphabetical, Hierarchical and for Themes. A full
search allows to find any term by searching for the whole term or a part of it
and as well as allows to filter the results by themes. Within a collaborative
initiative with the CNR-Institute of Applied Mathematics and Information
Technology (IMATI) a SKOS (Simple Knowledge Organization System) version of
EARTh was developed. This will ensure the possibility to support the use of the
thesaurus within the framework of the Semantic Web in order to be used in
decentralized metadata applications
"
292,"Fractional counting of citations in research evaluation: An option for
  cross- and interdisciplinary assessments","  In the case of the scientometric evaluation of multi- or interdisciplinary
units one risks to compare apples with oranges: each paper has to assessed in
comparison to an appropriate reference set. We suggest that the set of citing
papers first can be considered as the relevant representation of the field of
impact. In order to normalize for differences in citation behavior among
fields, citations can be fractionally counted proportionately to the length of
the reference lists in the citing papers. This new method enables us to compare
among units with different disciplinary affiliations at the paper level and
also to assess the statistical significance of differences among sets.
Twenty-seven departments of the Tsinghua University in Beijing are thus
compared. Among them, the Department of Chinese Language and Linguistics is
upgraded from the 19th to the second position in the ranking. The overall
impact of 19 of the 27 departments is not significantly different at the 5%
level when thus normalized for different citation potentials.
"
293,"Ontology and Knowledge Management System on Epilepsy and Epileptic
  Seizures","  A Knowledge Management System developed for supporting creation, capture,
storage and dissemination of information about Epilepsy and Epileptic Seizures
is presented. We present an Ontology on Epilepsy and a Web-based prototype that
together create the KMS.
"
294,The CALBC RDF Triple Store: retrieval over large literature content,"  Integration of the scientific literature into a biomedical research
infrastructure requires the processing of the literature, identification of the
contained named entities (NEs) and concepts, and to represent the content in a
standardised way. The CALBC project partners (PPs) have produced a large-scale
annotated biomedical corpus with four different semantic groups through the
harmonisation of annotations from automatic text mining solutions (Silver
Standard Corpus, SSC). The four semantic groups were chemical entities and
drugs (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and
species (SPE). The content of the SSC has been fully integrated into RDF Triple
Store (4,568,678 triples) and has been aligned with content from the GeneAtlas
(182,840 triples), UniProtKb (12,552,239 triples for human) and the lexical
resource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific
literature and bioinformatics resources at the same time for evidence of
genetic causes, such as drug targets and disease involvement.
"
295,Import of ENZYME data into the ConceptWiki and its representation as RDF,"  Solutions to the classic problems of dealing with heterogeneous data and
making entire collections interoperable while ensuring that any annotation,
which includes the recognition-and-reward system of scientific publishing, need
to fit into a seamless beginning to end to attract large numbers of end users.
The latest trend in Web applications encourages highly interactive Web sites
with rich user interfaces featuring content integrated from various sources
around the Web. The obvious potential of RDF, SPARQL, and OWL to provide
flexible data modeling, easier data integration, and networked data access may
be the answer to the classic problems. Using Semantic Web technologies we have
created a Web application, the ConceptWiki, as an end-to-end solution for
creating browserbased readwrite triples using RDF, which focus on data
integration and ease of use for the end user. Here we will demonstrate the
integration of a biological data source, the ENZYME database, into the
ConceptWiki and it's representation in RDF.
"
296,"The Butterfly Effect: Correlations Between Modeling in Nuclear-Particle
  Physics and Socioeconomic Factors","  A scientometric analysis has been performed on selected physics journals to
estimate the presence of simulation and modeling in physics literature in the
past fifty years. Correlations between the observed trends and several social
and economical factors have been evaluated.
"
297,"What the Cited and Citing Environments Reveal of ""Advances in
  Atmospheric Sciences""?","  The networking ability of journals reflects their academic influence among
peer journals. This paper analyzes the cited and citing environments of the
journal--Advances in Atmospheric Sciences--using methods from social network
analysis. The journal has been actively participating in the international
journal environment, but one has a tendency to cite papers published in
international journals. Advances in Atmospheric Sciences is intensely
interrelated with international peer journals in terms of similar citing
pattern. However, there is still room for an increase in its academic
visibility given the comparatively smaller reception in terms of cited
references.
"
298,"Comparative Analysis of Existing Methods and Algorithms for Automatic
  Assignment of Reviewers to Papers","  The article focuses on the importance of the automatic assignment of
reviewers to papers for increasing the assignment accuracy therefore the
quality of the scientific event itself. It discusses the main aspects that
influence the assignment accuracy, performs a detailed analysis of the methods
of describing papers and reviewers' competences used by the existing conference
management systems and suggests some improvements in the way the similarity
factors are calculated.
"
299,Selection in Scientific Networks,"  One of the most interesting scientific challenges nowadays deals with the
analysis and the understanding of complex networks' dynamics. A major issue is
the definition of new frameworks for the exploration of the dynamics at play in
real dynamic networks. Here, we focus on scientific communities by analyzing
the ""social part"" of Science through a descriptive approach that aims at
identifying the social determinants (e.g. goals and potential interactions
among individuals) behind the emergence and the resilience of scientific
communities. We consider that scientific communities are at the same time
communities of practice (through co-authorship) and that they exist also as
representations in the scientists' mind, since references to other scientists'
works is not merely an objective link to a relevant work, but it reveals social
objects that one manipulates and refers to. In this paper we identify the
patterns about the evolution of a scientific field by analyzing a portion of
the arXiv repository covering a period of 10 years of publications in physics.
As a citation represents a deliberative selection related to the relevance of a
work in its scientific domain, our analysis approaches the co-existence between
co-authorship and citation behaviors in a community by focusing on the most
proficient and cited authors interactions patterns. We focus in turn, on how
these patterns are affected by the selection process of citations. Such a
selection a) produces self-organization because it is played by a group of
individuals which act, compete and collaborate in a common environment in order
to advance Science and b) determines the success (emergence) of both topics and
scientists working on them. The dataset is analyzed a) at a global level, e.g.
the network evolution, b) at the meso-level, e.g. communities emergence, and c)
at a micro-level, e.g. nodes' aggregation patterns.
"
300,Semantic Web: Who is who in the field - A bibliometric analysis,"  The Semantic Web is one of the main efforts aiming to enhance human and
machine interaction by representing data in an understandable way for machines
to mediate data and services. It is a fast-moving and multidisciplinary field.
This study conducts a thorough bibliometric analysis of the field by collecting
data from Web of Science (WOS) and Scopus for the period of 1960-2009. It
utilizes a total of 44,157 papers with 651,673 citations from Scopus, and
22,951 papers with 571,911 citations from WOS. Based on these papers and
citations, it evaluates the research performance of the Semantic Web (SW) by
identifying the most productive players, major scholarly communication media,
highly cited authors, influential papers and emerging stars.
"
301,"Applying centrality measures to impact analysis: A coauthorship network
  analysis","  Many studies on coauthorship networks focus on network topology and network
statistical mechanics. This article takes a different approach by studying
micro-level network properties, with the aim to apply centrality measures to
impact analysis. Using coauthorship data from 16 journals in the field of
library and information science (LIS) with a time span of twenty years
(1988-2007), we construct an evolving coauthorship network and calculate four
centrality measures (closeness, betweenness, degree and PageRank) for authors
in this network. We find out that the four centrality measures are
significantly correlated with citation counts. We also discuss the usability of
centrality measures in author ranking, and suggest that centrality measures can
be useful indicators for impact analysis.
"
302,Discovering author impact: A PageRank perspective,"  This article provides an alternative perspective for measuring author impact
by applying PageRank algorithm to a coauthorship network. A weighted PageRank
algorithm considering citation and coauthorship network topology is proposed.
We test this algorithm under different damping factors by evaluating author
impact in the informetrics research community. In addition, we also compare
this weighted PageRank with the h-index, citation, and program committee (PC)
membership of the International Society for Scientometrics and Informetrics
(ISSI) conferences. Findings show that this weighted PageRank algorithm
provides reliable results in measuring author impact.
"
303,Popular and/or Prestigious? Measures of Scholarly Esteem,"  Citation analysis does not generally take the quality of citations into
account: all citations are weighted equally irrespective of source. However, a
scholar may be highly cited but not highly regarded: popularity and prestige
are not identical measures of esteem. In this study we define popularity as the
number of times an author is cited and prestige as the number of times an
author is cited by highly cited papers. Information Retrieval (IR) is the test
field. We compare the 40 leading researchers in terms of their popularity and
prestige over time. Some authors are ranked high on prestige but not on
popularity, while others are ranked high on popularity but not on prestige. We
also relate measures of popularity and prestige to date of Ph.D. award, number
of key publications, organizational affiliation, receipt of prizes/honors, and
gender.
"
304,PageRank for ranking authors in co-citation networks,"  Google's PageRank has created a new synergy to information retrieval for a
better ranking of Web pages. It ranks documents depending on the topology of
the graphs and the weights of the nodes. PageRank has significantly advanced
the field of information retrieval and keeps Google ahead of competitors in the
search engine market. It has been deployed in bibliometrics to evaluate
research impact, yet few of these studies focus on the important impact of the
damping factor (d) for ranking purposes. This paper studies how varied damping
factors in the PageRank algorithm can provide additional insight into the
ranking of authors in an author co-citation network. Furthermore, we propose
weighted PageRank algorithms. We select 108 most highly cited authors in the
information retrieval (IR) area from the 1970s to 2008 to form the author
co-citation network. We calculate the ranks of these 108 authors based on
PageRank with damping factor ranging from 0.05 to 0.95. In order to test the
relationship between these different measures, we compare PageRank and weighted
PageRank results with the citation ranking, h-index, and centrality measures.
We found that in our author co-citation network, citation rank is highly
correlated with PageRank's with different damping factors and also with
different PageRank algorithms; citation rank and PageRank are not significantly
correlated with centrality measures; and h-index is not significantly
correlated with centrality measures.
"
305,Weighted citation: An indicator of an article's prestige,"  We propose using the technique of weighted citation to measure an article's
prestige. The technique allocates a different weight to each reference by
taking into account the impact of citing journals and citation time intervals.
Weighted citation captures prestige, whereas citation counts capture
popularity. We compare the value variances for popularity and prestige for
articles published in the Journal of the American Society for Information
Science and Technology from 1998 to 2007, and find that the majority have
comparable status.
"
306,"Severe Language Effect in University Rankings: Particularly Germany and
  France are wronged in citation-based rankings","  We applied a set of standard bibliometric indicators to monitor the
scientific state-of-arte of 500 universities worldwide and constructed a
ranking on the basis of these indicators (Leiden Ranking 2010). We find a
dramatic and hitherto largely underestimated language effect in the
bibliometric, citation-based measurement of research performance when comparing
the ranking based on all Web of Science (WoS) covered publications and on only
English WoS covered publications, particularly for Germany and France.
"
307,Rescaling citations of publications in physics,"  We analyze the citation distributions of all papers published in Physical
Review journals between 1985 and 2009. The average number of citations received
by papers published in a given year and in a given field is computed. Large
variations are found, showing that it is not fair to compare citation numbers
across fields and years. However, when a rescaling procedure by the average is
used, it is possible to compare impartially articles across years and fields.
We make the rescaling factors available for use by the readers. We also show
that rescaling citation numbers by the number of publication authors has strong
effects and should therefore be taken into account when assessing the
bibliometric performance of researchers.
"
308,Analysis of Computer Science Communities Based on DBLP,"  It is popular nowadays to bring techniques from bibliometrics and
scientometrics into the world of digital libraries to analyze the collaboration
patterns and explore mechanisms which underlie community development. In this
paper we use the DBLP data to investigate the author's scientific career and
provide an in-depth exploration of some of the computer science communities. We
compare them in terms of productivity, population stability and collaboration
trends.Besides we use these features to compare the sets of topranked
conferences with their lower ranked counterparts.
"
309,Generalized Linear Weights for Sharing Credits Among Multiple Authors,"  Assignment of weights to multiple authors of a paper is a challenging task
due to its dependence on the conventions that may be different among different
fields of research and research groups. In this paper, we describe a scheme for
assignment of weights to multiple authors of a paper. In our scheme, weights
are assigned in a linearly decreasing/increasing fashion depending upon the
weight decrement/increment parameter. We call our scheme Arithmetic: Type-2
scheme as the weights follow an arithmetic series. We analyze the proposed
weight assignment scheme and compare it with the existing schemes such as
equal, arithmetic, geometric, and harmonic. We argue that the a positional
weight assignment scheme, called arithmetic scheme, which we refer to
Arithmetic: Type-1 in this paper, and the equal weight assignment scheme can be
treated as special cases of the proposed Arithmetic: Type-2 scheme.
"
310,"""Structuration"" by Intellectual Organization: The Configuration of
  Knowledge in Relations among Structural Components in Networks of Science","  Using aggregated journal-journal citation networks, the measurement of the
knowledge base in empirical systems is factor-analyzed in two cases of
interdisciplinary developments during the period 1995-2005: (i) the development
of nanotechnology in the natural sciences and (ii) the development of
communication studies as an interdiscipline between social psychology and
political science. The results are compared with a case of stable development:
the citation networks of core journals in chemistry. These citation networks
are intellectually organized by networks of expectations in the knowledge base
at the specialty (that is, above-journal) level. This ""structuration"" of
structural components (over time) can be measured as configurational
information. The latter is compared with the Shannon-type information generated
in the interactions among structural components: the difference between these
two measures provides us with a measure for the redundancy generated by the
specification of a model in the knowledge base of the system. This knowledge
base incurs (against the entropy law) to variable extents on the knowledge
infrastructures provided by the observable networks of relations.
"
311,Distributed Collections of Web Pages in the Wild,"  As the Distributed Collection Manager's work on building tools to support
users maintaining collections of changing web-based resources has progressed,
questions about the characteristics of people's collections of web pages have
arisen. Simultaneously, work in the areas of social bookmarking, social news,
and subscription-based technologies have been taking the existence, usage, and
utility of this data for granted with neither investigation into what people
are doing with their collections nor how they are trying to maintain them. In
order to address these concerns, we performed an online user study of 125
individuals from a variety of online and offline communities, such as the
reddit social news user community and the graduate student body in our
department. From this study we were able to examine a user's needs for a system
to manage their web-based distributed collections, how their current tools
affect their ability to maintain their collections, and what the
characteristics of their current practices and problems in maintaining their
web-based collections were. We also present extensions and improvements being
made to the system both in order to adapt DCM for usage in the Ensemble project
and to meet the requirements found by our user study.
"
312,A Science Model Driven Retrieval Prototype,"  This paper is about a better understanding on the structure and dynamics of
science and the usage of these insights for compensating the typical problems
that arises in metadata-driven Digital Libraries. Three science model driven
retrieval services are presented: co-word analysis based query expansion,
re-ranking via Bradfordizing and author centrality. The services are evaluated
with relevance assessments from which two important implications emerge: (1)
precision values of the retrieval service are the same or better than the
tf-idf retrieval baseline and (2) each service retrieved a disjoint set of
documents. The different services each favor quite other - but still relevant -
documents than pure term-frequency based rankings. The proposed models and
derived retrieval services therefore open up new viewpoints on the scientific
knowledge space and provide an alternative framework to structure scholarly
information systems.
"
313,Applying Science Models for Search,"  The paper proposes three different kinds of science models as value-added
services that are integrated in the retrieval process to enhance retrieval
quality. The paper discusses the approaches Search Term Recommendation,
Bradfordizing and Author Centrality on a general level and addresses
implementation issues of the models within a real-life retrieval environment.
"
314,"Emerging Search Regimes: Measuring Co-evolutions among Research,
  Science, and Society","  Scientometric data is used to investigate empirically the emergence of search
regimes in Biotechnology, Genomics, and Nanotechnology. Complex regimes can
emerge when three independent sources of variance interact. In our model,
researchers can be considered as the nodes that carry the science system.
Research is geographically situated with site-specific skills, tacit knowledge
and infrastructures. Second, the emergent science level refers to the formal
communication of codified knowledge published in journals. Third, the
socio-economic dynamics indicate the ways in which knowledge production relates
to society. Although Biotechnology, Genomics, and Nanotechnology can all be
characterised by rapid growth and divergent dynamics, the regimes differ in
terms of self-organization among these three sources of variance. The scope of
opportunities for researchers to contribute within the constraints of the
existing body of knowledge are different in each field. Furthermore, the
relevance of the context of application contributes to the knowledge dynamics
to various degrees.
"
315,"A Proposal to Classify Latinamerican Scientific Journals using Citation
  Indicators: Case Study in Colombia","  Colombian scientific journals are poorly represented in international digital
libraries; however, through Google Scholar (GS) it is possible to determine
their use by the community. Between the years of 2003 and 2007 a classification
of 185 Colombian journals indexed in the Colombian National Bibliographical
Index (IBNP) was performed using the information provided by GS, basing
categorization on size indicators, indexation and citation. The indicators were
analyzed by grouping the journals in two general areas: sciences and social
sciences. In each area, the indicators provided by the digital libraries
Scopus, Redalyc and Scielo were compared. Additionally, the indicators provided
by IBNP journals categories (A1, A2, B and C) were also compared. The sciences
and social sciences had a similar pattern in their indicators. The existence of
positive correlations was established between some indicators and they
predicted that the number of citations per journal in GS and the h index
depends on its visibility in GS and Scopus. We put forward that the current
IBNP categories (A1, A2, B or C) faintly reflect the use of journals by the
community and we propose a classification based on the h index as an infometric
indicator, which reflects not only its visibility in Google Scholar, but also
its inclusion in certain international digital libraries, particularly Scopus.
Our results may be applied to the creation of public policies regarding science
and technology in Colombia and in developing countries.
"
316,"Resequencing: A Method for Conforming to Conventions for Sharing Credits
  Among Multiple Authors","  Devising an appropriate scheme that assigns the weights to share credits
among multiple authors of a paper is a challenging task. This challenge comes
from the fact that different types of conventions might be followed among
different research discipline or research groups. In this paper, we discuss
that for the purpose of evaluating the quality of research produced by authors,
one can resequence either authors or weights and can apply a weight assignment
policy which the evaluator deems fit for the particular research discipline or
research group.
"
317,Data Preservation in High Energy Physics,"  Data from high-energy physics (HEP) experiments are collected with
significant financial and human effort and are in many cases unique. At the
same time, HEP has no coherent strategy for data preservation and re-use, and
many important and complex data sets are simply lost. In a period of a few
years, several important and unique experimental programs will come to an end,
including those at HERA, the b-factories and at the Tevatron. An
inter-experimental study group on HEP data preservation and long-term analysis
(DPHEP) was formed and a series of workshops were held to investigate this
issue in a systematic way. The physics case for data preservation and the
preservation models established by the group are presented, as well as a
description of the transverse global projects and strategies already in place.
"
318,Bio-inspired Methods for Dynamic Network Analysis in Science Mapping,"  We apply bio-inspired methods for the analysis of different dynamic
bibliometric networks (linking papers by citation, authors, and keywords,
respectively). Biological species are clusters of individuals defined by widely
different criteria and in the biological perspective it is natural to (1) use
different categorizations on the same entities (2) to compare the different
categorizations and to analyze the dissimilarities, especially as they change
over time. We employ the same methodology to comparisons of bibliometric
classifications. We constructed them as analogs of three species concepts:
cladistic or lineage based, similarity based, and ""biological species"" (based
on co-reproductive ability). We use the Rand and Jaccard indexes to compare
classifications in different time intervals. The experiment is aimed to address
the classic problem of science mapping, as to what extent the various
techniques based on different bibliometric indicators, such as citations,
keywords or authors are able to detect convergent structures in the
litrerature, that is, to identify coherent specialities or research directions
and their dynamics.
"
319,"Turning the tables in citation analysis one more time: Principles for
  comparing sets of documents","  We submit newly developed citation impact indicators based not on arithmetic
averages of citations but on percentile ranks. Citation distributions are-as a
rule-highly skewed and should not be arithmetically averaged. With percentile
ranks, the citation of each paper is rated in terms of its percentile in the
citation distribution. The percentile ranks approach allows for the formulation
of a more abstract indicator scheme that can be used to organize and/or
schematize different impact indicators according to three degrees of freedom:
the selection of the reference sets, the evaluation criteria, and the choice of
whether or not to define the publication sets as independent. Bibliometric data
of seven principal investigators (PIs) of the Academic Medical Center of the
University of Amsterdam is used as an exemplary data set. We demonstrate that
the proposed indicators [R(6), R(100), R(6,k), R(100,k)] are an improvement of
averages-based indicators because one can account for the shape of the
distributions of citations over papers.
"
320,"Fractional counting of citations in research evaluation: A cross- and
  interdisciplinary assessment of the Tsinghua University in Beijing","  In the case of the scientometric evaluation of multi- or interdisciplinary
units one risks to compare apples with oranges: each paper has to be assessed
in comparison to an appropriate reference set. We suggest that the set of
citing papers can be considered as the relevant representation of the field of
impact. In order to normalize for differences in citation behavior among
fields, citations can be fractionally counted proportionately to the length of
the reference lists in the citing papers. This new method enables us to compare
among units with different disciplinary affiliations at the paper level and
also to assess the statistical significance of differences among sets.
Twenty-seven departments of the Tsinghua University in Beijing are thus
compared. Among them, the Department of Chinese Language and Linguistics is
upgraded from the 19th to the second position in the ranking. The overall
impact of 19 of the 27 departments is not significantly different at the 5%
level when thus normalized for different citation potentials.
"
321,"Highlights from the SOAP project survey. What Scientists Think about
  Open Access Publishing","  The SOAP (Study of Open Access Publishing) project has run a large-scale
survey of the attitudes of researchers on, and the experiences with, open
access publishing. Around forty thousands answers were collected across
disciplines and around the world, showing an overwhelming support for the idea
of open access, while highlighting funding and (perceived) quality as the main
barriers to publishing in open access journals. This article serves as an
introduction to the survey and presents this and other highlights from a
preliminary analysis of the survey responses. To allow a maximal re-use of the
information collected by this survey, the data are hereby released under a CC0
waiver, so to allow libraries, publishers, funding agencies and academics to
further analyse risks and opportunities, drivers and barriers, in the
transition to open access publishing.
"
322,"The Documents and Assets Created During the Video Game Production
  Process","  The purpose of this paper is to take that first step in helping archivists
understand the video game industry by examining the documents and assets
created by game companies. This paper is intended as a survey of the records
generated during video game production, and an overview of why and how those
records are created. It is not intended to be a statement on archiving best
practices, but rather a tool for archivists to use when assessing and
processing video game collections. It is an overview of how a video game is
made and the paper trail left behind that an archivist might encounter.
"
323,"An Evaluation of Link Neighborhood Lexical Signatures to Rediscover
  Missing Web Pages","  For discovering the new URI of a missing web page, lexical signatures, which
consist of a small number of words chosen to represent the ""aboutness"" of a
page, have been previously proposed. However, prior methods relied on computing
the lexical signature before the page was lost, or using cached or archived
versions of the page to calculate a lexical signature. We demonstrate a system
of constructing a lexical signature for a page from its link neighborhood, that
is the ""backlinks"", or pages that link to the missing page. After testing
various methods, we show that one can construct a lexical signature for a
missing web page using only ten backlink pages. Further, we show that only the
first level of backlinks are useful in this effort. The text that the backlinks
use to point to the missing page is used as input for the creation of a
four-word lexical signature. That lexical signature is shown to successfully
find the target URI in over half of the test cases.
"
324,A Decade of Database Research Publications,"  We analyze the database research publications of four major core database
technology conferences (SIGMOD, VLDB, ICDE, EDBT), two main theoretical
database conferences (PODS, ICDT) and three database journals (TODS, VLDB
Journal, TKDE) over a period of 10 years (2001 - 2010). Our analysis considers
only regular papers as we do not include short papers, demo papers, posters,
tutorials or panels into our statistics. We rank the research scholars
according to their number of publication in each conference/journal separately
and in combined. We also report about the growth in the number of research
publications and the size of the research community in the last decade.
"
325,"Seats at the table: the network of the editorial boards in information
  and library science","  The structural properties of the network generated by the editorial
activities of the members of the boards of ""Information Science & Library
Science"" journals are explored through network analysis techniques. The crossed
presence of scholars on editorial boards, the phenomenon called interlocking
editorship, is considered a proxy of the similarity of editorial policies. The
evidences support the idea that this group of journals is better described as a
set of only relatively connected subfields. In particular two main subfield are
identified, consisting of research oriented journals devoted respectively to
LIS and MIS. The links between these two subsets are weak. Around these two
subsets there are a lot of (relatively) isolated professional journals or
journals characterized more by their subject-matter content than by their focus
on information flows. It is possible to suggest that this configuration of the
network may be the consequence of the youthfulness of Information Science &
Library Science, which has not permitted yet to reach a general consensus
through scholars on research aims, methods and instruments.
"
326,"Interlocking editorship. A network analysis of the links between
  economic journals","  The exploratory analysis developed in this paper relies on the hypothesis
that each editor possesses some power in the definition of the editorial policy
of her journal. Consequently if the same scholar sits on the board of editors
of two journals, those journals could have some common elements in their
editorial policies. The proximity of the editorial policies of two scientific
journals can be assessed by the number of common editors sitting on their
boards. A database of all editors of ECONLIT journals is used. The structure of
the network generated by interlocking editorship is explored by applying the
instruments of network analysis. Evidences have been found of a compact network
containing different components. This is interpreted as the result of a
plurality of perspectives about the appropriate methods for the investigation
of problems and the construction of theories within the domain of economics.
"
327,Applying weighted PageRank to author citation networks,"  This paper aims to identify whether different weighted PageRank algorithms
can be applied to author citation networks to measure the popularity and
prestige of a scholar from a citation perspective. Information Retrieval (IR)
was selected as a test field and data from 1956-2008 were collected from Web of
Science (WOS). Weighted PageRank with citation and publication as weighted
vectors were calculated on author citation networks. The results indicate that
both popularity rank and prestige rank were highly correlated with the weighted
PageRank. Principal Component Analysis (PCA) was conducted to detect
relationships among these different measures. For capturing prize winners
within the IR field, prestige rank outperformed all the other measures.
"
328,"The structure of the Arts & Humanities Citation Index: A mapping on the
  basis of aggregated citations among 1,157 journals","  Using the Arts & Humanities Citation Index (A&HCI) 2008, we apply mapping
techniques previously developed for mapping journal structures in the Science
and Social Science Citation Indices. Citation relations among the 110,718
records were aggregated at the level of 1,157 journals specific to the A&HCI,
and the journal structures are questioned on whether a cognitive structure can
be reconstructed and visualized. Both cosine-normalization (bottom up) and
factor analysis (top down) suggest a division into approximately twelve
subsets. The relations among these subsets are explored using various
visualization techniques. However, we were not able to retrieve this structure
using the ISI Subject Categories, including the 25 categories which are
specific to the A&HCI. We discuss options for validation such as against the
categories of the Humanities Indicators of the American Academy of Arts and
Sciences, the panel structure of the European Reference Index for the
Humanities (ERIH), and compare our results with the curriculum organization of
the Humanities Section of the College of Letters and Sciences of UCLA as an
example of institutional organization.
"
329,"Simple arithmetic versus intuitive understanding: The case of the impact
  factor","  We show that as a consequence of basic properties of elementary arithmetic
journal impact factors show a counterintuitive behaviour with respect to adding
non-cited articles. Synchronous as well as diachronous journal impact factors
are affected. Our findings provide a rationale for not taking uncitable
publications into account in impact factor calculations, at least if these
items are truly uncitable.
"
330,"Citation analysis cannot legitimate the strategic selection of
  excellence","  In reaction to a previous critique(Opthof & Leydesdorff, 2010), the Center
for Science and Technology Studies (CWTS) in Leiden proposed to change their
old ""crown"" indicator in citation analysis into a new one. Waltman et al.
(2011)argue that this change does not affect rankings at various aggregated
levels. However, CWTS data is not publicly available for testing and criticism.
In this correspondence, we use previously published data of Van Raan (2006) to
address the pivotal issue of how the results of citation analysis correlate
with the results of peer review. A quality parameter based on peer review was
neither significantly correlated with the two parameters developed by the CWTS
in the past (CPP/JCSm or CPP/FCSm) nor with the more recently proposed h-index
(Hirsch, 2005). Given the high correlations between the old and new ""crown""
indicators, one can expect that the lack of correlation with the peer-review
based quality indicator applies equally to the newly developed ones.
"
331,Statistical analysis of the Hirsch Index,"  The Hirsch index (commonly referred to as h-index) is a bibliometric
indicator which is widely recognized as effective for measuring the scientific
production of a scholar since it summarizes size and impact of the research
output. In a formal setting, the h-index is actually an empirical functional of
the distribution of the citation counts received by the scholar. Under this
approach, the asymptotic theory for the empirical h-index has been recently
exploited when the citation counts follow a continuous distribution and, in
particular, variance estimation has been considered for the Pareto-type and the
Weibull-type distribution families. However, in bibliometric applications,
citation counts display a distribution supported by the integers. Thus, we
provide general properties for the empirical h-index under the small- and
large-sample settings. In addition, we also introduce consistent nonparametric
variance estimation, which allows for the implemention of large-sample set
estimation for the theoretical h-index.
"
332,The Nuclear Science References (NSR) Database and Web Retrieval System,"  The Nuclear Science References (NSR) database together with its associated
Web interface, is the world's only comprehensive source of easily accessible
low- and intermediate-energy nuclear physics bibliographic information for more
than 200,000 articles since the beginning of nuclear science. The
weekly-updated NSR database provides essential support for nuclear data
evaluation, compilation and research activities. The principles of the database
and Web application development and maintenance are described. Examples of
nuclear structure, reaction and decay applications are specifically included.
The complete NSR database is freely available at the websites of the National
Nuclear Data Center http://www.nndc.bnl.gov/nsr and the International Atomic
Energy Agency http://www-nds.iaea.org/nsr.
"
333,Usage Bibliometrics,"  Scholarly usage data provides unique opportunities to address the known
shortcomings of citation analysis. However, the collection, processing and
analysis of usage data remains an area of active research. This article
provides a review of the state-of-the-art in usage-based informetric, i.e. the
use of usage data to study the scholarly process.
"
334,"Publish or Patent: Bibliometric evidence for empirical trade-offs in
  national funding strategies","  Multivariate linear regression models suggest a trade-off in allocations of
national R&D investments. Government funding, and spending in the higher
education sector, seem to encourage publications, whereas other components such
as industrial funding, and spending in the business sector, encourage
patenting. Our results help explain why the US trails the EU in publications,
because of its focus on industrial funding - some 70% of its total R&D
investment. Conversely, it also helps explain why the EU trails the US in
patenting. Government funding is indicated as a negative incentive to
high-quality patenting. The models here can also be used to predict an output
indicator for a country, once the appropriate input indicator is known. This
usually is done within a dataset for a single year, but the process can be
extended to predict outputs a few years into the future, if reasonable
forecasts can be made of the input indicators. We provide new forecasts about
the further relationships of the US, the EU-27, and the PRC in the case of
publishing. Models for individual countries may be more successful, however,
than regression models whose parameters are averaged over a set of countries.
"
335,PaperBricks: An Alternative to Complete-Story Peer Reviewing,"  The peer review system as used in several computer science communities has
several flaws including long review times, overloaded reviewers, as well as
fostering of niche topics. These flaws decrease quality, lower impact, slowdown
the innovation process, and lead to frustration of authors, readers, and
reviewers. In order to fix this, we propose a new peer review system termed
paper bricks. Paper bricks has several advantages over the existing system
including shorter publications, better competition for new ideas, as well as an
accelerated innovation process. Furthermore, paper bricks may be implemented
with minimal change to the existing peer review systems.
"
336,"The detection of ""hot regions"" in the geography of science: A
  visualization approach by using density maps","  Spatial scientometrics has attracted a lot of attention in the very recent
past. The visualization methods (density maps) presented in this paper allow
for an analysis revealing regions of excellence around the world using computer
programs that are freely available. Based on Scopus and Web of Science data,
field-specific and field-overlapping scientific excellence can be identified in
broader regions (worldwide or for a specific continent) where high quality
papers (highly cited papers or papers published in Nature or Science) were
published. We used a geographic information system to produce our density maps.
We also briefly discuss the use of Google Earth.
"
337,"An Analytical Model for Service Profile Based Service Quality of an
  Institutional eLibrary","  Devising a scheme for evaluating the service quality of an institutional
electronic library is a difficult and challenging task. The challenge comes
from the fact that the services provided by an institutional electronic library
depend upon the contents requested by the users and the contents housed by the
library. Different types of users might be interested in different types of
contents. In this paper, we propose a technique for evaluating the service
quality of an institutional electronic library. Our scheme is based on the
service profiles of contents requested by the users at the server side which is
hosted at the library. Further, we propose models to analyze the service
quality of an electronic library. For analyzing the service quality, we present
two analytical models. The first one is based on the number of days by which
the item to be served by the library is delayed and the penalty points per day
for the duration for which the item is delayed. The second model is based on
the credits earned by the library if the item is served in a timely fashion,
and the penalties, thereof, if the item is delayed. These models may help in
evaluating the service quality of an electronic library and taking the
corrective measures to improve it.
"
338,"Statistics of statisticians: Critical mass of statistics and operational
  research groups in the UK","  Using a recently developed model, inspired by mean field theory in
statistical physics, and data from the UK's Research Assessment Exercise, we
analyse the relationship between the quality of statistics and operational
research groups and the quantity researchers in them. Similar to other academic
disciplines, we provide evidence for a linear dependency of quality on quantity
up to an upper critical mass, which is interpreted as the average maximum
number of colleagues with whom a researcher can communicate meaningfully within
a research group. The model also predicts a lower critical mass, which research
groups should strive to achieve to avoid extinction. For statistics and
operational research, the lower critical mass is estimated to be 9 $\pm$ 3. The
upper critical mass, beyond which research quality does not significantly
depend on group size, is about twice this value.
"
339,"Liber Mathematicae: A Web-Based Documentation and Collaboration Project
  for Mathematics","  Traditionally, mathematical knowledge is published in printed media such as
books or journals. With the advent of the Internet, a new method of publication
became available. To date, however, most online mathematical publications do
not employ the full capabilities of the medium. We describe a project to
modernize online mathematics presentation and build a community-focused
environment in which the lines between ""author"" and ""reader"" are blurred,
enhancing collaboration and improving publication quality.
"
340,A Medial Axis Based Thinning Strategy for Character Images,"  Thinning of character images is a big challenge. Removal of strokes or
deformities in thinning is a difficult problem. In this paper, we have proposed
a medial axis based thinning strategy used for performing skeletonization of
printed and handwritten character images. In this method, we have used shape
characteristics of text to get skeleton of nearly same as the true character
shape. This approach helps to preserve the local features and true shape of the
character images. The proposed algorithm produces one pixel width thin
skeleton. As a by-product of our thinning approach, the skeleton also gets
segmented into strokes in vector form. Hence further stroke segmentation is not
required. Experiment is done on printed English and Bengali characters and we
obtain less spurious branches comparing with other thinning methods without any
post processing.
"
341,"The Planetary System: Executable Science, Technology, Engineering and
  Math Papers","  Executable scientific papers contain not just layouted text for reading. They
contain, or link to, machine-comprehensible representations of the scientific
findings or experiments they describe. Client-side players can thus enable
readers to ""check, manipulate and explore the result space"". We have realized
executable papers in the STEM domain with the Planetary system. Semantic
annotations associate the papers with a content commons holding the background
ontology, the annotations are exposed as Linked Data, and a frontend player
application hooks modular interactive services into the semantic annotations.
"
342,"Development of Computer Science Disciplines - A Social Network Analysis
  Approach","  In contrast to many other scientific disciplines, computer science considers
conference publications. Conferences have the advantage of providing fast
publication of papers and of bringing researchers together to present and
discuss the paper with peers. Previous work on knowledge mapping focused on the
map of all sciences or a particular domain based on ISI published JCR (Journal
Citation Report). Although this data covers most of important journals, it
lacks computer science conference and workshop proceedings. That results in an
imprecise and incomplete analysis of the computer science knowledge. This paper
presents an analysis on the computer science knowledge network constructed from
all types of publications, aiming at providing a complete view of computer
science research. Based on the combination of two important digital libraries
(DBLP and CiteSeerX), we study the knowledge network created at
journal/conference level using citation linkage, to identify the development of
sub-disciplines. We investigate the collaborative and citation behavior of
journals/conferences by analyzing the properties of their co-authorship and
citation subgraphs. The paper draws several important conclusions. First,
conferences constitute social structures that shape the computer science
knowledge. Second, computer science is becoming more interdisciplinary. Third,
experts are the key success factor for sustainability of journals/conferences.
"
343,Statistical regularities in the rank-citation profile of scientists,"  Recent ""science of science"" research shows that scientific impact measures
for journals and individual articles have quantifiable regularities across both
time and discipline. However, little is known about the scientific impact
distribution at the scale of an individual scientist. We analyze the aggregate
scientific production and impact of individual careers using the rank-citation
profile c_{i}(r) of 200 distinguished professors and 100 assistant professors.
For the entire range of paper rank r, we fit each c_{i}(r) to a common
distribution function that is parameterized by two scaling exponents. Since two
scientists with equivalent Hirsch h-index can have significantly different
c_{i}(r) profiles, our results demonstrate the utility of the \beta_{i} scaling
parameter in conjunction with h_{i} for quantifying individual publication
impact. We show that the total number of citations C_{i} tallied from a
scientist's N_{i} papers scales as C_{i} \sim h_{i}^{1+\beta_{i}}. Such
statistical regularities in the input-output patterns of scientists can be used
as benchmarks for theoretical models of career progress.
"
344,"Polynomial Weights or Generalized Geometric Weights: Yet Another Scheme
  for Assigning Credits to Multiple Authors","  Devising a weight assignment policy for assigning credits to multiple authors
of a manuscript is a challenging task. In this paper, we present a scheme for
assigning credits to multiple authors that we call a polynomial weight
assignment scheme. We compare our scheme with other schemes proposed in the
literature.
"
345,Connectivity Damage to a Graph by the Removal of an Edge or a Vertex,"  The approach of quantifying the damage inflicted on a graph in Albert, Jeong
and Barabsi's (AJB) report ""Error and Attack Tolerance of Complex Networks""
using the size of the largest connected component and the average size of the
remaining components does not capture our intuitive idea of the damage to a
graph caused by disconnections. We evaluate an alternative metric based on
average inverse path lengths (AIPLs) that better fits our intuition that a
graph can still be reasonably functional even when it is disconnected. We
compare our metric with AJB's using a test set of graphs and report the
differences. AJB's report should not be confused with a report by Crucitti et
al. with the same name. Based on our analysis of graphs of different sizes and
types, and using various numerical and statistical tools; the ratio of the
average inverse path lengths of a connected graph of the same size as the sum
of the size of the fragments of the disconnected graph can be used as a metric
about the damage of a graph by the removal of an edge or a node. This damage is
reported in the range (0,1) where 0 means that the removal had no effect on the
graph's capability to perform its functions. A 1 means that the graph is
totally dysfunctional. We exercise our metric on a collection of sample graphs
that have been subjected to various attack profiles that focus on edge, node or
degree betweenness values. We believe that this metric can be used to quantify
the damage done to the graph by an attacker, and that it can be used in
evaluating the positive effect of adding additional edges to an existing graph.
"
346,The h-index in Australian Astronomy,"  The Hirsch (2005) h-index is now widely used as a metric to compare
individual researchers. To evaluate it in the context of Australian Astronomy,
the h-index for every member of the Astronomical Society of Australia (ASA) is
found using NASA's Astrophysics Data System Bibliographic Services (ADS).
Percentiles of the h-index distribution are detailed for a variety of
categories of ASA members, including students. This enables a list of the top
ten Australian researchers by h-index to be produced. These top researchers
have h-index values in the range 53<h<77, which is less than that recently
reported for the American Astronomical Society Membership. We suggest that
membership of extremely large consortia such as SDSS may partially explain the
difference. We further suggest that many student ASA members with large h-index
values have probably already received their Ph.D.'s and need to upgrade their
ASA membership status. To attempt to specify the h-index distribution relative
to opportunity, we also detail the percentiles of its distribution by years
since Ph.D. award date. This shows a steady increase in h-index with seniority,
as can be expected.
"
347,"Which cities produce excellent papers worldwide more than can be
  expected? A new mapping approach--using Google Maps--based on statistical
  significance testing","  The methods presented in this paper allow for a statistical analysis
revealing centers of excellence around the world using programs that are freely
available. Based on Web of Science data, field-specific excellence can be
identified in cities where highly-cited papers were published significantly.
Compared to the mapping approaches published hitherto, our approach is more
analytically oriented by allowing the assessment of an observed number of
excellent papers for a city (in the sample) against the expected number. Using
this test, the approach cannot only identify the top performers in output but
the ""true jewels."" These are cities locating authors who publish significantly
more top cited papers than can be expected. As the examples in this paper show
for physics, chemistry, and psychology, these cities do not necessarily have a
high output of excellent papers.
"
348,Globalisation of science in kilometres,"  The ongoing globalisation of science has undisputedly a major impact on how
and where scientific research is being conducted nowadays. Yet, the big picture
remains blurred. It is largely unknown where this process is heading, and at
which rate. Which countries are leading or lagging? Many of its key features
are difficult if not impossible to capture in measurements and comparative
statistics. Our empirical study measures the extent and growth of scientific
globalisation in terms of physical distances between co-authoring researchers.
Our analysis, drawing on 21 million research publications across all countries
and fields of science, reveals that contemporary science has globalised at a
fairly steady rate during recent decades. The average collaboration distance
per publication has increased from 334 kilometres in 1980 to 1553 in 2009.
Despite significant differences in globalisation rates across countries and
fields of science, we observe a pervasive process in motion, moving towards a
truly interconnected global science system.
"
349,Universal Metadata Standard,"  The creation of a next generation internet (semantic web) is impossible
without attributes, allowing the semantic association of documents and their
integration into information context. To achieve these goals, the Universal
Metadata Standard (ums) may be an ultimative tool, which could serve as a basis
for documentography, and is functionally required for interpretation of
documents by the automatic operating systems.
"
350,Linking Literature and Data: Status Report and Future Efforts,"  In the current era of data-intensive science, it is increasingly important
for researchers to be able to have access to published results, the supporting
data, and the processes used to produce them. Six years ago, recognizing this
need, the American Astronomical Society and the Astrophysics Data Centers
Executive Committee (ADEC) sponsored an effort to facilitate the annotation and
linking of datasets during the publishing process, with limited success. I will
review the status of this effort and describe a new, more general one now being
considered in the context of the Virtual Astronomical Observatory.
"
351,"Bounds and Inequalities Relating h-Index, g-Index, e-Index and
  Generalized Impact Factor","  Finding relationships among different indices such as h-index, g-index,
e-index, and generalized impact factor is a challenging task. In this paper, we
describe some bounds and inequalities relating h-index, g-index, e-index, and
generalized impact factor. We derive the bounds and inequalities relating these
indexing parameters from their basic definitions and without assuming any
continuous model to be followed by any of them.
"
352,"Progress of concepts and processes in library information system:
  towards Library 2.0","  The main principle of the Library 2.0 is in the fact that the information has
to be spread from the library to the user and viceversa, to allow fast and
permanent adaptation of the library services. Within the framework of the
implementation of the ""Departmental Plan of the Public Services Reading"" by the
""General Council of Moselle"", the division of the public reading develops a
departmental portal as main vector of the information with various users'
profile. The context of this research work takes a part of a Master degree
training Diploma in STI-Economic Intelligence (Nancy2 University), combining
facets of R&D in a professional context at the ""Conseil G\'en\'eral de la
Moselle"" in France.
"
353,"Integrated Impact Indicators (I3) compared with Impact Factors (IFs): An
  alternative research design with policy implications","  In bibliometrics, the association of ""impact"" with central-tendency
statistics is mistaken. Impacts add up, and citation curves should therefore be
integrated instead of averaged. For example, the journals MIS Quarterly and
JASIST differ by a factor of two in terms of their respective impact factors
(IF), but the journal with the lower IF has the higher impact. Using percentile
ranks (e.g., top-1%, top-10%, etc.), an integrated impact indicator (I3) can be
based on integration of the citation curves, but after normalization of the
citation curves to the same scale. The results across document sets can be
compared as percentages of the total impact of a reference set. Total number of
citations, however, should not be used instead because the shape of the
citation curves is then not appreciated. I3 can be applied to any document set
and any citation window. The results of the integration (summation) are fully
decomposable in terms of journals or instititutional units such as nations,
universities, etc., because percentile ranks are determined at the paper level.
In this study, we first compare I3 with IFs for the journals in two ISI Subject
Categories (""Information Science & Library Science"" and ""Multidisciplinary
Sciences""). The LIS set is additionally decomposed in terms of nations. Policy
implications of this possible paradigm shift in citation impact analysis are
specified.
"
354,Telescope Bibliometrics 101,"  During recent years, bibliometric studies have become increasingly important
in evaluating individual scientists, institutes, and entire observatories. In
astronomy, often librarians are involved in maintaining publication databases
and compiling statistics for their institutions. In this paper, we present a
look behind the scenes to understand who is interested in bibliometric
statistics, which methodologies astronomy librarians apply, and what kind of
features next-generation bibliographies may include.
"
355,Semantic Interlinking of Resources in the Virtual Observatory Era,"  In the coming era of data-intensive science, it will be increasingly
important to be able to seamlessly move between scientific results, the data
analyzed in them, and the processes used to produce them. As observations,
derived data products, publications, and object metadata are curated by
different projects and archived in different locations, establishing the proper
linkages between these resources and describing their relationships becomes an
essential activity in their curation and preservation. In this paper we
describe initial efforts to create a semantic knowledge base allowing easier
integration and linking of the body of heterogeneous astronomical resources
which we call the Virtual Observatory (VO). The ultimate goal of this effort is
the creation of a semantic layer over existing resources, allowing applications
to cross boundaries between archives. The proposed approach follows the current
best practices in Semantic Computing and the architecture of the web, allowing
the use of off-the-shelf technologies and providing a path for VO resources to
become part of the global web of linked data.
"
356,"The Anna Karenina principle: A concept for the explanation of success in
  science","  The first sentence of Leo Tolstoy's novel Anna Karenina is: ""Happy families
are all alike; every unhappy family is unhappy in its own way."" Here Tolstoy
means that for a family to be happy, several key aspects must be given (such as
good health of all family members, acceptable financial security, and mutual
affection). If there is a deficiency in any one or more of these key aspects,
the family will be unhappy. In this paper we introduce the Anna Karenina
principle as a concept that can explain success in science. Here we will refer
to three central areas in modern science in which scarce resources will most
usually lead to failure: (1) peer review of research grant proposals and
manuscripts (money and journal space as scarce resources), (2) citation of
publications (reception as a scarce resource), and (3) new scientific
discoveries (recognition as a scarce resource). If resources are scarce
(journal space, funds, reception, and recognition), there can be success only
when several key prerequisites for the allocation of the resources are
fulfilled. If any one of these prerequisites is not fulfilled, the grant
proposal, manuscript submission, the published paper, or the discovery will not
be successful.
"
357,Astronomy Librarians - Quo Vadis?,"  ""You don't look like a librarian"" is a phrase we often hear in the astronomy
department or observatory library. Astronomy librarians are a breed apart, and
are taking on new and non-traditional roles as information technology evolves.
This talk will explore the future of librarians and librarianship through the
lens of the recent talks given at the sixth ""Libraries and Information Services
in Astronomy"" conference held in Pune, India in February 2010. We will explore
the librarian's universe, illustrating how librarians use new technologies to
perform such tasks as bibliometrics, how we are re-fashioning our library
spaces in an increasingly digital world and how we are confronting the brave
new world of open access, to name but a few topics.
"
358,"SharedCanvas: A Collaborative Model for Medieval Manuscript Layout
  Dissemination","  In this paper we present a model based on the principles of Linked Data that
can be used to describe the interrelationships of images, texts and other
resources to facilitate the interoperability of repositories of medieval
manuscripts or other culturally important handwritten documents. The model is
designed from a set of requirements derived from the real world use cases of
some of the largest digitized medieval content holders, and instantiations of
the model are intended as the input to collection-independent page turning and
scholarly presentation interfaces. A canvas painting paradigm, such as in PDF
and SVG, was selected based on the lack of a one to one correlation between
image and page, and to fulfill complex requirements such as when the full text
of a page is known, but only fragments of the physical object remain. The model
is implemented using technologies such as OAI-ORE Aggregations and OAC
Annotations, as the fundamental building blocks of emerging Linked Digital
Libraries. The model and implementation are evaluated through prototypes of
both content providing and consuming applications. Although the system was
designed from requirements drawn from the medieval manuscript domain, it is
applicable to any layout-oriented presentation of images of text.
"
359,Modeling the clustering in citation networks,"  For the study of citation networks, a challenging problem is modeling the
high clustering. Existing studies indicate that the promising way to model the
high clustering is a copying strategy, i.e., a paper copies the references of
its neighbour as its own references. However, the line of models highly
underestimates the number of abundant triangles observed in real citation
networks and thus cannot well model the high clustering. In this paper, we
point out that the failure of existing models lies in that they do not capture
the connecting patterns among existing papers. By leveraging the knowledge
indicated by such connecting patterns, we further propose a new model for the
high clustering in citation networks. Experiments on two real world citation
networks, respectively from a special research area and a multidisciplinary
research area, demonstrate that our model can reproduce not only the power-law
degree distribution as traditional models but also the number of triangles, the
high clustering coefficient and the size distribution of co-citation clusters
as observed in these real networks.
"
360,Collaboration in computer science: a network science approach. Part II,"  We represent collaboration of authors in computer science papers in terms of
both affiliation and collaboration networks and observe how these networks
evolved over time since 1960. We investigate the temporal evolution of
bibliometric properties, like size of the discipline, productivity of scholars,
and collaboration level in papers, as well as of large-scale network
properties, like reachability and average separation distance among scientists,
distribution of the number of scholar collaborators, network clustering and
network assortativity by number of collaborators.
"
361,ChemXSeer Digital Library Gaussian Search,"  We report on the Gaussian file search system designed as part of the
ChemXSeer digital library. Gaussian files are produced by the Gaussian software
[4], a software package used for calculating molecular electronic structure and
properties. The output files are semi-structured, allowing relatively easy
access to the Gaussian attributes and metadata. Our system is currently capable
of searching Gaussian documents using a boolean combination of atoms (chemical
elements) and attributes. We have also implemented a faceted browsing feature
on three important Gaussian attribute types - Basis Set, Job Type and Method
Used. The faceted browsing feature enables a user to view and process a
smaller, filtered subset of documents.
"
362,"How journal rankings can suppress interdisciplinary research. A
  comparison between Innovation Studies and Business & Management","  This study provides quantitative evidence on how the use of journal rankings
can disadvantage interdisciplinary research in research evaluations. Using
publication and citation data, it compares the degree of interdisciplinarity
and the research performance of a number of Innovation Studies units with that
of leading Business & Management schools in the UK. On the basis of various
mappings and metrics, this study shows that: (i) Innovation Studies units are
consistently more interdisciplinary in their research than Business &
Management schools; (ii) the top journals in the Association of Business
Schools' rankings span a less diverse set of disciplines than lower-ranked
journals; (iii) this results in a more favourable assessment of the performance
of Business & Management schools, which are more disciplinary-focused. This
citation-based analysis challenges the journal ranking-based assessment. In
short, the investigation illustrates how ostensibly 'excellence-based' journal
rankings exhibit a systematic bias in favour of mono-disciplinary research. The
paper concludes with a discussion of implications of these phenomena, in
particular how the bias is likely to affect negatively the evaluation and
associated financial resourcing of interdisciplinary research organisations,
and may result in researchers becoming more compliant with disciplinary
authority over time.
"
363,How citation boosts promote scientific paradigm shifts and Nobel Prizes,"  Nobel Prizes are commonly seen to be among the most prestigious achievements
of our times. Based on mining several million citations, we quantitatively
analyze the processes driving paradigm shifts in science. We find that
groundbreaking discoveries of Nobel Prize Laureates and other famous scientists
are not only acknowledged by many citations of their landmark papers.
Surprisingly, they also boost the citation rates of their previous
publications. Given that innovations must outcompete the rich-gets-richer
effect for scientific citations, it turns out that they can make their way only
through citation cascades. A quantitative analysis reveals how and why they
happen. Science appears to behave like a self-organized critical system, in
which citation cascades of all sizes occur, from continuous scientific progress
all the way up to scientific revolutions, which change the way we see our
world. Measuring the ""boosting effect"" of landmark papers, our analysis reveals
how new ideas and new players can make their way and finally triumph in a world
dominated by established paradigms. The underlying ""boost factor"" is also
useful to discover scientific breakthroughs and talents much earlier than
through classical citation analysis, which by now has become a widespread
method to measure scientific excellence, influencing scientific careers and the
distribution of research funds. Our findings reveal patterns of collective
social behavior, which are also interesting from an attention economics
perspective. Understanding the origin of scientific authority may therefore
ultimately help to explain, how social influence comes about and why the value
of goods depends so strongly on the attention they attract.
"
364,"Workflows for the Management of Change in Science, Technologies,
  Engineering and Mathematics","  Mathematical knowledge is a central component in science, engineering, and
technology (documentation). Most of it is represented informally, and -- in
contrast to published research mathematics -- subject to continual change.
Unfortunately, machine support for change management has either been very
coarse grained and thus barely useful, or restricted to formal languages, where
automation is possible. In this paper, we report on an effort to extend change
management to collections of semi-formal documents which flexibly intermix
mathematical formulas and natural language and to integrate it into a semantic
publishing system for mathematical knowledge. We validate the long-standing
assumption that the semantic annotations in these flexiformal documents that
drive the machine-supported interaction with documents can support semantic
impact analyses at the same time. But in contrast to the fully formal setting,
where adaptations of impacted documents can be automated to some degree, the
flexiformal setting requires much more user interaction and thus a much tighter
integration into document management workflows.
"
365,Science Models as Value-Added Services for Scholarly Information Systems,"  The paper introduces scholarly Information Retrieval (IR) as a further
dimension that should be considered in the science modeling debate. The IR use
case is seen as a validation model of the adequacy of science models in
representing and predicting structure and dynamics in science. Particular
conceptualizations of scholarly activity and structures in science are used as
value-added search services to improve retrieval quality: a co-word model
depicting the cognitive structure of a field (used for query expansion), the
Bradford law of information concentration, and a model of co-authorship
networks (both used for re-ranking search results). An evaluation of the
retrieval quality when science model driven services are used turned out that
the models proposed actually provide beneficial effects to retrieval quality.
From an IR perspective, the models studied are therefore verified as expressive
conceptualizations of central phenomena in science. Thus, it could be shown
that the IR perspective can significantly contribute to a better understanding
of scholarly structures and activities.
"
366,"Comparing webometric with web-independent rankings: a case study with
  German universities","  In this paper we examine if hyperlink-based (webometric) indicators can be
used to rank academic websites. Therefore we analyzed the interlinking
structure of German university websites and compared our simple hyperlink-based
ranking with official and web-independent rankings of universities. We found
that link impact could not easily be seen as a prestige factor for
universities.
"
367,"Interactive Overlays: A New Method for Generating Global Journal Maps
  from Web-of-Science Data","  Recent advances in methods and techniques enable us to develop an interactive
overlay to the global map of science based on aggregated citation relations
among the 9,162 journals contained in the Science Citation Index and Social
Science Citation Index 2009 combined. The resulting mapping is provided by
VOSViewer. We first discuss the pros and cons of the various options: cited
versus citing, multidimensional scaling versus spring-embedded algorithms,
VOSViewer versus Gephi, and the various clustering algorithms and similarity
criteria. Our approach focuses on the positions of journals in the
multidimensional space spanned by the aggregated journal-journal citations. A
number of choices can be left to the user, but we provide default options
reflecting our preferences. Some examples are also provided; for example, the
potential of using this technique to assess the interdisciplinarity of
organizations and/or document sets.
"
368,Universality of citation distributions revisited,"  Radicchi, Fortunato, and Castellano [arXiv:0806.0974, PNAS 105(45), 17268]
claim that, apart from a scaling factor, all fields of science are
characterized by the same citation distribution. We present a large-scale
validation study of this universality-of-citation-distributions claim. Our
analysis shows that claiming citation distributions to be universal for all
fields of science is not warranted. Although many fields indeed seem to have
fairly similar citation distributions, there are quite some exceptions as well.
We also briefly discuss the consequences of our findings for the measurement of
scientific impact using citation-based bibliometric indicators.
"
369,"A recursive field-normalized bibliometric performance indicator: An
  application to the field of library and information science","  Two commonly used ideas in the development of citation-based research
performance indicators are the idea of normalizing citation counts based on a
field classification scheme and the idea of recursive citation weighing (like
in PageRank-inspired indicators). We combine these two ideas in a single
indicator, referred to as the recursive mean normalized citation score
indicator, and we study the validity of this indicator. Our empirical analysis
shows that the proposed indicator is highly sensitive to the field
classification scheme that is used. The indicator also has a strong tendency to
reinforce biases caused by the classification scheme. Based on these
observations, we advise against the use of indicators in which the idea of
normalization based on a field classification scheme and the idea of recursive
citation weighing are combined.
"
370,Scholarly Communication,"  The chapter tackles the role of scholarly publication in the research process
(quality, preservation) and looks at the consequences of new information
technologies in the organization of the scholarly communication ecology. It
will then show how new technologies have had an impact on the scholarly
communication process and made it depart from the traditional publishing
environment. Developments will address new editorial processes, dissemination
of new content and services, as well as the development of publication
archives. This last aspect will be covered on all levels (open access,
scientific, technical and legal aspects). A view on the possible evolutions of
the scientific publishing environment will be provided.
"
371,Analyzing the Persistence of Referenced Web Resources with Memento,"  In this paper we present the results of a study into the persistence and
availability of web resources referenced from papers in scholarly repositories.
Two repositories with different characteristics, arXiv and the UNT digital
library, are studied to determine if the nature of the repository, or of its
content, has a bearing on the availability of the web resources cited by that
content. Memento makes it possible to automate discovery of archived resources
and to consider the time between the publication of the research and the
archiving of the referenced URLs. This automation allows us to process more
than 160000 URLs, the largest known such study, and the repository metadata
allows consideration of the results by discipline. The results are startling:
45% (66096) of the URLs referenced from arXiv still exist, but are not
preserved for future generations, and 28% of resources referenced by UNT papers
have been lost. Moving forwards, we provide some initial recommendations,
including that repositories should publish URL lists extracted from papers that
could be used as seeds for web archiving systems.
"
372,Correction of Noisy Sentences using a Monolingual Corpus,"  Correction of Noisy Natural Language Text is an important and well studied
problem in Natural Language Processing. It has a number of applications in
domains like Statistical Machine Translation, Second Language Learning and
Natural Language Generation. In this work, we consider some statistical
techniques for Text Correction. We define the classes of errors commonly found
in text and describe algorithms to correct them. The data has been taken from a
poorly trained Machine Translation system. The algorithms use only a language
model in the target language in order to correct the sentences. We use phrase
based correction methods in both the algorithms. The phrases are replaced and
combined to give us the final corrected sentence. We also present the methods
to model different kinds of errors, in addition to results of the working of
the algorithms on the test set. We show that one of the approaches fail to
achieve the desired goal, whereas the other succeeds well. In the end, we
analyze the possible reasons for such a trend in performance.
"
373,"Mapping excellence in the geography of science: An approach based on
  Scopus data","  As research becomes an ever more globalized activity, there is growing
interest in national and international comparisons of standards and quality in
different countries and regions. A sign for this trend is the increasing
interest in rankings of universities according to their research performance,
both inside but also outside the scientific environment. New methods presented
in this paper, enable us to map centers of excellence around the world using
programs that are freely available. Based on Scopus data, field-specific
excellence can be identified and agglomerated in regions and cities where
recently highly-cited papers were published. Differences in performance rates
can be visualized on the map using colors and sizes of the marks.
"
374,"On the correlation between bibliometric indicators and peer review:
  Reply to Opthof and Leydesdorff","  Opthof and Leydesdorff [arXiv:1102.2569] reanalyze data reported by Van Raan
[arXiv:physics/0511206] and conclude that there is no significant correlation
between on the one hand average citation scores measured using the CPP/FCSm
indicator and on the other hand the quality judgment of peers. We point out
that Opthof and Leydesdorff draw their conclusions based on a very limited
amount of data. We also criticize the statistical methodology used by Opthof
and Leydesdorff. Using a larger amount of data and a more appropriate
statistical methodology, we do find a significant correlation between the
CPP/FCSm indicator and peer judgment.
"
375,Clustering and Classification in Text Collections Using Graph Modularity,"  A new fast algorithm for clustering and classification of large collections
of text documents is introduced. The new algorithm employs the bipartite graph
that realizes the word-document matrix of the collection. Namely, the
modularity of the bipartite graph is used as the optimization functional.
Experiments performed with the new algorithm on a number of text collections
had shown a competitive quality of the clustering (classification), and a
record-breaking speed.
"
376,Revealing digital documents. Concealed structures in data,"  This short paper gives an introduction to a research project to analyze how
digital documents are structured and described. Using a phenomenological
approach, this research will reveal common patterns that are used in data,
independent from the particular technology in which the data is available. The
ability to identify these patterns, on different levels of description, is
important for several applications in digital libraries. A better understanding
of data structuring will not only help to better capture singular
characteristics of data by metadata, but will also recover intended structures
of digital objects, beyond long term preservation.
"
377,"Need to categorize: A comparative look at the categories of the
  Universal Decimal Classification system (UDC) and Wikipedia","  This study analyzes the differences between the category structure of the
Universal Decimal Classification (UDC) system (which is one of the widely used
library classification systems in Europe) and Wikipedia. In particular, we
compare the emerging structure of category-links to the structure of classes in
the UDC. With this comparison we would like to scrutinize the question of how
do knowledge maps of the same domain differ when they are created socially
(i.e. Wikipedia) as opposed to when they are created formally (UDC) using
classificatio theory. As a case study, we focus on the category of ""Arts"".
"
378,Fractional counting of authorship to quantify scientific research output,"  We investigate the problem of counting co-authorhip in order to quantify the
impact and relevance of scientific research output through normalized
\textit{h-index} and \textit{g-index}. We use the papers whose authors belong
to a subset of full professors of the Italian Settore Scientifico Disciplinare
(SSD) FIS01 - Experimental Physics. In this SSD two populations, characterized
by the number of co-authors of each paper, are roughly present. The total
number of citations for each individuals, as well as their h-index and g-index,
strongly depends on the average number of co-authors. We show that, in order to
remove the dependence of the various indices on the two populations, the best
way to define a fractional counting of autorship is to divide the number of
citations received by each paper by the square root of the number of
co-authors. This allows us to obtain some information which can be used for a
better understanding of the scientific knowledge made through the process of
writing and publishing papers.
"
379,Using Lotkaian Informetrics for Ranking in Digital Libraries,"  The purpose of this paper is to propose the use of models, theories and laws
in bibliometrics and scientometrics to enhance information retrieval processes,
especially ranking. A common pattern in many man-made data sets is Lotka's Law
which follows the well-known power-law distributions. These informetric
distributions can be used to give an alternative order to large and scattered
result sets and can be applied as a new ranking mechanism. The
polyrepresentation of information in Digital Library systems is used to enhance
the retrieval quality, to overcome the drawbacks of the typical term-based
ranking approaches and to enable users to explore retrieved document sets from
a different perspective.
"
380,Probabilistic Management of OCR Data using an RDBMS,"  The digitization of scanned forms and documents is changing the data sources
that enterprises manage. To integrate these new data sources with enterprise
data, the current state-of-the-art approach is to convert the images to ASCII
text using optical character recognition (OCR) software and then to store the
resulting ASCII text in a relational database. The OCR problem is challenging,
and so the output of OCR often contains errors. In turn, queries on the output
of OCR may fail to retrieve relevant answers. State-of-the-art OCR programs,
e.g., the OCR powering Google Books, use a probabilistic model that captures
many alternatives during the OCR process. Only when the results of OCR are
stored in the database, do these approaches discard the uncertainty. In this
work, we propose to retain the probabilistic models produced by OCR process in
a relational database management system. A key technical challenge is that the
probabilistic data produced by OCR software is very large (a single book blows
up to 2GB from 400kB as ASCII). As a result, a baseline solution that
integrates these models with an RDBMS is over 1000x slower versus standard text
processing for single table select-project queries. However, many applications
may have quality-performance needs that are in between these two extremes of
ASCII and the complete model output by the OCR software. Thus, we propose a
novel approximation scheme called Staccato that allows a user to trade recall
for query performance. Additionally, we provide a formal analysis of our
scheme's properties, and describe how we integrate our scheme with
standard-RDBMS text indexing.
"
381,"A Novel Combined Term Suggestion Service for Domain-Specific Digital
  Libraries","  Interactive query expansion can assist users during their query formulation
process. We conducted a user study with over 4,000 unique visitors and four
different design approaches for a search term suggestion service. As a basis
for our evaluation we have implemented services which use three different
vocabularies: (1) user search terms, (2) terms from a terminology service and
(3) thesaurus terms. Additionally, we have created a new combined service which
utilizes thesaurus term and terms from a domain-specific search term
re-commender. Our results show that the thesaurus-based method clearly is used
more often compared to the other single-method implementations. We interpret
this as a strong indicator that term suggestion mechanisms should be
domain-specific to be close to the user terminology. Our novel combined
approach which interconnects a thesaurus service with additional statistical
relations out-performed all other implementations. All our observations show
that domain-specific vocabulary can support the user in finding alternative
concepts and formulating queries.
"
382,"Resolving Author Name Homonymy to Improve Resolution of Structures in
  Co-author Networks","  We investigate how author name homonymy distorts clustered large-scale
co-author networks, and present a simple, effective, scalable and generalizable
algorithm to ameliorate such distortions. We evaluate the performance of the
algorithm to improve the resolution of mesoscopic network structures. To this
end, we establish the ground truth for a sample of author names that is
statistically representative of different types of nodes in the co-author
network, distinguished by their role for the connectivity of the network. We
finally observe that this distinction of node roles based on the mesoscopic
structure of the network, in combination with a quantification of author name
commonality, suggests a new approach to assess network distortion by homonymy
and to analyze the reduction of distortion in the network after disambiguation,
without requiring ground truth sampling.
"
383,"Viewpoint: Journals for Certification, Conferences for Rapid
  Dissemination","  The publication culture in Computer Science is different from that of all
other disciplines. Whereas other disciplines focus on journal publication, the
standard practice in CS has been to publish in a conference and then
(sometimes) publish a journal version of the conference paper. We discuss the
role of journal publication in CS.
  Indeed, it is through publication in selective, leading conferences that the
quality of CS research is typically assessed.
"
384,The Art of Data Science,"  To flourish in the new data-intensive environment of 21st century science, we
need to evolve new skills. These can be expressed in terms of the systemized
framework that formed the basis of mediaeval education - the trivium (logic,
grammar, and rhetoric) and quadrivium (arithmetic, geometry, music, and
astronomy). However, rather than focusing on number, data is the new keystone.
We need to understand what rules it obeys, how it is symbolized and
communicated and what its relationship to physical space and time is. In this
paper, we will review this understanding in terms of the technologies and
processes that it requires. We contend that, at least, an appreciation of all
these aspects is crucial to enable us to extract scientific information and
knowledge from the data sets which threaten to engulf and overwhelm us.
"
385,Similarity-based Browsing over Linked Open Data,"  An increasing amount of data is published on the Web according to the Linked
Open Data (LOD) principles. End users would like to browse these data in a
flexible manner. In this paper we focus on similarity-based browsing and we
introduce a novel method for computing the similarity between two entities of a
given RDF/S graph. The distinctive characteristics of the proposed metric is
that it is generic (it can be used to compare nodes of any kind), it takes into
account the neighborhoods of the nodes, and it is configurable (with respect to
the accuracy vs computational complexity tradeoff). We demonstrate the behavior
of the metric using examples from an application over LOD. Finally, we
generalize and elaborate on implementation approaches harmonized with the
distributed nature of LOD which can be used for computing the most similar
entities using neighborhood-based similarity metrics.
"
386,Semantic Inference using Chemogenomics Data for Drug Discovery,"  Background Semantic Web Technology (SWT) makes it possible to integrate and
search the large volume of life science datasets in the public domain, as
demonstrated by well-known linked data projects such as LODD, Bio2RDF, and
Chem2Bio2RDF. Integration of these sets creates large networks of information.
We have previously described a tool called WENDI for aggregating information
pertaining to new chemical compounds, effectively creating evidence paths
relating the compounds to genes, diseases and so on. In this paper we examine
the utility of automatically inferring new compound-disease associations (and
thus new links in the network) based on semantically marked-up versions of
these evidence paths, rule-sets and inference engines.
  Results Through the implementation of a semantic inference algorithm, rule
set, Semantic Web methods (RDF, OWL and SPARQL) and new interfaces, we have
created a new tool called Chemogenomic Explorer that uses networks of
ontologically annotated RDF statements along with deductive reasoning tools to
infer new associations between the query structure and genes and diseases from
WENDI results. The tool then permits interactive clustering and filtering of
these evidence paths.
  Conclusions We present a new aggregate approach to inferring links between
chemical compounds and diseases using semantic inference. This approach allows
multiple evidence paths between compounds and diseases to be identified using a
rule-set and semantically annotated data, and for these evidence paths to be
clustered to show overall evidence linking the compound to a disease. We
believe this is a powerful approach, because it allows compound-disease
relationships to be ranked by the amount of evidence supporting them.
"
387,The Open Annotation Collaboration (OAC) Model,"  Annotations allow users to associate additional information with existing
resources. Using proprietary and closed systems on the Web, users are already
able to annotate multimedia resources such as images, audio and video. So far,
however, this information is almost always kept locked up and inaccessible to
the Web of Data. We believe that an important step to take is the integration
of multimedia annotations and the Linked Data principles. This should allow
clients to easily publish and consume, thus exchange annotations about
resources via common Web standards. We first present the current status of the
Open Annotation Collaboration, an international initiative that is currently
working on annotation interoperability specifications based on best practices
from the Linked Data effort. Then we present two use cases and early prototypes
that make use of the proposed annotation model and present lessons learned and
discuss yet open technical issues.
"
388,OpenPh - Numerical Physics Library,"  Numerical physics has gained a lot of importance in the last decade, its
efficiency being motivated and sustained by the growth of computational power.
This paper presents a concept that is to be developed in the next few years:
OpenPh. OpenPh is a numerical physics library that makes use of the advantages
of both open source software and MATLAB programming. Its aim is to deliver the
instruments for providing numerical and graphical solutions for various physics
problems. It has a modular structure, allowing the user to add new modules to
the existing ones and to create its own modules according to its needs, being
virtually unlimited extendable. The modules of OpenPh are implemented using
MATLAB engine because it is the best solution used in engineering and science,
providing a wide range of optimized methods to accomplish even the toughest
jobs. Current version of OpenPh includes two modules, the first one providing
tools for quantum physics and the second one for mechanics. The quantum physics
module deals with the photoelectric effect, the radioactive decay of carbon-11,
and the Schr\""odinger equation - particle in a box. The classical mechanics
module includes the study of the uniform circular motion, the forced damped
harmonic oscillations and the vibration of a fixed-fixed string.
"
389,The ADS in the Information Age - Impact on Discovery,"  The SAO/NASA Astrophysics Data System (ADS) grew up with and has been riding
the waves of the Information Age, closely monitoring and anticipating the needs
of its end-users. By now, all professional astronomers are using the ADS on a
daily basis, and a substantial fraction have been using it for their entire
professional career. In addition to being an indispensable tool for
professional scientists, the ADS also moved into the public domain, as a tool
for science education. In this paper we will highlight and discuss some aspects
indicative of the impact the ADS has had on research and the access to
scholarly publications.
  The ADS is funded by NASA Grant NNX09AB39G
"
390,"Hyperincursive Cogitata and Incursive Cogitantes: Scholarly Discourse as
  a Strongly Anticipatory System","  Strongly anticipatory systems-that is, systems which use models of themselves
for their further development-and which additionally may be able to run
hyperincursive routines-that is, develop only with reference to their future
states-cannot exist in res extensa, but can only be envisaged in res cogitans.
One needs incursive routines in cogitantes to instantiate these systems. Unlike
historical systems (with recursion), these hyper-incursive routines generate
redundancies by opening horizons of other possible states. Thus, intentional
systems can enrich our perceptions of the cases that have happened to occur.
The perspective of hindsight codified at the above-individual level enables us
furthermore to intervene technologically. The theory and computation of
anticipatory systems have made these loops between supra-individual
hyper-incursion, individual incursion (in instantiation), and historical
recursion accessible for modeling and empirical investigation.
"
391,"A multilingual/multicultural semantic-based approach to improve Data
  Sharing in an SDI for Nature Conservation","  The paper proposes an approach to transcend multicultural and multilingual
barriers in the use and reuse of geographical data at the European level. The
approach aims at sharing scientific terms in the field of nature conservation
with the goal of assisting different user communities with metadata compilation
and information discovery. A multi-thesauri solution is proposed, based on a
Common Thesaurus Framework for Nature Conservation, where different well-known
Knowledge Organization Systems are assembled and shared. It has been designed
according to semantic web and W3C recommendations employing SKOS standard
models and Linked Data to publish the thesauri as a whole in
machine-understandable format. The outcome is a powerful framework satisfying
the requirements of modularity and openness for further thesaurus extension and
updating, interlinking among thesauri, and exploitability from other systems.
The paper supports the employment of Linked Data to deal with terminologies in
complex domains such as nature conservation and it proposes a hands-on recipe
to publish thesauri in the framework.
"
392,Large Formal Wikis: Issues and Solutions,"  We present several steps towards large formal mathematical wikis. The Coq
proof assistant together with the CoRN repository are added to the pool of
systems handled by the general wiki system described in
\cite{DBLP:conf/aisc/UrbanARG10}. A smart re-verification scheme for the large
formal libraries in the wiki is suggested for Mizar/MML and Coq/CoRN, based on
recently developed precise tracking of mathematical dependencies. We propose to
use features of state-of-the-art filesystems to allow real-time cloning and
sandboxing of the entire libraries, allowing also to extend the wiki to a true
multi-user collaborative area. A number of related issues are discussed.
"
393,Licensing the Mizar Mathematical Library,"  The Mizar Mathematical Library (MML) is a large corpus of formalised
mathematical knowledge. It has been constructed over the course of many years
by a large number of authors and maintainers. Yet the legal status of these
efforts of the Mizar community has never been clarified. In 2010, after many
years of loose deliberations, the community decided to investigate the issue of
licensing the content of the MML, thereby clarifying and crystallizing the
status of the texts, the text's authors, and the library's long-term
maintainers. The community has settled on a copyright and license policy that
suits the peculiar features of Mizar and its community. In this paper we
discuss the copyright and license solutions. We offer our experience in the
hopes that the communities of other libraries of formalised mathematical
knowledge might take up the legal and scientific problems that we addressed for
Mizar.
"
394,Edit wars in Wikipedia,"  We present a new, efficient method for automatically detecting severe
conflicts `edit wars' in Wikipedia and evaluate this method on six different
language WPs. We discuss how the number of edits, reverts, the length of
discussions, the burstiness of edits and reverts deviate in such pages from
those following the general workflow, and argue that earlier work has
significantly over-estimated the contentiousness of the Wikipedia editing
process.
"
395,"mizar-items: Exploring fine-grained dependencies in the Mizar
  Mathematical Library","  The Mizar Mathematical Library (MML) is a rich database of formalized
mathematical proofs (see http://mizar.org). Owing to its large size (it
contains more than 1100 ""articles"" summing to nearly 2.5 million lines of text,
expressing more than 50000 theorems and 10000 definitions using more than 7000
symbols), the nature of its contents (the MML is slanted toward pure
mathematics), and its classical foundations (first-order logic, set theory,
natural deduction), the MML is an especially attractive target for research on
foundations of mathematics. We have implemented a system, mizar-items, on which
a variety of such foundational experiements can be based. The heart of
mizar-items is a method for decomposing the contents of the MML into
fine-grained ""items"" (e.g., theorem, definition, notation, etc.) and computing
dependency relations among these items. mizar-items also comes equipped with a
website for exploring these dependencies and interacting with them.
"
396,"Identifying Overlapping and Hierarchical Thematic Structures in Networks
  of Scholarly Papers: A Comparison of Three Approaches","  We implemented three recently proposed approaches to the identification of
overlapping and hierarchical substructures in graphs and applied the
corresponding algorithms to a network of 492 information-science papers coupled
via their cited sources. The thematic substructures obtained and overlaps
produced by the three hierarchical cluster algorithms were compared to a
content-based categorisation, which we based on the interpretation of titles
and keywords. We defined sets of papers dealing with three topics located on
different levels of aggregation: h-index, webometrics, and bibliometrics. We
identified these topics with branches in the dendrograms produced by the three
cluster algorithms and compared the overlapping topics they detected with one
another and with the three pre-defined paper sets. We discuss the advantages
and drawbacks of applying the three approaches to paper networks in research
fields.
"
397,"A small world of citations? The influence of collaboration networks on
  citation practices","  This paper examines the proximity of authors to those they cite using degrees
of separation in a co-author network, essentially using collaboration networks
to expand on the notion of self-citations. While the proportion of direct
self-citations (including co-authors of both citing and cited papers) is
relatively constant in time and across specialties in the natural sciences (10%
of citations) and the social sciences (20%), the same cannot be said for
citations to authors who are members of the co-author network. Differences
between fields and trends over time lie not only in the degree of co-authorship
which defines the large-scale topology of the collaboration network, but also
in the referencing practices within a given discipline, computed by defining a
propensity to cite at a given distance within the collaboration network.
Overall, there is little tendency to cite those nearby in the collaboration
network, excluding direct self-citations. By analyzing these social references,
we characterize the social capital of local collaboration networks in terms of
the knowledge production within scientific fields. These results have
implications for the long-standing debate over biases common to most types of
citation analysis, and for understanding citation practices across scientific
disciplines over the past 50 years. In addition, our findings have important
practical implications for the availability of 'arm's length' expert reviewers
of grant applications and manuscripts.
"
398,"Evolutionary Dynamics of Scientific Collaboration Networks: Multi-Levels
  and Cross-time Analysis","  Several studies exist which use scientific literature for comparing
scientific activities (e.g., productivity, and collaboration). In this study,
using co-authorship data over the last 40 years, we present the evolutionary
dynamics of multi level (i.e., individual, institutional and national)
collaboration networks for exploring the emergence of collaborations in the
research field of ""steel structures"". The collaboration network of scientists
in the field has been analyzed using author affiliations extracted from Scopus
between 1970 and 2009. We have studied collaboration distribution networks at
the micro-, meso- and macro-levels for the 40 years. We compared and analyzed a
number of properties of these networks (i.e., density, centrality measures, the
giant component and clustering coefficient) for presenting a longitudinal
analysis and statistical validation of the evolutionary dynamics of ""steel
structures"" collaboration networks. At all levels, the scientific
collaborations network structures were central considering the closeness
centralization while betweenness and degree centralization were much lower. In
general networks density, connectedness, centralization and clustering
coefficient were highest in marco-level and decreasing as the network size grow
to the lowest in micro-level. We also find that the average distance between
countries about two and institutes five and for authors eight meaning that only
about eight steps are necessary to get from one randomly chosen author to
another.
"
399,Tracing the Evolution of Physics on the Backbone of Citation Networks,"  Many innovations are inspired by past ideas in a non-trivial way. Tracing
these origins and identifying scientific branches is crucial for research
inspirations. In this paper, we use citation relations to identify the
descendant chart, i.e. the family tree of research papers. Unlike other
spanning trees which focus on cost or distance minimization, we make use of the
nature of citations and identify the most important parent for each
publication, leading to a tree-like backbone of the citation network. Measures
are introduced to validate the backbone as the descendant chart. We show that
citation backbones can well characterize the hierarchical and fractal structure
of scientific development, and lead to accurate classification of fields and
sub-fields.
"
400,It was twenty years ago today ...,"  To mark the 20th anniversary of the (14 Aug 1991) commencement of
hep-th@xxx.lanl.gov (now arXiv.org), I've adapted this article from one that
first appeared in Physics World (2008), was later reprinted (with permission)
in Learned Publishing (2009), but never appeared in arXiv. I trace some
historical context and early development of the resource, its later trajectory,
and close with some thoughts about the future.
  This version is closer to my original draft, with some updates for this
occasion, plus an astounding $2^5$ added footnotes.
"
401,"Influence, originality and similarity in directed acyclic graphs","  We introduce a framework for network analysis based on random walks on
directed acyclic graphs where the probability of passing through a given node
is the key ingredient. We illustrate its use in evaluating the mutual influence
of nodes and discovering seminal papers in a citation network. We further
introduce a new similarity metric and test it in a simple personalized
recommendation process. This metric's performance is comparable to that of
classical similarity metrics, thus further supporting the validity of our
framework.
"
402,The inconsistency of the h-index,"  The h-index is a popular bibliometric indicator for assessing individual
scientists. We criticize the h-index from a theoretical point of view. We argue
that for the purpose of measuring the overall scientific impact of a scientist
(or some other unit of analysis) the h-index behaves in a counterintuitive way.
In certain cases, the mechanism used by the h-index to aggregate publication
and citation statistics into a single number leads to inconsistencies in the
way in which scientists are ranked. Our conclusion is that the h-index cannot
be considered an appropriate indicator of a scientist's overall scientific
impact. Based on recent theoretical insights, we discuss what kind of
indicators can be used as an alternative to the h-index. We pay special
attention to the highly cited publications indicator. This indicator has a lot
in common with the h-index, but unlike the h-index it does not produce
inconsistent rankings.
"
403,"Extracting, Transforming and Archiving Scientific Data","  It is becoming common to archive research datasets that are not only large
but also numerous. In addition, their corresponding metadata and the software
required to analyse or display them need to be archived. Yet the manual
curation of research data can be difficult and expensive, particularly in very
large digital repositories, hence the importance of models and tools for
automating digital curation tasks. The automation of these tasks faces three
major challenges: (1) research data and data sources are highly heterogeneous,
(2) future research needs are difficult to anticipate, (3) data is hard to
index. To address these problems, we propose the Extract, Transform and Archive
(ETA) model for managing and mechanizing the curation of research data.
Specifically, we propose a scalable strategy for addressing the research-data
problem, ranging from the extraction of legacy data to its long-term storage.
We review some existing solutions and propose novel avenues of research.
"
404,The Information Flow Framework: A Descriptive Category Metatheory,"  The Information Flow Framework (IFF) is a descriptive category metatheory. It
is an experiment in foundations, which follows a bottom-up approach to logical
description. The IFF forms the structural aspect of the IEEE P1600.1 Standard
Upper Ontology (SUO) project. The categorical approach of the IFF provides a
principled framework for the modular design of object-level ontologies. The IFF
represents meta-logic, and as such operates at the structural level of
ontologies. In the IFF, there is a precise boundary between the metalevel and
the object level. The modular architecture of the IFF consists of metalevels,
namespaces and meta-ontologies. Each metalevel services the levels below by
providing a metalanguage used to declare and axiomatize those levels.
Corresponding to the metalevels are nested metalanguages, where each
metalanguage axiomatization includes specialization of the one immediately
above. Within each metalevel, the terminology is partitioned into namespaces,
and various namespaces are collected together into meaningful composites called
meta-ontologies. All of the various meta-ontologies in the IFF are anchored to
the IFF metastack. The IFF development is largely driven by the principles of
conceptual warrant, categorical design and institutional logic.
"
405,"The relationship between acquaintanceship and coauthorship in scientific
  collaboration networks","  This article examines the relationship between acquaintanceship and
coauthorship patterns in a multi-disciplinary, multi-institutional,
geographically distributed research center. Two social networks are constructed
and compared: a network of coauthorship, representing how researchers write
articles with one another, and a network of acquaintanceship, representing how
those researchers know each other on a personal level, based on their responses
to an online survey. Statistical analyses of the topology and community
structure of these networks point to the importance of small-scale, local,
personal networks predicated upon acquaintanceship for accomplishing
collaborative work in scientific communities.
"
406,"Publication patterns of award-winning forest scientists and implications
  for the ERA journal ranking","  Publication patterns of 79 forest scientists awarded major international
forestry prizes during 1990-2010 were compared with the journal classification
and ranking promoted as part of the 'Excellence in Research for Australia'
(ERA) by the Australian Research Council. The data revealed that these
scientists exhibited an elite publication performance during the decade before
and two decades following their first major award. An analysis of their 1703
articles in 431 journals revealed substantial differences between the journal
choices of these elite scientists and the ERA classification and ranking of
journals. Implications from these findings are that additional
cross-classifications should be added for many journals, and there should be an
adjustment to the ranking of several journals relevant to the ERA Field of
Research classified as 0705 Forestry Sciences.
"
407,"On the shoulders of students? The contribution of PhD students to the
  advancement of knowledge","  Using the participation in peer reviewed publications of all doctoral
students in Quebec over the 2000-2007 period this paper provides the first
large scale analysis of their research effort. It shows that PhD students
contribute to about a third of the publication output of the province, with
doctoral students in the natural and medical sciences being present in a higher
proportion of papers published than their colleagues of the social sciences and
humanities. Collaboration is an important component of this socialization:
disciplines in which student collaboration is higher are also those in which
doctoral students are the most involved in peer-reviewed publications. In terms
of scientific impact, papers co-signed by doctorate students obtain
significantly lower citation rates than other Quebec papers, except in natural
sciences and engineering. Finally, this paper shows that involving doctoral
students in publications is positively linked with degree completion and
ulterior career in research.
"
408,A Rejoinder on Energy versus Impact Indicators,"  Citation distributions are so skewed that using the mean or any other central
tendency measure is ill-advised. Unlike G. Prathap's scalar measures (Energy,
Exergy, and Entropy or EEE), the Integrated Impact Indicator (I3) is based on
non-parametric statistics using the (100) percentiles of the distribution.
Observed values can be tested against expected ones; impact can be qualified at
the article level and then aggregated.
"
409,Semantic Integration in the IFF,"  The IEEE P1600.1 Standard Upper Ontology (SUO) project aims to specify an
upper ontology that will provide a structure and a set of general concepts upon
which domain ontologies could be constructed. The Information Flow Framework
(IFF), which is being developed under the auspices of the SUO Working Group,
represents the structural aspect of the SUO. The IFF is based on category
theory. Semantic integration of object-level ontologies in the IFF is
represented with its fusion construction. The IFF maintains ontologies using
powerful composition primitives, which includes the fusion construction.
"
410,ATP and Presentation Service for Mizar Formalizations,"  This paper describes the Automated Reasoning for Mizar (MizAR) service, which
integrates several automated reasoning, artificial intelligence, and
presentation tools with Mizar and its authoring environment. The service
provides ATP assistance to Mizar authors in finding and explaining proofs, and
offers generation of Mizar problems as challenges to ATP systems. The service
is based on a sound translation from the Mizar language to that of first-order
ATP systems, and relies on the recent progress in application of ATP systems in
large theories containing tens of thousands of available facts. We present the
main features of MizAR services, followed by an account of initial experiments
in finding proofs with the ATP assistance. Our initial experience indicates
that the tool offers substantial help in exploring the Mizar library and in
preparing new Mizar articles.
"
411,"Beyond the Boundaries of Open, Closed and Pirate Archives: Lessons from
  a Hybrid Approach","  The creation of open archives i.e. archives where access is regulated by open
licensing models (content, source, data), should be seen as part of a broader
socio-economic phenomenon that finds legal expression in specific
organizational and technical formats.This paper examines the origins and main
characteristics of the open archives phenomenon. We investigate the extent to
which different models of production of economic or social value can be
expressed in different forms of licensing in the context of open archives.
Through this process, we assess the extent to which the digital archive is
moving towards providing access that is deeper (meaning, that offers more
access rights) and wider (in the sense that most of the information given is in
open content licensing) or face a gradual stratification and polarization of
the content. Such stratification entails the emergence of two types of content:
content to which access is extremely limited and content to which access
remains completely open. This differentiation between classes of content is the
result of multiple factors: from purely legislative, administrative and
contractual restrictions (e.g. data protection and confidentiality
restrictions) to information economics (e.g. peer production) or social
(minimum universal access).
  We claim that with respect to the access management model, most of the
current archiving processes include elements of openness. Usually, this is the
result of economic necessity expressed in licensing instruments or
organisational arrangements. The viability and the socio-economic importance of
the digital archives also contributes to the use of open archiving practices.
In such a context, although pure forms of open digital archives may remain an
ideal, the reality of hybrid open digital archives is a necessity.
"
412,The Information Flow Framework: New architecture,"  This presentation discusses a new, modular, more mature architecture for the
Information Flow Framework (IFF). The IFF uses institution theory as a
foundation for the semantic integration of ontologies. It represents metalogic,
and as such operates at the structural level of ontologies. The content, form
and experience of the IFF could contribute to the development of a standard
ontology for category theory. The foundational aspect of the IFF helps to
explain the relationship between the fundamental concepts of set theory and
category theory. The development of the IFF follows two design principles:
conceptual warrant and categorical design. Both are limitations of the logical
expression. Conceptual warrant limits the content of logical expression, by
requiring us to justify the introduction of new terminology (and attendant
axiomatizations). Categorical design limits the form of logical expression (of
all mathematical concepts and constraints) to atomic expressions: declarations,
equations or relational expressions. The IFF is a descriptive category
metatheory. It is descriptive, since it follows the principle of conceptual
warrant; it is categorical, since it follows the principle of categorical
design; and it is a metatheory, since it provides a framework for all theories.
"
413,"C-Rank: A Link-based Similarity Measure for Scientific Literature
  Databases","  As the number of people who use scientific literature databases grows, the
demand for literature retrieval services has been steadily increased. One of
the most popular retrieval services is to find a set of papers similar to the
paper under consideration, which requires a measure that computes similarities
between papers. Scientific literature databases exhibit two interesting
characteristics that are different from general databases. First, the papers
cited by old papers are often not included in the database due to technical and
economic reasons. Second, since a paper references the papers published before
it, few papers cite recently-published papers. These two characteristics cause
all existing similarity measures to fail in at least one of the following
cases: (1) measuring the similarity between old, but similar papers, (2)
measuring the similarity between recent, but similar papers, and (3) measuring
the similarity between two similar papers: one old, the other recent. In this
paper, we propose a new link-based similarity measure called C-Rank, which uses
both in-link and out-link by disregarding the direction of references. In
addition, we discuss the most suitable normalization method for scientific
literature databases and propose an evaluation method for measuring the
accuracy of similarity measures. We have used a database with real-world papers
from DBLP and their reference information crawled from Libra for experiments
and compared the performance of C-Rank with those of existing similarity
measures. Experimental results show that C-Rank achieves a higher accuracy than
existing similarity measures.
"
414,"Which cities' paper output and citation impact are above expectation in
  information science? Some improvements of our previous mapping approaches","  Bornmann and Leydesdorff (in press) proposed methods based on Web-of-Science
data to identify field-specific excellence in cities where highly-cited papers
were published more frequently than can be expected. Top performers in output
are cities in which authors are located who publish a number of highly-cited
papers that is statistically significantly higher than can be expected for
these cities. Using papers published between 1989 and 2009 in information
science improvements to the methods of Bornmann and Leydesdorff (in press) are
presented and an alternative mapping approach based on the indicator I3 is
introduced here. The I3 indicator was introduced by Leydesdorff and Bornmann
(in press).
"
415,"Quantifying the influence of scientists and their publications:
  Distinguish prestige from popularity","  The number of citations is a widely used metric to evaluate the scientific
credit of papers, scientists and journals. However, it does happen that a paper
with fewer citations from prestigious scientists is of higher influence than
papers with more citations. In this paper, we argue that from whom the paper is
being cited is of higher significance than merely the number of received
citations. Accordingly, we propose an interactive model on author-paper
bipartite networks as well as an iterative algorithm to get better rankings for
scientists and their publications. The main advantage of this method is
twofold: (i) it is a parameter-free algorithm; (ii) it considers the
relationship between the prestige of scientists and the quality of their
publications. We conducted real experiments on publications in econophysics,
and applied this method to evaluate the influences of related scientific
journals. The comparisons between the rankings by our method and simple
citation counts suggest that our method is effective to distinguish prestige
from popularity.
"
416,Conceptual Knowledge Markup Language: The central core,"  The conceptual knowledge framework OML/CKML needs several components for a
successful design. One important, but previously overlooked, component is the
central core of OML/CKML. The central core provides a theoretical link between
the ontological specification in OML and the conceptual knowledge
representation in CKML. This paper discusses the formal semantics and syntactic
styles of the central core, and also the important role it plays in defining
interoperability between OML/CKML, RDF/S and Ontolingua.
"
417,The use of microblogging for field-based scientific research,"  Documenting the context in which data are collected is an integral part of
the scientific research lifecycle. In field-based research, contextual
information provides a detailed description of scientific practices and thus
enables data interpretation and reuse. For field data, losing contextual
information often means losing the data altogether. Yet, documenting the
context of distributed, collaborative, field-based research can be a
significant challenge due to the unpredictable nature of real-world settings
and to the high degree of variability in data collection methods and scientific
practices of different researchers. In this article, we propose the use of
microblogging as a mechanism to support collection, ingestion, and publication
of contextual information about the variegated digital artifacts that are
produced in field research. We perform interviews with scholars involved in
field-based environmental and urban sensing research, to determine the extent
of adoption of Twitter and similar microblogging platforms and their potential
use for field-specific research applications. Based on the results of these
interviews as well as participant observation of field activities, we present
the design, development, and pilot evaluation of a microblogging application
integrated with an existing data collection platform on a handheld device. We
investigate whether microblogging accommodates the variable and unpredictable
nature of highly mobile research and whether it represents a suitable mechanism
to document the context of field research data early in the scientific
information lifecycle.
"
418,"Digital Libraries, Conceptual Knowledge Systems, and the Nebula
  Interface","  Concept Analysis provides a principled approach to effective management of
wide area information systems, such as the Nebula File System and Interface.
This not only offers evidence to support the assertion that a digital library
is a bounded collection of incommensurate information sources in a logical
space, but also sheds light on techniques for collaboration through coordinated
access to the shared organization of knowledge.
"
419,Text mining and visualization using VOSviewer,"  VOSviewer is a computer program for creating, visualizing, and exploring
bibliometric maps of science. In this report, the new text mining functionality
of VOSviewer is presented. A number of examples are given of applications in
which VOSviewer is used for analyzing large amounts of text data.
"
420,"An Evaluation of Impacts in ""Nanoscience & nanotechnology:"" Steps
  towards standards for citation analysis","  One is inclined to conceptualize impact in terms of citations per
publication, and thus as an average. However, citation distributions are
skewed, and the average has the disadvantage that the number of publications is
used in the denominator. Using hundred percentiles, one can integrate the
normalized citation curve and develop an indicator that can be compared across
document sets because percentile ranks are defined at the article level. I
apply this indicator to the set of 58 journals in the ISI Subject Category of
""Nanoscience & nanotechnology,"" and rank journals, countries, cities, and
institutes using non-parametric statistics. The significance levels of results
can thus be indicated. The results are first compared with the ISI-Impact
Factors, but this Integrated Impact Indicator (I3) can be used with any set
downloaded from the (Social) Science Citation Index. The software is made
publicly available at the Internet. Visualization techniques are also specified
for evaluation by positioning institutes on Google Map overlays.
"
421,"Dependencies in Formal Mathematics: Applications and Extraction for Coq
  and Mizar","  Two methods for extracting detailed formal dependencies from the Coq and
Mizar system are presented and compared. The methods are used for dependency
extraction from two large mathematical repositories: the Coq Repository at
Nijmegen and the Mizar Mathematical Library. Several applications of the
detailed dependency analysis are described and proposed. Motivated by the
different applications, we discuss the various kinds of dependencies that we
are interested in,and the suitability of various dependency extraction methods.
"
422,Viva the h-index,"  In their article 'The inconsistency of the h-index' Ludo Waltman and Nees Jan
van Neck give three examples to demonstrate the inconsistency of the h-index.
As will be explained, a little extension of their examples just illustrate the
opposite, a stable feature of the h-index. For starting authors it, the h-index
that is, focusses on the number of articles; for experienced authors its focus
shifts towards the citation scores. This feature may be liked or not but does
not make the h-index an inconsistent and inappropriate indicator, as the
authors claim.
"
423,Temporal effects in the growth of networks,"  We show that to explain the growth of the citation network by preferential
attachment (PA), one has to accept that individual nodes exhibit heterogeneous
fitness values that decay with time. While previous PA-based models assumed
either heterogeneity or decay in isolation, we propose a simple analytically
treatable model that combines these two factors. Depending on the input
assumptions, the resulting degree distribution shows an exponential, log-normal
or power-law decay, which makes the model an apt candidate for modeling a wide
range of real systems.
"
424,Editorial process in scientific journals: analysis and modeling,"  The editorial handling of papers in scientific journals as a human activity
process is considered. Using recently proposed approaches of human dynamics
theory we examine the probability distributions of random variables reflecting
the temporal characteristics of studied processes. The first part of this paper
contains our results of analysis of the real data about papers published in
scientific journals. The second part is devoted to modeling of time-series
connected with editorial work. The purpose of our work is to present new object
that can be studied in terms of human dynamics theory and to corroborate the
scientometrical application of the results obtained.
"
425,The Ten Thousand Kims,"  In the Korean culture the family members are recorded in special family
books. This makes it possible to follow the distribution of Korean family names
far back in history. It is here shown that these name distributions are well
described by a simple null model, the random group formation (RGF) model. This
model makes it possible to predict how the name distributions change and these
predictions are shown to be borne out. In particular, the RGF model predicts
that, for married women entering a collection of family books in a certain
year, the occurrence of the most common family name ""Kim"" should be directly
proportional the total number of married women with the same proportionality
constant for all the years. This prediction is also borne out to high degree.
We speculate that it reflects some inherent social stability in the Korean
culture. In addition, we obtain an estimate of the total population of the
Korean culture down to year 500 AD, based on the RGF model and find about ten
thousand Kims.
"
426,"OntologyNavigator: WEB 2.0 scalable ontology based CLIR portal to IT
  scientific corpus for researchers","  This work presents the architecture used in the ongoing OntologyNavigator
project. It is a research tool to help advanced learners to find adapted IT
papers to create scientific bibliographies. The purpose is the use of an IT
representation as educational research software for researchers. We use an
ontology based on the ACM's Computing Classification System in order to find
scientific papers directly related to the new researcher's domain without any
formal request. An ontology translation in French is automatically proposed and
can be based on Web 2.0 enhanced by a community of users. A visualization and
navigation model is proposed to make it more accessible and examples are given
to show the interface of the tool. This model offers the possibility of cross
language query. Users deeply interact with the translation by providing
alternative translation of the node label. Customers also enrich the ontology
node labels with implicit descriptors.
"
427,"Rapid, Impartial and Comprehensive (RIC) publishing: A new concept for
  scientific journals","  Publishing scientific journals governed by editors relying on anonymous peer
reviewing is slow (even one round of reviewing involves several communications
between authors, editor and reviewers), partial (arguments of authors can
hardly overrule those of reviewers) and not using all available scientific
material (even the most thorough and insightful reviews remain for the eyes of
authors and editors only). Here I propose a new concept for scientific journals
that ensures rapid, impartial and comprehensive (RIC) publishing. RIC concept
is based on implementation of two novel publishing principles: the first
(rapid) editorial screening of a submitted manuscript should result in its
either ""rejection"" or ""acceptance with optional revisions"", and, in the latter
case, the optionally revised (taking into account open reviews) paper should be
published along with all (positive and negative) reviews, presenting thereby to
the scientific community all available scientific material on the topic in
question.
"
428,Chandra Publication Statistics,"  In this study we develop and propose publication metrics, based on an
analysis of data from the Chandra bibliographic database, that are more
meaningful and less sensitive to observatory-specific characteristics than the
traditional metrics. They fall in three main categories: speed of publication;
fraction of observing time published; and archival usage. Citation of results
is a fourth category, but lends itself less well to definite statements. For
Chandra, the median time from observation to publication is 2.36 years; after
about 7 years 90% of the observing time is published; after 10 years 70% of the
observing time is published more than twice; and the total annual publication
output of the mission is 60-70% of the cumulative observing time available,
assuming a two year lag between data retrieval and publication.
"
429,"Product Review Summarization based on Facet Identification and Sentence
  Clustering","  Product review nowadays has become an important source of information, not
only for customers to find opinions about products easily and share their
reviews with peers, but also for product manufacturers to get feedback on their
products. As the number of product reviews grows, it becomes difficult for
users to search and utilize these resources in an efficient way. In this work,
we build a product review summarization system that can automatically process a
large collection of reviews and aggregate them to generate a concise summary.
More importantly, the drawback of existing product summarization systems is
that they cannot provide the underlying reasons to justify users' opinions. In
our method, we solve this problem by applying clustering, prior to selecting
representative candidates for summarization.
"
430,"World Shares of Publications of the USA, EU-27, and China Compared and
  Predicted using the New Interface of the Web-of-Science versus Scopus","  The new interface of the Web of Science (of Thomson Reuters) enables users to
retrieve sets larger than 100,000 documents in a single search. This makes it
possible to compare publication trends for China, the USA, EU-27, and a number
of smaller countries. China no longer grew exponentially during the 2000s, but
linearly. Contrary to previous predictions on the basis of exponential growth
or Scopus data, the cross-over of the lines for China and the USA is postponed
to the next decade (after 2020) according to this data. These long
extrapolations, however, should be used only as indicators and not as
predictions. Along with the dynamics in the publication trends, one also has to
take into account the dynamics of the databases used for the measurement.
"
431,Characterizing and modeling citation dynamics,"  Citation distributions are crucial for the analysis and modeling of the
activity of scientists. We investigated bibliometric data of papers published
in journals of the American Physical Society, searching for the type of
function which best describes the observed citation distributions. We used the
goodness of fit with Kolmogorov-Smirnov statistics for three classes of
functions: log-normal, simple power law and shifted power law. The shifted
power law turns out to be the most reliable hypothesis for all citation
networks we derived, which correspond to different time spans. We find that
citation dynamics is characterized by bursts, usually occurring within a few
years since publication of a paper, and the burst size spans several orders of
magnitude. We also investigated the microscopic mechanisms for the evolution of
citation networks, by proposing a linear preferential attachment with time
dependent initial attractiveness. The model successfully reproduces the
empirical citation distributions and accounts for the presence of citation
bursts as well.
"
432,"The new Excellence Indicator in the World Report of the SCImago
  Institutions Rankings 2011","  The new excellence indicator in the World Report of the SCImago Institutions
Rankings (SIR) makes it possible to test differences in the ranking in terms of
statistical significance. For example, at the 17th position of these rankings,
UCLA has an output of 37,994 papers with an excellence indicator of 28.9.
Stanford University follows at the 19th position with 37,885 papers and 29.1
excellence, and z = - 0.607. The difference between these two institution thus
is not statistically significant. We provide a calculator at
http://www.leydesdorff.net/scimago11/scimago11.xls in which one can fill out
this test for any two institutions and also for each institution on whether its
score is significantly above or below expectation (assuming that 10% of the
papers are for stochastic reasons in the top-10% set).
"
433,Semantic Technology to Exploit Digital Content Exposed as Linked Data,"  The paper illustrates the research result of the application of semantic
technology to ease the use and reuse of digital contents exposed as Linked Data
on the web. It focuses on the specific issue of explorative research for the
resource selection: a context dependent semantic similarity assessment is
proposed in order to compare datasets annotated through terminologies exposed
as Linked Data (e.g. habitats, species). Semantic similarity is shown as a
building block technology to sift linked data resources. From semantic
similarity application, we derived a set of recommendations underlying open
issues in scaling the similarity assessment up to the Web of Data.
"
434,"The CHRONIOUS Ontology-Driven Search Tool: Enabling Access to Focused
  and Up-to-Date Healthcare Literature","  This paper presents an advanced search engine prototype for bibliography
retrieval developed within the CHRONIOUS European IP project of the seventh
Framework Program (FP7). This search engine is specifically targeted to
clinicians and healthcare practitioners searching for documents related to
Chronic Obstructive Pulmonary Disease (COPD) and Chronic Kidney Disease (CKD).
To this aim, the presented tool exploits two pathology-specific ontologies that
allow focused document indexing and retrieval. These ontologies have been
developed on the top of the Middle Layer Ontology for Clinical Care (MLOCC),
which provides a link with the Basic Formal Ontology, a foundational ontology
used in the Open Biological and Biomedical Ontologies (OBO) Foundry. In
addition link with the terms of the MeSH (Medical Subject Heading) thesaurus
has been provided to guarantee the coverage with the general certified medical
terms and multilingual capabilities.
"
435,"Notas metodol\'ogicas para cubrir la etapa de documentar una
  investigaci\'on","  The search process of scientific articles (papers) and review articles
(reviews) is one of the pillars of the scientific world, and is performed by
people in the research as well as for people who want to keep abreast specific
topics. Scopus (there are other databases) or Google Scholar are proposed
options to find articles, but is recommended by Scopus its extensive database
and its versatility in the search options it offers. This paper proposes is a
plan that allows a systematic search and keep the items in an orderly,
consistent and coherent within own repository for cataloging and consultation,
which will serve for many tasks to establish the state of the art of a topic,
staff training in an area and/or writing articles, among others.
"
436,"The Myth of Global Science Collaboration - Collaboration patterns in
  epistemic communities","  Scientific collaboration is often perceived as a joint global process that
involves researchers worldwide, regardless of their place of work and
residence. Globalization of science, in this respect, implies that
collaboration among scientists takes place along the lines of common topics and
irrespective of the spatial distances between the collaborators. The networks
of collaborators, termed 'epistemic communities', should thus have a
space-independent structure. This paper shows that such a notion of globalized
scientific collaboration is not supported by empirical data. It introduces a
novel approach of analyzing distance-dependent probabilities of collaboration.
The results of the analysis of six distinct scientific fields reveal that
intra-country collaboration is about 10-50 times more likely to occur than
international collaboration. Moreover, strong dependencies exist between
collaboration activity (measured in co-authorships) and spatial distance when
confined to national borders. However, the fact that distance becomes
irrelevant once collaboration is taken to the international scale suggests a
globalized science system that is strongly influenced by the gravity of local
science clusters. The similarity of the probability functions of the six
science fields analyzed suggests a universal mode of spatial governance that is
independent from the mode of knowledge creation in science.
"
437,"Universality of Performance Indicators based on Citation and Reference
  Counts","  We find evidence for the universality of two relative bibliometric indicators
of the quality of individual scientific publications taken from different data
sets. One of these is a new index that considers both citation and reference
counts. We demonstrate this universality for relatively well cited publications
from a single institute, grouped by year of publication and by faculty or by
department. We show similar behaviour in publications submitted to the arXiv
e-print archive, grouped by year of submission and by sub-archive. We also find
that for reasonably well cited papers this distribution is well fitted by a
lognormal with a variance of around 1.3 which is consistent with the results of
Radicchi, Fortunato, and Castellano (2008). Our work demonstrates that
comparisons can be made between publications from different disciplines and
publication dates, regardless of their citation count and without expensive
access to the whole world-wide citation graph. Further, it shows that averages
of the logarithm of such relative bibliometric indices deal with the issue of
long tails and avoid the need for statistics based on lengthy ranking
procedures.
"
438,Evaluating the SharedCanvas Manuscript Data Model in CATCHPlus,"  In this paper, we present the SharedCanvas model for describing the layout of
culturally important, hand-written objects such as medieval manuscripts, which
is intended to be used as a common input format to presentation interfaces. The
model is evaluated using two collections from CATCHPlus not consulted during
the design phase, each with their own complex requirements, in order to
determine if further development is required or if the model is ready for
general usage. The model is applied to the new collections, revealing several
new areas of concern for user interface production and discovery of the
constituent resources. However, the fundamental information modelling aspects
of SharedCanvas and the underlying Open Annotation Collaboration ontology are
demonstrated to be sufficient to cover the challenging new requirements. The
distributed, Linked Open Data approach is validated as an important methodology
to seamlessly allow simultaneous interaction with multiple repositories, and at
the same time to facilitate both scholarly commentary and crowd-sourcing of the
production of transcriptions.
"
439,The large-scale structure of journal citation networks,"  We analyse the large-scale structure of the journal citation network built
from information contained in the Thomson-Reuters Journal Citation Reports. To
this end, we take advantage of the network science paraphernalia and explore
network properties like density, percolation robustness, average and largest
node distances, reciprocity, incoming and outgoing degree distributions, as
well as assortative mixing by node degrees. We discover that the journal
citation network is a dense, robust, small, and reciprocal world. Furthermore,
in and out node degree distributions display long-tails, with few vital
journals and many trivial ones, and they are strongly positively correlated.
"
440,Knowledge Organization Research in the last two decades: 1988-2008,"  We apply an automatic topic mapping system to records of publications in
knowledge organization published between 1988-2008. The data was collected from
journals publishing articles in the KO field from Web of Science database
(WoS). The results showed that while topics in the first decade (1988-1997)
were more traditional, the second decade (1998-2008) was marked by a more
technological orientation and by the appearance of more specialized topics
driven by the pervasiveness of the Web environment.
"
441,IVOA Recommendation: IVOA Support Interfaces,"  This document describes the minimum interface that a (SOAP- or REST-based)
web service requires to participate in the IVOA. Note that this is not required
of standard VO services developed prior to this specification, although uptake
is strongly encouraged on any subsequent revision. All new standard VO
services, however, must feature a VOSI-compliant interface.
  This document has been produced by the Grid and Web Services Working Group.
It has been reviewed by IVOA Members and other interested parties, and has been
endorsed by the IVOA Executive Committee as an IVOA Recommendation. It is a
stable document and may be used as reference material or cited as a normative
reference from another document. IVOA's role in making the Recommendation is to
draw attention to the specification and to promote its widespread deployment.
This enhances the functionality and interoperability inside the Astronomical
Community.
"
442,A Wikipedia Literature Review,"  This paper was originally designed as a literature review for a doctoral
dissertation focusing on Wikipedia. This exposition gives the structure of
Wikipedia and the latest trends in Wikipedia research.
"
443,Un modello di struttura dinamica per ebook scolastici,"  This article proposes a model of e-books for schools based on a graph in
which nodes represent individual subjects of a teaching program to a the
relatively low granularity, which facilitates their aggregation and
re-usability, and edges represent prerequisites between subjects (and, indeed,
between nodes). On this graph we will develop a series of simple algorithms
that allow both teachers and students to assemble an interactive and
personalized ebook that, respecting the prerequisites, it will be significant
from the point of methodological and stylistic sense. Therefore, teachers and
students do not have available a set of unrelated units neither a limited set a
few pre-packaged learning paths, as it is typical of some solutions on the Web,
but rather will have a network of topics that can be serialized in a
combinatorial vast number of alternatives, and therefore can create as many
custom ebook, but guaranteed from the point of view of the scientific
perspective.
"
444,"Using Automated Dependency Analysis To Generate Representation
  Information","  To preserve access to digital content, we must preserve the representation
information that captures the intended interpretation of the data. In
particular, we must be able to capture performance dependency requirements,
i.e. to identify the other resources that are required in order for the
intended interpretation to be constructed successfully. Critically, we must
identify the digital objects that are only referenced in the source data, but
are embedded in the performance, such as fonts. This paper describes a new
technique for analysing the dynamic dependencies of digital media, focussing on
analysing the process that underlies the performance, rather than parsing and
deconstructing the source data. This allows the results of format-specific
characterisation tools to be verified independently, and facilitates the
generation of representation information for any digital media format, even
when no suitable characterisation tool exists.
"
445,Data Preservation in High Energy Physics,"  Data from high-energy physics experiments are collected with significant
financial and human effort and are mostly unique. However, until recently no
coherent strategy existed for data preservation and re-use, and many important
and complex data sets have simply been lost. While the current focus is on the
LHC at CERN, in the current period several important and unique experimental
programs at other facilities are coming to an end, including those at HERA,
b-factories and the Tevatron. To address this issue, an inter-experimental
study group on HEP data preservation and long-term analysis (DPHEP) was
convened at the end of 2008. The group now aims to publish a full and detailed
review of the present status of data preservation in high energy physics. This
contribution summarises the results of the DPHEP study group, describing the
challenges of data preservation in high energy physics and the group's first
conclusions and recommendations. The physics motivation for data preservation,
generic computing and preservation models, technological expectations and
governance aspects at local and international levels are examined.
"
446,Universality in Bibliometrics,"  Many discussions have enlarged the literature in Bibliometrics since the
Hirsh proposal, the so called $h$-index. Ranking papers according to their
citations, this index quantifies a researcher only by its greatest possible
number of papers that are cited at least $h$ times. A closed formula for
$h$-index distribution that can be applied for distinct databases is not yet
known. In fact, to obtain such distribution, the knowledge of citation
distribution of the authors and its specificities are required. Instead of
dealing with researchers randomly chosen, here we address different groups
based on distinct databases. The first group is composed by physicists and
biologists, with data extracted from Institute of Scientific Information (ISI).
The second group composed by computer scientists, which data were extracted
from Google-Scholar system. In this paper, we obtain a general formula for the
$h$-index probability density function (pdf) for groups of authors by using
generalized exponentials in the context of escort probability. Our analysis
includes the use of several statistical methods to estimate the necessary
parameters. Also an exhaustive comparison among the possible candidate
distributions are used to describe the way the citations are distributed among
authors. The $h$-index pdf should be used to classify groups of researchers
from a quantitative point of view, which is meaningfully interesting to
eliminate obscure qualitative methods.
"
447,Linking to Data - Effect on Citation Rates in Astronomy,"  Is there a difference in citation rates between articles that were published
with links to data and articles that were not? Besides being interesting from a
purely academic point of view, this question is also highly relevant for the
process of furthering science. Data sharing not only helps the process of
verification of claims, but also the discovery of new findings in archival
data. However, linking to data still is a far cry away from being a ""practice"",
especially where it comes to authors providing these links during the writing
and submission process. You need to have both a willingness and a publication
mechanism in order to create such a practice. Showing that articles with links
to data get higher citation rates might increase the willingness of scientists
to take the extra steps of linking data sources to their publications. In this
presentation we will show this is indeed the case: articles with links to data
result in higher citation rates than articles without such links. The ADS is
funded by NASA Grant NNX09AB39G.
"
448,The ADS All-Sky Survey,"  The ADS All-Sky Survey (ADSASS) is an ongoing effort aimed at turning the
NASA Astrophysics Data System (ADS), widely known for its unrivaled value as a
literature resource for astronomers, into a data resource. The ADS is not a
data repository per se, but it implicitly contains valuable holdings of
astronomical data, in the form of images, tables and object references
contained within articles. The objective of the ADSASS effort is to extract
these data and make them discoverable and available through existing data
viewers. The resulting ADSASS data layer promises to greatly enhance workflows
and enable new research by tying astronomical literature and data assets into
one resource.
"
449,"The Disclosure of University Research for Third Parties: A Non-Market
  Perspective on an Italian University","  Nations, universities, and regional governments commit resources to promote
the dissemination of scientific and technical knowledge. One focuses on
knowledge-based innovations and the economic function of the university in
terms of technology transfer, intellectual property,
university-industry-government relations, etc. Faculties other than engineering
or applied sciences, however, may not be able to recognize opportunities in
this ""linear model"" of technology transfer. We elaborate a non-market
perspective on the third mission in terms of disclosure of the knowledge and
areas of expertise available for disclosure to other audiences at a provincial
university. The use of ICT can enhance communication between actors on the
supply and demand sides. Using an idea originally developed in the context of
the Dutch science shops, the university staff was questionnaired about keywords
and areas of expertise with the specific purpose of disclosing this information
to audiences other than academic colleagues. The results were brought online in
a thesaurus-like structure that enables users to access the university at the
level of individual email address. This model stimulates variation on both the
supply and demand side of the innovation process, and strengthens the
accessibility and embeddedness of the knowledge base in a regional economy.
"
450,"Testing the fairness of citation indicators for comparison across
  scientific domains: the case of fractional citation counts","  Citation numbers are extensively used for assessing the quality of scientific
research. The use of raw citation counts is generally misleading, especially
when applied to cross-disciplinary comparisons, since the average number of
citations received is strongly dependent on the scientific discipline of
reference of the paper. Measuring and eliminating biases in citation patterns
is crucial for a fair use of citation numbers. Several numerical indicators
have been introduced with this aim, but so far a specific statistical test for
estimating the fairness of these numerical indicators has not been developed.
Here we present a statistical method aimed at estimating the effectiveness of
numerical indicators in the suppression of citation biases. The method is
simple to implement and can be easily generalized to various scenarios. As a
practical example we test, in a controlled case, the fairness of fractional
citation count, which has been recently proposed as a tool for cross-discipline
comparison. We show that this indicator is not able to remove biases in
citation patterns and performs much worse than the rescaling of citation counts
with average values.
"
451,AstroDAbis: Annotations and Cross-Matches for Remote Catalogues,"  Astronomers are good at sharing data, but poorer at sharing knowledge.
  Almost all astronomical data ends up in open archives, and access to these is
being simplified by the development of the global Virtual Observatory (VO).
This is a great advance, but the fundamental problem remains that these
archives contain only basic observational data, whereas all the astrophysical
interpretation of that data -- which source is a quasar, which a low-mass star,
and which an image artefact -- is contained in journal papers, with very little
linkage back from the literature to the original data archives. It is therefore
currently impossible for an astronomer to pose a query like ""give me all
sources in this data archive that have been identified as quasars"" and this
limits the effective exploitation of these archives, as the user of an archive
has no direct means of taking advantage of the knowledge derived by its
previous users.
  The AstroDAbis service aims to address this, in a prototype service enabling
astronomers to record annotations and cross-identifications in the AstroDAbis
service, annotating objects in other catalogues. We have deployed two
interfaces to the annotations, namely one astronomy-specific one using the TAP
protocol}, and a second exploiting generic Linked Open Data (LOD) and RDF
techniques.
"
452,Astro-WISE Information System,"  Astro-WISE is a scientific information system for the data processing of
optical images. In this paper we review main features of Astro-WISE and
describe the current status of the system.
"
453,Astro-WISE processing of wide-field images and other data,"  Astro-WISE is the Astronomical Wide-field Imaging System for Europe. It is a
scientific information system which consists of hardware and software federated
over about a dozen institutes throughout Europe. It has been developed to
exploit the ever increasing avalanche of data produced by astronomical surveys
and data intensive scientific experiments in general.
  The demo explains the architecture of the Astro-WISE information system and
shows the use of Astro-WISE interfaces. Wide-field astronomical images are
derived from the raw image to the final catalog according to the user's
request. The demo is based on the standard Astro-WISE guided tour, which can be
accessed from the Astro-WISE website.
  The typical Astro-WISE data processing chain is shown, which can be used for
data handling for a variety of different instruments, currently 14, including
OmegaCAM, MegaCam, WFI, WFC, ACS/HST, etc.
"
454,"Architecture of a Conference Management System Providing Advanced Paper
  Assignment Features","  This paper proposes an architecture and assignment management model of a
conference management system that performs a precise and accurate automatic
assignment of reviewers to papers. The system relies on taxonomy of keywords to
describe papers and reviewers' competences. The implied hierarchical structure
of the taxonomy provides important additional information - the semantic
relationships between the separate keywords. It allows similarity measures to
take into account not only the number of exactly matching keywords between a
paper and a reviewer, but in case of non-matching ones to calculate how
semantically close they are. Reviewers are allowed to bid on the papers they
would like to (or not like to) review and to explicitly state conflicts of
interest (CoI) with papers. An automatic CoI detection is checking for
additional conflicts based on institutional affiliation, co-authorship (within
the local database) and previous co-authorship in the past (within the major
bibliographic indexes and digital libraries). The algorithm for automatic
assignment takes into account all - selected keywords, reviewers' bids and
conflicts of interest and tries to find the most accurate assignment while
maintaining load balancing among reviewers.
"
455,"A model of Cross Language Retrieval for IT domain papers through a map
  of ACM Computing Classification System","  This article presents a concept model, and the associated tool to help
advanced learners to find adapted bibliography. The purpose is the use of an IT
representation as educational research software for newcomers in research. We
use an ontology based on the ACM's Computing Classification System in order to
find scientific articles directly related to the new researcher's domain
without any formal request. An ontology translation in French is automatically
proposed and can be based on Web 2.0 enhanced by a community of users. A
visualization and navigation model is proposed to make it more accessible and
examples are given to show the interface of our tool: Ontology Navigator.
"
456,"YouASTRO: a web-based bibliography management system with distributed
  comments and rating features for SAO/NASA ADS papers","  We present a working prototype of YouASTRO (www.youastro.org), a web-based
BibTeX-compliant reference management software (RMS) for astrophysical papers
in the SAO/NASA ADS database. It also includes as a main feature the concept of
distributed paper comments and ratings. In these paper, we introduce the main
characteristics of the web application, and we will briefly discuss what could
be the advantages and drawbacks of such a system being widespread adopted by
the astrophysical community for its scientific literature.
"
457,"The DAME/VO-Neural Infrastructure: an Integrated Data Mining System
  Support for the Science Community","  Astronomical data are gathered through a very large number of heterogeneous
techniques and stored in very diversified and often incompatible data
repositories. Moreover in the e-science environment, it is needed to integrate
services across distributed, heterogeneous, dynamic ""virtual organizations""
formed by different resources within a single enterprise and/or external
resource sharing and service provider relationships. The DAME/VONeural project,
run jointly by the University Federico II, INAF (National Institute of
Astrophysics) Astronomical Observatories of Napoli and the California Institute
of Technology, aims at creating a single, sustainable, distributed
e-infrastructure for data mining and exploration in massive data sets, to be
offered to the astronomical (but not only) community as a web application. The
framework makes use of distributed computing environments (e.g. S.Co.P.E.) and
matches the international IVOA standards and requirements. The integration
process is technically challenging due to the need of achieving a specific
quality of service when running on top of different native platforms. In these
terms, the result of the DAME/VO-Neural project effort will be a
service-oriented architecture, obtained by using appropriate standards and
incorporating Grid paradigms and restful Web services frameworks where needed,
that will have as main target the integration of interdisciplinary distributed
systems within and across organizational domains.
"
458,"DAME: A Distributed Data Mining & Exploration Framework within the
  Virtual Observatory","  Nowadays, many scientific areas share the same broad requirements of being
able to deal with massive and distributed datasets while, when possible, being
integrated with services and applications. In order to solve the growing gap
between the incremental generation of data and our understanding of it, it is
required to know how to access, retrieve, analyze, mine and integrate data from
disparate sources. One of the fundamental aspects of any new generation of data
mining software tool or package which really wants to become a service for the
community is the possibility to use it within complex workflows which each user
can fine tune in order to match the specific demands of his scientific goal.
These workflows need often to access different resources (data, providers,
computing facilities and packages) and require a strict interoperability on (at
least) the client side. The project DAME (DAta Mining & Exploration) arises
from these requirements by providing a distributed WEB-based data mining
infrastructure specialized on Massive Data Sets exploration with Soft Computing
methods. Originally designed to deal with astrophysical use cases, where first
scientific application examples have demonstrated its effectiveness, the DAME
Suite results as a multi-disciplinary platform-independent tool perfectly
compliant with modern KDD (Knowledge Discovery in Databases) requirements and
Information & Communication Technology trends.
"
459,"A literature review: What exactly should we preserve? How scholars
  address this question and where is the gap","  This review addresses the question of what exactly should we preserve, and
how the digital preservation community and scholars address this question. The
paper first introduces the much-abused-term ""significant properties,"" before
revealing how some scholars are of the opinion that characteristics of digital
objects to be preserved (i.e., significant properties) can be identified and
should be expressed formally, while others are not of that opinion. The digital
preservation community's attempt to expound on the general characteristics of
digital objects and significant properties will then be discussed. Finally, the
review shows that while there may be ways to identify the technical makeup or
general characteristics of a digital object, there is currently no formal and
objective methodology to help stakeholders identify and decide what the
significant properties of the objects are. This review thus helps open
questions and generates a formative recommendation based on expert opinion that
expressing an object's functions in an explicit and formal way (using didactic
guides from the archives community) could be the solution to help stakeholders
decide what characteristics/ elements exactly we should preserve.
"
460,"Why don't we already have an Integrated Framework for the Publication
  and Preservation of all Data Products?","  Astronomy has long had a working network of archives supporting the curation
of publications and data. The discipline has already created many of the
features which perplex other areas of science: (1) data repositories:
(supra)national institutes, dedicated to large projects; a culture of
user-contributed data; practical experience of long-term data preservation; (2)
dataset identifiers: the community has already piloted experiments, knows what
can undermine these efforts, and is participating in the development of
next-generation standards; (3) citation of datasets in papers: the community
has an innovative and expanding infrastructure for the curation of data and
bibliographic resources, and through them a community of author s and editors
familiar with such electronic publication efforts; as well, it has experimented
with next-generation web standards (e.g. the Semantic Web); (4) publisher
buy-in: publishers in this area have been willing to innovate within the
constraints of their commercial imperatives. What can possibly be missing? Why
don't we have an integrated framework for the publication and preservation of
all data products already? Are there technical barriers? We don't believe so.
Are there cultural or commercial forces inhibiting this? We aren't aware of
any. This Birds of a Feather session (BoF) attempted to identify existing
barriers to the creation of such a framework, and attempted to identify the
parties or groups which can contribute to the creation of a VO-powered
data-publishing framework.
"
461,"Making QCD Lattice Data Accessible and Organized through Advanced Web
  Interfaces","  The Gauge Connection at qcd.nersc.gov is one of the most popular repositories
of QCD lattice ensembles. It is used to access 16TB of archived QCD data from
the High Performance Storage System (HPSS) at the National Energy Research
Scientific Computing Center (NERSC). Here, we present a new web interface for
qcd.nersc.gov which allows physicists to browse and search the data, as well as
download individual files or entire ensembles in batch. Our system
distinguishes itself from others because of its ease of use and web based
workflow.
"
462,Hybrid Centrality Measures for Binary and Weighted Networks,"  Existing centrality measures for social network analysis suggest the
im-portance of an actor and give consideration to actor's given structural
position in a network. These existing measures suggest specific attribute of an
actor (i.e., popularity, accessibility, and brokerage behavior). In this study,
we propose new hybrid centrality measures (i.e., Degree-Degree,
Degree-Closeness and Degree-Betweenness), by combining existing measures (i.e.,
degree, closeness and betweenness) with a proposition to better understand the
importance of actors in a given network. Generalized set of measures are also
proposed for weighted networks. Our analysis of co-authorship networks dataset
suggests significant correlation of our proposed new centrality measures
(especially weighted networks) than traditional centrality measures with
performance of the scholars. Thus, they are useful measures which can be used
instead of traditional measures to show prominence of the actors in a network.
"
463,Caveats for using statistical significance tests in research assessments,"  This paper raises concerns about the advantages of using statistical
significance tests in research assessments as has recently been suggested in
the debate about proper normalization procedures for citation indicators.
Statistical significance tests are highly controversial and numerous criticisms
have been leveled against their use. Based on examples from articles by
proponents of the use statistical significance tests in research assessments,
we address some of the numerous problems with such tests. The issues
specifically discussed are the ritual practice of such tests, their dichotomous
application in decision making, the difference between statistical and
substantive significance, the implausibility of most null hypotheses, the
crucial assumption of randomness, as well as the utility of standard errors and
confidence intervals for inferential purposes. We argue that applying
statistical significance tests and mechanically adhering to their results is
highly problematic and detrimental to critical thinking. We claim that the use
of such tests do not provide any advantages in relation to citation indicators,
interpretations of them, or the decision making processes based upon them. On
the contrary their use may be harmful. Like many other critics, we generally
believe that statistical significance tests are over- and misused in the social
sciences including scientometrics and we encourage a reform on these matters.
"
464,"Towards a Reference Model for Open Access and Knowledge Sharing, Lessons
  from Systems Research","  The Open Access Movement has been striving to grant universal unrestricted
access to the knowledge and data outputs of publicly funded research.
leveraging the real time, virtually cost free publishing opportunities offered
by the internet and the web. However, evidence suggests that in the systems
engineering domain open access policies are not widely adopted. This paper
presents the rationale, methodology and results of an evidence based inquiry
that investigates the dichotomy between policy and practice in Open Access (OA)
of systems engineering research in the UK, explores entangled dimensions of the
problem space from a socio-technical perspective, and issues a set of
recommendations, including a reference model outline for knowledge sharing in
systems research
"
465,"Legal Resources Information System for Information Agencies of
  Specialized Libraries","  In recent years, the rapid development of information technology and
communication has a strong impact to industry information - the library. The
mission of the industry when in fact the great social place to see the library
as knowledge management. Vietnam is in the process of building the rule of law
socialist orientation and improves the legal system. So in the current
development process, the law library plays an important role in the retention,
dissemination and provision of legal information service of legislative,
executive and judiciary, particularly especially research, teaching and
learning of law school. But the response of the legal information library
information agencies remains limited compared to the increasing demand of
users.
"
466,Multi-Connected Ontologies,"  Ontologies have been used for the purpose of bringing system and consistency
to subject and knowledge areas. We present a criticism of the present
mathematical structure of ontologies and indicate that they are not sufficient
in their present form to represent the many different valid expressions of a
subject knowledge domain. We propose an alternative structure for ontologies
based on a richer multi connected complex network which contains the present
ontology structure as a projection. We demonstrate how this new multi connected
ontology should be represented as an asymmetric probability matrix.
"
467,"Information Carriers and Identification of Information Objects: An
  Ontological Approach","  Even though library and archival practice, as well as Digital Preservation,
have a long tradition in identifying information objects, the question of their
precise identity under change of carrier or migration is still a riddle to
science. The objective of this paper is to provide criteria for the unique
identification of some important kinds of information objects, independent from
the kind of carrier or specific encoding. Our approach is based on the idea
that the substance of some kinds of information objects can completely be
described in terms of discrete arrangements of finite numbers of known kinds of
symbols, such as those implied by style guides for scientific journal
submissions. Our theory is also useful for selecting or describing what has to
be preserved. This is a fundamental problem since curators and archivists would
like to formally record the decisions of what has to be preserved over time and
to decide (or verify) whether a migration (transformation) preserves the
intended information content. Furthermore, it is important for reasoning about
the authenticity of digital objects, as well as for reducing the cost of
digital preservation.
"
468,ProofPeer - A Cloud-based Interactive Theorem Proving System,"  ProofPeer strives to be a system for cloud-based interactive theorem proving.
After illustrating why such a system is needed, the paper presents some of the
design challenges that ProofPeer needs to meet to succeed. Contexts are
presented as a solution to the problem of sharing proof state among the users
of ProofPeer. Chronicles are introduced as a way to organize and version
contexts.
"
469,"Distributed archive and single access system for accelerometric event
  data : a NERIES initiative","  We developed a common access facility to homogeneously formatted
accelerometric event data and to the corresponding sheet of ground motion
parameters. This paper is focused on the description of the technical
development of the accelerometric data server and the link with the
accelerometric data explorer. The server is the third node of the 3-tier
architecture of the distributed archive system for accelerometric data. The
server is the link between the data users and the accelero- metric data portal.
The server follows three main steps: (1) Reading and analysis of the end-user
request; (2) Processing and converting data; and (3) Archiving and updating the
accelerometric data explorer. This paper presents the description of the data
server and the data explorer for accessing data.
"
470,Network Analysis of the French Environmental Code,"  We perform a detailed analysis of the network constituted by the citations in
a legal code, we search for hidden structures and properties. The graph
associated to the Environmental code has a small-world structure and it is
partitioned in several hidden communities of articles that only partially
coincide with the organization of the code as given by its table of content.
Several articles are also connected with a low number of articles but are
intermediate between large communities. The structure of the Environmental Code
is contrasting with the reference network of all the French Legal Codes that
presents a rich-club of ten codes very central to the whole French legal
system, but no small-world property. This comparison shows that the structural
properties of the reference network associated to a legal system strongly
depends on the scale and granularity of the analysis, as is the case for many
complex systems
"
471,"Integrating Interactive Visualizations in the Search Process of Digital
  Libraries and IR Systems","  Interactive visualizations for exploring and retrieval have not yet become an
integral part of digital libraries and information retrieval systems. We have
integrated a set of interactive graphics in a real world social science digital
library. These visualizations support the exploration of search queries,
results and authors, can filter search results, show trends in the database and
can support the creation of new search queries. The use of weighted brushing
supports the identification of related metadata for search facets. We discuss
some use cases of the combination of IR systems and interactive graphics. In a
user study we verify that users can gain insights from statistical graphics
intuitively and can adopt interaction techniques.
"
472,"Metrics to evaluate research performance in academic institutions: A
  critique of ERA 2010 as applied in forestry and the indirect H2 index as a
  possible alternative","  Excellence for Research in Australia (ERA) is an attempt by the Australian
Research Council to rate Australian universities on a 5-point scale within 180
Fields of Research using metrics and peer evaluation by an evaluation
committee. Some of the bibliometric data contributing to this ranking suffer
statistical issues associated with skewed distributions. Other data are
standardised year-by-year, placing undue emphasis on the most recent
publications which may not yet have reliable citation patterns. The
bibliometric data offered to the evaluation committees is extensive, but lacks
effective syntheses such as the h-index and its variants. The indirect H2 index
is objective, can be computed automatically and efficiently, is resistant to
manipulation, and a good indicator of impact to assist the ERA evaluation
committees and to similar evaluations internationally.
"
473,"Impact Factor: outdated artefact or stepping-stone to journal
  certification?","  A review of Garfield's journal impact factor and its specific implementation
as the Thomson Reuters Impact Factor reveals several weaknesses in this
commonly-used indicator of journal standing. Key limitations include the
mismatch between citing and cited documents, the deceptive display of three
decimals that belies the real precision, and the absence of confidence
intervals. These are minor issues that are easily amended and should be
corrected, but more substantive improvements are needed. There are indications
that the scientific community seeks and needs better certification of journal
procedures to improve the quality of published science. Comprehensive
certification of editorial and review procedures could help ensure adequate
procedures to detect duplicate and fraudulent submissions.
"
474,"Characterizing Interdisciplinarity of Researchers and Research Topics
  Using Web Search Engines","  Researchers' networks have been subject to active modeling and analysis.
Earlier literature mostly focused on citation or co-authorship networks
reconstructed from annotated scientific publication databases, which have
several limitations. Recently, general-purpose web search engines have also
been utilized to collect information about social networks. Here we
reconstructed, using web search engines, a network representing the relatedness
of researchers to their peers as well as to various research topics.
Relatedness between researchers and research topics was characterized by
visibility boost-increase of a researcher's visibility by focusing on a
particular topic. It was observed that researchers who had high visibility
boosts by the same research topic tended to be close to each other in their
network. We calculated correlations between visibility boosts by research
topics and researchers' interdisciplinarity at individual level (diversity of
topics related to the researcher) and at social level (his/her centrality in
the researchers' network). We found that visibility boosts by certain research
topics were positively correlated with researchers' individual-level
interdisciplinarity despite their negative correlations with the general
popularity of researchers. It was also found that visibility boosts by
network-related topics had positive correlations with researchers' social-level
interdisciplinarity. Research topics' correlations with researchers'
individual- and social-level interdisciplinarities were found to be nearly
independent from each other. These findings suggest that the notion of
""interdisciplinarity"" of a researcher should be understood as a
multi-dimensional concept that should be evaluated using multiple assessment
means.
"
475,Elasticity on Ontology Matching of Folksodriven Structure Network,"  Nowadays folksonomy tags are used not just for personal organization, but for
communication and sharing between people sharing their own local interests. In
this paper is considered the new concept structure called ""Folksodriven"" to
represent folksonomies. The Folksodriven Structure Network (FSN) was thought as
folksonomy tags suggestions for the user on a dataset built on chosen websites
- based on Natural Language Processing (NLP). Morphological changes, such as
changes in folksonomy tags chose have direct impact on network connectivity
(structural plasticity) of the folksonomy tags considered. The goal of this
paper is on defining a base for a FSN plasticity theory to analyze. To perform
such goal it is necessary a systematic mathematical analysis on deformation and
fracture for the ontology matching on the FSN. The advantages of that approach
could be used on a new interesting method to be employed by a knowledge
management system.
"
476,"Technologie et pratiques bibliographiques associ\'ees \`a l'\'ecriture
  scientifique en milieu universitaire","  Observe and understand users of the Scientific and Technical Information, is
preparing to offer them appropriate services. This exploratory study provides
answers about the uses in the humanities, social sciences as well as technical
sciences. We also observe those who assist teachers in their scientific
research: librarians. Then we outline considerations and recommendations to
specify functionalities of an efficient e-Linrary.
"
477,"Alternatives to the Journal Impact Factor: I3 and the Top-10% (or
  Top-25%?) of the Most-Highly Cited Papers","  Journal Impact Factors (IFs) can be considered historically as the first
attempt to normalize citation distributions by using averages over two years.
However, it has been recognized that citation distributions vary among fields
of science and that one needs to normalize for this. Furthermore, the mean-or
any central-tendency statistics-is not a good representation of the citation
distribution because these distributions are skewed. Important steps have been
taken to solve these two problems during the last few years. First, one can
normalize at the article level using the citing audience as the reference set.
Second, one can use non-parametric statistics for testing the significance of
differences among ratings. A proportion of most-highly cited papers (the
top-10% or top-quartile) on the basis of fractional counting of the citations
may provide an alternative to the current IF. This indicator is intuitively
simple, allows for statistical testing, and accords with the state of the art.
"
478,"A further step forward in measuring journals' scientific prestige: The
  SJR2 indicator","  A new size-independent indicator of scientific journal prestige, the SJR2
indicator, is proposed. This indicator takes into account not only the prestige
of the citing scientific journal but also its closeness to the cited journal
using the cosine of the angle between the vectors of the two journals'
cocitation profiles. To eliminate the size effect, the accumulated prestige is
divided by the fraction of the journal's citable documents, thus eliminating
the decreasing tendency of this type of indicator and giving meaning to the
scores. Its method of computation is described, and the results of its
implementation on the Scopus 2008 dataset is compared with those of an ad hoc
Journal Impact Factor, JIF(3y), and SNIP, the comparison being made both
overall and within specific scientific areas. All three, the SJR2 indicator,
the SNIP indicator and the JIF distributions, were found to fit well to a
logarithmic law. Although the three metrics were strongly correlated, there
were major changes in rank. In addition, the SJR2 was distributed more
equalized than the JIF by Subject Area and almost as equalized as the SNIP, and
better than both at the lower level of Specific Subject Areas. The
incorporation of the cosine increased the values of the flows of prestige
between thematically close journals.
"
479,"Conception and Use of Ontologies for Indexing and Searching by Semantic
  Contents of Video Courses","  Nowadays, the video documents like educational courses available on the web
increases significantly. However, the information retrieval systems today can
not return to the users (students or teachers) of parts of those videos that
meet their exact needs expressed by a query consisting of semantic information.
In this paper, we present a model of pedagogical knowledge of current videos.
This knowledge is used throughout the process of indexing and semantic search
segments instructional videos. Our experimental results show that the proposed
approach is promising.
"
480,The relative significance of the H-index,"  Use of the Hirsch-index ($h$) as measure of an author's visibility in the
scientific literature has become popular as an alternative to a gross measure
like total citations (c). I show that, at least in astrophysics, $h$ correlates
tightly with overall citations. The mean relation is $h=0.5(\sqrt c+1)$.
Outliers are few and not too far from the mean, especially if `normalized' ADS
citations are used for $c$ and $h$. Whatever the theoretical reasoning behind
it, the Hirsch index in practice does not appear to measure something
significantly new.
"
481,T2Ku: Building a Semantic Wiki of Mathematics,"  We introduce T2Ku, an open source project that aims at building a semantic
wiki of mathematics featuring automated reasoning(AR) techniques. We want to
utilize AR techniques in a way that truly helps mathematical researchers solve
problems in the real world, instead of building another ambitious yet useless
system. By setting this as our objective, we exploit pragmatic design decisions
that have proven feasible in other projects, while still employs a loosely
coupled architecture to allow better inference programs to be integrated in the
future. In this paper, we state the motivations and examine state-of-the-art
systems, why we are not satisfied with those systems and how we are going to
improve. We then describe our architecture and the way we implemented the
system. We present examples showing how to use its facilities. T2Ku is an
on-going project. We conclude this paper by summarizing the development
progress and encouraging the reader to join the project.
"
482,Decomposition of the h-index,"  I introduce a decomposition of the h-index, which is nowadays the leading
criterion to assess the relevance of a scientist in his/her research field.
According to the proposed decomposition, the h-index is the product of two
indicators, the first of which measures the impact of the scientist on the
research community and the second may be seen as a measure of concentration of
the citations in correspondence of a reduced number of papers. The
decomposition is illustrated by an application based on data concerning a group
of top level economists.
"
483,ProofFlow: Flow Diagrams for Proofs,"  We present a light formalism for proofs that encodes their inferential
structure, along with a system that transforms these representations into
flow-chart diagrams. Such diagrams should improve the comprehensibility of
proofs. We discuss language syntax, diagram semantics, and our goal of building
a repository of diagrammatic representations of proofs from canonical
mathematical literature. The repository will be available online in the form of
a wiki at proofflow.org, where the flow chart drawing software will be
deployable through the wiki editor. We also consider the possibility of a
semantic tagging of the assertions in a proof, to permit data mining.
"
484,OWL: Yet to arrive on the Web of Data?,"  Seven years on from OWL becoming a W3C recommendation, and two years on from
the more recent OWL 2 W3C recommendation, OWL has still experienced only patchy
uptake on the Web. Although certain OWL features (like owl:sameAs) are very
popular, other features of OWL are largely neglected by publishers in the
Linked Data world. This may suggest that despite the promise of easy
implementations and the proposal of tractable profiles suggested in OWL's
second version, there is still no ""right"" standard fragment for the Linked Data
community. In this paper, we (1) analyse uptake of OWL on the Web of Data, (2)
gain insights into the OWL fragment that is actually used/usable on the Web,
where we arrive at the conclusion that this fragment is likely to be a
simplified profile based on OWL RL, (3) propose and discuss such a new
fragment, which we call OWL LD (for Linked Data).
"
485,Open Data: Reverse Engineering and Maintenance Perspective,"  Open data is an emerging paradigm to share large and diverse datasets --
primarily from governmental agencies, but also from other organizations -- with
the goal to enable the exploitation of the data for societal, academic, and
commercial gains. There are now already many datasets available with diverse
characteristics in terms of size, encoding and structure. These datasets are
often created and maintained in an ad-hoc manner. Thus, open data poses many
challenges and there is a need for effective tools and techniques to manage and
maintain it. In this paper we argue that software maintenance and reverse
engineering have an opportunity to contribute to open data and to shape its
future development. From the perspective of reverse engineering research, open
data is a new artifact that serves as input for reverse engineering techniques
and processes. Specific challenges of open data are document scraping, image
processing, and structure/schema recognition. From the perspective of
maintenance research, maintenance has to accommodate changes of open data
sources by third-party providers, traceability of data transformation
pipelines, and quality assurance of data and transformations. We believe that
the increasing importance of open data and the research challenges that it
brings with it may possibly lead to the emergence of new research streams for
reverse engineering as well as for maintenance.
"
486,Semantic Visualization and Navigation in Textual Corpus,"  This paper gives a survey of related work on the information visualization
domain and study the real integration of the cartography paradigms in actual
information search systems. Based on this study, we propose a semantic
visualization and navigation approach which offer to users three search modes:
precise search, connotative search and thematic search. The objective is to
propose to the users of an information search system, new interaction paradigms
which support the semantic aspect of the considered information space and guide
users in their searches by assisting them to locate their interest center and
to improve serendipity.
"
487,Global Maps of Science based on the new Web-of-Science Categories,"  In August 2011, Thomson Reuters launched version 5 of the Science and Social
Science Citation Index in the Web of Science (WoS). Among other things, the 222
ISI Subject Categories (SCs) for these two databases in version 4 of WoS were
renamed and extended to 225 WoS Categories (WCs). A new set of 151 Subject
Categories (SCs) was added, but at a higher level of aggregation. Since we
previously used the ISI SCs as the baseline for a global map in Pajek (Rafols
et al., 2010) and brought this facility online (at
http://www.leydesdorff.net/overlaytoolkit), we recalibrated this map for the
new WC categories using the Journal Citation Reports 2010. In the new
installation, the base maps can also be made using VOSviewer (Van Eck &
Waltman, 2010).
"
488,Temporal Analysis of Literary and Programming Prose,"  Literary works reference a variety of globally shared themes including
well-known people, events, and time periods. It is particularly interesting to
locate patterns that are either invariant across time or exhibit a
characteristic change across time, as they could imply something important
about society that those works record. This paper suggests the use of Google
n-gram viewer as a fast prototyping method for examining time-based properties
over a rich sample of literary prose. Using this method, we find that some
repeating periods of time, like Sunday, are referenced disproportionally,
allowing us to pose questions such as why a day like Thursday is so unpopular.
Furthermore, by treating software as a work of prose, we can apply a similar
analysis to open-source software repositories and explore time-based relations
in commit logs. Doing a simple statistical analysis on a few temporal keywords
in the log records, we reinforce and weaken a few beliefs on how college
students approach open source software. Finally, we help readers working on
their own temporal analysis by comparing the fundamental differences between
literary works and code repositories, and suggest blog or wiki as
recently-emerging works.
"
489,An evaluation of local shape descriptors for 3D shape retrieval,"  As the usage of 3D models increases, so does the importance of developing
accurate 3D shape retrieval algorithms. A common approach is to calculate a
shape descriptor for each object, which can then be compared to determine two
objects' similarity. However, these descriptors are often evaluated
independently and on different datasets, making them difficult to compare.
Using the SHREC 2011 Shape Retrieval Contest of Non-rigid 3D Watertight Meshes
dataset, we systematically evaluate a collection of local shape descriptors. We
apply each descriptor to the bag-of-words paradigm and assess the effects of
varying the dictionary's size and the number of sample points. In addition,
several salient point detection methods are used to choose sample points; these
methods are compared to each other and to random selection. Finally,
information from two local descriptors is combined in two ways and changes in
performance are investigated. This paper presents results of these experiment
"
490,"How the Scientific Community Reacts to Newly Submitted Preprints:
  Article Downloads, Twitter Mentions, and Citations","  We analyze the online response to the preprint publication of a cohort of
4,606 scientific articles submitted to the preprint database arXiv.org between
October 2010 and May 2011. We study three forms of responses to these
preprints: downloads on the arXiv.org site, mentions on the social media site
Twitter, and early citations in the scholarly record. We perform two analyses.
First, we analyze the delay and time span of article downloads and Twitter
mentions following submission, to understand the temporal configuration of
these reactions and whether one precedes or follows the other. Second, we run
regression and correlation tests to investigate the relationship between
Twitter mentions, arXiv downloads and article citations. We find that Twitter
mentions and arXiv downloads of scholarly articles follow two distinct temporal
patterns of activity, with Twitter mentions having shorter delays and narrower
time spans than arXiv downloads. We also find that the volume of Twitter
mentions is statistically correlated with arXiv downloads and early citations
just months after the publication of a preprint, with a possible bias that
favors highly mentioned articles.
"
491,Scienceography: the study of how science is written,"  Scientific literature has itself been the subject of much scientific study,
for a variety of reasons: understanding how results are communicated, how ideas
spread, and assessing the influence of areas or individuals. However, most
prior work has focused on extracting and analyzing citation and stylistic
patterns. In this work, we introduce the notion of 'scienceography', which
focuses on the writing of science. We provide a first large scale study using
data derived from the arXiv e-print repository. Crucially, our data includes
the ""source code"" of scientific papers-the LaTEX source-which enables us to
study features not present in the ""final product"", such as the tools used and
private comments between authors. Our study identifies broad patterns and
trends in two example areas-computer science and mathematics-as well as
highlighting key differences in the way that science is written in these
fields. Finally, we outline future directions to extend the new topic of
scienceography.
"
492,"Scientific impact evaluation and the effect of self-citations:
  mitigating the bias by discounting h-index","  In this paper, we propose a measure to assess scientific impact that
discounts self-citations and does not require any prior knowledge on the their
distribution among publications. This index can be applied to both researchers
and journals. In particular, we show that it fills the gap of h-index and
similar measures that do not take into account the effect of self-citations for
authors or journals impact evaluation. The paper provides with two real-world
examples: in the former, we evaluate the research impact of the most productive
scholars in Computer Science (according to DBLP); in the latter, we revisit the
impact of the journals ranked in the 'Computer Science Applications' section of
SCImago. We observe how self-citations, in many cases, affect the rankings
obtained according to different measures (including h-index and ch-index), and
show how the proposed measure mitigates this effect.
"
493,"Inconsistencies of Recently Proposed Citation Impact Indicators and how
  to Avoid Them","  It is shown that under certain circumstances in particular for small datasets
the recently proposed citation impact indicators I3(6PR) and R(6,k) behave
inconsistently when additional papers or citations are taken into
consideration. Three simple examples are presented, in which the indicators
fluctuate strongly and the ranking of scientists in the evaluated group is
sometimes completely mixed up by minor changes in the data base. The erratic
behavior is traced to the specific way in which weights are attributed to the
six percentile rank classes, specifically for the tied papers. For 100
percentile rank classes the effects will be less serious. For the 6 classes it
is demonstrated that a different way of assigning weights avoids these
problems, although the non-linearity of the weights for the different
percentile rank classes can still lead to (much less frequent) changes in the
ranking. This behavior is not undesired, because it can be used to correct for
differences in citation behavior in different fields. Remaining deviations from
the theoretical value R(6,k) = 1.91 can be avoided by a new scoring rule, the
fractional scoring. Previously proposed consistency criteria are amended by
another property of strict independence which a performance indicator should
aim at.
"
494,"The Leiden Ranking 2011/2012: Data collection, indicators, and
  interpretation","  The Leiden Ranking 2011/2012 is a ranking of universities based on
bibliometric indicators of publication output, citation impact, and scientific
collaboration. The ranking includes 500 major universities from 41 different
countries. This paper provides an extensive discussion of the Leiden Ranking
2011/2012. The ranking is compared with other global university rankings, in
particular the Academic Ranking of World Universities (commonly known as the
Shanghai Ranking) and the Times Higher Education World University Rankings.
Also, a detailed description is offered of the data collection methodology of
the Leiden Ranking 2011/2012 and of the indicators used in the ranking. Various
innovations in the Leiden Ranking 2011/2012 are presented. These innovations
include (1) an indicator based on counting a university's highly cited
publications, (2) indicators based on fractional rather than full counting of
collaborative publications, (3) the possibility of excluding non-English
language publications, and (4) the use of stability intervals. Finally, some
comments are made on the interpretation of the ranking, and a number of
limitations of the ranking are pointed out.
"
495,"Check Your Data Freedom: A Taxonomy to Assess Life Science Database
  Openness","  Molecular biology data are subject to terms of use that vary widely between
databases and curating institutions. This research presents a taxonomy of
contractual and technical restrictions applicable to databases in life science.
It builds upon research led by Science Commons demonstrating why open data and
the freedom to integrate facilitate innovation and how this openness can be
achieved. The taxonomy describes technical and legal restrictions applicable to
life science databases, and its metadata have been used to assess terms of use
of databases hosted by Life Science Resource Name (LSRN) Schema. While a few
public domain policies are standardized, most terms of use are not harmonized,
difficult to understand and impose controls that prevent others from
effectively reusing data. Identifying a small number of restrictions allows one
to quickly appreciate which databases are open. A checklist for data openness
is proposed in order to assist database curators who wish to make their data
more open to make sure they do so.
"
496,Citations to Australian Astronomy: 5 and 10 Year Benchmarks,"  Expanding upon Pimbblet's informative 2011 analysis of career h-indices for
members of the Astronomical Society of Australia, we provide additional
citation metrics which are geared to a) quantifying the current performance of
b) all professional astronomers in Australia. We have trawled the staff
web-pages of Australian Universities, Observatories and Research Organisations
hosting professional astronomers, and identified 383 PhD-qualified,
research-active, astronomers in the nation - 131 of these are not members of
the Astronomical Society of Australia. Using the SAO/NASA Astrophysics Data
System, we provide the three following common metrics based on publications in
the first decade of the 21st century (2001-2010): h-index, author-normalised
citation count and lead-author citation count. We additionally present a
somewhat more inclusive analysis, applicable for many early-career researchers,
that is based on publications from 2006-2010. Histograms and percentiles, plus
top-performer lists, are presented for each category. Finally, building on
Hirsch's empirical equation, we find that the (10-year) h-index and (10-year)
total citation count T can be approximated by the relation h =
(0.5+sqrt{T})/sqrt{5} for h > 5.
"
497,McCall's Area Transformation versus the Integrated Impact Indicator (I3),"  In a study entitled ""Skewed Citation Distributions and Bias Factors:
Solutions to two core problems with the journal impact factor,"" Mutz & Daniel
(2012) propose (i) McCall's (1922) Area Transformation of the skewed citation
distribution so that this data can be considered as normally distributed (Krus
& Kennedy, 1977), and (ii) to control for different document types as a
co-variate (Rubin, 1977). This approach provides an alternative to Leydesdorff
& Bornmann's (2011) Integrated Impact Indicator (I3). As the authors note, the
two approaches are akin.
  Can something be said about the relative quality of the two approaches? To
that end, I replicated the study of Mutz & Daniel for the 11 journals in the
Subject Category ""mathematical psychology,"" but using additionally I3 on the
basis of continuous quantiles (Leydesdorff & Bornmann, in press) and its
variant PR6 based on the six percentile rank classes distinguished by Bornmann
& Mutz (2011) as follows: the top-1%, 95-99%, 90-95%, 75-90%, 50-75%, and
bottom-50%.
"
498,When Should I Make Preservation Copies of Myself?,"  We investigate how different preservation policies ranging from least
aggressive to Most aggressive affect the level of preservation achieved by
autonomic processes used by smart digital objects (DOs). The mechanisms used to
support preservation across different hosts can be used for automatic link
generation and support preservation activities by moving data preservation from
an archive centric perspective to a data centric preservation. Based on
simulations of small-world graphs of DOs created using the Unsupervised
Small-World algorithm, we report quantitative and qualitative results for
graphs ranging in size from 10 to 5000 DOs. Our results show that a Most
aggressive preservation policy makes the best use of distributed host resources
while using one half of the number of messages of a Moderately aggressive
preservation policy.
"
499,Publication Trends in Astronomy: The Lone Author,"  In this short communication I highlight how the number of collaborators on
papers in the main astronomy journals has evolved over time. We see a trend of
moving away from single-author papers. This communication is based on data in
the holdings of the SAO/NASA Astrophysics Data System (ADS).
  The ADS is funded by NASA Grant NNX09AB39G.
"
500,"Culturomics meets random fractal theory: Insights into long-range
  correlations of social and natural phenomena over the past two centuries","  Culturomics was recently introduced as the application of high-throughput
data collection and analysis to the study of human culture. Here we make use of
this data by investigating fluctuations in yearly usage frequencies of specific
words that describe social and natural phenomena, as derived from books that
were published over the course of the past two centuries. We show that the
determination of the Hurst parameter by means of fractal analysis provides
fundamental insights into the nature of long-range correlations contained in
the culturomic trajectories, and by doing so, offers new interpretations as to
what might be the main driving forces behind the examined phenomena. Quite
remarkably, we find that social and natural phenomena are governed by
fundamentally different processes. While natural phenomena have properties that
are typical for processes with persistent long-range correlations, social
phenomena are better described as nonstationary, on-off intermittent, or Levy
walk processes.
"
501,Enhancing Navigation on Wikipedia with Social Tags,"  Social tagging has become an interesting approach to improve search and
navigation over the actual Web, since it aggregates the tags added by different
users to the same resource in a collaborative way. This way, it results in a
list of weighted tags describing its resource. Combined to a classical
taxonomic classification system such as that by Wikipedia, social tags can
enhance document navigation and search. On the one hand, social tags suggest
alternative navigation ways, including pivot-browsing, popularity-driven
navigation, and filtering. On the other hand, it provides new metadata,
sometimes uncovered by documents' content, that can substantially improve
document search. In this work, the inclusion of an interface to add
user-defined tags describing Wikipedia articles is proposed, as a way to
improve article navigation and retrieval. As a result, a prototype on applying
tags over Wikipedia is proposed in order to evaluate its effectiveness.
"
502,Analyzing Tag Distributions in Folksonomies for Resource Classification,"  Recent research has shown the usefulness of social tags as a data source to
feed resource classification. Little is known about the effect of settings on
folksonomies created on social tagging systems. In this work, we consider the
settings of social tagging systems to further understand tag distributions in
folksonomies. We analyze in depth the tag distributions on three large-scale
social tagging datasets, and analyze the effect on a resource classification
task. To this end, we study the appropriateness of applying weighting schemes
based on the well-known TF-IDF for resource classification. We show the great
importance of settings as to altering tag distributions. Among those settings,
tag suggestions produce very different folksonomies, which condition the
success of the employed weighting schemes. Our findings and analyses are
relevant for researchers studying tag-based resource classification, user
behavior in social networks, the structure of folksonomies and tag
distributions, as well as for developers of social tagging systems in search of
an appropriate setting.
"
503,"Innovation as a Nonlinear Process, the Scientometric Perspective, and
  the Specification of an ""Innovation Opportunities Explorer""","  The process of innovation follows non-linear patterns across the domains of
science, technology, and the economy. Novel bibliometric mapping techniques can
be used to investigate and represent distinctive, but complementary
perspectives on the innovation process (e.g., ""demand"" and ""supply"") as well as
the interactions among these perspectives. The perspectives can be represented
as ""continents"" of data related to varying extents over time. For example, the
different branches of Medical Subject Headings (MeSH) in the Medline database
provide sources of such perspectives (e.g., ""Diseases"" versus ""Drugs and
Chemicals""). The multiple-perspective approach enables us to reconstruct facets
of the dynamics of innovation, in terms of selection mechanisms shaping
localizable trajectories and/or resulting in more globalized regimes. By
expanding the data with patents and scholarly publications, we demonstrate the
use of this multi-perspective approach in the case of RNA Interference (RNAi).
The possibility to develop an ""Innovation Opportunities Explorer"" is specified.
"
504,Open Annotations on Multimedia Web Resources,"  Many Web portals allow users to associate additional information with
existing multimedia resources such as images, audio, and video. However, these
portals are usually closed systems and user-generated annotations are almost
always kept locked up and remain inaccessible to the Web of Data. We believe
that an important step to take is the integration of multimedia annotations and
the Linked Data principles. We present the current state of the Open Annotation
Model, explain our design rationale, and describe how the model can represent
user annotations on multimedia Web resources. Applying this model in Web
portals and devices, which support user annotations, should allow clients to
easily publish and consume, thus exchange annotations on multimedia Web
resources via common Web standards.
"
505,"A new methodology for constructing a publication-level classification
  system of science","  Classifying journals or publications into research areas is an essential
element of many bibliometric analyses. Classification usually takes place at
the level of journals, where the Web of Science subject categories are the most
popular classification system. However, journal-level classification systems
have two important limitations: They offer only a limited amount of detail, and
they have difficulties with multidisciplinary journals. To avoid these
limitations, we introduce a new methodology for constructing classification
systems at the level of individual publications. In the proposed methodology,
publications are clustered into research areas based on citation relations. The
methodology is able to deal with very large numbers of publications. We present
an application in which a classification system is produced that includes
almost ten million publications. Based on an extensive analysis of this
classification system, we discuss the strengths and the limitations of the
proposed methodology. Important strengths are the transparency and relative
simplicity of the methodology and its fairly modest computing and memory
requirements. The main limitation of the methodology is its exclusive reliance
on direct citation relations between publications. The accuracy of the
methodology can probably be increased by also taking into account other types
of relations, for instance based on bibliographic coupling.
"
506,Evolution of Wikipedia's Category Structure,"  Wikipedia, as a social phenomenon of collaborative knowledge creating, has
been studied extensively from various points of views. The category system of
Wikipedia, introduced in 2004, has attracted relatively little attention. In
this study, we focus on the documentation of knowledge, and the transformation
of this documentation with time. We take Wikipedia as a proxy for knowledge in
general and its category system as an aspect of the structure of this
knowledge. We investigate the evolution of the category structure of the
English Wikipedia from its birth in 2004 to 2008. We treat the category system
as if it is a hierarchical Knowledge Organization System, capturing the changes
in the distributions of the top categories. We investigate how the clustering
of articles, defined by the category system, matches the direct link network
between the articles and show how it changes over time. We find the Wikipedia
category network mostly stable, but with occasional reorganization. We show
that the clustering matches the link structure quite well, except short periods
preceding the reorganizations.
"
507,"Bibliometric Perspectives on Medical Innovation using the Medical
  Subject Headings (MeSH) of PubMed","  Multiple perspectives on the nonlinear processes of medical innovations can
be distinguished and combined using the Medical Subject Headings (MeSH) of the
Medline database. Focusing on three main branches-""diseases,"" ""drugs and
chemicals,"" and ""techniques and equipment""-we use base maps and overlay
techniques to investigate the translations and interactions and thus to gain a
bibliometric perspective on the dynamics of medical innovations. To this end,
we first analyze the Medline database, the MeSH index tree, and the various
options for a static mapping from different perspectives and at different
levels of aggregation. Following a specific innovation (RNA interference) over
time, the notion of a trajectory which leaves a signature in the database is
elaborated. Can the detailed index terms describing the dynamics of research be
used to predict the diffusion dynamics of research results? Possibilities are
specified for further integration between the Medline database, on the one
hand, and the Science Citation Index and Scopus (containing citation
information), on the other.
"
508,Literature-based knowledge discovery: the state of the art,"  Literature-based knowledge discovery method was introduced by Dr. Swanson in
1986. He hypothesized a connection between Raynaud's phenomenon and dietary
fish oil, the field of literature-based discovery (LBD) was born from then on.
During the subsequent two decades, LBD's research attracts some scientists
including information science, computer science, and biomedical science, etc..
It has been a part of knowledge discovery and text mining. This paper
summarizes the development of recent years about LBD and presents two parts,
methodology research and applied research. Lastly, some problems are pointed as
future research directions.
"
509,Metadata Management in Scientific Computing,"  Complex scientific codes and the datasets they generate are in need of a
sophisticated categorization environment that allows the community to store,
search, and enhance metadata in an open, dynamic system. Currently, data is
often presented in a read-only format, distilled and curated by a select group
of researchers. We envision a more open and dynamic system, where authors can
publish their data in a writeable format, allowing users to annotate the
datasets with their own comments and data. This would enable the scientific
community to collaborate on a higher level than before, where researchers could
for example annotate a published dataset with their citations.
  Such a system would require a complete set of permissions to ensure that any
individual's data cannot be altered by others unless they specifically allow
it. For this reason datasets and codes are generally presented read-only, to
protect the author's data; however, this also prevents the type of social
revolutions that the private sector has seen with Facebook and Twitter.
  In this paper, we present an alternative method of publishing codes and
datasets, based on Fluidinfo, which is an openly writeable and social metadata
engine. We will use the specific example of the Einstein Toolkit, a shared
scientific code built using the Cactus Framework, to illustrate how the code's
metadata may be published in writeable form via Fluidinfo.
"
510,"Research collaboration and the expanding science grid: Measuring
  globalization processes worldwide","  This paper applies a new model and analytical tool to measure and study
contemporary globalization processes in collaborative science - a world in
which scientists, scholars, technicians and engineers interact within a 'grid'
of interconnected research sites and collaboration networks. The building
blocks of our metrics are the cities where scientific research is conducted, as
mentioned in author addresses on research publications. The unit of analysis is
the geographical distance between those cities. In our macro-level trend
analysis, covering the years 2000-2010, we observe that research collaboration
distances have been increasing, while the share of collaborative contacts with
foreign cities has leveled off. Collaboration distances and growth rates differ
significantly between countries and between fields of science. The application
of a distance metrics to compare and track these processes opens avenues for
further studies, both at the meso-level and at the micro-level, into how
research collaboration patterns and trends are driving and shaping the
connectivity fabric of world science.
"
511,"Do Linguistic Style and Readability of Scientific Abstracts affect their
  Virality?","  Reactions to textual content posted in an online social network show
different dynamics depending on the linguistic style and readability of the
submitted content. Do similar dynamics exist for responses to scientific
articles? Our intuition, supported by previous research, suggests that the
success of a scientific article depends on its content, rather than on its
linguistic style. In this article, we examine a corpus of scientific abstracts
and three forms of associated reactions: article downloads, citations, and
bookmarks. Through a class-based psycholinguistic analysis and readability
indices tests, we show that certain stylistic and readability features of
abstracts clearly concur in determining the success and viral capability of a
scientific article.
"
512,"Can an Ad-hoc ontology Beat a Medical Search Engine? The Chronious
  Search Engine case","  Chronious is an Open, Ubiquitous and Adaptive Chronic Disease Management
Platform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease
(CKD) and Renal Insufficiency. It consists of several modules: an ontology
based literature search engine, a rule based decision support system, remote
sensors interacting with lifestyle interfaces (PDA, monitor touch-screen) and a
machine learning module. All these modules interact each other to allow the
monitoring of two types of chronic diseases and to help clinician in taking
decision for care purpose. This paper illustrates how the ontology search
engine was created and fed and how some comparative test indicated that the
ontology based approach give better results, on some estimation parameters,
than the main reference web search engine.
"
513,"Citation Analysis with Medical Subject Headings (MeSH) using the Web of
  Knowledge: A new routine","  Citation analysis of documents retrieved from the Medline database (at the
Web of Knowledge) has been possible only on a case-by-case basis. A technique
is here developed for citation analysis in batch mode using both Medical
Subject Headings (MeSH) at the Web of Knowledge and the Science Citation Index
at the Web of Science. This freeware routine is applied to the case of ""Brugada
Syndrome,"" a specific disease and field of research (since 1992). The journals
containing these publications, for example, are attributed to Web-of-Science
Categories other than ""Cardiac and Cardiovascular Systems""), perhaps because of
the possibility of genetic testing for this syndrome in the clinic. With this
routine, all the instruments available for citation analysis can now be used on
the basis of MeSH terms. Other options for crossing between Medline, WoS, and
Scopus are also reviewed.
"
514,Altmetrics in the wild: Using social media to explore scholarly impact,"  In growing numbers, scholars are integrating social media tools like blogs,
Twitter, and Mendeley into their professional communications. The online,
public nature of these tools exposes and reifies scholarly processes once
hidden and ephemeral. Metrics based on this activities could inform broader,
faster measures of impact, complementing traditional citation metrics. This
study explores the properties of these social media-based metrics or
""altmetrics"", sampling 24,331 articles published by the Public Library of
Science.
  We find that that different indicators vary greatly in activity. Around 5% of
sampled articles are cited in Wikipedia, while close to 80% have been included
in at least one Mendeley library. There is, however, an encouraging diversity;
a quarter of articles have nonzero data from five or more different sources.
Correlation and factor analysis suggest citation and altmetrics indicators
track related but distinct impacts, with neither able to describe the complete
picture of scholarly use alone. There are moderate correlations between
Mendeley and Web of Science citation, but many altmetric indicators seem to
measure impact mostly orthogonal to citation. Articles cluster in ways that
suggest five different impact ""flavors"", capturing impacts of different types
on different audiences; for instance, some articles may be heavily read and
saved by scholars but seldom cited. Together, these findings encourage more
research into altmetrics as complements to traditional citation measures.
"
515,Quantifying the interdisciplinarity of scientific journals and fields,"  There is an overall perception of increased interdisciplinarity in science,
but this is difficult to confirm quantitatively owing to the lack of adequate
methods to evaluate subjective phenomena. This is no different from the
difficulties in establishing quantitative relationships in human and social
sciences. In this paper we quantified the interdisciplinarity of scientific
journals and science fields by using an entropy measurement based on the
diversity of the subject categories of journals citing a specific journal. The
methodology consisted in building citation networks using the Journal Citation
Reports database, in which the nodes were journals and edges were established
based on citations among journals. The overall network for the 11-year period
(1999-2009) studied was small-world and scale free with regard to the
in-strength. Upon visualizing the network topology an overall structure of the
various science fields could be inferred, especially their interconnections. We
confirmed quantitatively that science fields are becoming increasingly
interdisciplinary, with the degree of interdisplinarity (i.e. entropy)
correlating strongly with the in-strength of journals and with the impact
factor.
"
516,"Evolutionary Events in a Mathematical Sciences Research Collaboration
  Network","  This study examines long-term trends and shifting behavior in the
collaboration network of mathematics literature, using a subset of data from
Mathematical Reviews spanning 1985-2009. Rather than modeling the network
cumulatively, this study traces the evolution of the ""here and now"" using
fixed-duration sliding windows. The analysis uses a suite of common network
diagnostics, including the distributions of degrees, distances, and clustering,
to track network structure. Several random models that call these diagnostics
as parameters help tease them apart as factors from the values of others. Some
behaviors are consistent over the entire interval, but most diagnostics
indicate that the network's structural evolution is dominated by occasional
dramatic shifts in otherwise steady trends. These behaviors are not distributed
evenly across the network; stark differences in evolution can be observed
between two major subnetworks, loosely thought of as ""pure"" and ""applied"",
which approximately partition the aggregate. The paper characterizes two major
events along the mathematics network trajectory and discusses possible
explanatory factors.
"
517,"Improving an Hybrid Literary Book Recommendation System through Author
  Ranking","  Literary reading is an important activity for individuals and choosing to
read a book can be a long time commitment, making book choice an important task
for book lovers and public library users. In this paper we present an hybrid
recommendation system to help readers decide which book to read next. We study
book and author recommendation in an hybrid recommendation setting and test our
approach in the LitRec data set. Our hybrid book recommendation approach
purposed combines two item-based collaborative filtering algorithms to predict
books and authors that the user will like. Author predictions are expanded in
to a book list that is subsequently aggregated with the former list generated
through the initial collaborative recommender. Finally, the resulting book list
is used to yield the top-n book recommendations. By means of various
experiments, we demonstrate that author recommendation can improve overall book
recommendation.
"
518,"Building Custom Term Suggestion Web Services with OAI-Harvested Open
  Data","  The problem that the same information need can be expressed in a variety of
ways is especially true for scientific literature. Each scientific discipline
has its own domain-specific language and vocabulary. This language is coded
into documentary tools like thesauri or classifications that are used to
document and describe scientific documents. When we think of information
retrieval as ""fundamentally a linguistic process"" (Blair, 2003) users have to
be aware of the most relevant search terms - which are the controlled thesauri
terms the documents are described with. This can be achieved with so-called
search-term-recommenders (STR) that map free search terms of a lay user to
controlled vocabulary terms which can then be used as a term suggestion or to
do an automatic query expansion (Hienert, Schaer, Schaible, & Mayr, 2011).
State-of-the-art repository software systems like DSpace or EPrints already
offer some kind of term suggestion features in search or input forms but these
implementations only work as simple auto completion mechanisms that don't
incorporate any kind of semantic mapping. Such software systems would gain a
lot in terms of usability and data consistency if tools like the proposed
domain-specific STRs would be freely available. We aim to implement a rich
toolbox of web services (like the mentioned domain-specific STRs) to support
users and providers of online Digital Library (DL) or repository systems.
"
519,"A reverse engineering approach to the suppression of citation biases
  reveals universal properties of citation distributions","  The large amount of information contained in bibliographic databases has
recently boosted the use of citations, and other indicators based on citation
numbers, as tools for the quantitative assessment of scientific research.
Citations counts are often interpreted as proxies for the scientific influence
of papers, journals, scholars, and institutions. However, a rigorous and
scientifically grounded methodology for a correct use of citation counts is
still missing. In particular, cross-disciplinary comparisons in terms of raw
citation counts systematically favors scientific disciplines with higher
citation and publication rates. Here we perform an exhaustive study of the
citation patterns of millions of papers, and derive a simple transformation of
citation counts able to suppress the disproportionate citation counts among
scientific domains. We find that the transformation is well described by a
power-law function, and that the parameter values of the transformation are
typical features of each scientific discipline. Universal properties of
citation patterns descend therefore from the fact that citation distributions
for papers in a specific field are all part of the same family of univariate
distributions.
"
520,Multidisciplinary Cognitive Content of Nanoscience and Nanotechnology,"  This article examines the cognitive evolution and disciplinary diversity of
nanotechnology as expressed through the terminology used in titles of nano
journal articles. The analysis is based on the NanoBank bibliographic database
of 287,106 nano articles published between 1981 and 2004. We perform
multifaceted analyses of title words, focusing on 100 most frequent terms.
Hierarchical clustering of title terms reveals three distinct time periods of
cognitive development of nano research: formative (1981-1990), early
(1991-1998), and current (after 1998). Early period is characterized by the
introduction of thin film deposition techniques, while the current period is
characterized by the increased focus on carbon nanotube and nanoparticle
research. We introduce a method to identify disciplinary components of
nanotechnology. It shows that the nano research is being carried out in a
number of diverse parent disciplines. Currently only 5% of articles are
published in dedicated nano-only journals. We find that some 85% of nano
research today is multidisciplinary. Hierarchical clustering of disciplinary
components reveals that the cognitive content of current nanoscience can be
divided into nine clusters. Some clusters account for a large fraction of nano
research and are identified with such parent disciplines as the condensed
matter and applied physics, materials science, and analytical chemistry. Other
clusters represent much smaller parts of nano research, but are as cognitively
distinct. In the decreasing order of size, these fields are: polymer science,
biotechnology, general chemistry, surface science, and pharmacology. Cognitive
content of research published in nano-only journals is closest to nano research
published in condensed matter and applied physics journals.
"
521,Persistence and Uncertainty in the Academic Career,"  Understanding how institutional changes within academia may affect the
overall potential of science requires a better quantitative representation of
how careers evolve over time. Since knowledge spillovers, cumulative advantage,
competition, and collaboration are distinctive features of the academic
profession, both the employment relationship and the procedures for assigning
recognition and allocating funding should be designed to account for these
factors. We study the annual production n_{i}(t) of a given scientist i by
analyzing longitudinal career data for 200 leading scientists and 100 assistant
professors from the physics community. We compare our results with 21,156
sports careers. Our empirical analysis of individual productivity dynamics
shows that (i) there are increasing returns for the top individuals within the
competitive cohort, and that (ii) the distribution of production growth is a
leptokurtic ""tent-shaped"" distribution that is remarkably symmetric. Our
methodology is general, and we speculate that similar features appear in other
disciplines where academic publication is essential and collaboration is a key
feature. We introduce a model of proportional growth which reproduces these two
observations, and additionally accounts for the significantly right-skewed
distributions of career longevity and achievement in science. Using this
theoretical model, we show that short-term contracts can amplify the effects of
competition and uncertainty making careers more vulnerable to early
termination, not necessarily due to lack of individual talent and persistence,
but because of random negative production shocks. We show that fluctuations in
scientific production are quantitatively related to a scientist's collaboration
radius and team efficiency.
"
522,Accounting for the Uncertainty in the Evaluation of Percentile Ranks,"  In a recent paper entitled ""Inconsistencies of Recently Proposed Citation
Impact Indicators and how to Avoid Them,"" Schreiber (2012, at arXiv:1202.3861)
proposed (i) a method to assess tied ranks consistently and (ii) fractional
attribution to percentile ranks in the case of relatively small samples (e.g.,
for n < 100). Schreiber's solution to the problem of how to handle tied ranks
is convincing, in my opinion (cf. Pudovkin & Garfield, 2009). The fractional
attribution, however, is computationally intensive and cannot be done manually
for even moderately large batches of documents. Schreiber attributed scores
fractionally to the six percentile rank classes used in the Science and
Engineering Indicators of the U.S. National Science Board, and thus missed, in
my opinion, the point that fractional attribution at the level of hundred
percentiles-or equivalently quantiles as the continuous random variable-is only
a linear, and therefore much less complex problem. Given the quantile-values,
the non-linear attribution to the six classes or any other evaluation scheme is
then a question of aggregation. A new routine based on these principles
(including Schreiber's solution for tied ranks) is made available as software
for the assessment of documents retrieved from the Web of Science (at
http://www.leydesdorff.net/software/i3).
"
523,Investigating Keyphrase Indexing with Text Denoising,"  In this paper, we report on indexing performance by a state-of-the-art
keyphrase indexer, Maui, when paired with a text extraction procedure called
text denoising. Text denoising is a method that extracts the denoised text,
comprising the content-rich sentences, from full texts. The performance of the
keyphrase indexer is demonstrated on three standard corpora collected from
three domains, namely food and agriculture, high energy physics, and biomedical
science. Maui is trained using the full texts and denoised texts. The indexer,
using its trained models, then extracts keyphrases from test sets comprising
full texts, and their denoised and noise parts (i.e., the part of texts that
remains after denoising). Experimental findings show that against a gold
standard, the denoised-text-trained indexer indexing full texts, performs
either better than or as good as its benchmark performance produced by a
full-text-trained indexer indexing full texts.
"
524,"Publishing Identifiable Experiment Code And Configuration Is Important,
  Good and Easy","  We argue for the value of publishing the exact code, configuration and data
processing scripts used to produce empirical work in robotics. In particular,
we recommend publishing a unique identifier for the code package in the paper
itself, as a promise to the reader that this is the relavant code. We review
some recent discussion of best practice for reproducibility in various
professional organisations and journals, and discuss the current reward
structure for publishing code in robotics, along with some ideas for
improvement.
"
525,Collaboratively Patching Linked Data,"  Today's Web of Data is noisy. Linked Data often needs extensive preprocessing
to enable efficient use of heterogeneous resources. While consistent and valid
data provides the key to efficient data processing and aggregation we are
facing two main challenges: (1st) Identification of erroneous facts and
tracking their origins in dynamically connected datasets is a difficult task,
and (2nd) efforts in the curation of deficient facts in Linked Data are
exchanged rather rarely. Since erroneous data often is duplicated and
(re-)distributed by mashup applications it is not only the responsibility of a
few original publishers to keep their data tidy, but progresses to be a mission
for all distributers and consumers of Linked Data too. We present a new
approach to expose and to reuse patches on erroneous data to enhance and to add
quality information to the Web of Data. The feasibility of our approach is
demonstrated by example of a collaborative game that patches statements in
DBpedia data and provides notifications for relevant changes.
"
526,Looking at a digital research data archive - Visual interfaces to EASY,"  In this paper we explore visually the structure of the collection of a
digital research data archive in terms of metadata for deposited datasets. We
look into the distribution of datasets over different scientific fields; the
role of main depositors (persons and institutions) in different fields, and
main access choices for the deposited datasets. We argue that visual analytics
of metadata of collections can be used in multiple ways: to inform the archive
about structure and growth of its collection; to foster collections strategies;
and to check metadata consistency. We combine visual analytics and visual
enhanced browsing introducing a set of web-based, interactive visual interfaces
to the archive's collection. We discuss how text based search combined with
visual enhanced browsing enhances data access, navigation, and reuse.
"
527,"Edited Volumes, Monographs, and Book Chapters in the Book Citation Index
  (BKCI) and Science Citation Index (SCI, SoSCI, A&HCI)","  In 2011, Thomson-Reuters introduced the Book Citation Index (BKCI) as part of
the Science Citation Index (SCI). The interface of the Web of Science version 5
enables users to search for both ""Books"" and ""Book Chapters"" as new categories.
Books and book chapters, however, were always among the cited references, and
book chapters have been included in the database since 2005. We explore the two
categories with both BKCI and SCI, and in the sister social sciences (SoSCI)
and the arts & humanities (A&HCI) databases. Book chapters in edited volumes
can be highly cited. Books contain many citing references but are relatively
less cited. This may find its origin in the slower circulation of books than of
journal articles. It is possible to distinguish between monographs and edited
volumes among the ""Books"" scientometrically. Monographs may be underrated in
terms of citation impact or overrated using publication performance indicators
because individual chapters are counted as contributions separately in terms of
articles, reviews, and/or book chapters.
"
528,"Proceedings of the first International Workshop On Open Data, WOD-2012","  WOD-2012 aims at facilitating new trends and ideas from a broad range of
topics concerned within the widely-spread Open Data movement, from the
viewpoint of computer science research.
  While being most commonly known from the recent Linked Open Data movement,
the concept of publishing data explicitly as Open Data has meanwhile developed
many variants and facets that go beyond publishing large and highly structured
RDF/S repositories. Open Data comprises text and semi-structured data, but also
open multi-modal contents, including music, images, and videos. With the
increasing amount of data that is published by governments (see, e.g.,
data.gov, data.gov.uk or data.gouv.fr), by international organizations
(data.worldbank.org or data.undp.org) and by scientific communities (tdar.org,
cds.u-strasbg.fr, GenBank, IRIS or KNB) explicitly under an Open Data policy,
new challenges arise not only due to the scale at which this data becomes
available.
  A number of community-based conferences accommodate tracks or workshops which
are dedicated to Open Data. However, WOD aims to be a premier venue to gather
researchers and practitioners who are contributing to and interested in the
emerging field of managing Open Data from a computer science perspective.
Hence, it is a unique opportunity to find in a single place up-to-date
scientific works on Web-scale Open Data issues that have so far only partially
been addressed by different research communities such as Databases, Data Mining
and Knowledge Management, Distributed Systems, Data Privacy, and Data
Visualization.
"
529,The evolution of classification systems: Ontogeny of the UDC,"  To classify is to put things in meaningful groups, but the criteria for doing
so can be problematic. Study of evolution of classification includes
ontogenetic analysis of change in classification over time. We present an
empirical analysis of the UDC over the entire period of its development. We
demonstrate stability in main classes, with major change driven by 20th century
scientific developments. But we also demonstrate a vast increase in the
complexity of auxiliaries. This study illustrates an alternative to Tennis'
""scheme-versioning"" method.
"
530,Your Two Weeks of Fame and Your Grandmother's,"  Did celebrity last longer in 1929, 1992 or 2009? We investigate the
phenomenon of fame by mining a collection of news articles that spans the
twentieth century, and also perform a side study on a collection of blog posts
from the last 10 years. By analyzing mentions of personal names, we measure
each person's time in the spotlight, using two simple metrics that evaluate,
roughly, the duration of a single news story about a person, and the overall
duration of public interest in a person. We watched the distribution evolve
from 1895 to 2010, expecting to find significantly shortening fame durations,
per the much popularly bemoaned shortening of society's attention spans and
quickening of media's news cycles. Instead, we conclusively demonstrate that,
through many decades of rapid technological and societal change, through the
appearance of Twitter, communication satellites, and the Internet, fame
durations did not decrease, neither for the typical case nor for the extremely
famous, with the last statistically significant fame duration decreases coming
in the early 20th century, perhaps from the spread of telegraphy and telephony.
Furthermore, while median fame durations stayed persistently constant, for the
most famous of the famous, as measured by either volume or duration of media
attention, fame durations have actually trended gently upward since the 1940s,
with statistically significant increases on 40-year timescales. Similar studies
have been done with much shorter timescales specifically in the context of
information spreading on Twitter and similar social networking sites. To the
best of our knowledge, this is the first massive scale study of this nature
that spans over a century of archived data, thereby allowing us to track
changes across decades.
"
531,"Reimplementing the Mathematical Subject Classification (MSC) as a Linked
  Open Dataset","  The Mathematics Subject Classification (MSC) is a widely used scheme for
classifying documents in mathematics by subject. Its traditional, idiosyncratic
conceptualization and representation makes the scheme hard to maintain and
requires custom implementations of search, query and annotation support. This
limits uptake e.g. in semantic web technologies in general and the creation and
exploration of connections between mathematics and related domains (e.g.
science) in particular.
  This paper presents the new official implementation of the MSC2010 as a
Linked Open Dataset, building on SKOS (Simple Knowledge Organization System).
We provide a brief overview of the dataset's structure, its available
implementations, and first applications.
"
532,Point-and-write --- Documenting Formal Mathematics by Reference,"  This paper describes the design and implementation of mechanisms for
light-weight inclusion of formal mathematics in informal mathematical writings,
particularly in a Web-based setting. This is conceptually done in three stages:
(i) by choosing a suitable representation layer (based on RDF) for encoding the
information about available resources of formal mathematics, (ii) by exporting
this information from formal libraries, and (iii) by providing syntax and
implementation for including formal mathematics in informal writings.
  We describe the use case of an author referring to formal text from an
informal narrative, and discuss design choices entailed by this use case.
Furthermore, we describe an implementation of the use case within the Agora
prototype: a Wiki for collaborating on formalized mathematics.
"
533,"Assessing Researcher Interdisciplinarity: A Case Study of the University
  of Hawaii NASA Astrobiology Institute","  In this study, we combine bibliometric techniques with a machine learning
algorithm, the sequential Information Bottleneck, to assess the
interdisciplinarity of research produced by the University of Hawaii NASA
Astrobiology Institute (UHNAI). In particular, we cluster abstract data to
evaluate Thomson Reuters Web of Knowledge subject categories as descriptive
labels for astrobiology documents, assess individual researcher
interdisciplinarity, and determine where collaboration opportunities might
occur. We find that the majority of the UHNAI team is engaged in
interdisciplinary research, and suggest that our method could be applied to
additional NASA Astrobiology Institute teams in particular, or other
interdisciplinary research teams more broadly, to identify and facilitate
collaboration opportunities.
"
534,"Efficient Video Indexing on the Web: A System that Leverages User
  Interactions with a Video Player","  In this paper, we propose a user-based video indexing method, that
automatically generates thumbnails of the most important scenes of an online
video stream, by analyzing users' interactions with a web video player. As a
test bench to verify our idea we have extended the YouTube video player into
the VideoSkip system. In addition, VideoSkip uses a web-database (Google
Application Engine) to keep a record of some important parameters, such as the
timing of basic user actions (play, pause, skip). Moreover, we implemented an
algorithm that selects representative thumbnails. Finally, we populated the
system with data from an experiment with nine users. We found that the
VideoSkip system indexes video content by leveraging implicit users
interactions, such as pause and thirty seconds skip. Our early findings point
toward improvements of the web video player and its thumbnail generation
technique. The VideSkip system could compliment content-based algorithms, in
order to achieve efficient video-indexing in difficult videos, such as lectures
or sports.
"
535,Harnessing Folksonomies for Resource Classification,"  In our daily lives, organizing resources into a set of categories is a common
task. Categorization becomes more useful as the collection of resources
increases. Large collections of books, movies, and web pages, for instance, are
cataloged in libraries, organized in databases and classified in directories,
respectively. However, the usual largeness of these collections requires a vast
endeavor and an outrageous expense to organize manually.
  Recent research is moving towards developing automated classifiers that
reduce the increasing costs and effort of the task. Little work has been done
analyzing the appropriateness of and exploring how to harness the annotations
provided by users on social tagging systems as a data source. Users on these
systems save resources as bookmarks in a social environment by attaching
annotations in the form of tags. It has been shown that these tags facilitate
retrieval of resources not only for the annotators themselves but also for the
whole community. Likewise, these tags provide meaningful metadata that refers
to the content of the resources.
  In this thesis, we deal with the utilization of these user-provided tags in
search of the most accurate classification of resources as compared to
expert-driven categorizations. To the best of our knowledge, this is the first
research work performing actual classification experiments utilizing social
tags. By exploring the characteristics and nature of these systems and the
underlying folksonomies, this thesis sheds new light on the way of getting the
most out of social tags for the sake of automated resource classification
tasks. Therefore, we believe that the contributions in this work are of utmost
interest for future researchers in the field, as well as for the scientific
community in order to better understand these systems and further utilize the
knowledge garnered from social tags.
"
536,Fully Digital: Policy and Process Implications for the AAS,"  Over the past two decades, every scholarly publisher has migrated at least
the mechanical aspects of their journal publishing so that they utilize digital
means. The academy was comfortable with that for a while, but publishers are
under increasing pressure to adapt further. At the American Astronomical
Society (AAS), we think that means bringing our publishing program to the point
of being fully digital, by establishing procedures and policies that regard the
digital objects of publication primarily. We have always thought about our
electronic journals as databases of digital articles, from which we can publish
and syndicate articles one at a time, and we must now put flesh on those bones
by developing practices that are consistent with the realities of article at a
time publication online. As a learned society that holds the long-term rights
to the literature, we have actively taken responsibility for the preservation
of the digital assets that constitute our journals, and in so doing we have not
forsaken the legacy pre-digital assets. All of us who serve as the long-term
stewards of scholarship must begin to evolve into fully digital publishers.
"
537,Indices to Quantify the Ranking of Arabic Journals and Research Output,"  I propose two simple indices to classify journals, published in Arabic
language, and different researchers. These indices depend upon the known impact
factor and h-index. The new indices give an easy way to judge the rank of any
journal (output of any researcher) without looking for other journals (output
of other researchers).
"
538,A Fitness Model for Scholarly Impact Analysis,"  We propose a model to analyze citation growth and influences of fitness
(competitiveness) factors in an evolving citation network. Applying the
proposed method to modeling citations to papers and scholars in the InfoVis
2004 data, a benchmark collection about a 31-year history of information
visualization, leads to findings consistent with citation distributions in
general and observations of the domain in particular. Fitness variables based
on prior impacts and the time factor have significant influences on citation
outcomes. We find considerably large effect sizes from the fitness modeling,
which suggest inevitable bias in citation analysis due to these factors. While
raw citation scores offer little insight into the growth of InfoVis,
normalization of the scores by influences of time and prior fitness offers a
reasonable depiction of the field's development. The analysis demonstrates the
proposed model's ability to produce results consistent with observed data and
to support meaningful comparison of citation scores over time.
"
539,On the calculation of percentile-based bibliometric indicators,"  A percentile-based bibliometric indicator is an indicator that values
publications based on their position within the citation distribution of their
field. The most straightforward percentile-based indicator is the proportion of
frequently cited publications, for instance the proportion of publications that
belong to the top 10% most frequently cited of their field. Recently, more
complex percentile-based indicators were proposed. A difficulty in the
calculation of percentile-based indicators is caused by the discrete nature of
citation distributions combined with the presence of many publications with the
same number of citations. We introduce an approach to calculating
percentile-based indicators that deals with this difficulty in a more
satisfactory way than earlier approaches suggested in the literature. We show
in a formal mathematical framework that our approach leads to indicators that
do not suffer from biases in favor of or against particular fields of science.
"
540,"How Can Journal Impact Factors be Normalized across Fields of Science?
  An Assessment in terms of Percentile Ranks and Fractional Counts","  Using the CD-ROM version of the Science Citation Index 2010 (N = 3,705
journals), we study the (combined) effects of (i) fractional counting on the
impact factor (IF) and (ii) transformation of the skewed citation distributions
into a distribution of 100 percentiles and six percentile rank classes (top-1%,
top-5%, etc.). Do these approaches lead to field-normalized impact measures for
journals? In addition to the two-year IF (IF2), we consider the five-year IF
(IF5), the respective numerators of these IFs, and the number of Total Cites,
counted both as integers and fractionally. These various indicators are tested
against the hypothesis that the classification of journals into 11 broad fields
by PatentBoard/National Science Foundation provides statistically significant
between-field effects. Using fractional counting the between-field variance is
reduced by 91.7% in the case of IF5, and by 79.2% in the case of IF2. However,
the differences in citation counts are not significantly affected by fractional
counting. These results accord with previous studies, but the longer citation
window of a fractionally counted IF5 can lead to significant improvement in the
normalization across fields.
"
541,"Citation impact of papers published from six prolific countries: A
  national comparison based on InCites data","  Using the InCites tool of Thomson Reuters, this study compares normalized
citation impact values calculated for China, Japan, France, Germany, United
States, and the UK throughout the time period from 1981 to 2010. The citation
impact values are normalized to four subject areas: natural sciences;
engineering and technology; medical and health sciences; and agricultural
sciences. The results show an increasing trend in citation impact values for
France, the UK and especially for Germany across the last thirty years in all
subject areas. The citation impact of papers from China is still at a
relatively low level (mostly below the world average), but the country follows
an increasing trend line. The USA exhibits a relatively stable pattern of high
citation impact values across the years. With small impact differences between
the publication years, the US trend is increasing in engineering and technology
but decreasing in medical and health sciences as well as in agricultural
sciences. Similar to the USA, Japan follows increasing as well as decreasing
trends in different subject areas, but the variability across the years is
small. In most of the years, papers from Japan perform below or approximately
at the world average in each subject area.
"
542,Scientific Utopia: I. Opening scientific communication,"  Existing norms for scientific communication are rooted in anachronistic
practices of bygone eras, making them needlessly inefficient. We outline a path
that moves away from the existing model of scientific communication to improve
the efficiency in meeting the purpose of public science - knowledge
accumulation. We call for six changes: (1) full embrace of digital
communication, (2) open access to all published research, (3) disentangling
publication from evaluation, (4) breaking the ""one article, one journal"" model
with a grading system for evaluation and diversified dissemination outlets, (5)
publishing peer review, and, (6) allowing open, continuous peer review. We
address conceptual and practical barriers to change, and provide examples
showing how the suggested practices are being used already. The critical
barriers to change are not technical or financial; they are social. While
scientists guard the status quo, they also have the power to change it.
"
543,"Recommendation on Academic Networks using Direction Aware Citation
  Analysis","  The literature search has always been an important part of an academic
research. It greatly helps to improve the quality of the research process and
output, and increase the efficiency of the researchers in terms of their novel
contribution to science. As the number of published papers increases every
year, a manual search becomes more exhaustive even with the help of today's
search engines since they are not specialized for this task. In academics, two
relevant papers do not always have to share keywords, cite one another, or even
be in the same field. Although a well-known paper is usually an easy pray in
such a hunt, relevant papers using a different terminology, especially recent
ones, are not obvious to the eye.
  In this work, we propose paper recommendation algorithms by using the
citation information among papers. The proposed algorithms are direction aware
in the sense that they can be tuned to find either recent or traditional
papers. The algorithms require a set of papers as input and recommend a set of
related ones. If the user wants to give negative or positive feedback on the
suggested paper set, the recommendation is refined. The search process can be
easily guided in that sense by relevance feedback. We show that this slight
guidance helps the user to reach a desired paper in a more efficient way. We
adapt our models and algorithms also for the venue and reviewer recommendation
tasks. Accuracy of the models and algorithms is thoroughly evaluated by
comparison with multiple baselines and algorithms from the literature in terms
of several objectives specific to citation, venue, and reviewer recommendation
tasks. All of these algorithms are implemented within a publicly available
web-service framework (http://theadvisor.osu.edu/) which currently uses the
data from DBLP and CiteSeer to construct the proposed citation graph.
"
544,Are e-readers suitable tools for scholarly work?,"  This paper aims to offer insights into the usability, acceptance and
limitations of e-readers with regard to the specific requirements of scholarly
text work. To fit into the academic workflow non-linear reading, bookmarking,
commenting, extracting text or the integration of non-textual elements must be
supported. A group of social science students were questioned about their
experiences with electronic publications for study purposes. This same group
executed several text-related tasks with the digitized material presented to
them in two different file formats on four different e-readers. Their
performances were subsequently evaluated by means of frequency analyses in
detail. Findings - e-Publications have made advances in the academic world;
however e-readers do not yet fit seamlessly into the established chain of
scholarly text-processing focusing on how readers use material during and after
reading. Our tests revealed major deficiencies in these techniques. With a
small number of participants (n=26) qualitative insights can be obtained, not
representative results. Further testing with participants from various
disciplines and of varying academic status is required to arrive at more
broadly applicable results. Practical implications - Our test results help to
optimize file conversion routines for scholarly texts. We evaluated our data on
the basis of descriptive statistics and abstained from any statistical
significance test. The usability test of e-readers in a scientific context
aligns with both studies on the prevalence of e-books in the sciences and
technical test reports of portable reading devices. Still, it takes a
distinctive angle in focusing on the characteristics and procedures of textual
work in the social sciences and measures the usability of e-readers and
file-features against these standards.
"
545,"An Integrated Impact Indicator (I3): A New Definition of ""Impact"" with
  Policy Relevance","  Allocation of research funding, as well as promotion and tenure decisions,
are increasingly made using indicators and impact factors drawn from citations
to published work. A debate among scientometricians about proper normalization
of citation counts has resolved with the creation of an Integrated Impact
Indicator (I3) that solves a number of problems found among previously used
indicators. The I3 applies non-parametric statistics using percentiles,
allowing highly-cited papers to be weighted more than less-cited ones. It
further allows unbundling of venues (i.e., journals or databases) at the
article level. Measures at the article level can be re-aggregated in terms of
units of evaluation. At the venue level, the I3 creates a properly weighted
alternative to the journal impact factor. I3 has the added advantage of
enabling and quantifying classifications such as the six percentile rank
classes used by the National Science Board's Science & Engineering Indicators.
"
546,"The Extraction of Community Structures from Publication Networks to
  Support Ethnographic Observations of Field Differences in Scientific
  Communication","  The scientific community of researchers in a research specialty is an
important unit of analysis for understanding the field specific shaping of
scientific communication practices. These scientific communities are, however,
a challenging unit of analysis to capture and compare because they overlap,
have fuzzy boundaries, and evolve over time. We describe a network analytic
approach that reveals the complexities of these communities through examination
of their publication networks in combination with insights from ethnographic
field studies. We suggest that the structures revealed indicate overlapping
sub- communities within a research specialty and we provide evidence that they
differ in disciplinary orientation and research practices. By mapping the
community structures of scientific fields we aim to increase confidence about
the domain of validity of ethnographic observations as well as of collaborative
patterns extracted from publication networks thereby enabling the systematic
study of field differences. The network analytic methods presented include
methods to optimize the delineation of a bibliographic data set in order to
adequately represent a research specialty, and methods to extract community
structures from this data. We demonstrate the application of these methods in a
case study of two research specialties in the physical and chemical sciences.
"
547,"Diachronic Linked Data: Towards Long-Term Preservation of Structured
  Interrelated Information","  The Linked Data Paradigm is one of the most promising technologies for
publishing, sharing, and connecting data on the Web, and offers a new way for
data integration and interoperability. However, the proliferation of
distributed, inter-connected sources of information and services on the Web
poses significant new challenges for managing consistently a huge number of
large datasets and their interdependencies. In this paper we focus on the key
problem of preserving evolving structured interlinked data. We argue that a
number of issues that hinder applications and users are related to the temporal
aspect that is intrinsic in linked data. We present a number of real use cases
to motivate our approach, we discuss the problems that occur, and propose a
direction for a solution.
"
548,"The citation-based indicator and combined impact indicator - New options
  for measuring impact","  Metrics based on percentile ranks (PRs) for measuring scholarly impact
involves complex treatment because of various defects such as overvaluing or
devaluing an object caused by percentile ranking schemes, ignoring precise
citation variation among those ranked next to each other, and inconsistency
caused by additional papers or citations. These defects are especially obvious
in a small-sized dataset. To avoid the complicated treatment of PRs based
metrics, we propose two new indicators - the citation-based indicator (CBI) and
the combined impact indicator (CII). Document types of publications are taken
into account. With the two indicators, one would no more be bothered by complex
issues encountered by PRs based indicators. For a small-sized dataset with less
than 100 papers, special calculation is no more needed. The CBI is based solely
on citation counts and the CII measures the integrate contributions of
publications and citations. Both virtual and empirical data are used so as to
compare the effect of related indicators. The CII and the PRs based indicator
I3 are highly correlated but the former reflects citation impact more and the
latter relates more to publications.
"
549,"Linking Social Networking Sites to Scholarly Information Portals by
  ScholarLib","  Online Social Networks usually provide no or limited way to access scholarly
information provided by Digital Libraries (DLs) in order to share and discuss
scholarly content with other online community members. The paper addresses the
potentials of Social Networking sites (SNSs) for science and proposes initial
use cases as well as a basic bi-directional model called ScholarLib for linking
SNSs to scholarly DLs. The major aim of ScholarLib is to make scholarly
information provided by DLs accessible at SNSs, and vice versa, to enhance
retrieval quality at DL side by social information provided by SNSs.
"
550,Public Data Integration with WebSmatch,"  Integrating open data sources can yield high value information but raises
major problems in terms of metadata extraction, data source integration and
visualization of integrated data. In this paper, we describe WebSmatch, a
flexible environment for Web data integration, based on a real, end-to-end data
integration scenario over public data from Data Publica. WebSmatch supports the
full process of importing, refining and integrating data sources and uses third
party tools for high quality visualization. We use a typical scenario of public
data integration which involves problems not solved by currents tools: poorly
structured input data sources (XLS files) and rich visualization of integrated
data.
"
551,Cumulative Revision Map,"  Unlike static documents, version-controlled documents are edited by one or
more authors over a certain period of time. Examples include large scale
computer code, papers authored by a team of scientists, and online discussion
boards. Such collaborative revision process makes traditional document modeling
and visualization techniques inappropriate. In this paper we propose a new
visualization technique for version-controlled documents that reveals
interesting authoring patterns in papers, computer code and Wikipedia articles.
The revealed authoring patterns are useful for the readers, participants in the
authoring process, and supervisors.
"
552,Uncertainties and Ambiguities in Percentiles and how to Avoid Them,"  The recently proposed fractional scoring scheme is used to attribute
publications to percentile rank classes. It is shown that in this way
uncertainties and ambiguities in the evaluation of percentile ranks do not
occur. Using the fractional scoring the total score of all papers exactly
reproduces the theoretical value.
"
553,Extraction of Historical Events from Wikipedia,"  The DBpedia project extracts structured information from Wikipedia and makes
it available on the web. Information is gathered mainly with the help of
infoboxes that contain structured information of the Wikipedia article. A lot
of information is only contained in the article body and is not yet included in
DBpedia. In this paper we focus on the extraction of historical events from
Wikipedia articles that are available for about 2,500 years for different
languages. We have extracted about 121,000 events with more than 325,000 links
to DBpedia entities and provide access to this data via a Web API, SPARQL
endpoint, Linked Data Interface and in a timeline application.
"
554,"Scientific Utopia: II. Restructuring incentives and practices to promote
  truth over publishability","  An academic scientist's professional success depends on publishing.
Publishing norms emphasize novel, positive results. As such, disciplinary
incentives encourage design, analysis, and reporting decisions that elicit
positive results and ignore negative results. Prior reports demonstrate how
these incentives inflate the rate of false effects in published science. When
incentives favor novelty over replication, false results persist in the
literature unchallenged, reducing efficiency in knowledge accumulation.
Previous suggestions to address this problem are unlikely to be effective. For
example, a journal of negative results publishes otherwise unpublishable
reports. This enshrines the low status of the journal and its content. The
persistence of false findings can be meliorated with strategies that make the
fundamental but abstract accuracy motive - getting it right - competitive with
the more tangible and concrete incentive - getting it published. We develop
strategies for improving scientific practices and knowledge accumulation that
account for ordinary human motivations and self-serving biases.
"
555,In praise of the referee,"  There has been a lively debate in many fields, including statistics and
related applied fields such as psychology and biomedical research, on possible
reforms of the scholarly publishing system. Currently, referees contribute so
much to improve scientific papers, both directly through constructive criticism
and indirectly through the threat of rejection. We discuss ways in which new
approaches to journal publication could continue to make use of the valuable
efforts of peer reviewers.
"
556,"The weakening relationship between the Impact Factor and papers'
  citations in the digital age","  Historically, papers have been physically bound to the journal in which they
were published but in the electronic age papers are available individually, no
longer tied to their respective journals. Hence, papers now can be read and
cited based on their own merits, independently of the journal's physical
availability, reputation, or Impact Factor. We compare the strength of the
relationship between journals' Impact Factors and the actual citations received
by their respective papers from 1902 to 2009. Throughout most of the 20th
century, papers' citation rates were increasingly linked to their respective
journals' Impact Factors. However, since 1990, the advent of the digital age,
the strength of the relation between Impact Factors and paper citations has
been decreasing. This decrease began sooner in physics, a field that was
quicker to make the transition into the electronic domain. Furthermore, since
1990, the proportion of highly cited papers coming from highly cited journals
has been decreasing, and accordingly, the proportion of highly cited papers not
coming from highly cited journals has also been increasing. Should this pattern
continue, it might bring an end to the use of the Impact Factor as a way to
evaluate the quality of journals, papers and researchers.
"
557,"Statistical inference on the h-index with an application to
  top-scientist performance","  Despite the huge amount of literature on h-index, few papers have been
devoted to the statistical analysis of h-index when a probabilistic
distribution is assumed for citation counts. The present contribution relies on
showing the available inferential techniques, by providing the details for
proper point and set estimation of the theoretical h-index. Moreover, some
issues on simultaneous inference - aimed to produce suitable scholar
comparisons - are carried out. Finally, the analysis of the citation dataset
for the Nobel Laureates (in the last five years) and for the Fields medallists
(from 2002 onward) is proposed.
"
558,ISWAR: An Imaging System with Watermarking and Attack Resilience,"  With the explosive growth of internet technology, easy transfer of digital
multimedia is feasible. However, this kind of convenience with which authorized
users can access information, turns out to be a mixed blessing due to
information piracy. The emerging field of Digital Rights Management (DRM)
systems addresses issues related to the intellectual property rights of digital
content. In this paper, an object-oriented (OO) DRM system, called ""Imaging
System with Watermarking and Attack Resilience"" (ISWAR), is presented that
generates and authenticates color images with embedded mechanisms for
protection against infringement of ownership rights as well as security
attacks. In addition to the methods, in the object-oriented sense, for
performing traditional encryption and decryption, the system implements methods
for visible and invisible watermarking. This paper presents one visible and one
invisible watermarking algorithm that have been integrated in the system. The
qualitative and quantitative results obtained for these two watermarking
algorithms with several benchmark images indicate that high-quality watermarked
images are produced by the algorithms. With the help of experimental results it
is demonstrated that the presented invisible watermarking techniques are
resilient to the well known benchmark attacks and hence a fail-safe method for
providing constant protection to ownership rights.
"
559,"Status Report of the DPHEP Study Group: Towards a Global Effort for
  Sustainable Data Preservation in High Energy Physics","  Data from high-energy physics (HEP) experiments are collected with
significant financial and human effort and are mostly unique. An
inter-experimental study group on HEP data preservation and long-term analysis
was convened as a panel of the International Committee for Future Accelerators
(ICFA). The group was formed by large collider-based experiments and
investigated the technical and organisational aspects of HEP data preservation.
An intermediate report was released in November 2009 addressing the general
issues of data preservation in HEP. This paper includes and extends the
intermediate report. It provides an analysis of the research case for data
preservation and a detailed description of the various projects at experiment,
laboratory and international levels. In addition, the paper provides a concrete
proposal for an international organisation in charge of the data management and
policies in high-energy physics.
"
560,Beyond citations: Scholars' visibility on the social Web,"  Traditionally, scholarly impact and visibility have been measured by counting
publications and citations in the scholarly literature. However, increasingly
scholars are also visible on the Web, establishing presences in a growing
variety of social ecosystems. But how wide and established is this presence,
and how do measures of social Web impact relate to their more traditional
counterparts? To answer this, we sampled 57 presenters from the 2010 Leiden STI
Conference, gathering publication and citations counts as well as data from the
presenters' Web ""footprints."" We found Web presence widespread and diverse: 84%
of scholars had homepages, 70% were on LinkedIn, 23% had public Google Scholar
profiles, and 16% were on Twitter. For sampled scholars' publications, social
reference manager bookmarks were compared to Scopus and Web of Science
citations; we found that Mendeley covers more than 80% of sampled articles, and
that Mendeley bookmarks are significantly correlated (r=.45) to Scopus citation
counts.
"
561,Publication Induced Research Analysis (PIRA) - Experiments on Real Data,"  This paper describes the first results obtained by implementing a novel
approach to rank vertices in a heterogeneous graph, based on the PageRank
family of algorithms and applied here to the bipartite graph of papers and
authors as a first evaluation of its relevance on real data samples. With this
approach to evaluate research activities, the ranking of a paper/author depends
on that of the papers/authors citing it/him or her. We compare the results
against existing ranking methods (including methods which simply apply PageRank
to the graph of papers or the graph of authors) through the analysis of simple
scenarios based on a real dataset built from DBLP and CiteseerX. The results
show that in all examined cases the obtained result is most pertinent with our
method which allows to orient our future work to optimizing the execution of
this algorithm.
"
562,Automatic Generation of OWL Ontology from XML Data Source,"  The eXtensible Markup Language (XML) can be used as data exchange format in
different domains. It allows different parties to exchange data by providing
common understanding of the basic concepts in the domain. XML covers the
syntactic level, but lacks support for reasoning. Ontology can provide a
semantic representation of domain knowledge which supports efficient reasoning
and expressive power. One of the most popular ontology languages is the Web
Ontology Language (OWL). It can represent domain knowledge using classes,
properties, axioms and instances for the use in a distributed environment such
as the World Wide Web. This paper presents a new method for automatic
generation of OWL ontology from XML data sources.
"
563,A Novel Semantic Software for Astronomical Concepts,"  We have created a new semantic tool called AstroConcepts, providing
definitions of astronomical concepts present on Web pages. This tool is a
Google Chrome plug-in that interrogates the Etymological Dictionary of
Astronomy and Astrophysics, developed at Paris Observatory. Thanks to this
tool, if one selects an astronomical concept on a web page, a pop-up window
will display the definition of the available English or French terms. Another
expected use of this facility could be its implementation in Virtual
Observatory services.
"
564,Finding Quality Issues in SKOS Vocabularies,"  The Simple Knowledge Organization System (SKOS) is a standard model for
controlled vocabularies on the Web. However, SKOS vocabularies often differ in
terms of quality, which reduces their applicability across system boundaries.
Here we investigate how we can support taxonomists in improving SKOS
vocabularies by pointing out quality issues that go beyond the integrity
constraints defined in the SKOS specification. We identified potential
quantifiable quality issues and formalized them into computable quality
checking functions that can find affected resources in a given SKOS vocabulary.
We implemented these functions in the qSKOS quality assessment tool, analyzed
15 existing vocabularies, and found possible quality issues in all of them.
"
565,"How to analyse percentile impact data meaningfully in bibliometrics: The
  statistical analysis of distributions, percentile rank classes and top-cited
  papers","  According to current research in bibliometrics, percentiles (or percentile
rank classes) are the most suitable method for normalising the citation counts
of individual publications in terms of the subject area, the document type and
the publication year. Up to now, bibliometric research has concerned itself
primarily with the calculation of percentiles. This study suggests how
percentiles can be analysed meaningfully for an evaluation study. Publication
sets from four universities are compared with each other to provide sample
data. These suggestions take into account on the one hand the distribution of
percentiles over the publications in the sets (here: universities) and on the
other hand concentrate on the range of publications with the highest citation
impact - that is, the range which is usually of most interest in the evaluation
of scientific performance.
"
566,"CyberChair: A Web-Based Groupware Application to Facilitate the Paper
  Reviewing Process","  In this paper we describe CyberChair, a web-based groupware application that
supports the review process for technical contributions to conferences.
CyberChair deals with most administrative tasks that are involved in the review
process, such as storing author information, abstracts, (camera-ready) papers
and reviews. It generates several overviews based on the reviews which support
the Program Committee (PC) in selecting the best papers. CyberChair points out
conflicting reviews and offers the reviewers means to easily communicate to
solve these conflicts. In his paper Identify the Champion, O. Nierstrasz
describes this review process in terms of a pattern language. CyberChair
supports PCs by using these patterns in its implementation.
"
567,Runaway Events Dominate the Heavy Tail of Citation Distributions,"  Statistical distributions with heavy tails are ubiquitous in natural and
social phenomena. Since the entries in heavy tail have disproportional
significance, the knowledge of its exact shape is very important. Citations of
scientific papers form one of the best-known heavy tail distributions. Even in
this case there is a considerable debate whether citation distribution follows
the log-normal or power-law fit. The goal of our study is to solve this debate
by measuring citation distribution for a very large and homogeneous data. We
measured citation distribution for 418,438 Physics papers published in
1980-1989 and cited by 2008. While the log-normal fit deviates too strong from
the data, the discrete power-law function with the exponent $\gamma=3.15$ does
better and fits 99.955% of the data. However, the extreme tail of the
distribution deviates upward even from the power-law fit and exhibits a
dramatic ""runaway"" behavior. The onset of the runaway regime is revealed
macroscopically as the paper garners 1000-1500 citations, however the
microscopic measurements of autocorrelation in citation rates are able to
predict this behavior in advance.
"
568,Extending Term Suggestion with Author Names,"  Term suggestion or recommendation modules can help users to formulate their
queries by mapping their personal vocabularies onto the specialized vocabulary
of a digital library. While we examined actual user queries of the social
sciences digital library Sowiport we could see that nearly one third of the
users were explicitly looking for author names rather than terms. Common term
recommenders neglect this fact. By picking up the idea of polyrepresentation we
could show that in a standardized IR evaluation setting we can significantly
increase the retrieval performances by adding topical-related author names to
the query. This positive effect only appears when the query is additionally
expanded with thesaurus terms. By just adding the author names to a query we
often observe a query drift which results in worse results.
"
569,Improving Retrieval Results with discipline-specific Query Expansion,"  Choosing the right terms to describe an information need is becoming more
difficult as the amount of available information increases.
Search-Term-Recommendation (STR) systems can help to overcome these problems.
This paper evaluates the benefits that may be gained from the use of STRs in
Query Expansion (QE). We create 17 STRs, 16 based on specific disciplines and
one giving general recommendations, and compare the retrieval performance of
these STRs. The main findings are: (1) QE with specific STRs leads to
significantly better results than QE with a general STR, (2) QE with specific
STRs selected by a heuristic mechanism of topic classification leads to better
results than the general STR, however (3) selecting the best matching specific
STR in an automatic way is a major challenge of this process.
"
570,"Complex Systems Science: Dreams of Universality, Reality of
  Interdisciplinarity","  Using a large database (~ 215 000 records) of relevant articles, we
empirically study the ""complex systems"" field and its claims to find universal
principles applying to systems in general. The study of references shared by
the papers allows us to obtain a global point of view on the structure of this
highly interdisciplinary field. We show that its overall coherence does not
arise from a universal theory but instead from computational techniques and
fruitful adaptations of the idea of self-organization to specific systems. We
also find that communication between different disciplines goes through
specific ""trading zones"", ie sub-communities that create an interface around
specific tools (a DNA microchip) or concepts (a network).
"
571,Search Strategies of Library Search Experts,"  Search engines like Google, Yahoo or Bing are an excellent support for
finding documents, but this strength also imposes a limitation. As they are
optimized for document retrieval tasks, they perform less well when it comes to
more complex search needs. Complex search tasks are usually described as
open-ended, abstract and poorly defined information needs with a multifaceted
character. In this paper we will present the results of an experiment carried
out with information professionals from libraries and museums in the course of
a search contest. The aim of the experiment was to analyze the search
strategies of experienced information workers trying to tackle search tasks of
varying complexity and get qualitative results on the impact of time pressure
on such an experiment.
"
572,Online open neuroimaging mass meta-analysis,"  We describe a system for meta-analysis where a wiki stores numerical data in
a simple format and a web service performs the numerical computation.
  We initially apply the system on multiple meta-analyses of structural
neuroimaging data results. The described system allows for mass meta-analysis,
e.g., meta-analysis across multiple brain regions and multiple mental
disorders.
"
573,"Green and Gold Open Access Percentages and Growth, by Discipline","  Most refereed journal articles today are published in subscription journals,
accessible only to subscribing institutions, hence losing considerable research
impact. Making articles freely accessible online (""Open Access,"" OA) maximizes
their impact. Articles can be made OA in two ways: by self-archiving them on
the web (""Green OA"") or by publishing them in OA journals (""Gold OA""). We
compared the percent and growth rate of Green and Gold OA for 14 disciplines in
two random samples of 1300 articles per discipline out of the 12,500 journals
indexed by Thomson-Reuters-ISI using a robot that trawled the web for OA
full-texts. We sampled in 2009 and 2011 for publication year ranges 1998-2006
and 2005-2010, respectively. Green OA (21.4%) exceeds Gold OA (2.4%) in
proportion and growth rate in all but the biomedical disciplines, probably
because it can be provided for all journals articles and does not require
paying extra Gold OA publication fees. The spontaneous overall OA growth rate
is still very slow (about 1% per year). If institutions make Green OA
self-archiving mandatory, however, it triples percent Green OA as well as
accelerating its growth rate.
"
574,Science Visualization and Discursive Knowledge,"  Positional and relational perspectives on network data have led to two
different research traditions in textual analysis and social network analysis,
respectively. Latent Semantic Analysis (LSA) focuses on the latent dimensions
in textual data; social network analysis (SNA) on the observable networks. The
two coupled topographies of information-processing in the network space and
meaning-processing in the vector space operate with different (nonlinear)
dynamics. The historical dynamics of information processing in observable
networks organizes the system into instantiations; the systems dynamics,
however, can be considered as self-organizing in terms of fluxes of
communication along the various dimensions that operate with different codes.
The development over time adds evolutionary differentiation to the historical
integration; a richer structure can process more complexity.
"
575,"Statistics for the Dynamic Analysis of Scientometric Data: The evolution
  of the sciences in terms of trajectories and regimes","  The gap in statistics between multi-variate and time-series analysis can be
bridged by using entropy statistics and recent developments in
multi-dimensional scaling. For explaining the evolution of the sciences as
non-linear dynamics, the configurations among variables can be important in
addition to the statistics of individual variables and trend lines. Animations
enable us to combine multiple perspectives (based on configurations of
variables) and to visualize path-dependencies in terms of trajectories and
regimes. Path-dependent transitions and systems formation can be tested using
entropy statistics.
"
576,"An empirical analysis of the use of alphabetical authorship in
  scientific publishing","  There are different ways in which the authors of a scientific publication can
determine the order in which their names are listed. Sometimes author names are
simply listed alphabetically. In other cases, authorship order is determined
based on the contribution authors have made to a publication.
Contribution-based authorship can facilitate proper credit assignment, for
instance by giving most credits to the first author. In the case of
alphabetical authorship, nothing can be inferred about the relative
contribution made by the different authors of a publication. In this paper, we
present an empirical analysis of the use of alphabetical authorship in
scientific publishing. Our analysis covers all fields of science. We find that
the use of alphabetical authorship is declining over time. In 2011, the authors
of less than 4% of all publications intentionally chose to list their names
alphabetically. The use of alphabetical authorship is most common in
mathematics, economics (including finance), and high energy physics. Also, the
use of alphabetical authorship is relatively more common in the case of
publications with either a small or a large number of authors.
"
577,The Planetary Project: Towards eMath3.0,"  The Planetary project develops a general framework - the Planetary system -
for social semantic portals that support users in interacting with STEM
(Science/Technology/Engineering/Mathematics) documents. Developed from an
initial attempt to replace the aging portal of PlanetMath.org with a mashup of
existing MKM technologies, the Planetary system is now in a state, where it can
serve as a basis for various eMath3.0 portals, ranging from eLearning systems
over scientific archives to semantic help systems.
"
578,Three Steps to Heaven: Semantic Publishing in a Real World Workflow,"  Semantic publishing offers the promise of computable papers, enriched
visualisation and a realisation of the linked data ideal. In reality, however,
the publication process contrives to prevent richer semantics while culminating
in a `lumpen' PDF. In this paper, we discuss a web-first approach to
publication, and describe a three-tiered approach which integrates with the
existing authoring tooling. Critically, although it adds limited semantics, it
does provide value to all the participants in the process: the author, the
reader and the machine.
"
579,Data Preservation and Long Term Analysis in High Energy Physics,"  Several important and unique experimental high-energy physics programmes at a
variety of facilities are coming to an end, including those at HERA, the
B-factories and the Tevatron. The wealth of physics data from these experiments
is the result of a significant financial and human effort, and yet until
recently no coherent strategy existed for data preservation and re-use. To
address this issue, an inter-experimental study group on data preservation and
long-term analysis in high-energy physics was convened at the end of 2008,
publishing an interim report in 2009. The membership of the study group has
since expanded, including the addition of the LHC experiments, and a full
status report has now been released. This report greatly expands on the ideas
contained in the original publication and provides a more solid set of
recommendations, not only concerning data preservation and its implementation
in high-energy physics, but also the future direction and organisational model
of the study group. The main messages of the status report were presented for
the first time at the 2012 International Conference on Computing in High Energy
and Nuclear Physics and are summarised in these proceedings.
"
580,The H1 Data Preservation Project,"  The H1 data preservation project was started in 2009 as part of the global
data preservation initiative in high-energy physics, DPHEP. In order to retain
the full potential for future improvements, the H1 Collaboration aims for level
4 of the DPHEP recommendations, which requires the full simulation and
reconstruction chain as well as the data to be preserved for future analysis. A
major goal of the H1 project is therefore to provide secure, long-lived and
validated access to the H1 data and analysis software, which is realised in
collaboration with DESY-IT using virtualisation techniques. By implementing
such a system, it is hoped that the lifetime of the unique ep collision data
from HERA will be extended, providing the possibility for novel analysis in the
future. The preservation of the data and software is performed alongside a
consolidation programme of digital and non-digital documentation, some of which
dates back to the early 1980s. A new organisational model of the H1
Collaboration, reflecting the change to the long term phase, is to be adopted
in July 2012.
"
581,IACTalks: an on-line archive of astronomy-related seminars,"  We present IACTalks, a free and open access seminars archive
(http://iactalks.iac.es) aimed at promoting astronomy and the exchange of ideas
by providing high-quality scientific seminars to the astronomical community.
The archive of seminars and talks given at the Instituto de Astrofi\'isica de
Canarias goes back to 2008. Over 360 talks and seminars are now freely
available by streaming over the internet. We describe the user interface, which
includes two video streams, one showing the speaker, the other the
presentation. A search function is available, and seminars are indexed by
keywords and in some cases by series, such as special training courses or the
2011 Winter School of Astrophysics, on secular evolution of galaxies. The
archive is made available as an open resource, to be used by scientists and the
public.
"
582,The observational roots of reference of the semantic web,"  Shared reference is an essential aspect of meaning. It is also indispensable
for the semantic web, since it enables to weave the global graph, i.e., it
allows different users to contribute to an identical referent. For example, an
essential kind of referent is a geographic place, to which users may contribute
observations. We argue for a human-centric, operational approach towards
reference, based on respective human competences. These competences encompass
perceptual, cognitive as well as technical ones, and together they allow humans
to inter-subjectively refer to a phenomenon in their environment. The
technology stack of the semantic web should be extended by such operations.
This would allow establishing new kinds of observation-based reference systems
that help constrain and integrate the semantic web bottom-up.
"
583,"Telescope Bibliographies: an Essential Component of Archival Data
  Management and Operations","  Assessing the impact of astronomical facilities rests upon an evaluation of
the scientific discoveries which their data have enabled. Telescope
bibliographies, which link data products with the literature, provide a way to
use bibliometrics as an impact measure for the underlying data. In this paper
we argue that the creation and maintenance of telescope bibliographies should
be considered an integral part of an observatory's operations. We review the
existing tools, services, and workflows which support these curation
activities, giving an estimate of the effort and expertise required to maintain
an archive-based telescope bibliography.
"
584,"A scientometrics law about co-authors and their ranking. The co-author
  core","  Rather than ""measuring"" a scientist impact through the number of citations
which his/her published work can have generated, isn't it more appropriate to
consider his/her value through his/her scientific network performance
illustrated by his/her co-author role, thus focussing on his/her joint
publications, - and their impact through citations? Whence, on one hand, this
paper very briefly examines bibliometric laws, like the $h$-index and
subsequent debate about co-authorship effects, but on the other hand, proposes
a measure of collaborative work through a new index. Based on data about the
publication output of a specific research group, a new bibliometric law is
found.
  Let a co-author $C$ have written $J$ (joint) publications with one or several
colleagues. Rank all the co-authors of that individual according to their
number of joint publications, giving a rank $r$ to each co-author, starting
with $r=1$ for the most prolific.
  It is empirically found that a very simple relationship holds between the
number of joint publications $J$ by coauthors and their rank of importance,
i.e. $J \propto 1/r$. Thereafter, in the same spirit as for the Hirsch core,
one can define a ""co-author core"", and introduce indices operating on an
author. It is emphasized that the new index has a quite different
(philosophical) perspective that the $h$-index. In the present case, one
focusses on ""relevant"" persons rather than on ""relevant"" publications.
  Although the numerical discussion is based on one case, there is little doubt
that the law can be verified in many other situations. Therefore, variants and
generalizations could be later produced in order to quantify co-author roles,
in a temporary or long lasting stable team(s), and lead to criteria about
funding, career measurements or even induce career strategies.
"
585,Visualising Virtual Communities: From Erd\H{o}s to the Arts,"  Monitoring communities has become increasingly easy on the web as the number
of visualisation tools and amount of data available about communities increase.
It is possible to visualise connections on social and professional networks
such as Facebook in the form of mathematical graphs. It is also possible to
visualise connections between authors of papers. In particular, Microsoft
Academic Search now has a large corpus of information on publications, together
with author and citation information, that can be visualised in a number of
ways. In mathematical circles, the concept of the ""Erd\H{o}s number"" has been
introduced, in honour of the Hungarian mathematician Paul Erd\H{o}s, measuring
the ""collaborative distance"" of a person away from Erd\H{o}s through links by
co-author. Similar metrics have been proposed in other fields, including
acting. The possibility of exploring and visualising such links in arts fields
is proposed in this paper.
"
586,On the time dependence of the $h$-index,"  The time dependence of the $h$-index is analyzed by considering the average
behaviour of $h$ as a function of the academic age $A_A$ for about 1400 Italian
physicists, with career lengths spanning from 3 to 46 years. The individual
$h$-index is strongly correlated with the square root of the total citations
$N_C$: $h \approx 0.53 \sqrt{N_C}$. For academic ages ranging from 12 to 24
years, the distribution of the time scaled index $h/\sqrt{A_A}$ is
approximately time-independent and it is well described by the Gompertz
function. The time scaled index $h/\sqrt{A_A}$ has an average approximately
equal to 3.8 and a standard deviation approximately equal to 1.6. Finally, the
time scaled index $h/\sqrt{A_A}$ appears to be strongly correlated with the
contemporary $h$-index $h_c$.
"
587,Managing Research Data in Big Science,"  The project which led to this report was funded by JISC in 2010--2011 as part
of its 'Managing Research Data' programme, to examine the way in which Big
Science data is managed, and produce any recommendations which may be
appropriate.
  Big science data is different: it comes in large volumes, and it is shared
and exploited in ways which may differ from other disciplines. This project has
explored these differences using as a case-study Gravitational Wave data
generated by the LSC, and has produced recommendations intended to be useful
variously to JISC, the funding council (STFC) and the LSC community.
  In Sect. 1 we define what we mean by 'big science', describe the overall data
culture there, laying stress on how it necessarily or contingently differs from
other disciplines.
  In Sect. 2 we discuss the benefits of a formal data-preservation strategy,
and the cases for open data and for well-preserved data that follow from that.
This leads to our recommendations that, in essence, funders should adopt rather
light-touch prescriptions regarding data preservation planning: normal data
management practice, in the areas under study, corresponds to notably good
practice in most other areas, so that the only change we suggest is to make
this planning more formal, which makes it more easily auditable, and more
amenable to constructive criticism.
  In Sect. 3 we briefly discuss the LIGO data management plan, and pull
together whatever information is available on the estimation of digital
preservation costs.
  The report is informed, throughout, by the OAIS reference model for an open
archive.
"
588,Developments and Obstacles in Chinese eBook Market,"  The purpose of this study was to provide insights into the eBook market in
China through case studies on eBook companies and a survey research with
individual eBook users. The information from three companies, Beijing Superstar
Electric Company, Beijing Founder APABI Technology Limited, and Beijing Sursen
Electronic Technology Company Limited, showed that the B2B market has been
developed due to the huge requirement from organization customers, universities
libraries in particularly, and the B2C market is still immature. The
information from interviews and relative data revealed that both Superstar and
Sursen have serious copyright infringement which is an important problem
impeding the further development of the eBook market. The questionnaire
explored awareness, purchase, reading and other experiences of eBook end-users.
Questions indicated that readers were attracted by the technical advantages
including costless to copy, easy to transfer, searchable and easy to store, but
did not want to pay for eBooks. Because the computers, especially desktop PCs,
were the main device for reading and the CRT displays were massive used while
there were few dedicated reading device in the market, many eBook end-users
still preferred to read extended passages of text on papers rather than
screens. Today the copyrights issue, user acceptance and the reading device are
three significant obstacles for eBook industry in China.
"
589,"An Integrated, Conditional Model of Information Extraction and
  Coreference with Applications to Citation Matching","  Although information extraction and coreference resolution appear together in
many applications, most current systems perform them as ndependent steps. This
paper describes an approach to integrated inference for extraction and
coreference based on conditionally-trained undirected graphical models. We
discuss the advantages of conditional probability training, and of a
coreference model structure based on graph partitioning. On a data set of
research paper citations, we show significant reduction in error by using
extraction uncertainty to improve coreference citation matching accuracy, and
using coreference to improve the accuracy of the extracted fields.
"
590,A Revised Publication Model for ECML PKDD,"  ECML PKDD is the main European conference on machine learning and data
mining. Since its foundation it implemented the publication model common in
computer science: there was one conference deadline; conference submissions
were reviewed by a program committee; papers were accepted with a low
acceptance rate. Proceedings were published in several Springer Lecture Notes
in Artificial (LNAI) volumes, while selected papers were invited to special
issues of the Machine Learning and Data Mining and Knowledge Discovery
journals. In recent years, this model has however come under stress. Problems
include: reviews are of highly variable quality; the purpose of bringing the
community together is lost; reviewing workloads are high; the information
content of conferences and journals decreases; there is confusion among
scientists in interdisciplinary contexts. In this paper, we present a new
publication model, which will be adopted for the ECML PKDD 2013 conference, and
aims to solve some of the problems of the traditional model. The key feature of
this model is the creation of a journal track, which is open to submissions all
year long and allows for revision cycles.
"
591,Recent advances in bibliometric indexes and the PaperRank problem,"  Bibliometric indexes are customary used in evaluating the impact of
scientific research, even though it is very well known that in different
research areas they may range in very different intervals. Sometimes, this is
evident even within a single given field of investigation making very difficult
(and inaccurate) the assessment of scientific papers. On the other hand, the
problem can be recast in the same framework which has allowed to efficiently
cope with the ordering of web-pages, i.e., to formulate the PageRank of Google.
For this reason, we call such problem the PaperRank problem, here solved by
using a similar approach to that employed by PageRank. The obtained solution,
which is mathematically grounded, will be used to compare the usual heuristics
of the number of citations with a new one here proposed. Some numerical tests
show that the new heuristics is much more reliable than the currently used
ones, based on the bare number of citations. Moreover, we show that our model
improves on recently proposed ones.
"
592,"Towards a Book Publishers Citation Reports. First approach using the
  Book Citation Index","  The absence of books and book chapters in the Web of Science Citation Indexes
(SCI, SSCI and A&HCI) has always been considered an important flaw but the
Thomson Reuters 'Book Citation Index' database was finally available in October
of 2010 indexing 29,618 books and 379,082 book chapters. The Book Citation
Index opens a new window of opportunities for analyzing these fields from a
bibliometric point of view. The main objective of this article is to analyze
different impact indicators referred to the scientific publishers included in
the Book Citation Index for the Social Sciences and Humanities fields during
2006-2011. This way we construct what we have called the 'Book Publishers
Citation Reports'. For this, we present a total of 19 rankings according to the
different disciplines in Humanities & Arts and Social Sciences & Law with six
indicators for scientific publishers
"
593,Electronic administration in Spain: from its beginnings to the present,"  This study presents the basic lines of electronic administration in Spain.
The complexity of the Spanish political-administrative system makes such a
study challenging, in view of the considerable degree of autonomy and
competences of the regional administrative bodies and local agencies with
respect to the central government, the former being more visible in the 17
regions of Spain. Nonetheless, the central government maintains a series of
legal instruments that allow a certain common framework of action to be
imposed, aside from what is put into effect through diverse programs aimed
precisely to develop common tools for the regions and municipalities of Spain.
  After an introduction that provides some necessary background, this study
describes the legislative framework in which Spain's electronic administrative
system has developed. The data included in the study refer to investment in
information and communication technologies (ICT) and the services offered by
the different Administrations on the internet; internet access by citizens,
homes, businesses, and employees, as well as the interactivity existing with
administrations by means of the internet; the origins and rise of various
political initiatives of the Central Government involving electronic
administration; and finally, the situation of civil service personnel, as
catalysts of the success of Information Society in the Public Administration
within Spain.
"
594,"The Distributed Ontology Language (DOL): Use Cases, Syntax, and
  Extensibility","  The Distributed Ontology Language (DOL) is currently being standardized
within the OntoIOp (Ontology Integration and Interoperability) activity of
ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies
formalized in heterogeneous logics, (2) modular ontologies, (3) links between
ontologies, and (4) annotation of ontologies. This paper presents the current
state of DOL's standardization. It focuses on use cases where distributed
ontologies enable interoperability and reusability. We demonstrate relevant
features of the DOL syntax and semantics and explain how these integrate into
existing knowledge engineering environments.
"
595,An Automat for the Semantic Processing of Structured Information,"  Using the database of the PuertoTerm project, an indexing system based on the
cognitive model of Brigitte Enders was built. By analyzing the cognitive
strategies of three abstractors, we built an automat that serves to simulate
human indexing processes. The automat allows the texts integrated in the system
to be assessed, evaluated and grouped by means of the bipartite spectral graph
partitioning algorithm, which also permits visualization of the terms and the
documents. The system features an ontology and a database to enhance its
operativity. As a result of the application, we achieved better rates of
exhaustivity in the indexing of documents, as well as greater precision and
retrieval of information, with high levels of efficiency.
"
596,Semantic Web Requirements through Web Mining Techniques,"  In recent years, Semantic web has become a topic of active research in
several fields of computer science and has applied in a wide range of domains
such as bioinformatics, life sciences, and knowledge management. The two
fast-developing research areas semantic web and web mining can complement each
other and their different techniques can be used jointly or separately to solve
the issues in both areas. In addition, since shifting from current web to
semantic web mainly depends on the enhancement of knowledge, web mining can
play a key role in facing numerous challenges of this changing. In this paper,
we analyze and classify the application of divers web mining techniques in
different challenges of the semantic web in form of an analytical framework.
"
597,Tracing scientist's research trends realtimely,"  In this research, we propose a method to trace scientists' research trends
realtimely. By monitoring the downloads of scientific articles in the journal
of Scientometrics for 744 hours, namely one month, we investigate the download
statistics. Then we aggregate the keywords in these downloaded research papers,
and analyze the trends of article downloading and keyword downloading.
Furthermore, taking both the download of keywords and articles into
consideration, we design a method to detect the emerging research trends. We
find that in scientometrics field, social media, new indices to quantify
scientific productivity (g-index), webometrics, semantic, text mining, open
access are emerging fields that scientometrics researchers are focusing on.
"
598,"Exploring scientists' working timetable: Do scientists often work
  overtime?","  A novel method is proposed to monitor and record scientists' working
timetable. We record the downloads information of scientific papers real-timely
from Springer round the clock, and try to explore scientists' working habits.
As our observation demonstrates, many scientists are still engaged in their
research after working hours every day. Many of them work far into the night,
even till next morning. In addition, research work also intrudes into their
weekends. Different working time patterns are revealed. In the US, overnight
work is more prevalent among scientists, while Chinese scientists mostly have
busy weekends with their scientific research.
"
599,Statistical Common Author Networks (SCAN),"  A new method for visualizing the relatedness of scientific areas is developed
that is based on measuring the overlap of researchers between areas. It is
found that closely related areas have a high propensity to share a larger
number of common authors. A methodology for comparing areas of vastly different
sizes and to handle name homonymy is constructed, allowing for the robust
deployment of this method on real data sets. A statistical analysis of the
probability distributions of the common author overlap that accounts for noise
is carried out along with the production of network maps with weighted links
proportional to the overlap strength. This is demonstrated on two case studies,
complexity science and neutrino physics, where the level of relatedness of
areas within each area is expected to vary greatly. It is found that the
results returned by this method closely match the intuitive expectation that
the broad, multidisciplinary area of complexity science possesses areas that
are weakly related to each other while the much narrower area of neutrino
physics shows very strongly related areas.
"
600,The Authorship Dilemma: Alphabetical or Contribution?,"  Scientific communities have adopted different conventions for ordering
authors on publications. Are these choices inconsequential, or do they have
significant influence on individual authors, the quality of the projects
completed, and research communities at large? What are the trade-offs of using
one convention over another? In order to investigate these questions, we
formulate a basic two-player game theoretic model, which already illustrates
interesting phenomena that can occur in more realistic settings.
  We find that alphabetical ordering can improve research quality, while
contribution-based ordering leads to a denser collaboration network and a
greater number of publications. Contrary to the assumption that free riding is
a weakness of the alphabetical ordering scheme, this phenomenon can occur under
any contribution scheme, and the worst case occurs under contribution-based
ordering. Finally, we show how authors working on multiple projects can
cooperate to attain optimal research quality and eliminate free riding given
either contribution scheme.
"
601,"Leveraging Subjective Human Annotation for Clustering Historic Newspaper
  Articles","  The New York Public Library is participating in the Chronicling America
initiative to develop an online searchable database of historically significant
newspaper articles. Microfilm copies of the newspapers are scanned and high
resolution Optical Character Recognition (OCR) software is run on them. The
text from the OCR provides a wealth of data and opinion for researchers and
historians. However, categorization of articles provided by the OCR engine is
rudimentary and a large number of the articles are labeled editorial without
further grouping. Manually sorting articles into fine-grained categories is
time consuming if not impossible given the size of the corpus. This paper
studies techniques for automatic categorization of newspaper articles so as to
enhance search and retrieval on the archive. We explore unsupervised (e.g.
KMeans) and semi-supervised (e.g. constrained clustering) learning algorithms
to develop article categorization schemes geared towards the needs of
end-users. A pilot study was designed to understand whether there was unanimous
agreement amongst patrons regarding how articles can be categorized. It was
found that the task was very subjective and consequently automated algorithms
that could deal with subjective labels were used. While the small scale pilot
study was extremely helpful in designing machine learning algorithms, a much
larger system needs to be developed to collect annotations from users of the
archive. The ""BODHI"" system currently being developed is a step in that
direction, allowing users to correct wrongly scanned OCR and providing keywords
and tags for newspaper articles used frequently. On successful implementation
of the beta version of this system, we hope that it can be integrated with
existing software being developed for the Chronicling America project.
"
602,Scientometrics,"  The paper provides an overview of the field of scientometrics, that is: the
study of science, technology, and innovation from a quantitative perspective.
We cover major historical milestones in the development of this specialism from
the 1960s to today and discuss its relationship with the sociology of
scientific knowledge, the library and information sciences, and science policy
issues such as indicator development. The disciplinary organization of
scientometrics is analyzed both conceptually and empirically, using a map of
journals cited in the core journal of the field, entitled Scientometrics. A
state-of-the-art review of five major research threads is provided: (1) the
measurement of impact; (2) the delineation of reference sets; (3) theories of
citation; (4) mapping science; and (5) the policy and management contexts of
indicator developments.
"
603,"Source normalized indicators of citation impact: An overview of
  different approaches and an empirical comparison","  Different scientific fields have different citation practices. Citation-based
bibliometric indicators need to normalize for such differences between fields
in order to allow for meaningful between-field comparisons of citation impact.
Traditionally, normalization for field differences has usually been done based
on a field classification system. In this approach, each publication belongs to
one or more fields and the citation impact of a publication is calculated
relative to the other publications in the same field. Recently, the idea of
source normalization was introduced, which offers an alternative approach to
normalize for field differences. In this approach, normalization is done by
looking at the referencing behavior of citing publications or citing journals.
In this paper, we provide an overview of a number of source normalization
approaches and we empirically compare these approaches with a traditional
normalization approach based on a field classification system. We also pay
attention to the issue of the selection of the journals to be included in a
normalization for field differences. Our analysis indicates a number of
problems of the traditional classification-system-based normalization approach,
suggesting that source normalization approaches may yield more accurate
results.
"
604,Supporting Structured Browsing for Full-Text Scientific Research Reports,"  Scientific research is highly structured and some of that structure is
reflected in research reports. Traditional scientific research reports are
yielding to interactive documents which expose their internal structure and are
richly linked to other materials. In these changes, there are opportunities to
take advantage of the structure in scientific research reports which previously
have not been systematically captured. Thus, we explore ways of capturing more
of the structure of research in reports about the research and we use that
structure to support the development of a new generation of document browsers
which include novel interaction widgets. We apply the browsers incorporating
the conceptual modeling framework to full-text research reports from the Public
Library of Science (PLoS). In addition, we describe the application of
model-oriented constructs to facilitating highly interlinked digital libraries.
"
605,"A History of Cluster Analysis Using the Classification Society's
  Bibliography Over Four Decades","  The Classification Literature Automated Search Service, an annual
bibliography based on citation of one or more of a set of around 80 book or
journal publications, ran from 1972 to 2012. We analyze here the years 1994 to
2011. The Classification Society's Service, as it was termed, has been produced
by the Classification Society. In earlier decades it was distributed as a
diskette or CD with the Journal of Classification. Among our findings are the
following: an enormous increase in scholarly production post approximately
2000; a very major increase in quantity, coupled with work in different
disciplines, from approximately 2004; and a major shift also from cluster
analysis in earlier times having mathematics and psychology as disciplines of
the journals published in, and affiliations of authors, contrasted with, in
more recent times, a ""centre of gravity"" in management and engineering.
"
606,"World citation and collaboration networks: uncovering the role of
  geography in science","  Modern information and communication technologies, especially the Internet,
have diminished the role of spatial distances and territorial boundaries on the
access and transmissibility of information. This has enabled scientists for
closer collaboration and internationalization. Nevertheless, geography remains
an important factor affecting the dynamics of science. Here we present a
systematic analysis of citation and collaboration networks between cities and
countries, by assigning papers to the geographic locations of their authors'
affiliations. The citation flows as well as the collaboration strengths between
cities decrease with the distance between them and follow gravity laws. In
addition, the total research impact of a country grows linearly with the amount
of national funding for research & development. However, the average impact
reveals a peculiar threshold effect: the scientific output of a country may
reach an impact larger than the world average only if the country invests more
than about 100,000 USD per researcher annually.
"
607,Some modifications to the SNIP journal impact indicator,"  The SNIP (source normalized impact per paper) indicator is an indicator of
the citation impact of scientific journals. The indicator, introduced by Henk
Moed in 2010, is included in Elsevier's Scopus database. The SNIP indicator
uses a source normalized approach to correct for differences in citation
practices between scientific fields. The strength of this approach is that it
does not require a field classification system in which the boundaries of
fields are explicitly defined. In this paper, a number of modifications that
will be made to the SNIP indicator are explained, and the advantages of the
resulting revised SNIP indicator are pointed out. It is argued that the
original SNIP indicator has some counterintuitive properties, and it is shown
mathematically that the revised SNIP indicator does not have these properties.
Empirically, the differences between the original SNIP indicator and the
revised one turn out to be relatively small, although some systematic
differences can be observed. Relations with other source normalized indicators
proposed in the literature are discussed as well.
"
608,Finding and Recommending Scholarly Articles,"  The rate at which scholarly literature is being produced has been increasing
at approximately 3.5 percent per year for decades. This means that during a
typical 40 year career the amount of new literature produced each year
increases by a factor of four. The methods scholars use to discover relevant
literature must change. Just like everybody else involved in information
discovery, scholars are confronted with information overload. Two decades ago,
this discovery process essentially consisted of paging through abstract books,
talking to colleagues and librarians, and browsing journals. A time-consuming
process, which could even be longer if material had to be shipped from
elsewhere. Now much of this discovery process is mediated by online scholarly
information systems. All these systems are relatively new, and all are still
changing. They all share a common goal: to provide their users with access to
the literature relevant to their specific needs. To achieve this each system
responds to actions by the user by displaying articles which the system judges
relevant to the user's current needs. Recently search systems which use
particularly sophisticated methodologies to recommend a few specific papers to
the user have been called ""recommender systems"". These methods are in line with
the current use of the term ""recommender system"" in computer science. We do not
adopt this definition, rather we view systems like these as components in a
larger whole, which is presented by the scholarly information systems
themselves. In what follows we view the recommender system as an aspect of the
entire information system; one which combines the massive memory capacities of
the machine with the cognitive abilities of the human user to achieve a
human-machine synergy.
"
609,Underspecified Scientific Claims in Nanopublications,"  The application range of nanopublications --- small entities of scientific
results in RDF representation --- could be greatly extended if complete formal
representations are not mandatory. To that aim, we present an approach to
represent and interlink scientific claims in an underspecified way, based on
independent English sentences.
"
610,"Evaluating the SiteStory Transactional Web Archive With the ApacheBench
  Tool","  Conventional Web archives are created by periodically crawling a web site and
archiving the responses from the Web server. Although easy to implement and
common deployed, this form of archiving typically misses updates and may not be
suitable for all preservation scenarios, for example a site that is required
(perhaps for records compliance) to keep a copy of all pages it has served. In
contrast, transactional archives work in conjunction with a Web server to
record all pages that have been served. Los Alamos National Laboratory has
developed SiteSory, an open-source transactional archive written in Java
solution that runs on Apache Web servers, provides a Memento compatible access
interface, and WARC file export features. We used the ApacheBench utility on a
pre-release version of to measure response time and content delivery time in
different environments and on different machines. The performance tests were
designed to determine the feasibility of SiteStory as a production-level
solution for high fidelity automatic Web archiving. We found that SiteStory
does not significantly affect content server performance when it is performing
transactional archiving. Content server performance slows from 0.076 seconds to
0.086 seconds per Web page access when the content server is under load, and
from 0.15 seconds to 0.21 seconds when the resource has many embedded and
changing resources.
"
611,Semantic web applications with regard to math and environment,"  The following is an outline of possible strategies in using semantic web
techniques and math with regard to environmental issues. The article uses
concrete examples and applications and provides partially a rather basic
treatment of semantic web techniques and math in order to adress a broader
audience.
"
612,A measure of total research impact independent of time and discipline,"  Authorship and citation practices evolve with time and differ by academic
discipline. As such, indicators of research productivity based on citation
records are naturally subject to historical and disciplinary effects. We
observe these effects on a corpus of astronomer career data constructed from a
database of refereed publications. We employ a simple mechanism to measure
research output using author and reference counts available in bibliographic
databases to develop a citation-based indicator of research productivity. The
total research impact (tori) quantifies, for an individual, the total amount of
scholarly work that others have devoted to his/her work, measured in the volume
of research papers. A derived measure, the research impact quotient (riq), is
an age independent measure of an individual's research ability. We demonstrate
that these measures are substantially less vulnerable to temporal debasement
and cross-disciplinary bias than the most popular current measures. The
proposed measures of research impact, tori and riq, have been implemented in
the Smithsonian/NASA Astrophysics Data System.
"
613,"A Plan For Curating ""Obsolete Data or Resources""","  Our cultural discourse is increasingly carried in the web. With the initial
emergence of the web many years ago, there was a period where conventional
mediums (e.g., music, movies, books, scholarly publications) were primary and
the web was a supplementary channel. This has now changed, where the web is
often the primary channel, and other publishing mechanisms, if present at all,
supplement the web. Unfortunately, the technology for publishing information on
the web always outstrips our technology for preservation. My concern is less
that we will lose data of known importance (e.g., scientific data, census
data), but rather that we will lose data that we do not yet know is important.
In this paper I review some of the issues and, where appropriate, proposed
solutions for increasing the archivability of the web.
"
614,True Peer Review,"  In computer science, conferences and journals conduct peer review in order to
decide what to publish. Many have pointed out the inherent weaknesses in peer
review, including those of bias, quality, and accountability. Many have
suggested and adopted refinements of peer review, for instance, double blind
peer review with author rebuttals.
  In this essay, I argue that peer review as currently practiced conflates the
sensible idea of getting comments on a paper with the irrevocably-flawed one
that we either accept or reject the paper, which I term gatekeeping. If we look
at the two separately, then it is clear that the ills associated with current
peer review systems are not due to the practice of getting comments, but due to
the practice of gatekeeping.
  True peer review constitutes my proposal for replacing existing peer review
systems. It embraces the idea of open debate on the merits of a paper; however,
it rejects unequivocally the exercise of gatekeeping. True peer review offers
all the benefits of current peer review systems but has none of its weaknesses.
True peer review will lead to a truly engaged community of researchers and
therefore better science.
"
615,"Losing My Revolution: How Many Resources Shared on Social Media Have
  Been Lost?","  Social media content has grown exponentially in the recent years and the role
of social media has evolved from just narrating life events to actually shaping
them. In this paper we explore how many resources shared in social media are
still available on the live web or in public web archives. By analyzing six
different event-centric datasets of resources shared in social media in the
period from June 2009 to March 2012, we found about 11% lost and 20% archived
after just a year and an average of 27% lost and 41% archived after two and a
half years. Furthermore, we found a nearly linear relationship between time of
sharing of the resource and the percentage lost, with a slightly less linear
relationship between time of sharing and archiving coverage of the resource.
From this model we conclude that after the first year of publishing, nearly 11%
of shared resources will be lost and after that we will continue to lose 0.02%
per day.
"
616,"Information Metrics (iMetrics): A Research Specialty with a
  Socio-Cognitive Identity?","  ""Bibliometrics"", ""scientometrics"", ""informetrics"", and ""webometrics"" can all
be considered as manifestations of a single research area with similar
objectives and methods, which we call ""information metrics"" or iMetrics. This
study explores the cognitive and social distinctness of iMetrics with respect
to the general information science (IS), focusing on a core of researchers,
shared vocabulary and literature/knowledge base. Our analysis investigates the
similarities and differences between four document sets. The document sets are
drawn from three core journals for iMetrics research (Scientometrics, Journal
of the American Society for Information Science and Technology, and Journal of
Informetrics). We split JASIST into document sets containing iMetrics and
general IS articles. The volume of publications in this representation of the
specialty has increased rapidly during the last decade. A core of researchers
that predominantly focus on iMetrics topics can thus be identified. This core
group has developed a shared vocabulary as exhibited in high similarity of
title words and one that shares a knowledge base. The research front of this
field moves faster than the research front of information science in general,
bringing it closer to Price's dream.
"
617,"Age-sensitive bibliographic coupling with an application in the history
  of science","  In science mapping, bibliographic coupling (BC) has been a standard tool for
discovering the cognitive structure of research areas, such as constituent
subareas, directions, schools of thought, or paradigms. Modelled as a set of
documents, research areas are often sorted into document clusters via BC
representing a thematic unit each. In this paper we propose an alternative
method called age-sensitive bibliographic coupling: the aim is to enable the
standard method to produce historically valid thematic units, that is, to yield
document clusters that represent the historical development of the thematic
structure of the subject as well. As such, the method is expected to be
especially beneficial for investigations on science dynamics and the history of
science. We apply the method within a bibliometric study in the modern history
of bioscience, addressing the development of a complex, interdisciplinary
discourse called the Species Problem.
"
618,Theorem Proving in Large Formal Mathematics as an Emerging AI Field,"  In the recent years, we have linked a large corpus of formal mathematics with
automated theorem proving (ATP) tools, and started to develop combined AI/ATP
systems working in this setting. In this paper we first relate this project to
the earlier large-scale automated developments done by Quaife with McCune's
Otter system, and to the discussions of the QED project about formalizing a
significant part of mathematics. Then we summarize our adventure so far, argue
that the QED dreams were right in anticipating the creation of a very
interesting semantic AI field, and discuss its further research directions.
"
619,Social Dynamics of Science,"  The birth and decline of disciplines are critical to science and society.
However, no quantitative model to date allows us to validate competing theories
of whether the emergence of scientific disciplines drives or follows the
formation of social communities of scholars. Here we propose an agent-based
model based on a \emph{social dynamics of science,} in which the evolution of
disciplines is guided mainly by the social interactions among scientists. We
find that such a social theory can account for a number of stylized facts about
the relationships between disciplines, authors, and publications. These results
provide strong quantitative support for the key role of social interactions in
shaping the dynamics of science. A ""science of science"" must gauge the role of
exogenous events, such as scientific discoveries and technological advances,
against this purely social baseline.
"
620,"In science ""there is no bad publicity"": Papers criticized in comments
  have high scientific impact","  Comments are special types of publications whose aim is to correct or
criticize previously published papers. For this reason, comments are believed
to make commented papers less worthy or trusty to the eyes of the scientific
community, and thus predestined to have low scientific impact. Here, we show
that such belief is not supported by empirical evidence. We consider thirteen
major publication outlets in science, and perform systematic comparisons
between the citations accumulated by commented and non commented articles. We
find that (i) commented papers are, on average, much more cited than non
commented papers, and (ii) commented papers are more likely to be among the
most cited papers of a journal. Since comments are published soon after
criticized papers, comments should be viewed as early indicators of the future
impact of criticized papers.
"
621,Diversifying Citation Recommendations,"  Literature search is arguably one of the most important phases of the
academic and non-academic research. The increase in the number of published
papers each year makes manual search inefficient and furthermore insufficient.
Hence, automatized methods such as search engines have been of interest in the
last thirty years. Unfortunately, these traditional engines use keyword-based
approaches to solve the search problem, but these approaches are prone to
ambiguity and synonymy. On the other hand, bibliographic search techniques
based only on the citation information are not prone to these problems since
they do not consider textual similarity. For many particular research areas and
topics, the amount of knowledge to humankind is immense, and obtaining the
desired information is as hard as looking for a needle in a haystack.
Furthermore, sometimes, what we are looking for is a set of documents where
each one is different than the others, but at the same time, as a whole we want
them to cover all the important parts of the literature relevant to our search.
This paper targets the problem of result diversification in citation-based
bibliographic search. It surveys a set of techniques which aim to find a set of
papers with satisfactory quality and diversity. We enhance these algorithms
with a direction-awareness functionality to allow the users to reach either
old, well-cited, well-known research papers or recent, less-known ones. We also
propose a set of novel techniques for a better diversification of the results.
All the techniques considered are compared by performing a rigorous
experimentation. The results show that some of the proposed techniques are very
successful in practice while performing a search in a bibliographic database.
"
622,TheSoz: A SKOS Representation of the Thesaurus for the Social Sciences,"  The Thesaurus for the Social Sciences (TheSoz) is a Linked Dataset in SKOS
format, which serves as a crucial instrument for information retrieval based on
e.g. document indexing or search term recommendation. Thesauri and similar
controlled vocabularies build a linking bridge for other datasets from the
Linked Open Data cloud - even between different domains. The information and
knowledge, which is exposed by such links, can be processed by Semantic Web
applications. In this article the conversion process of the TheSoz to SKOS is
described including the analysis of the original dataset and its structure, the
mapping to adequate SKOS classes and properties, and the technical conversion.
Furthermore mappings to other datasets and the appliance of the TheSoz are
presented. Finally, limitations and modeling issues encountered during the
creation process are discussed.
"
623,Publication patterns in HEP computing,"  An overview of the evolution of computing-oriented publications in high
energy physics following the start of operation of LHC. Quantitative analyses
are illustrated, which document the production of scholarly papers on
computing-related topics by high energy physics experiments and core tools
projects, and the citations they receive. Several scientometric indicators are
analyzed to characterize the role of computing in high energy physics
literature. Distinctive features of software-oriented and hardware-oriented
scholarly publications are highlighted. Current patterns and trends are
compared to the situation in previous generations' experiments.
"
624,"Citation analysis may severely underestimate the impact of clinical
  research as compared to basic research","  Background: Citation analysis has become an important tool for research
performance assessment in the medical sciences. However, different areas of
medical research may have considerably different citation practices, even
within the same medical field. Because of this, it is unclear to what extent
citation-based bibliometric indicators allow for valid comparisons between
research units active in different areas of medical research.
  Methodology: A visualization methodology is introduced that reveals
differences in citation practices between medical research areas. The
methodology extracts terms from the titles and abstracts of a large collection
of publications and uses these terms to visualize the structure of a medical
field and to indicate how research areas within this field differ from each
other in their average citation impact.
  Results: Visualizations are provided for 32 medical fields, defined based on
journal subject categories in the Web of Science database. The analysis focuses
on three fields. In each of these fields, there turn out to be large
differences in citation practices between research areas. Low-impact research
areas tend to focus on clinical intervention research, while high-impact
research areas are often more oriented on basic and diagnostic research.
  Conclusions: Popular bibliometric indicators, such as the h-index and the
impact factor, do not correct for differences in citation practices between
medical fields. These indicators therefore cannot be used to make accurate
between-field comparisons. More sophisticated bibliometric indicators do
correct for field differences but still fail to take into account within-field
heterogeneity in citation practices. As a consequence, the citation impact of
clinical intervention research may be substantially underestimated in
comparison with basic and diagnostic research.
"
625,Open Science Project in White Dwarf Research,"  I will propose a new way of advancing white dwarf research. Open science is a
method of doing research that lets everyone who has something to say about the
subject take part in the problem solving process.
  Already now, the amount of information we gather from observations, theory
and modelling is too vast for any one individual to comprehend and turn into
knowledge. And the amount of information just keeps growing in the future. A
platform that promotes sharing of thoughts and ideas allows us to pool our
collective knowledge of white dwarfs and get a clear picture of our research
field. It will also make it possible for researchers in fields closely related
to ours (AGB stars, planetary nebulae etc.) to join the scientific discourse.
  In the first stage this project would allow us to summarize what we know and
what we don't, and what we should search for next. Later, it could grow into a
large collaboration that would have the impact to, for example, suggest
instrument requirements for future telescopes to satisfy the needs of the white
dwarf community, or propose large surveys.
  A simple implementation would be a wiki page for collecting knowledge
combined with a forum for more extensive discussions. These would be simple and
cheap to maintain. A large community effort on the whole would be needed for
the project to succeed, but individual workload should stay at a low level.
"
626,Absolute and specific measures of research group excellence,"  A desirable goal of scientific management is to introduce, if it exists, a
simple and reliable way to measure the scientific excellence of publicly-funded
research institutions and universities to serve as a basis for their ranking
and financing. While citation-based indicators and metrics are easily
accessible, they are far from being universally accepted as way to automate or
inform evaluation processes or to replace evaluations based on peer review.
Here we consider absolute measurements of research excellence at an
amalgamated, institutional level and specific measures of research excellence
as performance per head. Using biology research institutions in the UK as a
test case, we examine the correlations between peer-review-based and
citation-based measures of research excellence on these two scales. We find
that citation-based indicators are very highly correlated with peer-evaluated
measures of group strength but are poorly correlated with group quality. Thus,
and almost paradoxically, our analysis indicates that citation counts could
possibly form a basis for deciding on how to fund research institutions but
they should not be used as a basis for ranking them in terms of quality.
"
627,"Stochastic dynamical model of a growing network based on self-exciting
  point process","  We perform experimental verification of the preferential attachment model
that is commonly accepted as a generating mechanism of the scale-free complex
networks. To this end we chose citation network of Physics papers and traced
citation history of 40,195 papers published in one year. Contrary to common
belief, we found that citation dynamics of the individual papers follows the
\emph{superlinear} preferential attachment, with the exponent $\alpha=
1.25-1.3$. Moreover, we showed that the citation process cannot be described as
a memoryless Markov chain since there is substantial correlation between the
present and recent citation rates of a paper. Basing on our findings we
constructed a stochastic growth model of the citation network, performed
numerical simulations based on this model and achieved an excellent agreement
with the measured citation distributions.
"
628,ADS Labs - Supporting Information Discovery in Science Education,"  The SAO/NASA Astrophysics Data System (ADS) is an open access digital library
portal for researchers in astronomy and physics, operated by the Smithsonian
Astrophysical Observatory (SAO) under a NASA grant, successfully serving the
professional science community for two decades. Currently there are about
55,000 frequent users (100+ queries per year), and up to 10 million infrequent
users per year. Access by the general public now accounts for about half of all
ADS use, demonstrating the vast reach of the content in our databases. The
visibility and use of content in the ADS can be measured by the fact that there
are over 17,000 links from Wikipedia pages to ADS content, a figure comparable
to the number of links that Wikipedia has to OCLCs WorldCat catalog. The ADS,
through its holdings and innovative techniques available in ADS Labs
(http://adslabs.org), offers an environment for information discovery that is
unlike any other service currently available to the astrophysics community.
Literature discovery and review are important components of science education,
aiding the process of preparing for a class, project, or presentation. The ADS
has been recognized as a rich source of information for the science education
community in astronomy, thanks to its collaborations within the astronomy
community, publishers and projects like Com- PADRE. One element that makes the
ADS uniquely relevant for the science education community is the availability
of powerful tools to explore aspects of the astronomy literature as well as the
relationship between topics, people, observations and scientific papers. The
other element is the extensive repository of scanned literature, a significant
fraction of which consists of historical literature.
"
629,Logical segmentation for article extraction in digitized old newspapers,"  Newspapers are documents made of news item and informative articles. They are
not meant to be red iteratively: the reader can pick his items in any order he
fancies. Ignoring this structural property, most digitized newspaper archives
only offer access by issue or at best by page to their content. We have built a
digitization workflow that automatically extracts newspaper articles from
images, which allows indexing and retrieval of information at the article
level. Our back-end system extracts the logical structure of the page to
produce the informative units: the articles. Each image is labelled at the
pixel level, through a machine learning based method, then the page logical
structure is constructed up from there by the detection of structuring entities
such as horizontal and vertical separators, titles and text lines. This logical
structure is stored in a METS wrapper associated to the ALTO file produced by
the system including the OCRed text. Our front-end system provides a web high
definition visualisation of images, textual indexing and retrieval facilities,
searching and reading at the article level. Articles transcriptions can be
collaboratively corrected, which as a consequence allows for better indexing.
We are currently testing our system on the archives of the Journal de Rouen,
one of France eldest local newspaper. These 250 years of publication amount to
300 000 pages of very variable image quality and layout complexity. Test year
1808 can be consulted at plair.univ-rouen.fr.
"
630,"Theoretical And Technological Building Blocks For An Innovation
  Accelerator","  The scientific system that we use today was devised centuries ago and is
inadequate for our current ICT-based society: the peer review system encourages
conservatism, journal publications are monolithic and slow, data is often not
available to other scientists, and the independent validation of results is
limited. Building on the Innovation Accelerator paper by Helbing and Balietti
(2011) this paper takes the initial global vision and reviews the theoretical
and technological building blocks that can be used for implementing an
innovation (in first place: science) accelerator platform driven by
re-imagining the science system. The envisioned platform would rest on four
pillars: (i) Redesign the incentive scheme to reduce behavior such as
conservatism, herding and hyping; (ii) Advance scientific publications by
breaking up the monolithic paper unit and introducing other building blocks
such as data, tools, experiment workflows, resources; (iii) Use machine
readable semantics for publications, debate structures, provenance etc. in
order to include the computer as a partner in the scientific process, and (iv)
Build an online platform for collaboration, including a network of trust and
reputation among the different types of stakeholders in the scientific system:
scientists, educators, funding agencies, policy makers, students and industrial
innovators among others. Any such improvements to the scientific system must
support the entire scientific process (unlike current tools that chop up the
scientific process into disconnected pieces), must facilitate and encourage
collaboration and interdisciplinarity (again unlike current tools), must
facilitate the inclusion of intelligent computing in the scientific process,
must facilitate not only the core scientific process, but also accommodate
other stakeholders such science policy makers, industrial innovators, and the
general public.
"
631,Formats over Time: Exploring UK Web History,"  Is software obsolescence a significant risk? To explore this issue, we
analysed a corpus of over 2.5 billion resources corresponding to the UK Web
domain, as crawled between 1996 and 2010. Using the DROID and Apache Tika
identification tools, we examined each resource and captured the results as
extended MIME types, embedding version, software and hardware identifiers
alongside the format information. The combined results form a detailed temporal
format profile of the corpus, which we have made available as open data. We
present the results of our initial analysis of this dataset. We look at image,
HTML and PDF resources in some detail, showing how the usage of different
formats, versions and software implementations has changed over time.
Furthermore, we show that software obsolescence is rare on the web and uncover
evidence indicating that network effects act to stabilise formats against
obsolescence.
"
632,An empirical study to order citation statistics between subject fields,"  An empirical study is conducted to compare citations per publication,
statistics and observed Hirsch indexes between subject fields using summary
statistics of countries. No distributional assumptions are made and ratios are
calculated. These ratios can be used to make approximate comparisons between
researchers of different subject fields with respect to the Hirsch index.
"
633,"How are academic age, productivity and collaboration related to citing
  behavior of researchers?","  References are an essential component of research articles and therefore of
scientific communication. In this study we investigate referencing (citing)
behavior in five diverse fields (astronomy, mathematics, robotics, ecology and
economics) based on 213,756 core journal articles. At the macro level we find:
(a) a steady increase in the number of references per article over the period
studied (50 years), which in some fields is due to a higher rate of usage,
while in others reflects longer articles and (b) an increase in all fields in
the fraction of older, foundational references since the 1980s, with no obvious
change in citing patterns associated with the introduction of the Internet. At
the meso level we explore current (2006-2010) referencing behavior of different
categories of authors (21,562 total) within each field, based on their academic
age, productivity and collaborative practices. Contrary to some previous
findings and expectations we find that senior researchers use references at the
same rate as their junior colleagues, with similar rates of re-citation (use of
same references in multiple papers). High Modified Price Index (MPI, which
measures the speed of the research front more accurately than the traditional
Price Index) of senior authors indicates that their research has the similar
cutting-edge aspect as that of their younger colleagues. In all fields both the
productive researchers and especially those who collaborate more use a
significantly lower fraction of foundational references and have much higher
MPI and lower re-citation rates, i.e., they are the ones pushing the research
front regardless of researcher age. This paper introduces improved bibliometric
methods to measure the speed of the research front, disambiguate lead authors
in co-authored papers and decouple measures of productivity and collaboration.
"
634,"Use of Repositories and its Significance for Engineering Education / El
  Uso de Repositorios y su Importancia para la Educaci\'on en Ingenier\'ia","  Institutional repositories are deposits of different types of digital files
for access, disseminate and preserve them. This paper aims to explain the
importance of repositories in the academic field of engineering as a way to
democratize knowledge by teachers, researchers and students to contribute to
social and human development. These repositories, usually framed in the Open
Access Initiative, allow to ensure access free and open (unrestricted legal and
economic) to different sectors of society and, thus, can make use of the
services they offer. Finally, that repositories are evolving in the academic
and scientific, and different disciplines of engineering should be prepared to
provide a range of services through these systems to society of today and
tomorrow.
"
635,Hidden Trends in 90 Years of Harvard Business Review,"  In this paper, we demonstrate and discuss results of our mining the abstracts
of the publications in Harvard Business Review between 1922 and 2012.
Techniques for computing n-grams, collocations, basic sentiment analysis, and
named-entity recognition were employed to uncover trends hidden in the
abstracts. We present findings about international relationships, sentiment in
HBR's abstracts, important international companies, influential technological
inventions, renown researchers in management theories, US presidents via
chronological analyses.
"
636,"Some Chances and Challenges in Applying Language Technologies to
  Historical Studies in Chinese","  We report applications of language technology to analyzing historical
documents in the Database for the Study of Modern Chinese Thoughts and
Literature (DSMCTL). We studied two historical issues with the reported
techniques: the conceptualization of ""huaren"" (Chinese people) and the attempt
to institute constitutional monarchy in the late Qing dynasty. We also discuss
research challenges for supporting sophisticated issues using our experience
with DSMCTL, the Database of Government Officials of the Republic of China, and
the Dream of the Red Chamber. Advanced techniques and tools for lexical,
syntactic, semantic, and pragmatic processing of language information, along
with more thorough data collection, are needed to strengthen the collaboration
between historians and computer scientists.
"
637,"Interactive Overlay Maps for US Patent (USPTO) Data Based on
  International Patent Classifications (IPC)","  We report on the development of an interface to the US Patent and Trademark
Office (USPTO) that allows for the mapping of patent portfolios as overlays to
basemaps constructed from citation relations among all patents contained in
this database during the period 1976-2011. Both the interface and the data are
in the public domain; the freeware programs VOSViewer and/or Pajek can be used
for the visualization. These basemaps and overlays can be generated at both the
3-digit and 4-digit levels of the International Patent Classifications (IPC) of
the World Intellectual Property Organization (WIPO). The basemaps can provide a
stable mental framework for analysts to follow developments over searches for
different years, which can be animated. The full flexibility of the advanced
search engines of USPTO are available for generating sets of patents and/or
patent applications which can thus be visualized and compared. This instrument
allows for addressing questions about technological distance, diversity in
portfolios, and animating the developments of both technologies and
technological capacities of organizations over time.
"
638,"A measure of similarity between scientific journals and of diversity of
  a list of publications","  The aim of this note is to propose a definition of the scientific diversity
and corollarly, a measure of the ""interdisciplinarity"" of collaborations. With
respect to previous studies, the proposed approach consists of 2 steps : first,
the definition of similarity between journals and second, these similarities
are used to characterize the homogeneity (or, on the contrary the diversity) of
a publication list (that can be for one individual or a team).
"
639,"Astronomy and Computing: a New Journal for the Astronomical Computing
  Community","  We introduce \emph{Astronomy and Computing}, a new journal for the growing
population of people working in the domain where astronomy overlaps with
computer science and information technology. The journal aims to provide a new
communication channel within that community, which is not well served by
current journals, and to help secure recognition of its true importance within
modern astronomy. In this inaugural editorial, we describe the rationale for
creating the journal, outline its scope and ambitions, and seek input from the
community in defining in detail how the journal should work towards its
high-level goals.
"
640,Testing the Finch Hypothesis on Green OA Mandate Ineffectiveness,"  We have now tested the Finch Committee's Hypothesis that Green Open Access
Mandates are ineffective in generating deposits in institutional repositories.
With data from ROARMAP on institutional Green OA mandates and data from ROAR on
institutional repositories, we show that deposit number and rate is
significantly correlated with mandate strength (classified as 1-12): The
stronger the mandate, the more the deposits. The strongest mandates generate
deposit rates of 70%+ within 2 years of adoption, compared to the un-mandated
deposit rate of 20%. The effect is already detectable at the national level,
where the UK, which has the largest proportion of Green OA mandates, has a
national OA rate of 35%, compared to the global baseline of 25%. The conclusion
is that, contrary to the Finch Hypothesis, Green Open Access Mandates do have a
major effect, and the stronger the mandate, the stronger the effect (the Liege
ID/OA mandate, linked to research performance evaluation, being the strongest
mandate model). RCUK (as well as all universities, research institutions and
research funders worldwide) would be well advised to adopt the strongest Green
OA mandates and to integrate institutional and funder mandates.
"
641,FuturICT - The Road towards Ethical ICT,"  The pervasive use of information and communication technology (ICT) in modern
societies enables countless opportunities for individuals, institutions,
businesses and scientists, but also raises difficult ethical and social
problems. In particular, ICT helped to make societies more complex and thus
harder to understand, which impedes social and political interventions to avoid
harm and to increase the common good. To overcome this obstacle, the
large-scale EU flagship proposal FuturICT intends to create a platform for
accessing global human knowledge as a public good and instruments to increase
our understanding of the information society by making use of ICT-based
research. In this contribution, we outline the ethical justification for such
an endeavor. We argue that the ethical issues raised by FuturICT research
projects overlap substantially with many of the known ethical problems emerging
from ICT use in general. By referring to the notion of Value Sensitive Design,
we show for the example of privacy how this core value of responsible ICT can
be protected in pursuing research in the framework of FuturICT. In addition, we
discuss further ethical issues and outline the institutional design of FuturICT
allowing to address them.
"
642,"The use of percentiles and percentile rank classes in the analysis of
  bibliometric data: Opportunities and limits","  Percentiles have been established in bibliometrics as an important
alternative to mean-based indicators for obtaining a normalized citation impact
of publications. Percentiles have a number of advantages over standard
bibliometric indicators used frequently: for example, their calculation is not
based on the arithmetic mean which should not be used for skewed bibliometric
data. This study describes the opportunities and limits and the advantages and
disadvantages of using percentiles in bibliometrics. We also address problems
in the calculation of percentiles and percentile rank classes for which there
is not (yet) a satisfactory solution. It will be hard to compare the results of
different percentile-based studies with each other unless it is clear that the
studies were done with the same choices for percentile calculation and rank
assignment.
"
643,"Enhancing Invenio Digital Library With An External Relevance Ranking
  Engine","  Invenio is a comprehensive web-based free digital library software suite
originally developed at CERN. In order to improve its information retrieval and
word similarity ranking capabilities, the goal of this thesis is to enhance
Invenio by bridging it with modern external information retrieval systems. In
the first part a comparison of various information retrieval systems such as
Solr and Xapian is made. In the second part a system-independent bridge for
word similarity ranking is designed and implemented. Subsequently, Solr and
Xapian are integrated in Invenio via adapters to the bridge. In the third part
scalability tests are performed. Finally, a future outlook is briefly
discussed.
"
644,"A Scienceographic Comparison of Physics Papers from the arXiv and viXra
  Archives","  arXiv is an e-print repository of papers in physics, computer science, and
biology, amongst others. viXra is a newer repository of e-prints on similar
topics. Scienceography is the study of the writing of science. In this work we
perform a scienceographic comparison of a selection of papers from the physics
section of each archive. We provide the first study of the viXra archive and
describe key differences on how science is written by these communities.
"
645,Trends in condensed matter physics: is research going faster and faster?,"  In this paper we study research trends in condensed matter physics. Trends
are analyzed by means of the the number of publications in the different
sub-fields as function of the years. We found that many research topics have a
similar behavior with an initial fast growth and a next slower exponential
decay. We derived a simple model to describe this behavior and built up some
predictions for future trends.
"
646,"The validation of (advanced) bibliometric indicators through peer
  assessments: A comparative study using data from InCites and F1000","  The data of F1000 provide us with the unique opportunity to investigate the
relationship between peers' ratings and bibliometric metrics on a broad and
comprehensive data set with high-quality ratings. F1000 is a post-publication
peer review system of the biomedical literature. The comparison of metrics with
peer evaluation has been widely acknowledged as a way of validating metrics.
Based on the seven indicators offered by InCites, we analyzed the validity of
raw citation counts (Times Cited, 2nd Generation Citations, and 2nd Generation
Citations per Citing Document), normalized indicators (Journal Actual/Expected
Citations, Category Actual/Expected Citations, and Percentile in Subject Area),
and a journal based indicator (Journal Impact Factor). The data set consists of
125 papers published in 2008 and belonging to the subject category cell biology
or immunology. As the results show, Percentile in Subject Area achieves the
highest correlation with F1000 ratings; we can assert that for further three
other indicators (Times Cited, 2nd Generation Citations, and Category
Actual/Expected Citations) the 'true' correlation with the ratings reaches at
least a medium effect size.
"
647,The role of gender in scholarly authorship,"  Gender disparities appear to be decreasing in academia according to a number
of metrics, such as grant funding, hiring, acceptance at scholarly journals,
and productivity, and it might be tempting to think that gender inequity will
soon be a problem of the past. However, a large-scale analysis based on over
eight million papers across the natural sciences, social sciences, and
humanities re- reveals a number of understated and persistent ways in which
gender inequities remain. For instance, even where raw publication counts seem
to be equal between genders, close inspection reveals that, in certain fields,
men predominate in the prestigious first and last author positions. Moreover,
women are significantly underrepresented as authors of single-authored papers.
Academics should be aware of the subtle ways that gender disparities can appear
in scholarly authorship.
"
648,"Annotations, Collaborative Tagging, and Searching Mathematics in
  E-Learning","  This paper presents a new framework for adding semantics into e-learning
system. The proposed approach relies on two principles. The first principle is
the automatic addition of semantic information when creating the mathematical
contents. The second principle is the collaborative tagging and annotation of
the e-learning contents and the use of an ontology to categorize the e-learning
contents. The proposed system encodes the mathematical contents using
presentation MathML with RDFa annotations. The system allows students to
highlight and annotate specific parts of the e-learning contents. The objective
is to add meaning into the e-learning contents, to add relationships between
contents, and to create a framework to facilitate searching the contents. This
semantic information can be used to answer semantic queries (e.g., SPARQL) to
retrieve information request of a user. This work is implemented as an embedded
code into Moodle e-learning system.
"
649,"Field-normalized Impact Factors: A Comparison of Rescaling versus
  Fractionally Counted IFs","  Two methods for comparing impact factors and citation rates across fields of
science are tested against each other using citations to the 3,705 journals in
the Science Citation Index 2010 (CD-Rom version of SCI) and the 13 field
categories used for the Science and Engineering Indicators of the US National
Science Board. We compare (i) normalization by counting citations in proportion
to the length of the reference list (1/N of references) with (ii) rescaling by
dividing citation scores by the arithmetic mean of the citation rate of the
cluster. Rescaling is analytical and therefore independent of the quality of
the attribution to the sets, whereas fractional counting provides an empirical
strategy for normalization among sets (by evaluating the between-group
variance). By the fairness test of Radicchi & Castellano (2012a), rescaling
outperforms fractional counting of citations for reasons that we consider.
"
650,"A semantic cache for enhancing Web services communities activities:
  Health care case study","  Collective memories are strong support for enhancing the activities of
capitalization, management and dissemination inside a Web services community.
"
651,"The Benefits of Model-Driven Development in Institutional Repositories -
  Los Beneficios del Desarrollo Dirigido por Modelos en los Repositorios
  Institucionales","  The Institutional Repositories (IR) have been consolidated into the
institutions in scientific and academic areas, as shown by the directories
existing open access repositories and the deposits daily of articles made by
different ways, such as by self-archiving of registered users and the
cataloging by librarians. IR systems are based on various conceptual models, so
in this paper a bibliographic survey Model-Driven Development (MDD) in systems
and applications for RI in order to expose the benefits of applying MDD in IR.
The MDD is a paradigm for building software that assigns a central role models
and active under which derive models ranging from the most abstract to the
concrete, this is done through successive transformations. This paradigm
provides a framework that allows interested parties to share their views and
directly manipulate representations of the entities of this domain. Therefore,
the benefits are grouped by actors that are present, namely, developers,
business owners and domain experts. In conclusion, these benefits help make
more formal software implementations, resulting in a consolidation of such
systems, where the main beneficiaries are the end users through the services
are offered
"
652,Ontology Based Information Extraction for Disease Intelligence,"  Disease Intelligence (DI) is based on the acquisition and aggregation of
fragmented knowledge of diseases at multiple sources all over the world to
provide valuable information to doctors, researchers and information seeking
community. Some diseases have their own characteristics changed rapidly at
different places of the world and are reported on documents as unrelated and
heterogeneous information which may be going unnoticed and may not be quickly
available. This research presents an Ontology based theoretical framework in
the context of medical intelligence and country/region. Ontology is designed
for storing information about rapidly spreading and changing diseases with
incorporating existing disease taxonomies to genetic information of both humans
and infectious organisms. It further maps disease symptoms to diseases and drug
effects to disease symptoms. The machine understandable disease ontology
represented as a website thus allows the drug effects to be evaluated on
disease symptoms and exposes genetic involvements in the human diseases.
Infectious agents which have no known place in an existing classification but
have data on genetics would still be identified as organisms through the
intelligence of this system. It will further facilitate researchers on the
subject to try out different solutions for curing diseases.
"
653,Universities Scale Like Cities,"  Recent studies of urban scaling show that important socioeconomic city
characteristics such as wealth and innovation capacity exhibit a nonlinear,
particularly a power law scaling with population size. These nonlinear effects
are common to all cities, with similar power law exponents. These findings mean
that the larger the city, the more disproportionally they are places of wealth
and innovation. Local properties of cities cause a deviation from the expected
behavior as predicted by the power law scaling. In this paper we demonstrate
that universities show a similar behavior as cities in the distribution of the
gross university income in terms of total number of citations over size in
terms of total number of publications. Moreover, the power law exponents for
university scaling are comparable to those for urban scaling. We find that
deviations from the expected behavior can indeed be explained by specific local
properties of universities, particularly the field-specific composition of a
university, and its quality in terms of field-normalized citation impact. By
studying both the set of the 500 largest universities worldwide and a specific
subset of these 500 universities -- the top-100 European universities -- we are
also able to distinguish between properties of universities with as well as
without selection of one specific local property, the quality of a university
in terms of its average field-normalized citation impact. It also reveals an
interesting observation concerning the working of a crucial property in
networked systems, preferential attachment.
"
654,"A bird's-eye view of scientific trading: Dependency relations among
  fields of science","  We use a trading metaphor to study knowledge transfer in the sciences as well
as the social sciences. The metaphor comprises four dimensions: (a) Discipline
Self-dependence, (b) Knowledge Exports/Imports, (c) Scientific Trading
Dynamics, and (d) Scientific Trading Impact. This framework is applied to a
dataset of 221 Web of Science subject categories. We find that: (i) the
Scientific Trading Impact and Dynamics of Materials Science And Transportation
Science have increased; (ii) Biomedical Disciplines, Physics, And Mathematics
are significant knowledge exporters, as is Statistics & Probability; (iii) in
the social sciences, Economics, Business, Psychology, Management, And Sociology
are important knowledge exporters; (iv) Discipline Self-dependence is
associated with specialized domains which have ties to professional practice
(e.g., Law, Ophthalmology, Dentistry, Oral Surgery & Medicine, Psychology,
Psychoanalysis, Veterinary Sciences, And Nursing).
"
655,"Citation content analysis (cca): A framework for syntactic and semantic
  analysis of citation content","  This paper proposes a new framework for Citation Content Analysis (CCA), for
syntactic and semantic analysis of citation content that can be used to better
analyze the rich sociocultural context of research behavior. The framework
could be considered the next generation of citation analysis. This paper
briefly reviews the history and features of content analysis in traditional
social sciences, and its previous application in Library and Information
Science. Based on critical discussion of the theoretical necessity of a new
method as well as the limits of citation analysis, the nature and purposes of
CCA are discussed, and potential procedures to conduct CCA, including
principles to identify the reference scope, a two-dimensional (citing and
cited) and two-modular (syntactic and semantic modules) codebook, are provided
and described. Future works and implications are also suggested.
"
656,Learning-Assisted Automated Reasoning with Flyspeck,"  The considerable mathematical knowledge encoded by the Flyspeck project is
combined with external automated theorem provers (ATPs) and machine-learning
premise selection methods trained on the proofs, producing an AI system capable
of answering a wide range of mathematical queries automatically. The
performance of this architecture is evaluated in a bootstrapping scenario
emulating the development of Flyspeck from axioms to the last theorem, each
time using only the previous theorems and proofs. It is shown that 39% of the
14185 theorems could be proved in a push-button mode (without any high-level
advice and user interaction) in 30 seconds of real time on a fourteen-CPU
workstation. The necessary work involves: (i) an implementation of sound
translations of the HOL Light logic to ATP formalisms: untyped first-order,
polymorphic typed first-order, and typed higher-order, (ii) export of the
dependency information from HOL Light and ATP proofs for the machine learners,
and (iii) choice of suitable representations and methods for learning from
previous proofs, and their integration as advisors with HOL Light. This work is
described and discussed here, and an initial analysis of the body of proofs
that were found fully automatically is provided.
"
657,Socializing the h-index,"  A variety of bibliometric measures have been proposed to quantify the impact
of researchers and their work. The h-index is a notable and widely-used example
which aims to improve over simple metrics such as raw counts of papers or
citations. However, a limitation of this measure is that it considers authors
in isolation and does not account for contributions through a collaborative
team. To address this, we propose a natural variant that we dub the Social
h-index. The idea is to redistribute the h-index score to reflect an
individual's impact on the research community. In addition to describing this
new measure, we provide examples, discuss its properties, and contrast with
other measures.
"
658,"Ranking and mapping of universities and research-focused institutions
  worldwide based on highly-cited papers: A visualization of results from
  multi-level models","  The web application presented in this paper allows for an analysis to reveal
centres of excellence in different fields worldwide using publication and
citation data. Only specific aspects of institutional performance are taken
into account and other aspects such as teaching performance or societal impact
of research are not considered. Based on data gathered from Scopus,
field-specific excellence can be identified in institutions where highly-cited
papers have been frequently published. The web application combines both a list
of institutions ordered by different indicator values and a map with circles
visualizing indicator values for geocoded institutions. Compared to the mapping
and ranking approaches introduced hitherto, our underlying statistics
(multi-level models) are analytically oriented by allowing (1) the estimation
of values for the number of excellent papers for an institution which are
statistically more appropriate than the observed values; (2) the calculation of
confidence intervals as measures of accuracy for the institutional citation
impact; (3) the comparison of a single institution with an ""average""
institution in a subject area, and (4) the direct comparison of at least two
institutions.
"
659,"Manipulating Google Scholar Citations and Google Scholar Metrics:
  simple, easy and tempting","  The launch of Google Scholar Citations and Google Scholar Metrics may provoke
a revolution in the research evaluation field as it places within every
researchers reach tools that allow bibliometric measuring. In order to alert
the research community over how easily one can manipulate the data and
bibliometric indicators offered by Google s products we present an experiment
in which we manipulate the Google Citations profiles of a research group
through the creation of false documents that cite their documents, and
consequently, the journals in which they have published modifying their H
index. For this purpose we created six documents authored by a faked author and
we uploaded them to a researcher s personal website under the University of
Granadas domain. The result of the experiment meant an increase of 774
citations in 129 papers (six citations per paper) increasing the authors and
journals H index. We analyse the malicious effect this type of practices can
cause to Google Scholar Citations and Google Scholar Metrics. Finally, we
conclude with several deliberations over the effects these malpractices may
have and the lack of control tools these tools offer
"
660,"Interdisciplinarity at the Journal and Specialty Level: The changing
  knowledge bases of the journal Cognitive Science","  Using the referencing patterns in articles in Cognitive Science over three
decades, we analyze the knowledge base of this literature in terms of its
changing disciplinary composition. Three periods are distinguished: (1)
construction of the interdisciplinary space in the 1980s; (2) development of an
interdisciplinary orientation in the 1990s; (3) reintegration into ""cognitive
psychology"" in the 2000s. The fluidity and fuzziness of the interdisciplinary
delineations in the different visualizations can be reduced and clarified using
factor analysis. We also explore newly available routines (""CorText"") to
analyze this development in terms of ""tubes"" using an alluvial map, and compare
the results with an animation (using ""visone""). The historical specificity of
this development can be compared with the development of ""artificial
intelligence"" into an integrated specialty during this same period.
""Interdisciplinarity"" should be defined differently at the level of journals
and of specialties.
"
661,"The Clustering of Author's Texts of English Fiction in the Vector Space
  of Semantic Fields","  The clustering of text documents in the vector space of semantic fields and
in the semantic space with orthogonal basis has been analysed. It is shown that
using the vector space model with the basis of semantic fields is effective in
the cluster analysis algorithms of author's texts in English fiction. The
analysis of the author's texts distribution in cluster structure showed the
presence of the areas of semantic space that represent the author's ideolects
of individual authors. SVD factorization of the semantic fields matrix makes it
possible to reduce significantly the dimension of the semantic space in the
cluster analysis of author's texts.
"
662,"Evolution of the most common English words and phrases over the
  centuries","  By determining which were the most common English words and phrases since the
beginning of the 16th century, we obtain a unique large-scale view of the
evolution of written text. We find that the most common words and phrases in
any given year had a much shorter popularity lifespan in the 16th than they had
in the 20th century. By measuring how their usage propagated across the years,
we show that for the past two centuries the process has been governed by linear
preferential attachment. Along with the steady growth of the English lexicon,
this provides an empirical explanation for the ubiquity of the Zipf's law in
language statistics and confirms that writing, although undoubtedly an
expression of art and skill, is not immune to the same influences of
self-organization that are known to regulate processes as diverse as the making
of new friends and World Wide Web growth.
"
663,"Bring out your codes! Bring out your codes! (Increasing Software
  Visibility and Re-use)","  Progress is being made in code discoverability and preservation, but as
discussed at ADASS XXI, many codes still remain hidden from public view. With
the Astrophysics Source Code Library (ASCL) now indexed by the SAO/NASA
Astrophysics Data System (ADS), the introduction of a new journal, Astronomy &
Computing, focused on astrophysics software, and the increasing success of
education efforts such as Software Carpentry and SciCoder, the community has
the opportunity to set a higher standard for its science by encouraging the
release of software for examination and possible reuse. We assembled
representatives of the community to present issues inhibiting code release and
sought suggestions for tackling these factors.
  The session began with brief statements by panelists; the floor was then
opened for discussion and ideas. Comments covered a diverse range of related
topics and points of view, with apparent support for the propositions that
algorithms should be readily available, code used to produce published
scientific results should be made available, and there should be discovery
mechanisms to allow these to be found easily. With increased use of resources
such as GitHub (for code availability), ASCL (for code discovery), and a stated
strong preference from the new journal Astronomy & Computing for code release,
we expect to see additional progress over the next few years.
"
664,Astrophysics Source Code Library,"  The Astrophysics Source Code Library (ASCL), founded in 1999, is a free
on-line registry for source codes of interest to astronomers and
astrophysicists. The library is housed on the discussion forum for Astronomy
Picture of the Day (APOD) and can be accessed at http://ascl.net. The ASCL has
a comprehensive listing that covers a significant number of the astrophysics
source codes used to generate results published in or submitted to refereed
journals and continues to grow. The ASCL currently has entries for over 500
codes; its records are citable and are indexed by ADS. The editors of the ASCL
and members of its Advisory Committee were on hand at a demonstration table in
the ADASS poster room to present the ASCL, accept code submissions, show how
the ASCL is starting to be used by the astrophysics community, and take
questions on and suggestions for improving the resource.
"
665,"The Possible Role of Resource Requirements and Academic Career-Choice
  Risk on Gender Differences in Publication Rate and Impact","  Many studies demonstrate that there is still a significant gender bias,
especially at higher career levels, in many areas including science,
technology, engineering, and mathematics (STEM). We investigated
field-dependent, gender-specific effects of the selective pressures individuals
experience as they pursue a career in academia within seven STEM disciplines.
We built a unique database that comprises 437,787 publications authored by
4,292 faculty members at top United States research universities. Our analyses
reveal that gender differences in publication rate and impact are
discipline-specific. Our results also support two hypotheses. First, the
widely-reported lower publication rates of female faculty are correlated with
the amount of research resources typically needed in the discipline considered,
and thus may be explained by the lower level of institutional support
historically received by females. Second, in disciplines where pursuing an
academic position incurs greater career risk, female faculty tend to have a
greater fraction of higher impact publications than males. Our findings have
significant, field-specific, policy implications for achieving diversity at the
faculty level within the STEM disciplines.
"
666,A Dynamic Network Approach to Breakthrough Innovation,"  This paper outlines a framework for the study of innovation that treats
discoveries as additions to evolving networks. As inventions enter they expand
or limit the reach of the ideas they build on by influencing how successive
discoveries use those ideas. The approach is grounded in novel measures of the
extent to which an innovation amplifies or disrupts the status quo. Those
measures index the effects inventions have on subsequent uses of prior
discoveries. In so doing, they characterize a theoretically important but
elusive feature of innovation. We validate our approach by showing it: (1)
discriminates among innovations of similar impact in analyses of U.S. patents;
(2) identifies discoveries that amplify and disrupt technology streams in
select case studies; (3) implies disruptive patents decrease the use of their
predecessors by 60% in difference-in-differences estimation; and, (4) yields
novel findings in analyses of patenting at 110 U.S. universities.
"
667,Discovering Links for Metadata Enrichment on Computer Science Papers,"  At the very beginning of compiling a bibliography, usually only basic
information, such as title, authors and publication date of an item are known.
In order to gather additional information about a specific item, one typically
has to search the library catalog or use a web search engine. This look-up
procedure implies a manual effort for every single item of a bibliography. In
this technical report we present a proof of concept which utilizes Linked Data
technology for the simple enrichment of sparse metadata sets. This is done by
discovering owl:sameAs links be- tween an initial set of computer science
papers and resources from external data sources like DBLP, ACM and the Semantic
Web Conference Corpus. In this report, we demonstrate how the link discovery
tool Silk is used to detect additional information and to enrich an initial set
of records in the computer science domain. The pros and cons of silk as link
discovery tool are summarized in the end.
"
668,Truthy: Enabling the Study of Online Social Networks,"  The broad adoption of online social networking platforms has made it possible
to study communication networks at an unprecedented scale. Digital trace data
can be compiled into large data sets of online discourse. However, it is a
challenge to collect, store, filter, and analyze large amounts of data, even by
experts in the computational sciences. Here we describe our recent extensions
to Truthy, a system that collects Twitter data to analyze discourse in near
real-time. We introduce several interactive visualizations and analytical tools
with the goal of enabling citizens, journalists, and researchers to understand
and study online social networks at multiple scales.
"
669,"easyGWAS: An integrated interspecies platform for performing genome-wide
  association studies","  Motivation: The rapid growth in genome-wide association studies (GWAS) in
plants and animals has brought about the need for a central resource that
facilitates i) performing GWAS, ii) accessing data and results of other GWAS,
and iii) enabling all users regardless of their background to exploit the
latest statistical techniques without having to manage complex software and
computing resources.
  Results: We present easyGWAS, a web platform that provides methods, tools and
dynamic visualizations to perform and analyze GWAS. In addition, easyGWAS makes
it simple to reproduce results of others, validate findings, and access larger
sample sizes through merging of public datasets.
  Availability: Detailed method and data descriptions as well as tutorials are
available in the supplementary materials. easyGWAS is available at
http://easygwas.tuebingen.mpg.de/.
  Contact: dominik.grimm@tuebingen.mpg.de
"
670,"A Study on the Open Source Digital Library Software's: Special Reference
  to DSpace, EPrints and Greenstone","  The richness in knowledge has changed access methods for all stake holders in
retrieving key knowledge and relevant information. This paper presents a study
of three open source digital library management software used to assimilate and
disseminate information to world audience. The methodology followed involves
online survey and study of related software documentation and associated
technical manuals.
"
671,"International Scientific Migration and Collaboration Patterns Following
  a Bibliometrics Line of Investigation","  A bibliometric approach is explored to tracking international scientific
migration, based on an analysis of the affiliation countries of authors
publishing in peer reviewed journals indexed in Scopus. The paper introduces a
model that relates base concepts in the study of migration to bibliometric
constructs, and discusses the potentialities and limitations of a bibliometric
approach both with respect to data accuracy and interpretation. Synchronous and
asynchronous analyses are presented for 10 rapidly growing countries and 7
scientifically established countries. Rough error rates of the proposed
indicators are estimated. It is concluded that the bibliometric approach is
promising provided that its outcomes are interpreted with care, based on
insight into the limits and potentialities of the bibliometric approach, and
combined with complementary data, obtained, for instance, from researchers
Curricula Vitae or survey or questionnaire based data. Error rates for units of
assessment with indicator values based on sufficiently large numbers are
estimated to be fairly below 10 per cent, but can be expected to vary
substantially among countries of origin, especially between Asian countries and
Western countries.
"
672,Bibliometric Networks,"  This text is based on a translation of a chapter in a handbook about network
analysis (published in German) where we tried to make beginners familiar with
some basic notions and recent developments of network analysis applied to
bibliometric issues (Havemann and Scharnhorst 2010). We have added some recent
references.
"
673,Topic Extraction and Bundling of Related Scientific Articles,"  Automatic classification of scientific articles based on common
characteristics is an interesting problem with many applications in digital
library and information retrieval systems. Properly organized articles can be
useful for automatic generation of taxonomies in scientific writings, textual
summarization, efficient information retrieval etc. Generating article bundles
from a large number of input articles, based on the associated features of the
articles is tedious and computationally expensive task. In this report we
propose an automatic two-step approach for topic extraction and bundling of
related articles from a set of scientific articles in real-time. For topic
extraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling
techniques and for bundling, we make use of hierarchical agglomerative
clustering techniques.
  We run experiments to validate our bundling semantics and compare it with
existing models in use. We make use of an online crowdsourcing marketplace
provided by Amazon called Amazon Mechanical Turk to carry out experiments. We
explain our experimental setup and empirical results in detail and show that
our method is advantageous over existing ones.
"
674,"Data Life Cycle Labs, A New Concept to Support Data-Intensive Science","  In many sciences the increasing amounts of data are reaching the limit of
established data handling and processing. With four large research centers of
the German Helmholtz association the Large Scale Data Management and Analysis
(LSDMA) project supports an initial set of scientific projects, initiatives and
instruments to organize and efficiently analyze the increasing amount of data
produced in modern science. LSDMA bridges the gap between data production and
data analysis using a novel approach by combining specific community support
and generic, cross community development. In the Data Life Cycle Labs (DLCL)
experts from the data domain work closely with scientific groups of selected
research domains in joint R&D where community-specific data life cycles are
iteratively optimized, data and meta-data formats are defined and standardized,
simple access and use is established as well as data and scientific insights
are preserved in long-term and open accessible archives.
"
675,How Much of the Web Is Archived?,"  Although the Internet Archive's Wayback Machine is the largest and most
well-known web archive, there have been a number of public web archives that
have emerged in the last several years. With varying resources, audiences and
collection development policies, these archives have varying levels of overlap
with each other. While individual archives can be measured in terms of number
of URIs, number of copies per URI, and intersection with other archives, to
date there has been no answer to the question ""How much of the Web is
archived?"" We study the question by approximating the Web using sample URIs
from DMOZ, Delicious, Bitly, and search engine indexes; and, counting the
number of copies of the sample URIs exist in various public web archives. Each
sample set provides its own bias. The results from our sample sets indicate
that range from 35%-90% of the Web has at least one archived copy, 17%-49% has
between 2-5 copies, 1%-8% has 6-10 copies, and 8%-63% has more than 10 copies
in public web archives. The number of URI copies varies as a function of time,
but no more than 31.3% of URIs are archived more than once per month.
"
676,"Identifying Research Fields within Business and Management: A Journal
  Cross-Citation Analysis","  A discipline such as business and management (B&M) is very broad and has many
fields within it, ranging from fairly scientific ones such as management
science or economics to softer ones such as information systems. There are at
least two reasons why it is important to identify these sub-fields accurately.
Firstly, for the purpose of normalizing citation data as it is well known that
citation rates vary significantly between different disciplines. Secondly,
because journal rankings and lists tend to split their classifications into
different subjects, for example the the Association of Business Schools (ABS)
list, which is a standard in the UK, has 22 different fields. Unfortunately, at
the moment these are created in an ad hoc manner with no underlying rigour. The
purpose of this paper is to identify possible sub-fields in B&M rigorously
based on actual citation patterns. We have examined 450 journals in B&M which
are included in the ISI Web of Science (WoS) and analysed the cross-citation
rates between them enabling us to generate sets of coherent and consistent
sub-fields that minimise the extent to which journals appear in several
categories. Implications and limitations of the analysis are discussed
"
677,Word Storms: Multiples of Word Clouds for Visual Comparison of Documents,"  Word clouds are a popular tool for visualizing documents, but they are not a
good tool for comparing documents, because identical words are not presented
consistently across different clouds. We introduce the concept of word storms,
a visualization tool for analysing corpora of documents. A word storm is a
group of word clouds, in which each cloud represents a single document,
juxtaposed to allow the viewer to compare and contrast the documents. We
present a novel algorithm that creates a coordinated word storm, in which words
that appear in multiple documents are placed in the same location, using the
same color and orientation, in all of the corresponding clouds. In this way,
similar documents are represented by similar-looking word clouds, making them
easier to compare and contrast visually. We evaluate the algorithm in two ways:
first, an automatic evaluation based on document classification; and second, a
user study. The results confirm that unlike standard word clouds, a coordinated
word storm better allows for visual comparison of documents.
"
678,International Collaboration in Science: The Global Map and the Network,"  The network of international co-authorship relations has been dominated by
certain European nations and the USA, but this network is rapidly expanding at
the global level. Between 40 and 50 countries appear in the center of the
international network in 2011, and almost all (201) nations are nowadays
involved in international collaboration. In this brief communication, we
present both a global map with the functionality of a Google Map (zooming,
etc.) and network maps based on normalized relations. These maps reveal
complementary aspects of the network. International collaboration in the
generation of knowledge claims (that is, the context of discovery) changes the
structural layering of the sciences. Previously, validation was at the global
level and discovery more dependent on local contexts. This changing
relationship between the geographical and intellectual dimensions of the
sciences also has implications for national science policies.
"
679,"Interactive Overlays of Journals and the Measurement of
  Interdisciplinarity on the basis of Aggregated Journal-Journal Citations","  Using ""Analyze Results"" at the Web of Science, one can directly generate
overlays onto global journal maps of science. The maps are based on the 10,000+
journals contained in the Journal Citation Reports (JCR) of the Science and
Social Science Citation Indices (2011). The disciplinary diversity of the
retrieval is measured in terms of Rao-Stirling's ""quadratic entropy."" Since
this indicator of interdisciplinarity is normalized between zero and one, the
interdisciplinarity can be compared among document sets and across years, cited
or citing. The colors used for the overlays are based on Blondel et al.'s
(2008) community-finding algorithms operating on the relations journals
included in JCRs. The results can be exported from VOSViewer with different
options such as proportional labels, heat maps, or cluster density maps. The
maps can also be web-started and/or animated (e.g., using PowerPoint). The
""citing"" dimension of the aggregated journal-journal citation matrix was found
to provide a more comprehensive description than the matrix based on the cited
archive. The relations between local and global maps and their different
functions in studying the sciences in terms of journal litteratures are further
discussed: local and global maps are based on different assumptions and can be
expected to serve different purposes for the explanation.
"
680,"Protecting Online Documents from an Unauthorized External Access (in
  Bulgarian)","  The modern multi-tier web applications and information systems store and
process various types of data. Some of them are stored in a database,
controlled by an external database management system, while other data are
stored directly within the server's file system. The database is secured by the
database management system itself, but it is a programmer's responsibility to
design and develop a security protection of the files managed by the
information system. This paper summarizes the existing and suggests new rules
for design and implementation of an in-depth security protection of file
resources, published on the Internet, from an unauthorized external access. The
paper is in Bulgarian.
"
681,Academic Ranking with Web Mining and Axiomatic Analysis,"  Academic ranking is a public topic, such as for universities, colleges, or
departments, which has significant educational, administrative and social
effects. Popular ranking systems include the US News & World Report (USNWR),
the Academic Ranking of World Universities (ARWU), and others. The most popular
observables for such ranking are academic publications and their citations.
However, a rigorous, quantitative and thorough methodology has been missing for
this purpose. With modern web technology and axiomatic bibliometric analysis,
here we perform a feasibility study on Microsoft Academic Search metadata and
obtain the first-of-its-kind ranking results for American departments of
computer science. This approach can be extended for fully automatic intuitional
and college ranking based on comprehensive data on Internet.
"
682,"Biases in the Experimental Annotations of Protein Function and their
  Effect on Our Understanding of Protein Function Space","  The ongoing functional annotation of proteins relies upon the work of
curators to capture experimental findings from scientific literature and apply
them to protein sequence and structure data. However, with the increasing use
of high-throughput experimental assays, a small number of experimental studies
dominate the functional protein annotations collected in databases. Here we
investigate just how prevalent is the ""few articles -- many proteins""
phenomenon. We examine the experimentally validated annotation of proteins
provided by several groups in the GO Consortium, and show that the distribution
of proteins per published study is exponential, with 0.14% of articles
providing the source of annotations for 25% of the proteins in the UniProt-GOA
compilation. Since each of the dominant articles describes the use of an assay
that can find only one function or a small group of functions, this leads to
substantial biases in what we know about the function of many proteins.
Mass-spectrometry, microscopy and RNAi experiments dominate high throughput
experiments. Consequently, the functional information derived from these
experiments is mostly of the subcellular location of proteins, and of the
participation of proteins in embryonic developmental pathways. For some
organisms, the information provided by different studies overlap by a large
amount. We also show that the information provided by high throughput
experiments is less specific than those provided by low throughput experiments.
Given the experimental techniques available, certain biases in protein function
annotation due to high-throughput experiments are unavoidable. Knowing that
these biases exist and understanding their characteristics and extent is
important for database curators, developers of function annotation programs,
and anyone who uses protein function annotation data to plan experiments.
"
683,"How to calculate the practical significance of citation impact
  differences? An empirical example from evaluative institutional bibliometrics
  using adjusted predictions and marginal effects","  Evaluative bibliometrics is concerned with comparing research units by using
statistical procedures. According to Williams (2012) an empirical study should
be concerned with the substantive and practical significance of the findings as
well as the sign and statistical significance of effects. In this study we will
explain what adjusted predictions and marginal effects are and how useful they
are for institutional evaluative bibliometrics. As an illustration, we will
calculate a regression model using publications (and citation data) produced by
four universities in German-speaking countries from 1980 to 2010. We will show
how these predictions and effects can be estimated and plotted, and how this
makes it far easier to get a practical feel for the substantive meaning of
results in evaluative bibliometric studies. We will focus particularly on
Average Adjusted Predictions (AAPs), Average Marginal Effects (AMEs), Adjusted
Predictions at Representative Values (APRVs) and Marginal Effects at
Representative Values (MERVs).
"
684,"How relevant is the predictive power of the h-index? A case study of the
  time-dependent Hirsch index","  The h-index has been shown to have predictive power. Here I report results of
an empirical study showing that the increase of the h-index with time often
depends for a long time on citations to rather old publications. This inert
behavior of the h-index means that it is difficult to use it as a measure for
predicting future scientific output.
"
685,Macro-trends in research on the central dogma of molecular biology,"  The central dogma of molecular biology, formulated more than five decades
ago, compartmentalized information exchange in the cell into the DNA, RNA and
protein domains. This formalization has served as an implicit thematic
distinguisher for cell biological research ever since. However, a clear account
of the distribution of research across this formalization over time does not
exist. Abstracts of >3.5 million publications focusing on the cell from 1975 to
2011 were analyzed for the frequency of 100 single-word DNA-, RNA- and
protein-centric search terms and amalgamated to produce domain- and
subdomain-specific trends. A preponderance of protein- over DNA- and in turn
over RNA-centric terms as a percentage of the total word count is evident until
the early 1990s, at which point the trends for protein and DNA begin to
coalesce while RNA percentages remain relatively unchanged. This term-based
census provides a yearly snapshot of the distribution of research interests
across the three domains of the central dogma of molecular biology. A frequency
chart of the most dominantly-studied elements of the periodic table is provided
as an addendum.
"
686,Science 3.0: Corrections to the Science 2.0 paradigm,"  The concept of Science 2.0 was introduced almost a decade ago to describe the
new generation of online-based tools for researchers allowing easier data
sharing, collaboration and publishing. Although technically sound, the concept
still does not work as expected. Here we provide a systematic line of arguments
to modify the concept of Science 2.0, making it more consistent with the spirit
and traditions of science and Internet. Our first correction to the Science 2.0
paradigm concerns the open-access publication models charging fees to the
authors. As discussed elsewhere, we show that the monopoly of such publishing
models increases biases and inequalities in the representation of scientific
ideas based on the author's income. Our second correction concerns
post-publication comments online, which are all essentially non-anonymous in
the current Science 2.0 paradigm. We conclude that scientific post-publication
discussions require special anonymization systems. We further analyze the
reasons of the failure of the current post-publication peer-review models and
suggest what needs to be changed in Science 3.0 to convert Internet into a
large journal club.
"
687,Mandated data archiving greatly improves access to research data,"  The data underlying scientific papers should be accessible to researchers
both now and in the future, but how best can we ensure that these data are
available? Here we examine the effectiveness of four approaches to data
archiving: no stated archiving policy, recommending (but not requiring)
archiving, and two versions of mandating data deposition at acceptance. We
control for differences between data types by trying to obtain data from papers
that use a single, widespread population genetic analysis, STRUCTURE. At one
extreme, we found that mandated data archiving policies that require the
inclusion of a data availability statement in the manuscript improve the odds
of finding the data online almost a thousand-fold compared to having no policy.
However, archiving rates at journals with less stringent policies were only
very slightly higher than those with no policy at all. At one extreme, we found
that mandated data archiving policies that require the inclusion of a data
availability statement in the manuscript improve the odds of finding the data
online almost a thousand fold compared to having no policy. However, archiving
rates at journals with less stringent policies were only very slightly higher
than those with no policy at all. We also assessed the effectiveness of asking
for data directly from authors and obtained over half of the requested
datasets, albeit with about 8 days delay and some disagreement with authors.
Given the long term benefits of data accessibility to the academic community,
we believe that journal based mandatory data archiving policies and mandatory
data availability statements should be more widely adopted.
"
688,Deep Impact: Unintended consequences of journal rank,"  Most researchers acknowledge an intrinsic hierarchy in the scholarly journals
('journal rank') that they submit their work to, and adjust not only their
submission but also their reading strategies accordingly. On the other hand,
much has been written about the negative effects of institutionalizing journal
rank as an impact measure. So far, contributions to the debate concerning the
limitations of journal rank as a scientific impact assessment tool have either
lacked data, or relied on only a few studies. In this review, we present the
most recent and pertinent data on the consequences of our current scholarly
communication system with respect to various measures of scientific quality
(such as utility/citations, methodological soundness, expert ratings or
retractions). These data corroborate previous hypotheses: using journal rank as
an assessment tool is bad scientific practice. Moreover, the data lead us to
argue that any journal rank (not only the currently-favored Impact Factor)
would have this negative impact. Therefore, we suggest that abandoning journals
altogether, in favor of a library-based scholarly communication system, will
ultimately be necessary. This new system will use modern information technology
to vastly improve the filter, sort and discovery functions of the current
journal system.
"
689,Do we need the g-index?,"  Using a very small sample of 8 datasets it was recently shown by De Visscher
(2011) that the g-index is very close to the square root of the total number of
citations. It was argued that there is no bibliometrically meaningful
difference. Using another somewhat larger empirical sample of 26 datasets I
show that the difference may be larger and I argue in favor of the g-index.
"
690,Counting publications and citations: Is more always better?,"  Is more always better? We address this question in the context of
bibliometric indices that aim to assess the scientific impact of individual
researchers by counting their number of highly cited publications. We propose a
simple model in which the number of citations of a publication depends not only
on the scientific impact of the publication but also on other 'random' factors.
Our model indicates that more need not always be better. It turns out that the
most influential researchers may have a systematically lower performance, in
terms of highly cited publications, than some of their less influential
colleagues. The model also suggests an improved way of counting highly cited
publications.
"
691,On bibliographic networks,"  In the paper we show that the bibliographic data can be transformed into a
collection of compatible networks. Using network multiplication different
interesting derived networks can be obtained. In defining them an appropriate
normalization should be considered. The proposed approach can be applied also
to other collections of compatible networks. We also discuss the question when
the multiplication of sparse networks preserves sparseness. The proposed
approaches are illustrated with analyses of collection of networks on the topic
""social network"" obtained from the Web of Science.
"
692,Ontology-based Recommender System of Economic Articles,"  Decision makers need economical information to drive their decisions. The
Company Actualis SARL is specialized in the production and distribution of a
press review about French regional economic actors. This economic review
represents for a client a prospecting tool on partners and competitors. To
reduce the overload of useless information, the company is moving towards a
customized review for each customer. Three issues appear to achieve this goal.
First, how to identify the elements in the text in order to extract objects
that match with the recommendation's criteria presented? Second, How to define
the structure of these objects, relationships and articles in order to provide
a source of knowledge usable by the extraction process to produce new knowledge
from articles? The latter issue is the feedback on customer experience to
identify the quality of distributed information in real-time and to improve the
relevance of the recommendations. This paper presents a new type of
recommendation based on the semantic description of both articles and user
profile.
"
693,"Mapping the network structure of science parks: An exploratory study of
  cross-sectoral interactions reflected on the web","  This study introduces a method based on link analysis to investigate the
structure of the R&D support infrastructure associated with science parks in
order to determine whether this webometric approach gives plausible results.
Three science parks from Yorkshire and the Humber in the UK were analysed with
webometric and social network analysis techniques. Interlinking networks were
generated through the combination of two different data sets extracted from
three sources (Yahoo!, Bing, SocSciBot). These networks suggest that
institutional sectors, representing business, universities and public bodies,
are primarily tied together by a core formed by research institutions, support
structure organisations and business developers. The comparison of the findings
with traditional indicators suggests that the web-based networks reflect the
offline conditions and policy measures adopted in the region, giving some
evidence that the webometric approach is plausible to investigating science
park networks. This is the first study that applies a web-based approach to
investigate to what extent the science parks facilitate a closer interaction
between the heterogeneous organisations that converge in R&D networks. This
indicates that link analysis may help to get a first insight into the
organisation of the R&D support infrastructure provided by science parks.
"
694,"A systematic empirical comparison of different approaches for
  normalizing citation impact indicators","  We address the question how citation-based bibliometric indicators can best
be normalized to ensure fair comparisons between publications from different
scientific fields and different years. In a systematic large-scale empirical
analysis, we compare a traditional normalization approach based on a field
classification system with three source normalization approaches. We pay
special attention to the selection of the publications included in the
analysis. Publications in national scientific journals, popular scientific
magazines, and trade magazines are not included. Unlike earlier studies, we use
algorithmically constructed classification systems to evaluate the different
normalization approaches. Our analysis shows that a source normalization
approach based on the recently introduced idea of fractional citation counting
does not perform well. Two other source normalization approaches generally
outperform the classification-system-based normalization approach that we
study. Our analysis therefore offers considerable support for the use of
source-normalized bibliometric indicators.
"
695,International collaboration clusters in Africa,"  Recent discussion about the increase in international research collaboration
suggests a comprehensive global network centred around a group of core
countries and driven by generic socio-economic factors where the global system
influences all national and institutional outcomes. In counterpoint, we
demonstrate that the collaboration pattern for countries in Africa is far from
universal. Instead, it exhibits layers of internal clusters and external links
that are explained not by monotypic global influences but by regional geography
and, perhaps even more strongly, by history, culture and language. Analysis of
these bottom-up, subjective, human factors is required in order to provide the
fuller explanation useful for policy and management purposes.
"
696,"""Seed+Expand"": A validated methodology for creating high quality
  publication oeuvres of individual researchers","  The study of science at the individual micro-level frequently requires the
disambiguation of author names. The creation of author's publication oeuvres
involves matching the list of unique author names to names used in publication
databases. Despite recent progress in the development of unique author
identifiers, e.g., ORCID, VIVO, or DAI, author disambiguation remains a key
problem when it comes to large-scale bibliometric analysis using data from
multiple databases. This study introduces and validates a new methodology
called seed+expand for semi-automatic bibliographic data collection for a given
set of individual authors. Specifically, we identify the oeuvre of a set of
Dutch full professors during the period 1980-2011. In particular, we combine
author records from the National Research Information System (NARCIS) with
publication records from the Web of Science. Starting with an initial list of
8,378 names, we identify ""seed publications"" for each author using five
different approaches. Subsequently, we ""expand"" the set of publication in three
different approaches. The different approaches are compared and resulting
oeuvres are evaluated on precision and recall using a ""gold standard"" dataset
of authors for which verified publications in the period 2001-2010 are
available.
"
697,A Single Journal Study : Malaysian Journal of Computer Science,"  Single journal studies are reviewed and measures used in the studies are
highlighted. The following quantitative measures are used to study 272 articles
published in Malaysian Journal of Computer Science, (1) the article
productivity of the journal from 1985 to 2007, (2) the observed and expected
authorship productivity tested using Lotka's Law of author productivity,
identification and listing of core authors; (3) the authorship, co-authorship
pattern by authors' country of origin and institutional affiliations; (4) the
subject areas of research; (5) the citation analysis of resources referenced as
well as the age and half-life of citations; the journals referenced and tested
for zonal distribution using Bradford's law of journal scattering; the extent
of web citations; and (6) the citations received by articles published in MJCS
and impact factor of the journal based on information obtained from Google
Scholar, the level of author and journal self-citation.
"
698,"Auditing scholarly journals published in Malaysia and assessing their
  visibility","  The problem with the identification of Malaysian scholarly journals lies in
the lack of a current and complete listing of journals published in Malaysia.
As a result, librarians are deprived of a tool that can be used for journal
selection and identification of gaps in their serials collection. This study
describes the audit carried out on scholarly journals, with the objectives (a)
to trace and characterized scholarly journal titles published in Malaysia, and
(b) to determine their visibility in international and national indexing
databases. A total of 464 titles were traced and their yearly trends, publisher
and publishing characteristics, bibliometrics and indexation in national,
international and subject-based indexes were described.
"
699,"Publication productivity and citation analysis of the Medical Journal of
  Malaysia: 2004 - 2008","  We analysed 580 articles (original articles only) published in Medical
Journal of Malaysia between 2004 and 2008, the resources referenced by the
articles and the citations and impact received. Our aim was to examine article
and author productivity, the age of references used and impact of the journal.
Publication data was obtained from MyAIS database and Google Scholar provided
the citation data. From the 580 articles analyzed, contributors mainly come
from the hospitals, universities and clinics. Contributions from foreign
authors are low. The useful lives of references cited were between 3 to 11
years. ISI derived Impact factor for MJM ranged between 0.378 to 0.616. Journal
self-citation is low. Out of the 580 sampled articles, 76.8% have been cited at
least once over the 5 years and the ratio of total publications to citations is
1: 2.6.
"
700,"Exploring the needs of Malay manuscript studies community for an
  e-learning platform","  Philology studies are often associated with traditional methods of teaching
and learning. This study explores the possibility of e-learning adoption
amongst Malay manuscripts learning community. The Soft System Methodology (SSM)
is used to guide the investigation. SSM emphasises on understanding the problem
situations faced by Malay manuscript learning community and expresses the
situations in rich pictures. The manuscript learning community comprises
lecturers, students and researchers in the field of philology. Data were
gathered from interviews, focus group discussions and observations. Academy of
Malay Studies, University of Malaya is the case study setting, focusing on
lecturers who teach and students who enrol in a philology course as well as
doctoral students researching on manuscript studies. The findings highlight
problems faced by the various stakeholders and propose solutions in the form of
a conceptual model for a collaborative electronic platform to improve teaching
and learning as well as utilizing digitized manuscript surrogates held in a
digital library of Malay manuscripts.
"
701,Measuring the influence of a journal using impact and diffusion factors,"  Presents the result of the calculated IS! equivalent Impact Factor, Relative
Diffusion Factor (RDF), and Journal Diffusion Factor (JDF) for articles
published in the Medical Journal of Malaysia (MJM) between the years 2004 and
2008 in both their synchronous and diachronous versions. The publication data
are collected from MyAis (Malaysian Abstracting & Indexing system) while the
citation data are collected from Google Scholar. The values of the synchronous
JDF ranges from 0.057 - 0.14 while the diachronous JDF ranges from 0.46 - 1.98.
The high diachronous JDF is explained by a relatively high number of different
citing journals against the number of publications. This implies that the
results of diachronous JDF is influenced by the numbers of publications and a
good comparison may be one of which the subject of analysis have similar number
of publications and citations period. The yearly values of the synchronous RDF
vary in the range of 0.66 - 1.00 while diachronous RDF ranges from 0.62 - 0.88.
The result shows that diachronous RDF is negatively correlated with the number
of citations, resulting in a low RDF value for highly cited publication years.
What this implies in practice is that the diffusion factors can be calculated
for every additional year at any journal level of analysis. This study
demonstrates that these indicators are valuable tools that help to show
development of journals as it changes through time.
"
702,International Contribution to Nipah Virus Research 1999-2010,"  This study examines 462 papers on Nipah virus research published from 1999 to
2010, identifying the active authors, institutions and citations received. Data
was extracted from SCI-Expanded database, (Web of Science) and analyzed using
descriptive figures and tables. The results show the growth of publication is
incremental up to 2010 even though the average citations received is
decreasing. The ratio of authors to articles is 1330: 426. The active
contributing countries are USA (41.0%), Australia (19.3%), Malaysia (16.0%),
England (6.5%) and France (5.6%). The productive authors are mainly affiliated
to the Centre for Disease Control and Prevention, USA and Commonwealth
Scientific and Industrial Research Organization (CSIRO) in Australia and
University of Malaya Medical Centre, Malaysia. A total of 10572 citations were
received and the ratio of articles to citation is 1: 24.8. Collaboration with
the bigger laboratories in USA and Australia is contributive to the sustained
growth of published literature and to access diverse expertise.
"
703,"Collection security management at university libraries: assessment of
  its implementation status","  This study examines the literature on library security and collection
security to identify factors to be considered to develop a collection security
management assessment instrument for university libraries. A ""house"" model was
proposed consisting of five factors; collection security governance, operations
and processes, people issues, physical and technical issues and the security
culture in libraries. An assessment instrument listing items covering the five
factors was pilot tested on 61 samples comprising chief librarians, deputy
librarians, departmental, sectional heads and professional staff working in
four university libraries in Nigeria. The level of security implementation is
assessed on a scale of 1=not-implemented, 2=planning stage, 3=partial
implementation, 4=close to completion, and 5=full implementation. The
instrument was also tested for reliability. Reliability tests indicate that all
five factors are reliable with Cronbach's alpha values between 0.7 and 0.9,
indicating that the instrument can be used for wider distribution to explore
and assess the level of collection security implementation in university
libraries from a holistic perspective.
"
704,"Information systems security in special and public libraries: an
  assessment of status","  Explores the use of an assessment instrument based on a model named library
information systems security assessment model (LISSAM) to assess the 155 status
in special and public libraries in Malaysia. The study aims to determine the
implementation status of technological and organizational components of the
LISSAM model. An implementation index as well as a scoring tool is presented to
assess the IS safeguarding measures in a library. Data used was based on
questionnaires distributed to a total of 50 individuals who are responsible for
the information systems (IS) or IT in the special and public libraries in
Malaysia. Findings revealed that over 95% of libraries have high level of
technological implementation but 54% were fair poorly on organizational
measures, especially on lack of security procedures, administrative tools and
awareness creation activities.
"
705,"Open Access repositories and journals for visibility: Implications for
  Malaysian libraries","  This paper describes the growth of Open Access (OA) repositories and journals
as reported by monitoring initiatives such as ROAR (Registry of Open Access
Repositories), Open DOAR (Open Directory of Open Access Repositories), DOAJ
(Directory of Open Access Journals), Directory of Web Ranking of World
Repositories by the Cybermetrics Laboratory in Spain and published literature.
The performance of Malaysian OA repositories and journals is highlighted. The
strength of OA channels in increasing visibility and citations are evidenced by
research findings. It is proposed that libraries champion OA initiatives by
making university or institutional governance aware; encouraging institutional
journal publishers to adopt OA platform; collaborating with research groups to
jumpstart OA institutional initiatives and to embed OA awareness into user and
researcher education programmes. By actively involved, libraries will be free
of permission, licensing and archiving barriers usually imposed in traditional
publishing situation.
"
706,"Internationalization of Malaysian Mathematical and Computer Science
  Journal","  The internationalization characteristics of two Malaysian journals, Bulletin
of the Malaysian Mathematical Sciences Society (indexed by ISI) and the
Malaysian Journal of Computer Science (indexed by Inspec and Scopus) is
observed. All issues for the years 2000 to 2007 were looked at to obtain the
following information, (i) total articles published between 2000 and 2007; (ii)
the distribution of foreign and Malaysian authors publishing in the journals;
(iii) the distribution of articles by country and (iv) the geographical
distribution of authors citing articles published in the journals. Citation to
articles is derived from information given by Google scholar. The results
indicate that both journals exhibit average internationalization
characteristics as they are current in their publications but with between 19%
-30% international composition of reviewers or editorials, publish between
36%-79% of foreign articles and receive between 60%-70% of citations from
foreign authors.
"
707,"Collaborative digital library of historical resources: Evaluation of
  first users","  This paper describes the digital library of historical resources, a research
project which involves building a testbed for the purpose of developing and
testing new collaborative digital library functionality and presents an initial
analysis of the digital library's public use on the web. The digital library is
modeled to focus on serving secondary students information needs in conducting
history projects. As such, in the implementation of the digital library, the
use of online resources would be an integral part of history project based
learning activities. Students should be enabled to access digital resources,
create and publish their own documents in the digital library and share them
with others. As a testbed system, the collaborative digital library known as
CoreDev has demonstrated its capabilities in serving an educational community
as has been reflected by the positive feedback on the functional requirements
from 44 users. Over 75% of the respondents in the user survey considered
themselves capable of using the digital library easily. The beta tester
demographics (n = 105) indicate that the digital library is reaching its target
communities.
"
708,"The Pattern of E-Book Use amongst Undergraduates an Malaysia: A Case of
  to Know is to Use","  This exploratory study focuses on identifying the usage pattern of e-books
especially on how, when, where and why undergraduates at the Faculty of
Computer Science and Information Technology (FCSIT), University of Malaya (UM),
Kuala Lumpur use or do not use the e-books service provided by the University
of Malaya library. A total of 206 (82%) useable questionnaires form the basis
of analysis. The results indicate even though the students are heavy users of
the Internet, rate themselves as skilled in Internet use and have positive
attitude towards the e-book service, the level of e-book use is still low
(39%). The students become aware of the e-book service mainly while visiting
the University of Malaya Library Website, or are referred to it by their
lecturers, friends or the librarians. About 70% rate positively on the e-book
service. Those who are users of e-books find e-books easy to use and their
usages are mainly for writing assignment or project work. Most respondents
prefer to use e-versions of textbooks and reference sources. Generally, both
users and non-users of e-books prefer to use the printed version of textbooks
especially if the text is continuously used. There are significant difference
between the frequency of e-book use and gender; between past usage of e-book
and preference for electronic textbooks and reference books. The possible
factors which may be related to e-book use are categorized into 4 groups and
presented in a model, which comprises the ICT competencies of the students,
their cognitive makeup, the degree of user access to the e-books and the
functional or use factors.
"
709,"Association between quality of clinical practice guidelines and
  citations given to their references","  It has been suggested that bibliometric analysis of different document types
may reveal new aspects of research performance. In medical research a number of
study types play different roles in the research process and it has been shown,
that the evidence-level of study types is associated with varying citation
rates. This study focuses on clinical practice guidelines, which are supposed
to gather the highest evidence on a given topic to give the best possible
recommendation for practitioners. The quality of clinical practice guidelines,
measured using the AGREE score, is compared to the citations given to the
references used in these guidelines, as it is hypothesised, that better
guidelines are based on higher cited references. AGREE scores are gathered from
reviews of clinical practice guidelines on a number of diseases and treatments.
Their references are collected from Web of Science and citation counts are
normalised using the item-oriented z-score and the PPtop-10% indicators. A
positive correlation between both citation indicators and the AGREE score of
clinical practice guidelines is found. Some potential confounding factors are
identified. While confounding cannot be excluded, results indicate low
likelihood for the identified confounders. The results provide a new
perspective to and application of citation analysis.
"
710,"Reuse, Temporal Dynamics, Interest Sharing, and Collaboration in Social
  Tagging Systems","  User-generated content is shaping the dynamics of the World Wide Web. Indeed,
an increasingly large number of systems provide mechanisms to support the
growing demand for content creation, sharing, and management. Tagging systems
are a particular class of these systems where users share and collaboratively
annotate content such as photos and URLs. This collaborative behavior and the
pool of user-generated metadata create opportunities to improve existing
systems and to design new mechanisms. However, to realize this potential, it is
necessary to understand the usage characteristics of current systems. This work
addresses this issue characterizing three tagging systems (CiteULike, Connotea
and del.icio.us) while focusing on three aspects: i) the patterns of
information (tags and items) production; ii) the temporal dynamics of users'
tag vocabularies; and, iii) the social aspects of tagging systems.
"
711,PDF articles metadata harvester,"  Scientific journals are very important in recording the finding from
researchers around the world. The recent media to disseminate scientific
journals is PDF. On scheme to find the scientific journals over the internet is
via metadata. Metadata stores information about article summary. Embedding
metadata into PDF of scientific article will grant the consistency of metadata
readness. Harvesting the metadata from scientific journal is very interesting
field at the moment. This paper will discuss about scientific journal metadata
harvesters involving XMP.
"
712,"Mutual Redundancies in Inter-human Communication Systems: Steps Towards
  a Calculus of Processing Meaning","  The study of inter-human communication requires a more complex framework than
Shannon's (1948) mathematical theory of communication because ""information"" is
defined in the latter case as meaningless uncertainty. Assuming that meaning
cannot be communicated, we extend Shannon's theory by defining mutual
redundancy as a positional counterpart of the relational communication of
information. Mutual redundancy indicates the surplus of meanings that can be
provided to the exchanges in reflexive communications. The information is
redundant because based on ""pure sets,"" that is, without subtraction of mutual
information in the overlaps. We show that in the three-dimensional case (e.g.,
of a Triple Helix of university-industry-government relations), mutual
redundancy is equal to mutual information (Rxyz = Txyz); but when the
dimensionality is even, the sign is different. We generalize to the measurement
in N dimensions and proceed to the interpretation. Using Luhmann's
social-systems theory and/or Giddens' structuration theory, mutual redundancy
can be provided with an interpretation in the sociological case: different
meaning-processing structures code and decode with other algorithms. A surplus
of (""absent"") options can then be generated that add to the redundancy.
Luhmann's ""functional (sub)systems"" of expectations or Giddens' ""rule-resource
sets"" are positioned mutually, but coupled operationally in events or
""instantiated"" in actions. Shannon-type information is generated by the
mediation, but the ""structures"" are (re-)positioned towards one another as sets
of (potentially counterfactual) expectations. The structural differences among
the coding and decoding algorithms provide a source of additional options in
reflexive and anticipatory communications.
"
713,"Empirical Evidence for the Relevance of Fractional Scoring in the
  Calculation of Percentile Rank Scores","  Fractional scoring has been proposed to avoid inconsistencies in the
attribution of publications to percentile rank classes. Uncertainties and
ambiguities in the evaluation of percentile ranks can be demonstrated most
easily with small datasets. But for larger datasets an often large number of
papers with the same citation count leads to the same uncertainties and
ambiguities which can be avoided by fractional scoring. This is demonstrated
for four different empirical datasets with several thousand publications each
which are assigned to 6 percentile rank classes. Only by utilizing fractional
scoring the total score of all papers exactly reproduces the theoretical value
in each case.
"
714,An OAI-PMH-based Web Service for the Generation of Co-Author Networks,"  We will present a new component of our technical framework that was built to
provide a brought range of reusable web services for the enhancement of typical
scientific retrieval processes. The proposed component computes betweenness of
authors in co-authorship networks extracted from publicly available metadata
that was harvested using OAI-PMH.
"
715,The citation wake of publications detects Nobel laureates' papers,"  For several decades, a leading paradigm of how to quantitatively assess
scientific research has been the analysis of the aggregated citation
information in a set of scientific publications. Although the representation of
this information as a citation network has already been coined in the 1960s, it
needed the systematic indexing of scientific literature to allow for impact
metrics that actually made use of this network as a whole improving on the then
prevailing metrics that were almost exclusively based on the number of direct
citations. However, besides focusing on the assignment of credit, the paper
citation network can also be studied in terms of the proliferation of
scientific ideas. Here we introduce a simple measure based on the
shortest-paths in the paper's in-component or, simply speaking, on the shape
and size of the wake of a paper within the citation network. Applied to a
citation network containing Physical Review publications from more than a
century, our approach is able to detect seminal articles which have introduced
concepts of obvious importance to the further development of physics. We
observe a large fraction of papers co-authored by Nobel Prize laureates in
physics among the top-ranked publications.
"
716,"Learning to Rank for Expert Search in Digital Libraries of Academic
  Publications","  The task of expert finding has been getting increasing attention in
information retrieval literature. However, the current state-of-the-art is
still lacking in principled approaches for combining different sources of
evidence in an optimal way. This paper explores the usage of learning to rank
methods as a principled approach for combining multiple estimators of
expertise, derived from the textual contents, from the graph-structure with the
citation patterns for the community of experts, and from profile information
about the experts. Experiments made over a dataset of academic publications,
for the area of Computer Science, attest for the adequacy of the proposed
approaches.
"
717,Benchmarking some Portuguese S&T system research units: 2nd Edition,"  The increasing use of productivity and impact metrics for evaluation and
comparison, not only of individual researchers but also of institutions,
universities and even countries, has prompted the development of bibliometrics.
Currently, metrics are becoming widely accepted as an easy and balanced way to
assist the peer review and evaluation of scientists and/or research units,
provided they have adequate precision and recall.
  This paper presents a benchmarking study of a selected list of representative
Portuguese research units, based on a fairly complete set of parameters:
bibliometric parameters, number of competitive projects and number of PhDs
produced. The study aimed at collecting productivity and impact data from the
selected research units in comparable conditions i.e., using objective metrics
based on public information, retrievable on-line and/or from official sources
and thus verifiable and repeatable. The study has thus focused on the activity
of the 2003-06 period, where such data was available from the latest official
evaluation.
  The main advantage of our study was the application of automatic tools,
achieving relevant results at a reduced cost. Moreover, the results over the
selected units suggest that this kind of analyses will be very useful to
benchmark scientific productivity and impact, and assist peer review.
"
718,Modeling citation networks based on vigorousness and dormancy,"  In citation networks, the activity of papers usually decreases with age and
dormant papers may be discovered and become fashionable again. To model this
phenomenon, a competition mechanism is suggested which incorporates two
factors: vigorousness and dormancy. Based on this idea, a citation network
model is proposed, in which a node has two discrete stage: vigorous and
dormant. Vigorous nodes can be deactivated and dormant nodes may be activated
and become vigorous. The evolution of the network couples addition of new nodes
and state transitions of old ones. Both analytical calculation and numerical
simulation show that the degree distribution of nodes in generated networks
displays a good right-skewed behavior. Particularly, scale-free networks are
obtained as the deactivated vertex is target selected and exponential networks
are realized for the random-selected case. Moreover, the measurement of four
real-world citation networks achieves a good agreement with the stochastic
model.
"
719,"On the use of Biplot analysis for multivariate bibliometric and
  scientific indicators","  Bibliometric mapping and visualization techniques represent one of the main
pillars in the field of scientometrics. Traditionally, the main methodologies
employed for representing data are Multi-Dimensional Scaling, Principal
Component Analysis or Correspondence Analysis. In this paper we aim at
presenting a visualization methodology known as Biplot analysis for
representing bibliometric and science and technology indicators. A Biplot is a
graphical representation of multivariate data, where the elements of a data
matrix are represented according to dots and vectors associated with the rows
and columns of the matrix. In this paper we explore the possibilities of
applying the Biplot analysis in the research policy area. More specifically we
will first describe and introduce the reader to this methodology and secondly,
we will analyze its strengths and weaknesses through three different study
cases: countries, universities and scientific fields. For this, we use a Biplot
analysis known as JK-Biplot. Finally we compare the Biplot representation with
other multivariate analysis techniques. We conclude that Biplot analysis could
be a useful technique in scientometrics when studying multivariate data and an
easy-to-read tool for research decision makers.
"
720,"Open Access, library and publisher competition, and the evolution of
  general commerce","  Discussions of the economics of scholarly communication are usually devoted
to Open Access, rising journal prices, publisher profits, and boycotts. That
ignores what seems a much more important development in this market.
Publishers, through the oft-reviled ""Big Deal"" packages, are providing much
greater and more egalitarian access to the journal literature, an approximation
to true Open Access. In the process they are also marginalizing libraries, and
obtaining a greater share of the resources going into scholarly communication.
This is enabling a continuation of publisher profits as well as of what for
decades has been called ""unsustainable journal price escalation"". It is also
inhibiting the spread of Open Access, and potentially leading to an oligopoly
of publishers controlling distribution through large-scale licensing.
  The ""Big Deal"" practices are worth studying for several general reasons. The
degree to which publishers succeed in diminishing the role of libraries may be
an indicator of the degree and speed at which universities transform
themselves. More importantly, these ""Big Deals"" appear to point the way to the
future of the whole economy, where progress is characterized by declining
privacy, increasing price discrimination, increasing opaqueness in pricing,
increasing reliance on low-paid or upaid work of others for profits, and
business models that depend on customer inertia.
"
721,Key Choices in the Design of Simple Knowledge Organization System (SKOS),"  Simple Knowledge Organization System (SKOS) provides a data model and
vocabulary for expressing Knowledge Organization Systems (KOSs) such as
thesauri and classification schemes in Semantic Web applications. This paper
presents the main components of SKOS and their formal expression in Web
Ontology Language (OWL), providing an extensive account of the design decisions
taken by the Semantic Web Deployment (SWD) Working Group of the World Wide Web
Consortium (W3C), which between 2006 and 2009 brought SKOS to the status of W3C
Recommendation. The paper explains key design principles such as ""minimal
ontological commitment"" and systematically cites the requirements and issues
that influenced the design of SKOS components.
  By reconstructing the discussion around alternative features and design
options and presenting the rationale for design decisions, the paper aims at
providing insight into how SKOS turned out as it did, and why. Assuming that
SKOS, like any other successful technology, may eventually be subject to
revision and improvement, the critical account offered here may help future
editors approach such a task with deeper understanding.
"
722,"H Index of History journals published in Spain according to Google
  Scholar Metrics (2007-2011)","  Google Scholar Metrics (GSM), which was recently launched in April 2012,
features new bibliometric systems for gauging scientific journals by counting
the number of citations obtained in Google Scholar. This way, it opens new
possibilities for measuring journal impacts in the field of Humanities. The
present article intends to evaluate the scope of this tool through analysing
GSM searches, from the 5th through 6th of December 2012, of History journals
published in Spain. In sum, 69 journals were identified, accounting for only
24% of the History journals published in Spain. The ranges of H index values
for this field are so small that the ranking can no longer be said to show a
discriminating potential. In the light of this, we would like to propose a
change in the way Google Scholar Metrics is designed so that it could also
accommodate production and citation patterns in the particular field of
History, and, in a broader scope, in the area of Humanities as well.
"
723,Editorial: Making GIScience Research More Open Access,"  This is the editorial for the special issue on ""data-intensive geospatial
computing"", which I guest edited with the International Journal of Geographical
Information Science (Taylor & Francis). As remarked in the editorial, the
special issue is particularly special in the sense that all source and data are
published together with the published papers. This editorial elaborates on
scholarly communication, with particular attention to publishing data alongside
papers and the emergence of open access journals, in order to make our research
more open access.
"
724,Ontology-Based Administration of Web Directories,"  Administration of a Web directory and maintenance of its content and the
associated structure is a delicate and labor intensive task performed
exclusively by human domain experts. Subsequently there is an imminent risk of
a directory structures becoming unbalanced, uneven and difficult to use to all
except for a few users proficient with the particular Web directory and its
domain. These problems emphasize the need to establish two important issues: i)
generic and objective measures of Web directories structure quality, and ii)
mechanism for fully automated development of a Web directory's structure. In
this paper we demonstrate how to formally and fully integrate Web directories
with the Semantic Web vision. We propose a set of criteria for evaluation of a
Web directory's structure quality. Some criterion functions are based on
heuristics while others require the application of ontologies. We also suggest
an ontology-based algorithm for construction of Web directories. By using
ontologies to describe the semantics of Web resources and Web directories'
categories it is possible to define algorithms that can build or rearrange the
structure of a Web directory. Assessment procedures can provide feedback and
help steer the ontology-based construction process. The issues raised in the
article can be equally applied to new and existing Web directories.
"
725,Assessing Semantic Quality of Web Directory Structure,"  The administration of a Web directory content and associated structure is a
labor intensive task performed by human domain experts. Because of that there
always exists a realistic risk of the structure becoming unbalanced, uneven and
difficult to use to all except for a few users proficient in a particular Web
directory. These problems emphasize the importance of generic and objective
measures of Web directories structure quality. In this paper we demonstrate how
to formally merge Web directories into the Semantic Web vision. We introduce a
set of objective criterions for evaluation of a Web directory's structure
quality. Some criteria functions are based on heuristics while others require
the application of ontologies.
"
726,"The transition towards immortality: non-linear autocatalytic growth of
  citations to scientific papers","  We discuss microscopic mechanisms of complex network growth, with the special
emphasis of how these mechanisms can be evaluated from the measurements on real
networks. As an example we consider the network of citations to scientific
papers. Contrary to common belief that its growth is determined by the linear
preferential attachment, our microscopic measurements show that it is driven by
the nonlinear autocatalytic growth. This invalidates the scale-free hypothesis
for the citation network. The nonlinearity is responsible for a dramatic
dynamical phase transition: while the citation lifetime of majority of papers
is 6-10 years, the highly-cited papers have practically infinite lifetime.
"
727,Is Europe Evolving Toward an Integrated Research Area?,"  An integrated European Research Area (ERA) is a critical component for a more
competitive and open European R&D system. However, the impact of EU-specific
integration policies aimed at overcoming innovation barriers associated with
national borders is not well understood. Here we analyze 2.4 x 10^6 patent
applications filed with the European Patent Office (EPO) over the 25-year
period 1986-2010 along with a sample of 2.6 x 10^5 records from the ISI Web of
Science to quantitatively measure the role of borders in international R&D
collaboration and mobility. From these data we construct five different
networks for each year analyzed: (i) the patent co-inventor network, (ii) the
publication co-author network, (iii) the co-applicant patent network, (iv) the
patent citation network, and (v) the patent mobility network. We use methods
from network science and econometrics to perform a comparative analysis across
time and between EU and non-EU countries to determine the ""treatment effect""
resulting from EU integration policies. Using non-EU countries as a control
set, we provide quantitative evidence that, despite decades of efforts to build
a European Research Area, there has been little integration above global trends
in patenting and publication. This analysis provides concrete evidence that
Europe remains a collection of national innovation systems.
"
728,The DPHEP Study Group: Data Preservation in High Energy Physics,"  An inter-experimental study group, DPHEP, was formed in 2009 to
systematically investigate the technical and organisational aspects of data
preservation and long-term analysis in high-energy physics, a subject which had
hitherto lacked clarity in the field. The study group includes representation
from all major high-energy physics collider-based experiments and laboratories,
as well as computing centres and funding agencies. A major report was released
in May 2012, greatly expanding on the ideas contained in a preliminary
publication three years earlier, and providing a more solid set of
recommendations, not only concerning data preservation and its implementation
in high-energy physics, but also the future direction and organisational model
of the study group. A brief description of the DPHEP Study Group and some of
the key messages from the major report are presented.
"
729,"How to evaluate individual researchers working in the natural and life
  sciences meaningfully? A proposal of methods based on percentiles of
  citations","  Although bibliometrics has been a separate research field for many years,
there is still no uniformity in the way bibliometric analyses are applied to
individual researchers. Therefore, this study aims to set up proposals how to
evaluate individual researchers working in the natural and life sciences. 2005
saw the introduction of the h index, which gives information about a
researcher's productivity and the impact of his or her publications in a single
number (h is the number of publications with at least h citations); however, it
is not possible to cover the multidimensional complexity of research
performance and to undertake inter-personal comparisons with this number. This
study therefore includes recommendations for a set of indicators to be used for
evaluating researchers. Our proposals relate to the selection of data on which
an evaluation is based, the analysis of the data and the presentation of the
results.
"
730,"Mapping Academic Institutions According to Their Journal Publication
  Profile: Spanish Universities as a Case Study","  We introduce a novel methodology for mapping academic institutions based on
their journal publication profiles. We believe that journals in which
researchers from academic institutions publish their works can be considered as
useful identifiers for representing the relationships between these
institutions and establishing comparisons. However, when academic journals are
used for research output representation, distinctions must be introduced
between them, based on their value as institution descriptors. This leads us to
the use of journal weights attached to the institution identifiers. Since a
journal in which researchers from a large proportion of institutions published
their papers may be a bad indicator of similarity between two academic
institutions, it seems reasonable to weight it in accordance with how
frequently researchers from different institutions published their papers in
this journal. Cluster analysis can then be applied to group the academic
institutions, and dendrograms can be provided to illustrate groups of
institutions following agglomerative hierarchical clustering. In order to test
this methodology, we use a sample of Spanish universities as a case study. We
first map the study sample according to an institution's overall research
output, then we use it for two scientific fields (Information and Communication
Technologies, as well as Medicine and Pharmacology) as a means to demonstrate
how our methodology can be applied, not only for analyzing institutions as a
whole, but also in different disciplinary contexts.
"
731,"Three-feature model to reproduce the topology of citation networks and
  the effects from authors' visibility on their h-index","  Various factors are believed to govern the selection of references in
citation networks, but a precise, quantitative determination of their
importance has remained elusive. In this paper, we show that three factors can
account for the referencing pattern of citation networks for two topics, namely
""graphenes"" and ""complex networks"", thus allowing one to reproduce the
topological features of the networks built with papers being the nodes and the
edges established by citations. The most relevant factor was content
similarity, while the other two - in-degree (i.e. citation counts) and {age of
publication} had varying importance depending on the topic studied. This
dependence indicates that additional factors could play a role. Indeed, by
intuition one should expect the reputation (or visibility) of authors and/or
institutions to affect the referencing pattern, and this is only indirectly
considered via the in-degree that should correlate with such reputation.
Because information on reputation is not readily available, we simulated its
effect on artificial citation networks considering two communities with
distinct fitness (visibility) parameters. One community was assumed to have
twice the fitness value of the other, which amounts to a double probability for
a paper being cited. While the h-index for authors in the community with larger
fitness evolved with time with slightly higher values than for the control
network (no fitness considered), a drastic effect was noted for the community
with smaller fitness.
"
732,"Citation impacts revisited: how novel impact measures reflect
  interdisciplinarity and structural change at the local and global level","  Citation networks have fed numerous works in scientific evaluation, science
mapping (and more recently large-scale network studies) for decades. The
variety of citation behavior across scientific fields is both a research topic
in sociology of science, and a problem in scientific evaluation. Normalization,
tantamount to a particular weighting of links in the citation network, is
necessary for allowing across-field comparisons of citation scores and
interdisciplinary studies. In addition to classical normalization which
drastically reduces all variability factors altogether, two tracks of research
have emerged in the recent years. One is the revival of iterative ""influence
measures"". The second is the ""citing-side"" normalization, whose only purpose is
to control for the main factor of variability, the inequality in citing
propensity, letting other aspects play: knowledge export/imports and growth.
When all variables are defined at the same field-level, two propositions are
established: (a) the gross impact measure identifies with the product of
relative growth rate, gross balance of citation exchanges, and relative number
of references (b) the normalized impact identifies with the product of relative
growth rate and normalized balance. At the science level, the variance of
growth rate over domains is a proxy for change in the system, and the variance
of balance a measure of inter-disciplinary dependences. This opens a new
perspective, where the resulting variance of normalized impact, and a related
measure, the sum of these variances proposed as a Change-Exchange Indicator,
summarize important aspects of science structure and dynamism. Results based on
a decade's data are discussed. The behavior of normalized impact according to
scale changes is also briefly discussed.
"
733,"On the use of topological features and hierarchical characterization for
  disambiguating names in collaborative networks","  Many features of complex systems can now be unveiled by applying statistical
physics methods to treat them as social networks. The power of the analysis may
be limited, however, by the presence of ambiguity in names, e.g., caused by
homonymy in collaborative networks. In this paper we show that the ability to
distinguish between homonymous authors is enhanced when longer-distance
connections are considered, rather than looking at only the immediate neighbors
of a node in the collaborative network. Optimized results were obtained upon
using the 3rd hierarchy in connections. Furthermore, reasonable distinction
among authors could also be achieved upon using pattern recognition strategies
for the data generated from the topology of the collaborative network. These
results were obtained with a network from papers in the arXiv repository, into
which homonymy was deliberately introduced to test the methods with a
controlled, reliable dataset. In all cases, several methods of supervised and
unsupervised machine learning were used, leading to the same overall results.
The suitability of using deeper hierarchies and network topology was confirmed
with a real database of movie actors, with the additional finding that the
distinguishing ability can be further enhanced by combining topology features
and long-range connections in the collaborative network.
"
734,"Linking Things on the Web: A Pragmatic Examination of Linked Data for
  Libraries, Archives and Museums","  The Web publishing paradigm of Linked Data has been gaining traction in the
cultural heritage sector: libraries, archives and museums. At first glance, the
principles of Linked Data seem simple enough. However experienced Web
developers, designers and architects who attempt to put these ideas into
practice often find themselves having to digest and understand debates about
Web architecture, the semantic web, artificial intelligence and the
philosophical nature of identity. In this paper I will discuss some of the
reasons why Linked Data is of interest to the cultural heritage community, what
some of the pain points are for deploying it, and characterize some pragmatic
ways for cultural heritage organizations to realize the goals of Linked Data
with examples from the Web we have today.
"
735,"An Ontology for Modelling and Supporting the Process of Authoring
  Technical Assessments","  In this paper, we present a semantic web approach for modelling the process
of creating new technical and regulatory documents related to the Building
sector. This industry, among other industries, is currently experiencing a
phenomenal growth in its technical and regulatory texts. Therefore, it is
urgent and crucial to improve the process of creating regulations by automating
it as much as possible. We focus on the creation of particular technical
documents issued by the French Scientific and Technical Centre for Building
(CSTB), called Technical Assessments, and we propose services based on Semantic
Web models and techniques for modelling the process of their creation.
"
736,"Technology Transfer and the End of the Bayh-Dole Effect: Patents as an
  Analytical Lens on University-Industry-Government Relations","  Three periods can be distinguished in university patenting at the U.S. Patent
and Trade Office (USPTO) since the Bayh-Dole Act of 1980: (1) a first period of
exponential increase in university patenting till 1995 (filing date) or 1999
(issuing date); (2) a period of relative decline since 1999; and (3) in most
recent years -- since 2008 -- a linear increase in university patenting. We
argue that this last period is driven by specific non-US universities (e.g.,
Tokyo University and Chinese universities) patenting increasingly in the U.S.A.
as the most competitive market for high-tech patents.
"
737,Mapping Citation Patterns of Book Chapters in the Book Citation Index,"  In this paper we provide the reader with a visual representation of
relationships among the impact of book chapters indexed in the Book Citation
Index using information gain values and published by different academic
publishers in specific disciplines. The impact of book chapters can be
characterized statistically by citations histograms. For instance, we can
compute the probability of occurrence of book chapters with a number of
citations in different intervals for each academic publisher. We predict the
similarity between two citation histograms based on the amount of relative
information between such characterizations. We observe that the citation
patterns of book chapters follow a Lotkaian distribution. This paper describes
the structure of the Book Citation Index using 'heliocentric clockwise maps'
which allow the reader not only to determine the grade of similarity of a given
academic publisher indexed in the Book Citation Index with a specific
discipline according to their citation distribution, but also to easily observe
the general structure of a discipline, identifying the publishers with higher
impact and output.
"
738,Inconsistencies of the Highly-Cited-Publications Indicator,"  One way of evaluating individual scientists is the determination of the
number of highly cited publications, where the threshold is given by a large
reference set. It is shown that this indicator behaves in a counterintuitive
way, leading to inconsistencies in the ranking of different scientists.
"
739,"How much do different ways of calculating percentiles influence the
  derived performance indicators? - A case study","  Bibliometric indicators can be determined by comparing specific citation
records with the percentiles of a reference set. However, there exists an
ambiguity in the computation of percentiles because usually a significant
number of papers with the same citation count are found at the border between
percentile rank classes. The present case study of the citations to the journal
Europhysics Letters (EPL) in comparison with all physics papers from the Web of
Science shows the deviations which occur due to the different ways of treating
the tied papers in the evaluation of the percentage of highly cited
publications. A strong bias can occur, if the papers tied at the threshold
number of citations are all considered as highly cited or all considered as not
highly cited.
"
740,How to derive an advantage from the arbitrariness of the g-index,"  The definition of the g-index is as arbitrary as that of the h-index, because
the threshold number g^2 of citations to the g most cited papers can be
modified by a prefactor at one's discretion, thus taking into account more or
less of the highly cited publications within a dataset. In a case study I
investigate the citation records of 26 physicists and show that the prefactor
influences the ranking in terms of the generalized g-index less than for the
generalized h-index. I propose specifically a prefactor of 2 for the g-index,
because then the resulting values are of the same order of magnitude as for the
common h-index. In this way one can avoid the disadvantage of the original
g-index, namely that the values are usually substantially larger than for the
h-index and thus the precision problem is substantially larger; while the
advantages of the g-index over the h-index are kept. Like for the generalized
h-index, also for the generalized g-index different prefactors might be more
useful for investigations which concentrate only on top scientists with high
citation frequencies or on junior researchers with small numbers of citations.
"
741,"Entropy-based disciplinarity indicator: role taxonomy of journals in
  scientific communication systems and isolation degree. Knowledge
  importation/exportation profiles from journals and disciplines","  In this research, a new indicator of disciplinarity-multidisciplinarity is
developed, discussed and applied. EBDI is based on the combination of the
frequency distribution of subject categories of journals citing or cited by the
analysis unit and the spread and diversity of the citations among subject
categories measured with Shannon-Wiener entropy. Its reproducibility,
robustness and consistence are discussed. Four of the combinations of its
values when applied to the cited and citing dimensions lead to a suggested
taxonomy of the role that the studied unit might have in terms of the
transformation of knowledge from different disciplines in the scientific
communication system and its position respect a hypothetical thematic core of
the discipline in which it has been classified. The indicator is applied to the
journals belonging to the first quartile of JCR-SSCI 2011 Library and
Information Science and an indicator-based taxonomy is applied and discussed,
pointing to differential thematic roles of the journals analyzed.
"
742,Characterizing scientific production and consumption in Physics,"  We analyze the entire publication database of the American Physical Society
generating longitudinal (50 years) citation networks geolocalized at the level
of single urban areas. We define the knowledge diffusion proxy, and scientific
production ranking algorithms to capture the spatio-temporal dynamics of
Physics knowledge worldwide. By using the knowledge diffusion proxy we identify
the key cities in the production and consumption of knowledge in Physics as a
function of time. The results from the scientific production ranking algorithm
allow us to characterize the top cities for scholarly research in Physics.
Although we focus on a single dataset concerning a specific field, the
methodology presented here opens the path to comparative studies of the
dynamics of knowledge across disciplines and research areas
"
743,"A Case Study of the Arbitrariness of the h-Index and the
  Highly-Cited-Publications Indicator","  The arbitrariness of the h-index becomes evident, when one requires q*h
instead of h citations as the threshold for the definition of the index, thus
changing the size of the core of the most influential publications of a
dataset. I analyze the citation records of 26 physicists in order to determine
how much the prefactor q influences the ranking. Likewise, the arbitrariness of
the highly-cited-publications indicator is due to the threshold value, given
either as an absolute number of citations or as a percentage of highly cited
papers. The analysis of the 26 citation records shows that the changes in the
rankings in dependence on these thresholds are rather large and comparable with
the respective changes for the h-index.
"
744,Tradition and Innovation in Scientists' Research Strategies,"  What factors affect a scientist's choice of research problem? Qualitative
research in the history, philosophy, and sociology of science suggests that
this choice is shaped by an ""essential tension"" between the professional demand
for productivity and a conflicting drive toward risky innovation. We examine
this tension empirically in the context of biomedical chemistry. We use complex
networks to represent the evolving state of scientific knowledge, as expressed
in publications. We then define research strategies relative to these networks.
Scientists can introduce novel chemicals or chemical relationships--or delve
deeper into known ones. They can consolidate existing knowledge clusters, or
bridge distant ones. Analyzing such choices in aggregate, we find that the
distribution of strategies remains remarkably stable, even as chemical
knowledge grows dramatically. High-risk strategies, which explore new chemical
relationships, are less prevalent in the literature, reflecting a growing focus
on established knowledge at the expense of new opportunities. Research
following a risky strategy is more likely to be ignored but also more likely to
achieve high impact and recognition. While the outcome of a risky strategy has
a higher expected reward than the outcome of a conservative strategy, the
additional reward is insufficient to compensate for the additional risk. By
studying the winners of 137 different prizes in biomedicine and chemistry, we
show that the occasional ""gamble"" for extraordinary impact is the most
plausible explanation for observed levels of risk-taking. Our empirical
demonstration and unpacking of the ""essential tension"" suggests policy
interventions that may foster more innovative research.
"
745,A measure for the impact of research,"  The last few years have seen the proliferation of measures that quantify the
scientific output of researches. Yet, these measures focus on productivity,
thus fostering the ""publish or perish"" paradigm. This article proposes a
measure that aims at quantifying the impact of research de-emphasizing
productivity, thus providing scientists an alternative, conceivably fairer,
evaluation of their work. Emphasis is placed on the scientist rather than on
quantities that can grow unbounded. The measure, defined initially for
manuscripts, is then extended to researchers and institutions.
"
746,"Estimating Thematic Similarity of Scholarly Papers with Their Resistance
  Distance in an Electric Network Model","  We calculate resistance distances between papers in a nearly bipartite
citation network of 492 papers and the sources cited by them. We validate that
this is a realistic measure of thematic distance if each citation link has an
electric resistance equal to the geometric mean of the number of the paper's
references and the citation number of the cited source.
"
747,The Semantic Web takes Wing: Programming Ontologies with Tawny-OWL,"  The Tawny-OWL library provides a fully-programmatic environment for ontology
building; it enables the use of a rich set of tools for ontology development,
by recasting development as a form of programming. It is built in Clojure - a
modern Lisp dialect, and is backed by the OWL API. Used simply, it has a
similar syntax to OWL Manchester syntax, but it provides arbitrary
extensibility and abstraction. It builds on existing facilities for Clojure,
which provides a rich and modern programming tool chain, for versioning,
distributed development, build, testing and continuous integration. In this
paper, we describe the library, this environment and the its potential
implications for the ontology development process.
"
748,Bibliometrics for Internet Media: Applying the h-Index to YouTube,"  The h-index can be a useful metric for evaluating a person's output of
Internet media. Here we advocate and demonstrate adaption of the h-index and
the g-index to the top video content creators on YouTube. The h-index for
Internet video media is based on videos and their view counts. The index h is
defined as the number of videos with >= h*10^5 views. The index g is defined as
the number of videos with >= g*10^5 views on average. When compared to a video
creator's total view count, the h-index and g-index better capture both
productivity and impact in a single metric.
"
749,Information Transfer in the Agricultural Sector in Spain,"  This article examines the structures of information transfer to the
agricultural (production) and agro-alimentary (transformation and
commercialization of the products) sector within Spain. An historical
perspective is provided to better illustrate the reality and complexity of
Spain with regard to the systems of agrarian extension, agricultural research,
the resources provided by Spain's central administration, and the use of
information by related enterprises.
  The Service of Agrarian Extension appeared in Spain in the 1950s, and new
political-administrative structures (agribusiness associations, cooperatives)
were founded when Spain became a democratic nation in the late 1970s and with
the arrival of electronic information, largely in the 1990s. We describe the
research and technological centers supporting innovation in the agro-alimentary
sector and the communication media dedicated to the agricultural sector. The
article illustrates that the systems of agricultural information in Spain have
been largely derived from initiatives of the Public Administration, with few
private initiatives.
"
750,Broadening the Scope of Nanopublications,"  In this paper, we present an approach for extending the existing concept of
nanopublications --- tiny entities of scientific results in RDF representation
--- to broaden their application range. The proposed extension uses English
sentences to represent informal and underspecified scientific claims. These
sentences follow a syntactic and semantic scheme that we call AIDA (Atomic,
Independent, Declarative, Absolute), which provides a uniform and succinct
representation of scientific assertions. Such AIDA nanopublications are
compatible with the existing nanopublication concept and enjoy most of its
advantages such as information sharing, interlinking of scientific findings,
and detailed attribution, while being more flexible and applicable to a much
wider range of scientific results. We show that users are able to create AIDA
sentences for given scientific results quickly and at high quality, and that it
is feasible to automatically extract and interlink AIDA nanopublications from
existing unstructured data sources. To demonstrate our approach, a web-based
interface is introduced, which also exemplifies the use of nanopublications for
non-scientific content, including meta-nanopublications that describe other
nanopublications.
"
751,FindZebra: A search engine for rare diseases,"  Background: The web has become a primary information resource about illnesses
and treatments for both medical and non-medical users. Standard web search is
by far the most common interface for such information. It is therefore of
interest to find out how well web search engines work for diagnostic queries
and what factors contribute to successes and failures. Among diseases, rare (or
orphan) diseases represent an especially challenging and thus interesting class
to diagnose as each is rare, diverse in symptoms and usually has scattered
resources associated with it. Methods: We use an evaluation approach for web
search engines for rare disease diagnosis which includes 56 real life
diagnostic cases, state-of-the-art evaluation measures, and curated information
resources. In addition, we introduce FindZebra, a specialized (vertical) rare
disease search engine. FindZebra is powered by open source search technology
and uses curated freely available online medical information. Results:
FindZebra outperforms Google Search in both default setup and customised to the
resources used by FindZebra. We extend FindZebra with specialized
functionalities exploiting medical ontological information and UMLS medical
concepts to demonstrate different ways of displaying the retrieved results to
medical experts. Conclusions: Our results indicate that a specialized search
engine can improve the diagnostic quality without compromising the ease of use
of the currently widely popular web search engines. The proposed evaluation
approach can be valuable for future development and benchmarking. The FindZebra
search engine is available at http://www.findzebra.com/.
"
752,"F1000 recommendations as a new data source for research evaluation: A
  comparison with citations","  F1000 is a post-publication peer review service for biological and medical
research. F1000 aims to recommend important publications in the biomedical
literature, and from this perspective F1000 could be an interesting tool for
research evaluation. By linking the complete database of F1000 recommendations
to the Web of Science bibliographic database, we are able to make a
comprehensive comparison between F1000 recommendations and citations. We find
that about 2% of the publications in the biomedical literature receive at least
one F1000 recommendation. Recommended publications on average receive 1.30
recommendations, and over 90% of the recommendations are given within half a
year after a publication has appeared. There turns out to be a clear
correlation between F1000 recommendations and citations. However, the
correlation is relatively weak, at least weaker than the correlation between
journal impact and citations. More research is needed to identify the main
reasons for differences between recommendations and citations in assessing the
impact of publications.
"
753,"Group-Based Trajectory Modeling of Citations in Scholarly Literature:
  Dynamic Qualities of ""Transient"" and ""Sticky Knowledge Claims""","  Group-based Trajectory Modeling (GBTM) is applied to the citation curves of
articles in six journals and to all citable items in a single field of science
(Virology, 24 journals), in order to distinguish among the developmental
trajectories in subpopulations. Can highly-cited citation patterns be
distinguished in an early phase as ""fast-breaking"" papers? Can ""late bloomers""
or ""sleeping beauties"" be identified? Most interesting, we find differences
between ""sticky knowledge claims"" that continue to be cited more than ten years
after publication, and ""transient knowledge claims"" that show a decay pattern
after reaching a peak within a few years. Only papers following the trajectory
of a ""sticky knowledge claim"" can be expected to have a sustained impact. These
findings raise questions about indicators of ""excellence"" that use aggregated
citation rates after two or three years (e.g., impact factors). Because
aggregated citation curves can also be composites of the two patterns,
5th-order polynomials (with four bending points) are needed to capture citation
curves precisely. For the journals under study, the most frequently cited
groups were furthermore much smaller than ten percent. Although GBTM has proved
a useful method for investigating differences among citation trajectories, the
methodology does not enable us to define a percentage of highly-cited papers
inductively across different fields and journals. Using multinomial logistic
regression, we conclude that predictor variables such as journal names, number
of authors, etc., do not affect the stickiness of knowledge claims in terms of
citations, but only the levels of aggregated citations (that are
field-specific).
"
754,"Strategic Intelligence on Emerging Technologies: Scientometric Overlay
  Mapping","  This paper examines the use of scientometric overlay mapping as a tool of
'strategic intelligence' to aid the governance of emerging technologies. We
develop an integrative synthesis of different overlay mapping techniques and
associated perspectives on technological emergence across the geographical,
social, and cognitive spaces. To do so, we longitudinally analyse (with
publication and patent data) three case-studies of emerging technologies in the
medical domain. These are: RNA interference (RNAi), Human Papilloma Virus (HPV)
testing technologies for cervical cancer, and Thiopurine Methyltransferase
(TPMT) genetic testing. Given the flexibility (i.e. adaptability to different
sources of data) and granularity (i.e. applicability across multiple levels of
data aggregation) of overlay mapping techniques, we argue that these techniques
can favour the integration and comparison of results from different contexts
and cases, thus potentially functioning as platform for a 'distributed'
strategic intelligence for analysts and decision-makers.
"
755,"Business information through Spain's Chambers of Commerce: meeting
  business needs","  From different public and private instances, mechanisms have been set in
action that allow for companies to obtain information in order to make
decisions with a stronger foundation. This article is focused on the
description of an entire information system for the business world, developed
in the realm of the Chambers of Commerce of Spain, which have given rise to the
creation of an authentic network of inter-chamber information.
  In Spain, the obligatory membership of businesses to the Chambers of Commerce
in their geographic areas, and therefore the compulsory payment of member
quotas, has traditionally generated some polemics, above all because many firms
have not perceived a material usefulness of the services offered by these
Chambers.
  Notwithstanding, the 85 Chambers currently existing in Spain, as well as the
organism that coordinates them -the Upper Council or Consejo Superior de
Camaras de Comercio- and the company created expressly to commercialize
informational services online, Camerdata, have developed genuinely informative
tools that cover a good part of the informational demands that a business might
claim, described here.
"
756,Web Maps and Their Algebra,"  A map is an abstract visual representation of a region, taken from a given
space, usually designed for final human consumption. Traditional cartography
focuses on the mapping of Euclidean spaces by using some distance metric. In
this paper we aim at mapping the Web space by leveraging its relational nature.
We introduce a general mathematical framework for maps and an algebra and
discuss the feasibility of maps suitable for interpretation not only by humans
but also by machines.
"
757,"Taming the zoo - about algorithms implementation in the ecosystem of
  Apache Hadoop","  Content Analysis System (CoAnSys) is a research framework for mining
scientific publications using Apache Hadoop. This article describes the
algorithms currently implemented in CoAnSys including classification,
categorization and citation matching of scientific publications. The size of
the input data classifies these algorithms in the range of big data problems,
which can be efficiently solved on Hadoop clusters.
"
758,Do scientists trace hot topics?,"  Do scientists follow hot topics in their scientific investigations? In this
paper, by performing analysis to papers published in the American Physical
Society (APS) Physical Review journals, it is found that papers are more likely
to be attracted by hot fields, where the hotness of a field is measured by the
number of papers belonging to the field. This indicates that scientists
generally do follow hot topics. However, there are qualitative differences
among scientists from various countries, among research works regarding
different number of authors, different number of affiliations and different
number of references. These observations could be valuable for policy makers
when deciding research funding and also for individual researchers when
searching for scientific projects.
"
759,"Ranking journals: Could Google Scholar Metrics be an alternative to
  Journal Citation Reports and Scimago Journal Rank?","  The launch of Google Scholar Metrics as a tool for assessing scientific
journals may be serious competition for Thomson Reuters Journal Citation
Reports, and for Scopus powered Scimago Journal Rank. A review of these
bibliometric journal evaluation products is performed. We compare their main
characteristics from different approaches: coverage, indexing policies, search
and visualization, bibliometric indicators, results analysis options, economic
cost and differences in their ranking of journals. Despite its shortcomings,
Google Scholar Metrics is a helpful tool for authors and editors in identifying
core journals. As an increasingly useful tool for ranking scientific journals,
it may also challenge established journals products
"
760,Large scale citation matching using Apache Hadoop,"  During the process of citation matching links from bibliography entries to
referenced publications are created. Such links are indicators of topical
similarity between linked texts, are used in assessing the impact of the
referenced document and improve navigation in the user interfaces of digital
libraries. In this paper we present a citation matching method and show how to
scale it up to handle great amounts of data using appropriate indexing and a
MapReduce paradigm in the Hadoop environment.
"
761,"Usage-based vs. Citation-based Methods for Recommending Scholarly
  Research Articles","  There are two principal data sources for collaborative filtering recommenders
in scholarly digital libraries: usage data obtained from harvesting a large,
distributed collection of Open URL web logs and citation data obtained from the
journal articles. This study explores the characteristics of recommendations
generated by implementations of these two methods: the 'bX' system by ExLibris
and an experimental citation-based recommender, Sarkanto. Recommendations from
each system were compared according to their semantic similarity to the seed
article that was used to generate them. Since the full text of the articles was
not available for all the recommendations in both systems, the semantic
similarity between the seed article and the recommended articles was deemed to
be the semantic distance between the journals in which the articles were
published. The semantic distance between journals was computed from the
""semantic vectors"" distance between all the terms in the full-text of the
available articles in that journal and this study shows that citation-based
recommendations are more semantically diverse than usage-based ones. These
recommenders are complementary since most of the time, when one recommender
produces recommendations the other does not.
"
762,Reputation and Impact in Academic Careers,"  Reputation is an important social construct in science, which enables
informed quality assessments of both publications and careers of scientists in
the absence of complete systemic information. However, the relation between
reputation and career growth of an individual remains poorly understood,
despite recent proliferation of quantitative research evaluation methods. Here
we develop an original framework for measuring how a publication's citation
rate $\Delta c$ depends on the reputation of its central author $i$, in
addition to its net citation count $c$. To estimate the strength of the
reputation effect, we perform a longitudinal analysis on the careers of 450
highly-cited scientists, using the total citations $C_{i}$ of each scientist as
his/her reputation measure. We find a citation crossover $c_{\times}$ which
distinguishes the strength of the reputation effect. For publications with $c <
c_{\times}$, the author's reputation is found to dominate the annual citation
rate. Hence, a new publication may gain a significant early advantage
corresponding to roughly a 66% increase in the citation rate for each tenfold
increase in $C_{i}$. However, the reputation effect becomes negligible for
highly cited publications meaning that for $c\geq c_{\times}$ the citation rate
measures scientific impact more transparently. In addition we have developed a
stochastic reputation model, which is found to reproduce numerous statistical
observations for real careers, thus providing insight into the microscopic
mechanisms underlying cumulative advantage in science.
"
763,Coauthorship and citation in scientific publishing,"  A large number of published studies have examined the properties of either
networks of citation among scientific papers or networks of coauthorship among
scientists. Here, using an extensive data set covering more than a century of
physics papers published in the Physical Review, we study a hybrid
coauthorship/citation network that combines the two, which we analyze to gain
insight into the correlations and interactions between authorship and citation.
Among other things, we investigate the extent to which individuals tend to cite
themselves or their collaborators more than others, the extent to which they
cite themselves or their collaborators more quickly after publication, and the
extent to which they tend to return the favor of a citation from another
scientist.
"
764,The case for caution in predicting scientists' future impact,"  We stress-test the career predictability model proposed by Acuna et al.
[Nature 489, 201-202 2012] by applying their model to a longitudinal career
data set of 100 Assistant professors in physics, two from each of the top 50
physics departments in the US. The Acuna model claims to predict h(t+\Delta t),
a scientist's h-index \Delta t years into the future, using a linear
combination of 5 cumulative career measures taken at career age t. Here we
investigate how the ""predictability"" depends on the aggregation of career data
across multiple age cohorts. We confirm that the Acuna model does a respectable
job of predicting h(t+\Delta t) up to roughly 6 years into the future when
aggregating all age cohorts together. However, when calculated using subsets of
specific age cohorts (e.g. using data for only t=3), we find that the model's
predictive power significantly decreases, especially when applied to early
career years. For young careers, the model does a much worse job of predicting
future impact, and hence, exposes a serious limitation. The limitation is
particularly concerning as early career decisions make up a significant
portion, if not the majority, of cases where quantitative approaches are likely
to be applied.
"
765,"Collective allocation of science funding: from funding agencies to
  scientific agency","  Public agencies like the U.S. National Science Foundation (NSF) and the
National Institutes of Health (NIH) award tens of billions of dollars in annual
science funding. How can this money be distributed as efficiently as possible
to best promote scientific innovation and productivity? The present system
relies primarily on peer review of project proposals. In 2010 alone, NSF
convened more than 15,000 scientists to review 55,542 proposals. Although
considered the scientific gold standard, peer review requires significant
overhead costs, and may be subject to biases, inconsistencies, and oversights.
We investigate a class of funding models in which all participants receive an
equal portion of yearly funding, but are then required to anonymously donate a
fraction of their funding to peers. The funding thus flows from one participant
to the next, each acting as if he or she were a funding agency themselves. Here
we show through a simulation conducted over large-scale citation data (37M
articles, 770M citations) that such a distributed system for science may yield
funding patterns similar to existing NIH and NSF distributions, but may do so
at much lower overhead while exhibiting a range of other desirable features.
Self-correcting mechanisms in scientific peer evaluation can yield an efficient
and fair distribution of funding. The proposed model can be applied in many
situations in which top-down or bottom-up allocation of public resources is
either impractical or undesirable, e.g. public investments, distribution
chains, and shared resource management.
"
766,"Analysis of bibliometric indicators for individual scholars in a large
  data set","  Citation numbers and other quantities derived from bibliographic databases
are becoming standard tools for the assessment of productivity and impact of
research activities. Though widely used, still their statistical properties
have not been well established so far. This is especially true in the case of
bibliometric indicators aimed at the evaluation of individual scholars, because
large-scale data sets are typically difficult to be retrieved. Here, we take
advantage of a recently introduced large bibliographic data set, Google Scholar
Citations, which collects the entire publication record of individual scholars.
We analyze the scientific profile of more than 30,000 researchers, and study
the relation between the h-index, the number of publications and the number of
citations of individual scientists. While the number of publications of a
scientist has a rather weak relation with his/her h-index, we find that the
h-index of a scientist is strongly correlated with the number of citations that
she/he has received so that the number of citations can be effectively be used
as a proxy of the h-index. Allowing for the h-index to depend on both the
number of citations and the number of publications, we find only a minor
improvement.
"
767,Report on the EuDML external cooperation model,"  One of the main tasks of the European Digital Mathematics Library project was
to define a cooperation model with a variety of stakeholders that would allow
building a reliable and durable global reference library, aiming to be
eventually exhaustive. In this paper we present the EuDML external cooperation
model and the business plan as the basis for its sustainability and further
development.
"
768,Semantic Tagging on Historical Maps,"  Tags assigned by users to shared content can be ambiguous. As a possible
solution, we propose semantic tagging as a collaborative process in which a
user selects and associates Web resources drawn from a knowledge context. We
applied this general technique in the specific context of online historical
maps and allowed users to annotate and tag them. To study the effects of
semantic tagging on tag production, the types and categories of obtained tags,
and user task load, we conducted an in-lab within-subject experiment with 24
participants who annotated and tagged two distinct maps. We found that the
semantic tagging implementation does not affect these parameters, while
providing tagging relationships to well-defined concept definitions. Compared
to label-based tagging, our technique also gathers positive and negative
tagging relationships. We believe that our findings carry implications for
designers who want to adopt semantic tagging in other contexts and systems on
the Web.
"
769,"Google Scholar and the h-index in biomedicine: the popularization of
  bibliometric asessment","  The aim of this paper is to review the features, benefits and limitations of
the new scientific evaluation products derived from Google Scholar; Google
Scholar Metrics and Google Scholar Citations, as well as the h-index which is
the standard bibliometric indicator adopted by these services. It also outlines
the potential of this new database as a source for studies in Biomedicine and
compares the h-index obtained by the most relevant journals and researchers in
the field of Intensive Care Medicine, by means of data extracted from Web of
Science, Scopus and Google Scholar. Results show that, although average h-index
values in Google Scholar are almost 30% higher than those obtained in Web of
Science and about 15% higher than those collected by Scopus, there are no
substantive changes in the rankings generated from either data source. Despite
some technical problems, it is concluded that Google Scholar is a valid tool
for researchers in Health Sciences, both for purposes of information retrieval
and computation of bibliometric indicators
"
770,Relative Positions of Countries in the World of Science,"  A novel picture of the relative positions of countries in the world of
science is offered through application of a two-dimensional mapping method
which is based on quantity and quality indicators of the scientific production
as peer-reviewed articles. To obtain such indicators, different influential
effects such as the background global trends, temporal fluctuations,
disciplinary characteristics, and mainly, the effect of countries resources
have been taken into account. Fifty countries with the highest scientific
production are studied in twelve years (1996-2007). A common clustering
algorithm is used to detect groups of co-evolving countries in the
two-dimensional map, and thereby countries are classified into four major
clusters based on their relative positions in the two-dimensional map. The
final results are in contrast with common views on relative positions of
countries in the world of science, as demonstrated by considering some examples
like USA, China or New Zealand. The proposed method and results thereof might
influence the concept of 'scientific advancement' and the future scientific
orientations of countries.
"
771,Maps of Computer Science,"  We describe a practical approach for visual exploration of research papers.
Specifically, we use the titles of papers from the DBLP database to create what
we call maps of computer science (MoCS). Words and phrases from the paper
titles are the cities in the map, and countries are created based on word and
phrase similarity, calculated using co-occurrence. With the help of heatmaps,
we can visualize the profile of a particular conference or journal over the
base map. Similarly, heatmap profiles can be made of individual researchers or
groups such as a department. The visualization system also makes it possible to
change the data used to generate the base map. For example, a specific journal
or conference can be used to generate the base map and then the heatmap
overlays can be used to show the evolution of research topics in the field over
the years. As before, individual researchers or research groups profiles can be
visualized using heatmap overlays but this time over the journal or conference
base map. Finally, research papers or abstracts easily generate visual
abstracts giving a visual representation of the distribution of topics in the
paper. We outline a modular and extensible system for term extraction using
natural language processing techniques, and show the applicability of methods
of information retrieval to calculation of term similarity and creation of a
topic map. The system is available at mocs.cs.arizona.edu.
"
772,"A Two-Dimensional Approach to Evaluate the Scientific Production of
  Countries (Case Study: The Basic Sciences)","  The quantity and quality of scientific output of the topmost 50 countries in
the four basic sciences (agricultural and biological sciences, chemistry,
mathematics, and physics and astronomy) are studied in the period of the recent
12 years (1996-2007). In order to rank the countries, a novel two-dimensional
method is proposed, which is inspired by the H-index and other methods based on
quality and quantity measures. The countries data are represented in a
""quantity-quality diagram"", and partitioned by a conventional statistical
algorithm (k-means), into three clusters, members of which are rather the same
in all of the basic sciences. The results offer a new perspective on the global
positions of countries with regards to their scientific output.
"
773,The Recomputation Manifesto,"  Replication of scientific experiments is critical to the advance of science.
Unfortunately, the discipline of Computer Science has never treated replication
seriously, even though computers are very good at doing the same thing over and
over again. Not only are experiments rarely replicated, they are rarely even
replicable in a meaningful way. Scientists are being encouraged to make their
source code available, but this is only a small step. Even in the happy event
that source code can be built and run successfully, running code is a long way
away from being able to replicate the experiment that code was used for. I
propose that the discipline of Computer Science must embrace replication of
experiments as standard practice. I propose that the only credible technique to
make experiments truly replicable is to provide copies of virtual machines in
which the experiments are validated to run. I propose that tools and
repositories should be made available to make this happen. I propose to be one
of those who makes it happen.
"
774,Fingerprint databases for theorems,"  We discuss the advantages of searchable, collaborative, language-independent
databases of mathematical results, indexed by ""fingerprints"" of small and
canonical data. Our motivating example is Neil Sloane's massively influential
On-Line Encyclopedia of Integer Sequences. We hope to encourage the greater
mathematical community to search for the appropriate fingerprints within each
discipline, and to compile fingerprint databases of results wherever possible.
The benefits of these databases are broad - advancing the state of knowledge,
enhancing experimental mathematics, enabling researchers to discover unexpected
connections between areas, and even improving the refereeing process for
journal publication.
"
775,"Best-in-class and Strategic Benchmarking of Scientific Subject
  Categories of Web of Science in 2010","  Here we show a novel technique for comparing subject categories, where the
prestige of academic journals in each category is represented statistically by
an impact-factor histogram. For each subject category we compute the
probability of occurrence of scholarly journals with impact factor in different
intervals. Here impact factor is measured with Thomson Reuters Impact Factor,
Eigenfactor Score, and Immediacy Index. Assuming the probabilities associated
with a pair of subject categories our objective is to measure the degree of
dissimilarity between them. To do so, we use an axiomatic characterization for
predicting dissimilarity between subject categories. The scientific subject
categories of Web of Science in 2010 were used to test the proposed approach
for benchmarking Cell Biology and Computer Science Information Systems with the
rest as two case studies. The former is best-in-class benchmarking that
involves studying the leading competitor category; the latter is strategic
benchmarking that involves observing how other scientific subject categories
compete.
"
776,"Assessing Visualization Techniques for the Search Process in Digital
  Libraries","  In this paper we present an overview of several visualization techniques to
support the search process in Digital Libraries (DLs). The search process
typically can be separated into three major phases: query formulation and
refinement, browsing through result lists and viewing and interacting with
documents and their properties. We discuss a selection of popular visualization
techniques that have been developed for the different phases to support the
user during the search process. Along prototypes based on the different
techniques we show how the approaches have been implemented. Although various
visualizations have been developed in prototypical systems very few of these
approaches have been adapted into today's DLs. We conclude that this is most
likely due to the fact that most systems are not evaluated intensely in
real-life scenarios with real information seekers and that results of the
interesting visualization techniques are often not comparable. We can say that
many of the assessed systems did not properly address the information need of
cur-rent users.
"
777,How important tasks are performed: peer review,"  The advancement of various fields of science depends on the actions of
individual scientists via the peer review process. The referees' work patterns
and stochastic nature of decision making both relate to the particular features
of refereeing and to the universal aspects of human behavior. Here, we show
that the time a referee takes to write a report on a scientific manuscript
depends on the final verdict. The data is compared to a model, where the review
takes place in an ongoing competition of completing an important composite task
with a large number of concurrent ones - a Deadline -effect. In peer review
human decision making and task completion combine both long-range
predictability and stochastic variation due to a large degree of ever-changing
external ""friction"".
"
778,"Impact maturity times and citation time windows: The 2-year maximum
  journal impact factor","  Journal metrics are employed for the assessment of scientific scholar
journals from a general bibliometric perspective. In this context, the Thomson
Reuters journal impact factors (JIF) are the citation-based indicators most
used. The 2-year journal impact factor (2-JIF) counts citations to one and two
year old articles, while the 5-year journal impact factor (5-JIF) counts
citations from one to five year old articles. Nevertheless, these indicators
are not comparable among fields of science for two reasons: (i) each field has
a different impact maturity time, and (ii) because of systematic differences in
publication and citation behaviour across disciplines. In fact, the 5-JIF
firstly appeared in the Journal Citation Reports (JCR) in 2007 with the purpose
of making more comparable impacts in fields in which impact matures slowly.
However, there is not an optimal fixed impact maturity time valid for all the
fields. In some of them two years provides a good performance whereas in others
three or more years are necessary. Therefore, there is a problem when comparing
a journal from a field in which impact matures slowly with a journal from a
field in which impact matures rapidly. In this work, we propose the 2-year
maximum journal impact factor (2M-JIF), a new impact indicator that considers
the 2-year rolling citation time window of maximum impact instead of the
previous 2-year time window. Finally, an empirical application comparing 2-JIF,
5-JIF, and 2M-JIF shows that the maximum rolling target window reduces the
between-group variance with respect to the within-group variance in a random
sample of about six hundred journals from eight different fields.
"
779,"Comparing journals from different fields of Science and Social Science
  through a JCR Subject Categories Normalized Impact Factor","  The journal Impact Factor (IF) is not comparable among fields of Science and
Social Science because of systematic differences in publication and citation
behaviour across disciplines. In this work, a decomposing of the field
aggregate impact factor into five normally distributed variables is presented.
Considering these factors, a Principal Component Analysis is employed to find
the sources of the variance in the JCR subject categories of Science and Social
Science. Although publication and citation behaviour differs largely across
disciplines, principal components explain more than 78% of the total variance
and the average number of references per paper is not the primary factor
explaining the variance in impact factors across categories. The Categories
Normalized Impact Factor (CNIF) based on the JCR subject category list is
proposed and compared with the IF. This normalization is achieved by
considering all the indexing categories of each journal. An empirical
application, with one hundred journals in two or more subject categories of
economics and business, shows that the gap between rankings is reduced around
32% in the journals analyzed. This gap is obtained as the maximum distance
among the ranking percentiles from all categories where each journal is
included.
"
780,"Central indexes to the citation distribution: A complement to the
  h-index","  The citation distribution of a researcher shows the impact of their
production and determines the success of their scientific career. However, its
application in scientific evaluation is difficult due to the bi-dimensional
character of the distribution. Some bibliometric indexes that try to synthesize
in a numerical value the principal characteristics of this distribution have
been proposed recently. In contrast with other bibliometric measures, the
biases that the distribution tails provoke, are reduced by the h-index.
However, some limitations in the discrimination among researchers with
different publication habits are presented in this index. This index penalizes
selective researchers, distinguished by the large number of citations received,
as compared to large producers. In this work, two original sets of indexes, the
central area indexes and the central interval indexes, that complement the
h-index to include the central shape of the citation distribution, are proposed
and compared.
"
781,Carbon Dating The Web: Estimating the Age of Web Resources,"  In the course of web research it is often necessary to estimate the creation
datetime for web resources (in the general case, this value can only be
estimated). While it is feasible to manually establish likely datetime values
for small numbers of resources, this becomes infeasible if the collection is
large. We present ""carbon date"", a simple web application that estimates the
creation date for a URI by polling a number of sources of evidence and
returning a machine-readable structure with their respective values. To
establish a likely datetime, we poll bitly for the first time someone shortened
the URI, topsy for the first time someone tweeted the URI, a Memento aggregator
for the first time it appeared in a public web archive, Google's time of last
crawl, and the Last-Modified HTTP response header of the resource itself. We
also examine the backlinks of the URI as reported by Google and apply the same
techniques for the resources that link to the URI. We evaluated our tool on a
gold-standard data set of 1200 URIs in which the creation date was manually
verified. We were able to estimate a creation date for 75.90% of the resources,
with 32.78% having the correct value. Given the different nature of the URIs,
the union of the various methods produces the best results. While the Google
last crawl date and topsy account for nearly 66% of the closest answers,
eliminating the web archives or Last-Modified from the results produces the
largest overall negative impact on the results. The carbon date application is
available for download or use via a webAPI.
"
782,Personalized Academic Research Paper Recommendation System,"  A huge number of academic papers are coming out from a lot of conferences and
journals these days. In these circumstances, most researchers rely on key-based
search or browsing through proceedings of top conferences and journals to find
their related work. To ease this difficulty, we propose a Personalized Academic
Research Paper Recommendation System, which recommends related articles, for
each researcher, that may be interesting to her/him. In this paper, we first
introduce our web crawler to retrieve research papers from the web. Then, we
define similarity between two research papers based on the text similarity
between them. Finally, we propose our recommender system developed using
collaborative filtering methods. Our evaluation results demonstrate that our
system recommends good quality research papers.
"
783,Making Math Searchable in Wikipedia,"  Wikipedia, the world largest encyclopedia contains a lot of knowledge that is
expressed as formulae exclusively. Unfortunately, this knowledge is currently
not fully accessible by intelligent information retrieval systems. This immense
body of knowledge is hidden form value-added services, such as search. In this
paper, we present our MathSearch implementation for Wikipedia that enables
users to perform a combined text and fully unlock the potential benefits.
"
784,"Art History on Wikipedia, a Macroscopic Observation","  How are articles about art historical actors interlinked within Wikipedia?
Lead by this question, we seek an overview on the link structure of a domain
specific subset of Wikipedia articles. We use an established domain-specific
person name authority, the Getty Union List of Artist Names (ULAN), in order to
externally identify relevant actors. Besides containing consistent biographical
person data, this database also provides associative relationships between its
person records, serving as a reference link structure for comparison. As a
first step, we use mappings between the ULAN and English Dbpedia provided by
the Virtual Internet Authority File (VIAF). This way, we are able to identify
18,002 relevant person articles. Examining the link structure between these
resources reveals interesting insight about the high level structure of art
historical knowledge as it is represented on Wikipedia.
"
785,"A Framework for Reproducible, Interactive Research: Application to
  health and social sciences","  The aim of this article is to introduce a reporting framework for
reproducible, interactive research applied to Big Clinical Data, based on open
source technologies. The framework is constituted by the following three axes:
(i) data, (ii) analytical codes and (iii) dissemination. In this paper,
different documentation formats and online repositories are introduced. To
integrate and manage the reproducible contents, we propose the R Language as
the tool of choice. All the information is then published and gathered in a
website for different projects. This framework is free and user friendly and is
proposed to enhance reproducibility of health-science reports.
"
786,"Genericity versus expressivity - an exercise in semantic interoperable
  research information systems for Web Science","  The web does not only enable new forms of science, it also creates new
possibilities to study science and new digital scholarship. This paper brings
together multiple perspectives: from individual researchers seeking the best
options to display their activities and market their skills on the academic job
market; to academic institutions, national funding agencies, and countries
needing to monitor the science system and account for public money spending. We
also address the research interests aimed at better understanding the
self-organising and complex nature of the science system through researcher
tracing, the identification of the emergence of new fields, and knowledge
discovery using large-data mining and non-linear dynamics. In particular this
paper draws attention to the need for standardisation and data interoperability
in the area of research information as an indispensable pre-condition for any
science modelling. We discuss which levels of complexity are needed to provide
a globally, interoperable, and expressive data infrastructure for research
information. With possible dynamic science model applications in mind, we
introduce the need for a ""middle-range"" level of complexity for data
representation and propose a conceptual model for research data based on a core
international ontology with national and local extensions.
"
787,"Mapping EINS -- An exercise in mapping the Network of Excellence in
  Internet Science","  This paper demonstrates the application of bibliometric mapping techniques in
the area of funded research networks. We discuss how science maps can be used
to facilitate communication inside newly formed communities, but also to
account for their activities to funding agencies. We present the mapping of
EINS as case -- an FP7 funded Network of Excellence. Finally, we discuss how
these techniques can be used to serve as knowledge maps for interdisciplinary
working experts.
"
788,Are elite journals declining?,"  Previous work indicates that over the past 20 years, the highest quality work
have been published in an increasingly diverse and larger group of journals. In
this paper we examine whether this diversification has also affected the
handful of elite journals that are traditionally considered to be the best. We
examine citation patterns over the past 40 years of 7 long-standing
traditionally elite journals and 6 journals that have been increasing in
importance over the past 20 years. To be among the top 5% or 1% cited papers,
papers now need about twice as many citations as they did 40 years ago. Since
the late 1980s and early 1990s elite journals have been publishing a decreasing
proportion of these top cited papers. This also applies to the two journals
that are typically considered as the top venues and often used as bibliometric
indicators of ""excellence"", Science and Nature. On the other hand, several new
and established journals are publishing an increasing proportion of most cited
papers. These changes bring new challenges and opportunities for all parties.
Journals can enact policies to increase or maintain their relative position in
the journal hierarchy. Researchers now have the option to publish in more
diverse venues knowing that their work can still reach the same audiences.
Finally, evaluators and administrators need to know that although there will
always be a certain prestige associated with publishing in ""elite"" journals,
journal hierarchies are in constant flux so inclusion of journals into this
group is not permanent.
"
789,"Technical report: Linking the scientific and clinical data with
  KI2NA-LHC","  We introduce a use case and propose a system for data and knowledge
integration in life sciences. In particular, we focus on linking clinical
resources (electronic patient records) with scientific documents and data
(research articles, biomedical ontologies and databases). Our motivation is
two-fold. Firstly, we aim to instantly provide scientific context of particular
patient cases for clinicians in order for them to propose treatments in a more
informed way. Secondly, we want to build a technical infrastructure for
researchers that will allow them to semi-automatically formulate and evaluate
their hypothesis against longitudinal patient data. This paper describes the
proposed system and its typical usage in a broader context of KI2NA, an ongoing
collaboration between the DERI research institute and Fujitsu Laboratories. We
introduce an architecture of the proposed framework called KI2NA-LHC (for
Linked Health Care) and outline the details of its implementation. We also
describe typical usage scenarios and propose a methodology for evaluation of
the whole framework. The main goal of this paper is to introduce our ongoing
work to a broader expert audience. By doing so, we aim to establish an
early-adopter community for our work and elicit feedback we could reflect in
the development of the prototype so that it is better tailored to the
requirements of target users.
"
790,Designing the W3C Open Annotation Data Model,"  The Open Annotation Core Data Model specifies an interoperable framework for
creating associations between related resources, called annotations, using a
methodology that conforms to the Architecture of the World Wide Web. Open
Annotations can easily be shared between platforms, with sufficient richness of
expression to satisfy complex requirements while remaining simple enough to
also allow for the most common use cases, such as attaching a piece of text to
a single web resource. This paper presents the W3C Open Annotation Community
Group specification and the rationale behind the scoping and technical
decisions that were made. It also motivates interoperable Annotations via use
cases, and provides a brief analysis of the advantages over previous
specifications.
"
791,"Interdisciplinarity and research on local issues: evidence from a
  developing country","  This paper explores the relationship between interdisciplinarity and research
pertaining to local issues. Using Colombian publications from 1991 until 2011
in the Web of Science, we investigate the relationship between the degree of
interdisciplinarity and the local orientation of the articles. We find that a
higher degree of interdisciplinarity in a publication is associated with a
greater emphasis on Colombian issues. In particular, our results suggest that
research that combines cognitively disparate disciplines, what we refer to as
distal interdisciplinarity, tends to be associated with more local focus of
research. We discuss the implications of these results in the context of
policies aiming to foster the local socio-economic impact of research in
developing countries.
"
792,Practices in source code sharing in astrophysics,"  While software and algorithms have become increasingly important in
astronomy, the majority of authors who publish computational astronomy research
do not share the source code they develop, making it difficult to replicate and
reuse the work. In this paper we discuss the importance of sharing scientific
source code with the entire astrophysics community, and propose that journals
require authors to make their code publicly available when a paper is
published. That is, we suggest that a paper that involves a computer program
not be accepted for publication unless the source code becomes publicly
available. The adoption of such a policy by editors, editorial boards, and
reviewers will improve the ability to replicate scientific results, and will
also make the computational astronomy methods more available to other
researchers who wish to apply them to their data.
"
793,A bibliometric index based on the complete list of cited publications,"  We propose a new index, the $j$-index, which is defined for an author as the
sum of the square roots of the numbers of citations to each of the author's
publications. The idea behind the $j$-index it to remedy a drawback of the
$h$-index $-$ that the $h$-index does not take into account the full citation
record of a researcher. The square root function is motivated by our desire to
avoid the possible bias that may occur with a simple sum when an author has
several very highly cited papers. We compare the $j$-index to the $h$-index,
the $g$-index and the total citation count for three subject areas using
several association measures.
  Our results indicate that that the association between the $j$-index and the
other indices varies according to the subject area. One explanation of this
variation may be due to the proportion of citations to publications of the
researcher that are in the $h$-core. The $j$-index is {\em not} an $h$-index
variant, and as such is intended to complement rather than necessarily replace
the $h$-index and other bibliometric indicators, thus providing a more complete
picture of a researcher's achievements.
"
794,"Twenty-Five Shades of Greycite: Semantics for referencing and
  preservation","  Semantic publishing can enable richer documents with clearer, computationally
interpretable properties. For this vision to become reality, however, authors
must benefit from this process, so that they are incentivised to add these
semantics. Moreover, the publication process that generates final content must
allow and enable this semantic content. Here we focus on author-led or ""grey""
literature, which uses a convenient and simple publication pipeline. We
describe how we have used metadata in articles to enable richer referencing of
these articles and how we have customised the addition of these semantics to
articles. Finally, we describe how we use the same semantics to aid in digital
preservation and non-repudiability of research articles.
"
795,"PAV ontology: Provenance, Authoring and Versioning","  Provenance is a critical ingredient for establishing trust of published
scientific content. This is true whether we are considering a data set, a
computational workflow, a peer-reviewed publication or a simple scientific
claim with supportive evidence. Existing vocabularies such as DC Terms and the
W3C PROV-O are domain-independent and general-purpose and they allow and
encourage for extensions to cover more specific needs. We identify the specific
need for identifying or distinguishing between the various roles assumed by
agents manipulating digital artifacts, such as author, contributor and curator.
  We present the Provenance, Authoring and Versioning ontology (PAV): a
lightweight ontology for capturing just enough descriptions essential for
tracking the provenance, authoring and versioning of web resources. We argue
that such descriptions are essential for digital scientific content. PAV
distinguishes between contributors, authors and curators of content and
creators of representations in addition to the provenance of originating
resources that have been accessed, transformed and consumed. We explore five
projects (and communities) that have adopted PAV illustrating their usage
through concrete examples. Moreover, we present mappings that show how PAV
extends the PROV-O ontology to support broader interoperability.
  The authors strived to keep PAV lightweight and compact by including only
those terms that have demonstrated to be pragmatically useful in existing
applications, and by recommending terms from existing ontologies when
plausible.
  We analyze and compare PAV with related approaches, namely Provenance
Vocabulary, DC Terms and BIBFRAME. We identify similarities and analyze their
differences with PAV, outlining strengths and weaknesses of our proposed model.
We specify SKOS mappings that align PAV with DC Terms.
"
796,"Coverage and adoption of altmetrics sources in the bibliometric
  community","  Altmetrics, indices based on social media platforms and tools, have recently
emerged as alternative means of measuring scholarly impact. Such indices assume
that scholars in fact populate online social environments, and interact with
scholarly products there. We tested this assumption by examining the use and
coverage of social media environments amongst a sample of bibliometricians. As
expected, coverage varied: 82% of articles published by sampled
bibliometricians were included in Mendeley libraries, while only 28% were
included in CiteULike. Mendeley bookmarking was moderately correlated (.45)
with Scopus citation. Over half of respondents asserted that social media tools
were affecting their professional lives, although uptake of online tools varied
widely. 68% of those surveyed had LinkedIn accounts, while Academia.edu,
Mendeley, and ResearchGate each claimed a fifth of respondents. Nearly half of
those responding had Twitter accounts, which they used both personally and
professionally. Surveyed bibliometricians had mixed opinions on altmetrics'
potential; 72% valued download counts, while a third saw potential in tracking
articles' influence in blogs, Wikipedia, reference managers, and social media.
Altogether, these findings suggest that some online tools are seeing
substantial use by bibliometricians, and that they present a potentially
valuable source of impact data.
"
797,Lobby index as a network centrality measure,"  We study the lobby index (l-index for short) as a local node centrality
measure for complex networks. The l-inde is compared with degree (a local
measure), betweenness and Eigenvector centralities (two global measures) in the
case of biological network (Yeast interaction protein-protein network) and a
linguistic network (Moby Thesaurus II). In both networks, the l-index has poor
correlation with betweenness but correlates with degree and Eigenvector. Being
a local measure, one can take advantage by using the l-index because it carries
more information about its neighbors when compared with degree centrality,
indeed it requires less time to compute when compared with Eigenvector
centrality. Results suggests that l-index produces better results than degree
and Eigenvector measures for ranking purposes, becoming suitable as a tool to
perform this task.
"
798,"Usage History of Scientific Literature: Nature Metrics and Metrics of
  Nature Publications","  In this study, we analyze the dynamic usage history of Nature publications
over time using Nature metrics data. We conduct analysis from two perspectives.
On the one hand, we examine how long it takes before the articles' downloads
reach 50%/80% of the total; on the other hand, we compare the percentage of
total downloads in 7 days, 30 days, and 100 days after publication. In general,
papers are downloaded most frequently within a short time period right after
their publication. And we find that compared with Non-Open Access papers,
readers' attention on Open Access publications are more enduring. Based on the
usage data of a newly published paper, regression analysis could predict the
future expected total usage counts.
"
799,GeoDBLP: Geo-Tagging DBLP for Mining the Sociology of Computer Science,"  Many collective human activities have been shown to exhibit universal
patterns. However, the possibility of universal patterns across timing events
of researcher migration has barely been explored at global scale. Here, we show
that timing events of migration within different countries exhibit remarkable
similarities. Specifically, we look at the distribution governing the data of
researcher migration inferred from the web. Compiling the data in itself
represents a significant advance in the field of quantitative analysis of
migration patterns. Official and commercial records are often access
restricted, incompatible between countries, and especially not registered
across researchers. Instead, we introduce GeoDBLP where we propagate
geographical seed locations retrieved from the web across the DBLP database of
1,080,958 authors and 1,894,758 papers. But perhaps more important is that we
are able to find statistical patterns and create models that explain the
migration of researchers. For instance, we show that the science job market can
be treated as a Poisson process with individual propensities to migrate
following a log-normal distribution over the researcher's career stage. That
is, although jobs enter the market constantly, researchers are generally not
""memoryless"" but have to care greatly about their next move. The propensity to
make k>1 migrations, however, follows a gamma distribution suggesting that
migration at later career stages is ""memoryless"". This aligns well but actually
goes beyond scientometric models typically postulated based on small case
studies. On a very large, transnational scale, we establish the first general
regularities that should have major implications on strategies for education
and research worldwide.
"
800,"Relevance distributions across Bradford Zones: Can Bradfordizing improve
  search?","  The purpose of this paper is to describe the evaluation of the effectiveness
of the bibliometric technique Bradfordizing in an information retrieval (IR)
scenario. Bradfordizing is used to re-rank topical document sets from
conventional abstracting & indexing (A&I) databases into core and more
peripheral document zones. Bradfordized lists of journal articles and
monographs will be tested in a controlled scenario consisting of different A&I
databases from social and political sciences, economics, psychology and medical
science, 164 standardized IR topics and intellectual assessments of the listed
documents. Does Bradfordizing improve the ratio of relevant documents in the
first third (core) compared to the second and last third (zone 2 and zone 3,
respectively)? The IR tests show that relevance distributions after re-ranking
improve at a significant level if documents in the core are compared with
documents in the succeeding zones. After Bradfordizing of document pools, the
core has a significant better average precision than zone 2, zone 3 and
baseline. This paper should be seen as an argument in favour of alternative
non-textual (bibliometric) re-ranking methods which can be simply applied in
text-based retrieval systems and in particular in A&I databases.
"
801,"A Comparison between Two Main Academic Literature Collections: Web of
  Science and Scopus Databases","  Nowadays, the worlds scientific community has been publishing an enormous
number of papers in different scientific fields. In such environment, it is
essential to know which databases are equally efficient and objective for
literature searches. It seems that two most extensive databases are Web of
Science and Scopus. Besides searching the literature, these two databases used
to rank journals in terms of their productivity and the total citations
received to indicate the journals impact, prestige or influence. This article
attempts to provide a comprehensive comparison of these databases to answer
frequent questions which researchers ask, such as: How Web of Science and
Scopus are different? In which aspects these two databases are similar? Or, if
the researchers are forced to choose one of them, which one should they prefer?
For answering these questions, these two databases will be compared based on
their qualitative and quantitative characteristics.
"
802,Does Criticisms Overcome the Praises of Journal Impact Factor?,"  Journal impact factor (IF) as a gauge of influence and impact of a particular
journal comparing with other journals in the same area of research, reports the
mean number of citations to the published articles in particular journal.
Although, IF attracts more attention and being used more frequently than other
measures, it has been subjected to criticisms, which overcome the advantages of
IF. Critically, extensive use of IF may result in destroying editorial and
researchers behaviour, which could compromise the quality of scientific
articles. Therefore, it is the time of the timeliness and importance of a new
invention of journal ranking techniques beyond the journal impact factor.
"
803,The role of twitter in the life cycle of a scientific publication,"  Twitter is a micro-blogging social media platform for short messages that can
have a long-term impact on how scientists create and publish ideas. We
investigate the usefulness of twitter in the development and distribution of
scientific knowledge. At the start of the life cycle of a scientific
publication, twitter provides a large virtual department of colleagues that can
help to rapidly generate, share and refine new ideas. As ideas become
manuscripts, twitter can be used as an informal arena for the pre-review of
works in progress. Finally, tweeting published findings can communicate
research to a broad audience of other researchers, decision makers, journalists
and the general public that can amplify the scientific and social impact of
publications. However, there are limitations, largely surrounding issues of
intellectual property and ownership, inclusiveness and misrepresentations of
science sound bites. Nevertheless, we believe twitter is a useful social media
tool that can provide a valuable contribution to scientific publishing in the
21st century.
"
804,Self-organization of progress across the century of physics,"  We make use of information provided in the titles and abstracts of over half
a million publications that were published by the American Physical Society
during the past 119 years. By identifying all unique words and phrases and
determining their monthly usage patterns, we obtain quantifiable insights into
the trends of physics discovery from the end of the 19th century to today. We
show that the magnitudes of upward and downward trends yield heavy-tailed
distributions, and that their emergence is due to the Matthew effect. This
indicates that both the rise and fall of scientific paradigms is driven by
robust principles of self-organization. Data also confirm that periods of war
decelerate scientific progress, and that the later is very much subject to
globalization.
"
805,"Mathematical practice, crowdsourcing, and social machines","  The highest level of mathematics has traditionally been seen as a solitary
endeavour, to produce a proof for review and acceptance by research peers.
Mathematics is now at a remarkable inflexion point, with new technology
radically extending the power and limits of individuals. Crowdsourcing pulls
together diverse experts to solve problems; symbolic computation tackles huge
routine calculations; and computers check proofs too long and complicated for
humans to comprehend.
  Mathematical practice is an emerging interdisciplinary field which draws on
philosophy and social science to understand how mathematics is produced. Online
mathematical activity provides a novel and rich source of data for empirical
investigation of mathematical practice - for example the community question
answering system {\it mathoverflow} contains around 40,000 mathematical
conversations, and {\it polymath} collaborations provide transcripts of the
process of discovering proofs. Our preliminary investigations have demonstrated
the importance of ""soft"" aspects such as analogy and creativity, alongside
deduction and proof, in the production of mathematics, and have given us new
ways to think about the roles of people and machines in creating new
mathematical knowledge. We discuss further investigation of these resources and
what it might reveal.
  Crowdsourced mathematical activity is an example of a ""social machine"", a new
paradigm, identified by Berners-Lee, for viewing a combination of people and
computers as a single problem-solving entity, and the subject of major
international research endeavours. We outline a future research agenda for
mathematics social machines, a combination of people, computers, and
mathematical archives to create and apply mathematics, with the potential to
change the way people do mathematics, and to transform the reach, pace, and
impact of mathematics research.
"
806,What does mathoverflow tell us about the production of mathematics?,"  The highest level of mathematics research is traditionally seen as a solitary
activity. Yet new innovations by mathematicians themselves are starting to
harness the power of social computation to create new modes of mathematical
production. We study the effectiveness of one such system, and make proposals
for enhancement, drawing on AI and computer based mathematics. We analyse the
content of a sample of questions and responses in the community question
answering system for research mathematicians, math-overflow. We find that
mathoverflow is very effective, with 90% of our sample of questions answered
completely or in part. A typical response is an informal dialogue, allowing
error and speculation, rather than rigorous mathematical argument: 37% of our
sample discussions acknowledged error. Responses typically present information
known to the respondent, and readily checked by other users: thus the
effectiveness of mathoverflow comes from information sharing. We conclude that
extending and the power and reach of mathoverflow through a combination of
people and machines raises new challenges for artificial intelligence and
computational mathematics, in particular how to handle error, analogy and
informal reasoning.
"
807,"An insight into the importance of national university rankings in an
  international context: The case of the I-UGR Rankings of Spanish universities","  The great importance international rankings have achieved in the research
policy arena warns against many threats consequence of the flaws and
shortcomings these tools present. One of them has to do with the inability to
accurately represent national university systems as their original purpose is
only to rank world-class universities. Another one has to do with the lack of
representativeness of universities' disciplinary profiles as they usually
provide a unique table. Although some rankings offer a great coverage and
others offer league tables by fields, no international ranking does both. In
order to surpass such limitation from a research policy viewpoint, this paper
analyzes the possibility of using national rankings in order to complement
international rankings. For this, we analyze the Spanish university system as a
study case presenting the I-UGR Rankings for Spanish universities by fields and
subfields. Then, we compare their results with those obtained by the Shanghai
Ranking, the QS Ranking, the Leiden Ranking and the NTU Ranking, as they all
have basic common grounds which allow such comparison. We conclude that it is
advisable to use national rankings in order to complement international
rankings, however we observe that this must be done with certain caution as
they differ on the methodology employed as well as on the construction of the
fields.
"
808,"Towards an Author-Topic-Term-Model Visualization of 100 Years of German
  Sociological Society Proceedings","  Author co-citation studies employ factor analysis to reduce high-dimensional
co-citation matrices to low-dimensional and possibly interpretable factors, but
these studies do not use any information from the text bodies of publications.
We hypothesise that term frequencies may yield useful information for
scientometric analysis. In our work we ask if word features in combination with
Bayesian analysis allow well-founded science mapping studies. This work goes
back to the roots of Mosteller and Wallace's (1964) statistical text analysis
using word frequency features and a Bayesian inference approach, tough with
different goals. To answer our research question we (i) introduce a new data
set on which the experiments are carried out, (ii) describe the Bayesian model
employed for inference and (iii) present first results of the analysis.
"
809,ResourceSync: Leveraging Sitemaps for Resource Synchronization,"  Many applications need up-to-date copies of collections of changing Web
resources. Such synchronization is currently achieved using ad-hoc or
proprietary solutions. We propose ResourceSync, a general Web resource
synchronization protocol that leverages XML Sitemaps. It provides a set of
capabilities that can be combined in a modular manner to meet local or
community requirements. We report on work to implement this protocol for
arXiv.org and also provide an experimental prototype for the English Wikipedia
as well as a client API.
"
810,"Analyzing the citation characteristics of books: edited books, book
  series and types of publishers in the Book Citation Index","  This paper presents a first approach to analyzing the factors that determine
the citation characteristics of books. For this we use the Thomson Reuters'
Book Citation Index, a novel multidisciplinary database launched in 2010 which
offers bibliometric data of books. We analyze three possible factors which are
considered to affect the citation impact of books: the presence of editors, the
inclusion in series and the type of publisher. Also, we focus on highly cited
books to see if these factors may affect them as well. We considered as highly
cited books, those in the top 5% of the most highly cited ones of the database.
We define these three aspects and we present the results for four major
scientific areas in order to identify field-based differences (Science,
Engineering & Technology, Social Sciences and Arts & Humanities). Finally we
conclude observing that differences were noted for edited books and types of
publishers. Although books included in series showed higher impact in two
areas.
"
811,"Most borrowed is most cited? Library loan statistics as a proxy for
  monograph selection in citation indexes","  This study aims to analyse whether library loans statistics can be used as a
measure of monograph use and as a selection criteria for inclusion in citation
indexes. For this, we conducted an exploratory study based on loan data (1000
most borrowed monographs) from two non-Anglo-Saxon European university
libraries (Granada and Vienna) with strong social sciences and humanities
components. Loans to scientists only were also analysed at the University of
Vienna. Furthermore, citation counts for the 100 most borrowed scientific
monographs (SM) and textbooks or manuals (MTB) were retrieved from Web of
Science and Google Scholar. The results show considerable similarities in both
libraries: the percentage of loans for books in national languages represents
almost 96 per cent of the total share and SM accounts only for 10 to 13 per
cent. When considering loans to scientists only, the percentage of English
books increases to 30 per cent, the percentage of SM loans also increases
(approx 80 per cent). Furthermore, we found no significant correlations between
loans and citations. Since loan statistics are currently insufficient for
measuring the use of monographs, their suggested use as an applicable selection
criterion for book citation indexes is not yet feasible. Data improvement and
aggregation at different levels is a challenge for modern libraries in order to
enable the exploitation of this invaluable information source for scientometric
purposes.
"
812,HTTP Mailbox - Asynchronous RESTful Communication,"  We describe HTTP Mailbox, a mechanism to enable RESTful HTTP communication in
an asynchronous mode with a full range of HTTP methods otherwise unavailable to
standard clients and servers. HTTP Mailbox allows for broadcast and multicast
semantics via HTTP. We evaluate a reference implementation using ApacheBench (a
server stress testing tool) demonstrating high throughput (on 1,000 concurrent
requests) and a systemic error rate of 0.01%. Finally, we demonstrate our HTTP
Mailbox implementation in a human assisted web preservation application called
""Preserve Me"".
"
813,"Ethics of using language editing services in an era of digital
  communication and heavily multiauthored papers","  Scientists of many countries in which English is not the primary language
routinely use a variety of manuscript preparation, correction or editing
services, a practice that is openly endorsed by many journals and scientific
institutions. These services vary tremendously in their scope; at one end there
is simple proof-reading, and at the other extreme there is in-depth and
extensive peer-reviewing, proposal preparation, statistical analyses,
re-writing and co-writing. In this paper, the various types of service are
reviewed, along with authorship guidelines, and the question is raised of
whether the high-end services surpass most guidelines' criteria for authorship.
Three other factors are considered. First, the ease of collaboration possible
in the internet era allows multiple iterations between authors and the editing
service, so essentially, papers can be co-written. Second, 'editing services'
often offer subject-specific experts who comment not only on the language, but
interpret and improve scientific content. Third, the trend towards heavily
multi-authored papers implies that the threshold necessary to earn authorship
is declining. The inevitable conclusion is that at some point the contributions
by 'editing services' should be deemed sufficient to warrant authorship. Trying
to enforce any guidelines would likely be futile, but nevertheless, it might be
time to revisit the ethics of using some of the high-end 'editing services'. In
an increasingly international job market, recognizing this problem might prove
progressively more important in authorship disputes, the allocation of research
grants, and hiring decisions
"
814,"Riding the crest of the altmetrics wave: How librarians can help prepare
  faculty for the next generation of research impact metrics","  As scholars migrate into online spaces like Mendeley, blogs, Twitter, and
more, they leave new traces of once-invisible interactions like reading,
saving, discussing, and recommending. Observing these traces can inform new
metrics of scholarly influence and impact -- so-called ""altmetrics.""
Stakeholders in academia are beginning to discuss how and where altmetrics can
be useful towards evaluating a researcher's academic contribution. As this
interest grows, libraries are in a unique position to help support an informed
dialog on campus. We suggest that librarians can provide this support in three
main ways: informing emerging conversations with the latest research,
supporting experimentation with emerging altmetrics tools, and engaging in
early altmetrics education and outreach. We include examples and lists of
resources to help librarians fill these roles.
"
815,"Micropublications: a Semantic Model for Claims, Evidence, Arguments and
  Annotations in Biomedical Communications","  The Micropublications semantic model for scientific claims, evidence,
argumentation and annotation in biomedical publications, is a metadata model of
scientific argumentation, designed to support several key requirements for
exchange and value-addition of semantic metadata across the biomedical
publications ecosystem.
  Micropublications allow formalizing the argument structure of scientific
publications so that (a) their internal structure is semantically clear and
computable; (b) citation networks can be easily constructed across large
corpora; (c) statements can be formalized in multiple useful abstraction
models; (d) statements in one work may cite statements in another,
individually; (e) support, similarity and challenge of assertions can be
modelled across corpora; (f) scientific assertions, particularly in review
articles, may be transitively closed to supporting evidence and methods.
  The model supports natural language statements; data; methods and materials
specifications; discussion and commentary; as well as challenge and
disagreement. A detailed analysis of nine use cases is provided, along with an
implementation in OWL 2 and SWRL, with several example instantiations in RDF.
"
816,Data Quality Principles in the Semantic Web,"  The increasing size and availability of web data make data quality a core
challenge in many applications. Principles of data quality are recognized as
essential to ensure that data fit for their intended use in operations,
decision-making, and planning. However, with the rise of the Semantic Web, new
data quality issues appear and require deeper consideration. In this paper, we
propose to extend the data quality principles to the context of Semantic Web.
Based on our extensive industrial experience in data integration, we identify
five main classes suited for data quality in Semantic Web. For each class, we
list the principles that are involved at all stages of the data management
process. Following these principles will provide a sound basis for better
decision-making within organizations and will maximize long-term data
integration and interoperability.
"
817,"International Co-authorship Relations in the Social Science Citation
  Index: Is Internationalization Leading the Network?","  We analyze international co-authorship relations in the Social Science
Citation Index 2011 using all citable items in the DVD-version of this index.
Network statistics indicate four groups of nations: (i) an Asian-Pacific one to
which all Anglo-Saxon nations (including the UK and Ireland) are attributed;
(ii) a continental European one including also the Latin-American countries;
(iii) the Scandinavian nations; and (iv) a community of African nations. Within
the EU-28 (including Croatia), eleven of the EU-15 states have dominant
positions. Collapsing the EU-28 into a single node leads to a bi-polar
structure between the US and EU-28; China is part of the US-pole. We develop an
information-theoretical test to distinguish whether international
collaborations or domestic collaborations prevail; the results are mixed, but
the international dimension is more important than the national one in the
aggregated sets (this was found in both SSCI and SCI). In France, however, the
national distribution is more important than the international one, while the
reverse is true for most European nations in the core group (UK, Germany, the
Netherlands, etc.). Decomposition of the USA in terms of states shows a
similarly mixed result; more US states are domestically oriented in SSCI,
whereas more internationally in SCI. The international networks have grown
during the last decades in addition to the national ones, but not by replacing
them.
"
818,"SHARE: A Web Service Based Framework for Distributed Querying and
  Reasoning on the Semantic Web","  Here we describe the SHARE system, a web service based framework for
distributed querying and reasoning on the semantic web. The main innovations of
SHARE are: (1) the extension of a SPARQL query engine to perform on-demand data
retrieval from web services, and (2) the extension of an OWL reasoner to test
property restrictions by means of web service invocations. In addition to
enabling queries across distributed datasets, the system allows for a target
dataset that is significantly larger than is possible under current,
centralized approaches. Although the architecture is equally applicable to all
types of data, the SHARE system targets bioinformatics, due to the large number
of interoperable web services that are already available in this area. SHARE is
built entirely on semantic web standards, and is the successor of the BioMOBY
project.
"
819,Extending Sitemaps for ResourceSync,"  The documents used in the ResourceSync synchronization framework are based on
the widely adopted document format defined by the Sitemap protocol. In order to
address requirements of the framework, extensions to the Sitemap format were
necessary. This short paper describes the concerns we had about introducing
such extensions, the tests we did to evaluate their validity, and aspects of
the framework to address them.
"
820,Investigating Deletion in Wikipedia,"  Several hundred Wikipedia articles are deleted every day because they lack
sufficient significance to be included in the encyclopedia. We collect a
dataset of deleted articles and analyze them to determine whether or not the
deletions were justified. We find evidence to support the hypothesis that many
deletions are carried out correctly, but also find that a large number were
done very quickly. Based on our conclusions, we make some recommendations to
reduce the number of non-significant pages and simultaneously improve retention
of new editors.
"
821,"The most controversial topics in Wikipedia: A multilingual and
  geographical analysis","  We present, visualize and analyse the similarities and differences between
the controversial topics related to ""edit wars"" identified in 10 different
language versions of Wikipedia. After a brief review of the related work we
describe the methods developed to locate, measure, and categorize the
controversial topics in the different languages. Visualizations of the degree
of overlap between the top 100 lists of most controversial articles in
different languages and the content related to geographical locations will be
presented. We discuss what the presented analysis and visualizations can tell
us about the multicultural aspects of Wikipedia and practices of
peer-production. Our results indicate that Wikipedia is more than just an
encyclopaedia; it is also a window into convergent and divergent social-spatial
priorities, interests and preferences.
"
822,"Math-Net.Ru as a Digital Archive of the Russian Mathematical Knowledge
  from the XIX Century to Today","  The main goal of the project Math-Net.Ru is to collect scientific
publications in Russian and Soviet mathematics journals since their foundation
to today and the authors of these publications into a single database and to
provide access to full-text articles for broad international mathematical
community. Leading Russian mathematics journals have been comprehensively
digitized dating back to the first volumes.
"
823,Formal Mathematics on Display: A Wiki for Flyspeck,"  The Agora system is a prototype ""Wiki for Formal Mathematics"", with an aim to
support developing and documenting large formalizations of mathematics in a
proof assistant. The functions implemented in Agora include in-browser editing,
strong AI/ATP proof advice, verification, and HTML rendering. The HTML
rendering contains hyperlinks and provides on-demand explanation of the proof
state for each proof step. In the present paper we show the prototype Flyspeck
Wiki as an instance of Agora for HOL Light formalizations. The wiki can be used
for formalizations of mathematics and for writing informal wiki pages about
mathematics. Such informal pages may contain islands of formal text, which is
used here for providing an initial cross-linking between Hales's informal
Flyspeck book, and the formal Flyspeck development.
  The Agora platform intends to address distributed wiki-style collaboration on
large formalization projects, in particular both the aspect of immediate
editing, verification and rendering of formal code, and the aspect of gradual
and mutual refactoring and correspondence of the initial informal text and its
formalization. Here, we highlight these features within the Flyspeck Wiki.
"
824,Escaping the Trap of too Precise Topic Queries,"  At the very center of digital mathematics libraries lie controlled
vocabularies which qualify the {\it topic} of the documents. These topics are
used when submitting a document to a digital mathematics library and to perform
searches in a library. The latter are refined by the use of these topics as
they allow a precise classification of the mathematics area this document
addresses. However, there is a major risk that users employ too precise topics
to specify their queries: they may be employing a topic that is only ""close-by""
but missing to match the right resource. We call this the {\it topic trap}.
Indeed, since 2009, this issue has appeared frequently on the i2geo.net
platform. Other mathematics portals experience the same phenomenon. An approach
to solve this issue is to introduce tolerance in the way queries are understood
by the user. In particular, the approach of including fuzzy matches but this
introduces noise which may prevent the user of understanding the function of
the search engine.
  In this paper, we propose a way to escape the topic trap by employing the
navigation between related topics and the count of search results for each
topic. This supports the user in that search for close-by topics is a click
away from a previous search. This approach was realized with the i2geo search
engine and is described in detail where the relation of being {\it related} is
computed by employing textual analysis of the definitions of the concepts
fetched from the Wikipedia encyclopedia.
"
825,New Index for Quantifying an Individual's Scientific Research Output,"  Classifying researchers according to the quality of their published work
rather than the quantity is a curtail issue. We attempt to introduce a new
formula of the percentage range to be used for evaluating qualitatively the
researchers' production. The suggested equation depends on the number of the
single-author published papers and their citations to be added as a new factor
to the known h-index. These factors give an advantage and make a clear evidence
of innovative authors and reduce the known h-index for authors who are gaining
citations by adding their names to multi-author papers. It is shown that
various dimensions of ethical integrity and originality will be effective in
this new index. An important scenario arising from the analysis is shown in
terms of examples. It refers to larger differences between the h- and the new
index which comes from the whole work and the one comes from the single-author
papers only, is shown.
"
826,"Comparison of a citation-based indicator and peer review for absolute
  and specific measures of research-group excellence","  Many different measures are used to assess academic research excellence and
these are subject to ongoing discussion and debate within the scientometric,
university-management and policy-making communities internationally. One topic
of continued importance is the extent to which citation-based indicators
compare with peer-review-based evaluation. Here we analyse the correlations
between values of a particular citation-based impact indicator and peer-review
scores in several academic disciplines, from natural to social sciences and
humanities. We perform the comparison for research groups rather than for
individuals. We make comparisons on two levels. At an absolute level, we
compare total impact and overall strength of the group as a whole. At a
specific level, we compare academic impact and quality, normalised by the size
of the group. We find very high correlations at the former level for some
disciplines and poor correlations at the latter level for all disciplines. This
means that, although the citation-based scores could help to describe
research-group strength, in particular for the so-called hard sciences, they
should not be used as a proxy for ranking or comparison of research groups.
Moreover, the correlation between peer-evaluated and citation-based scores is
weaker for soft sciences.
"
827,Universality of scholarly impact metrics,"  Given the growing use of impact metrics in the evaluation of scholars,
journals, academic institutions, and even countries, there is a critical need
for means to compare scientific impact across disciplinary boundaries.
Unfortunately, citation-based metrics are strongly biased by diverse field
sizes and publication and citation practices. As a result, we have witnessed an
explosion in the number of newly proposed metrics that claim to be ""universal.""
However, there is currently no way to objectively assess whether a normalized
metric can actually compensate for disciplinary bias. We introduce a new method
to assess the universality of any scholarly impact metric, and apply it to
evaluate a number of established metrics. We also define a very simple new
metric hs, which proves to be universal, thus allowing to compare the impact of
scholars across scientific disciplines. These results move us closer to a
formal methodology in the measure of scholarly impact.
"
828,Ovopub: Modular data publication with minimal provenance,"  With the growth of the Semantic Web as a medium for creating, consuming,
mashing up and republishing data, our ability to trace any statement(s) back to
their origin is becoming ever more important. Several approaches have now been
proposed to associate statements with provenance, with multiple applications in
data publication, attribution and argumentation. Here, we describe the ovopub,
a modular model for data publication that enables encapsulation, aggregation,
integrity checking, and selective-source query answering. We describe the
ovopub RDF specification, key design patterns and their application in the
publication and referral to data in the life sciences.
"
829,"For a Semantic Web based Peer-reviewing and Publication of Research
  Results","  This article shows why the diffusion and peer-reviewing of research results
would be more efficient, precise and relevant if all or at least some parts of
the descriptions and peer-reviews of research results took the form of a
fine-grained semantic network, within articles or knowledge bases, as part of
the Semantic Web. This article also shows some ways this can be done and hence
how research journal/proceeding publishers could allow this. So far, the World
Wide Web Consortium (W3C) has not proposed simple notations and cooperation
protocols - similar to those illustrated or referred to in this article - but
it now seems likely that Wikipedia/Wikidata, Google or the W3C will propose
them sooner or later. Then, research journal/proceeding publishers and
researchers may or may not quickly use this approach.
"
830,Organizing Linked Data Quality Related Methods,"  This article presents the top-level of an ontology categorizing and
generalizing best practices and quality criteria or measures for Linked Data.
It permits to compare these techniques and have a synthetic organized view of
what can or should be done for knowledge sharing purposes. This ontology is
part of a general knowledge base that can be accessed and complemented by any
Web user. Thus, it can be seen as a cooperatively built library for the above
cited elements. Since they permit to evaluate information objects and create
better ones, these elements also permit knowledge-based tools and techniques -
as well as knowledge providers - to be evaluated and categorized based on their
input/output information objects. One top-level distinction permitting to
organize this ontology is the one between content, medium and containers of
descriptions. Various structural, ontological, syntactical and lexical
distinctions are then used.
"
831,"A Focused Crawler Combinatory Link and Content Model Based on T-Graph
  Principles","  The two significant tasks of a focused Web crawler are finding relevant
topic-specific documents on the Web and analytically prioritizing them for
later effective and reliable download. For the first task, we propose a
sophisticated custom algorithm to fetch and analyze the most effective HTML
structural elements of the page as well as the topical boundary and anchor text
of each unvisited link, based on which the topical focus of an unvisited page
can be predicted and elicited with a high accuracy. Thus, our novel method
uniquely combines both link-based and content-based approaches. For the second
task, we propose a scoring function of the relevant URLs through the use of
T-Graph (Treasure Graph) to assist in prioritizing the unvisited links that
will later be put into the fetching queue. Our Web search system is called the
Treasure-Crawler. This research paper embodies the architectural design of the
Treasure-Crawler system which satisfies the principle requirements of a focused
Web crawler, and asserts the correctness of the system structure including all
its modules through illustrations and by the test results.
"
832,"A hybrid approach for semantic enrichment of MathML mathematical
  expressions","  In this paper, we present a new approach to the semantic enrichment of
mathematical expression problem. Our approach is a combination of statistical
machine translation and disambiguation which makes use of surrounding text of
the mathematical expressions. We first use Support Vector Machine classifier to
disambiguate mathematical terms using both their presentation form and
surrounding text. We then use the disambiguation result to enhance the semantic
enrichment of a statistical-machine-translation-based system. Experimental
results show that our system archives improvements over prior systems.
"
833,"Calibrated Fair Measures of Measure: Indices to Quantify an Individual's
  Scientific Research Output","  Are existing ways of measuring scientific quality reflecting disadvantages of
not being part of giant collaborations? How could possible discrimination be
avoided? We propose indices defined for each discipline (subfield) and which
count the plausible contributions added up by collaborators maintaining the
spirit of interdependency. Based on the growing debate about defining potential
biases and detecting unethical behavior, a standardized method to measure
contributions of the astronomical number of coauthors is introduced.
"
834,"Analysis and Evaluation of the Link and Content Based Focused
  Treasure-Crawler","  Indexing the Web is becoming a laborious task for search engines as the Web
exponentially grows in size and distribution. Presently, the most effective
known approach to overcome this problem is the use of focused crawlers. A
focused crawler applies a proper algorithm in order to detect the pages on the
Web that relate to its topic of interest. For this purpose we proposed a custom
method that uses specific HTML elements of a page to predict the topical focus
of all the pages that have an unvisited link within the current page. These
recognized on-topic pages have to be sorted later based on their relevance to
the main topic of the crawler for further actual downloads. In the
Treasure-Crawler, we use a hierarchical structure called the T-Graph which is
an exemplary guide to assign appropriate priority score to each unvisited link.
These URLs will later be downloaded based on this priority. This paper outlines
the architectural design and embodies the implementation, test results and
performance evaluation of the Treasure-Crawler system. The Treasure-Crawler is
evaluated in terms of information retrieval criteria such as recall and
precision, both with values close to 0.5. Gaining such outcome asserts the
significance of the proposed approach.
"
835,On the Predictability of Future Impact in Science,"  Correctly assessing a scientist's past research impact and potential for
future impact is key in recruitment decisions and other evaluation processes.
While a candidate's future impact is the main concern for these decisions, most
measures only quantify the impact of previous work. Recently, it has been
argued that linear regression models are capable of predicting a scientist's
future impact. By applying that future impact model to 762 careers drawn from
three disciplines: physics, biology, and mathematics, we identify a number of
subtle, but critical, flaws in current models. Specifically, cumulative
non-decreasing measures like the h-index contain intrinsic autocorrelation,
resulting in significant overestimation of their ""predictive power"". Moreover,
the predictive power of these models depend heavily upon scientists' career
age, producing least accurate estimates for young researchers. Our results
place in doubt the suitability of such models, and indicate further
investigation is required before they can be used in recruiting decisions.
"
836,Subfield Effects on the Core of Coauthors,"  It is examined whether the number ($J$) of (joint) publications of a ""main
scientist"" with her/his coauthors ranked according to rank ($r$) importance,
i.e. $ J \propto 1/r $, as found by Ausloos [1] still holds for subfields, i.e.
when the ""main scientist"" has worked on different, sometimes overlapping,
subfields. Two cases are studied. It is shown that the law holds for large
subfields. As shown, in an Appendix, is also useful to combine small topics
into large ones for better statistics. It is observed that the sub-cores are
much smaller than the overall coauthor core measure. Nevertheless, the
smallness of the core and sub-cores may imply further considerations for the
evaluation of team research purposes and activities.
"
837,"Does it Matter Which Citation Tool is Used to Compare the h-index of a
  Group of Highly Cited Researchers?","  h-index retrieved by citation indexes (Scopus, Google scholar, and Web of
Science) is used to measure the scientific performance and the research impact
studies based on the number of publications and citations of a scientist. It
also is easily available and may be used for performance measures of
scientists, and for recruitment decisions. The aim of this study is to
investigate the difference between the outputs and results from these three
citation databases namely Scopus, Google Scholar, and Web of Science based upon
the h-index of a group of highly cited researchers (Nobel Prize winner
scientist). The purposive sampling method was adopted to collect the required
data. The results showed that there is a significant difference in the h-index
between three citation indexes of Scopus, Google scholar, and Web of Science;
the Google scholar h-index was more than the h-index in two other databases. It
was also concluded that there is a significant positive relationship between
h-indices based on Google scholar and Scopus. The citation indexes of Scopus,
Google scholar, and Web of Science may be useful for evaluating h-index of
scientists but they have some limitations as well.
"
838,"Big Pharma, little science? A bibliometric perspective on big pharma's
  R&D decline","  There is a widespread perception that pharmaceutical R&D is facing a
productivity crisis characterised by stagnation in the numbers of new drug
approvals in the face of increasing R&D costs. This study explores
pharmaceutical R&D dynamics by examining the publication activities of all R&D
laboratories of the major European and US pharmaceutical firms during the
period 1995-2009. The empirical findings present an industry in
transformation.In the first place, we observe a decline of the total number of
publications by large firms. Second, we show a relative increase of their
external collaborations suggesting a tendency to outsource, and a
diversification of the disciplinary base, in particular towards computation,
health services and more clinical approaches. Also evident is a more pronounced
decline in publications by both R&D laboratories located in Europe and by firms
with European headquarters. Finally, while publications by Big Pharma in
emerging economies sharply increase, they remain extremely low compared with
those in developed countries. In summary, the trend in this transformation is
one of a gradual decrease in internal research efforts and increasing reliance
on external research. These empirical insights support the view that large
pharmaceutical firms are increasingly becoming networks integrators rather than
the prime locus of drug discovery.
"
839,swMATH - a new information service for mathematical software,"  An information service for mathematical software is presented. Publications
and software are two closely connected facets of mathematical knowledge. This
relation can be used to identify mathematical software and find relevant
information about it. The approach and the state of the art of the information
service are described here.
"
840,Web Synchronization Simulations using the ResourceSync Framework,"  Maintenance of multiple, distributed up-to-date copies of collections of
changing Web resources is important in many application contexts and is often
achieved using ad hoc or proprietary synchronization solutions. ResourceSync is
a resource synchronization framework that integrates with the Web architecture
and leverages XML sitemaps. We define a model for the ResourceSync framework as
a basis for understanding its properties. We then describe experiments in which
simulations of a variety of synchronization scenarios illustrate the effects of
model configuration on consistency, latency, and data transfer efficiency.
These results provide insight into which congurations are appropriate for
various application scenarios.
"
841,"Performing Informetric Analysis on Information Retrieval Test
  Collections: Preliminary Experiments in the Physics Domain","  The combination of informetric analysis and information retrieval allows a
twofold application. (1) While in-formetrics analysis is primarily used to gain
insights into a scientific domain, it can be used to build recommen-dation or
alternative ranking services. They are usually based on methods like
co-occurrence or citation analyses. (2) Information retrieval and its
decades-long tradition of rigorous evaluation using standard document corpora,
predefined topics and relevance judgements can be used as a test bed for
informetric analyses. We show a preliminary experiment on how both domains can
be connected using the iSearch test collection, a standard information
retrieval test collection derived from the open access arXiv.org preprint
server. In this paper the aim is to draw a conclusion about the appropriateness
of iSearch as a test bed for the evaluation of a retrieval or recommendation
system that applies informetric methods to improve retrieval results for the
user. Based on an interview study with physicists, bibliographic coupling and
author-co-citation analysis, important authors for ten different research
questions are identified. The results show that the analysed corpus includes
these authors and their corresponding documents. This study is a first step
towards a combination of retrieval evaluations and the evaluation of
informetric analyses methods.
"
842,"Effects of Publications in Proceedings on the Measure of the Core Size
  of Coauthors","  Coauthors (CA) of a ""lead investigator"" (LI) can receive a rank (r) according
to their ""importance"" in having published joint publications with the LI. It is
commonly accepted, without any proof, that publications in peer review journals
and e.g. conference proceedings do not have the same ""value"" in a CV. Same for
papers contributed to encyclopedia and book chapters. It is here examined
whether the relationship between the number (J) of publications of some
scientist with her/his coauthors, ranked according to their decreasing
importance, i.e. $ J \propto 1/r^{\alpha} $, as found by Ausloos, still holds
if the overall publication list is broken into such specific types of
publications. Several authors, with different careers, but mainly having worked
in the field of statistical mechanics, are studied here to sort out answers to
the questions. The exponent $\alpha$ turns out to be weakly scientist
dependent, only if the maximum value of J and r is large and is $\sim +1$ then.
The $m_A$ core value, i.e. the core number of CAs, for proceedings only is
about half of the total one, i.e. when all publications are counted.
Contributions to the numerical values from both encyclopedia and book chapters
are marginal. The role of a time span on $m_A$ is also examined in two cases in
relation to career activity considerations. It can considered that the findings
serve as a contrasting point of view on how to quantify an individual
(publication) career as recently done by Petersen et al., here emphasizing the
collaboration size and evolution, rather than a citation count, moreover
specifying the type of publication. Through the various $m_A$'s one can
distinguish different behavior patterns of a scientist publication with CAs.
"
843,"Hierarchical structuring of Cultural Heritage objects within large
  aggregations","  Huge amounts of cultural content have been digitised and are available
through digital libraries and aggregators like Europeana.eu. However, it is not
easy for a user to have an overall picture of what is available nor to find
related objects. We propose a method for hier- archically structuring cultural
objects at different similarity levels. We describe a fast, scalable clustering
algorithm with an automated field selection method for finding semantic
clusters. We report a qualitative evaluation on the cluster categories based on
records from the UK and a quantitative one on the results from the complete
Europeana dataset.
"
844,"Achieving interoperability between the CARARE schema for monuments and
  sites and the Europeana Data Model","  Mapping between different data models in a data aggregation context always
presents significant interoperability challenges. In this paper, we describe
the challenges faced and solutions developed when mapping the CARARE schema
designed for archaeological and architectural monuments and sites to the
Europeana Data Model (EDM), a model based on Linked Data principles, for the
purpose of integrating more than two million metadata records from national
monument collections and databases across Europe into the Europeana digital
library.
"
845,"arXiv e-prints and the journal of record: An analysis of roles and
  relationships","  Since its creation in 1991, arXiv has become central to the diffusion of
research in a number of fields. Combining data from the entirety of arXiv and
the Web of Science (WoS), this paper investigates (a) the proportion of papers
across all disciplines that are on arXiv and the proportion of arXiv papers
that are in the WoS, (b) elapsed time between arXiv submission and journal
publication, and (c) the aging characteristics and scientific impact of arXiv
e-prints and their published version. It shows that the proportion of WoS
papers found on arXiv varies across the specialties of physics and mathematics,
and that only a few specialties make extensive use of the repository. Elapsed
time between arXiv submission and journal publication has shortened but remains
longer in mathematics than in physics. In physics, mathematics, as well as in
astronomy and astrophysics, arXiv versions are cited more promptly and decay
faster than WoS papers. The arXiv versions of papers - both published and
unpublished - have lower citation rates than published papers, although there
is almost no difference in the impact of the arXiv versions of both published
and unpublished papers.
"
846,Quantifying Long-Term Scientific Impact,"  The lack of predictability of citation-based measures frequently used to
gauge impact, from impact factors to short-term citations, raises a fundamental
question: Is there long-term predictability in citation patterns? Here, we
derive a mechanistic model for the citation dynamics of individual papers,
allowing us to collapse the citation histories of papers from different
journals and disciplines into a single curve, indicating that all papers tend
to follow the same universal temporal pattern. The observed patterns not only
help us uncover basic mechanisms that govern scientific impact but also offer
reliable measures of influence that may have potential policy implications.
"
847,"An open diachronic corpus of historical Spanish: annotation criteria and
  automatic modernisation of spelling","  The IMPACT-es diachronic corpus of historical Spanish compiles over one
hundred books --containing approximately 8 million words-- in addition to a
complementary lexicon which links more than 10 thousand lemmas with
attestations of the different variants found in the documents. This textual
corpus and the accompanying lexicon have been released under an open license
(Creative Commons by-nc-sa) in order to permit their intensive exploitation in
linguistic research. Approximately 7% of the words in the corpus (a selection
aimed at enhancing the coverage of the most frequent word forms) have been
annotated with their lemma, part of speech, and modern equivalent. This paper
describes the annotation criteria followed and the standards, based on the Text
Encoding Initiative recommendations, used to the represent the texts in digital
form. As an illustration of the possible synergies between diachronic textual
resources and linguistic research, we describe the application of statistical
machine translation techniques to infer probabilistic context-sensitive rules
for the automatic modernisation of spelling. The automatic modernisation with
this type of statistical methods leads to very low character error rates when
the output is compared with the supervised modern version of the text.
"
848,The revised SNIP indicator of Elsevier's Scopus,"  The modified SNIP indicator of Elsevier, as recently explained by Waltman et
al. (2013) in this journal, solves some of the problems which Leydesdorff &
Opthof (2010 and 2011) indicated in relation to the original SNIP indicator
(Moed, 2010 and 2011). The use of an arithmetic average, however, remains
unfortunate in the case of scientometric distributions because these can be
extremely skewed (Seglen, 1992 and 1997). The new indicator cannot (or hardly)
be reproduced independently when used for evaluation purposes, and remains in
this sense opaque from the perspective of evaluated units and scholars.
"
849,UDC in Action,"  The UDC (Universal Decimal Classification) is not only a classification
language with a long history; it also presents a complex cognitive system
worthy of the attention of complexity theory. The elements of the UDC: classes,
auxiliaries, and operations are combined into symbolic strings, which in
essence represent a complex networks of concepts. This network forms a backbone
of ordering of knowledge and at the same time allows expression of different
perspectives on various products of human knowledge production. In this paper
we look at UDC strings derived from the holdings of libraries. In particular we
analyse the subject headings of holdings of the university library in Leuven,
and an extraction of UDC numbers from the OCLC WorldCat. Comparing those sets
with the Master Reference File, we look into the length of strings, the
occurrence of different auxiliary signs, and the resulting connections between
UDC classes. We apply methods and representations from complexity theory.
Mapping out basic statistics on UDC classes as used in libraries from a point
of view of complexity theory bears different benefits. Deploying its structure
could serve as an overview and basic information for users among the nature and
focus of specific collections. A closer view into combined UDC numbers reveals
the complex nature of the UDC as an example for a knowledge ordering system,
which deserves future exploration from a complexity theoretical perspective.
"
850,MYE: Missing Year Estimation in Academic Social Networks,"  In bibliometrics studies, a common challenge is how to deal with incorrect or
incomplete data. However, given a large volume of data, there often exists
certain relationships between the data items that can allow us to recover
missing data items and correct erroneous data. In this paper, we study a
particular problem of this sort - estimating the missing year information
associated with publications (and hence authors' years of active publication).
We first propose a simple algorithm that only makes use of the ""direct""
information, such as paper citation/reference relationships or paper-author
relationships. The result of this simple algorithm is used as a benchmark for
comparison. Our goal is to develop algorithms that increase both the coverage
(the percentage of missing year papers recovered) and accuracy (mean absolute
error of the estimated year to the real year). We propose some advanced
algorithms that extend inference by information propagation. For each
algorithm, we propose three versions according to the given academic social
network type: a) Homogeneous (only contains paper citation links), b) Bipartite
(only contains paper-author relations), and, c) Heterogeneous (both paper
citation and paper-author relations). We carry out experiments on the three
public data sets (MSR Libra, DBLP and APS), and evaluated by applying the
K-fold cross validation method. We show that the advanced algorithms can
improve both coverage and accuracy.
"
851,"Growth in the number of references in engineering journal papers during
  the 1972-2013 period","  The number of references per paper, perhaps the best single index of a
journal's scholarliness, has been studied in different disciplines and periods.
In this paper we present a four decade study of eight engineering journals. A
data set of over 70000 references was generated after automatic data gathering
and manual inspection for errors. Results show a significant increase in the
number of references per paper, the average rises from 8 in 1972 to 25 in 2013.
This growth presents an acceleration around the year 2000, consistent with a
much easier access to search engines and documents produced by the
generalization of the Internet.
"
852,"Which percentile-based approach should be preferred for calculating
  normalized citation impact values? An empirical comparison of five approaches
  including a newly developed citation-rank approach (P100)","  Percentile-based approaches have been proposed as a non-parametric
alternative to parametric central-tendency statistics to normalize observed
citation counts. Percentiles are based on an ordered set of citation counts in
a reference set, whereby the fraction of papers at or below the citation counts
of a focal paper is used as an indicator for its relative citation impact in
the set. In this study, we pursue two related objectives: (1) although
different percentile-based approaches have been developed, an approach is
hitherto missing that satisfies a number of criteria such as scaling of the
percentile ranks from zero (all other papers perform better) to 100 (all other
papers perform worse), and solving the problem with tied citation ranks
unambiguously. We introduce a new citation-rank approach having these
properties, namely P100. (2) We compare the reliability of P100 empirically
with other percentile-based approaches, such as the approaches developed by the
SCImago group, the Centre for Science and Technology Studies (CWTS), and
Thomson Reuters (InCites), using all papers published in 1980 in Thomson
Reuters Web of Science (WoS). How accurately can the different approaches
predict the long-term citation impact in 2010 (in year 31) using citation
impact measured in previous time windows (years 1 to 30)? The comparison of the
approaches shows that the method used by InCites overestimates citation impact
(because of using the highest percentile rank when papers are assigned to more
than a single subject category) whereas the SCImago indicator shows higher
power in predicting the long-term citation impact on the basis of citation
rates in early years. Since the results show a disadvantage in this predictive
ability for P100 against the other approaches, there is still room for further
improvements.
"
853,The Academic Social Network,"  Through academic publications, the authors of these publications form a
social network. Instead of sharing casual thoughts and photos (as in Facebook),
authors pick co-authors and reference papers written by other authors. Thanks
to various efforts (such as Microsoft Libra and DBLP), the data necessary for
analyzing the academic social network is becoming more available on the
Internet. What type of information and queries would be useful for users to
find out, beyond the search queries already available from services such as
Google Scholar? In this paper, we explore this question by defining a variety
of ranking metrics on different entities -authors, publication venues and
institutions. We go beyond traditional metrics such as paper counts, citations
and h-index. Specifically, we define metrics such as influence, connections and
exposure for authors. An author gains influence by receiving more citations,
but also citations from influential authors. An author increases his/her
connections by co-authoring with other authors, and specially from other
authors with high connections. An author receives exposure by publishing in
selective venues where publications received high citations in the past, and
the selectivity of these venues also depends on the influence of the authors
who publish there. We discuss the computation aspects of these metrics, and
similarity between different metrics. With additional information of
author-institution relationships, we are able to study institution rankings
based on the corresponding authors' rankings for each type of metric as well as
different domains. We are prepared to demonstrate these ideas with a web site
(http://pubstat.org) built from millions of publications and authors.
"
854,Table of Content detection using Machine Learning,"  Table of content (TOC) detection has drawn attention now a day because it
plays an important role in digitization of multipage document. Generally book
document is multipage document. So it becomes necessary to detect Table of
Content page for easy navigation of multipage document and also to make
information retrieval faster for desirable data from the multipage document.
All the Table of content pages follow the different layout, different way of
presenting the contents of the document like chapter, section, subsection etc.
This paper introduces a new method to detect Table of content using machine
learning technique with different features. With the main aim to detect Table
of Content pages is to structure the document according to their contents.
"
855,"The potential of preprints to accelerate scholarly communication - A
  bibliometric analysis based on selected journals","  This paper quantifies to which extent preprints in arXiv accelerate scholarly
communication. The following subject fields were investigated up to the year
2012: High Energy Physics (HEP), Mathematics, Astrophysics, Quantitative
Biology, and Library and Information Science (LIS). Publication and citation
data was downloaded from Scopus and matched with corresponding preprints in
arXiv. Furthermore, the INSPIRE HEP database was used to retrieve citation data
for papers related to HEP. The bibliometric analysis deals with the growth in
numbers of articles published having a previous preprint in arXiv and the
publication delay, which is defined as the chronological distance between the
deposit of a preprint in arXiv and its formal journal publication. Likewise,
the citation delay is analyzed, which describes the time it takes until the
first citation of preprints, and articles, respectively. Total citation numbers
are compared for sets of articles with a previous preprint and those without.
The results show that in all fields but biology a significant citation
advantage exists in terms of speed and citation rates for articles with a
previous preprint version on arXiv.
"
856,Static and Dynamic Aspects of Scientific Collaboration Networks,"  Collaboration networks arise when we map the connections between scientists
which are formed through joint publications. These networks thus display the
social structure of academia, and also allow conclusions about the structure of
scientific knowledge. Using the computer science publication database DBLP, we
compile relations between authors and publications as graphs and proceed with
examining and quantifying collaborative relations with graph-based methods. We
review standard properties of the network and rank authors and publications by
centrality. Additionally, we detect communities with modularity-based
clustering and compare the resulting clusters to a ground-truth based on
conferences and thus topical similarity. In a second part, we are the first to
combine DBLP network data with data from the Dagstuhl Seminars: We investigate
whether seminars of this kind, as social and academic events designed to
connect researchers, leave a visible track in the structure of the
collaboration network. Our results suggest that such single events are not
influential enough to change the network structure significantly. However, the
network structure seems to influence a participant's decision to accept or
decline an invitation.
"
857,"An introduction to the coverage of the Data Citation Index
  (Thomson-Reuters): disciplines, document types and repositories","  In the past years, the movement of data sharing has been enjoying great
popularity. Within this context, Thomson Reuters launched at the end of 2012 a
new product inside the Web of Knowledge family: the Data Citation Index. The
aim of this tool is to enable discovery and access, from a single place, to
data from a variety of data repositories from different subject areas and from
around the world. In this short note we present some preliminary results from
the analysis of the Data Citation Index. Specifically, we address the following
issues: discipline coverage, data types present in the database, and
repositories that were included at the time of the study
"
858,Altmetrics: New Indicators for Scientific Communication in Web 2.0,"  In this paper we review the socalled altmetrics or alternative metrics. This
concept raises from the development of new indicators based on Web 2.0, for the
evaluation of the research and academic activity. The basic assumption is that
variables such as mentions in blogs, number of twits or of researchers
bookmarking a research paper for instance, may be legitimate indicators for
measuring the use and impact of scientific publications. In this sense, these
indicators are currently the focus of the bibliometric community and are being
discussed and debated. We describe the main platforms and indicators and we
analyze as a sample the Spanish research output in Communication Studies.
Comparing traditional indicators such as citations with these new indicators.
The results show that the most cited papers are also the ones with a highest
impact according to the altmetrics. We conclude pointing out the main
shortcomings these metrics present and the role they may play when measuring
the research impact through 2.0 platforms.
"
859,The DeLiVerMATH project - Text analysis in mathematics,"  A high-quality content analysis is essential for retrieval functionalities
but the manual extraction of key phrases and classification is expensive.
Natural language processing provides a framework to automatize the process.
Here, a machine-based approach for the content analysis of mathematical texts
is described. A prototype for key phrase extraction and classification of
mathematical texts is presented.
"
860,"Reviewers' ratings and bibliometric indicators: hand in hand when
  assessing over research proposals?","  The peer review system has been traditionally challenged due to its many
limitations especially for allocating funding. Bibliometric indicators may well
present themselves as a complement. Objective: We analyze the relationship
between peers' ratings and bibliometric indicators for Spanish researchers in
the 2007 National R&D Plan for 23 research fields. We analyze peers' ratings
for 2333 applications. We also gathered principal investigators' research
output and impact and studied the differences between accepted and rejected
applications. We used the Web of Science database and focused on the 2002-2006
period. First, we analyzed the distribution of granted and rejected proposals
considering a given set of bibliometric indicators to test if there are
significant differences. Then, we applied a multiple logistic regression
analysis to determine if bibliometric indicators can explain by themselves the
concession of grant proposals. 63.4% of the applications were funded.
Bibliometric indicators for accepted proposals showed a better previous
performance than for those rejected; however the correlation between peer
review and bibliometric indicators is very heterogeneous among most areas. The
logistic regression analysis showed that the main bibliometric indicators that
explain the granting of research proposals in most cases are the output (number
of published articles) and the number of papers published in journals that
belong to the first quartile ranking of the Journal Citations Report.
Bibliometric indicators predict the concession of grant proposals at least as
well as peer ratings. Social Sciences and Education are the only areas where no
relation was found, although this may be due to the limitations of the Web of
Science's coverage. These findings encourage the use of bibliometric indicators
as a complement to peer review in most of the analyzed areas.
"
861,"From P100 to P100_: Conception and improvement of a new citation-rank
  approach in bibliometrics","  Properties of a percentile-based rating scale needed in bibliometrics are
formulated. Based on these properties, P100 was recently introduced as a new
citation-rank approach (Bornmann, Leydesdorff, & Wang, in press). In this
paper, we conceptualize P100 and propose an improvement which we call P100_.
Advantages and disadvantages of citation-rank indicators are noted.
"
862,Categorical and Geographical Separation in Science,"  We perform the analysis of scientific collaboration at the level of
universities. The scope of this study is to answer two fundamental questions:
(i) can one indicate a category (i.e., a scientific discipline) that has the
greatest impact on the rank of the university and (ii) do the best universities
collaborate with the best ones only? Using two university ranking lists (ARWU
and QS) as well as data from the Science Citation Index we show how the number
of publications in certain categories correlates with the university rank.
Moreover, using complex networks analysis, we give hints that the scientific
collaboration is highly embedded in the physical space and the number of common
papers decays with the distance between them. We also show the strength of the
ties between universities is proportional to product of their total number of
publications.
"
863,Quality indicators for scientific journals based on experts opinion,"  This paper presents the results and further development of a survey sent to
11,799 Spanish faculty members and researchers from various fields of the
social sciences and the humanities, obtaining a total of 45.6% (5,368
responses) usable answers. Respondents were asked (a) to indicate the three
most important journals in their field and (b) to rate them on a 0-10 scale
according to their quality. The information obtained has been synthesized in
two indicators which reflect the perceived quality of journals. Once the values
were obtained, the journals were categorized according to each indicator and
the ordinal positions were compared. Different profiles of journals are
analyzed in connection with experts opinion, such as regional orientation, and
the consensus among researchers is studied. Finally, the possibilities of
extending the research and indicators to sets of international journals are
explored.
"
864,"The elephant in the room: multi-authorship and the assessment of
  individual researchers","  When a group of individuals creates something, credit is usually divided
among them. Oddly, that does not apply to scientific papers. The most commonly
used performance measure for individual researchers is the h-index, which does
not correct for multi-authorship. Each author claims full credit for each paper
and each ensuing citation. This mismeasure of achievement is fuelling a
flagrant increase in multi-authorship. Several alternatives to the h-index have
been devised, and one of them, the individual h-index (hI), is logical,
intuitive and easily calculated. Correcting for multi-authorship would end
gratuitous authorship and allow proper attribution and unbiased comparisons.
"
865,"The ""Academic Trace"" of the Performance Matrix: A Mathematical Synthesis
  of the h-Index and the Integrated Impact Indicator (I3)","  The h-index provides us with nine natural classes which can be written as a
matrix of three vectors. The three vectors are: X=(X1, X2, X3) indicate
publication distribution in the h-core, the h-tail, and the uncited ones,
respectively; Y=(Y1, Y2, Y3) denote the citation distribution of the h-core,
the h-tail and the so-called ""excess"" citations (above the h-threshold),
respectively; and Z=(Z1, Z2, Z3)= (Y1-X1, Y2-X2, Y3-X3). The matrix V=(X,Y,Z)T
constructs a measure of academic performance, in which the nine numbers can all
be provided with meanings in different dimensions. The ""academic trace"" tr(V)
of this matrix follows naturally, and contributes a unique indicator for total
academic achievements by summarizing and weighting the accumulation of
publications and citations. This measure can also be used to combine the
advantages of the h-index and the Integrated Impact Indicator (I3) into a
single number with a meaningful interpretation of the values. We illustrate the
use of tr(V) for the cases of two journal sets, two universities, and ourselves
as two individual authors.
"
866,"H Index of scientific Nursing journals according to Google Scholar
  Metrics (2007-2011)","  The aim of this report is to present a ranking of Nursing journals covered in
Google Scholar Metrics (GSM), a Google product launched in 2012 to assess the
impact of scientific journals from citation counts this receive on Google
Scholar. Google has chosen to include only those journals that have published
at least 100 papers and have at least one citation in a period of five years
(2007-2011). Journal rankings are sorted by languages (showing the 100 papers
with the greatest impact). This tool allows to sort by subject areas and
disciplines, but only in the case of journals in English. In this case, it only
shows the 20 journals with the highest h index. This option is not available
for journals in the other nine languages present in Google (Chinese,
Portuguese, German, Spanish, French, Korean, Japanese, Dutch and Italian).
  Google Scholar Metrics doesnt currently allow to group and sort all journals
belonging to a scientific discipline. In the case of Nursing, in the ten
listings displayed by GSM we can only locate 34 journals. Therefore, in an
attempt to overcome this limitation, we have used the diversity of search
procedures allowed by GSM to identify the greatest number of scientific
journals of Nursing with h index calculated by this bibliometric tool.
Bibliographic searches were conducted between 10th and 30th May 2013.
  The result is a ranking of 337 nursing journals sorted by the same h index,
and mean as discriminating value. Journals are also grouped by quartiles.
"
867,"On house renovation and coauthoring (with a little excursus on the Holy
  Grail of bibliometrics)","  More than a paper, this is just a little divertissement about coauthoring,
the Hirsch h-index, and bibliometric evaluation in general. Without pretending
to yield any general conclusions, what I found rummaging through the physics
literature made me think quite a bit. I hope the same will happen to my
readers, even it they will likely be much less than 25, which is the audience
one of the greatest Italian writers (whom, is left to the reader to single out)
addresses to.
"
868,An Evaluation of Caching Policies for Memento TimeMaps,"  As defined by the Memento Framework, TimeMaps are ma-chine-readable lists of
time-specific copies -- called ""mementos"" -- of an archived original resource.
In theory, as an archive acquires additional mementos over time, a TimeMap
should be monotonically increasing. However, there are reasons why the number
of mementos in a TimeMap would decrease, for example: archival redaction of
some or all of the mementos, archival restructuring, and transient errors on
the part of one or more archives. We study TimeMaps for 4,000 original
resources over a three month period, note their change patterns, and develop a
caching algorithm for TimeMaps suitable for a reverse proxy in front of a
Memento aggregator. We show that TimeMap cardinality is constant or
monotonically increasing for 80.2% of all TimeMap downloads observed in the
observation period. The goal of the caching algorithm is to exploit the ideally
monotonically increasing nature of TimeMaps and not cache responses with fewer
mementos than the already cached TimeMap. This new caching algorithm uses
conditional cache replacement and a Time To Live (TTL) value to ensure the user
has access to the most complete TimeMap available. Based on our empirical data,
a TTL of 15 days will minimize the number of mementos missed by users, and
minimize the load on archives contributing to TimeMaps.
"
869,The predictability of the Hirsch index evolution,"  The h-index can be used as a predictor of itself. However, the evolution of
the h-index with time is shown in the present investigation to be dominated for
several years by citations to previous publications rather than by new
scientific achievements. This inert behaviour of the h-index raises questions,
whether the h-index can be used profitably in academic appointment processes or
for the allocation of research resources.
"
870,Is there currently a scientific revolution in scientometrics?,"  The author of this letter to the editor would like to set forth the argument
that scientometrics is currently in a phase in which a taxonomic change, and
hence a revolution, is taking place. One of the key terms in scientometrics is
scientific impact which nowadays is understood to mean not only the impact on
science but the impact on every area of society.
"
871,"Quantitative CV-based indicators for research quality, validated by peer
  review","  In a university, research assessments are organized at different policy
levels (faculties, research council) in different contexts (funding, council
membership, personnel evaluations). Each evaluation requires its own focus and
methodology. To conduct a coherent research policy however, data on which
different assessments are based should be well coordinated. A common set of
core indicators for any type of research assessment can provide a supportive
and objectivating tool for evaluations at different institutional levels and at
the same time promote coherent decision-making. The same indicators can also
form the basis for a 'light touch' monitoring instrument, signalling when and
where a more thorough evaluation could be considered. This poster paper shows
how peer review results were used to validate a set of quantitative indicators
for research quality for a first series of disciplines. The indicators
correspond to categories in the university's standard CV-format. Per
discipline, specific indicators are identified corresponding to their own
publication and funding characteristics. Also more globally valid indicators
are identified after normalization for discipline-characteristic performance
levels. The method can be applied to any system where peer ratings and
quantitative performance measures, both reliable and sufficiently detailed, can
be combined for the same entities.
"
872,"Composing a Publication List for Individual Researcher Assessment by
  Merging Information from Different Sources","  Citation and publication profiles are gaining importance for the evaluation
of top researchers when it comes to the appropriation of funding for excellence
programs or career promotion judgments. Indicators like the Normalized Mean
Citation Rate, the hindex or other distinguishing measures are increasingly
used to picture the characteristics of individual scholars. Using bibliometric
techniques for individual assessment is known to be particularly delicate, as
the chance of errors being averaged away becomes smaller whereas a minor
incompleteness can have a significant influence on the evaluation outcome. The
quality of the data becomes as such crucial to the legitimacy of the methods
used.
"
873,Impact Vitality - A Measure for Excellent Scientists,"  In many countries and at European level, research policy increasingly focuses
on 'excellent' researchers. The concept of excellence however is complex and
multidimensional. For individual scholars it involves talents for innovative
knowledge creation and successful transmission to peers, as well as management
capacities. Excellence is also a comparative concept, implying the ability to
surpass others [TIJSSEN, 2003]. Grants are in general awarded based on
assessments by expert committees. While peer review is a widely accepted
practice, it nevertheless is also subject to criticism. At higher aggregation
levels, peer assessments are often supported by quantitative measures. At
individual level, most of these measures are much less appropriate and there is
a need for new, dedicated indicators.
"
874,"Quality related publication categories in social sciences and
  humanities, based on a university's peer review assessments","  Bibliometric analysis has firmly conquered its place as an instrument for
evaluation and international comparison of performance levels. Consequently,
differences in coverage by standard bibliometric databases installed a
dichotomy between on the one hand the well covered 'exact' sciences, and on the
other hand most of the social sciences and humanities with a more limited
coverage (Nederhof, 2006). Also the latter domains need to be able to soundly
demonstrate their level of performance and claim or legitimate funding
accordingly. An important part of the output volume in social sciences appears
as books, book chapters and national literature (Hicks, 2004). To proceed from
publication data to performance measurement, quantitative publication counts
need to be combined with qualitative information, for example from peer
assessment or validation (European Expert Group on Assessment of
University-Based Research, 2010), to identify those categories that represent
research quality as perceived by peers. An accurate focus is crucial in order
to stimulate, recognize and reward high quality achievements only. This paper
demonstrates how such a selection of publication categories can be based on
correlations with peer judgments. It is also illustrated that the selection
should be sufficiently precise, to avoid subcategories negatively correlated
with peer judgments. The findings indicate that, also in social sciences and
humanities, publications in journals with an international referee system are
the most important category for evaluating quality. Book chapters with
international referee system and contributions in international conference
proceedings follow them.
"
875,"Output and citation impact of interdisciplinary networks: Experiences
  from a dedicated funding program","  In a context of ever more specialized scientists, interdisciplinarity
receives increasing attention as innovating ideas are often situated where the
disciplines meet. In many countries science policy makers installed dedicated
funding programs and policies. This induces a need for specific tools for their
support. There is however not yet a generally accepted quantitative method or
set of criteria to recognize and evaluate interdisciplinary research outputs
(Tracking and evaluating interdisciplinary research: metrics and maps, 12th
ISSI Conference, 2009). Interdisciplinarity also takes on very different forms,
as distinguished in overviews from the first codifications (Klein, 1990) to the
latest reference work (Frodeman et al., 2010). In the specific context of
research measurement and evaluation, interdisciplinarity was discussed e.g. by
Rinia (2007) and Porter et al. (2006). This empirical study aims to contribute
to the understanding and the measuring of interdisciplinary research at the
micro level, in the form of new synergies between disciplines. Investigation of
a specialized funding program shows how a new interdisciplinary synergy and its
citation impact are visible in co-publications and cocitations, and that these
are important parameters for assessment. The results also demonstrate the
effect of funding, which is clearly present after about three years.
"
876,"Interdisciplinary Research Collaborations: Evaluation of a Funding
  Program","  Innovative ideas are often situated where disciplines meet, and
socio-economic problems generally require contributions from several
disciplines. Ways to stimulate interdisciplinary research collaborations are
therefore an increasing point of attention for science policy. There is concern
that 'regular' funding programs, involving advice from disciplinary experts and
discipline-bound viewpoints, may not adequately stimulate, select or evaluate
this kind of research. This has led to specific policies aimed at
interdisciplinary research in many countries. There is however at this moment
no generally accepted method to adequately select and evaluate
interdisciplinary research. In the vast context of different forms of
interdisciplinarity, this paper aims to contribute to the debate on best
practices to stimulate and support interdisciplinary research collaborations.
It describes the selection procedures and results of a university program
supporting networks formed 'bottom up', integrating expertise from different
disciplines. The program's recent evaluation indicates that it is successful in
selecting and supporting the interdisciplinary synergies aimed for, responding
to a need experienced in the field. The analysis further confirms that
potential for interdisciplinary collaboration is present in all disciplines.
"
877,Research Excellence Milestones of BRIC and N-11 Countries,"  While scientific performance is an important aspect of a stable and healthy
economy, measures for it have yet to gain their place in economic country
profiles. As useful indicators for this performance dimension, this paper
introduces the concept of milestones for research excellence, as points of
transition to higher-level contributions at the leading edge of science. The
proposed milestones are based on two indicators associated with research
excellence, the impact vitality profile and the production of review type
publications, both applied to a country's publications in the top journals
Nature and Science. The milestones are determined for two distinct groups of
emerging market economies: the BRIC countries, which outperformed the relative
growth expected at their identification in 2001, and the N-11 or Next Eleven
countries, identified in 2005 as potential candidates for a BRIC-like
evolution. Results show how these two groups at different economic levels can
be clearly distinguished based on the research milestones, indicating a
potential utility as parameters in an economic context.
"
878,"Characteristics of International versus Non-International Scientific
  Publication Media in Team- and Author-Based Data","  The enlarged coverage of the international publication and citation databases
Web of Science and Scopus towards local media in social sciences was a welcome
response to an increased usage of these databases in evaluation and funding
systems. The mostly international journals available earlier were the basis for
the development of current standard bibliometric indicators. The same
indicators may no longer measure exactly the same concepts when applied to
newly introduced or extended media categories, with possibly different
characteristics than those of international journals. This paper investigates
differences between media with and without international dimension in
publication data at team and author level. The findings relate the
international publication categories to research quality, important for
validation of their usage in evaluation or funding models that aim to stimulate
quality.
"
879,"Groups of Highly Cited Publications: Stability in Content with Citation
  Window Length","  The growing focus in research policy worldwide on top scientists makes it
increasingly important to define adequate supporting measures to help identify
excellent scientists. Highly cited publications have since long been associated
to research excellence. At the same time, the analysis of the high-end of
citation distributions still is a challenging topic in evaluative
bibliometrics. Evaluations typically require indicators that generate
sufficiently stable results when applied to recent publication records of
limited size. Highly cited publications have been identified using two
techniques in particular: pre-set percentiles, and the parameter free
Characteristic Scores and Scales (CSS) (Gl\""anzel & Schubert, 1988). The
stability required in assessments of relatively small publication records,
concerns size as well as content of groups of highly cited publications.
Influencing factors include domain delineation and citation window length.
Stability in size is evident for the pre-set percentiles, and has been
demonstrated for the CSS-methodology beyond an initial citation period of about
three years (Gl\""anzel, 2007). Stability in content is less straightforward,
considering for instance that more highly cited publications can have a later
citation peak, as observed by Abt (1981) for astronomical papers. This paper
investigates the stability in content of groups of highly cited publications,
i.e. the extent to which individual publications enter and leave the group as
the citation window is enlarged.
"
880,"Partition-based Field Normalization: An approach to highly specialized
  publication records","  Field normalized citation rates are well-established indicators for research
performance from the broadest aggregation levels such as countries, down to
institutes and research teams. When applied to still more specialized
publication sets at the level of individual scientists, also a more accurate
delimitation is required of the reference domain that provides the expectations
to which a performance is compared. This necessity for sharper accuracy
challenges standard methodology based on predefined subject categories. This
paper proposes a way to define a reference domain that is more strongly
delimited than in standard methodology, by building it up out of cells of the
partition created by the pre-defined subject categories and their
intersections. This partition approach can be applied to different existing
field normalization variants. The resulting reference domain lies between those
generated by standard field normalization and journal normalization. Examples
based on fictive and real publication records illustrate how the potential
impact on results can exceed or be smaller than the effect of other currently
debated normalization variants, depending on the case studied. The proposed
Partition-based Field Normalization is expected to offer advantages in
particular at the level of individual scientists and other very specific
publication records, such as publication output from interdisciplinary
research.
"
881,Google Scholar Metrics 2013: nothing new under the sun,"  Main characteristics of Google Scholar Metrics new version (july 2013) are
presented. We outline the novelties and the weaknesses detected after a first
analysis. As main conclusion, we remark the lack of new functionalities with
respect to last editions, as the only modification is the update of the
timeframe (2008-2012). Hence, problems pointed out in our last reviews still
remain active. Finally, it seems Google Scholar Metrics will be updated in a
yearly basis
"
882,Reliability and Comparability of Peer Review Results,"  In this paper peer review reliability is investigated based on peer ratings
of research teams at two Belgian universities. It is found that outcomes can be
substantially influenced by the different ways in which experts attribute
ratings. To increase reliability of peer ratings, procedures creating a uniform
reference level should be envisaged. One should at least check for signs of low
reliability, which can be obtained from an analysis of the outcomes of the peer
evaluation itself. The peer review results are compared to outcomes from a
citation analysis of publications by the same teams, in subject fields well
covered by citation indexes. It is illustrated how, besides reliability,
comparability of results depends on the nature of the indicators, on the
subject area and on the intrinsic characteristics of the methods. The results
further confirm what is currently considered as good practice: the presentation
of results for not one but for a series of indicators.
"
883,"Research evaluation per discipline: a peer-review method and its
  outcomes","  This paper describes the method for ex-post peer review evaluation per
research discipline used at the Vrije Universiteit Brussel (VUB) and summarizes
the outcomes obtained from it. The method produces pertinent advice and
triggers responses - at the level of the individual researcher, the research
team and the university's research management - for the benefit of research
quality, competitivity and visibility. Imposed reflection and contacts during
and after the evaluation procedure modify the individual researcher's attitude,
improve the research teams' strategies and allow for the extraction of general
recommendations that are used as discipline-dependent guidelines in the
university's research management. The deep insights gained in the different
research disciplines and the substantial data sets on their research, support
the university management in its policy decisions and in building policy
instruments. Moreover, the results are used as a basis for comparison with
other assessments, leading to a better understanding of the possibilities and
limitations of different evaluation processes. The peer review method can be
applied systematically in a pluri-annual cycle of research discipline
evaluations to build up a complete overview, or it can be activated on an ad
hoc basis for a particular discipline, based on demands from research teams or
on strategic or policy arguments.
"
884,"Impact vitality: an indicator based on citing publications in search of
  excellent scientists","  This paper contributes to the quest for an operational definition of
'research excellence' and proposes a translation of the excellence concept into
a bibliometric indicator. Starting from a textual analysis of funding program
calls aimed at individual researchers and from the challenges for an indicator
at this level in particular, a new type of indicator is proposed. The Impact
Vitality indicator [RONS & AMEZ, 2008] reflects the vitality of the impact of a
researcher's publication output, based on the change in volume over time of the
citing publications. The introduced metric is shown to posses attractive
operational characteristics and meets a number of criteria which are desirable
when comparing individual researchers. The validity of one of the possible
indicator variants is tested using a small dataset of applicants for a senior
full time Research Fellowship. Options for further research involve testing
various indicator variants on larger samples linked to different kinds of
evaluations.
"
885,"Structure and Dynamics of Coauthorship, Citation, and Impact within CSCW","  CSCW has stabilized as an interdisciplinary venue for computer, information,
cognitive, and social scientists but has also undergone significant changes in
its format in recent years. This paper uses methods from social network
analysis and bibliometrics to re-examine the structures of CSCW a decade after
its last systematic analysis. Using data from the ACM Digital Library, we
analyze changes in structures of coauthorship and citation between 1986 and
2013. Statistical models reveal significant but distinct patterns between
papers and authors in how brokerage and closure in these networks affects
impact as measured by citations and downloads. Specifically, impact is unduly
influenced by structural position, such that ideas introduced by those in the
core of the CSCW community (e.g., elite researchers) are advantaged over those
introduced by peripheral participants (e.g., newcomers). This finding is
examined in the context of recent changes to the CSCW conference that may have
the effect of upsetting the preference for contributions from the core.
"
886,How far does scientific community look back?,"  How does the published scientific literature used by scientific community?
Many previous studies make analysis on the static usage data. In this research,
we propose the concept of dynamic usage data. Based on the platform of
realtime.springer.com, we have been monitoring and recording the dynamic usage
data of Scientometrics articles round the clock. Our analysis find that papers
published in recent four years have many more downloads than papers published
four years ago. According to our quantitative calculation, papers down-loaded
on one day have an average lifetime of 4.1 years approximately. Classic papers
are still being downloaded frequently even long after their publication.
Additionally, we find that social media may reboot the attention of old
scientific literature in a short time.
"
887,The normalization of citation counts based on classification systems,"  If we want to assess whether the paper in question has had a particularly
high or low citation impact compared to other papers, the standard practice in
bibliometrics is to normalize citations in respect of the subject category and
publication year. A number of proposals for an improved procedure in the
normalization of citation impact have been put forward in recent years. Against
the background of these proposals this study describes an ideal solution for
the normalization of citation impact: in a first step, the reference set for
the publication in question is collated by means of a classification scheme,
where every publication is associated with a single principal research field or
subfield entry (e. g. via Chemical Abstracts sections) and a publication year.
In a second step, percentiles of citation counts are calculated for this set
and used to assign the normalized citation impact score to the publications
(and also to the publication in question).
"
888,On the Change in Archivability of Websites Over Time,"  As web technologies evolve, web archivists work to keep up so that our
digital history is preserved. Recent advances in web technologies have
introduced client-side executed scripts that load data without a referential
identifier or that require user interaction (e.g., content loading when the
page has scrolled). These advances have made automating methods for capturing
web pages more difficult. Because of the evolving schemes of publishing web
pages along with the progressive capability of web preservation tools, the
archivability of pages on the web has varied over time. In this paper we show
that the archivability of a web page can be deduced from the type of page being
archived, which aligns with that page's accessibility in respect to dynamic
content. We show concrete examples of when these technologies were introduced
by referencing mementos of pages that have persisted through a long evolution
of available technologies. Identifying these reasons for the inability of these
web pages to be archived in the past in respect to accessibility serves as a
guide for ensuring that content that has longevity is published using good
practice methods that make it available for preservation.
"
889,"Detecting the historical roots of research fields by reference
  publication year spectroscopy (RPYS)","  We introduce the quantitative method named ""reference publication year
spectroscopy"" (RPYS). With this method one can determine the historical roots
of research fields and quantify their impact on current research. RPYS is based
on the analysis of the frequency with which references are cited in the
publications of a specific research field in terms of the publication years of
these cited references. The origins show up in the form of more or less
pronounced peaks mostly caused by individual publications which are cited
particularly frequently. In this study, we use research on graphene and on
solar cells to illustrate how RPYS functions, and what results it can deliver.
"
890,"Accuracy of simple, initials-based methods for author name
  disambiguation","  There are a number of solutions that perform unsupervised name disambiguation
based on the similarity of bibliographic records or common co-authorship
patterns. Whether the use of these advanced methods, which are often difficult
to implement, is warranted depends on whether the accuracy of the most basic
disambiguation methods, which only use the author's last name and initials, is
sufficient for a particular purpose. We derive realistic estimates for the
accuracy of simple, initials-based methods using simulated bibliographic
datasets in which the true identities of authors are known. Based on the
simulations in five diverse disciplines we find that the first initial method
already correctly identifies 97% of authors. An alternative simple method,
which takes all initials into account, is typically two times less accurate,
except in certain datasets that can be identified by applying a simple
criterion. Finally, we introduce a new name-based method that combines the
features of first initial and all initials methods by implicitly taking into
account the last name frequency and the size of the dataset. This hybrid method
reduces the fraction of incorrectly identified authors by 10-30% over the first
initial method.
"
891,WikiPulse - A News-Portal Based on Wikipedia,"  More and more user-generated content is complementing conventional
journalism. While we don't think that CNN or New York Times and its
professional journalists will disappear anytime soon, formidable competition is
emerging through humble Wikipedia editors. In earlier work (Becker 2012), we
found that entertainment and sports news appeared on average about two hours
earlier on Wikipedia than on CNN and Reuters online. In this project we build a
news-reader that automatically identifies late-breaking news among the most
recent Wikipedia articles and then displays it on a dedicated Web site.
"
892,"Science Fiction as a Worldwide Phenomenon: A Study of International
  Creation, Consumption and Dissemination","  This paper examines the international nature of science fiction. The focus of
this research is to determine whether science fiction is primarily English
speaking and Western or global; being created and consumed by people in
non-Western, non-English speaking countries? Science fiction's international
presence was found in three ways, by network analysis, by examining a online
retailer and with a survey. Condor, a program developed by GalaxyAdvisors was
used to determine if science fiction is being talked about by non-English
speakers. An analysis of the international Amazon.com websites was done to
discover if it was being consumed worldwide. A survey was also conducted to see
if people had experience with science fiction. All three research methods
revealed similar results. Science fiction was found to be international, with
science fiction creators originating in different countries and writing in a
host of different languages. English and non-English science fiction was being
created and consumed all over the world, not just in the English speaking West.
"
893,Acceptance Rates in Physical Review Letters: No Seasonal Bias,"  Are editorial decisions biased? A recent discussion in Learned Publishing has
focused on one aspect of potential bias in editorial decisions, namely seasonal
(e.g., monthly) variations in acceptance rates of research journals. In this
letter, we contribute to the discussion by analyzing data from Physical Review
Letters (PRL), a journal published by the American Physical Society. We studied
the 190,106 papers submitted to PRL from January 1990 until September 2012. No
statistically significant variations were found in the monthly acceptance
rates. We conclude that the time of year that the authors of a paper submit
their work to PRL has no effect on the fate of the paper through the review
process.
"
894,The Wisdom of Citing Scientists,"  This Brief Communication discusses the benefits of citation analysis in
research evaluation based on Galton's ""Wisdom of Crowds"" (1907). Citations are
based on the assessment of many which is why they can be ascribed a certain
amount of accuracy. However, we show that citations are incomplete assessments
and that one cannot assume that a high number of citations correlate with a
high level of usefulness. Only when one knows that a rarely cited paper has
been widely read is it possible to say (strictly speaking) that it was
obviously of little use for further research. Using a comparison with 'like'
data, we try to determine that cited reference analysis allows a more
meaningful analysis of bibliometric data than times-cited analysis.
"
895,"Tweeting biomedicine: an analysis of tweets and citations in the
  biomedical literature","  Data collected by social media platforms have recently been introduced as a
new source for indicators to help measure the impact of scholarly research in
ways that are complementary to traditional citation-based indicators. Data
generated from social media activities related to scholarly content can be used
to reflect broad types of impact. This paper aims to provide systematic
evidence regarding how often Twitter is used to diffuse journal articles in the
biomedical and life sciences. The analysis is based on a set of 1.4 million
documents covered by both PubMed and Web of Science (WoS) and published between
2010 and 2012. The number of tweets containing links to these documents was
analyzed to evaluate the degree to which certain journals, disciplines, and
specialties were represented on Twitter. It is shown that, with less than 10%
of PubMed articles mentioned on Twitter, its uptake is low in general. The
relationship between tweets and WoS citations was examined for each document at
the level of journals and specialties. The results show that tweeting behavior
varies between journals and specialties and correlations between tweets and
citations are low, implying that impact metrics based on tweets are different
from those based on citations. A framework utilizing the coverage of articles
and the correlation between Twitter mentions and citations is proposed to
facilitate the evaluation of novel social-media based metrics and to shed light
on the question in how far the number of tweets is a valid metric to measure
research impact.
"
896,Citation Counts and Evaluation of Researchers in the Internet Age,"  Bibliometric measures derived from citation counts are increasingly being
used as a research evaluation tool. Their strengths and weaknesses have been
widely analyzed in the literature and are often subject of vigorous debate. We
believe there are a few fundamental issues related to the impact of the web
that are not taken into account with the importance they deserve. We focus on
evaluation of researchers, but several of our arguments may be applied also to
evaluation of research institutions as well as of journals and conferences.
"
897,Archiving the Relaxed Consistency Web,"  The historical, cultural, and intellectual importance of archiving the web
has been widely recognized. Today, all countries with high Internet penetration
rate have established high-profile archiving initiatives to crawl and archive
the fast-disappearing web content for long-term use. As web technologies
evolve, established web archiving techniques face challenges. This paper
focuses on the potential impact of the relaxed consistency web design on
crawler driven web archiving. Relaxed consistent websites may disseminate,
albeit ephemerally, inaccurate and even contradictory information. If captured
and preserved in the web archives as historical records, such information will
degrade the overall archival quality. To assess the extent of such quality
degradation, we build a simplified feed-following application and simulate its
operation with synthetic workloads. The results indicate that a non-trivial
portion of a relaxed consistency web archive may contain observable
inconsistency, and the inconsistency window may extend significantly longer
than that observed at the data store. We discuss the nature of such quality
degradation and propose a few possible remedies.
"
898,"A framework for systematic analysis of Open Access journals and its
  application in software engineering and information systems","  This article is a contribution towards an understanding of Open Access (OA)
publishing. It proposes an analysis framework of 18 core attributes, divided
into the areas of Bibliographic information, Activity metrics, Economics,
Accessibility, and Predatory issues of OA journals. The framework has been
employed in a systematic analysis of 30 OA journals in software engineering
(SE) and information systems (IS), which were selected among 386 OA journals in
Computer Science from the Directory of OA Journals. An analysis is performed on
the sample of the journals, to provide an overview of the current situation of
OA journals in the fields of SE and IS. The journals are then compared
between-group, according to the presence of a publication fee. A within-group
analysis is performed on the journals requesting publication charges to
authors, in order to understand what is the value added according to different
price ranges. This article offers several contributions. It presents an
overview of OA definitions and models. It provides an analysis framework born
from the observation of data and the literature. It raises the need to study OA
in the fields of SE and IS while offering a first analysis. Finally, it
provides recommendations to readers of OA journals. This paper highlights
several concerns still threatening OA publishing in the fields of SE and IS.
Among them, it is shown that high publication fees are not sufficiently
justified by the publishers, which often lack transparency and may prevent
authors from adopting OA.
"
899,"Can inferred provenance and its visualisation be used to detect
  erroneous annotation? A case study using UniProtKB","  A constant influx of new data poses a challenge in keeping the annotation in
biological databases current. Most biological databases contain significant
quantities of textual annotation, which often contains the richest source of
knowledge. Many databases reuse existing knowledge, during the curation process
annotations are often propagated between entries. However, this is often not
made explicit. Therefore, it can be hard, potentially impossible, for a reader
to identify where an annotation originated from. Within this work we attempt to
identify annotation provenance and track its subsequent propagation.
Specifically, we exploit annotation reuse within the UniProt Knowledgebase
(UniProtKB), at the level of individual sentences. We describe a visualisation
approach for the provenance and propagation of sentences in UniProtKB which
enables a large-scale statistical analysis. Initially levels of sentence reuse
within UniProtKB were analysed, showing that reuse is heavily prevalent, which
enables the tracking of provenance and propagation. By analysing sentences
throughout UniProtKB, a number of interesting propagation patterns were
identified, covering over 100, 000 sentences. Over 8000 sentences remain in the
database after they have been removed from the entries where they originally
occurred. Analysing a subset of these sentences suggest that approximately 30%
are erroneous, whilst 35% appear to be inconsistent. These results suggest that
being able to visualise sentence propagation and provenance can aid in the
determination of the accuracy and quality of textual annotation. Source code
and supplementary data are available from the authors website.
"
900,R-Score: Reputation-based Scoring of Research Groups,"  To manage the problem of having a higher demand for resources than
availability of funds, research funding agencies usually rank the major
research groups in their area of knowledge. This ranking relies on a careful
analysis of the research groups in terms of their size, number of PhDs
graduated, research results and their impact, among other variables. While
research results are not the only variable to consider, they are frequently
given special attention because of the notoriety they confer to the researchers
and the programs they are affiliated with. In here we introduce a new metric
for quantifying publication output, called R-Score for reputation-based score,
which can be used in support to the ranking of research groups or programs. The
novelty is that the metric depends solely on the listings of the publications
of the members of a group, with no dependency on citation counts. R-Score has
some interesting properties: (a) it does not require access to the contents of
published material, (b) it can be curated to produce highly accurate results,
and (c) it can be naturally used to compare publication output of research
groups (e.g., graduate programs) inside a same country, geographical area, or
across the world. An experiment comparing the publication output of 25 CS
graduate programs from Brazil suggests that R-Score can be quite useful for
providing early insights into the publication patterns of the various research
groups one wants to compare.
"
901,"Toward an Interactive Directory for Norfolk, Nebraska: 1899-1900","  We describe steps toward an interactive directory for the town of Norfolk,
Nebraska for the years 1899 and 1900. This directory would extend the
traditional city directory by including a wider range of entities being
described, much richer information about the entities mentioned and linkages to
mentions of the entities in material such as digitized historical newspapers.
Such a directory would be useful to readers who browse the historical
newspapers by providing structured summaries of the entities mentioned. We
describe the occurrence of entities in two years of the Norfolk Weekly News,
focusing on several individuals to better understand the types of information
which can be gleaned from historical newspapers and other historical materials.
We also describe a prototype program which coordinates information about
entities from the traditional city directories, the federal census, and from
newspapers. We discuss the structured coding for these entities, noting that
richer coding would increasingly include descriptions of events and scenarios.
We propose that rich content about individuals and communities could eventually
be modeled with agents and woven into historical narratives.
"
902,"The Z-index: A geometric representation of productivity and impact which
  accounts for information in the entire rank-citation profile","  We present a simple generalization of Hirsch's h-index, Z =
\sqrt{h^{2}+C}/\sqrt{5}, where C is the total number of citations. Z is aimed
at correcting the potentially excessive penalty made by h on a scientist's
highly cited papers, because for the majority of scientists analyzed, we find
the excess citation fraction (C-h^{2})/C to be distributed closely around the
value 0.75, meaning that 75 percent of the author's impact is neglected.
Additionally, Z is less sensitive to local changes in a scientist's citation
profile, namely perturbations which increase h while only marginally affecting
C. Using real career data for 476 physicists careers and 488 biologist careers,
we analyze both the distribution of $Z$ and the rank stability of Z with
respect to the Hirsch index h and the Egghe index g. We analyze careers
distributed across a wide range of total impact, including top-cited physicists
and biologists for benchmark comparison. In practice, the Z-index requires the
same information needed to calculate h and could be effortlessly incorporated
within career profile databases, such as Google Scholar and ResearcherID.
Because Z incorporates information from the entire publication profile while
being more robust than h and g to local perturbations, we argue that Z is
better suited for ranking comparisons in academic decision-making scenarios
comprising a large number of scientists.
"
903,Categorizing Influential Authors Using Penalty Areas,"  The concept of h-index has been proposed to easily assess a researcher's
performance with a single two-dimensional number. However, by using only this
single number, we lose significant information about the distribution of the
number of citations per article of an author's publication list. Two authors
with the same h-index may have totally different distributions of the number of
citations per article. One may have a very long ""tail"" in the citation curve,
i.e. he may have published a great number of articles, which did not receive
relatively many citations. Another researcher may have a short tail, i.e.
almost all his publications got a relatively large number of citations. In this
article, we study an author's citation curve and we define some areas appearing
in this curve. These areas are used to further evaluate authors' research
performance from quantitative and qualitative point of view. We call these
areas as ""penalty"" ones, since the greater they are, the more an author's
performance is penalized. Moreover, we use these areas to establish new metrics
aiming at categorizing researchers in two distinct categories: ""influential""
ones vs. ""mass producers"".
"
904,"Tagging Scientific Publications using Wikipedia and Natural Language
  Processing Tools. Comparison on the ArXiv Dataset","  In this work, we compare two simple methods of tagging scientific
publications with labels reflecting their content. As a first source of labels
Wikipedia is employed, second label set is constructed from the noun phrases
occurring in the analyzed corpus. We examine the statistical properties and the
effectiveness of both approaches on the dataset consisting of abstracts from
0.7 million of scientific documents deposited in the ArXiv preprint collection.
We believe that obtained tags can be later on applied as useful document
features in various machine learning tasks (document similarity, clustering,
topic modelling, etc.).
"
905,"Statistiques et visibilit\'e des biblioth\`eques num\'eriques : quelles
  strat\'egies de diffusion ?","  We compared statistics of major digital libraries and we tried to see if
there is a relationship between the volume of digital libraries and online
visibility of each digitized document. Finally, we analyzed the consequences of
the diffusion strategies of French digital libraries. The statistics were
obtained by survey, gray literature, alexa.com, and Google Trends.
"
906,"nanoHUB.org: Experiences and Challenges in Software Sustainability for a
  Large Scientific Community","  Managing and growing a successful cyberinfrastructure such as nanoHUB.org
presents a variety of opportunities and challenges, particularly in regard to
software. This position paper details a number of those issues and how we have
approached them.
"
907,Social Interactive Media Tools and Knowledge Sharing: A Case Study,"  Purpose: Social Media Tools (SMT) have provided new opportunities for
libraries and librarians in the world. In academic libraries, we can use of
them as a powerful tool for communication. This study is to determine the use
of the social interactive media tools [Social Networking Tools (SNT), Social
Bookmarking Tools (SBT), Image or Video Sharing Tools (IVShT), and Mashup Tools
(MT)] in disseminating knowledge and information among librarians in the
Limerick University, Ireland. Methodology: The study was a descriptive survey.
The research population included all librarians in Glucksman library. A
questionnaire survey was done to collect data. Statistical software, SPSS16 was
used at two levels (descriptive and inferential statistics) for data analyzing.
Findings: The findings show that the mean (out of 5.00) of using each of SMT in
sharing knowledge by the librarians of Glucksman library is as the following:
SNT (2.49), SBT (2.92), IVShT (2.99) and MT (2.5). It shows that most of their
interaction related to share of image or video. Originality: SMT provides an
excellent platform for the exchange information between students, faculty
members, and the librarians themselves. The Glucksman library at the University
of Limerick is using this technology. This paper gives an example of how using
these tools in the field of Library and Information Science in Ireland. The
issues expressed could be beneficial for the development of library services in
general and knowledge sharing among librarians in particular.
"
908,Letter to the editor: Against the Resilience of Rejected Manuscripts,"  In this letter we propose the development of guidelines by the main editors
associations as well as protocols within online journal management systems for
keeping track of rejected manuscripts that are resubmitted as well as for the
interchange of referees reports between journals.
"
909,"The Google Scholar Experiment: how to index false papers and manipulate
  bibliometric indicators","  Google Scholar has been well received by the research community. Its promises
of free, universal and easy access to scientific literature as well as the
perception that it covers better than other traditional multidisciplinary
databases the areas of the Social Sciences and the Humanities have contributed
to the quick expansion of Google Scholar Citations and Google Scholar Metrics:
two new bibliometric products that offer citation data at the individual level
and at journal level. In this paper we show the results of a experiment
undertaken to analyze Google Scholar's capacity to detect citation counting
manipulation. For this, six documents were uploaded to an institutional web
domain authored by a false researcher and referencing all the publications of
the members of the EC3 research group at the University of Granada. The
detection of Google Scholar of these papers outburst the citations included in
the Google Scholar Citations profiles of the authors. We discuss the effects of
such outburst and how it could affect the future development of such products
not only at individual level but also at journal level, especially if Google
Scholar persists with its lack of transparency.
"
910,Just Google It - Digital Research Practices of Humanities Scholars,"  The transition from analogue to digital archives and the recent explosion of
online content offers researchers novel ways of engaging with data. The crucial
question for ensuring a balance between the supply and demand-side of data, is
whether this trend connects to existing scholarly practices and to the average
search skills of researchers. To gain insight into this process we conducted a
survey among nearly three hundred (N= 288) humanities scholars in the
Netherlands and Belgium with the aim of finding answers to the following
questions: 1) To what extent are digital databases and archives used? 2) What
are the preferences in search functionalities 3) Are there differences in
search strategies between novices and experts of information retrieval? Our
results show that while scholars actively engage in research online they mainly
search for text and images. General search systems such as Google and JSTOR are
predominant, while large-scale collections such as Europeana are rarely
consulted. Searching with keywords is the dominant search strategy and advanced
search options are rarely used. When comparing novice and more experienced
searchers, the first tend to have a more narrow selection of search engines,
and mostly use keywords. Our overall findings indicate that Google is the key
player among available search engines. This dominant use illustrates the
paradoxical attitude of scholars toward Google: while provenance and context
are deemed key academic requirements, the workings of the Google algorithm
remain unclear. We conclude that Google introduces a black box into digital
scholarly practices, indicating scholars will become increasingly dependent on
such black boxed algorithms. This calls for a reconsideration of the academic
principles of provenance and context.
"
911,Entitymetrics: Measuring the Impact of Entities,"  This paper proposes entitymetrics to measure the impact of knowledge units.
Entitymetrics highlight the importance of entities embedded in scientific
literature for further knowledge discovery. In this paper, we use Metformin, a
drug for diabetes, as an example to form an entity-entity citation network
based on literature related to Metformin. We then calculate the network
features and compare the centrality ranks of biological entities with results
from Comparative Toxicogenomics Database (CTD). The comparison demonstrates the
usefulness of entitymetrics to detect most of the outstanding interactions
manually curated in CTD.
"
912,Finding knowledge paths among scientific disciplines,"  This paper discovers patterns of knowledge dissemination among scientific
disciplines. While the transfer of knowledge is largely unobservable, citations
from one discipline to another have been proven to be an effective proxy to
study disciplinary knowledge flow. This study constructs a knowledge flow
network in that a node represents a Journal Citation Report subject category
and a link denotes the citations from one subject category to another. Using
the concept of shortest path, several quantitative measurements are proposed
and applied to a knowledge flow network. Based on an examination of subject
categories in Journal Citation Report, this paper finds that social science
domains tend to be more self-contained and thus it is more difficult for
knowledge from other domains to flow into them; at the same time, knowledge
from science domains, such as biomedicine-, chemistry-, and physics-related
domains can access and be accessed by other domains more easily. This paper
also finds that social science domains are more disunified than science
domains, as three fifths of the knowledge paths from one social science domain
to another need at least one science domain to serve as an intermediate. This
paper contributes to discussions on disciplinarity and interdisciplinarity by
providing empirical analysis.
"
913,"Resurrecting My Revolution: Using Social Link Neighborhood in Bringing
  Context to the Disappearing Web","  In previous work we reported that resources linked in tweets disappeared at
the rate of 11% in the first year followed by 7.3% each year afterwards. We
also found that in the first year 6.7%, and 14.6% in each subsequent year, of
the resources were archived in public web archives. In this paper we revisit
the same dataset of tweets and find that our prior model still holds and the
calculated error for estimating percentages missing was about 4%, but we found
the rate of archiving produced a higher error of about 11.5%. We also
discovered that resources have disappeared from the archives themselves (7.89%)
as well as reappeared on the live web after being declared missing (6.54%). We
have also tested the availability of the tweets themselves and found that
10.34% have disappeared from the live web. To mitigate the loss of resources on
the live web, we propose the use of a ""tweet signature"". Using the Topsy API,
we extract the top five most frequent terms from the union of all tweets about
a resource, and use these five terms as a query to Google. We found that using
tweet signatures results in discovering replacement resources with 70+% textual
similarity to the missing resource 41% of the time.
"
914,"Training in Data Curation as Service in a Federated Data Infrastructure
  - the FrontOffice-BackOffice Model","  The increasing volume and importance of research data leads to the emergence
of research data infrastructures in which data management plays an important
role. As a consequence, practices at digital archives and libraries change. In
this paper, we focus on a possible alliance between archives and libraries
around training activities in data curation. We introduce a so-called
\emph{FrontOffice--BackOffice model} and discuss experiences of its
implementation in the Netherlands. In this model, an efficient division of
tasks relies on a distributed infrastructure in which research institutions
(i.e., universities) use centralized storage and data curation services
provided by national research data archives. The training activities are aimed
at information professionals working at those research institutions, for
instance as digital librarians. We describe our experiences with the course
\emph{DataIntelligence4Librarians}. Eventually, we reflect about the
international dimension of education and training around data curation and
stewardship.
"
915,Revealing the intricate effect of collaboration on innovation,"  We study the Japan and U.S. patent records of several decades to demonstrate
the effect of collaboration on innovation. We find that statistically inventor
teams slightly outperform solo inventors while company teams perform equally
well as solo companies. By tracking the performance record of individual teams
we find that inventor teams' performance generally degrades with more repeat
collaborations. Though company teams' performance displays strongly bursty
behavior, long-term collaboration does not significantly help innovation at
all. To systematically study the effect of repeat collaboration, we define the
repeat collaboration number of a team as the average number of collaborations
over all the teammate pairs. We find that mild repeat collaboration improves
the performance of Japanese inventor teams and U.S. company teams. Yet,
excessive repeat collaboration does not significantly help innovation at both
the inventor and company levels in both countries. To control for unobserved
heterogeneity, we perform a detailed regression analysis and the results are
consistent with our simple observations. The presented results reveal the
intricate effect of collaboration on innovation, which may also be observed in
other creative projects.
"
916,"Inducing Honest Reporting Without Observing Outcomes: An Application to
  the Peer-Review Process","  When eliciting opinions from a group of experts, traditional devices used to
promote honest reporting assume that there is an observable future outcome. In
practice, however, this assumption is not always reasonable. In this paper, we
propose a scoring method built on strictly proper scoring rules to induce
honest reporting without assuming observable outcomes. Our method provides
scores based on pairwise comparisons between the reports made by each pair of
experts in the group. For ease of exposition, we introduce our scoring method
by illustrating its application to the peer-review process. In order to do so,
we start by modeling the peer-review process using a Bayesian model where the
uncertainty regarding the quality of the manuscript is taken into account.
Thereafter, we introduce our scoring method to evaluate the reported reviews.
Under the assumptions that reviewers are Bayesian decision-makers and that they
cannot influence the reviews of other reviewers, we show that risk-neutral
reviewers strictly maximize their expected scores by honestly disclosing their
reviews. We also show how the group's scores can be used to find a consensual
review. Experimental results show that encouraging honest reporting through the
proposed scoring method creates more accurate reviews than the traditional
peer-review process.
"
917,Mapping Mutable Genres in Structurally Complex Volumes,"  To mine large digital libraries in humanistically meaningful ways, scholars
need to divide them by genre. This is a task that classification algorithms are
well suited to assist, but they need adjustment to address the specific
challenges of this domain. Digital libraries pose two problems of scale not
usually found in the article datasets used to test these algorithms. 1) Because
libraries span several centuries, the genres being identified may change
gradually across the time axis. 2) Because volumes are much longer than
articles, they tend to be internally heterogeneous, and the classification task
needs to begin with segmentation. We describe a multi-layered solution that
trains hidden Markov models to segment volumes, and uses ensembles of
overlapping classifiers to address historical change. We test this approach on
a collection of 469,200 volumes drawn from HathiTrust Digital Library. To
demonstrate the humanistic value of these methods, we extract 32,209 volumes of
fiction from the digital library, and trace the changing proportions of first-
and third-person narration in the corpus. We note that narrative points of view
seem to have strong associations with particular themes and genres.
"
918,Profiling Web Archive Coverage for Top-Level Domain and Content Language,"  The Memento aggregator currently polls every known public web archive when
serving a request for an archived web page, even though some web archives focus
on only specific domains and ignore the others. Similar to query routing in
distributed search, we investigate the impact on aggregated Memento TimeMaps
(lists of when and where a web page was archived) by only sending queries to
archives likely to hold the archived page. We profile twelve public web
archives using data from a variety of sources (the web, archives' access logs,
and full-text queries to archives) and discover that only sending queries to
the top three web archives (i.e., a 75% reduction in the number of queries) for
any request produces the full TimeMaps on 84% of the cases.
"
919,Access Patterns for Robots and Humans in Web Archives,"  Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.
"
920,Who and What Links to the Internet Archive,"  The Internet Archive's (IA) Wayback Machine is the largest and oldest public
web archive and has become a significant repository of our recent history and
cultural heritage. Despite its importance, there has been little research about
how it is discovered and used. Based on web access logs, we analyze what users
are looking for, why they come to IA, where they come from, and how pages link
to IA. We find that users request English pages the most, followed by the
European languages. Most human users come to web archives because they do not
find the requested pages on the live web. About 65% of the requested archived
pages no longer exist on the live web. We find that more than 82% of human
sessions connect to the Wayback Machine via referrals from other web sites,
while only 15% of robots have referrers. Most of the links (86%) from websites
are to individual archived pages at specific points in time, and of those 83%
no longer exist on the live web.
"
921,"Developing a Robust Migration Workflow for Preserving and Curating
  Hand-held Media","  Many memory institutions hold large collections of hand-held media, which can
comprise hundreds of terabytes of data spread over many thousands of
data-carriers. Many of these carriers are at risk of significant physical
degradation over time, depending on their composition. Unfortunately, handling
them manually is enormously time consuming and so a full and frequent
evaluation of their condition is extremely expensive. It is, therefore,
important to develop scalable processes for stabilizing them onto backed-up
online storage where they can be subject to highquality digital preservation
management. This goes hand in hand with the need to establish efficient,
standardized ways of recording metadata and to deal with defective
data-carriers. This paper discusses processing approaches, workflows, technical
set-up, software solutions and touches on staffing needs for the stabilization
process. We have experimented with different disk copying robots, defined our
metadata, and addressed storage issues to scale stabilization to the vast
quantities of digital objects on hand-held data-carriers that need to be
preserved. Working closely with the content curators, we have been able to
build a robust data migration workflow and have stabilized over 16 terabytes of
data in a scalable and economical manner.
"
922,HOL(y)Hammer: Online ATP Service for HOL Light,"  HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)
mathematics encoded in the HOL Light system. The service allows its users to
upload and automatically process an arbitrary formal development (project)
based on HOL Light, and to attack arbitrary conjectures that use the concepts
defined in some of the uploaded projects. For that, the service uses several
automated reasoning systems combined with several premise selection methods
trained on all the project proofs. The projects that are readily available on
the server for such query answering include the recent versions of the
Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service
runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP
combinations and 4 decision procedures that contribute to its overall
performance. The system is also available for local installation by interested
users, who can customize it for their own proof development. An Emacs interface
allowing parallel asynchronous queries to the service is also provided. The
overall structure of the service is outlined, problems that arise and their
solutions are discussed, and an initial account of using the system is given.
"
923,"Author Name Co-Mention Analysis: Testing a Poor Man's Author Co-Citation
  Analysis Method","  As a social science information service for the German language countries, we
document research projects, publications, and data in relevant fields. At the
same time, we aim to provide well-founded bibliometric studies of these fields.
Performing a citation analysis on an area of the German social sciences is,
however, a serious challenge given the low and likely significantly biased
coverage of these fields in the standard citation databases. Citations, and
especially author citations, play a highly significant role in that literature,
however. In this work in progress, we report preliminary methods and results
for an author name co-mention analysis of a large fragment of a particularly
interesting corpus of German sociology: a quarter century's worth of the
full-text proceedings of the Deutsche Gesellschaft fuer Soziologie (DGS), which
celebrated its 100th anniversary meeting in 2012. Results are encouraging for
this poor cousin of author co-citation analysis, but considerable refinements,
especially of the underlying computational infrastructure for full-text
analysis, appear advisable for full-scale deployment of this method.
"
924,"An open dataset for research on audio field recording archives:
  freefield1010","  We introduce a free and open dataset of 7690 audio clips sampled from the
field-recording tag in the Freesound audio archive. The dataset is designed for
use in research related to data mining in audio archives of field recordings /
soundscapes. Audio is standardised, and audio and metadata are Creative Commons
licensed. We describe the data preparation process, characterise the dataset
descriptively, and illustrate its use through an auto-tagging experiment.
"
925,Extensible Generic Data Management Software,"  Extensibility mechanisms constitute a form of knowledge capture that is
essential for software re-use. The Data Intensive Cyber Environments (DICE)
group has collaborated with twenty-five science and engineering domains on the
application of the iRODS policy-based data management system. Based on these
collaborations, three types of extensibility mechanisms are sufficient to
capture the domain knowledge needed for interaction with domain resources:
computer actionable rules that control management policies; computer executable
micro-services that encapsulate operations or interaction protocols; and
middleware servers that apply standard operations at remote locations. These
mechanisms enable the creation of generic data management software that is
capable of implementing collections, data grids for sharing data, digital
libraries for publishing data, processing pipelines, and archives.
"
926,"Evaluating Sliding and Sticky Target Policies by Measuring Temporal
  Drift in Acyclic Walks Through a Web Archive","  When a user views an archived page using the archive's user interface (UI),
the user selects a datetime to view from a list. The archived web page, if
available, is then displayed. From this display, the web archive UI attempts to
simulate the web browsing experience by smoothly transitioning between archived
pages. During this process, the target datetime changes with each link
followed; drifting away from the datetime originally selected. When browsing
sparsely-archived pages, this nearly-silent drift can be many years in just a
few clicks. We conducted 200,000 acyclic walks of archived pages, following up
to 50 links per walk, comparing the results of two target datetime policies.
The Sliding Target policy allows the target datetime to change as it does in
archive UIs such as the Internet Archive's Wayback Machine. The Sticky Target
policy, represented by the Memento API, keeps the target datetime the same
throughout the walk. We found that the Sliding Target policy drift increases
with the number of walk steps, number of domains visited, and choice (number of
links available). However, the Sticky Target policy controls temporal drift,
holding it to less than 30 days on average regardless of walk length or number
of domains visited. The Sticky Target policy shows some increase as choice
increases, but this may be caused by other factors. We conclude that based on
walk length, the Sticky Target policy generally produces at least 30 days less
drift than the Sliding Target policy.
"
927,"Modeling nonuniversal citation distributions: the role of scientific
  journals","  Whether a scientific paper is cited is related not only to the influence of
its author(s) but also to the journal publishing it. Scientists, either
proficient or tender, usually submit their most important work to prestigious
journals which receives higher citations than the ordinary. How to model the
role of scientific journals in citation dynamics is of great importance. In
this paper we address this issue through two folds. One is the intrinsic
heterogeneity of a paper determined by the impact factor of the journal
publishing it. The other is the mechanism of a paper being cited which depends
on its citations and prestige. We develop a model for citation networks via an
intrinsic nodal weight function and an intuitive ageing mechanism. The node's
weight is drawn from the distribution of impact factors of journals and the
ageing transition is a function of the citation and the prestige. The
node-degree distribution of resulting networks shows nonuniversal scaling: the
distribution decays exponentially for small degree and has a power-law tail for
large degree, hence the dual behaviour. The higher the impact factor of the
journal, the larger the tipping point and the smaller the power exponent that
are obtained. With the increase of the journal rank, this phenomenon will fade
and evolve to pure power laws.
"
928,"Referenced Publication Years Spectroscopy applied to iMetrics:
  Scientometrics, Journal of Informetrics, and a relevant subset of JASIST","  We have developed a (freeware) routine for ""referenced publication years
spectroscopy"" (RPYS) and apply this method to the historiography of ""iMetrics,""
that is, the junction of the journals Scientometrics, Informetrics, and the
relevant subset of JASIST (approx. 20%) that shapes the intellectual space for
the development of information metrics (bibliometrics, scientometrics,
informetrics, and webometrics). The application to information metrics (our own
field of research) provides us with the opportunity to validate this
methodology, and to add a reflection about using citations for the historical
reconstruction. The results show that the field is rooted in individual
contributions of the 1920s-1950s (e.g., Alfred J. Lotka), and was then shaped
intellectually in the early 1960s by a confluence of the history of science
(Derek de Solla Price), documentation (e.g., Michael M. Kessler's
""bibliographic coupling""), and ""citation indexing"" (Eugene Garfield).
Institutional development at the interfaces between science studies and
information science has been reinforced by the new journal Informetrics since
2007. In a concluding reflection, we return to the question of how the
historiography of science using algorithmic means--in terms of citation
practices--can be different from an intellectual history of the field based,
for example, on reading source materials.
"
929,Describing Papers and Reviewers' Competences by Taxonomy of Keywords,"  This article focuses on the importance of the precise calculation of
similarity factors between papers and reviewers for performing a fair and
accurate automatic assignment of reviewers to papers. It suggests that papers
and reviewers' competences should be described by taxonomy of keywords so that
the implied hierarchical structure allows similarity measures to take into
account not only the number of exactly matching keywords, but in case of
non-matching ones to calculate how semantically close they are. The paper also
suggests a similarity measure derived from the well-known and widely-used
Dice's coefficient, but adapted in a way it could be also applied between sets
whose elements are semantically related to each other (as concepts in taxonomy
are). It allows a non-zero similarity factor to be accurately calculated
between a paper and a reviewer even if they do not share any keyword in common.
"
930,An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching,"  Ontologies are considered as the backbone of the Semantic Web. With the
rising success of the Semantic Web, the number of participating communities
from different countries is constantly increasing. The growing number of
ontologies available in different natural languages leads to an
interoperability problem. In this paper, we discuss several approaches for
ontology matching; examine similarities and differences, identify weaknesses,
and compare the existing automated approaches with the manual approaches for
integrating multilingual ontologies. In addition to that, we propose a new
architecture for a multilingual ontology matching service. As a case study we
used an example of two multilingual enterprise ontologies - the university
ontology of Freie Universitaet Berlin and the ontology for Fayoum University in
Egypt.
"
931,On the Internal Dynamics of the Shanghai Ranking,"  The Academic Ranking of World Universities (ARWU) published by researchers at
Shanghai Jiao Tong University has become a major source of information for
university administrators, country officials, students and the public at large.
Recent discoveries regarding its internal dynamics allow the inversion of
published ARWU indicator scores to reconstruct raw scores for five hundred
world class universities. This paper explores raw scores in the ARWU and in
other contests to contrast the dynamics of rank-driven and score-driven tables,
and to explain why the ARWU ranking is a score-driven procedure. We show that
the ARWU indicators constitute sub-scales of a single factor accounting for
research performance, and provide an account of the system of gains and
non-linearities used by ARWU. The paper discusses the non-linearities selected
by ARWU, concluding that they are designed to represent the regressive
character of indicators measuring research performance. We propose that the
utility and usability of the ARWU could be greatly improved by replacing the
unwanted dynamical effects of the annual re-scaling based on raw scores of the
best performers.
"
932,An Efficient Authorship Protection Scheme for Shared Multimedia Content,"  Many electronic content providers today like Flickr and Google, offer space
to users to publish their electronic media (e.g. photos and videos) in their
cloud infrastructures, so that they can be publicly accessed. Features like
including other information, such as keywords or owner information into the
digital material is already offered by existing providers. Despite the useful
features made available to users by such infrastructures, the authorship of the
published content is not protected against various attacks such as compression.
In this paper we propose a robust scheme that uses digital invisible
watermarking and hashing to protect the authorship of the digital content and
provide resistance against malicious manipulation of multimedia content. The
scheme is enhanced by an algorithm called MMBEC, that is an extension of an
established scheme MBEC, towards higher resistance.
"
933,"Research misconduct: definitions, manifestations and extent","  In recent years, the international scientific community has been rocked by a
number of serious cases of research misconduct. In one of these, Woo Suk Hwang,
a Korean stem cell researcher published two articles on research with
ground-breaking results in Science in 2004 and 2005. Both articles were later
revealed to be fakes. This paper provides an overview of what research
misconduct is generally understood to be, its manifestations and the extent to
which they are thought to exist.
"
934,"Bibliometric-enhanced Retrieval Models for Big Scholarly Information
  Systems","  Bibliometric techniques are not yet widely used to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. In this paper we will explore how statistical modelling of scholarship,
such as Bradfordizing or network analysis of coauthorship network, can improve
retrieval services for specific communities, as well as for large, cross-domain
large collections. This paper aims to raise awareness of the missing link
between information retrieval (IR) and bibliometrics / scientometrics and to
create a common ground for the incorporation of bibliometric-enhanced services
into retrieval at the digital library interface.
"
935,Use of Solr and Xapian in the Invenio document repository software,"  Invenio is a free comprehensive web-based document repository and digital
library software suite originally developed at CERN. It can serve a variety of
use cases from an institutional repository or digital library to a web journal.
In order to fully use full-text documents for efficient search and ranking,
Solr was integrated into Invenio through a generic bridge. Solr indexes
extracted full-texts and most relevant metadata. Consequently, Invenio takes
advantage of Solr's efficient search and word similarity ranking capabilities.
In this paper, we first give an overview of Invenio, its capabilities and
features. We then present our open source Solr integration as well as
scalability challenges that arose for an Invenio-based multi-million record
repository: the CERN Document Server. We also compare our Solr adapter to an
alternative Xapian adapter using the same generic bridge. Both integrations are
distributed with the Invenio package and ready to be used by the institutions
using or adopting Invenio.
"
936,"A Conceptual Network-Based Approach to Inferring the Cultural
  Evolutionary History of the Baltic Psaltery","  The application of conventional phylogenetic techniques for inferring
cultural history is problematic due to differences in the nature of information
transmission in biological and cultural realms. In culture, units of
transmission are not just measurable attributes, but communicable concepts.
Therefore, relatedness amongst cultural elements often resides at the
conceptual level not captured by traditional phylogenetic methods. This paper
takes a cognitively inspired approach to analyzing material cultural history.
We show that combining data for physical attributes of cultural artifacts with
conceptual information can uncover cultural influences among different
ethnolinguistic groups, and reveal new patterns of cultural ancestry. Using the
Baltic psaltery, a musical instrument with a well-documented ethnographic and
archaeological record, we recovered a previously unacknowledged pattern of
historical relationship that is more congruent with geographical distribution
and temporal data than is obtained with other approaches.
"
937,"Collaboration networks from a large CV database: dynamics, topology and
  bonus impact","  Understanding the dynamics of research production and collaboration may
reveal better strategies for scientific careers, academic institutions and
funding agencies. Here we propose the use of a large and multidisciplinar
database of scientific curricula in Brazil, namely, the Lattes Platform, to
study patterns of scientific production and collaboration. In this database,
detailed information about publications and researchers are made available by
themselves so that coauthorship is unambiguous and individuals can be evaluated
by scientific productivity, geographical location and field of expertise. Our
results show that the collaboration network is growing exponentially for the
last three decades, with a distribution of number of collaborators per
researcher that approaches a power-law as the network gets older. Moreover,
both the distributions of number of collaborators and production per researcher
obey power-law behaviors, regardless of the geographical location or field,
suggesting that the same universal mechanism might be responsible for network
growth and productivity.We also show that the collaboration network under
investigation displays a typical assortative mixing behavior, where teeming
researchers (i.e., with high degree) tend to collaborate with others alike.
Finally, our analysis reveals that the distinctive collaboration profile of
researchers awarded with governmental scholarships suggests a strong bonus
impact on their productivity.
"
938,First Author Advantage: Citation Labeling in Research,"  Citations among research papers, and the networks they form, are the primary
object of study in scientometrics. The act of making a citation reflects the
citer's knowledge of the related literature, and of the work being cited. We
aim to gain insight into this process by studying citation keys: user-chosen
labels to identify a cited work. Our main observation is that the first listed
author is disproportionately represented in such labels, implying a strong
mental bias towards the first author.
"
939,Lemma Mining over HOL Light,"  Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of all lemmas in the core HOL Light library,
adding thousands of the best lemmas to the pool of named statements that can be
re-used in later proofs. The usefulness of the new lemmas is then evaluated by
comparing the performance of automated proving of the core HOL Light theorems
with and without such added lemmas.
"
940,MizAR 40 for Mizar 40,"  As a present to Mizar on its 40th anniversary, we develop an AI/ATP system
that in 30 seconds of real time on a 14-CPU machine automatically proves 40% of
the theorems in the latest official version of the Mizar Mathematical Library
(MML). This is a considerable improvement over previous performance of large-
theory AI/ATP methods measured on the whole MML. To achieve that, a large suite
of AI/ATP methods is employed and further developed. We implement the most
useful methods efficiently, to scale them to the 150000 formulas in MML. This
reduces the training times over the corpus to 1-3 seconds, allowing a simple
practical deployment of the methods in the online automated reasoning service
for the Mizar users (MizAR).
"
941,"Talking With Scholars: Developing a Research Environment for Oral
  History Collections","  Scholars are yet to make optimal use of Oral History collections. For the
uptake of digital research tools in the daily working practice of researchers,
practices and conventions commonly adhered to in the subfields in the
humanities should be taken into account during development. To this end, in the
Oral History Today project a research tool for exploring Oral History
collections is developed in close collaboration with scholarly researchers.
This paper describes four stages of scholarly research and the first steps
undertaken to incorporate requirements of these stages in a digital research
environment.
"
942,Pennants for Descriptors,"  We present a new technique (called pennants) for displaying the descriptors
related to a descriptor across literatures, rather in a thesaurus. It has
definite implications for online searching and browsing. Pennants, named for
the flag they resemble, are a form of algorithmic prediction. Their cognitive
base is in relevance theory (RT) from linguistic pragmatics (Sperber & Wilson
1995).
"
943,Validation Rules for Assessing and Improving SKOS Mapping Quality,"  The Simple Knowledge Organization System (SKOS) is popular for expressing
controlled vocabularies, such as taxonomies, classifications, etc., for their
use in Semantic Web applications. Using SKOS, concepts can be linked to other
concepts and organized into hierarchies inside a single terminology system.
Meanwhile, expressing mappings between concepts in different terminology
systems is also possible. This paper discusses potential quality issues in
using SKOS to express these terminology mappings. Problematic patterns are
defined and corresponding rules are developed to automatically detect
situations where the mappings either result in 'SKOS Vocabulary Hijacking' to
the source vocabularies or cause conflicts. An example of using the rules to
validate sample mappings between two clinical terminologies is given. The
validation rules, expressed in N3 format, are available as open source.
"
944,"GV-Index:Scientific Contribution Rating Index That Takes into Account
  the Growth Degree of Research Area and Variance Values of the Publication
  Year of Cited Paper","  There are a wide variety of scientific contribution rating indices including
the impact factor and h-index. These are used for quantitative analyses on
research papers published in the past, and therefore unable to incorporate in
the assessment the growth, or deterioration, of the research area: whether the
research area of a particular paper is in decline or conversely in a growing
trend. Other hand, the use of the conventional rating indices may result in
higher rates for papers that are hardly referenced nowadays in other papers
although frequently cited in the past. This study proposes a new type of
scientific contribution ranking index, ""Growing Degree of Research Area and
Variance Values Index (GV-Index)"". The GV-Index is computed by a principal
component analysis based on an estimated value obtained by PageRank Algorithm,
which takes into account the growing degree of the research area and its
variance. We also propose visualization system of a scientist's network using
the GV-Index.
"
945,"Dynamic Extraction of Key Paper from the Cluster Using Variance Values
  of Cited Literature","  When looking into recent research trends in the field of academic landscape,
citation network analysis is common and automated clustering of many academic
papers has been achieved by making good use of various techniques. However,
specifying the features of each area identified by automated clustering or
dynamically extracted key papers in each research area has not yet been
achieved. In this study, therefore, we propose a method for dynamically
specifying the key papers in each area identified by clustering. We will
investigate variance values of the publication year of the cited literature and
calculate each cited paper's importance by applying the variance values to the
PageRank algorithm.
"
946,Text Classification For Authorship Attribution Analysis,"  Authorship attribution mainly deals with undecided authorship of literary
texts. Authorship attribution is useful in resolving issues like uncertain
authorship, recognize authorship of unknown texts, spot plagiarism so on.
Statistical methods can be used to set apart the approach of an author
numerically. The basic methodologies that are made use in computational
stylometry are word length, sentence length, vocabulary affluence, frequencies
etc. Each author has an inborn style of writing, which is particular to
himself. Statistical quantitative techniques can be used to differentiate the
approach of an author in a numerical way. The problem can be broken down into
three sub problems as author identification, author characterization and
similarity detection. The steps involved are pre-processing, extracting
features, classification and author identification. For this different
classifiers can be used. Here fuzzy learning classifier and SVM are used. After
author identification the SVM was found to have more accuracy than Fuzzy
classifier. Later combined the classifiers to obtain a better accuracy when
compared to individual SVM and fuzzy classifier.
"
947,"Journal Maps, Interactive Overlays, and the Measurement of
  Interdisciplinarity on the Basis of Scopus Data (1996-2012)","  Using Scopus data, we construct a global map of science based on aggregated
journal-journal citations from 1996-2012 (N of journals = 20,554). This base
map enables users to overlay downloads from Scopus interactively. Using a
single year (e.g., 2012), results can be compared with mappings based on the
Journal Citation Reports at the Web-of-Science (N = 10,936). The Scopus maps
are more detailed at both the local and global levels because of their greater
coverage, including, for example, the arts and humanities. The base maps can be
interactively overlaid with journal distributions in sets downloaded from
Scopus, for example, for the purpose of portfolio analysis. Rao-Stirling
diversity can be used as a measure of interdisciplinarity in the sets under
study. Maps at the global and the local level, however, can be very different
because of the different levels of aggregation involved. Two journals, for
example, can both belong to the humanities in the global map, but participate
in different specialty structures locally. The base map and interactive tools
are available online (with instructions) at
http://www.leydesdorff.net/scopus_ovl.
"
948,"CIDS country rankings: comparing documents and citations of USA, UK and
  China top researchers","  This technical report presents a bibliometric analysis of the top 30 cited
researchers from USA, UK and China. The analysis is based on Google Scholar
data using CIDS. The researchers were identified using their email suffix: edu,
uk and cn. This na\""{i}ve approach was able to produce rankings consistent with
the SCImago country rankings using mininal resources in a fully automated way.
"
949,Google matrix of the citation network of Physical Review,"  We study the statistical properties of spectrum and eigenstates of the Google
matrix of the citation network of Physical Review for the period 1893 - 2009.
The main fraction of complex eigenvalues with largest modulus is determined
numerically by different methods based on high precision computations with up
to $p=16384$ binary digits that allows to resolve hard numerical problems for
small eigenvalues. The nearly nilpotent matrix structure allows to obtain a
semi-analytical computation of eigenvalues. We find that the spectrum is
characterized by the fractal Weyl law with a fractal dimension $d_f \approx 1$.
It is found that the majority of eigenvectors are located in a localized phase.
The statistical distribution of articles in the PageRank-CheiRank plane is
established providing a better understanding of information flows on the
network. The concept of ImpactRank is proposed to determine an influence domain
of a given article. We also discuss the properties of random matrix models of
Perron-Frobenius operators.
"
950,Exploring Scientists' Working Timetable: A Global Survey,"  In our previous study (Wang et al., 2012), we analyzed scientists' working
timetable of 3 countries, using realtime downloading data of scientific
literatures. In this paper, we make a through analysis about global scientists'
working habits. Top 30 countries/territories from Europe, Asia, Australia,
North America, Latin America and Africa are selected as representatives and
analyzed in detail. Regional differences for scientists' working habits exists
in different countries. Besides different working cultures, social factors
could affect scientists' research activities and working patterns.
Nevertheless, a common conclusion is that scientists today are often working
overtime. Although scientists may feel engaged and fulfilled about their hard
working, working too much still warns us to reconsider the work - life balance.
"
951,"U.S. academic libraries: understanding their web presence and their
  relationship with economic indicators","  The main goal of this research is to analyze the web structure and
performance of units and services belonging to U.S. academic libraries in order
to check their suitability for webometric studies. Our objectives include
studying their possible correlation with economic data and assessing their use
for complementary evaluation purposes. We conducted a survey of library
homepages, institutional repositories, digital collections, and online catalogs
(a total of 374 URLs) belonging to the 100 U.S. universities with the highest
total expenditures in academic libraries according to data provided by the
National Center for Education Statistics (NCES). Several data points were taken
and analyzed, including web variables (page count, external links, and visits)
and economic variables (total expenditures, expenditures on printed and
electronic books, and physical visits). The results indicate that the variety
of URL syntaxes is wide, diverse and complex, which produces a
misrepresentation of academic library web resources and reduces the accuracy of
web analysis. On the other hand, institutional and web data indicators are not
highly correlated. Better results are obtained by correlating total library
expenditures with URL mentions measured by Google (r= 0.546) and visits
measured by Compete (r= 0.573), respectively. Because correlation values
obtained are not highly significant, we estimate such correlations will
increase if users can avoid linkage problems (due to the complexity of URLs)
and gain direct access to log files (for more accurate data about visits).
"
952,"Aggregation of the web performance of internal university units as a
  method of quantitative analysis of a university system: the case of Spain","  The aggregation of web performance (page count and visibility) of internal
university units could constitute a more precise indicator than the overall web
performance of the universities and, therefore, be of use in the design of
university web rankings. In order to test this hypothesis, a longitudinal
analysis of the internal units of the Spanish university system was conducted
over the course of 2010. For the 13800 URLs identified, page count and
visibility was calculated using the Yahoo API. The internal values obtained
were aggregated by university and compared with the values obtained from the
analysis of the university general URLs. The results indicate that, although
the correlations between general and internal values are high, internal
performance is low in comparison to general performance, and that they give
rise to different performance rankings. The conclusion is that the aggregation
of unit performance is of limited use due to the low levels of internal
development of the websites, and so its use is not recommended for the design
of rankings. Despite this, the internal analysis enabled the detection of,
amongst other things, a low correlation between page count and visibility due
to the widespread use of subdirectories and problems accessing certain content.
"
953,"Selective linking from social platforms to university websites: a case
  study of the Spanish academic system","  Mention indicators have frequently been used in Webometric studies because
they provide a powerful tool for determining the degree of visibility and
impact of web resources. Among mention indicators, hypertextual links were a
central part of many studies until Yahoo discontinued the linkdomain command in
2011. Selective links constitute a variant of external links where both the
source and target of the link can be selected. This paper intends to study the
influence of social platforms (measured through the number of selective
external links) on academic environments, in order to ascertain both the
percentage that they constitute and whether some of them can be used as
substitutes of total external links. For this purpose, 141 URLs belonging to 76
Spanish universities were compiled in 2010 (before Yahoo! stopped their link
services), and the number of links from 13 selected social platforms to these
universities were calculated. Results confirm a good correlation between total
external links and links that come from social platforms, with the exception of
some applications (such as Digg and Technorati). For those universities with a
higher number of total external links, the high correlation is only maintained
on Delicious and Wikipedia, which can be utilized as substitutes of total
external links in the context analyzed. Notwithstanding, the global percentage
of links from social platforms constitute only a small fraction of total links,
although a positive trend is detected, especially in services such as Twitter,
Youtube, and Facebook.
"
954,Proposal for a multilevel university cybermetric analysis model,"  University online seats have gradually become complex systems of dynamic
information where all their institutions and services are linked and
potentially accessible. These online seats now constitute a central node around
which universities construct and document their main activities and services.
This information can be quantitative measured by cybermetric techniques in
order to design university web rankings, taking the university as a global
reference unit. However, previous research into web subunits shows that it is
possible to carry out systemic web analyses, which open up the possibility of
carrying out studies which address university diversity, necessary for both
describing the university in greater detail and for establishing comparable
ranking units. To address this issue, a multilevel university cybermetric
analysis model is proposed, based on parts (core and satellite), levels
(institutional and external) and sublevels (contour and internal), providing a
deeper analysis of institutions. Finally the model is integrated into another
which is independent of the technique used, and applied by analysing Harvard
University as an example of use.
"
955,"What do university rankings by fields rank? Exploring discrepancies
  between the organizational structure of universities and bibliometric
  classifications","  University rankings by fields are usually based on the research output of
universities. However, research managers and rankings consumers expect to see
in such fields a reflection of the structure of their own organizational
institution. In this study we address such misinterpretation by developing the
research profile of the organizational units of two Spanish universities:
University of Granada and Pompeu Fabra University. We use two classification
systems, the subject categories offered by Thomson Scientific which are
commonly used on bibliometric studies, and the 37 disciplines displayed by the
Spanish I-UGR Rankings which are constructed from an aggregation of the former.
We also describe in detail problems encountered when working with address data
from a top down approach and we show differences between universities
structures derived from the interdisciplinary organizational forms of new
managerialism at universities. We conclude by highlighting that rankings by
fields should clearly state the methodology for the construction of such
fields. We indicate that the construction of research profiles may be a good
solution for universities for finding out levels of discrepancy between
organizational units and subject fields.
"
956,Google Scholar Metrics evolution: an analysis according to languages,"  In November 2012 the Google Scholar Metrics (GSM) journal rankings were
updated, making it possible to compare bibliometric indicators in the 10
languages indexed and their stability with the April 2012 version. The h-index
and h 5 median of 1000 journals were analysed, comparing their averages,
maximum and minimum values and the correlation coefficient within rankings. The
bibliometric figures grew significantly. In just seven and a half months the h
index of the journals increased by 15% and the median h-index by 17%. This
growth was observed for all the bibliometric indicators analysed and for
practically every journal. However, we found significant differences in growth
rates depending on the language in which the journal is published. Moreover,
the journal rankings seem to be stable between April and November, reinforcing
the credibility of the data held by Google Scholar and the reliability of the
GSM journal rankings, despite the uncontrolled growth of Google Scholar. Based
on the findings of this study we suggest, firstly, that Google should upgrade
its rankings at least semiannually and, secondly, that the results should be
displayed in each ranking proportionally to the number of journals indexed by
language
"
957,Web Annotation as a First Class Object,"  Scholars have made handwritten notes and comments in books and manuscripts
for centuries. Today's blogs and news sites typically invite users to express
their opinions on the published content; URLs allow web resources to be shared
with accompanying annotations and comments using third-party services like
Twitter or Facebook. These contributions have until recently been constrained
within specific services, making them second-class citizens of the Web.
  Web Annotations are now emerging as fully independent Linked Data in their
own right, no longer restricted to plain textual comments in application silos.
Annotations can now range from bookmarks and comments, to fine-grained
annotations of a selection of, for example, a section of a frame within a video
stream. Technologies and standards now exist to create, publish, syndicate,
mash-up and consume, finely targeted, semantically rich digital annotations on
practically any content, as first-class Web citizens. This development is being
driven by the need for collaboration and annotation reuse amongst domain
researchers, computer scientists, scientific publishers, and scholarly content
databases.
"
958,"H Index Communication Journals according to Google Scholar Metrics
  (2008-2012)","  The aim of this report is to present a ranking of Communication journals
covered in Google Scholar Metrics for the period 2008-2012. It corresponds to
the H Index update made last year for the period 2007-2011 (Delgado
L\'opez-C\'ozar and Repiso 2013). Google Scholar Metrics doesnt currently allow
to group and sort all journals belonging to a scientific discipline. In the
case of Communication, in the ten listings displayed by GSM we can only locate
46 journals. Therefore, in an attempt to overcome this limitation, we have used
the diversity of search procedures allowed by GSM to identify the greatest
number of scientific journals of Communication with H Index calculated by this
bibliometric tool. The result is a ranking of 354 communication journals sorted
by the same H Index, and mean as discriminating value. Journals are also
grouped by quartiles.
"
959,Prediction of highly cited papers,"  In an article written five years ago [arXiv:0809.0522], we described a method
for predicting which scientific papers will be highly cited in the future, even
if they are currently not highly cited. Applying the method to real citation
data we made predictions about papers we believed would end up being well
cited. Here we revisit those predictions, five years on, to see how well we
did. Among the over 2000 papers in our original data set, we examine the fifty
that, by the measures of our previous study, were predicted to do best and we
find that they have indeed received substantially more citations in the
intervening years than other papers, even after controlling for the number of
prior citations. On average these top fifty papers have received 23 times as
many citations in the last five years as the average paper in the data set as a
whole, and 15 times as many as the average paper in a randomly drawn control
group that started out with the same number of citations. Applying our
prediction technique to current data, we also make new predictions of papers
that we believe will be well cited in the next few years.
"
960,Transitive Reduction of Citation Networks,"  In many complex networks the vertices are ordered in time, and edges
represent causal connections. We propose methods of analysing such directed
acyclic graphs taking into account the constraints of causality and
highlighting the causal structure. We illustrate our approach using citation
networks formed from academic papers, patents, and US Supreme Court verdicts.
We show how transitive reduction reveals fundamental differences in the
citation practices of different areas, how it highlights particularly
interesting work, and how it can correct for the effect that the age of a
document has on its citation count. Finally, we transitively reduce null models
of citation networks with similar degree distributions and show the difference
in degree distributions after transitive reduction to illustrate the lack of
causal structure in such models.
"
961,Bibliometric-enhanced Information Retrieval,"  Bibliometric techniques are not yet widely used to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. In this workshop we will explore how statistical modelling of
scholarship, such as Bradfordizing or network analysis of coauthorship network,
can improve retrieval services for specific communities, as well as for large,
cross-domain collections. This workshop aims to raise awareness of the missing
link between information retrieval (IR) and bibliometrics/scientometrics and to
create a common ground for the incorporation of bibliometric-enhanced services
into retrieval at the digital library interface.
"
962,"The distorted mirror of Wikipedia: a quantitative analysis of Wikipedia
  coverage of academics","  Activity of modern scholarship creates online footprints galore. Along with
traditional metrics of research quality, such as citation counts, online images
of researchers and institutions increasingly matter in evaluating academic
impact, decisions about grant allocation, and promotion. We examined 400
biographical Wikipedia articles on academics from four scientific fields to
test if being featured in the world's largest online encyclopedia is correlated
with higher academic notability (assessed through citation counts). We found no
statistically significant correlation between Wikipedia articles metrics
(length, number of edits, number of incoming links from other articles, etc.)
and academic notability of the mentioned researchers. We also did not find any
evidence that the scientists with better WP representation are necessarily more
prominent in their fields. In addition, we inspected the Wikipedia coverage of
notable scientists sampled from Thomson Reuters list of ""highly cited
researchers"". In each of the examined fields, Wikipedia failed in covering
notable scholars properly. Both findings imply that Wikipedia might be
producing an inaccurate image of academics on the front end of science. By
shedding light on how public perception of academic progress is formed, this
study alerts that a subjective element might have been introduced into the
hitherto structured system of academic evaluation.
"
963,Motivation for hyperlink creation using inter-page relationships,"  Using raw hyperlink counts for webometrics research has been shown to be
unreliable and researchers have looked for alternatives. One alternative is
classifying hyperlinks in a website based on the motivation behind the
hyperlink creation. The method used for this type of classification involves
manually visiting a webpage and then classifying individual links on the
webpage. This is time consuming, making it infeasible for large scale studies.
This paper speeds up the classification of hyperlinks in UK academic websites
by using a machine learning technique, decision tree induction, to group web
pages found in UK academic websites into one of eight categories and then infer
the motivation for the creation of a hyperlink in a webpage based on the
linking pattern of the category the webpage belongs to.
"
964,Is modern science evolving in the wrong direction?,"  The present politically correct consensus is that increased exchange of
scientific insight, knowledge, practitioners and skills at the global level
brings significant benefits to all. The quantifiable scientometric changes
during the last decade, however, suggest that many areas of knowledge are
evolving in the opposite direction. Despite an increase during the last decade
of the numbers of journals and academic articles published, increases in the
number of citations the published articles receive, and increases in the number
of countries participating; important parts of the academic activity are
becoming more nationalistic. In addition, international collaboration is
decreasing in several subject areas, and in several geographic regions. For
example, countries in Asia are becoming scientifically more isolated; and
academics working in the humanities in all the regions of the world are very
nationalistic and are becoming more so. The precise consequences of this
dynamics are difficult to predict, but it certainly will have reverberations
beyond academia. The tendency of the humanities to become more provincial will
certainly not help in reducing international conflicts arising from poor
understanding of cultural differences and of diverging sociopolitical world
views. More and better data on these trends should give us a better
understanding for eventually improving academic policies worldwide.
  Keywords: International Collaboration, National, Science, Humanities, Policy
"
965,A Semi-automated Peer-review System,"  A semi-supervised model of peer review is introduced that is intended to
overcome the bias and incompleteness of traditional peer review. Traditional
approaches are reliant on human biases, while consensus decision-making is
constrained by sparse information. Here, the architecture for one potential
improvement (a semi-supervised, human-assisted classifier) to the traditional
approach will be introduced and evaluated. To evaluate the potential advantages
of such a system, hypothetical receiver operating characteristic (ROC) curves
for both approaches will be assessed. This will provide more specific
indications of how automation would be beneficial in the manuscript evaluation
process. In conclusion, the implications for such a system on measurements of
scientific impact and improving the quality of open submission repositories
will be discussed.
"
966,"Structuring research methods and data with the Research Object model:
  genomics workflows as a case study","  One of the main challenges for biomedical research lies in the
computer-assisted integrative study of large and increasingly complex
combinations of data in order to understand molecular mechanisms. The
preservation of the materials and methods of such computational experiments
with clear annotations is essential for understanding an experiment, and this
is increasingly recognized in the bioinformatics community. Our assumption is
that offering means of digital, structured aggregation and annotation of the
objects of an experiment will provide necessary meta-data for a scientist to
understand and recreate the results of an experiment. To support this we
explored a model for the semantic description of a workflow-centric Research
Object (RO), where an RO is defined as a resource that aggregates other
resources, e.g., datasets, software, spreadsheets, text, etc. We applied this
model to a case study where we analysed human metabolite variation by
workflows.
"
967,"Contribution of Information and Communication Technology (ICT) in
  Country'S H-Index","  The aim of this study is to examine the effect of Information and
Communication Technology (ICT) development on country's scientific ranking as
measured by H-index. Moreover, this study applies ICT development sub-indices
including ICT Use, ICT Access and ICT skill to find the distinct effect of
these sub-indices on country's H-index. To this purpose, required data for the
panel of 14 Middle East countries over the period 1995 to 2009 is collected.
Findings of the current study show that ICT development increases the H-index
of the sample countries. The results also indicate that ICT Use and ICT Skill
sub-indices positively contribute to higher H-index but the effect of ICT
access on country's H-index is not clear.
"
968,"Ranking users, papers and authors in online scientific communities","  The ever-increasing quantity and complexity of scientific production have
made it difficult for researchers to keep track of advances in their own
fields. This, together with growing popularity of online scientific
communities, calls for the development of effective information filtering
tools. We propose here a method to simultaneously compute reputation of users
and quality of scientific artifacts in an online scientific community.
Evaluation on artificially-generated data and real data from the Econophysics
Forum is used to determine the method's best-performing variants. We show that
when the method is extended by considering author credit, its performance
improves on multiple levels. In particular, top papers have higher citation
count and top authors have higher $h$-index than top papers and top authors
chosen by other algorithms.
"
969,"Journals, repositories, peer review, non-peer review, and the future of
  scholarly communication","  Peer reviewed journals are a key part of the system by which academic
knowledge is developed and communicated. Problems have often been noted, and
alternatives proposed, but the journal system still survives. In this article I
focus on problems relating to reliance on subject-specific journals and peer
review. Contrary to what is often assumed, there are alternatives to the
current system, some of which have only becoming viable since the rise of the
world wide web. The market for academic ideas should be opened up by separating
the publication service from the review service: the former would ideally be
served by an open access, web-based repository system encompassing all
disciplines, whereas the latter should be opened up to encourage non-peer
reviews from different perspectives, user reviews, statistics reviews, reviews
from the perspective of different disciplines, and so on. The possibility of
multiple reviews of the same artefact should encourage competition between
reviewing organizations and should make the system more responsive to the
requirements of the differing audience groups. These possibilities offer the
potential to make the academic system far more productive.
  Keywords: Academic journals, Open access, Peer review, Scholarly
communication, Science communication.
"
970,"How to improve the prediction based on citation impact percentiles for
  years shortly after the publication date?","  The findings of Bornmann, Leydesdorff, and Wang (in press) revealed that the
consideration of journal impact improves the prediction of long-term citation
impact. This paper further explores the possibility of improving citation
impact measurements on the base of a short citation window by the consideration
of journal impact and other variables, such as the number of authors, the
number of cited references, and the number of pages. The dataset contains
475,391 journal papers published in 1980 and indexed in Web of Science (WoS,
Thomson Reuters), and all annual citation counts (from 1980 to 2010) for these
papers. As an indicator of citation impact, we used percentiles of citations
calculated using the approach of Hazen (1914). Our results show that citation
impact measurement can really be improved: If factors generally influencing
citation impact are considered in the statistical analysis, the explained
variance in the long-term citation impact can be much increased. However, this
increase is only visible when using the years shortly after publication but not
when using later years.
"
971,"Scholarly literature and the press: scientific impact and social
  perception of physics computing","  The broad coverage of the search for the Higgs boson in the mainstream media
is a relative novelty for high energy physics (HEP) research, whose
achievements have traditionally been limited to scholarly literature. This
paper illustrates the results of a scientometric analysis of HEP computing in
scientific literature, institutional media and the press, and a comparative
overview of similar metrics concerning representative particle physics
measurements. The picture emerging from these scientometric data documents the
scientific impact and social perception of HEP computing. The results of this
analysis suggest that improved communication of the scientific and social role
of HEP computing would be beneficial to the high energy physics community.
"
972,"Tracing the origin of a scientific legend by Reference Publication Year
  Spectroscopy (RPYS): the legend of the Darwin finches","  In a previews paper we introduced the quantitative method named Reference
Publication Year Spectroscopy (RPYS). With this method one can determine the
historical roots of research fields and quantify their impact on current
research. RPYS is based on the analysis of the frequency with which references
are cited in the publications of a specific research field in terms of the
publication years of these cited references. In this study, we illustrate that
RPYS can also be used to reveal the origin of scientific legends. We selected
Darwin finches as an example for illustration. Charles Darwin, the originator
of evolutionary theory, was given credit for finches he did not see and for
observations and insights about the finches he never made. We have shown that a
book published in 1947 is the most-highly cited early reference cited within
the relevant literature. This book had already been revealed as the origin of
the term Darwin finches by Sulloway through careful historical analysis.
"
973,A brief network analysis of Artificial Intelligence publication,"  In this paper, we present an illustration to the history of Artificial
Intelligence(AI) with a statistical analysis of publish since 1940. We
collected and mined through the IEEE publish data base to analysis the
geological and chronological variance of the activeness of research in AI. The
connections between different institutes are showed. The result shows that the
leading community of AI research are mainly in the USA, China, the Europe and
Japan. The key institutes, authors and the research hotspots are revealed. It
is found that the research institutes in the fields like Data Mining, Computer
Vision, Pattern Recognition and some other fields of Machine Learning are quite
consistent, implying a strong interaction between the community of each field.
It is also showed that the research of Electronic Engineering and Industrial or
Commercial applications are very active in California. Japan is also publishing
a lot of papers in robotics. Due to the limitation of data source, the result
might be overly influenced by the number of published articles, which is to our
best improved by applying network keynode analysis on the research community
instead of merely count the number of publish.
"
974,"Measuring the evaluation and impact of scientific works and their
  authors","  Problems for evaluation and impact of published scientific works and their
authors are discussed. The role of citations in this process is pointed out.
Different bibliometric indicators are reviewed in this connection and ways for
generation of new bibliometric indices are given. The influence of different
circumstances, like self-citations, number of authors, time dependence and
publication types, on the evaluation and impact of scientific papers are
considered. The repercussion of works citations and their content is
investigated in this respect. Attention is paid also on implicit citations
which are not covered by the modern bibliometrics but often are reflected in
the peer reviews. Some aspects of the Web analogues of citations and new
possibilities of the Internet resources in evaluating authors achievements are
presented.
"
975,"Crossing the hurdle: the determinants of individual scientific
  performance","  An original cross sectional dataset referring to a medium sized Italian
university is implemented in order to analyze the determinants of scientific
research production at individual level. The dataset includes 942 permanent
researchers of various scientific sectors for a three year time span (2008 -
2010). Three different indicators - based on the number of publications or
citations - are considered as response variables. The corresponding
distributions are highly skewed and display an excess of zero - valued
observations. In this setting, the goodness of fit of several Poisson mixture
regression models are explored by assuming an extensive set of explanatory
variables. As to the personal observable characteristics of the researchers,
the results emphasize the age effect and the gender productivity gap, as
previously documented by existing studies. Analogously, the analysis confirm
that productivity is strongly affected by the publication and citation
practices adopted in different scientific disciplines. The empirical evidence
on the connection between teaching and research activities suggests that no
univocal substitution or complementarity thesis can be claimed: a major
teaching load does not affect the odds to be a non-active researcher and does
not significantly reduce the number of publications for active researchers. In
addition, new evidence emerges on the effect of researchers administrative
tasks, which seem to be negatively related with researcher's productivity, and
on the composition of departments. Researchers' productivity is apparently
enhanced by operating in department filled with more administrative and
technical staff, and it is not significantly affected by the composition of the
department in terms of senior or junior researchers.
"
976,A pattern-driven approach to biomedical ontology engineering,"  Developing ontologies can be expensive, time-consuming, as well as difficult
to develop and maintain. This is especially true for more expressive and/or
larger ontologies. Some ontologies are, however, relatively repetitive, reusing
design patterns; building these with both generic and bespoke patterns should
reduce duplication and increase regularity which in turn should impact on the
cost of development.
  Here we report on the usage of patterns applied to two biomedical ontologies:
firstly a novel ontology for karyotypes which has been built ground-up using a
pattern based approach; and, secondly, our initial refactoring of the SIO
ontology to make explicit use of patterns at development time. To enable this,
we use the Tawny-OWL library which enables full-programmatic development of
ontologies. We show how this approach can generate large numbers of classes
from much simpler data structures which is highly beneficial within biomedical
ontology engineering.
"
977,Multilinguals and Wikipedia Editing,"  This article analyzes one month of edits to Wikipedia in order to examine the
role of users editing multiple language editions (referred to as multilingual
users). Such multilingual users may serve an important function in diffusing
information across different language editions of the encyclopedia, and prior
work has suggested this could reduce the level of self-focus bias in each
edition. This study finds multilingual users are much more active than their
single-edition (monolingual) counterparts. They are found in all language
editions, but smaller-sized editions with fewer users have a higher percentage
of multilingual users than larger-sized editions. About a quarter of
multilingual users always edit the same articles in multiple languages, while
just over 40% of multilingual users edit different articles in different
languages. When non-English users do edit a second language edition, that
edition is most frequently English. Nonetheless, several regional and
linguistic cross-editing patterns are also present.
"
978,"Flexible and Extensible Digital Object and Repository Architecture
  (FEDORA)","  We describe a digital object and respository architecture for storing and
disseminating digital library content. The key features of the architecture
are: (1) support for heterogeneous data types; (2) accommodation of new types
as they emerge; (3) aggregation of mixed, possibly distributed, data into
complex objects; (4) the ability to specify multiple content disseminations of
these objects; and (5) the ability to associate rights management schemes with
these disseminations. This architecture is being implemented in the context of
a broader research project to develop next-generation service modules for a
layered digital library architecture.
"
979,"Policy-Carrying, Policy-Enforcing Digital Objects","  We describe the motivation for moving policy enforcement for access control
down to the digital object level. The reasons for this include handling of
item-specific behaviors, adapting to evolution of digital objects, and
permitting objects to move among repositories and portable devices. We then
describe our experiments that integrate the Fedora architecture for digital
objects and repositories and the PoET implementation of security automata to
effect such objectcentric policy enforcement.
"
980,"An Ontology Model for Organizing Information Resources Sharing on
  Personal Web","  Retrieve information resources made by the machine processing may refer to
multiple sources. A personal web as part of information resources in the
Internet requires a feature that can be understood by computer machines.
Therefore, in this paper an ontology semantic web approach is used to map the
resources in a meaningful scheme. In the design of concept, resources on the
web are viewed as documents that have some property and ownership. Domain
interest or web scope is used to describe a classification of resources that
navigate into relevant documents. If instances are completed to the concept,
then the ontology file can be loaded and shared as annotation on personal web.
This allows computer machine to query multiple ontology from different personal
webs that use it.
"
981,"The Mellon Fedora Project: Digital Library Architecture Meets XML and
  Web Services","  The University of Virginia received a grant of $1,000,000 from the Andrew W.
Mellon Foundation to enable the Library, in collaboration with Cornell
University, to build a digital object repository system based on the Flexible
Extensible Digital Object and Repository Architecture (Fedora). The new system
demonstrates how distributed digital library architecture can be deployed using
web-based technologies, including XML and Web services. The new system is
designed to be a foundation upon which interoperable web-based digital
libraries can be built. Virginia and collaborating partners in the US and UK
will evaluate the system using a diverse set of digital collections. The
software will be made available to the public as an open-source release.
"
982,Statistical Modelling of Citation Exchange Between Statistics Journals,"  Rankings of scholarly journals based on citation data are often met with
skepticism by the scientific community. Part of the skepticism is due to
disparity between the common perception of journals' prestige and their ranking
based on citation counts. A more serious concern is the inappropriate use of
journal rankings to evaluate the scientific influence of authors. This paper
focuses on analysis of the table of cross-citations among a selection of
Statistics journals. Data are collected from the Web of Science database
published by Thomson Reuters. Our results suggest that modelling the exchange
of citations between journals is useful to highlight the most prestigious
journals, but also that journal citation data are characterized by considerable
heterogeneity, which needs to be properly summarized. Inferential conclusions
require care in order to avoid potential over-interpretation of insignificant
differences between journal ratings. Comparison with published ratings of
institutions from the UK's Research Assessment Exercise shows strong
correlation at aggregate level between assessed research quality and journal
citation `export scores' within the discipline of Statistics.
"
983,"Author Impact Factor: tracking the dynamics of individual scientific
  impact","  The impact factor (IF) of scientific journals has acquired a major role in
the evaluations of the output of scholars, departments and whole institutions.
Typically papers appearing in journals with large values of the IF receive a
high weight in such evaluations. However, at the end of the day one is
interested in assessing the impact of individuals, rather than papers. Here we
introduce Author Impact Factor (AIF), which is the extension of the IF to
authors. The AIF of an author A in year $t$ is the average number of citations
given by papers published in year $t$ to papers published by A in a period of
$\Delta t$ years before year $t$. Due to its intrinsic dynamic character, AIF
is capable to capture trends and variations of the impact of the scientific
output of scholars in time, unlike the $h$-index, which is a growing measure
taking into account the whole career path.
"
984,"Coverage, field specialization and impact of scientific publishers
  indexed in the 'Book Citation Index'","  Purpose: The aim of this study is to analyze the disciplinary coverage of the
Thomson Reuters' Book Citation Index database focusing on publisher presence,
impact and specialization. Design/Methodology/approach: We conduct a
descriptive study in which we examine coverage by discipline, publisher
distribution by field and country of publication, and publisher impact. For
this the Thomson Reuters' Subject Categories were aggregated into 15
disciplines. Findings: 30% of the total share of this database belongs to the
fields of Humanities and Social Sciences. Most of the disciplines are covered
by very few publishers mainly from the UK and USA (75.05% of the books), in
fact 33 publishers concentrate 90% of the whole share. Regarding publisher
impact, 80.5% of the books and chapters remained uncited. Two serious errors
were found in this database. Firstly, the Book Citation Index does not retrieve
all citations for books and chapters. Secondly, book citations do not include
citations to their chapters. Research limitations/implications: The Book
Citation Index is still underdeveloped and has serious limitations which call
into caution when using it for bibliometric purposes. Practical implications:
The results obtained from this study warn against the use of this database for
bibliometric purposes, but opens a new window of opportunities for covering
long neglected areas such as Humanities and Social Sciences. The target
audience of this study is librarians, bibliometricians, researchers, scientific
publishers, prospective authors and evaluation agencies. Originality/Value:
There are currently no studies analyzing in depth the coverage of this novel
database which covers monographs.
"
985,"How have the Eastern European countries of the former Warsaw Pact
  developed since 1990? A bibliometric study","  Did the demise of the Soviet Union in 1991 influence the scientific
performance of the researchers in Eastern European countries? Did this
historical event affect international collaboration by researchers from the
Eastern European countries with those of Western countries? Did it also change
international collaboration among researchers from the Eastern European
countries? Trying to answer these questions, this study aims to shed light on
international collaboration by researchers from the Eastern European countries
(Russia, Ukraine, Belarus, Moldova, Bulgaria, the Czech Republic, Hungary,
Poland, Romania and Slovakia). The number of publications and normalized
citation impact values are compared for these countries based on InCites
(Thomson Reuters), from 1981 up to 2011. The international collaboration by
researchers affiliated to institutions in Eastern European countries at the
time points of 1990, 2000 and 2011 was studied with the help of Pajek and
VOSviewer software, based on data from the Science Citation Index (Thomson
Reuters). Our results show that the breakdown of the communist regime did not
lead, on average, to a huge improvement in the publication performance of the
Eastern European countries and that the increase in international co-authorship
relations by the researchers affiliated to institutions in these countries was
smaller than expected. Most of the Eastern European countries are still subject
to changes and are still awaiting their boost in scientific development.
"
986,"Eugene Garfield, Francis Narin, and PageRank: The Theoretical Bases of
  the Google Search Engine","  This paper presents a test of the validity of using Google Scholar to
evaluate the publications of researchers by comparing the premises on which its
search engine, PageRank, is based, to those of Garfield's theory of citation
indexing. It finds that the premises are identical and that PageRank and
Garfield's theory of citation indexing validate each other.
"
987,"RDF Translator: A RESTful Multi-Format Data Converter for the Semantic
  Web","  The interdisciplinary nature of the Semantic Web and the many projects put
forward by the community led to a large number of widely accepted serialization
formats for RDF. Most of these RDF syntaxes have been developed out of a
necessity to serve specific purposes better than existing ones, e.g. RDFa was
proposed as an extension to HTML for embedding non-intrusive RDF statements in
human-readable documents. Nonetheless, the RDF serialization formats are
generally transducible among themselves given that they are commonly based on
the RDF model. In this paper, we present (1) a RESTful Web service based on the
HTTP protocol that translates between different serializations. In addition to
its core functionality, our proposed solution provides (2) features to
accommodate frequent needs of Semantic Web developers, namely a straightforward
user interface with copy-to-clipboard functionality, syntax highlighting,
persistent URI links for easy sharing, cool URI patterns, and content
negotiation using respective HTTP headers. We demonstrate the benefit of our
converter by presenting two use cases.
"
988,Semantic Annotation: The Mainstay of Semantic Web,"  Given that semantic Web realization is based on the critical mass of metadata
accessibility and the representation of data with formal knowledge, it needs to
generate metadata that is specific, easy to understand and well-defined.
However, semantic annotation of the web documents is the successful way to make
the Semantic Web vision a reality. This paper introduces the Semantic Web and
its vision (stack layers) with regard to some concept definitions that helps
the understanding of semantic annotation. Additionally, this paper introduces
the semantic annotation categories, tools, domains and models.
"
989,The availability of research data declines rapidly with article age,"  Policies ensuring that research data are available on public archives are
increasingly being implemented at the government [1], funding agency [2-4], and
journal [5,6] level. These policies are predicated on the idea that authors are
poor stewards of their data, particularly over the long term [7], and indeed
many studies have found that authors are often unable or unwilling to share
their data [8-11]. However, there are no systematic estimates of how the
availability of research data changes with time since publication. We therefore
requested datasets from a relatively homogenous set of 516 articles published
between 2 and 22 years ago, and found that availability of the data was
strongly affected by article age. For papers where the authors gave the status
of their data, the odds of a dataset being extant fell by 17% per year. In
addition, the odds that we could find a working email address for the first,
last or corresponding author fell by 7% per year. Our results reinforce the
notion that, in the long term, research data cannot be reliably preserved by
individual researchers, and further demonstrate the urgent need for policies
mandating data sharing via public archives.
"
990,"Exploring Regional Development of Digital Humanities Research: A Case
  Study for Taiwan","  This study analyzed references and source papers of the Proceedings of
2009-2012 International Conference of Digital Archives and Digital Humanities
(DADH), which was held annually in Taiwan. A total of 59 sources and 1,104
references were investigated, based on descriptive analysis and subject
analysis of library practices on cataloguing. Preliminary results showed
historical materials, events, bureaucracies, and people of Taiwan and China in
the Qing Dynasty were the major subjects in the tempo-spatial dimensions. The
subject-date figure depicted a long-low head and short-high tail curve, which
demonstrated both characteristics of research of humanities and application of
technology in digital humanities. The dates of publication of the references
spanned over 360 years, which shows a long time span in research materials. A
majority of the papers (61.41%) were single-authored, which is in line with the
common research practice in the humanities. Books published by general
publishers were the major type of references, and this was the same as that of
established humanities research. The next step of this study will focus on the
comparison of characteristics of both sources and references of international
journals with those reported in this article.
"
991,Astrophysics Source Code Library: Incite to Cite!,"  The Astrophysics Source Code Library (ASCL, http://ascl.net/) is an online
registry of over 700 source codes that are of interest to astrophysicists, with
more being added regularly. The ASCL actively seeks out codes as well as
accepting submissions from the code authors, and all entries are citable and
indexed by ADS. All codes have been used to generate results published in or
submitted to a refereed journal and are available either via a download site or
froman identified source. In addition to being the largest directory of
scientist-written astrophysics programs available, the ASCL is also an active
participant in the reproducible research movement with presentations at various
conferences, numerous blog posts and a journal article. This poster provides a
description of the ASCL and the changes that we are starting to see in the
astrophysics community as a result of the work we are doing.
"
992,Weighted Multiplex Networks,"  One of the most important challenges in network science is to quantify the
information encoded in complex network structures. Disentangling randomness
from organizational principles is even more demanding when networks have a
multiplex nature. Multiplex networks are multilayer systems of $N$ nodes that
can be linked in multiple interacting and co-evolving layers. In these
networks, relevant information might not be captured if the single layers were
analyzed separately. Here we demonstrate that such partial analysis of layers
fails to capture significant correlations between weights and topology of
complex multiplex networks. To this end, we study two weighted multiplex
co-authorship and citation networks involving the authors included in the
American Physical Society. We show that in these networks weights are strongly
correlated with multiplex structure, and provide empirical evidence in favor of
the advantage of studying weighted measures of multiplex networks, such as
multistrength and the inverse multiparticipation ratio. Finally, we introduce a
theoretical framework based on the entropy of multiplex ensembles to quantify
the information stored in multiplex networks that would remain undetected if
the single layers were analyzed in isolation.
"
993,"Application of polynomial texture mapping in process of digitalization
  of cultural heritage","  In this paper we present modern texture mapping techniques and several
applications of polynomial texture mapping in cultural heritage programs. We
also consider some well-known and some new methods for mathematical procedure
that is involved in generation of polynomial texture maps.
"
994,Empirical modeling of the impact factor distribution,"  The distribution of impact factors has been modeled in the recent informetric
literature using two-exponent law proposed by Mansilla et al. (2007). This
paper shows that two distributions widely-used in economics, namely the Dagum
and Singh-Maddala models, possess several advantages over the two-exponent
model. Compared to the latter, the former give as good as or slightly better
fit to data on impact factors in eight important scientific fields. In contrast
to the two-exponent model, both proposed distributions have closed-from
probability density functions and cumulative distribution functions, which
facilitates fitting these distributions to data and deriving their statistical
properties.
"
995,Ideas for Advancing Code Sharing (A Different Kind of Hack Day),"  How do we as a community encourage the reuse of software for telescope
operations, data processing, and calibration? How can we support making codes
used in research available for others to examine? Continuing the discussion
from last year Bring out your codes! BoF session, participants separated into
groups to brainstorm ideas to mitigate factors which inhibit code sharing and
nurture those which encourage code sharing. The BoF concluded with the sharing
of ideas that arose from the brainstorming sessions and a brief summary by the
moderator.
"
996,"An Effective End-User Development Approach Through Domain-Specific
  Mashups for Research Impact Evaluation","  Over the last decade, there has been growing interest in the assessment of
the performance of researchers, research groups, universities and even
countries. The assessment of productivity is an instrument to select and
promote personnel, assign research grants and measure the results of research
projects. One particular assessment approach is bibliometrics i.e., the
quantitative analysis of scientific publications through citation and content
analysis. However, there is little consensus today on how research evaluation
should be performed, and it is commonly acknowledged that the quantitative
metrics available today are largely unsatisfactory. A number of different
scientific data sources available on the Web (e.g., DBLP, Google Scholar) that
are used for such analysis purposes. Taking data from these diverse sources,
performing the analysis and visualizing results in different ways is not a
trivial and straight forward task. Moreover, people involved in such evaluation
processes are not always IT experts and hence not capable to crawl data
sources, merge them and compute the needed evaluation procedures. The recent
emergence of mashup tools has refueled research on end-user development, i.e.,
on enabling end-users without programming skills to produce their own
applications. We believe that the heart of the problem is that it is
impractical to design tools that are generic enough to cover a wide range of
application domains, powerful enough to enable the specification of non-trivial
logic, and simple enough to be actually accessible to non-programmers. This
thesis presents a novel approach for an effective end-user development,
specifically for non-programmers. That is, we introduce a domain-specific
approach to mashups that ""speaks the language of users""., i.e., that is aware
of the terminology, concepts, rules, and conventions (the domain) the user is
comfortable with.
"
997,Of course we share! Testing Assumptions about Social Tagging Systems,"  Social tagging systems have established themselves as an important part in
today's web and have attracted the interest from our research community in a
variety of investigations. The overall vision of our community is that simply
through interactions with the system, i.e., through tagging and sharing of
resources, users would contribute to building useful semantic structures as
well as resource indexes using uncontrolled vocabulary not only due to the
easy-to-use mechanics. Henceforth, a variety of assumptions about social
tagging systems have emerged, yet testing them has been difficult due to the
absence of suitable data. In this work we thoroughly investigate three
available assumptions - e.g., is a tagging system really social? - by examining
live log data gathered from the real-world public social tagging system
BibSonomy. Our empirical results indicate that while some of these assumptions
hold to a certain extent, other assumptions need to be reflected and viewed in
a very critical light. Our observations have implications for the design of
future search and other algorithms to better reflect the actual user behavior.
"
998,"A survey on the importance of visualization and social collaboration in
  academic digital libraries","  From more than half a century ago indexing scientific articles has been
studied intensively to provide a more efficient data retrieval and to conserve
researchers invaluable time. In the last two decades with the emergence of the
World Wide Web and the rapid growth in the number of scientific documents
online many academic databases and search engines were launched with almost
similar structure in order to reduce the difficulty in finding, relating and
sorting of the existing scientific documents published online. The dramatic
increase of the scientific documents in the last few years makes it necessary
that the retrieved information by the search engines be analyzed and more
organized and interpretable representation be displayed to the users.
Information visualization is a great way for exploration of large and complex
data sets, therefore it can be a natural candidate for the purpose of
generating more comprehensible search results for the citation and academic
databases. In this survey the usage pattern of the participants and their
demands and ideas for the existence of other beneficial methods for literature
review has been questioned and the results are quantitatively analyzed.
"
999,Empirical Patterns in Google Scholar Citation Counts,"  Scholarly impact may be metricized using an author's total number of
citations as a stand-in for real worth, but this measure varies in
applicability between disciplines. The detail of the number of citations per
publication is nowadays mapped in much more detail on the Web, exposing certain
empirical patterns. This paper explores those patterns, using the citation data
from Google Scholar for a number of authors.
"
1000,10 Simple Rules for the Care and Feeding of Scientific Data,"  This article offers a short guide to the steps scientists can take to ensure
that their data and associated analyses continue to be of value and to be
recognized. In just the past few years, hundreds of scholarly papers and
reports have been written on questions of data sharing, data provenance,
research reproducibility, licensing, attribution, privacy, and more, but our
goal here is not to review that literature. Instead, we present a short guide
intended for researchers who want to know why it is important to ""care for and
feed"" data, with some practical advice on how to do that.
"
1001,Sampling Issues in Bibliometric Analysis,"  Bibliometricians face several issues when drawing and analyzing samples of
citation records for their research. Drawing samples that are too small may
make it difficult or impossible for studies to achieve their goals, while
drawing samples that are too large may drain resources that could be better
used for other purposes. This paper considers three common situations and
offers advice for dealing with each. First, an entire population of records is
available for an institution. We argue that, even though all records have been
collected, the use of inferential statistics, significance testing, and
confidence intervals is both common and desirable. Second, because of limited
resources or other factors, a sample of records needs to be drawn. We
demonstrate how power analyses can be used to determine in advance how large
the sample needs to be to achieve the study's goals. Third, the sample size may
already be determined, either because the data have already been collected or
because resources are limited. We show how power analyses can again be used to
determine how large effects need to be in order to find effects that are
statistically significant. Such information can then help bibliometricians to
develop reasonable expectations as to what their analysis can accomplish. While
we focus on issues of interest to bibliometricians, our recommendations and
procedures can easily be adapted for other fields of study.
"
1002,A Survey of Volunteered Open Geo-Knowledge Bases in the Semantic Web,"  Over the past decade, rapid advances in web technologies, coupled with
innovative models of spatial data collection and consumption, have generated a
robust growth in geo-referenced information, resulting in spatial information
overload. Increasing 'geographic intelligence' in traditional text-based
information retrieval has become a prominent approach to respond to this issue
and to fulfill users' spatial information needs. Numerous efforts in the
Semantic Geospatial Web, Volunteered Geographic Information (VGI), and the
Linking Open Data initiative have converged in a constellation of open
knowledge bases, freely available online. In this article, we survey these open
knowledge bases, focusing on their geospatial dimension. Particular attention
is devoted to the crucial issue of the quality of geo-knowledge bases, as well
as of crowdsourced data. A new knowledge base, the OpenStreetMap Semantic
Network, is outlined as our contribution to this area. Research directions in
information integration and Geographic Information Retrieval (GIR) are then
reviewed, with a critical discussion of their current limitations and future
prospects.
"
1003,"Patents as Instruments for Exploring Innovation Dynamics: Geographic and
  Technological Perspectives on ""Photovoltaic Cells""","  The dynamics of innovation are nonlinear and complex: geographical,
technological, and economic selection environments can be expected to interact.
Can patents provide an analytical lens to this process in terms of different
attributes such as inventor addresses, classification codes, backward and
forward citations, etc.? Two recently developed patent maps with interactive
overlay techniques--Google Maps and maps based on citation relations among
International Patent Classifications (IPC)--are elaborated into dynamic
versions that allow for online animations and comparisons by using split
screens. Various forms of animation are explored. The recently developed
Cooperative Patent Classifications (CPC) of the U.S. Patent and Trade Office
(USPTO) and the European Patent Office (EPO) provide new options for a precise
delineation of samples in both USPTO data and the Worldwide Patent Statistics
Database (PatStat) of EPO. Among the ""technologies for the mitigation of
climate change"" (class Y02), we zoom in on nine material technologies for
photovoltaic cells; and focus on one of them (CuInSe2) as a lead case. The
longitudinal development of Rao-Stirling diversity in the IPC-based maps
provides a heuristics for studying technological generations during the period
under study (1975-2012). The sequencing of generations prevails in USPTO data
more than in PatStat data because PatStat aggregates patent information from
countries in different stages of technological development, whereas one can
expect USPTO patents to be competitive at the technological edge.
"
1004,"What is the effect of country-specific characteristics on the research
  performance of scientific institutions? Using multi-level statistical models
  to rank and map universities and research-focused institutions worldwide","  Bornmann, Stefaner, de Moya Anegon, and Mutz (in press) have introduced a web
application (www.excellencemapping.net) which is linked to both academic
ranking lists published hitherto (e.g. the Academic Ranking of World
Universities) as well as spatial visualization approaches. The web application
visualizes institutional performance within specific subject areas as ranking
lists and on custom tile-based maps. The new, substantially enhanced version of
the web application and the multilevel logistic regression on which it is based
are described in this paper. Scopus data were used which have been collected
for the SCImago Institutions Ranking. Only those universities and
research-focused institutions are considered that have published at least 500
articles, reviews and conference papers in the period 2006 to 2010 in a certain
Scopus subject area. In the enhanced version, the effect of single covariates
(such as the per capita GDP of a country in which an institution is located) on
two performance metrics (best paper rate and best journal rate) is examined and
visualized. A covariate-adjusted ranking and mapping of the institutions is
produced in which the single covariates are held constant. The results on the
performance of institutions can then be interpreted as if the institutions all
had the same value (reference point) for the covariate in question. For
example, those institutions can be identified worldwide showing a very good
performance despite a bad financial situation in the corresponding country.
"
1005,Binary Scientific Star Coauthors Core Size,"  It is examined whether the relationship $ J \propto A/r^{\alpha}$, and the
subsequent coauthor core notion (Ausloos 2013), between the number ($J$) of
joint publications (JP) by a ""main scientist"" (LI) with her/his coauthors (CAs)
can be extended to a team-like system. This is done by considering that each
coauthor can be so strongly tied to the LI that they are forming {\it binary
scientific star} (BSS) systems with respect to their other collaborators.
Moreover, publications in peer review journals and in ""proceedings"", both often
thought to be of ""different quality"", are separetely distinguished. The role of
a time interval for measuring $J$ and $\alpha$ is also examined. New indirect
measures are also introduced.
  For making the point, two LI cases with numerous CAs are studied. It is found
that only a few BSS need to be usefully examined. The exponent $\alpha$ turns
out to be ""second scientist"" weakly dependent, but still ""size"" and
""publication type"" dependent, according to the number of CAs or JP. The CA core
value is found to be (CA or JP) size and publication type dependent, but
remains in an understandable range. Somewhat unexpectedly, no special
qualitative difference on the binary scientific star CA core value is found
between publications in peer review journals and in proceedings.
  In conclusion, some remark is made on partner cooperation in BSS teams. It is
suggested that such measures can serve as criteria for distinguishing the role
of scientists in a team.
"
1006,"The Research Object Suite of Ontologies: Sharing and Exchanging Research
  Data and Methods on the Open Web","  Research in life sciences is increasingly being conducted in a digital and
online environment. In particular, life scientists have been pioneers in
embracing new computational tools to conduct their investigations. To support
the sharing of digital objects produced during such research investigations, we
have witnessed in the last few years the emergence of specialized repositories,
e.g., DataVerse and FigShare. Such repositories provide users with the means to
share and publish datasets that were used or generated in research
investigations. While these repositories have proven their usefulness,
interpreting and reusing evidence for most research results is a challenging
task. Additional contextual descriptions are needed to understand how those
results were generated and/or the circumstances under which they were
concluded. Because of this, scientists are calling for models that go beyond
the publication of datasets to systematically capture the life cycle of
scientific investigations and provide a single entry point to access the
information about the hypothesis investigated, the datasets used, the
experiments carried out, the results of the experiments, the people involved in
the research, etc. In this paper we present the Research Object (RO) suite of
ontologies, which provide a structured container to encapsulate research data
and methods along with essential metadata descriptions. Research Objects are
portable units that enable the sharing, preservation, interpretation and reuse
of research investigation results. The ontologies we present have been designed
in the light of requirements that we gathered from life scientists. They have
been built upon existing popular vocabularies to facilitate interoperability.
Furthermore, we have developed tools to support the creation and sharing of
Research Objects, thereby promoting and facilitating their adoption.
"
1007,"Do altmetrics correlate with citations? Extensive comparison of
  altmetric indicators with citations from a multidisciplinary perspective","  An extensive analysis of the presence of different altmetric indicators
provided by Altmetric.com across scientific fields is presented, particularly
focusing on their relationship with citations. Our results confirm that the
presence and density of social media altmetric counts are still very low and
not very frequent among scientific publications, with 15%-24% of the
publications presenting some altmetric activity and concentrating in the most
recent publications, although their presence is increasing over time.
Publications from the social sciences, humanities and the medical and life
sciences show the highest presence of altmetrics, indicating their potential
value and interest for these fields. The analysis of the relationships between
altmetrics and citations confirms previous claims of positive correlations but
relatively weak, thus supporting the idea that altmetrics do not reflect the
same concept of impact as citations. Also, altmetric counts do not always
present a better filtering of highly cited publications than journal citation
scores. Altmetrics scores (particularly mentions in blogs) are able to identify
highly cited publications with higher levels of precision than journal citation
scores (JCS), but they have a lower level of recall. The value of altmetrics as
a complementary tool of citation analysis is highlighted, although more
research is suggested to disentangle the potential meaning and value of
altmetric indicators for research evaluation.
"
1008,"Solving reviewer assignment problem in software peer review: An approach
  based on preference matrix and asymmetric TSP model","  Optimized reviewer assignment can effectively utilize limited intellectual
resources and significantly assure review quality in various scenarios such as
paper selection in conference or journal, proposal selection in funding
agencies and so on. However, little research on reviewer assignment of software
peer review has been found. In this study, an optimization approach is proposed
based on students' preference matrix and the model of asymmetric traveling
salesman problem (ATSP). Due to the most critical role of rule matrix in this
approach, we conduct a questionnaire to obtain students' preference matrices
and convert them to rule matrices. With the help of software ILOG CPLEX, the
approach is accomplished by controlling the exit criterion of ATSP model. The
comparative study shows that the assignment strategies with both reviewers'
preference matrix and authors' preference matrix get better performance than
the random assignment. Especially, it is found that the performance is just a
little better than that of random assignment when the reviewers' and authors'
preference matrices are merged. In other words, the majority of students have a
strong wish of harmonious development even though high-level students are not
willing to do that.
"
1009,"Government and Social Media: A Case Study of 31 Informational World
  Cities","  Social media platforms are increasingly being used by governments to foster
user interaction. Particularly in cities with enhanced ICT infrastructures
(i.e., Informational World Cities) and high internet penetration rates, social
media platforms are valuable tools for reaching high numbers of citizens. This
empirical investigation of 31 Informational World Cities will provide an
overview of social media services used for governmental purposes, of their
popularity among governments, and of their usage intensity in broadcasting
information online.
"
1010,Universal hierarchical behavior of citation networks,"  Many of the essential features of the evolution of scientific research are
imprinted in the structure of citation networks. Connections in these networks
imply information about the transfer of knowledge among papers, or in other
words, edges describe the impact of papers on other publications. This inherent
meaning of the edges infers that citation networks can exhibit hierarchical
features, that is typical of networks based on decision-making. In this paper,
we investigate the hierarchical structure of citation networks consisting of
papers in the same field. We find that the majority of the networks follow a
universal trend towards a highly hierarchical state, and i) the various fields
display differences only concerning their phase in life (distance from the
""birth"" of a field) or ii) the characteristic time according to which they are
approaching the stationary state. We also show by a simple argument that the
alterations in the behavior are related to and can be understood by the degree
of specialization corresponding to the fields. Our results suggest that during
the accumulation of knowledge in a given field, some papers are gradually
becoming relatively more influential than most of the other papers.
"
1011,"How science maps reveal knowledge transfer: new measurement for a
  historical case","  Modelling actors of science via science (overlay) maps has recently become a
popular practice in Interdisciplinarity Research (IDR). The benefits of this
toolkit have also been recognized for other areas of scientometrics, such as
the study of science dynamics. In this paper we propose novel methods of
measuring knowledge diffusion/integration based on previous applications of the
overlay methodology. New indices called Mean Overlay Distance and Overlay
Diversity Ratio, respectively, are being drawn from previous uses of the
Stirling index as the main proxy for knowledge diversification. We demonstrate
the added value of this proposal via a case study addressing the development of
a rather complex discourse in biology, usually referred to as the Species
Problem. The selected topic is known for a history connecting various research
fields and traditions, being, therefore, both an ideal and challenging case for
the study of knowledge diffusion.
"
1012,"Trusty URIs: Verifiable, Immutable, and Permanent Digital Artifacts for
  Linked Data","  To make digital resources on the web verifiable, immutable, and permanent, we
propose a technique to include cryptographic hash values in URIs. We call them
trusty URIs and we show how they can be used for approaches like
nanopublications to make not only specific resources but their entire reference
trees verifiable. Digital artifacts can be identified not only on the byte
level but on more abstract levels such as RDF graphs, which means that
resources keep their hash values even when presented in a different format. Our
approach sticks to the core principles of the web, namely openness and
decentralized architecture, is fully compatible with existing standards and
protocols, and can therefore be used right away. Evaluation of our reference
implementations shows that these desired properties are indeed accomplished by
our approach, and that it remains practical even for very large files.
"
1013,"How are excellent (highly cited) papers defined in bibliometrics? A
  quantitative analysis of the literature","  As the subject of research excellence has received increasing attention (in
science policy) over the last few decades, increasing numbers of bibliometric
studies have been published dealing with excellent papers. However, many
different methods have been used in these studies to identify excellent papers.
The present quantitative analysis of the literature has been carried out in
order to acquire an overview of these methods and an indication of an ""average""
or ""most frequent"" bibliometric practice. The search in the Web of Science
yielded 321 papers dealing with ""highly cited"", ""most cited"", ""top cited"" and
""most frequently cited"". Of the 321 papers, 16 could not be used in this study.
In around 80% of the papers analyzed in this study, a quantitative definition
has been provided with which to identify excellent papers. With definitions
which relate to an absolute number, either a certain number of top cited papers
(58%) or papers with a minimum number of citations are selected (17%). Around
23% worked with percentile rank classes. Over these papers, there is an
arithmetic average of the top 7.6% (arithmetic average) or of the top 3%
(median). The top 1% is used most frequently in the papers, followed by the top
10%. With the thresholds presented in this study, in future, it will be
possible to identify excellent papers based on an ""average"" or ""most frequent""
practice among bibliometricians.
"
1014,Exploiting citation networks for large-scale author name disambiguation,"  We present a novel algorithm and validation method for disambiguating author
names in very large bibliographic data sets and apply it to the full Web of
Science (WoS) citation index. Our algorithm relies only upon the author and
citation graphs available for the whole period covered by the WoS. A pair-wise
publication similarity metric, which is based on common co-authors,
self-citations, shared references and citations, is established to perform a
two-step agglomerative clustering that first connects individual papers and
then merges similar clusters. This parameterized model is optimized using an
h-index based recall measure, favoring the correct assignment of well-cited
publications, and a name-initials-based precision using WoS metadata and
cross-referenced Google Scholar profiles. Despite the use of limited metadata,
we reach a recall of 87% and a precision of 88% with a preference for
researchers with high h-index values. 47 million articles of WoS can be
disambiguated on a single machine in less than a day. We develop an h-index
distribution model, confirming that the prediction is in excellent agreement
with the empirical data, and yielding insight into the utility of the h-index
in real academic ranking scenarios.
"
1015,"Inequality in Societies, Academic Institutions and Science Journals:
  Gini and k-indices","  Social inequality is traditionally measured by the Gini-index ($g$). The
$g$-index takes values from $0$ to $1$ where $g=0$ represents complete equality
and $g=1$ represents complete inequality. Most of the estimates of the income
or wealth data indicate the $g$ value to be widely dispersed across the
countries of the world: \textit{g} values typically range from $0.30$ to $0.65$
at a particular time (year). We estimated similarly the Gini-index for the
citations earned by the yearly publications of various academic institutions
and the science journals. The ISI web of science data suggests remarkably
strong inequality and universality ($g=0.70\pm0.07$) across all the
universities and institutions of the world, while for the journals we find
$g=0.65\pm0.15$ for any typical year. We define a new inequality measure,
namely the $k$-index, saying that the cumulative income or citations of ($1-k$)
fraction of people or papers exceed those earned by the fraction ($k$) of the
people or publications respectively. We find, while the $k$-index value for
income ranges from $0.60$ to $0.75$ for income distributions across the world,
it has a value around $0.75\pm0.05$ for different universities and institutions
across the world and around $0.77\pm0.10$ for the science journals. Apart from
above indices, we also analyze the same institution and journal citation data
by measuring Pietra index and median index.
"
1016,"Contexts of diffusion: Adoption of research synthesis in Social Work and
  Women's Studies","  Texts reveal the subjects of interest in research fields, and the values,
beliefs, and practices of researchers. In this study, texts are examined
through bibliometric mapping and topic modeling to provide a birds eye view of
the social dynamics associated with the diffusion of research synthesis methods
in the contexts of Social Work and Women's Studies. Research synthesis texts
are especially revealing because the methods, which include meta-analysis and
systematic review, are reliant on the availability of past research and data,
sometimes idealized as objective, egalitarian approaches to research
evaluation, fundamentally tied to past research practices, and performed with
the goal informing future research and practice. This study highlights the
co-influence of past and subsequent research within research fields;
illustrates dynamics of the diffusion process; and provides insight into the
cultural contexts of research in Social Work and Women's Studies. This study
suggests the potential to further develop bibliometric mapping and topic
modeling techniques to inform research problem selection and resource
allocation.
"
1017,Web Based Reputation Index of Turkish Universities,"  This paper attempts to develop an online reputation index of Turkish
universities through their online impact and effectiveness. Using 16 different
web based parameters and employing normalization process of the results, we
have ranked websites of Turkish universities in terms of their web presence.
This index is first attempt to determine the tools of reputation of Turkish
academic websites and would be a basis for further studies to examine the
relation between reputation and the online effectiveness of the universities.
"
1018,"Bots vs. Wikipedians, Anons vs. Logged-Ins","  Wikipedia is a global crowdsourced encyclopedia that at time of writing is
available in 287 languages. Wikidata is a likewise global crowdsourced
knowledge base that provides shared facts to be used by Wikipedias. In the
context of this research, we have developed an application and an underlying
Application Programming Interface (API) capable of monitoring realtime edit
activity of all language versions of Wikipedia and Wikidata. This application
allows us to easily analyze edits in order to answer questions such as ""Bots
vs. Wikipedians, who edits more?"", ""Which is the most anonymously edited
Wikipedia?"", or ""Who are the bots and what do they edit?"". To the best of our
knowledge, this is the first time such an analysis could be done in realtime
for Wikidata and for really all Wikipedias--large and small. Our application is
available publicly online at the URL http://wikipedia-edits.herokuapp.com/, its
code has been open-sourced under the Apache 2.0 license.
"
1019,A Framework for Evaluation of Composite Memento Temporal Coherence,"  Most archived HTML pages embed other web resources, such as images and
stylesheets. Playback of the archived web pages typically provides only the
capture date (or Memento-Datetime) of the root resource and not the
Memento-Datetime of the embedded resources. In the course of our research, we
have discovered that the Memento-Datetime of embedded resources can be up to
several years in the future or past, relative to the Memento-Datetime of the
embedding root resource. We introduce a framework for assessing temporal
coherence between a root resource and its embedded resource depending on
Memento-Datetime, Last-Modified datetime, and entity body.
"
1020,"A New Approach to Reporting Archaeological Surveys: Connecting Rough
  Cilicia, Visible Past and Open Context through loose coupling and 3d codes","  The project presents the strategy adopted by the Rough Cilicia Archaeological
Survey team for publishing its primary data and reports via three potentially
transformative strategies for digital humanities: Loose coupling of digital
data curation and publishing platforms. In loosely coupled systems, components
share only a limited set of simple assumptions, which enables systems to evolve
dynamically. Collaborative creation of map based narrative content. Connecting
print scholarship (book, reports, article) to online resources via
two-dimensional barcodes (2D codes) that can be printed on paper and can call
up hyperlinks when scanned with a Smartphone. The three strategies are made
possible by loosely coupling two autonomous services: Visible Past, dedicated
to web collaboration and digital-print publishing and Open Context, which is a
geo-historical data archiving and publishing service. The Rough Cilicia
Archaeological Survey, Visible Past, and Open Context work together to
illustrate a new genre of scholarship, which combine qualitative narratives and
quantitative representations of space and social phenomena. The project
provides tools for collaborative creation of rich scholarly narratives that are
spatially located and for connecting print publications to the digital realm.
The project is a case study for utilizing the three new strategies for creating
and publishing spatial humanities scholarship more broadly for ancient
historians.
"
1021,Authoris: a tool for authority control in the semantic web,"  Purpose: The purpose of this paper is to propose a tool that generates
authority files to be integrated with linked data by means of learning rules.
AUTHORIS is software developed to enhance authority control and information
exchange among bibliographic and non-bibliographic entities.
  Design / methodology / approach: The article analyzes different methods
previously developed for authority control as well as IFLA and ALA standards
for managing bibliographic records. Semantic Web technologies are also
evaluated. AUTHORIS relies on Drupal and incorporates the protocols of Dublin
Core, SIOC, SKOS and FOAF. The tool has also taken into account the
obsolescence of MARC and its substitution by FRBR and RDA. Its effectiveness
was evaluated applying a learning test proposed by RDA. Over 80 percent of the
actions were carried out correctly.
  Findings: The use of learning rules and the facilities of linked data make it
easier for information organizations to reutilize products for authority
control and distribute them in a fair and efficient manner.
  Research limitations / implications: The ISAD-G records were the ones
presenting most errors. EAD was found to be second in the number of errors
produced. The rest of the formats --MARC 21, Dublin Core, FRAD, RDF, OWL, XBRL
and FOAF-- showed fewer than 20 errors in total.
  Practical implications: AUTHORIS offers institutions the means of sharing
data with a high level of stability, helping to detect records that are
duplicated and contributing to lexical disambiguation and data enrichment.
  Originality / value: The software combines the facilities of linked data, the
potency of the algorithms for converting bibliographic data, and the precision
of learning rules.
"
1022,Visualizing Digital Collections,"  Data visualizations can greatly enhance search in digital collections by
providing information about the scope and context of a collection and allowing
users to more easily browse and explore the contents. This article discusses
the benefits of incorporating visualizations into digital collections based on
the experiences of the Cold War International History Project (CWIHP) in
developing a user-friendly tool for searching and visualizing the project's
complex set of historical documents. The paper concludes with a tutorial on
using the free Library of Congress tool Viewshare to create visualizations
based on real data from the CWIHP Digital Archive.
"
1023,Authorship Analysis based on Data Compression,"  This paper proposes to perform authorship analysis using the Fast Compression
Distance (FCD), a similarity measure based on compression with dictionaries
directly extracted from the written texts. The FCD computes a similarity
between two documents through an effective binary search on the intersection
set between the two related dictionaries. In the reported experiments the
proposed method is applied to documents which are heterogeneous in style,
written in five different languages and coming from different historical
periods. Results are comparable to the state of the art and outperform
traditional compression-based methods.
"
1024,Scientific works citation analysis,"  Several questions of scientometrics parameters organization are considered.
Two new indices for scientific works citation analysis are proposed. They
provide more detailed and reliable scientific significance assessment of
individual authors and scientific groups basing on the publication activity
"
1025,Designing an Ontology for the Data Documentation Initiative,"  An ontology of the DDI 3 data model will be designed by following the
ontology engineering methodology to be evolved based on state-of-the-art
methodologies. Hence DDI 3 data and metadata can be represented in form of a
standard web interchange format RDF and processed by highly available RDF
tools. As a consequence the DDI community has the possibility to publish and
link LOD data sets to become part of the LOD cloud.
"
1026,Learning-assisted Theorem Proving with Millions of Lemmas,"  Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of the lemmas in the HOL Light and Flyspeck
libraries, adding up to millions of the best lemmas to the pool of statements
that can be re-used in later proofs. We show that in combination with
learning-based relevance filtering, such methods significantly strengthen
automated theorem proving of new conjectures over large formal mathematical
libraries such as Flyspeck.
"
1027,Power laws in citation distributions: Evidence from Scopus,"  Modeling distributions of citations to scientific papers is crucial for
understanding how science develops. However, there is a considerable empirical
controversy on which statistical model fits the citation distributions best.
This paper is concerned with rigorous empirical detection of power-law
behaviour in the distribution of citations received by the most highly cited
scientific papers. We have used a large, novel data set on citations to
scientific papers published between 1998 and 2002 drawn from Scopus. The
power-law model is compared with a number of alternative models using a
likelihood ratio test. We have found that the power-law hypothesis is rejected
for around half of the Scopus fields of science. For these fields of science,
the Yule, power-law with exponential cut-off and log-normal distributions seem
to fit the data better than the pure power-law model. On the other hand, when
the power-law hypothesis is not rejected, it is usually empirically
indistinguishable from most of the alternative models. The pure power-law model
seems to be the best model only for the most highly cited papers in ""Physics
and Astronomy"". Overall, our results seem to support theories implying that the
most highly cited scientific papers follow the Yule, power-law with exponential
cut-off or log-normal distribution. Our findings suggest also that power laws
in citation distributions, when present, account only for a very small fraction
of the published papers (less than 1% for most of science fields) and that the
power-law scaling parameter (exponent) is substantially higher (from around 3.2
to around 4.7) than found in the older literature.
"
1028,"Analysis of Personalized Information Service System for Digital
  Libraries","  Along with the rapid development of digital library, digital library of the
third generation, taking the main characteristics of the personalized service,
has already become the mainstream today. This article first analyzes the
concept and characteristics of digital library, then elaborates the
inevitability of personalized service as well as the necessity of its
development and based on the Propose of establishing interested knowledge-base,
enumerates several ways of personalized service, at last according to
individualized service, this paper proposes the model of ""learningoriented
individual digital library"" and prospects the trends of digital library.
"
1029,"On the origins and the historical roots of the Higgs boson research from
  a bibliometric perspective","  Subject of our present paper is the analysis of the origins or historical
roots of the Higgs boson research from a bibliometric perspective, using a
segmented regression analysis in a reference publication year spectroscopy
(RPYS). Our analysis is based on the references cited in the Higgs boson
publications published since 1974. The objective of our analysis consists of
identifying concrete individual publications in the Higgs boson research
context to which the scientific community frequently had referred to. As a
consequence, we are interested in seminal works which contributed to a high
extent to the discovery of the Higgs boson. Our results show that researchers
in the Higgs boson field preferably refer to more recently published papers -
particular papers published since the beginning of the sixties. For example,
our analysis reveals seven major contributions which appeared within the
sixties: Englert and Brout (1964), Higgs (1964, 2 papers), and Guralnik et al.
(1964) on the Higgs mechanism as well as Glashow (1961), Weinberg (1967), and
Salam (1968) on the unification of weak and electromagnetic interaction. Even
if the Nobel Prize award highlights the outstanding importance of the work of
Peter Higgs and Francois Englert, bibliometrics offer the additional
possibility of getting hints to other publications in this research field
(especially to historical publications), which are of vital importance from the
expert point of view.
"
1030,"Growth rates of modern science: A bibliometric analysis based on the
  number of publications and cited references","  Many studies in information science have looked at the growth of science. In
this study, we re-examine the question of the growth of science. To do this we
(i) use current data up to publication year 2012 and (ii) analyse it across all
disciplines and also separately for the natural sciences and for the medical
and health sciences. Furthermore, the data are analysed with an advanced
statistical technique - segmented regression analysis - which can identify
specific segments with similar growth rates in the history of science. The
study is based on two different sets of bibliometric data: (1) The number of
publications held as source items in the Web of Science (WoS, Thomson Reuters)
per publication year and (2) the number of cited references in the publications
of the source items per cited reference year. We have looked at the rate at
which science has grown since the mid-1600s. In our analysis of cited
references we identified three growth phases in the development of science,
which each led to growth rates tripling in comparison with the previous phase:
from less than 1% up to the middle of the 18th century, to 2 to 3% up to the
period between the two world wars and 8 to 9% to 2012.
"
1031,Open science in machine learning,"  We present OpenML and mldata, open science platforms that provides easy
access to machine learning data, software and results to encourage further
study and application. They go beyond the more traditional repositories for
data sets and software packages in that they allow researchers to also easily
share the results they obtained in experiments and to compare their solutions
with those of others.
"
1032,"Journal topic citation potential and between-field comparisons: The
  topic normalized impact factor","  The journal impact factor is not comparable among fields of science and
social science because of systematic differences in publication and citation
behaviour across disciplines. In this work, a source normalization of the
journal impact factor is proposed. We use the aggregate impact factor of the
citing journals as a measure of the citation potential in the journal topic,
and we employ this citation potential in the normalization of the journal
impact factor to make it comparable between scientific fields. An empirical
application comparing some impact indicators with our topic normalized impact
factor in a set of 224 journals from four different fields shows that our
normalization, using the citation potential in the journal topic, reduces the
between-group variance with respect to the within-group variance in a higher
proportion than the rest of indicators analysed. The effect of journal
self-citations over the normalization process is also studied.
"
1033,Predicting Scientific Success Based on Coauthorship Networks,"  We address the question to what extent the success of scientific articles is
due to social influence. Analyzing a data set of over 100000 publications from
the field of Computer Science, we study how centrality in the coauthorship
network differs between authors who have highly cited papers and those who do
not. We further show that a machine learning classifier, based only on
coauthorship network centrality measures at time of publication, is able to
predict with high precision whether an article will be highly cited five years
after publication. By this we provide quantitative insight into the social
dimension of scientific publishing - challenging the perception of citations as
an objective, socially unbiased measure of scientific success.
"
1034,"Semantic Annotation and Search for Educational Resources Supporting
  Distance Learning","  Multimedia educational resources play an important role in education,
particularly for distance learning environments. With the rapid growth of the
multimedia web, large numbers of education articles video resources are
increasingly being created by several different organizations. It is crucial to
explore, share, reuse, and link these educational resources for better
e-learning experiences. Most of the video resources are currently annotated in
an isolated way, which means that they lack semantic connections. Thus,
providing the facilities for annotating these video resources is highly
demanded. These facilities create the semantic connections among video
resources and allow their metadata to be understood globally. Adopting Linked
Data technology, this paper introduces a video annotation and browser platform
with two online tools: Notitia and Sansu-Wolke. Notitia enables users to
semantically annotate video resources using vocabularies defined in the Linked
Data cloud. Sansu-Wolke allows users to browse semantically linked educational
video resources with enhanced web information from different online resources.
In the prototype development, the platform uses existing video resources for
education articles. The result of the initial development demonstrates the
benefits of applying Linked Data technology in the aspects of reusability,
scalability, and extensibility
"
1035,A distributed Integrity Catalog for digital repositories,"  Digital repositories, either digital preservation systems or archival
systems, periodically check the integrity of stored objects to assure users of
their correctness. To do so, prior solutions calculate integrity metadata and
require the repository to store it alongside the actual data objects. This
integrity metadata is essential for regularly verifying the correctness of the
stored data objects. To safeguard and detect damage to this metadata, prior
solutions rely on widely visible media, that is unaffiliated third parties, to
store and provide back digests of the metadata to verify it is intact. However,
they do not address recovery of the integrity metadata in case of damage or
attack by an adversary. In essence, they do not preserve this metadata. We
introduce IntegrityCatalog, a system that collects all integrity related
metadata in a single component, and treats them as first class objects,
managing both their integrity and their preservation. We introduce a
treap-based persistent authenticated dictionary managing arbitrary length
key/value pairs, which we use to store all integrity metadata, accessible
simply by object name. Additionally, IntegrityCatalog is a distributed system
that includes a network protocol that manages both corruption detection and
preservation of this metadata, using administrator-selected network peers with
two possible roles. Verifiers store and offer attestations on digests and have
minimal storage requirements, while preservers efficiently synchronize a
complete copy of the catalog to assist in recovery in case of a detected
catalog compromise on the local system. We describe our prototype
implementation of IntegrityCatalog, measure its performance empirically, and
demonstrate its effectiveness in real-world situations, with worst measured
throughput of approximately 1K insertions per second, and 2K verified search
operations per second.
"
1036,"AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based
  Assignments","  Plagiarism is one of the growing issues in academia and is always a concern
in Universities and other academic institutions. The situation is becoming even
worse with the availability of ample resources on the web. This paper focuses
on creating an effective and fast tool for plagiarism detection for text based
electronic assignments. Our plagiarism detection tool named AntiPlag is
developed using the tri-gram sequence matching technique. Three sets of text
based assignments were tested by AntiPlag and the results were compared against
an existing commercial plagiarism detection tool. AntiPlag showed better
results in terms of false positives compared to the commercial tool due to the
pre-processing steps performed in AntiPlag. In addition, to improve the
detection latency, AntiPlag applies a data clustering technique making it four
times faster than the commercial tool considered. AntiPlag could be used to
isolate plagiarized text based assignments from non-plagiarised assignments
easily. Therefore, we present AntiPlag, a fast and effective tool for
plagiarism detection on text based electronic assignments.
"
1037,"Learning Soft Linear Constraints with Application to Citation Field
  Extraction","  Accurately segmenting a citation string into fields for authors, titles, etc.
is a challenging task because the output typically obeys various global
constraints. Previous work has shown that modeling soft constraints, where the
model is encouraged, but not require to obey the constraints, can substantially
improve segmentation performance. On the other hand, for imposing hard
constraints, dual decomposition is a popular technique for efficient prediction
given existing algorithms for unconstrained inference. We extend the technique
to perform prediction subject to soft constraints. Moreover, with a technique
for performing inference given soft constraints, it is easy to automatically
generate large families of constraints and learn their costs with a simple
convex optimization problem during training. This allows us to obtain
substantial gains in accuracy on a new, challenging citation extraction
dataset.
"
1038,On the evolution and utility of annual citation indices,"  We study the statistics of citations made to the top ranked indexed journals
for Science and Social Science databases in the Journal Citation Reports using
different measures. Total annual citation and impact factor, as well as a third
measure called the annual citation rate are used to make the detailed analysis.
We observe that the distribution of the annual citation rate has an universal
feature - it shows a maximum at the rate scaled by half the average,
irrespective of how the journals are ranked, and even across Science and Social
Science journals, and fits well to log-Gumbel distribution. Correlations
between different quantities are studied and a comparative analysis of the
three measures is presented. The newly introduced annual citation rate factor
helps in understanding the effect of scaling the number of citation by the
total number of publications. The effect of the impact factor on authors
contributing to the journals as well as on editorial policies is also
discussed.
"
1039,Straightforward Bibliography Management in R with the RefManageR Package,"  This work introduces the R package RefManageR, which provides tools for
importing and working with bibliographic references. It extends the bibentry
class in R in a number of useful ways, including providing R with previously
unavailable support for BibLaTeX. BibLaTeX provides a superset of the
functionality of BibTeX, including full Unicode support, no memory limitations,
additional fields and entry types, and more sophisticated sorting of
references. RefManageR provides functions for citing and generating a
bibliography with hyperlinks for documents prepared with RMarkdown or RHTML.
Existing .bib files can be read into R and converted from BibTeX to BibLaTeX
and vice versa. References can also be imported via queries to NCBI's Entrez,
Zotero libraries, Google Scholar, and CrossRef. Additionally, references can be
created by reading PDFs stored on the user's machine with the help of Poppler.
Entries stored in the reference manager can be easily searched by any field, by
date ranges, and by various formats for name lists (author by last names,
translator by full names, etc.). Entries can also be updated, combined, sorted,
printed in a number of styles, and exported.
"
1040,"Determinants of Patent Citations in Biotechnology: An Analysis of Patent
  Influence Across the Industrial and Organizational Boundaries","  The present paper extends the literature investigating key drivers leading
certain patents to exert a stronger influence on the subsequent technological
developments (inventions) than other ones. We investigated six key
determinants, as (i) the use of scientific knowledge, (ii) the breadth of the
technological base, (iii) the existence of collaboration in patent development,
(iv) the number of claims, (v) the scope, and (vi) the novelty, and how the
effect of these determinants varies when patent influence - as measured by the
number of forward citations the patent received - is distinguished as within
and across the industrial and organizational boundaries. We conducted an
empirical analysis on a sample of 5671 patents granted to 293 US biotechnology
firms from 1976 to 2003. Results reveal that the contribution of the
determinants to patent influence differs across the domains that are identified
by the industrial and organizational boundaries. Findings, for example, show
that the use of scientific knowledge negatively affects patent influence
outside the biotechnology industry, while it positively contributes to make a
patent more relevant for the assignee's subsequent technological developments.
In addition, the broader the scope of a patent the higher the number of
citations the patent receives from subsequent non-biotechnology patents. This
relationship is inverted-U shaped when considering the influence of a patent on
inventions granted to other organizations than the patent's assignee. Finally,
the novelty of a patent is inverted-U related with the influence the patent
exerts on the subsequent inventions granted across the industrial and
organizational boundaries.
"
1041,Scientometrics: Untangling the topics,"  Measuring science is based on comparing articles to similar others. However,
keyword-based groups of thematically similar articles are dominantly small.
These small sizes keep the statistical errors of comparisons high. With the
growing availability of bibliographic data such statistical errors can be
reduced by merging methods of thematic grouping, citation networks and keyword
co-usage.
"
1042,"A RESTful API for exchanging Materials Data in the AFLOWLIB.org
  consortium","  The continued advancement of science depends on shared and reproducible data.
In the field of computational materials science and rational materials design
this entails the construction of large open databases of materials properties.
To this end, an Application Program Interface (API) following REST principles
is introduced for the AFLOWLIB.org materials data repositories consortium.
AUIDs (Aflowlib Unique IDentifier) and AURLs (Aflowlib Uniform Resource
locator) are assigned to the database resources according to a well-defined
protocol described herein, which enables the client to access, through
appropriate queries, the desired data for post-processing. This introduces a
new level of openness into the AFLOWLIB repository, allowing the community to
construct high-level work-flows and tools exploiting its rich data set of
calculated structural, thermodynamic, and electronic properties. Furthermore,
federating these tools would open the door to collaborative investigation of
the data by an unprecedented extended community of users to accelerate the
advancement of computational materials design and development.
"
1043,"Handling Large and Complex Data in a Photovoltaic Research Institution
  Using a Custom Laboratory Information Management System","  Twenty-five years ago the desktop computer started becoming ubiquitous in the
scientific lab. Researchers were delighted with its ability to both control
instrumentation and acquire data on a single system, but they were not
completely satisfied. There were often gaps in knowledge that they thought
might be gained if they just had more data and they could get the data faster.
Computer technology has evolved in keeping with Moore's Law meeting those
desires; however those improvement have of late become both a boon and bane for
researchers. Computers are now capable of producing high speed data streams
containing terabytes of information; capabilities that evolved faster than
envisioned last century. Software to handle large scientific data sets has not
kept up. How much information might be lost through accidental mismanagement or
how many discoveries are missed through data overload are now vital questions.
An important new task in most scientific disciplines involves developing
methods to address those issues and to create software that can handle large
data sets with an eye towards scalability. This software must create archived,
indexed, and searchable data from heterogeneous instrumentation for the
implementation of a strong data-driven materials development strategy. At the
National Center for Photovoltaics in the National Renewable Energy Lab, we
began development a few years ago on a Laboratory Information Management System
(LIMS) designed to handle lab-wide scientific data acquisition, management,
processing, and mining needs for physics and materials science data. and with a
specific focus on future scalability for new equipment or research focuses. We
will present the decisions, process, and problems we went through while
building our LIMS for materials research, its current operational state, and
our steps for future development.
"
1044,Principles of scientific research team formation and evolution,"  Research teams are the fundamental social unit of science, and yet there is
currently no model that describes their basic property: size. In most fields
teams have grown significantly in recent decades. We show that this is partly
due to the change in the character of team-size distribution. We explain these
changes with a comprehensive yet straightforward model of how teams of
different sizes emerge and grow. This model accurately reproduces the evolution
of empirical team-size distribution over the period of 50 years. The modeling
reveals that there are two modes of knowledge production. The first and more
fundamental mode employs relatively small, core teams. Core teams form by a
Poisson process and produce a Poisson distribution of team sizes in which
larger teams are exceedingly rare. The second mode employs extended teams,
which started as core teams, but subsequently accumulated new members
proportional to the past productivity of their members. Given time, this mode
gives rise to a power-law tail of large teams (10-1000 members), which features
in many fields today. Based on this model we construct an analytical functional
form that allows the contribution of different modes of authorship to be
determined directly from the data and is applicable to any field. The model
also offers a solid foundation for studying other social aspects of science,
such as productivity and collaboration.
"
1045,People Like Us: Mining Scholarly Data for Comparable Researchers,"  We present the problem of finding comparable researchers for any given
researcher. This problem has many motivations. Firstly, know thyself. The
answers of where we stand among research community and who we are most alike
may not be easily found by existing evaluations of ones' research mainly based
on citation counts. Secondly, there are many situations where one needs to find
comparable researchers e.g., for reviewing peers, constructing programming
committees or compiling teams for grants. It is often done through an ad hoc
and informal basis. Utilizing the large scale scholarly data accessible on the
web, we address the problem of automatically finding comparable researchers. We
propose a standard to quantify the quality of research output, via the quality
of publishing venues. We represent a researcher as a sequence of her
publication records, and develop a framework of comparison of researchers by
sequence matching. Several variations of comparisons are considered including
matching by quality of publication venue and research topics, and performing
prefix matching. We evaluate our methods on a large corpus and demonstrate the
effectiveness of our methods through examples. In the end, we identify several
promising directions for further work.
"
1046,"Social and Natural Sciences Differ in Their Research Strategies, Adapted
  to Work for Different Knowledge Landscapes","  Do different fields of knowledge require different research strategies? A
numerical model exploring different virtual knowledge landscapes, revealed two
diverging optimal search strategies. Trend following is maximized when the
popularity of new discoveries determine the number of individuals researching
it. This strategy works best when many researchers explore few large areas of
knowledge. In contrast, individuals or small groups of researchers are better
in discovering small bits of information in dispersed knowledge landscapes.
Bibliometric data of scientific publications showed a continuous bipolar
distribution of these strategies, ranging from natural sciences, with highly
cited publications in journals containing a large number of articles, to the
social sciences, with rarely cited publications in many journals containing a
small number of articles. The natural sciences seem to adapt their research
strategies to landscapes with large concentrated knowledge clusters, whereas
social sciences seem to have adapted to search in landscapes with many small
isolated knowledge clusters. Similar bipolar distributions were obtained when
comparing levels of insularity estimated by indicators of international
collaboration and levels of country-self citations: researchers in academic
areas with many journals such as social sciences, arts and humanities, were the
most isolated, and that was true in different regions of the world. The work
shows that quantitative measures estimating differences between academic
disciplines improve our understanding of different research strategies,
eventually helping interdisciplinary research and may be also help improve
science policies worldwide.
"
1047,The Unified Astronomy Thesaurus,"  The Unified Astronomy Thesaurus (UAT) is an open, interoperable and
community-supported thesaurus which unifies the existing divergent and isolated
Astronomy & Astrophysics vocabularies into a single high-quality,
freely-available open thesaurus formalizing astronomical concepts and their
inter-relationships. The UAT builds upon the existing IAU Thesaurus with major
contributions from the astronomy portions of the thesauri developed by the
Institute of Physics Publishing, the American Institute of Physics, and SPIE.
We describe the effort behind the creation of the UAT and the process through
which we plan to maintain the document updated through broad community
participation.
"
1048,The State of Technology for Digital Archiving,"  The Windsor Study Group on Digital Archiving was commissioned to recommend
strategies, policies, and technologies necessary for ensuring the integrity and
longevity of electronic publications. The goal of this work is to inform
institutions of the challenges and opportunities faced by information stewards
in fulfilling their mission of guaranteeing a permanent and authoritative
scholarly record in the digital age. This white paper focuses specifically on
the technological dimensions of digital archiving. It provides an analysis of
the current state of technologies as well as a forecast of how digital archive
systems are likely to evolve over the next decade. The thesis of this white
paper is that technology does not present a barrier to long term digital
archiving, but instead presents an opportunity to harness the current and
future power of these technologies to create the architectural underpinnings of
a comprehensive digital archiving strategy.
"
1049,"Motif-based success scores in coauthorship networks are highly sensitive
  to author name disambiguation","  Following the work of Krumov et al. [Eur. Phys. J. B 84, 535 (2011)] we
revisit the question whether the usage of large citation datasets allows for
the quantitative assessment of social (by means of coauthorship of
publications) influence on the progression of science. Applying a more
comprehensive and well-curated dataset containing the publications in the
journals of the American Physical Society during the whole 20th century we find
that the measure chosen in the original study, a score based on small induced
subgraphs, has to be used with caution, since the obtained results are highly
sensitive to the exact implementation of the author disambiguation task.
"
1050,Identifying User Behavior in domain-specific Repositories,"  This paper presents an analysis of the user behavior of two different
domain-specific repositories. The web analytic tool etracker was used to gain a
first overall insight into the user behavior of these repositories. Moreover,
we extended our work to describe an apache web log analysis approach which
focuses on the identification of the user behavior. Therefore the user traffic
within our systems is visualized using chord diagrams. We could find that
recommendations are used frequently and users do rarely combine searching with
faceting or filtering.
"
1051,A quantitative perspective on ethics in large team science,"  The gradual crowding out of singleton and small team science by large team
endeavors is challenging key features of research culture. It is therefore
important for the future of scientific practice to reflect upon the individual
scientist's ethical responsibilities within teams. To facilitate this
reflection we show labor force trends in the US revealing a skewed growth in
academic ranks and increased levels of competition for promotion within the
system; we analyze teaming trends across disciplines and national borders
demonstrating why it is becoming difficult to distribute credit and to avoid
conflicts of interest; and we use more than a century of Nobel prize data to
show how science is outgrowing its old institutions of singleton awards. Of
particular concern within the large team environment is the weakening of the
mentor-mentee relation, which undermines the cultivation of virtue ethics
across scientific generations. These trends and emerging organizational
complexities call for a universal set of behavioral norms that transcend team
heterogeneity and hierarchy. To this end, our expository analysis provides a
survey of ethical issues in team settings to inform science ethics education
and science policy.
"
1052,"Inter-rater reliability and convergent validity of F1000Prime peer
  review","  Peer review is the backbone of modern science. F1000Prime is a
post-publication peer review system of the biomedical literature (papers from
medical and biological journals). This study is concerned with the inter-rater
reliability and convergent validity of the peer recommendations formulated in
the F1000Prime peer review system. The study is based on around 100,000 papers
with recommendations from Faculty members. Even if intersubjectivity plays a
fundamental role in science, the analyses of the reliability of the F1000Prime
peer review system show a rather low level of agreement between Faculty
members. This result is in agreement with most other studies which have been
published on the journal peer review system. Logistic regression models are
used to investigate the convergent validity of the F1000Prime peer review
system. As the results show, the proportion of highly cited papers among those
selected by the Faculty members is significantly higher than expected. In
addition, better recommendation scores are also connected with better
performance of the papers.
"
1053,"The European Union, China, and the United States in the Top-1% and
  Top-10% Layers of Most-Frequently-Cited Publications: Competition and
  Collaborations","  The percentages of shares of world publications of the European Union and its
member states, China, and the United States have been represented differently
as a result of using different databases. An analytical variant of the
Web-of-Science (of Thomson Reuters) enables us to study the dynamics in the
world publication system in terms of the field-normalized top-1% and top-10%
most-frequently-cited publications. Comparing the EU28, USA, and China at the
global level shows a top-level dynamics that is different from the analysis in
terms of shares of publications: the United States remains far more productive
in the top-1% of all papers; China drops out of the competition for elite
status; and the EU28 increased its share among the top-cited papers from
2000-2010. Some of the EU28 member states overtook the U.S. during this decade,
but a clear divide remains between EU15 (Western Europe) and the Accession
Countries. Network analysis shows that internationally co-authored top-1%
publications perform far above expectation and also above top-10% ones. In
2005, China was embedded in this top-layer of internationally co-authored
publications. These publications often involve more than a single European
nation.
"
1054,"How well developed are altmetrics? A cross-disciplinary analysis of the
  presence of 'alternative metrics' in scientific publications","  In this paper an analysis of the presence and possibilities of altmetrics for
bibliometric and performance analysis is carried out. Using the web based tool
Impact Story, we collected metrics for 20,000 random publications from the Web
of Science. We studied both the presence and distribution of altmetrics in the
set of publications, across fields, document types and over publication years,
as well as the extent to which altmetrics correlate with citation indicators.
The main result of the study is that the altmetrics source that provides the
most metrics is Mendeley, with metrics on readerships for 62.6% of all the
publications studied, other sources only provide marginal information. In terms
of relation with citations, a moderate spearman correlation (r=0.49) has been
found between Mendeley readership counts and citation indicators. Other
possibilities and limitations of these indicators are discussed and future
research lines are outlined.
"
1055,"Text Based Approach For Indexing And Retrieval Of Image And Video: A
  Review","  Text data present in multimedia contain useful information for automatic
annotation, indexing. Extracted information used for recognition of the overlay
or scene text from a given video or image. The Extracted text can be used for
retrieving the videos and images. In this paper, firstly, we are discussed the
different techniques for text extraction from images and videos. Secondly, we
are reviewed the techniques for indexing and retrieval of image and videos by
using extracted text.
"
1056,"Aggregated journal-journal citation relations in Scopus and
  Web-of-Science matched and compared in terms of networks, maps, and
  interactive overlays","  We compare the network of aggregated journal-journal citation relations
provided by the Journal Citation Reports (JCR) 2012 of the Science and Social
Science Citation Indexes (SCI and SSCI) with similar data based on Scopus 2012.
First, global maps were developed for the two sets separately; sets of
documents can then be compared using overlays to both maps. Using fuzzy-string
matching and ISSN numbers, we were able to match 10,524 journal names between
the two sets; that is, 96.4% of the 10,936 journals contained in JCR or 51.2%
of the 20,554 journals covered by Scopus. Network analysis was then pursued on
the set of journals shared between the two databases and the two sets of unique
journals. Citations among the shared journals are more comprehensively covered
in JCR than Scopus, so the network in JCR is denser and more connected than in
Scopus. The ranking of shared journals in terms of indegree (that is, numbers
of citing journals) or total citations is similar in both databases overall
(Spearman's \r{ho} > 0.97), but some individual journals rank very differently.
Journals that are unique to Scopus seem to be less important--they are citing
shared journals rather than being cited by them--but the humanities are covered
better in Scopus than in JCR.
"
1057,Completing $h$,"  Nearly a decade ago, the science community was introduced to the $h$-index, a
proposed statistical measure of the collective impact of the publications of
any individual researcher. It is of course undeniable that any method of
reducing a complex data set to a single number will necessarily have certain
limitations and introduce certain biases. However, in this paper we point out
that the definition of the $h$-index actually suffers from something far
deeper: a hidden mathematical incompleteness intrinsic to its definition. In
particular, we point out that one critical step within the definition of $h$
has been missed until now, resulting in an index which only achieves its stated
objectives under certain rather limited circumstances. For example, this
incompleteness explains why the $h$-index ultimately has more utility in
certain scientific subfields than others. In this paper, we expose the origin
of this incompleteness and then also propose a method of completing the
definition of $h$ in a way which remains close to its original guiding
principle. As a result, this ""completed"" $h$ not only reduces to the usual $h$
in cases where the $h$-index already achieves its objectives, but also extends
the validity of the $h$-index into situations where it currently does not.
"
1058,Automatic Detection of Reuses and Citations in Literary Texts,"  For more than forty years now, modern theories of literature (Compagnon,
1979) insist on the role of paraphrases, rewritings, citations, reciprocal
borrowings and mutual contributions of any kinds. The notions of
intertextuality, transtextuality, hypertextuality/hypotextuality, were
introduced in the seventies and eighties to approach these phenomena. The
careful analysis of these references is of particular interest in evaluating
the distance that the creator voluntarily introduces with his/her masters.
Phoebus is collaborative project that makes computer scientists from the
University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary
teams of Paris-Sorbonne University with the aim to develop efficient tools for
literary studies that take advantage of modern computer science techniques. In
this context, we have developed a piece of software that automatically detects
and explores networks of textual reuses in classical literature. This paper
describes the principles on which is based this program, the significant
results that have already been obtained and the perspectives for the near
future.
"
1059,"Bibliometric Indicators of Young Authors in Astrophysics: Can Later
  Stars be Predicted?","  We test 16 bibliometric indicators with respect to their validity at the
level of the individual researcher by estimating their power to predict later
successful researchers. We compare the indicators of a sample of astrophysics
researchers who later co-authored highly cited papers before their first
landmark paper with the distributions of these indicators over a random control
group of young authors in astronomy and astrophysics. We find that field and
citation-window normalisation substantially improves the predicting power of
citation indicators. The two indicators of total influence based on citation
numbers normalised with expected citation numbers are the only indicators which
show differences between later stars and random authors significant on a 1%
level. Indicators of paper output are not very useful to predict later stars.
The famous $h$-index makes no difference at all between later stars and the
random control group.
"
1060,"The substantive and practical significance of citation impact
  differences between institutions: Guidelines for the analysis of percentiles
  using effect sizes and confidence intervals","  In our chapter we address the statistical analysis of percentiles: How should
the citation impact of institutions be compared? In educational and
psychological testing, percentiles are already used widely as a standard to
evaluate an individual's test scores - intelligence tests for example - by
comparing them with the percentiles of a calibrated sample. Percentiles, or
percentile rank classes, are also a very suitable method for bibliometrics to
normalize citations of publications in terms of the subject category and the
publication year and, unlike the mean-based indicators (the relative citation
rates), percentiles are scarcely affected by skewed distributions of citations.
The percentile of a certain publication provides information about the citation
impact this publication has achieved in comparison to other similar
publications in the same subject category and publication year. Analyses of
percentiles, however, have not always been presented in the most effective and
meaningful way. New APA guidelines (American Psychological Association, 2010)
suggest a lesser emphasis on significance tests and a greater emphasis on the
substantive and practical significance of findings. Drawing on work by Cumming
(2012) we show how examinations of effect sizes (e.g. Cohen's d statistic) and
confidence intervals can lead to a clear understanding of citation impact
differences.
"
1061,"BRICS countries and scientific excellence: A bibliometric analysis of
  most frequently-cited papers","  The BRICS countries (Brazil, Russia, India, and China, and South Africa) are
noted for their increasing participation in science and technology. The
governments of these countries have been boosting their investments in research
and development to become part of the group of nations doing research at a
world-class level. This study investigates the development of the BRICS
countries in the domain of top-cited papers (top 10% and 1% most frequently
cited papers) between 1990 and 2010. To assess the extent to which these
countries have become important players on the top level, we compare the BRICS
countries with the top-performing countries worldwide. As the analyses of the
(annual) growth rates show, with the exception of Russia, the BRICS countries
have increased their output in terms of most frequently-cited papers at a
higher rate than the top-cited countries worldwide. In a further step of
analysis for this study, we generate co-authorship networks among authors of
highly cited papers for four time points to view changes in BRICS participation
(1995, 2000, 2005, and 2010). Here, the results show that all BRICS countries
succeeded in becoming part of this network, whereby the Chinese collaboration
activities focus on the USA.
"
1062,Inheritance patterns in citation networks reveal scientific memes,"  Memes are the cultural equivalent of genes that spread across human culture
by means of imitation. What makes a meme and what distinguishes it from other
forms of information, however, is still poorly understood. Our analysis of
memes in the scientific literature reveals that they are governed by a
surprisingly simple relationship between frequency of occurrence and the degree
to which they propagate along the citation graph. We propose a simple
formalization of this pattern and we validate it with data from close to 50
million publication records from the Web of Science, PubMed Central, and the
American Physical Society. Evaluations relying on human annotators, citation
network randomizations, and comparisons with several alternative approaches
confirm that our formula is accurate and effective, without a dependence on
linguistic or ontological knowledge and without the application of arbitrary
thresholds or filters.
"
1063,"Comparison of the Research Effectiveness of Chemistry Nobelists and
  Fields Medalist Mathematicians with Google Scholar: the Yule-Simon Model","  This paper uses the Yule-Simon model to estimate to what extent the work of
chemistry Nobelists and Fields medalist mathematicians is incorporated into the
knowledge corpus of their disciplines as measured by Google Scholar inlinks.
Due to differences in the disciplines and prizes, it finds that the work of
chemistry Nobelists is better incorporated than that of Fields medalists.
"
1064,"CitNetExplorer: A new software tool for analyzing and visualizing
  citation networks","  We present CitNetExplorer, a new software tool for analyzing and visualizing
citation networks of scientific publications. CitNetExplorer can for instance
be used to study the development of a research field, to delineate the
literature on a research topic, and to support literature reviewing. We first
introduce the main concepts that need to be understood when working with
CitNetExplorer. We then demonstrate CitNetExplorer by using the tool to analyze
the scientometric literature and the literature on community detection in
networks. Finally, we discuss some technical details on the construction,
visualization, and analysis of citation networks in CitNetExplorer.
"
1065,"Mathoid: Robust, Scalable, Fast and Accessible Math Rendering for
  Wikipedia","  Wikipedia is the first address for scientists who want to recap basic
mathematical and physical laws and concepts. Today, formulae in those pages are
displayed as Portable Network Graphics images. Those images do not integrate
well into the text, can not be edited after copying, are inaccessible to screen
readers for people with special needs, do not support line breaks for small
screens and do not scale for high resolution devices. Mathoid improves this
situation and converts formulae specified by Wikipedia editors in a TeX-like
input format to MathML, with Scalable Vector Graphics images as a fallback
solution.
"
1066,"Math Indexer and Searcher Web Interface: Towards Fulfillment of
  Mathematicians' Information Needs","  We are designing and developing a web user interface for digital mathematics
libraries called WebMIaS. It allows queries to be expressed by mathematicians
through a faceted search interface. Users can combine standard textual
autocompleted keywords with keywords in the form of mathematical formulae in
LaTeX or MathML formats. Formulae are shown rendered by the web browser
on-the-fly for users' feedback. We describe WebMIaS design principles and our
experiences deploying in the European Digital Mathematics Library (EuDML). We
further describe the issues addressed by formulae canonicalization and by
extending the MIaS indexing engine with Content MathML support.
"
1067,Digital Repository of Mathematical Formulae,"  The purpose of the NIST Digital Repository of Mathematical Formulae (DRMF) is
to create a digital compendium of mathematical formulae for orthogonal
polynomials and special functions (OPSF) and of associated mathematical data.
The DRMF addresses needs of working mathematicians, physicists and engineers:
providing a platform for publication and interaction with OPSF formulae on the
web. Using MediaWiki extensions and other existing technology (such as software
and macro collections developed for the NIST Digital Library of Mathematical
Functions), the DRMF acts as an interactive web domain for OPSF formulae.
Whereas Wikipedia and other web authoring tools manifest notions or
descriptions as first class objects, the DRMF does that with mathematical
formulae. See http://gw32.iu.xsede.org/index.php/Main_Page.
"
1068,Recent Developments in China-U.S. Cooperation in Science,"  China's remarkable gains in science over the past 25 years have been well
documented (e.g., Jin and Rousseau, 2005a; Zhou and Leydesdorff, 2006; Shelton
& Foland, 2009) but it is less well known that China and the United States have
become each other's top collaborating country. Science and technology has been
a primary vehicle for growing the bilateral relationship between China and the
United States since the opening of relations between the two countries in the
late 1970s. During the 2000s, the scientific relationship between China and the
United States--as measured in coauthored papers--showed significant growth.
Chinese scientists claim first authorship much more frequently than U.S.
counterparts by the end of the decade. The sustained rate of increase of
collaboration with one other country is unprecedented on the U.S. side. Even
growth in relations with eastern European nations does not match the growth in
the relationship between China and the United States. Both countries can
benefit from the relationship, but for the U.S., greater benefit would come
from a more targeted strategy.
"
1069,E-books and Graphics with LaTeXML,"  Marked by the highlights of native generation of EPUB E-books and TikZ
support for creating SVG images, we present an annual report of LaTeXML
development in 2013. LaTeXML provides a reimplementation of the $\TeX$ parser,
geared towards preserving macro semantics; it supports an array of output
formats, notably HTML5, EPUB, XHTML and its own $\LaTeX$-near XML. Other
highlights include enhancing performance when used inside high-throughput
build-systems, via incorporating a native ZIP archive workflow, as well as a
simplified installation procedure that now allows to deploy LaTeXML as a cloud
service. To this end, we also introduce an official plugin-based scheme for
publishing new features that go beyond the core scope of LaTeXML, such as web
services or unconventional post-processors. The software suite has now migrated
to GitHub and we welcome forks and patches from the wider FLOSS community.
"
1070,NNexus Reloaded,"  Interlinking knowledge is one of the cornerstones of online collaboration.
While wiki systems typically rely on links supplied by authors, in the early
2000s the mathematics encyclopedia at PlanetMath.org introduced a feature that
provides automatic linking for previously defined concepts. The NNexus software
suite was developed to support the necessary subtasks of concept indexing,
concept discovery and link-annotation. In this paper, we describe our recent
reimplementation and revisioning of the NNexus system.
"
1071,LaTeXML 2012 - A Year of LaTeXML,"  LaTeXML, a $\TeX$ to XML converter, is being used in a wide range of MKM
applications. In this paper, we present a progress report for the 2012 calendar
year. Noteworthy enhancements include: increased coverage such as Wikipedia
syntax; enhanced capabilities such as embeddable JavaScript and CSS resources
and RDFa support; a web service for remote processing via web-sockets; along
with general accuracy and reliability improvements.
"
1072,"Empirical Evidences in Citation-Based Search Engines: Is Microsoft
  Academic Search dead?","  The goal of this working paper is to summarize the main empirical evidences
provided by the scientific community as regards the comparison between the two
main citation based academic search engines: Google Scholar and Microsoft
Academic Search, paying special attention to the following issues: coverage,
correlations between journal rankings, and usage of these academic search
engines. Additionally, selfelaborated data is offered, which are intended to
provide current evidence about the popularity of these tools on the Web, by
measuring the number of rich files PDF, PPT and DOC in which these tools are
mentioned, the amount of external links that both products receive, and the
search queries frequency from Google Trends. The poor results obtained by MAS
led us to an unexpected and unnoticed discovery: Microsoft Academic Search is
outdated since 2013. Therefore, the second part of the working paper aims at
advancing some data demonstrating this lack of update. For this purpose we
gathered the number of total records indexed by Microsoft Academic Search since
2000. The data shows an abrupt drop in the number of documents indexed from
2,346,228 in 2010 to 8,147 in 2013 and 802 in 2014. This decrease is offered
according to 15 thematic areas as well. In view of these problems it seems
logical not only that Microsoft Academic Searchwas poorly used to search for
articles by academics and students, who mostly use Google or Google Scholar,
but virtually ignored by bibliometricians
"
1073,"Editorial for the Bibliometric-enhanced Information Retrieval Workshop
  at ECIR 2014","  This first ""Bibliometric-enhanced Information Retrieval"" (BIR 2014) workshop
aims to engage with the IR community about possible links to bibliometrics and
scholarly communication. Bibliometric techniques are not yet widely used to
enhance retrieval processes in digital libraries, although they offer
value-added effects for users. In this workshop we will explore how statistical
modelling of scholarship, such as Bradfordizing or network analysis of
co-authorship network, can improve retrieval services for specific communities,
as well as for large, cross-domain collections. This workshop aims to raise
awareness of the missing link between information retrieval (IR) and
bibliometrics / scientometrics and to create a common ground for the
incorporation of bibliometric-enhanced services into retrieval at the digital
library interface. Our interests include information retrieval, information
seeking, science modelling, network analysis, and digital libraries. The goal
is to apply insights from bibliometrics, scientometrics, and informetrics to
concrete practical problems of information retrieval and browsing.
"
1074,An Introduction to the Patstat Database with Example Queries,"  This paper provides an introduction to the Patstat patent database. It offers
guided examples of ten popular queries that are relevant for research purposes
and that cover the most important data tables. It is targeted at academic
researchers and practitioners willing to learn the basics of the database.
"
1075,Aca 2.0: Questions and Answers,"  ""Academia 2.0"" is a proposal to organize scientific publishing around true
peer-to-peer distributed dissemination channels and eliminate the traditional
role of the academic publisher. This model will be first presented at the 2014
workshop on Reproducible Research Methodologies and New Publication Models in
Computer Engineering (TRUST'14) in the form of a high-level overview, so as to
stimulate discussion and gather feedback on its merits and feasibility. This
report complements the 6-page introductory article presented at TRUST, by
detailing the review processes, some use scenarios and answering the reviewer's
comments in detail.
"
1076,Automated Attribution and Intertextual Analysis,"  In this work, we employ quantitative methods from the realm of statistics and
machine learning to develop novel methodologies for author attribution and
textual analysis. In particular, we develop techniques and software suitable
for applications to Classical study, and we illustrate the efficacy of our
approach in several interesting open questions in the field. We apply our
numerical analysis techniques to questions of authorship attribution in the
case of the Greek tragedian Euripides, to instances of intertextuality and
influence in the poetry of the Roman statesman Seneca the Younger, and to cases
of ""interpolated"" text with respect to the histories of Livy.
"
1077,"x-index: a fantastic new indicator for quantifying a scientist's
  scientific impact","  h-index has become the most popular indicator for quantifying a scientist's
scientific impact in various scientific fields. h-index is defined as the
largest number of papers with citation number larger than or equal to h and it
treats each citation equally. However, different citations usually come from
different papers with different influence and quality, and a citation from a
highly influential paper is a greater recognition of the target paper than a
citation from an ordinary paper. Based on this assumption, we proposed a new
indicator named x-index to quantify a scientist's scientific impact by
considering only the citations coming from influential papers. x-index is
defined as the largest number of papers with influential citation number larger
than or equal to x, where each influential citation comes from a paper for
which the average ACNPP (Average Citation Number Per Paper) of its authors
larger than or equal to x . Through analysis on the APS dataset, we find that
the proposed x-index has much better ability to discriminate between Physics
Prize Winners and ordinary physicists.
"
1078,"Comment on ""Quantifying Long-term Scientific Impact""","  The paper comments on ""Quantifying long-term scientific impact"". It indicates
that there is a mistake of [D. S. Wang , C. Song, A. L. Barabasi, Quantifying
long-term scientific impact, Science 342, 127 (2013), arXiv:1306.3293].
"
1079,A Retrieval Mechanism for Multi-versioned Digital Collection Using TAG,"  As the marvellous growth of the digital library in each year, the problems
with indexing and searching a digital library is increased in a high rate. When
the researchers search for the earlier versions, only a few recent versions in
the back volumes can be retrieved soon. It is unpredictable that researchers
require the earlier versions in a specific boundary. In order to facilitate the
researchers, who may access any version at any time, we propose a VTAG
technique for indexing. Our experiments indicate that the proposed retrieval
technique, VTAG, effectively retrieves any version in considerable amount of
time than the existing method.
"
1080,Why we need an independent index of the Web,"  The path to greater diversity, as we have seen, cannot be achieved by merely
hoping for a new search engine nor will government support for a single
alternative achieve this goal. What is instead required is to create the
conditions that will make establishing such a search engine possible in the
first place. I describe how building and maintaining a proprietary index is the
greatest deterrent to such an undertaking. We must first overcome this
obstacle. Doing so will still not solve the problem of the lack of diversity in
the search engine marketplace. But it may establish the conditions necessary to
achieve that desired end.
"
1081,Support for Various HTTP Methods on the Web,"  We examine how well various HTTP methods are supported by public web
services. We sample 40,870 live URIs from the DMOZ collection (a curated
directory of World Wide Web URIs) and found that about 55% URIs claim support
(in the Allow header) for GET and POST methods, but less than 2% of the URIs
claim support for one or more of PUT, PATCH, or DELETE methods.
"
1082,"The Past and the Future of Holocaust Research: From Disparate Sources to
  an Integrated European Holocaust Research Infrastructure","  The European Holocaust Research Infrastructure (EHRI) has been set up by the
European Union to create a sustainable complex of services for researchers.
EHRI will bring together information about dispersed collections, based on
currently more than 20 partner organisations in 13 countries and many other
archives. EHRI, which brings together historians, archivists and specialists in
digital humanities, strives to develop innovative on-line tools for finding,
researching and sharing knowledge about the Holocaust. While connecting
information about Holocaust collections, it strives to create tools and
approaches applicable to other digital archival projects. The paper describes
its current progress and collaboration across the disciplines involved.
"
1083,"Mapping the UK Webspace: Fifteen Years of British Universities on the
  Web","  This paper maps the national UK web presence on the basis of an analysis of
the .uk domain from 1996 to 2010. It reviews previous attempts to use web
archives to understand national web domains and describes the dataset. Next, it
presents an analysis of the .uk domain, including the overall number of links
in the archive and changes in the link density of different second-level
domains over time. We then explore changes over time within a particular
second-level domain, the academic subdomain .ac.uk, and compare linking
practices with variables, including institutional affiliation, league table
ranking, and geographic location. We do not detect institutional affiliation
affecting linking practices and find only partial evidence of league table
ranking affecting network centrality, but find a clear inverse relationship
between the density of links and the geographical distance between
universities. This echoes prior findings regarding offline academic activity,
which allows us to argue that real-world factors like geography continue to
shape academic relationships even in the Internet age. We conclude with
directions for future uses of web archive resources in this emerging area of
research.
"
1084,"Developing Corpus-based Translation Methods between Informal and Formal
  Mathematics: Project Description","  The goal of this project is to (i) accumulate annotated informal/formal
mathematical corpora suitable for training semi-automated translation between
informal and formal mathematics by statistical machine-translation methods,
(ii) to develop such methods oriented at the formalization task, and in
particular (iii) to combine such methods with learning-assisted automated
reasoning that will serve as a strong semantic component. We describe these
ideas, the initial set of corpora, and some initial experiments done over them.
"
1085,Search Interfaces for Mathematicians,"  Access to mathematical knowledge has changed dramatically in recent years,
therefore changing mathematical search practices. Our aim with this study is to
scrutinize professional mathematicians' search behavior. With this
understanding we want to be able to reason why mathematicians use which tool
for what search problem in what phase of the search process. To gain these
insights we conducted 24 repertory grid interviews with mathematically inclined
people (ranging from senior professional mathematicians to non-mathematicians).
From the interview data we elicited patterns for the user group
""mathematicians"" that can be applied when understanding design issues or
creating new designs for mathematical search interfaces.
"
1086,Fighting Authorship Linkability with Crowdsourcing,"  Massive amounts of contributed content -- including traditional literature,
blogs, music, videos, reviews and tweets -- are available on the Internet
today, with authors numbering in many millions. Textual information, such as
product or service reviews, is an important and increasingly popular type of
content that is being used as a foundation of many trendy community-based
reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown
that, due partly to their specialized/topical nature, sets of reviews authored
by the same person are readily linkable based on simple stylometric features.
In practice, this means that individuals who author more than a few reviews
under different accounts (whether within one site or across multiple sites) can
be linked, which represents a significant loss of privacy.
  In this paper, we start by showing that the problem is actually worse than
previously believed. We then explore ways to mitigate authorship linkability in
community-based reviewing. We first attempt to harness the global power of
crowdsourcing by engaging random strangers into the process of re-writing
reviews. As our empirical results (obtained from Amazon Mechanical Turk)
clearly demonstrate, crowdsourcing yields impressively sensible reviews that
reflect sufficiently different stylometric characteristics such that prior
stylometric linkability techniques become largely ineffective. We also consider
using machine translation to automatically re-write reviews. Contrary to what
was previously believed, our results show that translation decreases authorship
linkability as the number of intermediate languages grows. Finally, we explore
the combination of crowdsourcing and machine translation and report on the
results.
"
1087,Case study to approaches to finding patterns in citation networks,"  Analysis of a dataset including a network of LED patents and their metadata
is carried out using several methods in order to answer questions about the
domain. We are interested in finding the relationship between the metadata and
the network structure; for example, are central patents in the network produced
by larger or smaller companies? We begin by exploring the structure of the
network without any metadata, applying known techniques in citation analysis
and a simple clustering scheme. These techinques are then combined with
metadata analysis to draw preliminary conclusions about the dataset.
"
1088,Grid-based Search Technique for Massive Academic Publications,"  The numerical size of academic publications that are being published in
recent years had grown rapidly. Accessing and searching massive academic
publications that are distributed over several locations need large amount of
computing resources to increase the system performance. Therefore, many
grid-based search techniques were proposed to provide flexible methods for
searching the distributed extensive data. This paper proposes search technique
that is capable of searching the extensive publications by utilizing grid
computing technology. The search technique is implemented as interconnected
grid services to offer a mechanism to access different data locations. The
experimental result shows that the grid-based search technique has enhanced the
performance of the search.
"
1089,"Research at UNIS - The University Centre in Svalbard. A bibliometric
  study","  The scientific output 1994-2014 of the University Centre in Svalbard (UNIS)
was bibliometrically analysed. It was found that the majority of the papers
have been published as international cooperations and rank above world average.
Analysis of the content of the papers reveals that UNIS works and publishes in
a wide variety of scientific topics.
"
1090,The Dawn of Open Access to Phylogenetic Data,"  The scientific enterprise depends critically on the preservation of and open
access to published data. This basic tenet applies acutely to phylogenies
(estimates of evolutionary relationships among species). Increasingly,
phylogenies are estimated from increasingly large, genome-scale datasets using
increasingly complex statistical methods that require increasing levels of
expertise and computational investment. Moreover, the resulting phylogenetic
data provide an explicit historical perspective that critically informs
research in a vast and growing number of scientific disciplines. One such use
is the study of changes in rates of lineage diversification (speciation -
extinction) through time. As part of a meta-analysis in this area, we sought to
collect phylogenetic data (comprising nucleotide sequence alignment and tree
files) from 217 studies published in 46 journals over a 13-year period. We
document our attempts to procure those data (from online archives and by direct
request to corresponding authors), and report results of analyses (using
Bayesian logistic regression) to assess the impact of various factors on the
success of our efforts. Overall, complete phylogenetic data for ~60% of these
studies are effectively lost to science. Our study indicates that phylogenetic
data are more likely to be deposited in online archives and/or shared upon
request when: (1) the publishing journal has a strong data-sharing policy; (2)
the publishing journal has a higher impact factor, and; (3) the data are
requested from faculty rather than students. Although the situation appears
dire, our analyses suggest that it is far from hopeless: recent initiatives by
the scientific community -- including policy changes by journals and funding
agencies -- are improving the state of affairs.
"
1091,"Assessing Educational Research -- An Information Service for Monitoring
  a Heterogeneous Research Field","  The paper presents a web prototype that visualises different characteristics
of research projects in the heterogeneous domain of educational research. The
concept of the application derives from the project ""Monitoring Educational
Research"" (MoBi) that aims at identifying and implementing indicators that
adequately describe structural properties and dynamics of the research field.
The prototype enables users to visualise data regarding different indicators,
e.g. ""research activity"", ""funding"", ""qualification project"", ""disciplinary
area"". Since the application is based on Semantic MediaWikitechnology it
furthermore provides an easily accessible opportunity to collaboratively work
on a database of research projects. Users can jointly and in a semantically
controlled way enter metadata on research projects which are the basis for the
computation and visualisation of indicators.
"
1092,The Nobel Prize delay,"  The time lag between the publication of a Nobel discovery and the conferment
of the prize has been rapidly increasing for all disciplines, especially for
Physics. Does this mean that fundamental science is running out of
groundbreaking discoveries?
"
1093,"PyRDM: A Python-based library for automating the management and online
  publication of scientific software and data","  The recomputability and reproducibility of results from scientific software
requires access to both the source code and all associated input and output
data. However, the full collection of these resources often does not accompany
the key findings published in journal articles, thereby making it difficult or
impossible for the wider scientific community to verify the correctness of a
result or to build further research on it. This paper presents a new
Python-based library, PyRDM, whose functionality aims to automate the process
of sharing the software and data via online, citable repositories such as
Figshare. The library is integrated into the workflow of an open-source
computational fluid dynamics package, Fluidity, to demonstrate an example of
its usage.
"
1094,"Robustness of journal rankings by network flows with different amounts
  of memory","  As the number of scientific journals has multiplied, journal rankings have
become increasingly important for scientific decisions. From submissions and
subscriptions to grants and hirings, researchers, policy makers, and funding
agencies make important decisions with influence from journal rankings such as
the ISI journal impact factor. Typically, the rankings are derived from the
citation network between a selection of journals and unavoidably depend on this
selection. However, little is known about how robust rankings are to the
selection of included journals. Here we compare the robustness of three journal
rankings based on network flows induced on citation networks. They model
pathways of researchers navigating scholarly literature, stepping between
journals and remembering their previous steps to different degree: zero-step
memory as impact factor, one-step memory as Eigenfactor, and two-step memory,
corresponding to zero-, first-, and second-order Markov models of citation flow
between journals. We conclude that higher-order Markov models perform better
and are more robust to the selection of journals. Whereas our analysis
indicates that higher-order models perform better, the performance gain for the
second-order Markov model comes at the cost of requiring more citation data
over a longer time period.
"
1095,Knowledge Maps and Information Retrieval (KMIR),"  Information systems usually show as a particular point of failure the
vagueness between user search terms and the knowledge orders of the information
space in question. Some kind of guided searching therefore becomes more and
more important in order to precisely discover information without knowing the
right search terms. Knowledge maps of digital library collections are promising
navigation tools through knowledge spaces but still far away from being
applicable for searching digital libraries. However, there is no continuous
knowledge exchange between the ""map makers"" on the one hand and the Information
Retrieval (IR) specialists on the other hand. Thus, there is also a lack of
models that properly combine insights of the two strands. The proposed workshop
aims at bringing together these two communities: experts in IR reflecting on
visual enhanced search interfaces and experts in knowledge mapping reflecting
on visualizations of the content of a collection that might also present a
context for a search term in a visual manner. The intention of the workshop is
to raise awareness of the potential of interactive knowledge maps for
information seeking purposes and to create a common ground for experiments
aiming at the incorporation of knowledge maps into IR models at the level of
the user interface.
"
1096,LODE: Linking Digital Humanities Content to the Web of Data,"  Numerous digital humanities projects maintain their data collections in the
form of text, images, and metadata. While data may be stored in many formats,
from plain text to XML to relational databases, the use of the resource
description framework (RDF) as a standardized representation has gained
considerable traction during the last five years. Almost every digital
humanities meeting has at least one session concerned with the topic of digital
humanities, RDF, and linked data. While most existing work in linked data has
focused on improving algorithms for entity matching, the aim of the
LinkedHumanities project is to build digital humanities tools that work ""out of
the box,"" enabling their use by humanities scholars, computer scientists,
librarians, and information scientists alike. With this paper, we report on the
Linked Open Data Enhancer (LODE) framework developed as part of the
LinkedHumanities project. With LODE we support non-technical users to enrich a
local RDF repository with high-quality data from the Linked Open Data cloud.
LODE links and enhances the local RDF repository without compromising the
quality of the data. In particular, LODE supports the user in the enhancement
and linking process by providing intuitive user-interfaces and by suggesting
high-quality linking candidates using tailored matching algorithms. We hope
that the LODE framework will be useful to digital humanities scholars
complementing other digital humanities tools.
"
1097,Evolution of citation networks with the hypergraph formalism,"  In this paper, we proposed an evolving model via the hypergraph to illustrate
the evolution of the citation network. In the evolving model, we consider the
mechanism combined with preferential attachment and the aging influence.
Simulation results show that the proposed model can characterize the citation
distribution of the real system very well. In addition, we give the analytical
result of the citation distribution using the master equation. Detailed
analysis showed that the time decay factor should be the origin of the same
citation distribution between the proposed model and the empirical result. The
proposed model might shed some lights in understanding the underlying laws
governing the structure of real citation networks.
"
1098,Identifying Duplicate and Contradictory Information in Wikipedia,"  Our study identifies sentences in Wikipedia articles that are either
identical or highly similar by applying techniques for near-duplicate detection
of web pages. This is accomplished with a MapReduce implementation of minhash
to identify clusters of sentences with high Jaccard similarity. We show that
these clusters can be categorized into six different types, two of which are
particularly interesting: identical sentences quantify the extent to which
content in Wikipedia is copied and pasted, and near-duplicate sentences that
state contradictory facts point to quality issues in Wikipedia.
"
1099,Are 140 Characters Enough? A Large-Scale Linkability Study of Tweets,"  Microblogging is a very popular Internet activity that informs and entertains
great multitudes of people world-wide via quickly and scalably disseminated
terse messages containing all kinds of newsworthy utterances. Even though
microblogging is neither designed nor meant to emphasize privacy, numerous
contributors hide behind pseudonyms and compartmentalize their different
incarnations via multiple accounts within the same, or across multiple,
site(s). Prior work has shown that stylometric analysis is a very powerful tool
capable of linking product or service reviews and blogs that are produced by
the same author when the number of authors is large. In this paper, we explore
linkability of tweets. Our results, based on a very large corpus of tweets,
clearly demonstrate that, at least for relatively active tweeters, linkability
of tweets by the same author is easily attained even when the number of
tweeters is large. We also show that our linkability results hold for a set of
actual Twitter users who tweet from multiple accounts. This has some obvious
privacy implications, both positive and negative.
"
1100,Towards a Frontier of Spatial Scientometric Studies,"  The research field of spatial scientometrics is dedicated to measuring and
analyzing science with spatial components (e.g., location, place, mapping).
Because of the dynamic nature of this field, researchers from multidisciplinary
domains constantly contribute qualitative, quantitative and computational
approaches and technologies into scientometric analysis. This article aims to
giving a brief overview about this field by analyzing the publications in
(spatial) scientometrics collected from the Scopus database and introduces
recent frontier researches which integrate advanced spatial analysis and
geovisualization with Semantic Web technologies.
"
1101,POS Tagging and its Applications for Mathematics,"  Content analysis of scientific publications is a nontrivial task, but a
useful and important one for scientific information services. In the Gutenberg
era it was a domain of human experts; in the digital age many machine-based
methods, e.g., graph analysis tools and machine-learning techniques, have been
developed for it. Natural Language Processing (NLP) is a powerful
machine-learning approach to semiautomatic speech and language processing,
which is also applicable to mathematics. The well established methods of NLP
have to be adjusted for the special needs of mathematics, in particular for
handling mathematical formulae. We demonstrate a mathematics-aware part of
speech tagger and give a short overview about our adaptation of NLP methods for
mathematical publications. We show the use of the tools developed for key
phrase extraction and classification in the database zbMATH.
"
1102,"The role of handbooks in knowledge creation and diffusion: A case of
  science and technology studies","  Genre is considered to be an important element in scholarly communication and
in the practice of scientific disciplines. However, scientometric studies have
typically focused on a single genre, the journal article. The goal of this
study is to understand the role that handbooks play in knowledge creation and
diffusion and their relationship with the genre of journal articles,
particularly in highly interdisciplinary and emergent social science and
humanities disciplines. To shed light on these questions we focused on
handbooks and journal articles published over the last four decades belonging
to the research area of Science and Technology Studies (STS), broadly defined.
To get a detailed picture we used the full-text of five handbooks (500,000
words) and a well-defined set of 11,700 STS articles. We confirmed the
methodological split of STS into qualitative and quantitative (scientometric)
approaches. Even when the two traditions explore similar topics (e.g., science
and gender) they approach them from different starting points. The change in
cognitive foci in both handbooks and articles partially reflects the changing
trends in STS research, often driven by technology. Using text similarity
measures we found that, in the case of STS, handbooks play no special role in
either focusing the research efforts or marking their decline. In general, they
do not represent the summaries of research directions that have emerged since
the previous edition of the handbook.
"
1103,Reaction to New Security Threat Class,"  Each new identified security threat class triggers new research and
development efforts by the scientific and professional communities. In this
study, we investigate the rate at which the scientific and professional
communities react to new identified threat classes as it is reflected in the
number of patents, scientific articles and professional publications over a
long period of time. The following threat classes were studied: Phishing; SQL
Injection; BotNet; Distributed Denial of Service; and Advanced Persistent
Threat. Our findings suggest that in most cases it takes a year for the
scientific community and more than two years for industry to react to a new
threat class with patents. Since new products follow patents, it is reasonable
to expect that there will be a window of approximately two to three years in
which no effective product is available to cope with the new threat class.
"
1104,"Bringing Web Time Travel to MediaWiki: An Assessment of the Memento
  MediaWiki Extension","  We have implemented the Memento MediaWiki Extension Version 2.0, which brings
the Memento Protocol to MediaWiki, used by Wikipedia and the Wikimedia
Foundation. Test results show that the extension has a negligible impact on
performance. Two 302 status code datetime negotiation patterns, as defined by
Memento, have been examined for the extension: Pattern 1.1, which requires 2
requests, versus Pattern 2.1, which requires 3 requests. Our test results and
mathematical review find that, contrary to intuition, Pattern 2.1 performs
better than Pattern 1.1 due to idiosyncrasies in MediaWiki. In addition to
implementing Memento, Version 2.0 allows administrators to choose the optional
200-style datetime negotiation Pattern 1.2 instead of Pattern 2.1. It also
permits administrators the ability to have the Memento MediaWiki Extension
return full HTTP 400 and 500 status codes rather than using standard MediaWiki
error pages. Finally, version 2.0 permits administrators to turn off
recommended Memento headers if desired. Seeing as much of our work focuses on
producing the correct revision of a wiki page in response to a user's datetime
input, we also examine the problem of finding the correct revisions of the
embedded resources, including images, stylesheets, and JavaScript; identifying
the issues and discussing whether or not MediaWiki must be changed to support
this functionality.
"
1105,Community-driven reviewing and validation of publications,"  In this report, we share our practical experience on crowdsourcing evaluation
of research artifacts and reviewing of publications since 2008. We also briefly
discuss encountered problems including reproducibility of experimental results
and possible solutions.
"
1106,"Matching MEDLINE/PubMed Data with Web of Science (WoS): A Routine in R
  language","  We present a novel routine, namely medlineR, based on R-language, that
enables the user to match data from MEDLINE/PubMed with records indexed in the
ISI Web of Science (WoS) database. The matching allows exploiting the rich and
controlled vocabulary of Medical Subject Headings (MeSH) of MEDLINE/PubMed with
additional fields of WoS. The integration provides data (e.g. citation data,
list of cited reference, full list of the addresses of authors' host
organisations, WoS subject categories) to perform a variety of scientometric
analyses. This brief communication describes medlineR, the methodology on which
it relies, and the steps the user should follow to perform the matching across
the two databases. In order to specify the differences from Leydesdorff and
Opthof (2013), we conclude the brief communication by testing the routine on
the case of the ""Burgada Syndrome"".
"
1107,"The dark side of Open Access in Google and Google Scholar: the case of
  Latin-American repositories","  Since repositories are a key tool in making scholarly knowledge open access,
determining their presence and impact on the Web is essential, particularly in
Google (search engine par excellence) and Google Scholar (a tool increasingly
used by researchers to search for academic information). The few studies
conducted so far have been limited to very specific geographic areas (USA),
which makes it necessary to find out what is happening in other regions that
are not part of mainstream academia, and where repositories play a decisive
role in the visibility of scholarly production. The main objective of this
study is to ascertain the presence and visibility of Latin American
repositories in Google and Google Scholar through the application of page count
and visibility indicators. For a sample of 137 repositories, the results
indicate that the indexing ratio is low in Google, and virtually nonexistent in
Google Scholar; they also indicate a complete lack of correspondence between
the repository records and the data produced by these two search tools. These
results are mainly attributable to limitations arising from the use of
description schemas that are incompatible with Google Scholar (repository
design) and the reliability of web indicators (search engines). We conclude
that neither Google nor Google Scholar accurately represent the actual size of
open access content published by Latin American repositories; this may indicate
a non-indexed, hidden side to open access, which could be limiting the
dissemination and consumption of open access scholarly literature.
"
1108,Computing and Using Metrics in the ADS,"  Finding measures for research impact, be it for individuals, institutions,
instruments or projects, has gained a lot of popularity. More papers than ever
are being written on new impact measures, and problems with existing measures
are being pointed out on a regular basis. Funding agencies require impact
statistics in their reports, job candidates incorporate them in their resumes,
and publication metrics have even been used in at least one recent court case.
To support this need for research impact indicators, the SAO/NASA Astrophysics
Data System (ADS) has developed a service which provides a broad overview of
various impact measures. In this presentation we discuss how the ADS can be
used to quench the thirst for impact measures. We will also discuss a couple of
the lesser known indicators in the metrics overview and the main issues to be
aware of when compiling publication-based metrics in the ADS, namely author
name ambiguity and citation incompleteness.
"
1109,The Multidimensional Assessment of Scholarly Research Impact,"  This article introduces the Multidimensional Research Assessment Matrix of
scientific output. Its base notion holds that the choice of metrics to be
applied in a research assessment process depends upon the unit of assessment,
the research dimension to be assessed, and the purposes and policy context of
the assessment. An indicator may by highly useful within one assessment
process, but less so in another. For instance, publication counts are useful
tools to help discriminating between those staff members who are research
active, and those who are not, but are of little value if active scientists are
to be compared one another according to their research performance. This paper
gives a systematic account of the potential usefulness and limitations of a set
of 10 important metrics including altmetrics, applied at the level of
individual articles, individual researchers, research groups and institutions.
It presents a typology of research impact dimensions, and indicates which
metrics are the most appropriate to measure each dimension. It introduces the
concept of a meta-analysis of the units under assessment in which metrics are
not used as tools to evaluate individual units, but to reach policy inferences
regarding the objectives and general setup of an assessment process.
"
1110,"Information, Meaning, and Intellectual Organization in Networks of
  Inter-Human Communication","  The Shannon-Weaver model of linear information transmission is extended with
two loops potentially generating redundancies: (i) meaning is provided locally
to the information from the perspective of hindsight, and (ii) meanings can be
codified differently and then refer to other horizons of meaning. Thus, three
layers are distinguished: variations in the communications, historical
organization at each moment of time, and evolutionary self-organization of the
codes of communication over time. Furthermore, the codes of communication can
functionally be different and then the system is both horizontally and
vertically differentiated. All these subdynamics operate in parallel and
necessarily generate uncertainty. However, meaningful information can be
considered as the specific selection of a signal from the noise; the codes of
communication are social constructs that can generate redundancy by giving
different meanings to the same information. Reflexively, one can translate
among codes in more elaborate discourses. The second (instantiating) layer can
be operationalized in terms of semantic maps using the vector space model; the
third in terms of mutual redundancy among the latent dimensions of the vector
space. Using Blaise Cronin's {\oe}uvre, the different operations of the three
layers are demonstrated empirically.
"
1111,PDF/A-3u as an archival format for Accessible mathematics,"  Including LaTeX source of mathematical expressions, within the PDF document
of a text-book or research paper, has definite benefits regarding
`Accessibility' considerations. Here we describe three ways in which this can
be done, fully compatibly with international standards ISO 32000, ISO 19005-3,
and the forthcoming ISO 32000-2 (PDF 2.0). Two methods use embedded files, also
known as `attachments', holding information in either LaTeX or MathML formats,
but use different PDF structures to relate these attachments to regions of the
document window. One uses structure, so is applicable to a fully `Tagged PDF'
context, while the other uses /AF tagging of the relevant content. The third
method requires no tagging at all, instead including the source coding as the
/ActualText replacement of a so-called `fake space'. Information provided this
way is extracted via simple Select/Copy/Paste actions, and is available to
existing screen-reading software and assistive technologies.
"
1112,"From Citation count to Argumentation count: a new metric to indicate the
  usefulness of an article","  Citation count is a quantifiable measure to indicate the number of times an
article is cited by other articles. It is believed that if an article is cited
often then it must be an important or influential article; however, there is no
guarantee that the most cited articles are good in quality. In this paper, the
author suggests argumentation count, a new metric for citation analysis. The
proposed metric, argumentation count is a triplet of quantities for each
concept of an article that helps in providing a quantifiable measure about the
usefulness of an article.
"
1113,"Do altmetrics point to the broader impact of research? An overview of
  benefits and disadvantages of altmetrics","  Today, it is not clear how the impact of research on other areas of society
than science should be measured. While peer review and bibliometrics have
become standard methods for measuring the impact of research in science, there
is not yet an accepted framework within which to measure societal impact.
Alternative metrics (called altmetrics to distinguish them from bibliometrics)
are considered an interesting option for assessing the societal impact of
research, as they offer new ways to measure (public) engagement with research
output. Altmetrics is a term to describe web-based metrics for the impact of
publications and other scholarly material by using data from social media
platforms (e.g. Twitter or Mendeley). This overview of studies explores the
potential of altmetrics for measuring societal impact. It deals with the
definition and classification of altmetrics. Furthermore, their benefits and
disadvantages for measuring impact are discussed.
"
1114,New measures for evaluating creativity in scientific publications,"  The goal of our research is to understand how ideas propagate, combine and
are created in large social networks. In this work, we look at a sample of
relevant scientific publications in the area of high-frequency analog circuit
design and their citation distribution. A novel aspect of our work is the way
in which we categorize citations based on the reason and place of it in a
publication. We created seven citation categories from general domain
references, references to specific methods used in the same domain problem,
references to an analysis method, references for experimental comparison and so
on. This added information allows us to define two new measures to characterize
the creativity (novelty and usefulness) of a publication based on its pattern
of citations clustered by reason, place and citing scientific group. We
analyzed 30 publications in relevant journals since 2000 and their about 300
citations, all in the area of high-frequency analog circuit design. We observed
that the number of citations a publication receives from different scientific
groups matches a Levy type distribution: with a large number of groups citing a
publication relatively few times, and a very small number of groups citing a
publication a large number of times. We looked at the motifs a publication is
cited differently by different scientific groups.
"
1115,"Validity of altmetrics data for measuring societal impact: A study using
  data from Altmetric and F1000Prime","  Can altmetric data be validly used for the measurement of societal impact?
The current study seeks to answer this question with a comprehensive dataset
(about 100,000 records) from very disparate sources (F1000, Altmetric, and an
in-house database based on Web of Science). In the F1000 peer review system,
experts attach particular tags to scientific papers which indicate whether a
paper could be of interest for science or rather for other segments of society.
The results show that papers with the tag ""good for teaching"" do achieve higher
altmetric counts than papers without this tag - if the quality of the papers is
controlled. At the same time, a higher citation count is shown especially by
papers with a tag that is specifically scientifically oriented (""new finding"").
The findings indicate that papers tailored for a readership outside the area of
research should lead to societal impact. If altmetric data is to be used for
the measurement of societal impact, the question arises of its normalization.
In bibliometrics, citations are normalized for the papers' subject area and
publication year. This study has taken a second analytic step involving a
possible normalization of altmetric data. As the results show there are
particular scientific topics which are of especial interest for a wide
audience. Since these more or less interesting topics are not completely
reflected in Thomson Reuters' journal sets, a normalization of altmetric data
should not be based on the level of subject categories, but on the level of
topics.
"
1116,Crowd-Sourcing Fuzzy and Faceted Classification for Concept Search,"  Searching for concepts in science and technology is often a difficult task.
To facilitate concept search, different types of human-generated metadata have
been created to define the content of scientific and technical disclosures.
Classification schemes such as the International Patent Classification (IPC)
and MEDLINE's MeSH are structured and controlled, but require trained experts
and central management to restrict ambiguity (Mork, 2013). While unstructured
tags of folksonomies can be processed to produce a degree of structure
(Kalendar, 2010; Karampinas, 2012; Sarasua, 2012; Bragg, 2013) the freedom
enjoyed by the crowd typically results in less precision (Stock 2007).
  Existing classification schemes suffer from inflexibility and ambiguity.
Since humans understand language, inference, implication, abstraction and hence
concepts better than computers, we propose to harness the collective wisdom of
the crowd. To do so, we propose a novel classification scheme that is
sufficiently intuitive for the crowd to use, yet powerful enough to facilitate
search by analogy, and flexible enough to deal with ambiguity. The system will
enhance existing classification information. Linking up with the semantic web
and computer intelligence, a Citizen Science effort (Good, 2013) would support
innovation by improving the quality of granted patents, reducing duplicitous
research, and stimulating problem-oriented solution design.
  A prototype of our design is in preparation. A crowd-sourced fuzzy and
faceted classification scheme will allow for better concept search and improved
access to prior art in science and technology.
"
1117,Mathematical Language Processing Project,"  In natural language, words and phrases themselves imply the semantics. In
contrast, the meaning of identifiers in mathematical formulae is undefined.
Thus scientists must study the context to decode the meaning. The Mathematical
Language Processing (MLP) project aims to support that process. In this paper,
we compare two approaches to discover identifier-definition tuples. At first we
use a simple pattern matching approach. Second, we present the MLP approach
that uses part-of-speech tag based distances as well as sentence positions to
calculate identifier-definition probabilities. The evaluation of our
prototypical system, applied on the Wikipedia text corpus, shows that our
approach augments the user experience substantially. While hovering the
identifiers in the formula, tool-tips with the most probable definitions occur.
Tests with random samples show that the displayed definitions provide a good
match with the actual meaning of the identifiers.
"
1118,"Exploring the relationship between the Engineering and Physical Sciences
  and the Health and Life Sciences by advanced bibliometric methods","  We investigate the extent to which advances in the health and life sciences
(HLS) are dependent on research in the engineering and physical sciences (EPS),
particularly physics, chemistry, mathematics, and engineering. The analysis
combines two different bibliometric approaches. The first approach to analyze
the 'EPS-HLS interface' is based on term map visualizations of HLS research
fields. We consider 16 clinical fields and five life science fields. On the
basis of expert judgment, EPS research in these fields is studied by
identifying EPS-related terms in the term maps. In the second approach, a
large-scale citation-based network analysis is applied to publications from all
fields of science. We work with about 22,000 clusters of publications, each
representing a topic in the scientific literature. Citation relations are used
to identify topics at the EPS-HLS interface. The two approaches complement each
other. The advantages of working with textual data compensate for the
limitations of working with citation relations and the other way around. An
important advantage of working with textual data is in the in-depth qualitative
insights it provides. Working with citation relations, on the other hand,
yields many relevant quantitative statistics. We find that EPS research
contributes to HLS developments mainly in the following five ways: new
materials and their properties; chemical methods for analysis and molecular
synthesis; imaging of parts of the body as well as of biomaterial surfaces;
medical engineering mainly related to imaging, radiation therapy, signal
processing technology, and other medical instrumentation; mathematical and
statistical methods for data analysis. In our analysis, about 10% of all EPS
and HLS publications are classified as being at the EPS-HLS interface. This
percentage has remained more or less constant during the past decade.
"
1119,"Understanding Repository Growth at the University of North Texas: A Case
  Study","  Over the past decade the University of North Texas Libraries (UNTL) has
developed a sizable digital library infrastructure for use in carrying out its
core mission to the students, faculty, staff and associated communities of the
university. This repository of content offers countless research possibilities
for end users across the Internet when it is discovered and used in research,
scholarship, entertainment, and lifelong learning. The characteristics of the
repository itself provide insight into the workings of a modern digital library
infrastructure, how it was created, how often it is updated, or how often it is
modified. In that vein, the authors created a dataset comprised of information
extracted from the UNT Libraries' archival repository Coda and analyzed this
dataset in order to demonstrate the value and insights that can be gained from
sharing repository characteristics more broadly. This case study presents the
findings from an analysis of this dataset.
"
1120,"Establishing an Online Access Panel for Interactive Information
  Retrieval Research","  We propose an online access panel to support the evaluation process of
Interactive Information Retrieval (IIR) systems - called IIRpanel. By
maintaining an online access panel with users of IIR systems we assume that the
recurring effort to recruit participants for web-based as well as for lab
studies can be minimized. We target on using the online access panel not only
for our own development processes but to open it for other interested
researchers in the field of IIR. In this paper we present the concept of
IIRpanel as well as first implementation details.
"
1121,Future Influence Ranking of Scientific Literature,"  Researchers or students entering a emerging research area are particularly
interested in what newly published papers will be most cited and which young
researchers will become influential in the future, so that they can catch the
most recent advances and find valuable research directions. However, predicting
the future importance of scientific articles and authors is challenging due to
the dynamic nature of literature networks and evolving research topics.
Different from most previous studies aiming to rank the current importance of
literatures and authors, we focus on \emph{ranking the future popularity of new
publications and young researchers} by proposing a unified ranking model to
combine various available information. Specifically, we first propose to
extract two kinds of text features, words and words co-occurrence to
characterize innovative papers and authors. Then, instead of using static and
un-weighted graphs, we construct time-aware weighted graphs to distinguish the
various importance of links established at different time. Finally, by
leveraging both the constructed text features and graphs, we propose a mutual
reinforcement ranking framework called \emph{MRFRank} to rank the future
importance of papers and authors simultaneously. Experimental results on the
ArnetMiner dataset show that the proposed approach significantly outperforms
the baselines on the metric \emph{recommendation intensity}.
"
1122,"Discovering Beaten Paths in Collaborative Ontology-Engineering Projects
  using Markov Chains","  Biomedical taxonomies, thesauri and ontologies in the form of the
International Classification of Diseases (ICD) as a taxonomy or the National
Cancer Institute Thesaurus as an OWL-based ontology, play a critical role in
acquiring, representing and processing information about human health. With
increasing adoption and relevance, biomedical ontologies have also
significantly increased in size. For example, the 11th revision of the ICD,
which is currently under active development by the WHO contains nearly 50,000
classes representing a vast variety of different diseases and causes of death.
This evolution in terms of size was accompanied by an evolution in the way
ontologies are engineered. Because no single individual has the expertise to
develop such large-scale ontologies, ontology-engineering projects have evolved
from small-scale efforts involving just a few domain experts to large-scale
projects that require effective collaboration between dozens or even hundreds
of experts, practitioners and other stakeholders. Understanding how these
stakeholders collaborate will enable us to improve editing environments that
support such collaborations. We uncover how large ontology-engineering
projects, such as the ICD in its 11th revision, unfold by analyzing usage logs
of five different biomedical ontology-engineering projects of varying sizes and
scopes using Markov chains. We discover intriguing interaction patterns (e.g.,
which properties users subsequently change) that suggest that large
collaborative ontology-engineering projects are governed by a few general
principles that determine and drive development. From our analysis, we identify
commonalities and differences between different projects that have implications
for project managers, ontology editors, developers and contributors working on
collaborative ontology-engineering projects and tools in the biomedical domain.
"
1123,"Which of the world's institutions employ the most highly cited
  researchers? An analysis of the data from highlycited.com","  A few weeks ago, Thomson Reuters published a list of the highly cited
researchers worldwide (highlycited.com). Since the data is freely available for
downloading and includes the names of the researchers' institutions, we
produced a ranking of the institutions on the basis of the number of highly
cited researchers per institution. This ranking is intended to be a helpful
amendment of other available institutional rankings.
"
1124,Modeling Collaboration in Academia: A Game Theoretic Approach,"  In this work, we aim to understand the mechanisms driving academic
collaboration. We begin by building a model for how researchers split their
effort between multiple papers, and how collaboration affects the number of
citations a paper receives, supported by observations from a large real-world
publication and citation dataset, which we call the h-Reinvestment model. Using
tools from the field of Game Theory, we study researchers' collaborative
behavior over time under this model, with the premise that each researcher
wants to maximize his or her academic success. We find analytically that there
is a strong incentive to collaborate rather than work in isolation, and that
studying collaborative behavior through a game-theoretic lens is a promising
approach to help us better understand the nature and dynamics of academic
collaboration.
"
1125,Google Scholar Metrics 2014: a low cost bibliometric tool,"  We analyse the main features of the third edition of Google Scholar Metrics
(GSM), released in June 2014, focusing on its more important changes,
strengths, and weaknesses. Additionally, we present some figures that outline
the dimensions of this new edition, and we compare them to those of previous
editions. Principal among these figures are the number of visualized
publications, publication types, languages, and the maximum and minimum
h5-index and h5-median values by language, subject area, and subcategory. This
new edition is marked by continuity. There is nothing new other than the
updating of the time frame (2009-2013) and the removal of some redundant
subcategories (from 268 to 261) for English written publications. Google has
just updated the data, which means that some of the errors discussed in
previous studies still persist. To sum up, GSM is a minimalist information
product with few features, closed (it cannot be customized by the user), and
simple (navigating it only takes a few clicks). For these reasons, we consider
it a 'low cost' bibliometric tool, and propose a list of features it should
incorporate in order to stop being labeled as such. Notwithstanding the above,
this product presents a stability in its bibliometric indicators that supports
its ability to measure and track the impact of scientific publications.
"
1126,h-index Research in Scientometrics: A Summary,"  A Letter to the Editor shortly summing up ten or so years of research into
the h-index.
"
1127,The Virtual Observatory Registry,"  In the Virtual Observatory (VO), the Registry provides the mechanism with
which users and applications discover and select resources -- typically, data
and services -- that are relevant for a particular scientific problem. Even
though the VO adopted technologies in particular from the bibliographic
community where available, building the Registry system involved a major
standardisation effort, involving about a dozen interdependent standard texts.
This paper discusses the server-side aspects of the standards and their
application, as regards the functional components (registries), the resource
records in both format and content, the exchange of resource records between
registries (harvesting), as well as the creation and management of the
identifiers used in the system based on the notion of authorities. Registry
record authors, registry operators or even advanced users thus receive a big
picture serving as a guideline through the body of relevant standard texts. To
complete this picture, we also mention common usage patterns and open issues as
appropriate.
"
1128,"Examples for counterintuitive behavior of the new citation-rank
  indicator P100 for bibliometric evaluations","  A new percentile-based rating scale P100 has recently been proposed to
describe the citation impact in terms of the distribution of the unique
citation values. Here I investigate P100 for 5 example datasets, two simple
fictitious models and three larger empirical samples. Counterintuitive behavior
is demonstrated in the model datasets, pointing to difficulties when the
evolution with time of the indicator is analyzed or when different fields or
publication years are compared. It is shown that similar problems can occur for
the three larger datasets of empirical citation values. Further, it is observed
that the performance evalution result in terms of percentiles can be influenced
by selecting different journals for publication of a manuscript.
"
1129,"When data sharing gets close to 100%: what ancient human DNA studies can
  teach the Open Science movement","  This study analyzes rates and ways of data sharing regarding mitochondrial, Y
chromosomal and autosomal polymorphisms in a total of 162 papers on human
ancient DNA published between 1988 and 2013. For the most part, data are
available in such a way as to make their scrutiny and reuse possible. The
estimated sharing rate is not far from totality (97.6% +/- 2.1%) and
substantially higher than observed in other fields of genetic research
(Evolutionary, Medical and Forensic Genetics). A questionnaire-based survey
suggests that the authors awareness of the importance of openness and
transparency for scientific progress is a fundamental factor for the
achievement of such a high sharing rate. Most data were made available through
body text, but the use of primary databases increased with the application of
complete mitochondrial and next generation sequencing methods. Our study
highlights three important aspects. First, we provide evidence that researchers
motivations are as necessary as stakeholders policies and norms to achieve very
high sharing rates. Second, careful analyses of the ways in which data are made
available are an important first step to maximize data findability,
accessibility, useability and preservation. Third and finally, the case of
human ancient DNA studies demonstrates how Open Science can foster scientific
advancements, showing that openness and transparency can help build rigorous
and reliable scientific practices even in the presence of complex experimental
challenges.
"
1130,"Expertise localization discovered through correlation of key term
  distribution and community detection in co-author networks","  We present an efficient and effective automatic method for determining the
research focus of scientific communities found in co-authorship networks. It
utilizes bibliographic data from a database to form the network, followed by
fastgreedy community detection to identify communities within large connected
components of the network. Text analysis techniques are used to identify
community-specific significant terms which represent the topic of the
community. In order to greatly reduce computation time, the `Topics' field of
each publication in the network is analyzed rather than its entire text. Using
this text analysis approach requires a certain level of statistical
confidence,therefore analyzing very small communities is not effective with
this technique. We find a minimum community size threshold of 8 coauthored
papers; below this value, the community's topic cannot be reliably identified
with this method. Additionally, we consider the implications this study has
regarding factors involved in the formation of scientific communities in
co-authorship networks.
"
1131,$OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics,"  In this paper, we present an ontology of mathematical knowledge concepts that
covers a wide range of the fields of mathematics and introduces a balanced
representation between comprehensive and sensible models. We demonstrate the
applications of this representation in information extraction, semantic search,
and education. We argue that the ontology can be a core of future integration
of math-aware data sets in the Web of Data and, therefore, provide mappings
onto relevant datasets, such as DBpedia and ScienceWISE.
"
1132,"One file to share them all: Using the COMBINE Archive and the OMEX
  format to share all information about a modeling project","  Background: With the ever increasing use of computational models in the
biosciences, the need to share models and reproduce the results of published
studies efficiently and easily is becoming more important. To this end, various
standards have been proposed that can be used to describe models, simulations,
data or other essential information in a consistent fashion. These constitute
various separate components required to reproduce a given published scientific
result.
  Results: We describe the Open Modeling EXchange format (OMEX). Together with
the use of other standard formats from the Computational Modeling in Biology
Network (COMBINE), OMEX is the basis of the COMBINE Archive, a single file that
supports the exchange of all the information necessary for a modeling and
simulation experiment in biology. An OMEX file is a ZIP container that includes
a manifest file, listing the content of the archive, an optional metadata file
adding information about the archive and its content, and the files describing
the model. The content of a COMBINE Archive consists of files encoded in
COMBINE standards whenever possible, but may include additional files defined
by an Internet Media Type. Several tools that support the COMBINE Archive are
available, either as independent libraries or embedded in modeling software.
  Conclusions: The COMBINE Archive facilitates the reproduction of modeling and
simulation experiments in biology by embedding all the relevant information in
one file. Having all the information stored and exchanged at once also helps in
building activity logs and audit trails. We anticipate that the COMBINE Archive
will become a significant help for modellers, as the domain moves to larger,
more complex experiments such as multi-scale models of organs, digital
organisms, and bioengineering.
"
1133,Implementing Transitive Credit with JSON-LD,"  Science and engineering research increasingly relies on activities that
facilitate research but are not currently rewarded or recognized, such as: data
sharing; developing common data resources, software and methodologies; and
annotating data and publications. To promote and advance these activities, we
must develop mechanisms for assigning credit, facilitate the appropriate
attribution of research outcomes, devise incentives for activities that
facilitate research, and allocate funds to maximize return on investment. In
this article, we focus on addressing the issue of assigning credit for both
direct and indirect contributions, specifically by using JSON-LD to implement a
prototype transitive credit system.
"
1134,Looking before leaping: Creating a software registry,"  What lessons can be learned from examining numerous efforts to create a
repository or directory of scientist-written software for a discipline?
Astronomy has seen a number of efforts to build such a resource, one of which
is the Astrophysics Source Code Library (ASCL). The ASCL (ascl.net) was founded
in 1999, had a period of dormancy, and was restarted in 2010. When taking over
responsibility for the ASCL in 2010, the new editor sought to answer the
opening question, hoping this would better inform the work to be done. We also
provide specific steps the ASCL is taking to try to improve code sharing and
discovery in astronomy and share recent improvements to the resource.
"
1135,About the size of Google Scholar: playing the numbers,"  The emergence of academic search engines (Google Scholar and Microsoft
Academic Search essentially) has revived and increased the interest in the size
of the academic web, since their aspiration is to index the entirety of current
academic knowledge. The search engine functionality and human search patterns
lead us to believe, sometimes, that what you see in the search engine's results
page is all that really exists. And, even when this is not true, we wonder
which information is missing and why. The main objective of this working paper
is to calculate the size of Google Scholar at present (May 2014). To do this,
we present, apply and discuss up to 4 empirical methods: Khabsa & Giles's
method, an estimate based on empirical data, and estimates based on direct
queries and absurd queries. The results, despite providing disparate values,
place the estimated size of Google Scholar in about 160 million documents.
However, the fact that all methods show great inconsistencies, limitations and
uncertainties, makes us wonder why Google does not simply provide this
information to the scientific community if the company really knows this
figure.
"
1136,"The recent Italian regulations about the open-access availability of
  publicly-funded research publications, and the documentation landscape in
  astrophysics","  In October 2013 Italy enacted Law 112 2013, containing the first national
regulations about the open access availability of publicly-funded research
results (publications). The impact of these new regulations with the specific
situation of that open access discipline which is astrophysics, has been
considered. Under a strictly technical point of view, in the light of the new
dispositions the open nature of a part of the astrophysical scholarly
literature which has been made available online free to the reader during the
last twenty years, might be questionable. Some possible ways to make
astrophysicists' scholarly dissemination entirely compliant with law
requirements are considered.
"
1137,Two years of ALMA bibliography - lessons learned,"  Telescope bibliographies are integral parts of observing facilities. They are
used to associate the published literature with archived observational data, to
measure an observatory's scientific output through publication and citation
statistics, and to define guidelines for future observing strategies.
  The ESO and NRAO librarians as well as NAOJ jointly maintain the ALMA
(Atacama Large Millimeter/submillimeter Array) bibliography, a database of
refereed papers that use ALMA data.
  In this paper, we illustrate how relevant articles are identified, which
procedures are used to tag entries in the database and link them to the correct
observations, and how results are communicated to ALMA stakeholders and the
wider community. Efforts made to streamline the process will be explained and
evaluated, and a first analysis of ALMA papers published after two years of
observations will be given.
"
1138,"Mapping R&D support infrastructures: A scientometric and webometric
  study of UK science parks","  This thesis analyses UK SPs with an informetric approach to study (1) the
role of public science and HEIs in research and development (R&D) networks
associated with SPs, and (2) the web-based patterns that reflect the
configuration of R&D support infrastructures associated with SPs.
"
1139,Recommender Systems using Pennant Diagrams in Digital Libraries,"  In digital libraries recommendations can be valuable for researchers, e.g.
recommending related literature to a given context. Typically, in a scientific
context the simple presentation of related content is not sufficient. Often the
users demand a more detailed view on the connection of a document and its
specific recommendations. The aim of pennants introduced by Howard White (2007)
is to provide the user with a graph showing the relatedness / distance between
a given document and related documents. Co-citation but also co-occurrence
analysis are established methods for finding related documents to a seed. A
seed could be for instance an author, a keyword, or a publication. In this
paper we introduce a recommender system in the digital library sowiport using
pennant diagrams which can be created from co-citation and/or co-occurrence
analysis. The presentation at the NKOS workshop will present demos of pennants
in sowiport and will elaborate on practical questions in visualizing pennants
and evaluating the utility of pennants for search.
"
1140,"The Operationalization of ""Fields"" as WoS Subject Categories (WCs) in
  Evaluative Bibliometrics: The cases of ""Library and Information Science"" and
  ""Science & Technology Studies""","  Normalization of citation scores using reference sets based on Web-of-Science
Subject Categories (WCs) has become an established (""best"") practice in
evaluative bibliometrics. For example, the Times Higher Education World
University Rankings are, among other things, based on this operationalization.
However, WCs were developed decades ago for the purpose of information
retrieval and evolved incrementally with the database; the classification is
machine-based and partially manually corrected. Using the WC ""information
science & library science"" and the WCs attributed to journals in the field of
""science and technology studies,"" we show that WCs do not provide sufficient
analytical clarity to carry bibliometric normalization in evaluation practices
because of ""indexer effects."" Can the compliance with ""best practices"" be
replaced with an ambition to develop ""best possible practices""? New research
questions can then be envisaged.
"
1141,"Alternative metrics in scientometrics: A meta-analysis of research into
  three altmetrics","  Alternative metrics are currently one of the most popular research topics in
scientometric research. This paper provides an overview of research into three
of the most important altmetrics: microblogging (Twitter), online reference
managers (Mendeley and CiteULike) and blogging. The literature is discussed in
relation to the possible use of altmetrics in research evaluation. Since the
research was particularly interested in the correlation between altmetrics
counts and citation counts, this overview focuses particularly on this
correlation. For each altmetric, a meta-analysis is calculated for its
correlation with traditional citation counts. As the results of the
meta-analyses show, the correlation with traditional citations for
micro-blogging counts is negligible (pooled r=0.003), for blog counts it is
small (pooled r=0.12) and for bookmark counts from online reference managers,
medium to large (CiteULike pooled r=0.23; Mendeley pooled r=0.51).
"
1142,A model for dynamical evolution of science in space,"  How does the topological space of science emerge? Inspired by the concept of
maps of science, i.e. mapping scientific topics to a scientific space, we ask
which topological structure a dynamical process of authors collaborating and
publishing papers can generate. We propose a dynamical process where papers as
well as new groups receive topical positions embedded in a two-dimensional
euclidean space. The precise position of new papers depends on previous topics
of the respective authors and is chosen randomly in a surrounding neighborhood
including novelty and interdisciplinarity. Depending on parameters, the spatial
structure resembles a simple Gaussian distribution, or spatial clusters of
side-topics are observed. We quantify the time-evolution of the spatial
structure and discuss the influence of inhomogenities.
"
1143,"Genesis of Altmetrics or Article-level Metrics for Measuring Efficacy of
  Scholarly Communications: Current Perspectives","  The article-level metrics (ALMs) or altmetrics becomes a new trendsetter in
recent times for measuring the impact of scientific publications and their
social outreach to intended audiences. The popular social networks such as
Facebook, Twitter, and Linkedin and social bookmarks such as Mendeley and
CiteULike are nowadays widely used for communicating research to larger
transnational audiences. In 2012, the San Francisco Declaration on Research
Assessment got signed by the scientific and researchers communities across the
world. This declaration has given preference to the ALM or altmetrics over
traditional but faulty journal impact factor (JIF)-based assessment of career
scientists. JIF does not consider impact or influence beyond citations count as
this count reflected only through Thomson Reuters' Web of Science database.
Furthermore, JIF provides indicator related to the journal, but not related to
a published paper. Thus, altmetrics now becomes an alternative metrics for
performance assessment of individual scientists and their contributed scholarly
publications. This paper provides a glimpse of genesis of altmetrics in
measuring efficacy of scholarly communications and highlights available
altmetric tools and social platforms linking altmetric tools, which are widely
used in deriving altmetric scores of scholarly publications. The paper thus
argues for institutions and policy makers to pay more attention to altmetrics
based indicators for evaluation purpose but cautions that proper safeguards and
validations are needed before their adoption.
"
1144,"New data, new possibilities: Exploring the insides of Altmetric.com","  This paper analyzes Altmetric.com, one of the most important altmetric data
providers currently used. We have analyzed a set of publications with DOI
number indexed in the Web of Science during the period 2011-2013 and collected
their data with the Altmetric API. 19% of the original set of papers was
retrieved from Altmetric.com including some altmetric data. We identified 16
different social media sources from which Altmetric.com retrieves data. However
five of them cover 95.5% of the total set. Twitter (87.1%) and Mendeley (64.8%)
have the highest coverage. We conclude that Altmetric.com is a transparent,
rich and accurate tool for altmetric data. Nevertheless, there are still
potential limitations on its exhaustiveness as well as on the selection of
social media sources that need further research.
"
1145,"Unstable markup: A template-based information extraction from web sites
  with unstable markup","  This paper presents results of a work on crawling CEUR Workshop proceedings
web site to a Linked Open Data (LOD) dataset in the framework of ESWC 2014
Semantic Publishing Challenge 2014. Our approach is based on using an
extensible template-dependent crawler and DBpedia for linking extracted
entities, such as the names of universities and countries.
"
1146,What is the dimension of citation space?,"  Citation networks represent the flow of information between agents. They are
constrained in time and so form directed acyclic graphs which have a causal
structure. Here we provide novel quantitative methods to characterise that
structure by adapting methods used in the causal set approach to quantum
gravity by considering the networks to be embedded in a Minkowski spacetime and
measuring its dimension using Myrheim-Meyer and Midpoint-scaling estimates. We
illustrate these methods on citation networks from the arXiv, supreme court
judgements from the USA, and patents and find that otherwise similar citation
networks have measurably different dimensions. We suggest that these
differences can be interpreted in terms of the level of diversity or narrowness
in citation behaviour.
"
1147,"The Digital Public Library of America Ingestion Ecosystem: Lessons
  Learned After One Year of Large-Scale Collaborative Metadata Aggregation","  The Digital Public Library of America (DPLA) aggregates metadata for cultural
heritage materials from 20 direct partners, or Hubs, across the United States.
While the initial build-out of the DPLA's infrastructure used a lightweight
ingestion system that was ultimately pushed into production, a year's
experience has allowed DPLA and its partners to identify limitations to that
system, the quality and scalability of metadata remediation and enhancement
possible, and areas for collaboration and leadership across the partnership.
Although improved infrastructure is needed to support aggregation at this scale
and complexity, ultimately DPLA needs to balance responsibilities across the
partnership and establish a strong community that shares ownership of the
aggregation process.
"
1148,"Case Studies and Challenges in Reproducibility in the Computational
  Sciences","  This paper investigates the reproducibility of computational science research
and identifies key challenges facing the community today. It is the result of
the First Summer School on Experimental Methodology in Computational Science
Research (https://blogs.cs.st-andrews.ac.uk/emcsr2014/).
  First, we consider how to reproduce experiments that involve human subjects,
and in particular how to deal with different ethics requirements at different
institutions. Second, we look at whether parallel and distributed computational
experiments are more or less reproducible than serial ones. Third, we consider
reproducible computational experiments from fields outside computer science.
Our final case study looks at whether reproducibility for one researcher is the
same as for another, by having an author attempt to have others reproduce their
own, reproducible, paper.
  This paper is open, executable and reproducible: the whole process of writing
this paper is captured in the source control repository hosting both the source
of the paper, supplementary codes and data; we are providing setup for several
experiments on which we were working; finally, we try to describe what we have
achieved during the week of the school in a way that others may reproduce (and
hopefully improve) our experiments.
"
1149,Representing Dataset Quality Metadata using Multi-Dimensional Views,"  Data quality is commonly defined as fitness for use. The problem of
identifying quality of data is faced by many data consumers. Data publishers
often do not have the means to identify quality problems in their data. To make
the task for both stakeholders easier, we have developed the Dataset Quality
Ontology (daQ). daQ is a core vocabulary for representing the results of
quality benchmarking of a linked dataset. It represents quality metadata as
multi-dimensional and statistical observations using the Data Cube vocabulary.
Quality metadata are organised as a self-contained graph, which can, e.g., be
embedded into linked open datasets. We discuss the design considerations, give
examples for extending daQ by custom quality metrics, and present use cases
such as analysing data versions, browsing datasets by quality, and link
identification. We finally discuss how data cube visualisation tools enable
data publishers and consumers to analyse better the quality of their data.
"
1150,"How to improve the outcome of performance evaluations in terms of
  percentiles for citation frequencies of my papers","  Using empirical data I demonstrate that the result of performance evaluations
by percentiles can be drastically influenced by the proper choice of the
journal in which a manuscript is published.
"
1151,Modelling Citation Networks,"  The distribution of the number of academic publications as a function of
citation count for a given year is remarkably similar from year to year. We
measure this similarity as a width of the distribution and find it to be
approximately constant from year to year. We show that simple citation models
fail to capture this behaviour. We then provide a simple three parameter
citation network model using a mixture of local and global search processes
which can reproduce the correct distribution over time. We use the citation
network of papers from the hep-th section of arXiv to test our model. For this
data, around 20% of citations use global information to reference recently
published papers, while the remaining 80% are found using local searches. We
note that this is consistent with other studies though our motivation is very
different from previous work. Finally, we also find that the fluctuations in
the size of an academic publication's bibliography is important for the model.
This is not addressed in most models and needs further work.
"
1152,Toward a deeper understanding of Visualization through keyword analysis,"  We present the results of a comprehensive analysis of visualization paper
keywords supplied for 4366 papers submitted to five main visualization
conferences. We describe main keywords, topic areas, and 10-year historic
trends from two datasets: (1) the standardized PCS taxonomy keywords in use for
paper submissions for IEEE InfoVis, IEEE Vis-SciVis, IEEE VAST, EuroVis, and
IEEE PacificVis since 2009 and (2) the author-chosen keywords for papers
published in the IEEE Visualization conference series (now called IEEE VIS)
since 2004. Our analysis of research topics in visualization can serve as a
starting point to (a) help create a common vocabulary to improve communication
among different visualization sub-groups, (b) facilitate the process of
understanding differences and commonalities of the various research sub-fields
in visualization, (c) provide an understanding of emerging new research trends,
(d) facilitate the crucial step of finding the right reviewers for research
submissions, and (e) it can eventually lead to a comprehensive taxonomy of
visualization research. One additional tangible outcome of our work is an
application that allows visualization researchers to easily browse the 2600+
keywords used for IEEE VIS papers during the past 10 years, aiming at more
informed and, hence, more effective keyword selections for future visualization
publications.
"
1153,Collective credit allocation in science,"  Collaboration among researchers is an essential component of the modern
scientific enterprise, playing a particularly important role in
multidisciplinary research. However, we continue to wrestle with allocating
credit to the coauthors of publications with multiple authors, since the
relative contribution of each author is difficult to determine. At the same
time, the scientific community runs an informal field-dependent credit
allocation process that assigns credit in a collective fashion to each work.
Here we develop a credit allocation algorithm that captures the coauthors'
contribution to a publication as perceived by the scientific community,
reproducing the informal collective credit allocation of science. We validate
the method by identifying the authors of Nobel-winning papers that are credited
for the discovery, independent of their positions in the author list. The
method can also compare the relative impact of researchers working in the same
field, even if they did not publish together. The ability to accurately measure
the relative credit of researchers could affect many aspects of credit
allocation in science, potentially impacting hiring, funding, and promotion
decisions.
"
1154,"Semantic Publishing Challenge -- Assessing the Quality of Scientific
  Output","  Linked Open Datasets about scholarly publications enable the development and
integration of sophisticated end-user services; however, richer datasets are
still needed. The first goal of this Challenge was to investigate novel
approaches to obtain such semantic data. In particular, we were seeking methods
and tools to extract information from scholarly publications, to publish it as
LOD, and to use queries over this LOD to assess quality. This year we focused
on the quality of workshop proceedings, and of journal articles w.r.t. their
citation network. A third, open task, asked to showcase how such semantic data
could be exploited and how Semantic Web technologies could help in this
emerging context.
"
1155,Fairer citation-based metrics,"  I describe a simple modification which can be applied to any citation
count-based index (e.g. Hirsch's h-index) quantifying a researcher's
publication output. The key idea behind the proposed approach is that the merit
for the citations of a paper should be distributed amongst its authors
according to their relative contributions. In addition to producing inherently
fairer metrics I show that the proposed modification has the potential to
partially normalize for the unfair effects of honorary authorship and thus
discourage this practice.
"
1156,"Are topic-specific search term, journal name and author name
  recommendations relevant for researchers?","  In this paper we describe a case study where researchers in the social
sciences (n=19) assess topical relevance for controlled search terms, journal
names and author names which have been compiled automatically by
bibliometric-enhanced information retrieval (IR) services. We call these
bibliometric-enhanced IR services Search Term Recommender (STR), Journal Name
Recommender (JNR) and Author Name Recommender (ANR) in this paper. The
researchers in our study (practitioners, PhD students and postdocs) were asked
to assess the top n pre-processed recommendations from each recommender for
specific research topics which have been named by them in an interview before
the experiment. Our results show clearly that the presented search term,
journal name and author name recommendations are highly relevant to the
researchers' topic and can easily be integrated for search in Digital
Libraries. The average precision for top ranked recommendations is 0.75 for
author names, 0.74 for search terms and 0.73 for journal names. The relevance
distribution differs largely across topics and researcher types. Practitioners
seem to favor author name recommendations while postdocs have rated author name
recommendations the lowest. In the experiment the small postdoc group (n=3)
favor journal name recommendations.
"
1157,"Is Evaluating Visual Search Interfaces in Digital Libraries Still an
  Issue?","  Although various visual interfaces for digital libraries have been developed
in prototypical systems, very few of these visual approaches have been
integrated into today's digital libraries. In this position paper we argue that
this is most likely due to the fact that the evaluation results of most visual
systems lack comparability. There is no fix standard on how to evaluate visual
interactive user interfaces. Therefore it is not possible to identify which
approach is more suitable for a certain context. We feel that the comparability
of evaluation results could be improved by building a common evaluation setup
consisting of a reference system, based on a standardized corpus with fixed
tasks and a panel for possible participants.
"
1158,"A review of the characteristics of 108 author-level bibliometric
  indicators","  An increasing demand for bibliometric assessment of individuals has led to a
growth of new bibliometric indicators as well as new variants or combinations
of established ones. The aim of this review is to contribute with objective
facts about the usefulness of bibliometric indicators of the effects of
publication activity at the individual level. This paper reviews 108 indicators
that can potentially be used to measure performance on the individual author
level, and examines the complexity of their calculations in relation to what
they are supposed to reflect and ease of end-user application.
"
1159,On the causes of subject-specific citation rates in Web of Science,"  It is well known in bibliometrics that the average number of citations per
paper differs greatly between the various disciplines. The differing citation
culture (in particular the different average number of references per paper and
thereby the different probability of being cited) is widely seen as the cause
of this variation. Based on all Web of Science (WoS) records published in 1990,
1995, 2000, 2005, and 2010 we demonstrate that almost all disciplines show
similar numbers of references in the appendices of their papers. Our results
suggest that the average citation rate is far more influenced by the extent to
which the papers (cited as references) are included in WoS as linked database
records. For example, the comparatively low citation rates in the humanities
are not at all the result of a lower average number of references per paper but
are caused by the low fraction of linked references which refer to papers
published in the core journals covered by WoS.
"
1160,A Multi-agent Based Digital Preservation Model,"  Master's Degree Thesis: Department of Physics, University of Turin
  Supervisor: Prof. Marco Maggiora, Department of Physics, University of Turin;
email: marco.maggiora@unito.it
  Co-Supervisor: Prof. Walter Allasia, Innovation Department, EURIX; email:
allasia@eurix.it
  The thesis describes an agent-based model aimed to simulate those processes
in which a digital object faces the risk of obsolescence, a migration process
has to be performed and the most appropriate file format has to be adopted.
Agents have been designed in order to monitor and control the local system
where they reside and its environment. They are able to become aware of
obsolescent formats based on global parameters such as their diffusion. They
communicate as well with each other to find out the most suitable preservation
action to be performed. Agents request suggestions that are evaluated and
propagated according to a weighting based on the level of trust assigned to
both the agents who identified the problem and proposed the solution. In the
current research, the definition of the trust level has been chosen based on
the cultural and geographical distances, the expertise of the involved agents
and the file format numerosity. The level of trust between two agents is
automatically updated after every interaction by the mean of a feedback
mechanism profiting of an inter agent communication based on stigmergy. Summing
up, the thesis demonstrates how a multi-agent system can either perform an
autonomous preservation action or suggest a list of best candidate solutions to
the user. It benefits the management of several kinds of digital archive,
especially those with limited resources specifically dedicated to digital
preservation, such as small personal collections and many public institutions.
"
1161,"A Note on the Ranking of Saudi Arabian Universities based on
  highlycited.com","  Recently, Thomson Reuters has published its 2014 list of highly cited
researchers (HCRs)[1]. Initial studies over the list [2] suggested that some
universities (for instance, King Abdulaziz University in Saudi Arabia) may have
been manipulating its world ranking by contracting with highly cited
researchers. In this work, we analyse the ranking of other Saudi universities
based solely on the list. Our analysis suggests that other universities in
Saudi Arabia do not follow the steps of King Abdulaziz University when it comes
to contracting with HCRs.
"
1162,Mathematical Knowledge Representation: Semantic Models and Formalisms,"  The paper provides a survey of semantic methods for solution of fundamental
tasks in mathematical knowledge management. Ontological models and formalisms
are discussed. We propose an ontology of mathematical knowledge, covering a
wide range of fields of mathematics. We demonstrate applications of this
representation in mathematical formula search, and learning.
"
1163,"Costly Collaborations: The Impact of Scientific Fraud on Co-authors'
  Careers","  Over the last few years, several major scientific fraud cases have shocked
the scientific community. The number of retractions each year has also
increased tremendously, especially in the biomedical field, and scientific
misconduct accounts for approximately more than half of those retractions. It
is assumed that co-authors of retracted papers are affected by their
colleagues' misconduct, and the aim of this study is to provide empirical
evidence of the effect of retractions in biomedical research on co-authors'
research careers. Using data from the Web of Science (WOS), we measured the
productivity, impact and collaboration of 1,123 co-authors of 293 retracted
articles for a period of five years before and after the retraction. We found
clear evidence that collaborators do suffer consequences of their colleagues'
misconduct, and that a retraction for fraud has higher consequences than a
retraction for error. Our results also suggest that the extent of these
consequences is closely linked with the ranking of co-authors on the retracted
paper, being felt most strongly by first authors, followed by the last authors,
while the impact is less important for middle authors.
"
1164,"Visualization of Co-Readership Patterns from an Online Reference
  Management System","  In this paper, we analyze the adequacy and applicability of readership
statistics recorded in social reference management systems for creating
knowledge domain visualizations. First, we investigate the distribution of
subject areas in user libraries of educational technology researchers on
Mendeley. The results show that around 69% of the publications in an average
user library can be attributed to a single subject area. Then, we use
co-readership patterns to map the field of educational technology. The
resulting visualization prototype, based on the most read publications in this
field on Mendeley, reveals 13 topic areas of educational technology research.
The visualization is a recent representation of the field: 80% of the
publications included were published within ten years of data collection. The
characteristics of the readers, however, introduce certain biases to the
visualization. Knowledge domain visualizations based on readership statistics
are therefore multifaceted and timely, but it is important that the
characteristics of the underlying sample are made transparent.
"
1165,How many citations are there in the Data Citation Index?,"  Descriptive analysis on the citation distribution of the Thomson Reuters'
Data Citation Index by publication type and four broad areas: Science,
Engineering & Technology, Humanities & Arts and Social Sciences.
"
1166,"Improving Accessibility of Archived Raster Dictionaries of Complex
  Script Languages","  We propose an approach to index raster images of dictionary pages which in
turn would require very little manual effort to enable direct access to the
appropriate pages of the dictionary for lookup. Accessibility is further
improved by feedback and crowdsourcing that enables highlighting of the
specific location on the page where the lookup word is found, annotation,
digitization, and fielded searching. This approach is equally applicable on
simple scripts as well as complex writing systems. Using our proposed approach,
we have built a Web application called ""Dictionary Explorer"" which supports
word indexes in various languages and every language can have multiple
dictionaries associated with it. Word lookup gives direct access to appropriate
pages of all the dictionaries of that language simultaneously. The application
has exploration features like searching, pagination, and navigating the word
index through a tree-like interface. The application also supports feedback,
annotation, and digitization features. Apart from the scanned images,
""Dictionary Explorer"" aggregates results from various sources and user
contributions in Unicode. We have evaluated the time required for indexing
dictionaries of different sizes and complexities in the Urdu language and
examined various trade-offs in our implementation. Using our approach, a single
person can make a dictionary of 1,000 pages searchable in less than an hour.
"
1167,Network analysis of Zentralblatt MATH data,"  We analyze the data about works (papers, books) from the time period
1990-2010 that are collected in Zentralblatt MATH database. The data were
converted into four 2-mode networks (works $\times$ authors, works $\times$
journals, works $\times$ keywords and works $\times$ MSCs) and into a partition
of works by publication year. The networks were analyzed using Pajek -- a
program for analysis and visualization of large networks. We explore the
distributions of some properties of works and the collaborations among
mathematicians. We also take a closer look at the characteristics of the field
of graph theory as were realized with the publications.
"
1168,Revealing Comparative Advantages in the Backbone of Science,"  Mapping Science across countries is a challenging task in the field of
Scientometrics. A number of efforts trying to cope with this task has been
discussed in the state of the art, addressing this challenge by processing
collections of scientific digital libraries and visualizing author-based
measures (for instance, the h-index) or document-based measures (for instance,
the averaged number of citations per document). A major drawback of these
approaches is related to the presence of bias. The bigger the country, the
higher the measure value. We explore the use of an econometric index to tackle
this limitation, known as the Revealed Comparative Advantage measure (RCA).
Using RCA, the diversity and ubiquity of each field of knowledge is mapped
across countries. Then, a RCA-based proximity function is explored to visualize
citation and h-index ubiquity. Science maps relating 27 knowledge areas and 237
countries are introduced using data crawled from Scimago that ranges from 1996
to 2011. Our results shows that the proposal is feasible and can be extended to
ellaborate a global scientific production characterization.
"
1169,"Investigation of Partition Cells as a Structural Basis Suitable for
  Assessments of Individual Scientists","  Individual, excellent scientists have become increasingly important in the
research funding landscape. Accurate bibliometric measures of an individual's
performance could help identify excellent scientists, but still present a
challenge. One crucial aspect in this respect is an adequate delineation of the
sets of publications that determine the reference values to which a scientist's
publication record and its citation impact should be compared. The structure of
partition cells formed by intersecting fixed subject categories in a database
has been proposed to approximate a scientist's specialty more closely than can
be done with the broader subject categories. This paper investigates this cell
structure's suitability as an underlying basis for methodologies to assess
individual scientists, from two perspectives: (1) Proximity to the actual
structure of publication records of individual scientists: The distribution and
concentration of publications over the highly fragmented structure of partition
cells are examined for a sample of ERC grantees; (2) Proximity to customary
levels of accuracy: Differences in commonly used reference values (mean
expected number of citations per publication, and threshold number of citations
for highly cited publications) between adjacent partition cells are compared to
differences in two other dimensions: successive publication years and
successive citation window lengths. Findings from both perspectives are in
support of partition cells rather than the larger subject categories as a
journal based structure on which to construct and apply methodologies for the
assessment of highly specialized publication records such as those of
individual scientists.
"
1170,"Usefulness of altmetrics for measuring the broader impact of research: A
  case study using data from PLOS (altmetrics) and F1000Prime (paper tags)","  Purpose: Whereas citation counts allow the measurement of the impact of
research on research itself, an important role in the measurement of the impact
of research on other parts of society is ascribed to altmetrics. The present
case study investigates the usefulness of altmetrics for measuring the broader
impact of research. Methods: This case study is essentially based on a dataset
with papers obtained from F1000. The dataset was augmented with altmetrics
(such as Twitter counts) which were provided by PLOS (the Public Library of
Science). In total, the case study covers a total of 1,082 papers. Findings:
The F1000 dataset contains tags on papers which were assigned intellectually by
experts and which can characterise a paper. The most interesting tag for
altmetric research is ""good for teaching"". This tag is assigned to papers which
could be of interest to a wider circle of readers than the peers in a
specialist area. Particularly on Facebook and Twitter, one could expect papers
with this tag to be mentioned more often than those without this tag. With
respect to the ""good for teaching"" tag, the results from regression models were
able to confirm these expectations: Papers with this tag show significantly
higher Facebook and Twitter counts than papers without this tag. This
association could not be seen with Mendeley or Figshare counts (that is with
counts from platforms which are chiefly of interest in a scientific context).
Conclusions: The results of the current study indicate that Facebook and
Twitter, but not Figshare or Mendeley, can provide indications of papers which
are of interest to a broader circle of readers (and not only for the peers in a
specialist area), and seem therefore be useful for societal impact measurement.
"
1171,A variant of the h-index to measure recent performance,"  The predictive power of the h-index has been shown to depend for a long time
on citations to rather old publications. This has raised doubts about its
usefulness for predicting future scientific achievements. Here I investigate a
variant which considers only the recent publications and is therefore more
useful in academic hiring processes and for the allocation of research
resources. It is simply defined in analogy to the usual h-index, but taking
into account only the publications from recent years, and it can easily be
determined from the ISI Web of Knowledge.
"
1172,"The impact of a few: The effect of alternative formulas for recruiting
  talent in a non-competitive system","  This paper analyzes the effect of the Catalan programme ICREA for the
selection of talented researchers on the percentage of highly cited papers of
Catalan universities
"
1173,The 7 Habits of Highly Effective Research Communicators,"  The emergence of Web 2.0 and simultaneously Library 2.0 platforms has helped
the library and information professionals to outreach to new audiences beyond
their physical boundaries. In a globalized society, information becomes very
useful resource for socio-economic empowerment of marginalized communities,
economic prosperity of common citizens, and knowledge enrichment of liberated
minds. Scholarly information becomes both developmental and functional for
researchers working towards advancement of knowledge. We must recognize a relay
of information flow and information ecology while pursuing scholarly research.
Published scholarly literatures we consult that help us in creation of new
knowledge. Similarly, our published scholarly works should be outreached to
future researchers for regeneration of next dimension of knowledge.
Fortunately, present day research communicators have many freely available
personalized digital tools to outreach to globalized research audiences having
similar research interests. These tools and techniques, already adopted by many
researchers in different subject areas across the world, should be
enthusiastically utilized by LIS researchers in South Asia for global
dissemination of their scholarly research works. This newly found enthusiasm
will soon become integral part of the positive habits and cultural practices of
research communicators in LIS domain.
"
1174,"From Attention to Citation, What and How Does Altmetrics Work?","  Scholarly and social impacts of scientific publications could be measured by
various metrics. In this study, the relationship between various metrics of
63,805 PLOS research articles are studied. Generally, article views correlate
well with citation, however, different types of article view have different
levels of correlation with citation, when pdf download correlates the citation
most significantly. It's necessary for publishers and journals to provide
detailed and comprehensive article metrics. Although the low correlation
between social attention and citation is confirmed by this study and previous
studies, more than ever, we find that social attention is highly correlated
with article view, especially the browser html view. Social attention is the
important source that bringing network traffic to browser html view and may
lead to citation subsequently. High altmetric score has the potential role in
promoting the long-term academic impact of articles, when a conceptual model is
proposed to interpret the conversion from social attention to article view, and
to citation finally.
"
1175,The Generation of Large Networks from Web-of-Science Data,"  During the 1990s, one of us developed a series of freeware routines
(http://www.leydesdorff.net/indicators) that enable the user to organize
downloads from the Web-of-Science (Thomson Reuters) into a relational database,
and then to export matrices for further analysis in various formats (for
example, for co-author analysis). The basic format of the matrices displays
each document as a case in a row that can be attributed different variables in
the columns. One limitation to this approach was hitherto that relational
databases typically have an upper limit for the number of variables, such as
256 or 1024. In this brief communication, we report on a way to circumvent this
limitation by using txt2Pajek.exe, available as freeware from
http://www.pfeffer.at/txt2pajek/.
"
1176,Is the new citation-rank approach P100' in bibliometrics really new?,"  The percentile-based rating scale P100 describes the citation impact in terms
of the distribution of unique citation values. This approach has recently been
refined by considering also the frequency of papers with the same citation
counts. Here I compare the resulting P100' with P100 for an empirical dataset
and a simple fictitious model dataset. It is shown that P100' is not much
different from standard percentile-based ratings in terms of citation
frequencies. A new indicator P100'' is introduced.
"
1177,The Scientific Competitiveness of Nations,"  We use citation data of scientific articles produced by individual nations in
different scientific domains to determine the structure and efficiency of
national research systems. We characterize the scientific fitness of each
nation (that is, the competitiveness of its research system) and the complexity
of each scientific domain by means of a non-linear iterative algorithm able to
assess quantitatively the advantage of scientific diversification. We find that
technological leading nations, beyond having the largest production of
scientific papers and the largest number of citations, do not specialize in a
few scientific domains. Rather, they diversify as much as possible their
research system. On the other side, less developed nations are competitive only
in scientific domains where also many other nations are present.
Diversification thus represents the key element that correlates with scientific
and technological competitiveness. A remarkable implication of this structure
of the scientific competition is that the scientific domains playing the role
of ""markers"" of national scientific competitiveness are those not necessarily
of high technological requirements, but rather addressing the most
""sophisticated"" needs of the society.
"
1178,"Identification of Influential Scientists vs. Mass Producers by the
  Perfectionism Index","  The concept of h-index has been proposed to easily assess a researcher's
performance with a single number. However, by using only this number, we lose
significant information about the distribution of citations per article in an
author's publication list. In this article, we study an author's citation curve
and we define two new areas related to this curve. We call these ""penalty
areas"", since the greater they are, the more an author's performance is
penalized. We exploit these areas to establish new indices, namely PI and XPI,
aiming at categorizing researchers in two distinct categories: ""influentials""
and ""mass producers""; the former category produces articles which are (almost
all) with high impact, and the latter category produces a lot of articles with
moderate or no impact at all. Using data from Microsoft Academic Service, we
evaluate the merits mainly of PI as a useful tool for scientometric studies. We
establish its effectiveness into separating the scientists into influentials
and mass producers; we demonstrate its robustness against self-citations, and
its uncorrelation to traditional indices. Finally, we apply PI to rank
prominent scientists in the areas of databases, networks and multimedia,
exhibiting the strength of the index in fulfilling its design goal.
"
1179,"Knowledge discovery via multidimensional science maps: the case of the
  Species Problem","  Science mapping (SM), the study of the organization and development of
science and technology, is a rapidly developing field within information
science. The volume of available data allows this methodology to empirically
address such issues as the historical development of topics, discourses, fields
or the entire science system. Based on the pool of related methods, we are
proposing an integration of various maps to obtain a novel kind of science map
we call multidimensional. The basic idea behind is to combine the most
informative relations available from various maps based on different
bibliometric indicators, in order to produce a rich structrue for the study of
knowledge dynamics, with special emphasis on causal-historical connections. As
a proof of concept, we deploy the proposed framework in an extensive case study
on a historical topic from the life sciences, namely, the debate on the species
concept in biosystematics.
"
1180,An Analysis of Publication Venues for Automatic Differentiation Research,"  We present the results of our analysis of publication venues for papers on
automatic differentiation (AD), covering academic journals and conference
proceedings. Our data are collected from the AD publications database
maintained by the autodiff.org community website. The database is purpose-built
for the AD field and is expanding via submissions by AD researchers. Therefore,
it provides a relatively noise-free list of publications relating to the field.
However, it does include noise in the form of variant spellings of journal and
conference names. We handle this by manually correcting and merging these
variants under the official names of corresponding venues. We also share the
raw data we get after these corrections.
"
1181,"Is your EPL attractive? Classification of publications through download
  statistics","  Here we consider the download statistics of EPL publications. We find that
papers in the journal are characterised by fast accumulations of downloads
during the first couple of months after publication, followed by slower rates
thereafter, behaviour which can be represented by a model with predictive
power. We also find that individual papers can be classified in various ways,
allowing us to compare categories for open-access and non-open-access papers.
For example, for the latter publications, which comprise the bulk of EPL
papers, a small proportion (2%) display intense bursts of download activity,
possibly following an extended period of less remarkable behaviour. About 18%
have an especially high degree of attractiveness over and above what is typical
for the journal. One can also classify the ageing of attractiveness by
examining download half-lives. Approximately 18% have strong interest
initially, waning in time. A further 20% exhibit ""delayed recognition"" with
relatively late spurs in download activity. Although open-access papers enjoy
more downloads on average, the proportions falling into each category are
similar.
"
1182,"Universality of citation distributions for academic institutions and
  journals","  Citations measure the importance of a publication, and may serve as a proxy
for its popularity and quality of its contents. Here we study the distributions
of citations to publications from individual academic institutions for a single
year. The average number of citations have large variations between different
institutions across the world, but the probability distributions of citations
for individual institutions can be rescaled to a common form by scaling the
citations by the average number of citations for that institution. We find this
feature seem to be universal for a broad selection of institutions irrespective
of the average number of citations per article. A similar analysis for
citations to publications in a particular journal in a single year reveals
similar results. We find high absolute inequality for both these sets, Gini
coefficients being around $0.66$ and $0.58$ for institutions and journals
respectively. We also find that the top $25$% of the articles hold about $75$%
of the total citations for institutions and the top $29$% of the articles hold
about $71$% of the total citations for journals.
"
1183,Uncovering the dynamics of citations of scientific papers,"  We demonstrate a comprehensive framework that accounts for citation dynamics
of scientific papers and for the age distribution of references. We show that
citation dynamics of scientific papers is nonlinear and this nonlinearity has
far-reaching consequences, such as diverging citation distributions and runaway
papers. We propose a nonlinear stochastic dynamic model of citation dynamics
based on link copying/redirection mechanism. The model is fully calibrated by
empirical data and does not contain free parameters. This model can be a basis
for quantitative probabilistic prediction of citation dynamics of individual
papers and of the journal impact factor.
"
1184,"Tweets vs. Mendeley readers: How do these two social media metrics
  differ?","  A set of 1.4 million biomedical papers was analyzed with regards to how often
articles are mentioned on Twitter or saved by users on Mendeley. While Twitter
is a microblogging platform used by a general audience to distribute
information, Mendeley is a reference manager targeted at an academic user group
to organize scholarly literature. Both platforms are used as sources for
so-called altmetrics to measure a new kind of research impact. This analysis
shows in how far they differ and compare to traditional citation impact metrics
based on a large set of PubMed papers.
"
1185,"International Scientific Collaboration of China: Collaborating
  Countries, Institutions and Individuals","  Using bibliometric methods, we investigate China's international scientific
collaboration from 3 levels of collaborating countries, institutions and
individuals. We design a database in SQL Server, and make analysis of Chinese
SCI papers based on the corresponding author field. We find that China's
international scientific collaboration is focused on a handful of countries.
Nearly 95% international co-authored papers are collaborated with only 20
countries, among which the USA account for more than 40% of all. Results also
show that Chinese lineage in the international co-authorship is obvious, which
means Chinese immigrant scientists are playing an important role in China's
international scientific collaboration, especially in English-speaking
countries.
"
1186,"The Role of Chinese-American Scientists in China-US Scientific
  Collaboration: A Study in Nanotechnology","  In this paper, we use bibliometric methods and social network analysis to
analyze the pattern of China-US scientific collaboration on individual level in
nanotechnology. Results show that Chinese-American scientists have been playing
an important role in China-US scientific collaboration. We find that China-US
collaboration in nanotechnology mainly occurs between Chinese and
Chinese-American scientists. In the co-authorship network, Chinese-American
scientists tend to have higher betweenness centrality. Moreover, the series of
polices implemented by the Chinese government to recruit oversea experts seems
to contribute a lot to China-US scientific collaboration.
"
1187,"A macro level scientometric analysis of world tribology research output
  (1998 - 2012)","  Bibliographic records related to tribology research were extracted from
SCOPUS and Web of Science databases for the period of 15 years from 1998 to
2012. Macro-level scientometric indicators such as growth rate, share of
international collaborative papers, citations per paper, and share of non-cited
papers were employed. Further, the Gini coefficient and Simpson Index of
Diversity were used. Two new relative indicators : Relative International
Collaboration Rate (RICR) and Relative Growth Index (RGI) are proposed in this
study. The performance of top countries contributing more than 1000 papers
across the study period was discussed. Contributions and share of continents
and countries by income groups were examined. Further research contributions
and citation impact of selected country groups such as the Developing Eight
Countries (D8), the Association of Southeast Asian Nations (ASEAN), the Union
of South American Nations (UNASUR) and the Emerging and Growth-Leading
Economies (EAGLEs) countries were analyzed. High levels of interdisciplinarity
exist in tribology research. Inequality of distribution between countries is
highest for number of publications and citations. Asia outperforms the other
world regions and China contributes most of the papers (25%), while the United
States receives most of the citations (22%). 84% of total output was
contributed by the Asiatic region, Western Europe and North America together.
Publications from these three world regions received 88% of total citations.
Around 50% of global research output was contributed by China, the United
States and Japan.
"
1188,"Astrophysicists on Twitter: An in-depth analysis of tweeting and
  scientific publication behavior","  This paper analyzes the tweeting behavior of 37 astrophysicists on Twitter
and compares their tweeting behavior with their publication behavior and
citation impact to show whether they tweet research-related topics or not.
Astrophysicists on Twitter are selected to compare their tweets with their
publications from Web of Science. Different user groups are identified based on
tweeting and publication frequency. A moderate negative correlation (p=-0.390*)
is found between the number of publications and tweets per day, while retweet
and citation rates do not correlate. The similarity between tweets and
abstracts is very low (cos=0.081). User groups show different tweeting behavior
such as retweeting and including hashtags, usernames and URLs. The study is
limited in terms of the small set of astrophysicists. Results are not
necessarily representative of the entire astrophysicist community on Twitter
and they most certainly do not apply to scientists in general. Future research
should apply the methods to a larger set of researchers and other scientific
disciplines. To a certain extent, this study helps to understand how
researchers use Twitter. The results hint at the fact that impact on Twitter
can neither be equated with nor replace traditional research impact metrics.
However, tweets and other so-called altmetrics might be able to reflect other
impact of scientists such as public outreach and science communication. To the
best of our knowledge, this is the first in-depth study comparing researchers'
tweeting activity and behavior with scientific publication output in terms of
quantity, content and impact.
"
1189,"Measuring impact in research evaluations: A thorough discussion of
  methods for, effects of, and problems with impact measurements","  Impact of science is one of the most important topics in scientometrics.
Recent developments show a fundamental change in impact measurements from
impact on science to impact on society. Since impact measurement is currently
in a state of far reaching changes, this paper describes recent developments
and facing problems in this area. For that the results of key publications
(dealing with impact measurement) are discussed. The paper discusses how impact
is generally measured within science and beyond (section 2), which effects
impact measurements have on the science system (section 3), and which problems
are associated with impact measurement (section 4). The problems associated
with impact measurement constitute the focus of this paper: Science is marked
by inequality, random chance, anomalies, the right to make mistakes,
unpredictability, and a high significance of extreme events, which might
distort impact measurements. Scientometricians as the producer of impact scores
and decision makers as their consumers should be aware of these problems and
should consider them in the generation and interpretation of bibliometric
results, respectively.
"
1190,"An approach to the author citation potential: Measures of scientific
  performance which are invariant across scientific fields","  The citation potential is a measure of the probability of being cited.
Obviously, it is different among fields of science, social science, and
humanities because of systematic differences in publication and citation
behaviour across disciplines. In the past, the citation potential was studied
at journal level considering the average number of references in established
groups of journals (for example, the crown indicator is based on the journal
subject categories in the Web of Science database). In this paper, some
characterizations of the author's scientific research through three different
research dimensions are proposed: production (journal papers), impact (journal
citations), and reference (bibliographical sources). Then, we propose different
measures of the citation potential for authors based on a proportion of these
dimensions. An empirical application, in a set of 120 randomly selected highly
productive authors from the CSIC Research Centre (Spain) in four subject areas,
shows that the ratio between production and impact dimensions is a normalized
measure of the citation potential at the level of individual authors. Moreover,
this ratio reduces the between-group variance in relation to the within-group
variance in a higher proportion than the rest of the indicators analysed.
Furthermore, it is consistent with the type of journal impact indicator used. A
possible application of this result is in the selection and promotion process
within interdisciplinary institutions, since it allows comparisons of authors
based on their particular scientific research.
"
1191,Rise of the Rest: The Growing Impact of Non-Elite Journals,"  In this paper, we examine the evolution of the impact of non-elite journals.
We attempt to answer two questions. First, what fraction of the top-cited
articles are published in non-elite journals and how has this changed over
time. Second, what fraction of the total citations are to non-elite journals
and how has this changed over time.
  We studied citations to articles published in 1995-2013. We computed the 10
most-cited journals and the 1000 most-cited articles each year for all 261
subject categories in Scholar Metrics. We marked the 10 most-cited journals in
a category as the elite journals for the category and the rest as non-elite.
  There are two conclusions from our study. First, the fraction of top-cited
articles published in non-elite journals increased steadily over 1995-2013.
While the elite journals still publish a substantial fraction of high-impact
articles, many more authors of well-regarded papers in diverse research fields
are choosing other venues.
  The number of top-1000 papers published in non-elite journals for the
representative subject category went from 149 in 1995 to 245 in 2013, a growth
of 64%. Looking at broad research areas, 4 out of 9 areas saw at least
one-third of the top-cited articles published in non-elite journals in 2013.
For 6 out of 9 areas, the fraction of top-cited papers published in non-elite
journals for the representative subject category grew by 45% or more.
  Second, now that finding and reading relevant articles in non-elite journals
is about as easy as finding and reading articles in elite journals, researchers
are increasingly building on and citing work published everywhere. Considering
citations to all articles, the percentage of citations to articles in non-elite
journals went from 27% in 1995 to 47% in 2013. Six out of nine broad areas had
at least 50% of citations going to articles published in non-elite journals in
2013.
"
1192,Coauthorship and Citation Networks for Statisticians,"  We have collected and cleaned two network data sets: Coauthorship and
Citation networks for statisticians. The data sets are based on all research
papers published in four of the top journals in statistics from $2003$ to the
first half of $2012$. We analyze the data sets from many different
perspectives, focusing on (a) centrality, (b) community structures, and (c)
productivity, patterns and trends.
  For (a), we have identified the most prolific/collaborative/highly cited
authors. We have also identified a handful of ""hot"" papers, suggesting
""Variable Selection"" as one of the ""hot"" areas.
  For (b), we have identified about $15$ meaningful communities or research
groups, including large-size ones such as ""Spatial Statistics"", ""Large-Scale
Multiple Testing"", ""Variable Selection"" as well as small-size ones such as
""Dimensional Reduction"", ""Objective Bayes"", ""Quantile Regression"", and
""Theoretical Machine Learning"".
  For (c), we find that over the 10-year period, both the average number of
papers per author and the fraction of self citations have been decreasing, but
the proportion of distant citations has been increasing. These suggest that the
statistics community has become increasingly more collaborative, competitive,
and globalized.
  Our findings shed light on research habits, trends, and topological patterns
of statisticians. The data sets provide a fertile ground for future researches
on or related to social networks of statisticians.
"
1193,Estimating Open Access Mandate Effectiveness: The MELIBEA Score,"  MELIBEA is a Spanish database that uses a composite formula with eight
weighted conditions to estimate the effectiveness of Open Access mandates
(registered in ROARMAP). We analyzed 68 mandated institutions for publication
years 2011-2013 to determine how well the MELIBEA score and its individual
conditions predict what percentage of published articles indexed by Web of
Knowledge is deposited in each institution's OA repository, and when. We found
a small but significant positive correlation (0.18) between MELIBEA score and
deposit percentage. We also found that for three of the eight MELIBEA
conditions (deposit timing, internal use, and opt-outs), one value of each was
strongly associated with deposit percentage or deposit latency (immediate
deposit required, deposit required for performance evaluation, unconditional
opt-out allowed for the OA requirement but no opt-out for deposit requirement).
When we updated the initial values and weights of the MELIBEA formula for
mandate effectiveness to reflect the empirical association we had found, the
score's predictive power doubled (.36). There are not yet enough OA mandates to
test further mandate conditions that might contribute to mandate effectiveness,
but these findings already suggest that it would be useful for future mandates
to adopt these three conditions so as to maximize their effectiveness, and
thereby the growth of OA.
"
1194,"Whole counting vs. whole-normalized counting: A country level
  comparative study of internationally collaborated papers on Tribology","  The purpose of this study is to compare the changing behavior of two counting
methods (whole counting and whole-normalized counting) and inflation rate at
country level research productivity and impact. For this, publication data on
tribology research published between 1998 and 2012 from SCOPUS has been used.
Only internationally collaborated papers are considered for comparison between
two counting methods. The result of correlation tests shows that there is
highly correlation in all the four indicators between the two counting methods.
However, the result of t-test shows that there is significant difference in the
three indicators (paper count, citation count and h-index) between the two
counting methods. This study concludes that whole-normalized counting
(fractional) is the better choice for publication and citations counting at the
country level assessment.
"
1195,"Philosophy of science viewed through the lense of ""References
  Publication Years spectrosopy"" (RPYS)","  We examine the sub-field of philosophy of science using a new method
developed in information science, Referenced Publication Years Spectroscopy
(RPYS). RPYS allows us to identify peak years in citations in a field, which
promises to help scholars identify the key contributions to a field, and
revolutionary discoveries in a field. We discovered that philosophy of science,
a sub-field in the humanities, differs significantly from other fields examined
with this method. Books play a more important role in philosophy of science
than in the sciences. Further, Einstein's famous 1905 papers created a citation
peak in the philosophy of science literature. But rather than being a
contribution to the philosophy of science, their importance lies in the fact
that they are revolutionary contributions to physics with important
implications for philosophy of science.
"
1196,Data engineering for archive evolution,"  From the moment astronomical observations are made the resulting data
products begin to grow stale. Even if perfect binary copies are preserved
through repeated timely migration to more robust storage media, data standards
evolve and new tools are created that require different kinds of data or
metadata. The expectations of the astronomical community change even if the
data do not. We discuss data engineering to mitigate the ensuing risks with
examples from a recent project to refactor seven million archival images to new
standards of nomenclature, metadata, format, and compression.
"
1197,"NetworkRepository: An Interactive Data Repository with Multi-scale
  Visual Analytics","  Network Repository (NR) is the first interactive data repository with a
web-based platform for visual interactive analytics. Unlike other data
repositories (e.g., UCI ML Data Repository, and SNAP), the network data
repository (networkrepository.com) allows users to not only download, but to
interactively analyze and visualize such data using our web-based interactive
graph analytics platform. Users can in real-time analyze, visualize, compare,
and explore data along many different dimensions. The aim of NR is to make it
easy to discover key insights into the data extremely fast with little effort
while also providing a medium for users to share data, visualizations, and
insights. Other key factors that differentiate NR from the current data
repositories is the number of graph datasets, their size, and variety. While
other data repositories are static, they also lack a means for users to
collaboratively discuss a particular dataset, corrections, or challenges with
using the data for certain applications. In contrast, we have incorporated many
social and collaborative aspects into NR in hopes of further facilitating
scientific research (e.g., users can discuss each graph, post observations,
visualizations, etc.).
"
1198,"Does A Paper Being Featured on The Cover of A Journal Guarantee More
  Attention and Greater Impact?","  Paper featured on the cover of a journal has more visibility in an issue
compared with other ordinary articles for both printed and electronic journal.
Does this kind of visibility guarantee more attention and greater impact of its
associated content than the non-cover papers? In this research, usage and
citation data of 60 issues of PLOS Biology from 2006 to 2010 are analyzed to
compare the attention and scholarly impact between cover and non-cover paper.
Our empirical study confirms that, in most cases, the group difference between
cover and non-cover paper is not significant for attention or impact. Cover
paper is not the best one, nor at the upper level in one issue considering the
attention or the citation impact. Having a paper featured on the cover of a
journal may be a source of pride to researchers, many institutions and
researchers would even release news about it. However, a paper being featured
on the cover of a journal doesn't guarantee more attention and greater impact.
"
1199,"Google Scholar makes it Hard - the complexity of organizing one's
  publications","  With Google Scholar, scientists can maintain their publications on personal
profile pages, while the citations to these works are automatically collected
and counted. Maintenance of publications is done manually by the researcher
herself, and involves deleting erroneous ones, merging ones that are the same
but which were not recognized as the same, adding forgotten co-authors, and
correcting titles of papers and venues. The publications are presented on pages
with 20 or 100 papers in the web page interface from 2012--2014. The interface
does not allow a scientist to merge two version of a paper if they appear on
different pages. This not only implies that a scientist who wants to merge
certain subsets of publications will sometimes be unable to do so, but also, we
show in this note that the decision problem to determine if it is possible to
merge given subsets of papers is NP-complete.
"
1200,"Tweets as impact indicators: Examining the implications of automated bot
  accounts on Twitter","  This brief communication presents preliminary findings on automated Twitter
accounts distributing links to scientific papers deposited on the preprint
repository arXiv. It discusses the implication of the presence of such bots
from the perspective of social media metrics (altmetrics), where mentions of
scholarly documents on Twitter have been suggested as a means of measuring
impact that is both broader and timelier than citations. We present preliminary
findings that automated Twitter accounts create a considerable amount of tweets
to scientific papers and that they behave differently than common social bots,
which has critical implications for the use of raw tweet counts in research
evaluation and assessment. We discuss some definitions of Twitter cyborgs and
bots in scholarly communication and propose differentiating between different
levels of engagement from tweeting only bibliographic information to discussing
or commenting on the content of a paper.
"
1201,"Research status and trends in Operations Research and Management Science
  (OR/MS) journals: A bibliometric analysis based on the Web of Science
  database 2001-2012","  A bibliometric analysis to evaluate global scientific production in the
subject category of Operations Research and Management Science (OR/MS) from
2001 to 2012 was applied. Data was based on the Web of Science (Science
Citation Index) database compiled by Thomson Reuters. The results showed that
the OR/MS research has significantly increased over the past twelve years. The
Bradford core journals in the category were identified. The researchers paid
great attention to networks, control, and simulation. Among the countries, USA
attained a dominant position in global research in the field.
"
1202,"Methods for the generation of normalized citation impact scores in
  bibliometrics: Which method best reflects the judgements of experts?","  Evaluative bibliometrics compares the citation impact of researchers,
research groups and institutions with each other across time scales and
disciplines. Both factors - discipline and period - have an influence on the
citation count which is independent of the quality of the publication.
Normalizing the citation impact of papers for these two factors started in the
mid-1980s. Since then, a range of different methods have been presented for
producing normalized citation impact scores. The current study uses a data set
of over 50,000 records to test which of the methods so far presented correlate
better with the assessment of papers by peers. The peer assessments come from
F1000Prime - a post-publication peer review system of the biomedical
literature. Of the normalized indicators, the current study involves not only
cited-side indicators, such as the mean normalized citation score, but also
citing-side indicators. As the results show, the correlations of the indicators
with the peer assessments all turn out to be very similar. Since F1000 focuses
on biomedicine, it is important that the results of this study are validated by
other studies based on datasets from other disciplines or (ideally) based on
multi-disciplinary datasets.
"
1203,Towards a Virtual Data Centre for Classics,"  The paper presents some of our work on integrating datasets in Classics. We
present the results of various projects we had in this domain. The conclusions
from LaQuAT concerned limitations to the approach rather than solutions. The
relational model followed by OGSA-DAI was more effective for resources that
consist primarily of structured data (which we call data-centric) rather than
for largely unstructured text (which we call text-centric), which makes up a
significant component of the datasets we were using. This approach was,
moreover, insufficiently flexible to deal with the semantic issues. The gMan
project, on the other hand, addressed these problems by virtualizing data
resources using full-text indexes, which can then be used to provide different
views onto the collections and services that more closely match the sort of
information organization and retrieval activities found in the humanities, in
an environment that is more interactive, researcher-focused, and
researcher-driven.
"
1204,Does Google Scholar contain all highly cited documents (1950-2013)?,"  The study of highly cited documents on Google Scholar (GS) has never been
addressed to date in a comprehensive manner. The objective of this work is to
identify the set of highly cited documents in Google Scholar and define their
core characteristics: their languages, their file format, or how many of them
can be accessed free of charge. We will also try to answer some additional
questions that hopefully shed some light about the use of GS as a tool for
assessing scientific impact through citations. The decalogue of research
questions is shown below:
  1. Which are the most cited documents in GS?
  2. Which are the most cited document types in GS?
  3. What languages are the most cited documents written in GS?
  4. How many highly cited documents are freely accessible?
  4.1 What file types are the most commonly used to store these highly cited
documents?
  4.2 Which are the main providers of these documents?
  5. How many of the highly cited documents indexed by GS are also indexed by
WoS?
  6. Is there a correlation between the number of citations that these highly
cited documents have received in GS and the number of citations they have
received in WoS?
  7. How many versions of these highly cited documents has GS detected?
  8. Is there a correlation between the number of versions GS has detected for
these documents, and the number citations they have received?
  9. Is there a correlation between the number of versions GS has detected for
these documents, and their position in the search engine result pages?
  10. Is there some relation between the positions these documents occupy in
the search engine result pages, and the number of citations they have received?
"
1205,Team size matters: Collaboration and scientific impact since 1900,"  This paper provides the first historical analysis of the relationship between
collaboration and scientific impact, using three indicators of collaboration
(number of authors, number of addresses, and number of countries) and including
articles published between 1900 and 2011. The results demonstrate that an
increase in the number of authors leads to an increase in impact--from the
beginning of the last century onwards--and that this is not simply due to
self-citations. A similar trend is also observed for the number of addresses
and number of countries represented in the byline of an article. However, the
constant inflation of collaboration since 1900 has resulted in diminishing
citation returns: larger and more diverse (in terms of institutional and
country affiliation) teams are necessary to realize higher impact. The paper
concludes with a discussion of the potential causes of the impact gain in
citations of collaborative papers.
"
1206,"A simple interpretation of the growth of scientific/technological
  research impact leading to hype-type evolution curves","  The empirical and theoretical justification of Gartner hype curves is a very
relevant open question in the field of Technological Life Cycle analysis. The
scope of the present paper is to introduce a simple model describing the growth
of scientific/technological research impact, in the specific case where science
is the main source of a new idea driving a technological development, leading
to hype-type evolution curves. The main idea of the model is that, in a first
stage, the growth of the scientific interest of a new specific field (as can be
measured by publication numbers) basically follows the classical logistic
growth curve. At a second stage, starting at a later trigger time, the
technological development based on that scientific idea (as can be measured by
patent deposits) can be described as the integral (in a mathematical sense) of
the first curve, since technology is based on the overall accumulated
scientific knowledge. The model is tested through a bibliometric analysis of
the publication and patent deposit rate for Organic Light Emitting Diodes
(OLED) scientific research and technology, as well as for other emerging
technologies.
"
1207,Study of Citation Networks in Tribology Research,"  CitNetExplorer has been used to study the citation networks among the
scientific publications on tribology during the 15 years period from 1998-2012.
Three data sets from Web of Science have been analyzed: (1) Core publications
of tribology research, (2) publications on nanotribology and (3) publications
of Bharat Bhushan (a top-contributor to nanotribology research). Based on this
study, some suggestions are made to improve the CitNetExplorer.
"
1208,On the Shoulders of Giants: The Growing Impact of Older Articles,"  In this paper, we examine the evolution of the impact of older scholarly
articles. We attempt to answer four questions. First, how often are older
articles cited and how has this changed over time. Second, how does the impact
of older articles vary across different research fields. Third, is the change
in the impact of older articles accelerating or slowing down. Fourth, are these
trends different for much older articles.
  To answer these questions, we studied citations from articles published in
1990-2013. We computed the fraction of citations to older articles from
articles published each year as the measure of impact. We considered articles
that were published at least 10 years before the citing article as older
articles. We computed these numbers for 261 subject categories and 9 broad
areas of research. Finally, we repeated the computation for two other
definitions of older articles, 15 years and older and 20 years and older.
  There are three conclusions from our study. First, the impact of older
articles has grown substantially over 1990-2013. In 2013, 36% of citations were
to articles that are at least 10 years old; this fraction has grown 28% since
1990. The fraction of older citations increased over 1990-2013 for 7 out of 9
broad areas and 231 out of 261 subject categories. Second, the increase over
the second half (2002-2013) was double the increase in the first half
(1990-2001).
  Third, the trend of a growing impact of older articles also holds for even
older articles. In 2013, 21% of citations were to articles >= 15 years old with
an increase of 30% since 1990 and 13% of citations were to articles >= 20 years
old with an increase of 36%.
  Now that finding and reading relevant older articles is about as easy as
finding and reading recently published articles, significant advances aren't
getting lost on the shelves and are influencing work worldwide for years after.
"
1209,"The ""Tournaments"" Metaphor in Citation Impact Studies: Power-Weakness
  Ratios (PWR) as a Journal Indicator","  Ramanujacharyulu's (1964) Power-Weakness Ratio (PWR) measures impact by
recursively multiplying the citation matrix by itself until convergence is
reached in both the cited and citing dimensions; the quotient of these values
is defined as PWR, whereby ""cited"" is considered as power and ""citing"" as
weakness. Analytically, PWR is an attractive candidate for measuring journal
impact because of its symmetrical handling of the rows and columns in the
asymmetrical citation matrix, its recursive algorithm, and its mathematical
elegance. In this study, PWR is discussed and critically assessed in relation
to other size-independent recursive metrics. A test using the set of 83
journals in ""information and library science"" (according to the Web-of-Science
categorization) converged, but did not provide interpretable results. Further
decomposition of this set into homogeneous sub-graphs shows that--like most
other journal indicators--PWR can perhaps be used within homogeneous sets, but
not across citation communities.
"
1210,"Power-law distributions, the h-index, and Google Scholar (GS) citations:
  a test of their relationship with economics Nobelists","  This paper presents proof that Google Scholar (GS) can construct documentary
sets relevant for evaluating researchers' works. Nobelists in economics were
the researchers under analysis, and two types of tests of the GS cites to their
works were performed: distributional and semantic. Distributional tests found
that the GS cites to the laureates' works conformed to the power-law model with
an asymptote or ""tail"" conterminous with their h-index demarcating their core
oeuvre, validating both GS and the h-index. Semantic tests revealed that their
works highest in GS cites were on topics for which they were awarded the prize.
"
1211,"Science and Ethnicity: How Ethnicities Shape the Evolution of Computer
  Science Research Community","  Globalization and the world wide web has resulted in academia and science
being an international and multicultural community forged by researchers and
scientists with different ethnicities. How ethnicity shapes the evolution of
membership, status and interactions of the scientific community, however, is
not well understood. This is due to the difficulty of ethnicity identification
at the large scale. We use name ethnicity classification as an indicator of
ethnicity. Based on automatic name ethnicity classification of 1.7+ million
authors gathered from Web, the name ethnicity of computer science scholars is
investigated by population size, publication contribution and collaboration
strength. By showing the evolution of name ethnicity from 1936 to 2010, we
discover that ethnicity diversity has increased significantly over time and
that different research communities in certain publication venues have
different ethnicity compositions. We notice a clear rise in the number of Asian
name ethnicities in papers. Their fraction of publication contribution
increases from approximately 10% to near 50% from 1970 to 2010. We also find
that name ethnicity acts as a homophily factor on coauthor networks, shaping
the formation of coauthorship as well as evolution of research communities.
"
1212,"Bibliometric Indicators for Publishers: Data processing, indicators and
  interpretation","  Here we describe the Bibliometric Indicators for Publishers Project, an
initiative undertaken by EC3Metrics SL for the analysis and development of
indicators based on books and book chapters. Its goal is to study and analyze
the publication and citation patterns of books and book chapters considering
academic publishers as the unit of analysis. It aims at developing new
methodologies and indicators that can better capture and define the research
impact of publishers. It is an on-going project in which data sources and
indicators are tested. We consider academic publishers as an analogy of
journals, focusing on them as the unit of analysis. In this working paper we
present the http://bipublishers.es website where all findings derived from the
project are displayed. We describe the data retrieval and normalization process
and we show the main results. A total 482,470 records have been retrieved and
processed, identifying 342 publishers from which 254 have been analyzed. Then
six indicators have been calculated for each publisher for four fields and 38
disciplines and displayed.
"
1213,Scientometrics and Information Retrieval - weak-links revitalized,"  This special issue brings together eight papers from experts of communities
which often have been perceived as different once: bibliometrics,
scientometrics and informetrics on the one side and information retrieval on
the other. The idea of this special issue started at the workshop ""Combining
Bibliometrics and Information Retrieval"" held at the 14th International
Conference of Scientometrics and Informetrics, Vienna, July 14-19, 2013. Our
motivation as guest editors started from the observation that main discourses
in both fields are different, that communities are only partly overlapping and
from the belief that a knowledge transfer would be profitable for both sides.
"
1214,Intriguing Trends in Nuclear Physics Articles Authorship,"  The increase in authorship of nuclear physics publications has been
investigated using the large statistical samples. This has been accomplished
with nuclear data mining of nuclear science references (NSR) and experimental
nuclear reaction (EXFOR) databases. The results of this study will be discussed
and conclusions will be given.
"
1215,"Is there any measurable benefit in publishing preprints in the arXiv
  section Quantitative Biology?","  A public preprint server such as arXiv allows authors to publish their
manuscripts before submitting them to journals for peer review. It offers the
chance to establish priority by making the results available upon completion.
This article presents the arXiv section Quantitative Biology and investigates
the advantages of preprint publications in terms of reception, which can be
measured by means of citations. This paper focuses on the publication and
citation delay, citation counts and the authors publishing their e-prints on
arXiv. Moreover, the paper discusses the benefit for scientists as well as
publishers. The results that are based on 12 selected journals show that
submitting preprints to arXiv has become more common in the past few years, but
the number of papers submitted to Quantitative Biology is still small and
represents only a fraction of the total research output in biology. An immense
advantage of arXiv is to overcome the long publication delay resulting from
peer review. Although preprints are visible prior to the officially published
articles, a significant citation advantage was only found for the Journal of
Theoretical Biology.
"
1216,"Predicting results of the Research Excellence Framework using
  departmental $h$-Index","  We compare estimates for past institutional research performances coming from
two bibliometric indicators to the results of the UK's Research Assessment
Exercise which last took place in 2008. We demonstrate that a version of the
departmental h-index is better correlated with the actual results of that
peer-review exercise than a competing metric known as the normalised
citation-based indicator. We then determine the corresponding h-indices for
2008-2013, the period examined in the UK's Research Excellence Framework (REF)
2014. We place herewith the resulting predictions on the arXiv in advance of
the REF results being published (December 2014). These may be considered as
unbiased predictions of relative performances in that exercise. We will revisit
this paper after the REF results are available and comment on the reliability
or otherwise of these bibliometrics as compared with peer review.
"
1217,Astrophysics Source Code Library Enhancements,"  The Astrophysics Source Code Library (ASCL; ascl.net) is a free online
registry of codes used in astronomy research; it currently contains over 900
codes and is indexed by ADS. The ASCL has recently moved a new infrastructure
into production. The new site provides a true database for the code entries and
integrates the WordPress news and information pages and the discussion forum
into one site. Previous capabilities are retained and permalinks to ascl.net
continue to work. This improvement offers more functionality and flexibility
than the previous site, is easier to maintain, and offers new possibilities for
collaboration. This presentation covers these recent changes to the ASCL.
"
1218,"Public Domain Rank: Identifying Notable Individuals with the Wisdom of
  the Crowd","  Identifying literary, scientific, and technical works of enduring interest is
challenging. Few are able to name significant works across more than a handful
of domains or languages. This paper introduces an automatic method for
identifying authors of notable works throughout history. Notability is defined
using the record of which works volunteers have made available in public domain
digital editions. A significant benefit of this bottom-up approach is that it
also provides a novel and reproducible index of notability for all individuals
with Wikipedia pages. The method promises to supplement the work of cultural
organizations and institutions seeking to publicize the availability of notable
works and prioritize works for preservation and digitization.
"
1219,"Publishing without Publishers: a Decentralized Approach to
  Dissemination, Retrieval, and Archiving of Data","  Making available and archiving scientific results is for the most part still
considered the task of classical publishing companies, despite the fact that
classical forms of publishing centered around printed narrative articles no
longer seem well-suited in the digital age. In particular, there exist
currently no efficient, reliable, and agreed-upon methods for publishing
scientific datasets, which have become increasingly important for science. Here
we propose to design scientific data publishing as a Web-based bottom-up
process, without top-down control of central authorities such as publishing
companies. Based on a novel combination of existing concepts and technologies,
we present a server network to decentrally store and archive data in the form
of nanopublications, an RDF-based format to represent scientific data. We show
how this approach allows researchers to publish, retrieve, verify, and
recombine datasets of nanopublications in a reliable and trustworthy manner,
and we argue that this architecture could be used for the Semantic Web in
general. Evaluation of the current small network shows that this system is
efficient and reliable.
"
1220,"Can Technology Life-Cycles Be Indicated by Diversity in Patent
  Classifications? The crucial role of variety","  In a previous study of patent classifications in nine material technologies
for photovoltaic cells, Leydesdorff et al. (2015) reported cyclical patterns in
the longitudinal development of Rao-Stirling diversity. We suggested that these
cyclical patterns can be used to indicate technological life-cycles. Upon
decomposition, however, the cycles are exclusively due to increases and
decreases in the variety of the classifications, and not to disparity or
technological distance, measured as (1 - cosine). A single frequency component
can accordingly be shown in the periodogram. Furthermore, the cyclical patterns
are associated with the numbers of inventors in the respective technologies.
Sometimes increased variety leads to a boost in the number of inventors, but in
early phases--when the technology is still under constructio--it can also be
the other way round. Since the development of the cycles thus seems independent
of technological distances among the patents, the visualization in terms of
patent maps can be considered as addressing an analytically different set of
research questions.
"
1221,Correlation of Scholarly Networks and Social Networks,"  In previous studies, much attention from multidisciplinary fields has been
devoted to understand the mechanism of underlying scholarly networks including
bibliographic networks, citation networks and co-citation networks.
Particularly focusing on networks constructed by means of either authors
affinities or the mutual content. Missing a valuable dimension of network,
which is an audience scholarly paper. We aim at this paper to assess the impact
that social networks and media can have on scholarly papers. We also examine
the process of information flow in such networks. We also mention some observa-
tions of attractive incidents that our proposed network model revealed.
"
1222,"Inequality and cumulative advantage in science careers: a case study of
  high-impact journals","  Analyzing a large data set of publications drawn from the most competitive
journals in the natural and social sciences we show that research careers
exhibit the broad distributions of individual achievement characteristic of
systems in which cumulative advantage plays a key role. While most researchers
are personally aware of the competition implicit in the publication process,
little is known about the levels of inequality at the level of individual
researchers. We analyzed both productivity and impact measures for a large set
of researchers publishing in high-impact journals. For each researcher cohort
we calculated Gini inequality coefficients, with average Gini values around
0.48 for total publications and 0.73 for total citations. For perspective,
these observed values are well in excess of the inequality levels observed for
personal income in developing countries. Investigating possible sources of this
inequality, we identify two potential mechanisms that act at the level of the
individual that may play defining roles in the emergence of the broad
productivity and impact distributions found in science. First, we show that the
average time interval between a researcher's successive publications in top
journals decreases with each subsequent publication. Second, after controlling
for the time dependent features of citation distributions, we compare the
citation impact of subsequent publications within a researcher's publication
record. We find that as researchers continue to publish in top journals, there
is more likely to be a decreasing trend in the relative citation impact with
each subsequent publication. This pattern highlights the difficulty of
repeatedly publishing high-impact research and the intriguing possibility that
confirmation bias plays a role in the evaluation of scientific careers.
"
1223,"From orphan works, a new role of libraries for the public domain and
  public interest (Dalle opere orfane, un nuovo ruolo delle biblioteche per il
  pubblico dominio e l'utilit\`a sociale)","  Summarising the new orphan works law of Italy, we show how it makes the
public interest prevail and allows libraries and other beneficiaries to improve
their services. We then argue that such services are part of their mission
towards the public domain and are a first step for its complete accomplishment,
by the work of each and a reform of european copyright. Failing that, European
culture will disappear.
  --
  Sintetizzando le nuove norme sulle opere orfane, mostriamo come esse
affermino la prevalenza dell'interesse pubblico e consentano a biblioteche e
altri enti beneficiari di migliorare i propri servizi. Sosteniamo quindi che
questi si inquadrano nella loro missione nei confronti del pubblico dominio e
sono un primo passo per la sua completa realizzazione, mediante il lavoro di
ciascuno e la riforma del diritto d'autore europeo. In caso contrario, la
cultura europea sparir\`a.
"
1224,"Continuous, Dynamic and Comprehensive Article-Level Evaluation of
  Scientific Literature","  It is time to make changes to the current research evaluation system, which
is built on the journal selection. In this study, we propose the idea of
continuous, dynamic and comprehensive article-level-evaluation based on
article-level-metrics. Different kinds of metrics are integrated into a
comprehensive indicator, which could quantify both the academic and societal
impact of the article. At different phases after the publication, the weights
of different metrics are dynamically adjusted to mediate the long term and
short term impact of the paper. Using the sample data, we make empirical study
of the article-level-evaluation method.
"
1225,Quality versus quantity in scientific impact,"  Citation metrics are becoming pervasive in the quantitative evaluation of
scholars, journals and institutions. More then ever before, hiring, promotion,
and funding decisions rely on a variety of impact metrics that cannot
disentangle quality from quantity of scientific output, and are biased by
factors such as discipline and academic age. Biases affecting the evaluation of
single papers are compounded when one aggregates citation-based metrics across
an entire publication record. It is not trivial to compare the quality of two
scholars that during their careers have published at different rates in
different disciplines in different periods of time. We propose a novel solution
based on the generation of a statistical baseline specifically tailored on the
academic profile of each researcher. Our method can decouple the roles of
quantity and quality of publications to explain how a certain level of impact
is achieved. The method is flexible enough to allow for the evaluation of, and
fair comparison among, arbitrary collections of papers --- scholar publication
records, journals, and entire institutions; and can be extended to
simultaneously suppresses any source of bias. We show that our method can
capture the quality of the work of Nobel laureates irrespective of number of
publications, academic age, and discipline, even when traditional metrics
indicate low impact in absolute terms. We further apply our methodology to
almost a million scholars and over six thousand journals to measure the impact
that cannot be explained by the volume of publications alone.
"
1226,"Paradigm shifts. Part I. Collagen. Confirming and complementing the work
  of Henry Small","  The paradigm shift in collagen research during the early 1970s marked by the
discovery of the collagen precursor molecule procollagen was traced using
co-citation analysis and title word frequency determination, confirming
previous work performed by Henry Small.
"
1227,"Paradigm shifts. Part II. Reverse Transcriptase. Analysis of reference
  stability and word frequencies","  The reverse transcription paradigm shift in RNA tumor virus research marked
by the discovery of the reverse transcriptase in 1970 was traced using
co-citation and title word frequency analysis. It is shown that this event is
associated with a break in citation patterns and the occurrence of previously
unknown technical terms.
"
1228,Patterns of Text Reuse in a Scientific Corpus,"  We consider the incidence of text ""reuse"" by researchers, via a systematic
pairwise comparison of the text content of all articles deposited to arXiv.org
from 1991--2012. We measure the global frequencies of three classes of text
reuse, and measure how chronic text reuse is distributed among authors in the
dataset. We infer a baseline for accepted practice, perhaps surprisingly
permissive compared with other societal contexts, and a clearly delineated set
of aberrant authors. We find a negative correlation between the amount of
reused text in an article and its influence, as measured by subsequent
citations. Finally, we consider the distribution of countries of origin of
articles containing large amounts of reused text.
"
1229,Quantitative Analysis of the Italian National Scientific Qualification,"  The Italian National Scientific Qualification (ASN) was introduced in 2010 as
part of a major reform of the national university system. Under the new
regulation, the scientific qualification for a specific role (associate or full
professor) and field of study is required to apply to a permanent professor
position. The ASN is peculiar since it makes use of bibliometric indicators
with associated thresholds as one of the parameters used to assess applicants.
Overall, more than 59000 applications were submitted, and the results have been
made publicly available for a short period of time, including the values of the
quantitative indicators for each applicant. The availability of this wealth of
information provides an opportunity to draw a fairly detailed picture of a
nation-wide evaluation exercise, and to study the impact of the bibliometric
indicators on the qualification results. In this paper we provide a first
account of the Italian ASN from a quantitative point of view. We show that
significant differences exist among scientific disciplines, in particular with
respect to the fraction of qualified applicants, that can not be easily
explained. Furthermore, we describe some issues related to the definition and
use of the bibliometric indicators and thresholds. Our analysis aims at drawing
attention to potential problems that should be addressed by decision-makers in
future ASN rounds.
"
1230,Will This Paper Increase Your h-index? Scientific Impact Prediction,"  Scientific impact plays a central role in the evaluation of the output of
scholars, departments, and institutions. A widely used measure of scientific
impact is citations, with a growing body of literature focused on predicting
the number of citations obtained by any given publication. The effectiveness of
such predictions, however, is fundamentally limited by the power-law
distribution of citations, whereby publications with few citations are
extremely common and publications with many citations are relatively rare.
Given this limitation, in this work we instead address a related question asked
by many academic researchers in the course of writing a paper, namely: ""Will
this paper increase my h-index?"" Using a real academic dataset with over 1.7
million authors, 2 million papers, and 8 million citation relationships from
the premier online academic service ArnetMiner, we formalize a novel scientific
impact prediction problem to examine several factors that can drive a paper to
increase the primary author's h-index. We find that the researcher's authority
on the publication topic and the venue in which the paper is published are
crucial factors to the increase of the primary author's h-index, while the
topic popularity and the co-authors' h-indices are of surprisingly little
relevance. By leveraging relevant factors, we find a greater than 87.5%
potential predictability for whether a paper will contribute to an author's
h-index within five years. As a further experiment, we generate a
self-prediction for this paper, estimating that there is a 76% probability that
it will contribute to the h-index of the co-author with the highest current
h-index in five years. We conclude that our findings on the quantification of
scientific impact can help researchers to expand their influence and more
effectively leverage their position of ""standing on the shoulders of giants.""
"
1231,"Restricting the h-index to a citation time window: A case study of a
  timed Hirsch index","  The h-index has been shown to increase in many cases mostly because of
citations to rather old publications. This inertia can be circumvented by
restricting the evaluation to a citation time window. Here I report results of
an empirical study analyzing the evolution of the thus defined timed h-index in
dependence on the length of the citation time window.
"
1232,"H-Index Manipulation by Merging Articles: Models, Theory, and
  Experiments","  An author's profile on Google Scholar consists of indexed articles and
associated data, such as the number of citations and the H-index. The author is
allowed to merge articles; this may affect the H-index. We analyze the
(parameterized) computational complexity of maximizing the H-index using
article merges. Herein, to model realistic manipulation scenarios, we define a
compatibility graph whose edges correspond to plausible merges. Moreover, we
consider several different measures for computing the citation count of a
merged article. For the measure used by Google Scholar, we give an algorithm
that maximizes the H-index in linear time if the compatibility graph has
constant-size connected components. In contrast, if we allow to merge arbitrary
articles (that is, for compatibility graphs that are cliques), then already
increasing the H-index by one is NP-hard. Experiments on Google Scholar
profiles of AI researchers show that the H-index can be manipulated
substantially only if one merges articles with highly dissimilar titles.
"
1233,Annotation as a New Paradigm in Research Archiving,"  We outline a paradigm to preserve results of digital scholarship, whether
they are query results, feature values, or topic assignments. This paradigm is
characterized by using annotations as multifunctional carriers and making them
portable. The testing grounds we have chosen are two significant enterprises,
one in the history of science, and one in Hebrew scholarship. The first one
(CKCC) focuses on the results of a project where a Dutch consortium of
universities, research institutes, and cultural heritage institutions
experimented for 4 years with language techniques and topic modeling methods
with the aim to analyze the emergence of scholarly debates. The data: a complex
set of about 20.000 letters. The second one (DTHB) is a multi-year effort to
express the linguistic features of the Hebrew bible in a text database, which
is still growing in detail and sophistication. Versions of this database are
packaged in commercial bible study software. We state that the results of these
forms of scholarship require new knowledge management and archive practices.
Only when researchers can build efficiently on each other's (intermediate)
results, they can achieve the aggregations of quality data by which new
questions can be answered, and hidden patterns visualized. Archives are
required to find a balance between preserving authoritative versions of sources
and supporting collaborative efforts in digital scholarship. Annotations are
promising vehicles for preserving and reusing research results. Keywords
annotation, portability, archiving, queries, features, topics, keywords,
Republic of Letters, Hebrew text databases.
"
1234,Visual Concept Ontology for Image Annotations,"  In spite of the development of content-based data management, text-based
searching remains the primary means of multimedia retrieval in many areas.
Automatic creation of text metadata is thus a crucial tool for increasing the
findability of multimedia objects. Search-based annotation tools try to provide
content-descriptive keywords by exploiting web data, which are easily available
but unstructured and noisy. Such data need to be analyzed with the help of
semantic resources that provide knowledge about objects and relationships in a
given domain. In this paper, we focus on the task of general-purpose image
annotation and present the VCO, a new ontology of visual concepts developed as
a part of image annotation framework. The ontology is linked with the WordNet
lexical database, so the annotation tools can easily integrate information from
both these resources.
"
1235,Educational Technology as Seen Through the Eyes of the Readers,"  In this paper, I present the evaluation of a novel knowledge domain
visualization of educational technology. The interactive visualization is based
on readership patterns in the online reference management system Mendeley. It
comprises of 13 topic areas, spanning psychological, pedagogical, and
methodological foundations, learning methods and technologies, and social and
technological developments. The visualization was evaluated with (1) a
qualitative comparison to knowledge domain visualizations based on citations,
and (2) expert interviews. The results show that the co-readership
visualization is a recent representation of pedagogical and psychological
research in educational technology. Furthermore, the co-readership analysis
covers more areas than comparable visualizations based on co-citation patterns.
Areas related to computer science, however, are missing from the co-readership
visualization and more research is needed to explore the interpretations of
size and placement of research areas on the map.
"
1236,"Knowledge Integration and Diffusion: Measures and Mapping of Diversity
  and Coherence","  I present a framework based on the concepts of diversity and coherence for
the analysis of knowledge integration and diffusion. Visualisations that help
understand insights gained are also introduced. The key novelty offered by this
framework compared to previous approaches is the inclusion of cognitive
distance (or proximity) between the categories that characterise the body of
knowledge under study. I briefly discuss the different methods to map the
cognitive dimension.
"
1237,"On the relationship between interdisciplinarity and impact: different
  modalities of interdisciplinarity lead to different types of impact","  There is increasing interest among funding agencies to understand how they
can best contribute to enhancing the socio-economic impact of research.
Interdisciplinarity is often presented as a research mode that can facilitate
impact but there exist a limited number of analytical studies that have
attempted to examine whether or how interdisciplinarity can affect the societal
relevance of research. We investigate fifteen Social Sciences research
investments in the UK to examine how they have achieved impact. We analyse
research drivers, cognitive distances, degree of integration, collaborative
practices, stakeholder engagement and the type of impact generated. The
analysis suggests that interdisciplinarity cannot be associated with a single
type of impact mechanism. Also, interdisciplinarity is neither a sufficient nor
a necessary condition for achieving societal relevance and impact. However, we
identify a specific modality -- ""long-range"" interdisciplinarity, which appears
more likely to be associated with societal impact because of its focused
problem-orientation and its strong interaction with stakeholders.
"
1238,"Proceedings Scholar Metrics: H Index of proceedings on Computer Science,
  Electrical & Electronic Engineering, and Communications according to Google
  Scholar Metrics (2009-2013)","  The objective of this report is to present a list of proceedings
(conferences, workshops, symposia, meetings) in the areas of Computer Science,
Electrical & Electronic Engineering, and Communications covered by Google
Scholar Metrics and ranked according to their h-index. Google Scholar Metrics
only displays publications that have published at least 100 papers and have
received at least one citation in the last five years (2009-2013). The searches
were conducted between the 15th and 22nd of December, 2014. A total of 1208
proceedings have been identified
"
1239,"Return on citation: a consistent metric to evaluate papers, journals and
  researchers","  Evaluating and comparing the academic performance of a journal, a researcher
or a single paper has long remained a critical, necessary but also
controversial issue. Most of existing metrics invalidate comparison across
different fields of science or even between different types of papers in the
same field. This paper proposes a new metric, called return on citation (ROC),
which is simply a citation ratio but applies to evaluating the paper, the
journal and the researcher in a consistent way, allowing comparison across
different fields of science and between different types of papers and
discouraging unnecessary and coercive/self-citation.
"
1240,"The Effects of Research Level and Article Type on the Differences
  between Citation Metrics and F1000 Recommendations","  F1000 recommendations have been validated as a potential data source for
research evaluation, but reasons for differences between F1000 Article Factor
(FFa scores) and citations remain to be explored. By linking 28254 publications
in F1000 to citations in Scopus, we investigated the effect of research level
and article type on the internal consistency of assessments based on citations
and FFa scores. It turns out that research level has little impact, while
article type has big effect on the differences. These two measures are
significantly different for two groups: non-primary research or evidence-based
research publications are more highly cited rather than highly recommended,
however, translational research or transformative research publications are
more highly recommended by faculty members but gather relatively lower
citations. This can be expected because citation activities are usually
practiced by academic authors while the potential for scientific revolutions
and the suitability for clinical practice of an article should be investigated
from the practitioners' points of view. We conclude with a policy relevant
recommendation that the application of bibliometric approaches in research
evaluation procedures should include the proportion of three types of
publications: evidence-based research, transformative research, and
translational research. The latter two types are more suitable to be assessed
through peer review.
"
1241,Experimental Research Data Quality In Materials Science,"  In materials sciences, a large amount of research data is generated through a
broad spectrum of different experiments. As of today, experimental research
data including meta-data in materials science is often stored decentralized by
the researcher(s) conducting the experiments without generally accepted
standards on what and how to store data. The conducted research and experiments
often involve a considerable investment from public funding agencies that
desire the results to be made available in order to increase their impact. In
order to achieve the goal of citable and (openly) accessible materials science
experimental research data in the future, not only an adequate infrastructure
needs to be established but the question of how to measure the quality of the
experimental research data also to be addressed. In this publication, the
authors identify requirements and challenges towards a systematic methodology
to measure experimental research data quality prior to publication and derive
different approaches on that basis. These methods are critically discussed and
assessed by their contribution and limitations towards the set goals.
Concluding, a combination of selected methods is presented as a systematic,
functional and practical quality measurement and assurance approach for
experimental research data in materials science with the goal of supporting the
accessibility and dissemination of existing data sets.
"
1242,The Hebrew Bible as Data: Laboratory - Sharing - Experiences,"  The systematic study of ancient texts including their production,
transmission and interpretation is greatly aided by the digital methods that
started taking off in the 1970s. But how is that research in turn transmitted
to new generations of researchers? We tell a story of Bible and computer across
the decades and then point out the current challenges: (1) finding a stable
data representation for changing methods of computation; (2) sharing results in
inter- and intra-disciplinary ways, for reproducibility and
cross-fertilization. We report recent developments in meeting these challenges.
The scene is the text database of the Hebrew Bible, constructed by the Eep
Talstra Centre for Bible and Computer (ETCBC), which is still growing in detail
and sophistication. We show how a subtle mix of computational ingredients
enable scholars to research the transmission and interpretation of the Hebrew
Bible in new ways: (1) a standard data format, Linguistic Annotation Framework
(LAF); (2) the methods of scientific computing, made accessible by
(interactive) Python and its associated ecosystem. Additionally, we show how
these efforts have culminated in the construction of a new, publicly accessible
search engine SHEBANQ, where the text of the Hebrew Bible and its underlying
data can be queried in a simple, yet powerful query language MQL, and where
those queries can be saved and shared.
"
1243,Reviving the past: the growth of citations to old documents,"  In this Digest we review a recent study released by the Google Scholar team
on the apparently increasing fraction of citations to old articles from studies
published in the last 24 years (1990-2013). First, we describe the main
findings of their article. Secondly, we conduct an analogue study, using a
different data source as well as different measures which throw very similar
results, thus confirming the phenomenon. Lastly, we discuss the possible causes
of this phenomenon.
"
1244,"Assessing the true role of coauthors in the h-index measure of an author
  scientific impact","  A method based on the classical principal component analysis leads to
demonstrate that the role of co-authors should give a h-index measure to a
group leader higher than usually accepted. The method rather easily gives what
is usually searched for, i.e. an estimate of the role (or ""weight"") of
co-authors, as the additional value to an author papers' popularity. The
construction of the co-authorship popularity H-matrix is exemplified and the
role of eigenvalues and the main eigenvector component are discussed. An
example illustrates the points and serves as the basis for suggesting a
generally practical application of the concept.
"
1245,"Bibliometric-enhanced Information Retrieval: 2nd International BIR
  Workshop","  This workshop brings together experts of communities which often have been
perceived as different once: bibliometrics / scientometrics / informetrics on
the one side and information retrieval on the other. Our motivation as
organizers of the workshop started from the observation that main discourses in
both fields are different, that communities are only partly overlapping and
from the belief that a knowledge transfer would be profitable for both sides.
Bibliometric techniques are not yet widely used to enhance retrieval processes
in digital libraries, although they offer value-added effects for users. On the
other side, more and more information professionals, working in libraries and
archives are confronted with applying bibliometric techniques in their
services. This way knowledge exchange becomes more urgent. The first workshop
set the research agenda, by introducing in each other methods, reporting about
current research problems and brainstorming about common interests. This
follow-up workshop continues the overall communication, but also puts one
problem into the focus. In particular, we will explore how statistical
modelling of scholarship can improve retrieval services for specific
communities, as well as for large, cross-domain collections like Mendeley or
ResearchGate. This second BIR workshop continues to raise awareness of the
missing link between Information Retrieval (IR) and bibliometrics and
contributes to create a common ground for the incorporation of
bibliometric-enhanced services into retrieval at the scholarly search engine
interface.
"
1246,Research Data Explored: Citations versus Altmetrics,"  The study explores the citedness of research data, its distribution over time
and how it is related to the availability of a DOI (Digital Object Identifier)
in Thomson Reuters' DCI (Data Citation Index). We investigate if cited research
data ""impact"" the (social) web, reflected by altmetrics scores, and if there is
any relationship between the number of citations and the sum of altmetrics
scores from various social media-platforms. Three tools are used to collect and
compare altmetrics scores, i.e. PlumX, ImpactStory, and Altmetric.com. In terms
of coverage, PlumX is the most helpful altmetrics tool. While research data
remain mostly uncited (about 85%), there has been a growing trend in citing
data sets published since 2007. Surprisingly, the percentage of the number of
cited research data with a DOI in DCI has decreased in the last years. Only
nine repositories account for research data with DOIs and two or more
citations. The number of cited research data with altmetrics scores is even
lower (4 to 9%) but shows a higher coverage of research data from the last
decade. However, no correlation between the number of citations and the total
number of altmetrics scores is observable. Certain data types (i.e. survey,
aggregate data, and sequence data) are more often cited and receive higher
altmetrics scores.
"
1247,"Field-normalized citation impact indicators and the choice of an
  appropriate counting method","  Bibliometric studies often rely on field-normalized citation impact
indicators in order to make comparisons between scientific fields. We discuss
the connection between field normalization and the choice of a counting method
for handling publications with multiple co-authors. Our focus is on the choice
between full counting and fractional counting. Based on an extensive
theoretical and empirical analysis, we argue that properly field-normalized
results cannot be obtained when full counting is used. Fractional counting does
provide results that are properly field normalized. We therefore recommend the
use of fractional counting in bibliometric studies that require field
normalization, especially in studies at the level of countries and research
organizations. We also compare different variants of fractional counting. In
general, it seems best to use either the author-level or the address-level
variant of fractional counting.
"
1248,"CITEX: A new citation index to measure the relative importance of
  authors and papers in scientific publications","  Evaluating the performance of researchers and measuring the impact of papers
written by scientists is the main objective of citation analysis. Various
indices and metrics have been proposed for this. In this paper, we propose a
new citation index CITEX, which gives normalized scores to authors and papers
to determine their rankings. To the best of our knowledge, this is the first
citation index which simultaneously assigns scores to both authors and papers.
Using these scores, we can get an objective measure of the reputation of an
author and the impact of a paper.
  We model this problem as an iterative computation on a publication graph,
whose vertices are authors and papers, and whose edges indicate which author
has written which paper. We prove that this iterative computation converges in
the limit, by using a powerful theorem from linear algebra. We run this
algorithm on several examples, and find that the author and paper scores match
closely with what is suggested by our intuition. The algorithm is theoretically
sound and runs very fast in practice. We compare this index with several
existing metrics and find that CITEX gives far more accurate scores compared to
the traditional metrics.
"
1249,"Uma an\'alise bibliom\'etrica do Congresso Nacional de Bibliotec\'arios,
  Arquivistas e Documentalistas (1985-2012)","  This article is the first bibliometric analysis of the 708 lectures published
by The Librarians and Archivists National Congress between 1985 and 2012,
having been developed markers for production, productivity, institutional
origin and thematic analysis, in a quantitative, relational and diachronic
perspective. Its results show a dynamic congress, essentially national and
professional, with a strong majority of individual authorships, even with the
recent growth of the ratio of collaborations. In its thematic approach,
emphasis is given to public services of information, with the greatest focus
being on libraries, while still giving relevance to reflections on professional
and academic training in the area of Information Sciences, and also following
the most recent technological developments.
"
1250,"Thematic Identification of 'Little Science': Trends in Portuguese IS&LS
  Literature by Controlled Vocabulary and Co-Word Analysis","  This study presents an overview of IS&LS thematic trends in Portugal between
2001 and 2012. The results were obtained by means of an analysis, using
expeditious qualitative and quantitative techniques, of the bibliographic
records of proceedings papers identified during this period. These records were
processed using two techniques: a manual subject classification and an
automated co-word analysis of the Author-Assigned Keywords. From this we
designed cluster and co-occurrence maps, using the VOSviewer and the Pajek
software packages. The results indicated an accentuated dynamism in the
thematic evolution of this documental corpus, apart from revealing a
significant difference among the themes transmitted in nationally and
internationally visible production.
"
1251,A Review of Theory and Practice in Scientometrics,"  Scientometrics is the study of the quantitative aspects of the process of
science as a communication system. It is centrally, but not only, concerned
with the analysis of citations in the academic literature. In recent years it
has come to play a major role in the measurement and evaluation of research
performance. In this review we consider: the historical development of
scientometrics, sources of citation data, citation metrics and the ""laws"" of
scientometrics, normalisation, journal impact factors and other journal
metrics, visualising and mapping science, evaluation and policy, and future
developments.
"
1252,Analyzing data citation practices using the Data Citation Index,"  We present an analysis of data citation practices based on the Data Citation
Index from Thomson Reuters. This database launched in 2012 aims to link data
sets and data studies with citations received from the other citation indexes.
The DCI harvests citations to research data from papers indexed in the Web of
Science. It relies on the information provided by the data repository as data
citation practices are inconsistent or inexistent in many cases. The findings
of this study show that data citation practices are far from common in most
research fields. Some differences have been reported on the way researchers
cite data: while in the areas of Science and Engineering and Technology data
sets were the most cited, in Social Sciences and Arts and Humanities data
studies play a greater role. A total of 88.1 percent of the records have
received no citation, but some repositories show very low uncitedness rates.
Although data citation practices are rare in most fields, they have expanded in
disciplines such as crystallography and genomics. We conclude by emphasizing
the role that the DCI could play in encouraging the consistent, standardized
citation of research data; a role that would enhance their value as a means of
following the research process from data collection to publication.
"
1253,Measuring academic influence: Not all citations are equal,"  The importance of a research article is routinely measured by counting how
many times it has been cited. However, treating all citations with equal weight
ignores the wide variety of functions that citations perform. We want to
automatically identify the subset of references in a bibliography that have a
central academic influence on the citing paper. For this purpose, we examine
the effectiveness of a variety of features for determining the academic
influence of a citation. By asking authors to identify the key references in
their own work, we created a data set in which citations were labeled according
to their academic influence. Using automatic feature selection with supervised
machine learning, we found a model for predicting academic influence that
achieves good performance on this data set using only four features. The best
features, among those we evaluated, were those based on the number of times a
reference is mentioned in the body of a citing paper. The performance of these
features inspired us to design an influence-primed h-index (the hip-index).
Unlike the conventional h-index, it weights citations by how many times a
reference is mentioned. According to our experiments, the hip-index is a better
indicator of researcher performance than the conventional h-index.
"
1254,Can Science and Technology Capacity be Measured?,"  The ability of a nation to participate in the global knowledge economy
depends to some extent on its capacities in science and technology. In an
effort to assess the capacity of different countries in science and technology,
this article updates a classification scheme developed by RAND to measure
science and technology capacity for 150 countries of the world.
"
1255,"Predicting Results of the Research Excellence Framework using
  Departmental h-Index -- Revisited","  We revisit our recent study [Predicting results of the Research Excellence
Framework using departmental h-index, Scientometrics, 2014, 1-16;
arXiv:1411.1996] in which we attempted to predict outcomes of the UK's Research
Excellence Framework (REF~2014) using the so-called departmental $h$-index.
Here we report that our predictions failed to anticipate with any accuracy
either overall REF outcomes or movements of individual institutions in the
rankings relative to their positions in the previous Research Assessment
Exercise (RAE~2008).
"
1256,"Can ""Hot Spots"" in the Sciences Be Mapped Using the Dynamics of
  Aggregated Journal-Journal Citation Relations?","  Using three years of the Journal Citation Reports (2011, 2012, and 2013),
indicators of transitions in 2012 (between 2011 and 2013) are studied using
methodologies based on entropy statistics. Changes can be indicated at the
level of journals using the margin totals of entropy production along the row
or column vectors, but also at the level of links among journals by importing
the transition matrices into network analysis and visualization programs (and
using community-finding algorithms). Seventy-four journals are flagged in terms
of discontinuous changes in their citations; but 3,114 journals are involved in
""hot"" links. Most of these links are embedded in a main component; 78 clusters
(containing 172 journals) are flagged as potential ""hot spots"" emerging at the
network level. An additional finding is that PLoS ONE introduced a new
communication dynamics into the database. The limitations of the methodology
are elaborated using an example. The results of the study indicate where
developments in the citation dynamics can be considered as significantly
unexpected. This can be used as heuristic information; but what a ""hot spot"" in
terms of the entropy statistics of aggregated citation relations means
substantively can be expected to vary from case to case.
"
1257,Modeling and Analysis of Scholar Mobility on Scientific Landscape,"  Scientific literature till date can be thought of as a partially revealed
landscape, where scholars continue to unveil hidden knowledge by exploring
novel research topics. How do scholars explore the scientific landscape , i.e.,
choose research topics to work on? We propose an agent-based model of topic
mobility behavior where scholars migrate across research topics on the space of
science following different strategies, seeking different utilities. We use
this model to study whether strategies widely used in current scientific
community can provide a balance between individual scientific success and the
efficiency and diversity of the whole academic society. Through extensive
simulations, we provide insights into the roles of different strategies, such
as choosing topics according to research potential or the popularity. Our model
provides a conceptual framework and a computational approach to analyze
scholars' behavior and its impact on scientific production. We also discuss how
such an agent-based modeling approach can be integrated with big real-world
scholarly data.
"
1258,"Throwing Out the Baby with the Bathwater: The Undesirable Effects of
  National Research Assessment Exercises on Research","  The evaluation of the quality of research at a national level has become
increasingly common. The UK has been at the forefront of this trend having
undertaken many assessments since 1986, the latest being the Research
Excellence Framework in 2014. The argument of this paper is that, whatever the
intended results in terms of evaluating and improving research, there have been
many, presumably unintended, results that are highly undesirable for research
and the university community more generally. We situate our analysis using
Bourdieu's theory of cultural reproduction and then focus on the peculiarities
of the 2008 RAE and the 2014 REF the rules of which allowed for, and indeed
encouraged, significant game-playing on the part of striving universities. We
conclude with practical recommendations to maintain the general intention of
research assessment without the undesirable side-effects.
"
1259,"A proposal for regularly updated review/survey articles: ""Perpetual
  Reviews""","  We advocate the publication of review/survey articles that will be updated
regularly, both in traditional journals and novel venues. We call these
""perpetual reviews."" This idea naturally builds on the dissemination and
archival capabilities present in the modern internet, and indeed perpetual
reviews exist already in some forms. Perpetual review articles allow authors to
maintain over time the relevance of non-research scholarship that requires a
significant investment of effort. Further, such reviews published in a purely
electronic format without space constraints can also permit more pedagogical
scholarship and clearer treatment of technical issues that remain obscure in a
brief treatment.
"
1260,"Editorial for the Proceedings of the Workshop Knowledge Maps and
  Information Retrieval (KMIR2014) at Digital Libraries 2014","  Knowledge maps are promising tools for visualizing the structure of
large-scale information spaces, but still far away from being applicable for
searching. The first international workshop on ""Knowledge Maps and Information
Retrieval (KMIR)"", held as part of the International Conference on Digital
Libraries 2014 in London, aimed at bringing together experts in Information
Retrieval (IR) and knowledge mapping in order to discuss the potential of
interactive knowledge maps for information seeking purposes.
"
1261,Of Matters Condensed,"  The American Physical Society (APS) March Meeting of condensed matter physics
has grown to nearly 10,000 participants, comprises 23 individual APS groups,
and even warrants its own hashtag (#apsmarch). Here we analyze the text and
data from March Meeting abstracts of the past nine years and discuss trends in
condensed matter physics over this time period. We find that in comparison to
atomic, molecular, and optical physics, condensed matter changes rapidly, and
that condensed matter appears to be moving increasingly toward subject matter
that is traditionally in materials science and engineering.
"
1262,"What makes us a community: structure, correlations, and success in
  scientific world","  We explore the statistical structure of scientific community based on
multivariate analysis of publication (or other identifiable metrics)
distribution in the author space. Here, we define community based on keywords,
i.e. projecting semantic content of the documents on predefined meanings;
however, more complex approaches based on semantic clustering of publications
are possible. Remarkably, this simple statistical analysis of publication
metadata allows understanding of internal interactions with community in
general agreement with experience acquired over decades of social interaction
within it. We further discuss potential applications of this approach for
ranking within the community, reviewer selection, and optimization of community
output.
"
1263,"A scientometric study of General Relativity and Quantum Cosmology from
  2000 to 2012","  2015 is the centennial of Einstein General Relativity. On this occasion, we
examine the General Relativity and Quantum Cosmology (GRQC) field of research
by analysing 38291 papers uploaded on the electronic archives arXiv.org from
2000 to 2012. We establish a map of the countries contributing to GRQC in 2012.
We determine the main journals publishing GRQC papers and which countries
publish in which journals. We find that more and more papers are written by
groups (instead of single) of authors with more and more international
collaborations. There are huge differences between countries. Hence Russia is
the country where most of papers are written by single authors whereas Canada
is one of the countries where the most of papers imply international
collaborations. We also study authors mobility, determining how some groups of
authors spread worldwide with time in different countries. The largest
mobilities are between USA-UK and USA-Germany. Countries attracting the most of
GRQC authors are Netherlands and Canada whereas those undergoing a brain drain
are Italy and India. There are few mobility between Europe and Asia contrarily
to mobility between USA and Asia.
"
1264,"Improving Access to Digitized Historical Newspapers with Text Mining,
  Coordinated Models, and Formative User Interface Design","  Most tools for accessing digitized historical newspapers emphasize relatively
simple search; but, as increasing numbers of digitized historical newspapers
and other historical resources become available we can consider much richer
modes of interaction with these collections. For instance, users might use
exploratory search for looking at larger issues and events such as elections
and campaigns or to get a sense of ""the texture of the city"" or ""what the city
was thinking"". To take full advantage of rich interface tools, the content of
the newspapers needs to be described systematically and accurately. Moreover,
collections of multiple newspapers need to be richly cross-indexed across
titles and even with historical resources beyond the newspapers.
"
1265,"Evaluating Open Access Paper Repository In Higher Education For Asean
  Region","  Paper repository at higher education is a collection of scientific articles
created by the academic society. This study took as many as 80 universities in
the Webometrics ranking of repositories in the Southeast Asia region. The tools
used in this research is Google for number of web page and Google Scholar for
number of document paper repository and Ahrefs for referring page, backlink and
reffering domain. The result of this study, Eprints is the most widely used
tools in higher education, as many as 37 higher educations (46,25%). Institut
Teknologi Sepuluh November got the highest score in number of web page in
Google (2.010.000), Bogor Agricultural University Scientific Repository got the
highest score for number of document paper (44.300). University of Sumatera
Utara Repository got the highest score for reffering page (82588) and backlink
(86421). Universiti Teknologi Malaysia Institutional Repository got the highest
score for reffering domain (532).
"
1266,"Structured Descriptions of Roles, Activities,and Procedures in the Roman
  Constitution","  A highly structured description of entities and events in histories can
support flexible exploration of those histories by users and, ultimately,
support richly-linked full-text digital libraries. Here, we apply the Basic
Formal Ontology (BFO) to structure a passage about the Roman Constitution from
Gibbon's Decline and Fall of the Roman Empire. Specifically, we consider the
specification of Roles such as Consuls, Activities associated with those Roles,
and Procedures for accomplishing those Activities.
"
1267,"Hyperlinks embedded in Twitter as a proxy for total external inlinks to
  international university websites","  This article analyzes Twitter as a potential alternative source of external
links for use in webometric analysis because of its capacity to embed
hyperlinks in different tweets. Given the limitations on searching Twitter's
public API, we decided to use the Topsy search engine as a source for compiling
tweets. To this end, we took a global sample of 200 universities and compiled
all the tweets with hyperlinks to any of these institutions. Further link data
was obtained from alternative sources (MajesticSEO and OpenSiteExplorer) in
order to compare the results. Thereafter, various statistical tests were
performed to determine the correlation between the indicators and the ability
to predict external links from the collected tweets. The results indicate a
high volume of tweets, although they are skewed by the presence and performance
of specific universities and countries. The data provided by Topsy correlated
significantly with all link indicators, particularly with OpenSiteExplorer
(r=0.769). Finally, prediction models do not provide optimum results because of
high error rates, which fall slightly in nonlinear models applied to specific
environments. We conclude that the use of Twitter (via Topsy) as a source of
hyperlinks to universities produces promising results due to its high
correlation with link indicators, though limited by policies and culture
regarding use and presence in social networks.
"
1268,"Network-based statistical comparison of citation topology of
  bibliographic databases","  Modern bibliographic databases provide the basis for scientific research and
its evaluation. While their content and structure differ substantially, there
exist only informal notions on their reliability. Here we compare the
topological consistency of citation networks extracted from six popular
bibliographic databases including Web of Science, CiteSeer and arXiv.org. The
networks are assessed through a rich set of local and global graph statistics.
We first reveal statistically significant inconsistencies between some of the
databases with respect to individual statistics. For example, the introduced
field bow-tie decomposition of DBLP Computer Science Bibliography substantially
differs from the rest due to the coverage of the database, while the citation
information within arXiv.org is the most exhaustive. Finally, we compare the
databases over multiple graph statistics using the critical difference diagram.
The citation topology of DBLP Computer Science Bibliography is the least
consistent with the rest, while, not surprisingly, Web of Science is
significantly more reliable from the perspective of consistency. This work can
serve either as a reference for scholars in bibliometrics and scientometrics or
a scientific evaluation guideline for governments and research agencies.
"
1269,"Journal Portfolio Analysis for Countries, Cities, and Organizations:
  Maps and Comparisons","  Using Web-of-Science data, portfolio analysis in terms of journal coverage
can be projected on a base map for units of analysis such as countries, cities,
universities, and firms. The units of analysis under study can be compared
statistically across the 10,000+ journals. The interdisciplinarity of the
portfolios is measured using Rao-Stirling diversity or Zhang et al.'s (in
press) improved measure 2D3. At the country level we find regional
differentiation (e.g., Latin-American or Asian countries), but also a major
divide between advanced and less-developed countries. Israel and Israeli cities
outperform other nations and cities in terms of diversity. Universities appear
to be specifically related to firms when a number of these units are
exploratively compared. The instrument is relatively simple and
straightforward, and one can generalize the application to any document set
retrieved from WoS. Further instruction is provided online at
http://www.leydesdorff.net/portfolio .
"
1270,"Interpreting ""altmetrics"": viewing acts on social media through the lens
  of citation and social theories","  More than 30 years after Cronin's seminal paper on ""the need for a theory of
citing"" (Cronin, 1981), the metrics community is once again in need of a new
theory, this time one for so-called ""altmetrics"". Altmetrics, short for
alternative (to citation) metrics -- and as such a misnomer -- refers to a new
group of metrics based (largely) on social media events relating to scholarly
communication. As current definitions of altmetrics are shaped and limited by
active platforms, technical possibilities, and business models of aggregators
such as Altmetric.com, ImpactStory, PLOS, and Plum Analytics, and as such
constantly changing, this work refrains from defining an umbrella term for
these very heterogeneous new metrics. Instead a framework is presented that
describes acts leading to (online) events on which the metrics are based. These
activities occur in the context of social media, such as discussing on Twitter
or saving to Mendeley, as well as downloading and citing. The framework groups
various types of acts into three categories -- accessing, appraising, and
applying -- and provides examples of actions that lead to visibility and
traceability online. To improve the understanding of the acts, which result in
online events from which metrics are collected, select citation and social
theories are used to interpret the phenomena being measured. Citation theories
are used because the new metrics based on these events are supposed to replace
or complement citations as indicators of impact. Social theories, on the other
hand, are discussed because there is an inherent social aspect to the
measurements.
"
1271,"Persistent, Global Identity for Scientists via ORCID","  Scientists have an inherent interest in claiming their contributions to the
scholarly record, but the fragmented state of identity management across the
landscape of astronomy, physics, and other fields makes highlighting the
contributions of any single individual a formidable and often frustratingly
complex task. The problem is exacerbated by the expanding variety of academic
research products and the growing footprints of large collaborations and
interdisciplinary teams. In this essay, we outline the benefits of a unique
scholarly identifier with persistent value on a global scale and we review
astronomy and physics engagement with the Open Researcher and Contributor iD
(ORCID) service as a solution.
"
1272,"Distortive Effects of Initial-Based Name Disambiguation on Measurements
  of Large-Scale Coauthorship Networks","  Scholars have often relied on name initials to resolve name ambiguities in
large-scale coauthorship network research. This approach bears the risk of
incorrectly merging or splitting author identities. The use of initial-based
disambiguation has been justified by the assumption that such errors would not
affect research findings too much. This paper tests this assumption by
analyzing coauthorship networks from five academic fields - biology, computer
science, nanoscience, neuroscience, and physics - and an interdisciplinary
journal, PNAS. Name instances in datasets of this study were disambiguated
based on heuristics gained from previous algorithmic disambiguation solutions.
We use disambiguated data as a proxy of ground-truth to test the performance of
three types of initial-based disambiguation. Our results show that
initial-based disambiguation can misrepresent statistical properties of
coauthorship networks: it deflates the number of unique authors, number of
component, average shortest paths, clustering coefficient, and assortativity,
while it inflates average productivity, density, average coauthor number per
author, and largest component size. Also, on average, more than half of top 10
productive or collaborative authors drop off the lists. Asian names were found
to account for the majority of misidentification by initial-based
disambiguation due to their common surname and given name initials.
"
1273,Bibliometrics/Citation networks,"  In addition to shaping social networks, for example, in terms of
co-authorship relations, scientific communications induce and reproduce
cognitive structures. Scientific literature is intellectually organized in
terms of disciplines and specialties; these structures are reproduced and
networked reflexively by making references to the authors, concepts and texts
embedded in these literatures. The concept of a cognitive structure was
introduced in social network analysis (SNA) in 1987 by David Krackhardt, but
the focus in SNA has hitherto been on cognition as a psychological attribute of
human agency. In bibliometrics, and in science and technology studies (STS)
more generally, socio-cognitive structures refer to intellectual organization
at the supra-individual level. This intellectual organization emerges and is
reproduced by the collectives of authors who are organized not only in terms of
inter-personal relations, but also more abstractly in terms of codes of
communication that are field-specific. Citations can serve as indicators of
this codification process.
"
1274,Author Name Disambiguation by Using Deep Neural Network,"  Author name ambiguity decreases the quality and reliability of information
retrieved from digital libraries. Existing methods have tried to solve this
problem by predefining a feature set based on expert's knowledge for a specific
dataset. In this paper, we propose a new approach which uses deep neural
network to learn features automatically from data. Additionally, we propose the
general system architecture for author name disambiguation on any dataset. In
this research, we evaluate the proposed method on a dataset containing
Vietnamese author names. The results show that this method significantly
outperforms other methods that use predefined feature set. The proposed method
achieves 99.31% in terms of accuracy. Prediction error rate decreases from
1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with
other methods that use predefined feature set (Table 3).
"
1275,"SciRecSys: A Recommendation System for Scientific Publication by
  Discovering Keyword Relationships","  In this work, we propose a new approach for discovering various relationships
among keywords over the scientific publications based on a Markov Chain model.
It is an important problem since keywords are the basic elements for
representing abstract objects such as documents, user profiles, topics and many
things else. Our model is very effective since it combines four important
factors in scientific publications: content, publicity, impact and randomness.
Particularly, a recommendation system (called SciRecSys) has been presented to
support users to efficiently find out relevant articles.
"
1276,"Coauthorship networks: A directed network approach considering the order
  and number of coauthors","  In many scientific fields, the order of coauthors on a paper conveys
information about each individual's contribution to a piece of joint work. We
argue that in prior network analyses of coauthorship networks, the information
on ordering has been insufficiently considered because ties between authors are
typically symmetrized. This is basically the same as assuming that each
co-author has contributed equally to a paper. We introduce a solution to this
problem by adopting a coauthorship credit allocation model proposed by Kim and
Diesner (2014), which in its core conceptualizes co-authoring as a directed,
weighted, and self-looped network. We test and validate our application of the
adopted framework based on a sample data of 861 authors who have published in
the journal Psychometrika. Results suggest that this novel sociometric approach
can complement traditional measures based on undirected networks and expand
insights into coauthoring patterns such as the hierarchy of collaboration among
scholars. As another form of validation, we also show how our approach
accurately detects prominent scholars in the Psychometric Society affiliated
with the journal.
"
1277,"A Reputation Economy: Results from an Empirical Survey on Academic Data
  Sharing","  Academic data sharing is a way for researchers to collaborate and thereby
meet the needs of an increasingly complex research landscape. It enables
researchers to verify results and to pursuit new research questions with ""old""
data. It is therefore not surprising that data sharing is advocated by funding
agencies, journals, and researchers alike. We surveyed 2661 individual academic
researchers across all disciplines on their dealings with data, their
publication practices, and motives for sharing or withholding research data.
The results for 1564 valid responses show that researchers across disciplines
recognise the benefit of secondary research data for their own work and for
scientific progress as a whole-still they only practice it in moderation. An
explanation for this evidence could be an academic system that is not driven by
monetary incentives, nor the desire for scientific progress, but by individual
reputation-expressed in (high ranked journal) publications. We label this
system a Reputation Economy. This special economy explains our findings that
show that researchers have a nuanced idea how to provide adequate formal
recognition for making data available to others-namely data citations. We
conclude that data sharing will only be widely adopted among research
professionals if sharing pays in form of reputation. Thus, policy measures that
intend to foster research collaboration need to understand academia as a
reputation economy. Successful measures must value intermediate products, such
as research data, more highly than it is the case now.
"
1278,Assessing a human mediated current awareness service,"  In this paper, we present an approach for analyzing the behavior of editors
in the large current awareness service ""NEP: New Economics Papers"". We
processed data from more than 38,000 issues derived from 90 different NEP
reports over the past ten years. The aim of our analysis was to gain an inside
to the editor behaviour when creating an issue and to look for factors that
influence the success of a report. In our study we looked at the following
features: average editing time, the average number of papers in an issue and
the editor effort measured on presorted issues as relative search length (RSL).
We found an average issue size of 12.4 documents per issue. The average editing
time is rather low with 14.5 minute. We get to the point that the success of a
report is mainly driven by its topic and the number of subscribers, as well as
proactive action by the editor to promote the report in her community.
"
1279,Research Data Explored II: the Anatomy and Reception of figshare,"  This is the second paper in a series of bibliometric studies of research
data. In this paper, we present an analysis of figshare, one of the largest
multidisciplinary repositories for research materials to date. We analysed the
structure of items archived in figshare, their usage, and their reception in
two altmetrics sources (PlumX and ImpactStory). We found that figshare acts (1)
as a personal repository for yet unpublished materials, (2) as a platform for
newly published research materials, and (3) as an archive for PLOS. Depending
on the function, we found different bibliometric characteristics. Items
archived from PLOS tend to be coming from the natural sciences and are often
unviewed and non-downloaded. Self-archived items, however, come from a variety
of disciplines and exhibit some patterns of higher usage. In the altmetrics
analysis, we found that Twitter was the social media service where research
data gained most attention; generally, research data published in 2014 were
most popular across social media services. PlumX detects considerably more
items in social media and also finds higher altmetric scores than ImpactStory.
"
1280,"Journal rank in the Science and Technology domain: A lightweight
  quantitative approach for evaluation","  The evaluation of journals based on their influence is of interest for
numerous reasons. Various methods of computing a score have been proposed for
measuring the scientific influence of scholarly journals. Typically the
computation of any of these scores involves compiling the citation information
pertaining to the journal under consideration. This involves significant
overhead since the article citation information of not only the journal under
consideration but also that of other journals for the recent few years need to
be stored. Our work is motivated by the idea of developing a computationally
lightweight approach that does not require any data storage, yet yields a score
which is useful for measuring the importance of journals. In this paper, a
regression analysis based method is proposed to calculate Journal Influence
Score. Proposed model is validated using historical data from the SCImago
portal. The results show that the error is small between rankings obtained
using the proposed method and the SCImago Journal Rank, thus proving that the
proposed approach is a feasible and effective method of calculating scientific
impact of journals.
"
1281,Attention decay in science,"  The exponential growth in the number of scientific papers makes it
increasingly difficult for researchers to keep track of all the publications
relevant to their work. Consequently, the attention that can be devoted to
individual papers, measured by their citation counts, is bound to decay
rapidly. In this work we make a thorough study of the life-cycle of papers in
different disciplines. Typically, the citation rate of a paper increases up to
a few years after its publication, reaches a peak and then decreases rapidly.
This decay can be described by an exponential or a power law behavior, as in
ultradiffusive processes, with exponential fitting better than power law for
the majority of cases. The decay is also becoming faster over the years,
signaling that nowadays papers are forgotten more quickly. However, when time
is counted in terms of the number of published papers, the rate of decay of
citations is fairly independent of the period considered. This indicates that
the attention of scholars depends on the number of published items, and not on
real time.
"
1282,Conferences vs. Journals: Throwing the baby out with the bath water?,"  Criticism of the conference model should be put in context. Evidences suggest
that the essential features of this model have emerged as responses to
challenges posed by current trends of scientific research and the impact of the
new techno-economic paradigm, the age of Information and Communication
Technology. This context seems indispensable when discussing today's problems
of scientific evaluation, in particular the Conference vs. Journal (CvJ)
debate. This debate, also, would benefit from systematic historical and
sociological studies of these practices. In this note we briefly develop these
arguments.
"
1283,Measuring Technological Distance for Patent Mapping,"  Recent works in the information science literature have presented cases of
using patent databases and patent classification information to construct
network maps of technology fields, which aim to aid in competitive intelligence
analysis and innovation decision making. Constructing such a patent network
requires a proper measure of the distance between different classes of patents
in the patent classification systems. Despite the existence of various distance
measures in the literature, it is unclear how to consistently assess and
compare them, and which ones to select for constructing patent technology
network maps. This ambiguity has limited the development and applications of
such technology maps. Herein, we propose to compare alternative distance
measures and identify the superior ones by analyzing the differences and
similarities in the structural properties of resulting patent network maps.
Using United States patent data from 1976 to 2006 and International Patent
Classification system, we compare 12 representative distance measures, which
quantify inter-field knowledge base proximity, field-crossing diversification
likelihood or frequency of innovation agents, and co-occurrences of patent
classes in the same patents. Our comparative analyses suggest the patent
technology network maps based on normalized co-reference and inventor
diversification likelihood measures are the best representatives.
"
1284,Modelling the Structure and Dynamics of Science Using Books,"  Scientific research is a major driving force in a knowledge based economy.
Income, health and wellbeing depend on scientific progress. The better we
understand the inner workings of the scientific enterprise, the better we can
prompt, manage, steer, and utilize scientific progress. Diverse indicators and
approaches exist to evaluate and monitor research activities, from calculating
the reputation of a researcher, institution, or country to analyzing and
visualizing global brain circulation. However, there are very few predictive
models of science that are used by key decision makers in academia, industry,
or government interested to improve the quality and impact of scholarly
efforts. We present a novel 'bibliographic bibliometric' analysis which we
apply to a large collection of books relevant for the modelling of science. We
explain the data collection together with the results of the data analyses and
visualizations. In the final section we discuss how the analysis of books that
describe different modelling approaches can inform the design of new models of
science.
"
1285,ADS: The Next Generation Search Platform,"  Four years after the last LISA meeting, the NASA Astrophysics Data System
(ADS) finds itself in the middle of major changes to the infrastructure and
contents of its database. In this paper we highlight a number of features of
great importance to librarians and discuss the additional functionality that we
are currently developing. Starting in 2011, the ADS started to systematically
collect, parse and index full-text documents for all the major publications in
Physics and Astronomy as well as many smaller Astronomy journals and arXiv
e-prints, for a total of over 3.5 million papers. Our citation coverage has
doubled since 2010 and now consists of over 70 million citations. We are
normalizing the affiliation information in our records and, in collaboration
with the CfA library and NASA, we have started collecting and linking funding
sources with papers in our system. At the same time, we are undergoing major
technology changes in the ADS platform which affect all aspects of the system
and its operations. We have rolled out and are now enhancing a new
high-performance search engine capable of performing full-text as well as
metadata searches using an intuitive query language which supports fielded,
unfielded and functional searches. We are currently able to index
acknowledgments, affiliations, citations, funding sources, and to the extent
that these metadata are available to us they are now searchable under our new
platform. The ADS private library system is being enhanced to support reading
groups, collaborative editing of lists of papers, tagging, and a variety of
privacy settings when managing one's paper collection. While this effort is
still ongoing, some of its benefits are already available through the ADS Labs
user interface and API at http://adslabs.org/adsabs/
"
1286,"Ariadne's Thread - Interactive Navigation in a World of Networked
  Information","  This work-in-progress paper introduces an interface for the interactive
visual exploration of the context of queries using the ArticleFirst database, a
product of OCLC. We describe a workflow which allows the user to browse live
entities associated with 65 million articles. In the on-line interface, each
query leads to a specific network representation of the most prevailing
entities: topics (words), authors, journals and Dewey decimal classes linked to
the set of terms in the query. This network represents the context of a query.
Each of the network nodes is clickable: by clicking through, a user traverses a
large space of articles along dimensions of authors, journals, Dewey classes
and words simultaneously. We present different use cases of such an interface.
This paper provides a link between the quest for maps of science and on-going
debates in HCI about the use of interactive information visualisation to
empower users in their search.
"
1287,Science Bots: a Model for the Future of Scientific Computation?,"  As a response to the trends of the increasing importance of computational
approaches and the accelerating pace in science, I propose in this position
paper to establish the concept of ""science bots"" that autonomously perform
programmed tasks on input data they encounter and immediately publish the
results. We can let such bots participate in a reputation system together with
human users, meaning that bots and humans get positive or negative feedback by
other participants. Positive reputation given to these bots would also shine on
their owners, motivating them to contribute to this system, while negative
reputation will allow us to filter out low-quality data, which is inevitable in
an open and decentralized system.
"
1288,Exploring Coverage and Distribution of Identifiers on the Scholarly Web,"  In a scientific publishing environment that is increasingly moving online,
identifiers of scholarly work are gaining in importance. In this paper, we
analysed identifier distribution and coverage of articles from the discipline
of quantitative biology using arXiv, Mendeley and CrossRef as data sources. The
results show that when retrieving arXiv articles from Mendeley, we were able to
find more papers using the DOI than the arXiv ID. This indicates that DOI may
be a better identifier with respect to findability. We also find that coverage
of articles on Mendeley decreases in the most recent years, whereas the
coverage of DOIs does not decrease in the same order of magnitude. This hints
at the fact that there is a certain time lag involved, before articles are
covered in crowd-sourced services on the scholarly web.
"
1289,Can we track the geography of surnames based on bibliographic data?,"  In this paper we explore the possibility of using bibliographic databases for
tracking the geographic origin of surnames. Surnames are used as a proxy to
determine the ethnic, genetic or geographic origin of individuals in many
fields such as Genetics or Demography; however they could also be used for
bibliometric purposes such as the analysis of scientific migration flows. Here
we present two relevant methodologies for determining the most probable country
to which a surname could be assigned. The first methodology assigns surnames
based on the most common country that can be assigned to a surname and the
Kullback-Liebler divergence measure. The second method uses the Gini Index to
evaluate the assignment of surnames to countries. We test both methodologies
with control groups and conclude that, despite needing further analysis on its
validity; these methodologies already show promising results.
"
1290,"The Open Access Advantage Considering Citation, Article Usage and Social
  Media Attention","  In this study, we compare the difference in the impact between open access
(OA) and non-open access (non-OA) articles. 1761 Nature Communications articles
published from 1 Jan. 2012 to 31 Aug. 2013 are selected as our research
objects, including 587 OA articles and 1174 non-OA articles. Citation data and
daily updated article-level metrics data are harvested directly from the
platform of nature.com. Data is analyzed from the static versus
temporal-dynamic perspectives. The OA citation advantage is confirmed, and the
OA advantage is also applicable when extending the comparing from citation to
article views and social media attention. More important, we find that OA
papers not only have the great advantage of total downloads, but also have the
feature of keeping sustained and steady downloads for a long time. For article
downloads, non-OA papers only have a short period of attention, when the
advantage of OA papers exists for a much longer time.
"
1291,"ADS 2.0: new architecture, API and services","  The ADS platform is undergoing the biggest rewrite of its 20-year history.
While several components have been added to its architecture over the past
couple of years, this talk will concentrate on the underpinnings of ADS's
search layer and its API. To illustrate the design of the components in the new
system, we will show how the new ADS user interface is built exclusively on top
of the API using RESTful web services. Taking one step further, we will discuss
how we plan to expose the treasure trove of information hosted by ADS (10
million records and fulltext for much of the Astronomy and Physics refereed
literature) to partners interested in using this API. This will provide you
(and your intelligent applications) with access to ADS's underlying data to
enable the extraction of new knowledge and the ingestion of these results back
into the ADS. Using this framework, researchers could run controlled
experiments with content extraction, machine learning, natural language
processing, etc. In this talk, we will discuss what is already implemented,
what will be available soon, and where we are going next.
"
1292,"On the categorization of scientific citation profiles in computer
  sciences","  A common consensus in the literature is that the citation profile of
published articles in general follows a universal pattern - an initial growth
in the number of citations within the first two to three years after
publication followed by a steady peak of one to two years and then a final
decline over the rest of the lifetime of the article. This observation has long
been the underlying heuristic in determining major bibliometric factors such as
the quality of a publication, the growth of scientific communities, impact
factor of publication venues etc. In this paper, we gather and analyze a
massive dataset of scientific papers from the computer science domain and
notice that the citation count of the articles over the years follows a
remarkably diverse set of patterns - a profile with an initial peak (PeakInit),
with distinct multiple peaks (PeakMul), with a peak late in time (PeakLate),
that is monotonically decreasing (MonDec), that is monotonically increasing
(MonIncr) and that can not be categorized into any of the above (Oth). We
conduct a thorough experiment to investigate several important characteristics
of these categories such as how individual categories attract citations, how
the categorization is influenced by the year and the venue of publication of
papers, how each category is affected by self-citations, the stability of the
categories over time, and how much each of these categories contribute to the
core of the network. Further, we show that the traditional preferential
attachment models fail to explain these citation profiles. Therefore, we
propose a novel dynamic growth model that takes both the preferential
attachment and the aging factor into account in order to replicate the
real-world behavior of various citation profiles. We believe that this paper
opens the scope for a serious re-investigation of the existing bibliometric
indices for scientific research.
"
1293,Citation Analysis with Mark-and-Recapture,"  Mark-and-Recapture is a methodology from Population Biology to estimate the
number of a species without counting every individual. This is done by multiple
samplings of the species using traps and discounting the instances that were
caught repeated. In this paper we show that this methodology is applicable for
citation analysis as it is also not feasible to count all the relevant
publications of a research topic. In addition this estimation also allows us to
propose a stopping rule for researchers to decide how far one should extend
their search for relevant literature.
"
1294,Extending search facilities via bibliometric-enhanced stratagems,"  The paper introduces simple bibliometric-enhanced search facilities which are
derived from the famous stratagems by Bates. Moves, tactics and stratagems are
revisited from a Digital Library perspective. The potential of extended
versions of ""journal run"" or ""citation search"" for interactive information
retrieval is outlined. The authors elaborate on the future implementation and
evaluation of new bibliometric-enhanced search services.
"
1295,"Walking through a library remotely - Why we need maps for collections
  and how KnoweScape can help us to make them?","  There is no escape from the expansion of information, so that structuring and
locating meaningful knowledge becomes ever more difficult. The question of how
to order our knowledge is as old as the systematic acquisition, circulation,
and storage of knowledge. Classification systems have been known since ancient
times. On the Internet, one finds both classifications and taxonomies designed
by information professionals and folksonomies based on social tagging.
Nevertheless, a user navigating through large information spaces is still
confronted with a text based search interface and a list of hits as outcome.
There is still an obvious gap between a physical encounter with, for example, a
librarys collection and browsing its content through an on-line catalogue. This
paper starts from the need of digital scholarship for effective knowledge
inquiry, revisits traditional ways to support knowledge ordering and
information retrieval, and introduces into a newly funded research network
where five different communities from all corners of the scientific landscape
join forces in a quest for knowledge maps. It can be read as a manifesto for a
newly funded specific research network KnoweScape. At the same time it is a
general reflection about what one has to take into account when representing
structure and evolution of data, information and knowledge and designing
instruments to help scholars and others to navigate across the lands and oceans
of knowledge.
"
1296,P-score: A Publication-based Metric for Academic Productivity,"  In this work we propose a metric to assess academic productivity based on
publication outputs. We are interested in knowing how well a research group in
an area of knowledge is doing relatively to a pre-selected set of reference
groups, where each group is composed by academics or researchers. To assess
academic productivity we propose a new metric, which we call P-score. Our
metric P-score assigns weights to venues using only the publication patterns of
selected reference groups. This implies that P-score does not depend on
citation data and thus, that it is simpler to compute particularly in contexts
in which citation data is not easily available. Also, preliminary experiments
suggest that P-score preserves strong correlation with citation-based metrics.
"
1297,Mapping ceramics research and its evolution,"  We show here how a simple data mining of bibliographic records can be used to
follow and help understand the evolution of a research domain, at a level that
cannot be captured by reading individual papers in a field of this size. We
illustrate the approach by investigating 43 years of research on ceramic
materials, covered by 253k bibliographic records. The patterns of keywords used
reveal the trends and the evolution of research ideas and priorities within the
field. Simple, interactive tools based on co-word network analysis help us
better appreciate the organization and relationships of ideas or individuals,
and hopefully allow identification of unexplored concepts, connections, or
approaches on a given topic.
"
1298,A Population Model for the Academic Ecosystem,"  In recent times, the academic ecosystem has seen a tremendous growth in
number of authors and publications. While most temporal studies in this area
focus on evolution of co-author and citation network structure, this systemic
inflation has received very little attention. In this paper, we address this
issue by proposing a population model for academia, derived from publication
records in the Computer Science domain. We use a generalized branching process
as an overarching framework, which enables us to describe the evolution and
composition of the research community in a systematic manner. Further, the
observed patterns allow us to shed light on researchers' lifecycle encompassing
arrival, academic life expectancy, activity, productivity and offspring
distribution in the ecosystem. We believe such a study will help develop better
bibliometric indices which account for the inflation, and also provide insights
into sustainable and efficient resource management for academia.
"
1299,A Preliminary Review of Influential Works in Data-Driven Discovery,"  The Gordon and Betty Moore Foundation ran an Investigator Competition as part
of its Data-Driven Discovery Initiative in 2014. We received about 1,100
applications and each applicant had the opportunity to list up to five
influential works in the general field of ""Big Data"" for scientific discovery.
We collected nearly 5,000 references and 53 works were cited at least six
times. This paper contains our preliminary findings.
"
1300,"The Normalization of Occurrence and Co-occurrence Matrices in
  Bibliometrics using Cosine Similarities and Ochiai Coefficients","  We prove that Ochiai similarity of the co-occurrence matrix is equal to
cosine similarity in the underlying occurrence matrix. Neither the cosine nor
the Pearson correlation should be used for the normalization of co-occurrence
matrices because the similarity is then normalized twice, and therefore
over-estimated; the Ochiai coefficient can be used instead. Results are shown
using a small matrix (5 cases, 4 variables) for didactic reasons, and also
Ahlgren et al.'s (2003) co-occurrence matrix of 24 authors in library and
information sciences. The over-estimation is shown numerically and will be
illustrated using multidimensional scaling and cluster dendograms. If the
occurrence matrix is not available (such as in internet research or author
co-citation analysis) using Ochiai for the normalization is preferable to using
the cosine.
"
1301,Predicting the long-term citation impact of recent publications,"  A fundamental problem in citation analysis is the prediction of the long-term
citation impact of recent publications. We propose a model to predict a
probability distribution for the future number of citations of a publication.
Two predictors are used: The impact factor of the journal in which a
publication has appeared and the number of citations a publication has received
one year after its appearance. The proposed model is based on quantile
regression. We employ the model to predict the future number of citations of a
large set of publications in the field of physics. Our analysis shows that both
predictors (i.e., impact factor and early citations) contribute to the accurate
prediction of long-term citation impact. We also analytically study the
behavior of the quantile regression coefficients for high quantiles of the
distribution of citations. This is done by linking the quantile regression
approach to a quantile estimation technique from extreme value theory. Our work
provides insight into the influence of the impact factor and early citations on
the long-term citation impact of a publication, and it takes a step toward a
methodology that can be used to assess research institutions based on their
most recently published work.
"
1302,Mapeamento Sistematico,"  A systematic mapping is a way to identify, evaluate and interpret all
relevant research available to a matter of particular research. One of the
reasons for conducting systematic reviews is that it summarizes the existing
evidence regarding treatment or technology [Kitchenham, 2004].
"
1303,Social media in scholarly communication,"  Social media metrics - commonly coined as ""altmetrics"" - have been heralded
as great democratizers of science, providing broader and timelier indicators of
impact than citations. These metrics come from a range of sources, including
Twitter, blogs, social reference managers, post-publication peer review, and
other social media platforms. Social media metrics have begun to be used as
indicators of scientific impact, yet the theoretical foundation, empirical
validity, and extent of use of platforms underlying these metrics lack thorough
treatment in the literature. This editorial provides an overview of terminology
and definitions of altmetrics and summarizes current research regarding social
media use in academia, social media metrics as well as data reliability and
validity. The papers of the special issue are introduced.
"
1304,"Designing a Linked Data Migrational Framework for Singapore Government
  Datasets","  The subject area of this report is Linked Data and its application to the
Government domain. Linked Data is an alternative method of data representation
that aims to interlink data from varied sources through relationships.
Governments around the world have started publishing their data in this format
to assist citizens in making better use of public services. This report
provides an eight step migrational framework for converting Singapore
Government data from legacy systems to Linked Data format. The framework
formulation is based on a study of the Singapore data ecosystem with help from
Infocomm Development Authority (iDA) of Singapore. Each step in the migrational
framework has been constructed with objectives, recommendations, best practices
and issues with entry and exit points. This work builds on the existing Linked
Data literature, implementations in other countries and cookbooks provided by
Linked Data researchers. iDA can use this report to gain an understanding of
the effort and work involved in the implementation of Linked Data system on top
of their legacy systems. The framework can be evaluated by building a Proof of
Concept (POC) application.
"
1305,"Mining and discovering biographical information in Difangzhi with a
  language-model-based approach","  We present results of expanding the contents of the China Biographical
Database by text mining historical local gazetteers, difangzhi. The goal of the
database is to see how people are connected together, through kinship, social
connections, and the places and offices in which they served. The gazetteers
are the single most important collection of names and offices covering the Song
through Qing periods. Although we begin with local officials we shall
eventually include lists of local examination candidates, people from the
locality who served in government, and notable local figures with biographies.
The more data we collect the more connections emerge. The value of doing
systematic text mining work is that we can identify relevant connections that
are either directly informative or can become useful without deep historical
research. Academia Sinica is developing a name database for officials in the
central governments of the Ming and Qing dynasties.
"
1306,"Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual
  Entailment in NTCIR RITE Evaluation Tasks","  We computed linguistic information at the lexical, syntactic, and semantic
levels for Recognizing Inference in Text (RITE) tasks for both traditional and
simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,
named-entity recognition, and near synonym recognition were employed, and
features like counts of common words, statement lengths, negation words, and
antonyms were considered to judge the entailment relationships of two
statements, while we explored both heuristics-based functions and
machine-learning approaches. The reported systems showed robustness by
simultaneously achieving second positions in the binary-classification subtasks
for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted
more experiments with the test data of NTCIR-9 RITE, with good results. We also
extended our work to search for better configurations of our classifiers and
investigated contributions of individual features. This extended work showed
interesting results and should encourage further discussion.
"
1307,"Open Access Policy: Numbers, Analysis, Effectiveness","  The PASTEUR4OA project analyses what makes an Open Access (OA) policy
effective. The total number of institutional or funder OA policies worldwide is
now 663 (March 2015), over half of them mandatory. ROARMAP, the policy
registry, has been rebuilt to record more policy detail and provide more
extensive search functionality. Deposit rates were measured for articles in
institutions' repositories and compared to the total number of WoS-indexed
articles published from those institutions. Average deposit rate was over four
times as high for institutions with a mandatory policy. Six positive
correlations were found between deposit rates and (1) Must-Deposit; (2)
Cannot-Waive-Deposit; (3) Deposit-Linked-to-Research-Evaluation; (4)
Cannot-Waive-Rights-Retention; (5) Must-Make-Deposit-OA (after allowable
embargo) and (6) Can-Waive-OA. For deposit latency, there is a positive
correlation between earlier deposit and (7) Must-Deposit-Immediately as well as
with (4) Cannot-Waive-Rights-Retention and with mandate age. There are not yet
enough OA policies to test whether still further policy conditions would
contribute to mandate effectiveness but the present findings already suggest
that it would be useful for current and future OA policies to adopt the seven
positive conditions so as to accelerate and maximise the growth of OA.
"
1308,"Highly-cited papers in Library and Information Science (LIS): Authors,
  institutions, and network structures","  As a follow-up to the highly-cited authors list published by Thomson Reuters
in June 2014, we analyze the top-1% most frequently cited papers published
between 2002 and 2012 included in the Web of Science (WoS) subject category
""Information Science & Library Science."" 798 authors contributed to 305 top-1%
publications; these authors were employed at 275 institutions. The authors at
Harvard University contributed the largest number of papers, when the addresses
are whole-number counted. However, Leiden University leads the ranking, if
fractional counting is used.
  Twenty-three of the 798 authors were also listed as most highly-cited authors
by Thomson Reuters in June 2014 (http://highlycited.com/). Twelve of these 23
authors were involved in publishing four or more of the 305 papers under study.
Analysis of co-authorship relations among the 798 highly-cited scientists shows
that co-authorships are based on common interests in a specific topic. Three
topics were important between 2002 and 2012: (1) collection and exploitation of
information in clinical practices, (2) the use of internet in public
communication and commerce, and (3) scientometrics.
"
1309,"Bibliometric author evaluation through linear regression on the coauthor
  network","  The rising trend of coauthored academic works obscures the credit assignment
that is the basis for decisions of funding and career advancements. In this
paper, a simple model based on the assumption of an unvarying ""author ability""
is introduced. With this assumption, the weight of author contributions to a
body of coauthored work can be statistically estimated. The method is tested on
a set of some more than five-hundred authors in a coauthor network from the
CiteSeerX database. The ranking obtained agrees fairly well with that given by
total fractional citation counts for an author, but noticeable differences
exist.
"
1310,"Contextualization of topics - browsing through terms, authors, journals
  and cluster allocations","  This paper builds on an innovative Information Retrieval tool, Ariadne. The
tool has been developed as an interactive network visualization and browsing
tool for large-scale bibliographic databases. It basically allows to gain
insights into a topic by contextualizing a search query (Koopman et al., 2015).
In this paper, we apply the Ariadne tool to a far smaller dataset of 111,616
documents in astronomy and astrophysics. Labeled as the Berlin dataset, this
data have been used by several research teams to apply and later compare
different clustering algorithms. The quest for this team effort is how to
delineate topics. This paper contributes to this challenge in two different
ways. First, we produce one of the different cluster solution and second, we
use Ariadne (the method behind it, and the interface - called LittleAriadne) to
display cluster solutions of the different group members. By providing a tool
that allows the visual inspection of the similarity of article clusters
produced by different algorithms, we present a complementary approach to other
possible means of comparison. More particular, we discuss how we can - with
LittleAriadne - browse through the network of topical terms, authors, journals
and cluster solutions in the Berlin dataset and compare cluster solutions as
well as see their context.
"
1311,"Evaluating the Quality of RDF Data Sets on Common Vocabularies in the
  Social, Behavioral, and Economic Sciences","  From 2012 to 2015 together with other Linked Data community members and
experts from the social, behavioral, and economic sciences (SBE), we developed
diverse vocabularies to represent SBE metadata and tabular data in RDF. The
DDI-RDF Discovery Vocabulary (DDI-RDF) is designed to support the
dissemination, management, and reuse of unit-record data, i.e., data about
individuals, households, and businesses, collected in form of responses to
studies and archived for research purposes. The RDF Data Cube Vocabulary (QB)
is a W3C recommendation for expressing data cubes, i.e. multi-dimensional
aggregate data and its metadata. Physical Data Description (PHDD) is a
vocabulary to model data in rectangular format, i.e., tabular data. The data
could either be represented in records with character-separated values (CSV) or
fixed length. The Simple Knowledge Organization System (SKOS) is a vocabulary
to build knowledge organization systems such as thesauri, classification
schemes, and taxonomies. XKOS is a SKOS extension to describe formal
statistical classifications.
  To ensure high quality of and trust in both metadata and data, their
representation in RDF must satisfy certain criteria - specified in terms of RDF
constraints. In this paper, we evaluate the data quality of 15,694 data sets
(4.26 billion triples) of research data for the social, behavioral, and
economic sciences obtained from 33 SPARQL endpoints. We checked 115 constraints
on three different and representative SBE vocabularies (DDI-RDF, QB, and SKOS)
by means of the RDF Validator, a validation environment which is available at
http://purl.org/net/rdfval-demo.
"
1312,"Constraints to Validate RDF Data Quality on Common Vocabularies in the
  Social, Behavioral, and Economic Sciences","  To ensure high quality of and trust in both metadata and data, their
representation in RDF must satisfy certain criteria - specified in terms of RDF
constraints. From 2012 to 2015 together with other Linked Data community
members and experts from the social, behavioral, and economic sciences (SBE),
we developed diverse vocabularies to represent SBE metadata and rectangular
data in RDF.
  The DDI-RDF Discovery Vocabulary (DDI-RDF) is designed to support the
dissemination, management, and reuse of unit-record data, i.e., data about
individuals, households, and businesses, collected in form of responses to
studies and archived for research purposes. The RDF Data Cube Vocabulary (QB)
is a W3C recommendation for expressing data cubes, i.e. multi-dimensional
aggregate data and its metadata. Physical Data Description (PHDD) is a
vocabulary to model data in rectangular format, i.e., tabular data. The data
could either be represented in records with character-separated values (CSV) or
fixed length. The Simple Knowledge Organization System (SKOS) is a vocabulary
to build knowledge organization systems such as thesauri, classification
schemes, and taxonomies. XKOS is a SKOS extension to describe formal
statistical classifications.
  In this paper, we describe RDF constraints to validate metadata on
unit-record data (DDI-RDF), aggregated data (QB), thesauri (SKOS), and
statistical classifications (XKOS) and to validate tabular data (PHDD) - all of
them represented in RDF. We classified these constraints according to the
severity of occurring constraint violations. This technical report is updated
continuously as modifying, adding, and deleting constraints remains ongoing
work.
"
1313,"Design and Implementation of an Integrated Information System to Support
  Scientific Research","  Computerization of research activities led to the creation of large
specialized information resources, platforms, services and software to support
scientific research. However, their shortcomings do not allow to fully
realizing the comprehensive support of scientific activity, and the absence of
a single entry point to divide the scientific community fragmented groups
interests. The article based on analysing the existing solutions and approaches
to the tools of information and communication technologies of various types of
scientific activity, and taking into account the research lifecycle proposed
and formulated the basic principles of designing and implementing an integrated
information system to support scientific research.
"
1314,"Can Intellectual Processes in the Sciences Also Be Simulated? The
  Anticipation and Visualization of Possible Future States","  Socio-cognitive action reproduces and changes both social and cognitive
structures. The analytical distinction between these dimensions of structure
provides us with richer models of scientific development. In this study, I
assume that (i) social structures organize expectations into belief structures
that can be attributed to individuals and communities; (ii) expectations are
specified in scholarly literature; and (iii) intellectually the sciences
(disciplines, specialties) tend to self-organize as systems of rationalized
expectations. Whereas social organizations remain localized, academic writings
can circulate, and expectations can be stabilized and globalized using
symbolically generalized codes of communication. The intellectual
restructuring, however, remains latent as a second-order dynamics that can be
accessed by participants only reflexively. Yet, the emerging ""horizons of
meaning"" provide feedback to the historically developing organizations by
constraining the possible future states as boundary conditions. I propose to
model these possible future states using incursive and hyper-incursive
equations from the computation of anticipatory systems. Simulations of these
equations enable us to visualize the couplings among the historical--i.e.,
recursive--progression of social structures along trajectories, the
evolutionary--i.e., hyper-incursive--development of systems of expectations at
the regime level, and the incursive instantiations of expectations in actions,
organizations, and texts.
"
1315,"Complex Integration of Digital Collections into Scientific Information
  Space","  The article considers the solution of problems of accumulation and
integration of scientific electronic collections into information space of
scientific researches. On the basis of the analysis of the existing standards
and solutions the choice of methodology and technologies of representation of
electronic materials of the scientific conference 'Internet and Modern Society'
locates. The concept of the project of integration of the created electronic
collection into the main world and domestic information systems and aggregators
of scientific information is considered.
"
1316,"Computing a consensus journal meta-ranking using paired comparisons and
  adaptive lasso estimators","  In a ""publish-or-perish culture"", the ranking of scientific journals plays a
central role in assessing performance in the current research environment. With
a wide range of existing methods and approaches to deriving journal rankings,
meta-rankings have gained popularity as a means of aggregating different
information sources. In this paper, we propose a method to create a consensus
meta-ranking using heterogeneous journal rankings. Using a parametric model for
paired comparison data we estimate quality scores for 58 journals in the OR/MS
community, which together with a shrinkage procedure allows for the
identification of clusters of journals with similar quality. The use of paired
comparisons provides a flexible framework for deriving a consensus score while
eliminating the problem of data missingness.
"
1317,"A new generation of science overlay maps with an application to the
  history of biosystematics","  The paper proposes a text-mining based analytical framework aiming at the
cognitive organization of complex scientific discourses. The approach is based
on models recently developed in science mapping, being a generalization of the
so-called Science Overlay Mapping methodology, referred to as Topic Overlay
Mapping (TOM). It is shown that via applications of TOM in visualization,
document clustering, time series analysis etc. the in-depth exploration and
even the measurement of cognitive complexity and its dynamics is feasible for
scientific domains. As a use case, an empirical study is presented into the
discovery of a long-standing complex, interdisciplinary discourse, the debate
on the species concept in biosystematics.
"
1318,"The Dynamics of Triads in Aggregated Journal-Journal Citation Relations:
  Specialty Developments at the Above-Journal Level","  Dyads of journals related by citations can agglomerate into specialties
through the mechanism of triadic closure. Using the Journal Citation Reports
2011, 2012, and 2013, we analyze triad formation as indicators of integration
(specialty growth) and disintegration (restructuring). The strongest
integration is found among the large journals that report on studies in
different scientific specialties, such as PLoS ONE, Nature Communications,
Nature, and Science. This tendency towards large-scale integration has not yet
stabilized. Using the Islands algorithm, we also distinguish 51 local maxima of
integration. We zoom into the cited articles that carry the integration for:
(i) a new development within high-energy physics and (ii) an emerging interface
between the journals Applied Mathematical Modeling and the International
Journal of Advanced Manufacturing Technology. In the first case, integration is
brought about by a specific communication reaching across specialty boundaries,
whereas in the second, the dyad of journals indicates an emerging interface
between specialties. These results suggest that integration picks up
substantive developments at the specialty level. An advantage of the bottom-up
method is that no ex ante classification of journals is assumed in the dynamic
analysis.
"
1319,Cited Half-Life of the Journal Literature,"  Analyzing 13,455 journals listed in the Journal Citation Report (Thomson
Reuters) from 1997 through 2013, we report that the mean cited half-life of the
scholarly literature is 6.5 years and growing at a rate of 0.13 years per
annum. Focusing on a subset of journals (N=4,937) for which we have a
continuous series of half-life observations, 209 of 229 (91%) subject
categories experienced increasing cited half-lives. Contrary to the overall
trend, engineering and chemistry journals experienced declining cited
half-lives. Last, as journals attracted more citations, a larger proportion of
them were directed toward older papers. The trend to cite older papers is not
fully explained by technology (digital publishing, search and retrieval, etc.),
but may be the result of a structural shift to fund incremental and applied
research over fundamental science.
"
1320,"Networks of reader and country status: An analysis of Mendeley reader
  statistics","  The number of papers published in journals indexed by the Web of Science core
collection is steadily increasing. In recent years, nearly two million new
papers were published each year; somewhat more than one million papers when
primary research papers are considered only (articles and reviews are the
document types where primary research is usually reported or reviewed).
However, who reads these papers? More precisely, which groups of researchers
from which (self-assigned) scientific disciplines and countries are reading
these papers? Is it possible to visualize readership patterns for certain
countries, scientific disciplines, or academic status groups? One popular
method to answer these questions is a network analysis. In this study, we
analyze Mendeley readership data of a set of 1,133,224 articles and 64,960
reviews with publication year 2012 to generate three different kinds of
networks: (1) The network based on disciplinary affiliations of Mendeley
readers contains four groups: (i) biology, (ii) social science and humanities
(including relevant computer science), (iii) bio-medical sciences, and (iv)
natural science and engineering. In all four groups, the category with the
addition ""miscellaneous"" prevails. (2) The network of co-readers in terms of
professional status shows that a common interest in papers is mainly shared
among PhD students, Master's students, and postdocs. (3) The country network
focusses on global readership patterns: a group of 53 nations is identified as
core to the scientific enterprise, including Russia and China as well as two
thirds of the OECD (Organisation for Economic Co-operation and Development)
countries.
"
1321,"Do they agree? Bibliometric evaluation vs informed peer review in the
  Italian research assessment exercise","  During the Italian research assessment exercise, the national agency ANVUR
performed an experiment to assess agreement between grades attributed to
journal articles by informed peer review (IR) and by bibliometrics. A sample of
articles was evaluated by using both methods and agreement was analyzed by
weighted Cohen's kappas. ANVUR presented results as indicating an overall
'good' or 'more than adequate' agreement. This paper re-examines the experiment
results according to the available statistical guidelines for interpreting
kappa values, by showing that the degree of agreement, always in the range
0.09-0.42 has to be interpreted, for all research fields, as unacceptable, poor
or, in a few cases, as, at most, fair. The only notable exception, confirmed
also by a statistical meta-analysis, was a moderate agreement for economics and
statistics (Area 13) and its sub-fields. We show that the experiment protocol
adopted in Area 13 was substantially modified with respect to all the other
research fields, to the point that results for economics and statistics have to
be considered as fatally flawed. The evidence of a poor agreement supports the
conclusion that IR and bibliometrics do not produce similar results, and that
the adoption of both methods in the Italian research assessment possibly
introduced systematic and unknown biases in its final results. The conclusion
reached by ANVUR must be reversed: the available evidence does not justify at
all the joint use of IR and bibliometrics within the same research assessment
exercise.
"
1322,Towards the Ontology Web Search Engine,"  The project of the Ontology Web Search Engine is presented in this paper. The
main purpose of this paper is to develop such a project that can be easily
implemented. Ontology Web Search Engine is software to look for and index
ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and
they are necessary for the functioning of the SWES (Semantic Web Expert
System). SWES is an expert system that will use found ontologies from the Web,
generating rules from them, and will supplement its knowledge base with these
generated rules. It is expected that the SWES will serve as a universal expert
system for the average user.
"
1323,"When is an article actually published? An analysis of online
  availability, publication, and indexation dates","  With the acceleration of scholarly communication in the digital era, the
publication year is no longer a sufficient level of time aggregation for
bibliometric and social media indicators. Papers are increasingly cited before
they have been officially published in a journal issue and mentioned on Twitter
within days of online availability. In order to find a suitable proxy for the
day of online publication allowing for the computation of more accurate
benchmarks and fine-grained citation and social media event windows, various
dates are compared for a set of 58,896 papers published by Nature Publishing
Group, PLOS, Springer and Wiley-Blackwell in 2012. Dates include the online
date provided by the publishers, the month of the journal issue, the Web of
Science indexing date, the date of the first tweet mentioning the paper as well
as the Altmetric.com publication and first-seen dates. Comparing these dates,
the analysis reveals that large differences exist between publishers, leading
to the conclusion that more transparency and standardization is needed in the
reporting of publication dates. The date on which the fixed journal article
(Version of Record) is first made available on the publisher's website is
proposed as a consistent definition of the online date.
"
1324,"Green open access in computer science - an exploratory study on
  author-based self-archiving awareness, practice, and inhibitors","  Access to the work of others is something that is too often taken for
granted, yet problematic and difficult to be obtained unless someone pays for
it. Green and gold open access are claimed to be a solution to this problem.
While open access is gaining momentum in some fields, there is a limited and
seasoned knowledge about self-archiving in computer science. In particular,
there is an inadequate understanding of author-based self-archiving awareness,
practice, and inhibitors. This article reports an exploratory study of the
awareness of self-archiving, the practice of self-archiving, and the inhibitors
of self-archiving among authors in an Italian computer science faculty.
Forty-nine individuals among interns, PhD students, researchers, and professors
were recruited in a questionnaire (response rate of 72.8%). The quantitative
and qualitative responses suggested that there is still work needed in terms of
advocating green open access to computer science authors who seldom
self-archive and when they do, they often infringe the copyright transfer
agreements (CTAs) of the publishers. In addition, tools from the open-source
community are needed to facilitate author-based self-archiving, which should
comprise of an automatic check of the CTAs. The study identified nine factors
inhibiting the act of self-archiving among computer scientists. As a first
step, this study proposes several propositions regarding author-based
self-archiving in computer science that can be further investigated.
Recommendations to foster self-archiving in computer science, based on the
results, are provided.
"
1325,"The BiPublishers ranking: Main results and methodological problems when
  constructing rankings of academic publishers","  We present the results of the Bibliometric Indicators for Publishers project
(also known as BiPublishers). This project represents the first attempt to
systematically develop bibliometric publisher rankings. The data for this
project was derived from the Book Citation Index, and the study time period was
2009-2013. We have developed 42 rankings: 4 for by fields and 38 by
disciplines. We display six indicators by publisher divided into three types:
output, impact and publisher's profile. The aim is to capture different
characteristics of the research performance of publishers. 254 publishers were
processed and classified according to publisher type: commercial publishers and
university presses. We present the main publishers by fields. Then, we discuss
the main challenges presented when developing this type of tools. The
BiPublishers ranking is an on-going project which aims to develop and explore
new data sources and indicators to better capture and define the research
impact of publishers.
"
1326,"Analyzing readerships of International Iranian publications in Mendeley:
  an altmetrics study","  In this study, the presence and distribution of both Mendeley readerships and
Web of Science citations for the publications published in the 43 Iranian
international journals indexed in Journal Citation Reports have been
investigated. The aim was to determine the impact, visibility and use of the
publications published by the Iranian international journals in Mendeley
compared to their citation impact; furthermore, to explore if there is any
relation between these two impact indicators (Mendeley readership counts and
WoS citation counts) for these publications. The DOIs of the 1,884 publications
used to extract the readerships data from Mendeley REST API in February 2014
and citations data until end of 2013 calculated using CWTS in-house WoS
database. SPSS (version 21) used to analyze the relationship between the
readerships and citations for those publications. The Mendeley usage
distribution both at the publication level (across publications years, fields
and document types) and at the user level (across users disciplines, academic
status and countries) have been investigated. These information will help to
understand the visibility and usage vs citation pattern and impact of Iranian
scientific outputs.
"
1327,"Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of
  Methods and Tools","  The Open Access movement in scientific publishing and search engines like
Google Scholar have made scientific articles more broadly accessible. During
the last decade, the availability of scientific papers in full text has become
more and more widespread thanks to the growing number of publications on online
platforms such as ArXiv and CiteSeer. The efforts to provide articles in
machine-readable formats and the rise of Open Access publishing have resulted
in a number of standardized formats for scientific papers (such as NLM-JATS,
TEI, DocBook). Our aim is to stimulate research at the intersection of
Bibliometrics and Computational Linguistics in order to study the ways
Bibliometrics can benefit from large-scale text analytics and sense mining of
scientific papers, thus exploring the interdisciplinarity of Bibliometrics and
Natural Language Processing.
"
1328,"Growing the Digital Repository of Mathematical Formulae with Generic
  LaTeX Sources","  One initial goal for the DRMF is to seed our digital compendium with
fundamental orthogonal polynomial formulae. We had used the data from the NIST
Digital Library of Mathematical Functions (DLMF) as initial seed for our DRMF
project. The DLMF input LaTeX source already contains some semantic information
encoded using a highly customized set of semantic LaTeX macros. Those macros
could be converted to content MathML using LaTeXML. During that conversion the
semantics were translated to an implicit DLMF content dictionary. This year, we
have developed a semantic enrichment process whose goal is to infer semantic
information from generic LaTeX sources. The generated context-free semantic
information is used to build DRMF formula home pages for each individual
formula. We demonstrate this process using selected chapters from the book
""Hypergeometric Orthogonal Polynomials and their $q$-Analogues"" (2010) by
Koekoek, Lesky and Swarttouw (KLS) as well as an actively maintained addendum
to this book by Koornwinder (KLSadd). The generic input KLS and KLSadd LaTeX
sources describe the printed representation of the formulae, but does not
contain explicit semantic information. See http://drmf.wmflabs.org.
"
1329,Differences in Personal and Professional Tweets of Scholars,"  Purpose: This study shows that there were differences in the use of Twitter
by professors at universities in the Association of American Universities
(AAU). Affordance use differed between the personal and professional tweets of
professors. Framing behaviors were described that could impact the
interpretation of tweets by audience members. Design/methodology/approach: A
three phase research design was used that included surveys of professors,
categorization of tweets by Amazon's Mechanical Turk workers (i.e., turkers),
and categorization of tweets by active professors on Twitter. Findings: There
were significant differences found between professors that reported having a
Twitter account, significant differences found between types of Twitter
accounts (personal, professional, or both), and significant differences in the
affordances used in personal and professional tweets. Framing behaviors were
described that may assist altmetric researchers in distinguishing between
personal and professional tweets. Research limitations/implications (if
applicable): The study is limited by the sample population, survey instrument,
low survey response rate, and low Cohen's kappa. Practical implications (if
applicable): An overview of various affordances found in Twitter is provided
and a novel use of Amazon's Mechanical Turk for the categorization of tweets is
described that can be applied to future altmetric studies. Originality/value:
This work utilizes a socio-technical framework integrating social and
psychological theories to interpret results from the tweeting behavior of
professors and the interpretation of tweets by workers in Amazon's Mechanical
Turk.
"
1330,Do PageRank-based author rankings outperform simple citation counts?,"  The basic indicators of a researcher's productivity and impact are still the
number of publications and their citation counts. These metrics are clear,
straightforward, and easy to obtain. When a ranking of scholars is needed, for
instance in grant, award, or promotion procedures, their use is the fastest and
cheapest way of prioritizing some scientists over others. However, due to their
nature, there is a danger of oversimplifying scientific achievements.
Therefore, many other indicators have been proposed including the usage of the
PageRank algorithm known for the ranking of webpages and its modifications
suited to citation networks. Nevertheless, this recursive method is
computationally expensive and even if it has the advantage of favouring
prestige over popularity, its application should be well justified,
particularly when compared to the standard citation counts. In this study, we
analyze three large datasets of computer science papers in the categories of
artificial intelligence, software engineering, and theory and methods and apply
12 different ranking methods to the citation networks of authors. We compare
the resulting rankings with self-compiled lists of outstanding researchers
selected as frequent editorial board members of prestigious journals in the
field and conclude that there is no evidence of PageRank-based methods
outperforming simple citation counts.
"
1331,Quantifying the consistency of scientific databases,"  Science is a social process with far-reaching impact on our modern society.
In the recent years, for the first time we are able to scientifically study the
science itself. This is enabled by massive amounts of data on scientific
publications that is increasingly becoming available. The data is contained in
several databases such as Web of Science or PubMed, maintained by various
public and private entities. Unfortunately, these databases are not always
consistent, which considerably hinders this study. Relying on the powerful
framework of complex networks, we conduct a systematic analysis of the
consistency among six major scientific databases. We found that identifying a
single ""best"" database is far from easy. Nevertheless, our results indicate
appreciable differences in mutual consistency of different databases, which we
interpret as recipes for future bibliometric studies.
"
1332,Informetric Analyses of Knowledge Organization Systems (KOSs),"  A knowledge organization system (KOS) is made up of concepts and semantic
relations between the concepts which represent a knowledge domain
terminologically. We distinguish between five approaches to KOSs:
nomenclatures, classification systems, thesauri, ontologies and, as a
borderline case of KOSs, folksonomies. The research question of this paper is:
How can we informetrically analyze the effectiveness of KOSs? Quantitative
informetric measures and indicators allow for the description, for comparative
analyses as well as for evaluation of KOSs and their quality. We describe the
state of the art of KOS evaluation. Most of the evaluation studies found in the
literature are about ontologies. We introduce measures of the structure of KOSs
(e.g., groundedness, tangledness, fan-out factor, or granularity) and
indicators of KOS quality (completeness, consistency, overlap, and use).
"
1333,A critical cluster analysis of 44 indicators of author-level performance,"  This paper explores the relationship between author-level bibliometric
indicators and the researchers the ""measure"", exemplified across five academic
seniorities and four disciplines. Using cluster methodology, the disciplinary
and seniority appropriateness of author-level indicators is examined.
Publication and citation data for 741 researchers across Astronomy,
Environmental Science, Philosophy and Public Health was collected in Web of
Science (WoS). Forty-four indicators of individual performance were computed
using the data. A two-step cluster analysis using IBM SPSS version 22 was
performed, followed by a risk analysis and ordinal logistic regression to
explore cluster membership. Indicator scores were contextualized using the
individual researcher's curriculum vitae. Four different clusters based on
indicator scores ranked researchers as low, middle, high and extremely high
performers. The results show that different indicators were appropriate in
demarcating ranked performance in different disciplines. In Astronomy the h2
indicator, sum pp top prop in Environmental Science, Q2 in Philosophy and
e-index in Public Health. The regression and odds analysis showed individual
level indicator scores were primarily dependent on the number of years since
the researcher's first publication registered in WoS, number of publications
and number of citations. Seniority classification was secondary therefore no
seniority appropriate indicators were confidently identified. Cluster
methodology proved useful in identifying disciplinary appropriate indicators
providing the preliminary data preparation was thorough but needed to be
supplemented by other analyses to validate the results. A general disconnection
between the performance of the researcher on their curriculum vitae and the
performance of the researcher based on bibliometric indicators was observed.
"
1334,"Scientometrics and Science Studies: From Words and Co-Words to
  Information and Probabilistic Entropy","  The tension between qualitative theorizing and quantitative methods is
pervasive in the social sciences, and poses a constant challenge to empirical
research. But in science studies as an interdisciplinary specialty, there are
additional reasons why a more reflexive consciousness of the differences among
the relevant disciplines is necessary. How can qualitative insights from the
history of ideas and the sociology of science be combined with the quantitative
perspective? By using the example of the lexical and semantic value of word
occurrences, the issue of qualitatively different meanings of the same
phenomena is discussed as a methodological problem. Nine criteria for methods
which are needed for the development of science studies as an integrated
enterprise can then be specified. Information calculus is suggested as a method
which can comply with these criteria.
"
1335,Defining and identifying Sleeping Beauties in science,"  A Sleeping Beauty (SB) in science refers to a paper whose importance is not
recognized for several years after publication. Its citation history exhibits a
long hibernation period followed by a sudden spike of popularity. Previous
studies suggest a relative scarcity of SBs. The reliability of this conclusion
is, however, heavily dependent on identification methods based on arbitrary
threshold parameters for sleeping time and number of citations, applied to
small or monodisciplinary bibliographic datasets. Here we present a systematic,
large-scale, and multidisciplinary analysis of the SB phenomenon in science. We
introduce a parameter-free measure that quantifies the extent to which a
specific paper can be considered an SB. We apply our method to 22 million
scientific papers published in all disciplines of natural and social sciences
over a time span longer than a century. Our results reveal that the SB
phenomenon is not exceptional. There is a continuous spectrum of delayed
recognition where both the hibernation period and the awakening intensity are
taken into account. Although many cases of SBs can be identified by looking at
monodisciplinary bibliographic data, the SB phenomenon becomes much more
apparent with the analysis of multidisciplinary datasets, where we can observe
many examples of papers achieving delayed yet exceptional importance in
disciplines different from those where they were originally published. Our
analysis emphasizes a complex feature of citation dynamics that so far has
received little attention, and also provides empirical evidence against the use
of short-term citation metrics in the quantification of scientific impact.
"
1336,A Survey on Retrieval of Mathematical Knowledge,"  We present a short survey of the literature on indexing and retrieval of
mathematical knowledge, with pointers to 72 papers and tentative taxonomies of
both retrieval problems and recurring techniques.
"
1337,Chinese Interpreting Studies: Genesis of a Discipline,"  The growth of Chinese Interpreting Studies (CIS) has been robust over the
past two decades; this is reflected in the total number of research papers
produced. This paper takes a scientometric approach to assessing the
production, themes and theoretical influences of those papers over time. The
most productive authors, universities, and regions, as well as patterns of
research collaboration, were analyzed to gain a deeper understanding of the CIS
landscape. This study reveals that the general culture of the discipline
remained constant throughout the period, none of its theoretical influences or
topics having gained significantly in popularity. However, certain limitations
in the way research is conducted (lack of collaboration, inadequate academic
policies, etc.) hinder its potential for future growth.
"
1338,Coauthorship and Thematic Networks in AAEP Annual Meetings,"  We analyze the coauthorship production of the AAEP Annual Meeting since 1964.
We use social network analysis for creating coauthorship networks and given
that any paper must be tagged with two JEL codes, we use this information for
also structuring a thematic network. Then we calculate network metrics and find
main actors and clusters for coauthors and topics. We distinguish a gender gap
in the sample. Thematic networks show a cluster of codes and the analysis of
the cluster shows the preeminence of the tags related to trade, econometric,
distribution/poverty and health and education topics.
"
1339,"Weighted Impact Factor (WIF) for assessing the quality of scientific
  journals","  Nowadays impact factor is the significant indicator for journal evaluation.
In impact factor calculation is used number of all citations to journal,
regardless of the prestige of cited journals, however, scientific units (paper,
researcher, journal or scientific organization) cited by journals with high
impact factor or researchers with high Hirsch index are more important than
objects cited by journals without impact factor or unknown researcher. In this
paper was offered weighted impact factor for getting more accurate rankings for
journals, which consider not only quantity of citations, but also quality of
citing journals. Correlation coefficients among different indicators for
journal evaluation: impact factors by Thomson Scientific, weighted impact
factors offered by different researchers, average and medians of all citing
journals impact factors and 5-year impact factors were analysed.
"
1340,Methods for estimating the size of Google Scholar,"  The emergence of academic search engines (mainly Google Scholar and Microsoft
Academic Search) that aspire to index the entirety of current academic
knowledge has revived and increased interest in the size of the academic web.
The main objective of this paper is to propose various methods to estimate the
current size (number of indexed documents) of Google Scholar (May 2014) and to
determine its validity, precision and reliability. To do this, we present,
apply and discuss three empirical methods: an external estimate based on
empirical studies of Google Scholar coverage, and two internal estimate methods
based on direct, empty and absurd queries, respectively. The results, despite
providing disparate values, place the estimated size of Google Scholar at
around 160 to 165 million documents. However, all the methods show considerable
limitations and uncertainties due to inconsistencies in the Google Scholar
search functionalities.
"
1341,"Revealing the online network between University and Industry: The case
  of Turkey","  The present paper attempts to explore the relationship between the Turkish
academic and industry systems by mapping the relationships under web
indicators. We used the top 100 Turkish universities and the top 10 Turkish
companies in 10 industrial sectors in order to observe the performance of web
impact indicators. Total page count metric is obtained through Google Turkey
and the pure link metrics have been gathered from Open Site Explorer. The
indicators obtained both for web presence and web visibility indicated that
there are significant differences between the group of academic institutions
and those related to companies within the web space of Turkey. However, this
current study is exploratory and should be replicated with a larger sample of
both Turkish universities and companies in each sector. Likewise, a
longitudinal study rather than sectional would eliminate or smooth fluctuations
of web data (especially URL mentions) as a more adequate understanding of the
relations between Turkish institutions, and their web impact, is reached.
"
1342,"Disclosing the network structure of private companies on the web: the
  case of Spanish IBEX 35 share index","  It is common for an international company to have different brands, products
or services, information for investors, a corporate blog, affiliates, branches
in different countries, etc. If all these contents appear as independent
additional web domains (AWD), the company should be represented on the web by
all these web domains, since many of these AWDs may acquire remarkable
performance that could mask or distort the real web performance of the company,
affecting therefore on the understanding of web metrics. The main objective of
this study is to determine the amount, type, web impact and topology of the
additional web domains in commercial companies in order to get a better
understanding on their complete web impact and structure. The set of companies
belonging to the Spanish IBEX-35 stock index has been analyzed as testing
bench. We proceeded to identify and categorize all AWDs belonging to these
companies, and to apply both web impact (web presence and visibility) and
network metrics. The results show that AWDs get a high web presence but
relatively low web visibility, due to certain opacity or less dissemination of
some AWDs, favoring its isolation. This is verified by the low network density
values obtained, that occur because AWDs are strongly connected with the
corporate domain (although asymmetrically), but very weakly linked each other.
Although the processes of AWDs creation and categorization are complex (web
policy seems not to be driven by a defined or conscious plan), their influence
on the web performance of IBEX 35companies is meaningful. This research
measures the AWDs influence on companies under webometric terms for the first
time.
"
1343,"Mapping Large Scale Research Metadata to Linked Data: A Performance
  Comparison of HBase, CSV and XML","  OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a
database of all EC FP7 and H2020 funded research projects, including metadata
of their results (publications and datasets). These data are stored in an HBase
NoSQL database, post-processed, and exposed as HTML for human consumption, and
as XML through a web service interface. As an intermediate format to facilitate
statistical computations, CSV is generated internally. To interlink the
OpenAIRE data with related data on the Web, we aim at exporting them as Linked
Open Data (LOD). The LOD export is required to integrate into the overall data
processing workflow, where derived data are regenerated from the base data
every day. We thus faced the challenge of identifying the best-performing
conversion approach.We evaluated the performances of creating LOD by a
MapReduce job on top of HBase, by mapping the intermediate CSV files, and by
mapping the XML output.
"
1344,"Coherent measures of the impact of co-authors in peer review journals
  and in proceedings publications","  This paper focuses on the coauthor effect in different types of publications,
usually not equally respected in measuring research impact. {\it A priori}
unexpected relationships are found between the total coauthor core value,
$m_a$, of a leading investigator (LI), and the related values for their
publications in either peer review journals ($j$) or in proceedings ($p$). A
surprisingly linear relationship is found: $ m_a^{(j)} + 0.4\;m_a^{(p)} =
m_a^{(jp)} $. Furthermore, another relationship is found concerning the measure
of the total number of citations, $A_a$, i.e. the surface of the citation
size-rank histogram up to $m_a$. Another linear relationship exists :
$A_a^{(j)} + 1.36\; A_a^{(p)} = A_a^{(jp)} $. These empirical findings
coefficients (0.4 and 1.36) are supported by considerations based on an
empirical power law found between the number of joint publications of an author
and the rank of a coauthor. Moreover, a simple power law relationship is found
between $m_a$ and the number ($r_M$) of coauthors of a LI: $m_a\simeq
r_M^{\mu}$; the power law exponent $\mu$ depends on the type ($j$ or $p$) of
publications. These simple relations, at this time limited to publications in
physics, imply that coauthors are a ""more positive measure"" of a principal
investigator role, in both types of scientific outputs, than the Hirsch index
could indicate. Therefore, to scorn upon co-authors in publications, in
particular in proceedings, is incorrect. On the contrary, the findings suggest
an immediate test of coherence of scientific authorship in scientific policy
processes.
"
1345,"Editorial for the First Workshop on Mining Scientific Papers:
  Computational Linguistics and Bibliometrics","  The workshop ""Mining Scientific Papers: Computational Linguistics and
Bibliometrics"" (CLBib 2015), co-located with the 15th International Society of
Scientometrics and Informetrics Conference (ISSI 2015), brought together
researchers in Bibliometrics and Computational Linguistics in order to study
the ways Bibliometrics can benefit from large-scale text analytics and sense
mining of scientific papers, thus exploring the interdisciplinarity of
Bibliometrics and Natural Language Processing (NLP). The goals of the workshop
were to answer questions like: How can we enhance author network analysis and
Bibliometrics using data obtained by text analytics? What insights can NLP
provide on the structure of scientific writing, on citation networks, and on
in-text citation analysis? This workshop is the first step to foster the
reflection on the interdisciplinarity and the benefits that the two disciplines
Bibliometrics and Natural Language Processing can drive from it.
"
1346,"Best of both worlds? Simultaneous evaluation of researchers and their
  works","  This paper explores a dual score system that simultaneously evaluates the
relative importance of researchers and their works. It is a modification of the
CITEX algorithm recently described in Pal and Ruj (2015). Using available
publication data for $m$ author keywords (as a proxy for researchers) and $n$
papers it is possible to construct a $m \times n$ author-paper feature matrix.
This is further combined with citation data to construct a HITS-like algorithm
that iteratively satisfies two criteria: first, \emph{a good author is cited by
good authors}, and second, \emph{a good paper is cited by good authors}.
Following Pal and Ruj, the resulting algorithm produces an author eigenscore
and a paper eigenscore. The algorithm is tested on 213,530 citable publications
listed under Thomson ISI's ""\emph{Information Science \& Library Science}"" JCR
category from 1980--2012.
"
1347,An Open Science Platform for the Next Generation of Data,"  Imagine an online work environment where researchers have direct and
immediate access to myriad data sources and tools and data management
resources, useful throughout the research lifecycle. This is our vision for the
next generation of the Dataverse Network: an Open Science Platform (OSP). For
the first time, researchers would be able to seamlessly access and create
primary and derived data from a variety of sources: prior research results,
public data sets, harvested online data, physical instruments, private data
collections, and even data from other standalone repositories. Researchers
could recruit research participants and conduct research directly on the OSP,
if desired, using readily available tools. Researchers could create private or
shared workspaces to house data, access tools, and computation and could
publish data directly on the platform or publish elsewhere with persistent,
data citations on the OSP. This manuscript describes the details of an Open
Science Platform and its construction. Having an Open Science Platform will
especially impact the rate of new scientific discoveries and make scientific
findings more credible and accountable.
"
1348,Hierarchical networks of scientific journals,"  Scientific journals are the repositories of the gradually accumulating
knowledge of mankind about the world surrounding us. Just as our knowledge is
organised into classes ranging from major disciplines, subjects and fields to
increasingly specific topics, journals can also be categorised into groups
using various metrics. In addition to the set of topics characteristic for a
journal, they can also be ranked regarding their relevance from the point of
overall influence. One widespread measure is impact factor, but in the present
paper we intend to reconstruct a much more detailed description by studying the
hierarchical relations between the journals based on citation data. We use a
measure related to the notion of m-reaching centrality and find a network which
shows the level of influence of a journal from the point of the direction and
efficiency with which information spreads through the network. We can also
obtain an alternative network using a suitably modified nested hierarchy
extraction method applied to the same data. The results are weakly
methodology-dependent and reveal non-trivial relations among journals. The two
alternative hierarchies show large similarity with some striking differences,
providing together a complex picture of the intricate relations between
scientific journals.
"
1349,Query Expansion for Survey Question Retrieval in the Social Sciences,"  In recent years, the importance of research data and the need to archive and
to share it in the scientific community have increased enormously. This
introduces a whole new set of challenges for digital libraries. In the social
sciences typical research data sets consist of surveys and questionnaires. In
this paper we focus on the use case of social science survey question reuse and
on mechanisms to support users in the query formulation for data sets. We
describe and evaluate thesaurus- and co-occurrence-based approaches for query
expansion to improve retrieval quality in digital libraries and research data
archives. The challenge here is to translate the information need and the
underlying sociological phenomena into proper queries. As we can show retrieval
quality can be improved by adding related terms to the queries. In a direct
comparison automatically expanded queries using extracted co-occurring terms
can provide better results than queries manually reformulated by a domain
expert and better results than a keyword-based BM25 baseline.
"
1350,Avoiding Spoilers in Fan Wikis of Episodic Fiction,"  A variety of fan-based wikis about episodic fiction (e.g., television shows,
novels, movies) exist on the World Wide Web. These wikis provide a wealth of
information about complex stories, but if readers are behind in their viewing
they run the risk of encountering ""spoilers"" -- information that gives away key
plot points before the intended time of the show's writers. Enterprising
readers might browse the wiki in a web archive so as to view the page prior to
a specific episode date and thereby avoid spoilers. Unfortunately, due to how
web archives choose the ""best"" page, it is still possible to see spoilers
(especially in sparse archives).
  In this paper we discuss how to use Memento to avoid spoilers. Memento uses
TimeGates to determine which best archived page to give back to the user,
currently using a minimum distance heuristic. We quantify how this heuristic is
inadequate for avoiding spoilers, analyzing data collected from fan wikis and
the Internet Archive. We create an algorithm for calculating the probability of
encountering a spoiler in a given wiki article. We conduct an experiment with
16 wiki sites for popular television shows. We find that 38% of those pages are
unavailable in the Internet Archive. We find that when accessing fan wiki pages
in the Internet Archive there is as much as a 66% chance of encountering a
spoiler. Using sample access logs from the Internet Archive, we find that 19%
of actual requests to the Wayback Machine for wikia.com pages ended in
spoilers. We suggest the use of a different minimum distance heuristic,
minpast, for wikis, using the desired datetime as an upper bound.
"
1351,A Probe into Causes of Non-citation Based on Survey Data,"  Empirical analysis results about the possible causes leading to non-citation
may help increase the potential of researchers' work to be cited and editorial
staffs of journals to identify contributions with potential high quality. In
this study, we conduct a survey on the possible causes leading to citation or
non-citation based on a questionnaire. We then perform a statistical analysis
to identify the major causes leading to non-citation in combination with the
analysis on the data collected through the survey. Most respondents to our
questionnaire identified eight major causes that facilitate easy citation of
one's papers, such as research hotspots and novel topics of content, longer
intervals after publication, research topics similar to my work, high quality
of content, reasonable self-citation, highlighted title, prestigious authors,
academic tastes and interests similar to mine.They also pointed out that the
vast difference between their current and former research directions as the
primary reason for their previously uncited papers. They feel that text that
includes notes, comments, and letters to editors are rarely cited, and the same
is true for too short or too lengthy papers. In comparison, it is easier for
reviews, articles, or papers of intermediate length to be cited.
"
1352,"Amplifying the Impact of Open Access: Wikipedia and the Diffusion of
  Science","  With the rise of Wikipedia as a first-stop source for scientific knowledge,
it is important to compare its representation of that knowledge to that of the
academic literature. Here we identify the 250 most heavily used journals in
each of 26 research fields (4,721 journals, 19.4M articles in total) indexed by
the Scopus database, and test whether topic, academic status, and accessibility
make articles from these journals more or less likely to be referenced on
Wikipedia. We find that a journal's academic status (impact factor) and
accessibility (open access policy) both strongly increase the probability of it
being referenced on Wikipedia. Controlling for field and impact factor, the
odds that an open access journal is referenced on the English Wikipedia are 47%
higher compared to paywall journals. One of the implications of this study is
that a major consequence of open access policies is to significantly amplify
the diffusion of science, through an intermediary like Wikipedia, to a broad
audience.
"
1353,Classification of Research Citations (CRC),"  Research is a continuous phenomenon. It is recursive in nature. Every
research is based on some earlier research outcome. A general approach in
reviewing the literature for a problem is to categorize earlier work for the
same problem as positive and negative citations. In this paper, we propose a
novel automated technique, which classifies whether an earlier work is cited as
sentiment positive or sentiment negative. Our approach first extracted the
portion of the cited text from citing paper. Using a sentiment lexicon we
classify the citation as positive or negative by picking a window of at most
five (5) sentences around the cited place (corpus). We have used Na\""ive-Bayes
Classifier for sentiment analysis. The algorithm is evaluated on a manually
annotated and class labelled collection of 150 research papers from the domain
of computer science. Our preliminary results show an accuracy of 80%. We assert
that our approach can be generalized to classification of scientific research
papers in different disciplines.
"
1354,Influence of study type on Twitter activity for medical research papers,"  Twitter has been identified as one of the most popular and promising
altmetrics data sources, as it possibly reflects a broader use of research
articles by the general public. Several factors, such as document age,
scientific discipline, number of authors and document type, have been shown to
affect the number of tweets received by scientific documents. The particular
meaning of tweets mentioning scholarly papers is, however, not entirely
understood and their validity as impact indicators debatable. This study
contributes to the understanding of factors influencing Twitter popularity of
medical papers investigating differences between medical study types. 162,830
documents indexed in Embase to a medical study type have been analysed for the
study type specific tweet frequency. Meta-analyses, systematic reviews and
clinical trials were found to be tweeted substantially more frequently than
other study types, while all basic research received less attention than the
average. The findings correspond well with clinical evidence hierarchies. It is
suggested that interest from laymen and patients may be a factor in the
observed effects.
"
1355,Ranking Journals Using Altmetrics,"  The rank of a journal based on simple citation information is a popular
measure. The simplicity and availability of rankings such as Impact Factor,
Eigenfactor and SciMago Journal Rank based on trusted commercial sources
ensures their widespread use for many important tasks despite the well-known
limitations of such rankings. In this paper we look at an alternative approach
based on information on papers from social and mainstream media sources. Our
data comes from altmetric.com who identify mentions of individual academic
papers in sources such as Twitter, Facebook, blogs and news outlets. We
consider several different methods to produce a ranking of journals from such
data. We show that most (but not all) schemes produce results, which are
roughly similar, suggesting that there is a basic consistency between social
media based approaches and traditional citation based methods. Most ranking
schemes applied to one data set produce relatively little variation and we
suggest this provides a measure of the uncertainty in any journal rating. The
differences we find between data sources also shows they are capturing
different aspects of journal impact. We conclude a small number of such ratings
will provide the best information on journal impact.
"
1356,Strategies for Parallel Markup,"  Cross-referenced parallel markup for mathematics allows the combination of
both presentation and content representations while associating the components
of each. Interesting applications are enabled by such an arrangement, such as
interaction with parts of the presentation to manipulate and querying the
corresponding content, and enhanced search indexing. Although the idea of such
markup is hardly new, effective techniques for creating and manipulating it are
more difficult than it appears. Since the structures and tokens in the two
formats often do not correspond one-to-one, decisions and heuristics must be
developed to determine in which way each component refers to and is referred to
by components of the other representation. Conversion between fine and coarse
grained parallel markup complicates ID assignments. In this paper, we will
describe the techniques developed for \LaTeXML, a \TeX/\LaTeX to XML converter,
to create cross-referenced parallel MathML. While we do not yet consider
\LaTeXML's content MathML to be useful, the current effort is a step towards
that continuing goal.
"
1357,Book to the Future - a manifesto for book liberation,"  The Book Liberation Manifesto is an exploration of publishing outside of
current corporate constraints and beyond the confines of book piracy. We
believe that knowledge should be in free circulation to benefit humankind,
which means an equitable and vibrant economy to support publishing, instead of
the prevailing capitalist hand-me-down system of Sisyphean economic
sustainability. Readers and books have been forced into pirate libraries, while
sales channels have been monopolised by the big Internet giants which exact
extortionate fees from publishers. We have three proposals. First, publications
should be free-at-the-point-of-reading under a variety of open intellectual
property regimes. Second, they should become fully digital -- in order to
facilitate ready reuse, distribution, algorithmic and computational use.
Finally, Open Source software for publishing should be treated as public
infrastructure, with sustained research and investment. The result of such
robust infrastructures will mean lower costs for manufacturing and faster
publishing lifecycles, so that publishers and publics will be more readily able
to afford to invent new futures. For more information on the Hybrid Publishing
Consortium.
"
1358,"Altmetrics (Chapter from Beyond Bibliometrics: Harnessing
  Multidimensional Indicators of Scholarly Impact)","  This chapter discusses altmetrics (short for ""alternative metrics""), an
approach to uncovering previously-invisible traces of scholarly impact by
observing activity in online tools and systems. I argue that citations, while
useful, miss many important kinds of impacts, and that the increasing scholarly
use of online tools like Mendeley, Twitter, and blogs may allow us to measure
these hidden impacts. Next, I define altmetrics and discuss research on
altmetric sources--both research mapping the growth of these sources, and
scientometric research measuring activity on them. Following a discussion of
the potential uses of altmetrics, I consider the limitations of altmetrics and
recommend areas ripe for future research.
"
1359,Time and Citation Networks,"  Citation networks emerge from a number of different social systems, such as
academia (from published papers), business (through patents) and law (through
legal judgements). A citation represents a transfer of information, and so
studying the structure of the citation network will help us understand how
knowledge is passed on. What distinguishes citation networks from other
networks is time; documents can only cite older documents. We propose that
existing network measures do not take account of the strong constraint imposed
by time. We will illustrate our approach with two types of causally aware
analysis. We apply our methods to the citation networks formed by academic
papers on the arXiv, to US patents and to US Supreme Court judgements. We show
that our tools can reveal that citation networks which appear to have very
similar structure by standard network measures turn out to have significantly
different properties. We interpret our results as indicating that many papers
in a bibliography were not directly relevant to the work and that we can
provide a simple indicator of the important citations. We suggest our methods
may highlight papers which are of more interest for interdisciplinary research.
We also quantify differences in the diversity of research directions of
different fields.
"
1360,Making Digital Artifacts on the Web Verifiable and Reliable,"  The current Web has no general mechanisms to make digital artifacts --- such
as datasets, code, texts, and images --- verifiable and permanent. For digital
artifacts that are supposed to be immutable, there is moreover no commonly
accepted method to enforce this immutability. These shortcomings have a serious
negative impact on the ability to reproduce the results of processes that rely
on Web resources, which in turn heavily impacts areas such as science where
reproducibility is important. To solve this problem, we propose trusty URIs
containing cryptographic hash values. We show how trusty URIs can be used for
the verification of digital artifacts, in a manner that is independent of the
serialization format in the case of structured data files such as
nanopublications. We demonstrate how the contents of these files become
immutable, including dependencies to external digital artifacts and thereby
extending the range of verifiability to the entire reference tree. Our approach
sticks to the core principles of the Web, namely openness and decentralized
architecture, and is fully compatible with existing standards and protocols.
Evaluation of our reference implementations shows that these design goals are
indeed accomplished by our approach, and that it remains practical even for
very large files.
"
1361,Adapting sentiment analysis for tweets linking to scientific papers,"  In the context of altmetrics, tweets have been discussed as potential
indicators of immediate and broader societal impact of scientific documents.
However, it is not yet clear to what extent Twitter captures actual research
impact. A small case study (Thelwall et al., 2013b) suggests that tweets to
journal articles neither comment on nor express any sentiments towards the
publication, which suggests that tweets merely disseminate bibliographic
information, often even automatically. This study analyses the sentiments of
tweets for a large representative set of scientific papers by specifically
adapting different methods to academic articles distributed on Twitter. Results
will help to improve the understanding of Twitter's role in scholarly
communication and the meaning of tweets as impact metrics.
"
1362,Archaeology in the Digital Age: From Paper to Databases,"  Research units in archaeology often manage large and precious archives
containing various documents, including reports on fieldwork, scholarly studies
and reference books. These archives are of course invaluable, recording decades
of work, but are generally hard to consult and access. In this context,
digitizing full text documents is not enough: information must be formalized,
structured and easy to access thanks to friendly user interfaces.
"
1363,"Do Mendeley readership counts help to filter highly cited WoS
  publications better than average citation impact of journals (JCS)?","  In this study, the academic status of users of scientific publications in
Mendeley is explored in order to analyse the usage pattern of Mendeley users in
terms of subject fields, citation and readership impact. The main focus of this
study is on studying the filtering capacity of Mendeley readership counts
compared to journal citation scores in detecting highly cited WoS publications.
Main finding suggests a faster reception of Mendeley readerships as compared to
citations across 5 major field of science. The higher correlations of
scientific users with citations indicate the similarity between reading and
citation behaviour among these users. It is confirmed that Mendeley readership
counts filter highly cited publications (PPtop 10%) better than journal
citation scores in all subject fields and by most of user types. This result
reinforces the potential role that Mendeley readerships could play for
informing scientific and alternative impacts.
"
1364,"How well developed are Altmetrics? Cross-disciplinary analysis of the
  presence of alternative metrics in scientific publications?","  In this paper an analysis of the presence and possibilities of altmetrics for
bibliometric and performance analysis is carried out. Using the web based tool
Impact Story, we have collected metrics for 20,000 random publications from the
Web of Science. We studied the presence and frequency of altmetrics in the set
of publications, across fields, document types and also through the years. The
main result of the study is that less than 50% of the publications have some
kind of altmetrics. The source that provides most metrics is Mendeley, with
metrics on readerships for around 37% of all the publications studied. Other
sources only provide marginal information. Possibilities and limitations of
these indicators are discussed and future research lines are outlined. We also
assessed the accuracy of the data retrieved through Impact Story by focusing on
the analysis of the accuracy of data from Mendeley; in a follow up study, the
accuracy and validity of other data sources not included here will be assessed.
"
1365,A review of the literature on citation impact indicators,"  Citation impact indicators nowadays play an important role in research
evaluation, and consequently these indicators have received a lot of attention
in the bibliometric and scientometric literature. This paper provides an
in-depth review of the literature on citation impact indicators. First, an
overview is given of the literature on bibliographic databases that can be used
to calculate citation impact indicators (Web of Science, Scopus, and Google
Scholar). Next, selected topics in the literature on citation impact indicators
are reviewed in detail. The first topic is the selection of publications and
citations to be included in the calculation of citation impact indicators. The
second topic is the normalization of citation impact indicators, in particular
normalization for field differences. Counting methods for dealing with
co-authored publications are the third topic, and citation impact indicators
for journals are the last topic. The paper concludes by offering some
recommendations for future research.
"
1366,Mining and Analyzing the Future Works in Scientific Articles,"  Future works in scientific articles are valuable for researchers and they can
guide researchers to new research directions or ideas. In this paper, we mine
the future works in scientific articles in order to 1) provide an insight for
future work analysis and 2) facilitate researchers to search and browse future
works in a research area. First, we study the problem of future work extraction
and propose a regular expression based method to address the problem. Second,
we define four different categories for the future works by observing the data
and investigate the multi-class future work classification problem. Third, we
apply the extraction method and the classification model to a paper dataset in
the computer science field and conduct a further analysis of the future works.
Finally, we design a prototype system to search and demonstrate the future
works mined from the scientific papers. Our evaluation results show that our
extraction method can get high precision and recall values and our
classification model can also get good results and it outperforms several
baseline models. Further analysis of the future work sentences also indicates
interesting results.
"
1367,Greek Astronomy PhDs: The last 200 years,"  We have recently compiled a database with all doctoral dissertations (PhDs)
completed in modern Greece (1837-2014), in the general area of astronomy and
astrophysics, as well as in space and ionospheric physics. A preliminary
statistical analysis of the data is presented, along with a discussion of the
general trends observed.
"
1368,"Evaluation of the citation matching algorithms of CWTS and iFQ in
  comparison to Web of Science","  The results of bibliometric studies provided by bibliometric research groups,
e.g. the Centre for Science and Technology Studies (CWTS) and the Institute for
Research Information and Quality Assurance (iFQ), are often used in the process
of research assessment. Their databases use Web of Science (WoS) citation data,
which they match according to their own matching algorithms - in the case of
CWTS for standard usage in their studies and in the case of iFQ on an
experimental basis. Since the problem of non-matched citations in WoS persists
because of inaccuracies in the references or inaccuracies introduced in the
data extraction process, it is important to ascertain how well these
inaccuracies are rectified in these citation matching algorithms. This paper
evaluates the algorithms of CWTS and iFQ in comparison to WoS in a quantitative
and a qualitative analysis. The analysis builds upon the methodology and the
manually verified corpus of a previous study. The algorithm of CWTS performs
best, closely followed by that of iFQ. The WoS algorithm still performs quite
well (F1 score: 96.41 percent), but shows deficits in matching references
containing inaccuracies. An additional problem is posed by incorrectly provided
cited reference information in source articles by WoS.
"
1369,S-index: Towards Better Metrics for Quantifying Research Impact,"  The ongoing growth in the volume of scientific literature available today
precludes researchers from efficiently discerning the relevant from irrelevant
content. Researchers are constantly interested in impactful papers, authors and
venues in their respective fields. Moreover, they are interested in the
so-called recent ""rising stars"" of these contexts which may lead to attractive
directions for future work, collaborations or impactful publication venues. In
this work, we address the problem of quantifying research impact in each of
these contexts, in order to better direct attention of researchers and
streamline the processes of comparison, ranking and evaluation of contribution.
Specifically, we begin by outlining intuitive underlying assumptions that
impact quantification methods should obey and evaluate when current
state-of-the-art methods fail to satisfy these properties. To this end, we
introduce the s-index metric which quantifies research impact through influence
propagation over a heterogeneous citation network. s-index is tailored from
these intuitive assumptions and offers a number of desirable qualities
including robustness, natural temporality and straightforward extensibility
from the paper impact to broader author and venue impact contexts. We evaluate
its effectiveness on the publicly available Microsoft Academic Search citation
graph with over 119 million papers and 1 billion citation edges with 103
million and 21 thousand associated authors and venues respectively.
"
1370,"Assessing evaluation procedures for individual researchers: the case of
  the Italian National Scientific Qualification","  The Italian National Scientific Qualification (ASN) was introduced as a
prerequisite for applying for tenured associate or full professor positions at
state-recognized universities. The ASN is meant to attest that an individual
has reached a suitable level of scientific maturity to apply for professorship
positions. A five member panel, appointed for each scientific discipline, is in
charge of evaluating applicants by means of quantitative indicators of impact
and productivity, and through an assessment of their research profile. Many
concerns were raised on the appropriateness of the evaluation criteria, and in
particular on the use of bibliometrics for the evaluation of individual
researchers. Additional concerns were related to the perceived poor quality of
the final evaluation reports. In this paper we assess the ASN in terms of
appropriateness of the applied methodology, and the quality of the feedback
provided to the applicants. We argue that the ASN is not fully compliant with
the best practices for the use of bibliometric indicators for the evaluation of
individual researchers; moreover, the quality of final reports varies
considerably across the panels, suggesting that measures should be put in place
to prevent sloppy practices in future ASN rounds.
"
1371,"Comparing the hierarchy of author given tags and repository given tags
  in a large document archive","  Folksonomies - large databases arising from collaborative tagging of items by
independent users - are becoming an increasingly important way of categorizing
information. In these systems users can tag items with free words, resulting in
a tripartite item-tag-user network. Although there are no prescribed relations
between tags, the way users think about the different categories presumably has
some built in hierarchy, in which more special concepts are descendants of some
more general categories. Several applications would benefit from the knowledge
of this hierarchy. Here we apply a recent method to check the differences and
similarities of hierarchies resulting from tags given by independent
individuals and from tags given by a centrally managed repository system. The
results from out method showed substantial differences between the lower part
of the hierarchies, and in contrast, a relatively high similarity at the top of
the hierarchies.
"
1372,Toward a new language of legal drafting,"  Lawyers should write in document markup language just like web developers,
digital publishers, scientists, and almost everyone else
"
1373,Analyzing research performance: proposition of a new complementary index,"  A researcher collaborating with many groups will normally have more papers
(and thus higher citations and $h$-index) than a researcher spending all
his/her time working alone or in a small group. While analyzing an author's
research merit, it is therefore not enough to consider only the collective
impact of the published papers, it is also necessary to quantify his/her share
in the impact. For this quantification, here I propose the $I$-index which is
defined as an author's percentage share in the total citations that his/her
papers have attracted. It is argued that this $I$-index does not directly
depend on the most of the subjective issues like an author's influence,
affiliation, seniority or career break. A simple application of the Central
Limit Theorem shows that, the scheme of equidistribution of credit among the
coauthors of a paper will give us the most probable value of the $I$-index
(with an associated small standard deviation which decreases with increasing
$h$-index). I show that the total citations ($N_c$), the $h$-index and the
$I$-index are three independent parameters (within their bounds), and together
they give a comprehensive idea of an author's overall research performance.
"
1374,Authorship Patterns in Computer Science Research in the Philippines,"  We studied patterns of authorship in computer science~(CS) research in the
Philippines by using data mining and graph theory techniques on archives of
scientific papers presented in the Philippine Computer Science Congresses from
2000 to 2010 involving 326~papers written by 605~authors. We inferred from
these archives various graphs namely, a paper--author bipartite graph, a
co-authorship graph, and two mixing graphs. Our results show that the
scientific articles by Filipino computer scientists were generated at a rate of
33~papers per year, while the papers were written by an average of 2.64~authors
(maximum=13). The frequency distribution of the number of authors per paper
follows a power-law with a power of $\varphi=-2.04$ ($R^2=0.71$). The number of
Filipino CS researchers increases at an annual rate of 60~new scientists. The
researchers have written an average of 1.42~papers (maximum=20) and have
collaborated with 3.70~other computer scientists (maximum=54). The frequency
distribution of the number of papers per author follows a power law with
$\varphi=-1.88$ ($R^2=0.83$). This distribution closely agrees with Lotka's
{\em law of scientific productivity} having $\varphi\approx -2$. The number of
co-authors per author also follows a power-law with $\varphi=-1.65$
($R^2=0.80$). These results suggest that most CS~papers in the country were
written by scientists who prefer to work alone or at most in small groups.
These also suggest that few papers were written by scientists who were involved
in large collaboration efforts. The productivity of the Philippines' CS
researchers, as measured by their number of papers, is positively correlated
with their participation in collaborative research efforts, as measured by
their number of co-authors (Pearson $r=0.7425$).
"
1375,Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment,"  The principle of peer review is central to the evaluation of research, by
ensuring that only high-quality items are funded or published. But peer review
has also received criticism, as the selection of reviewers may introduce biases
in the system. In 2014, the organizers of the ``Neural Information Processing
Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted
manuscripts (166 items) went through the review process twice. Arbitrariness
was measured as the conditional probability for an accepted submission to get
rejected if examined by the second committee. This number was equal to $60\%$,
for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian
analysis of those two numbers, by introducing a hidden parameter which measures
the probability that a submission meets basic quality criteria. The standard
quality criteria usually include novelty, clarity, reproducibility, correctness
and no form of misconduct, and are met by a large proportions of submitted
items. The Bayesian estimate for the hidden parameter was equal to $56\%$
($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result
suggested the total acceptance rate should be increased in order to decrease
arbitrariness estimates in future review processes.
"
1376,"Investigating the interplay between fundamentals of national research
  systems: performance, investments and international collaborations","  We discuss, at the macro-level of nations, the contribution of research
funding and rate of international collaboration to research performance, with
important implications for the science of science policy. In particular, we
cross-correlate suitable measures of these quantities with a
scientometric-based assessment of scientific success, studying both the average
performance of nations and their temporal dynamics in the space defined by
these variables during the last decade. We find significant differences among
nations in terms of efficiency in turning (financial) input into
bibliometrically measurable output, and we confirm that growth of international
collaboration positively correlate with scientific success, with significant
benefits brought by EU integration policies. Various geo-cultural clusters of
nations naturally emerge from our analysis. We critically discuss the possible
factors that potentially determine the observed patterns.
"
1377,The Common HOL Platform,"  The Common HOL project aims to facilitate porting source code and proofs
between members of the HOL family of theorem provers. At the heart of the
project is the Common HOL Platform, which defines a standard HOL theory and API
that aims to be compatible with all HOL systems. So far, HOL Light and hol90
have been adapted for conformance, and HOL Zero was originally developed to
conform. In this paper we provide motivation for a platform, give an overview
of the Common HOL Platform's theory and API components, and show how to adapt
legacy systems. We also report on the platform's successful application in the
hand-translation of a few thousand lines of source code from HOL Light to HOL
Zero.
"
1378,"Review times in peer review: quantitative analysis of editorial
  workflows","  We examine selected aspects of peer review and suggest possible improvements.
To this end, we analyse a dataset containing information about 300 papers
submitted to the Biochemistry and Biotechnology section of the Journal of the
Serbian Chemical Society. After separating the peer review process into stages
that each review has to go through, we use a weighted directed graph to
describe it in a probabilistic manner and test the impact of some modifications
of the editorial policy on the efficiency of the whole process.
"
1379,t factor: A metric for measuring impact on Twitter,"  Based on the definition of the well-known h index we propose a t factor for
measuring the impact of publications (and other entities) on Twitter. The new
index combines tweet and retweet data in a balanced way whereby retweets are
seen as data reflecting the impact of initial tweets. The t factor is defined
as follows: A unit (single publication, journal, researcher, research group
etc.) has factor t if t of its Nt tweets have at least t retweets each and the
other (Nt-t) tweets have <=t retweets each.
"
1380,Archiving Deferred Representations Using a Two-Tiered Crawling Approach,"  Web resources are increasingly interactive, resulting in resources that are
increasingly difficult to archive. The archival difficulty is based on the use
of client-side technologies (e.g., JavaScript) to change the client-side state
of a representation after it has initially loaded. We refer to these
representations as deferred representations. We can better archive deferred
representations using tools like headless browsing clients. We use 10,000 seed
Universal Resource Identifiers (URIs) to explore the impact of including
PhantomJS -- a headless browsing tool -- into the crawling process by comparing
the performance of wget (the baseline), PhantomJS, and Heritrix. Heritrix
crawled 2.065 URIs per second, 12.15 times faster than PhantomJS and 2.4 times
faster than wget. However, PhantomJS discovered 531,484 URIs, 1.75 times more
than Heritrix and 4.11 times more than wget. To take advantage of the
performance benefits of Heritrix and the URI discovery of PhantomJS, we
recommend a tiered crawling strategy in which a classifier predicts whether a
representation will be deferred or not, and only resources with deferred
representations are crawled with PhantomJS while resources without deferred
representations are crawled with Heritrix. We show that this approach is 5.2
times faster than using only PhantomJS and creates a frontier (set of URIs to
be crawled) 1.8 times larger than using only Heritrix.
"
1381,A New Ranking Scheme for the Institutional Scientific Performance,"  We propose a new performance indicator to evaluate the productivity of
research institutions by their disseminated scientific papers. The new quality
measure includes two principle components: the normalized impact factor of the
journal in which paper was published, and the number of citations received per
year since it was published. In both components, the scientific impacts are
weighted by the contribution of authors from the evaluated institution. As a
whole, our new metric, namely, the institutional performance score takes into
account both journal based impact and articles specific impacts. We apply this
new scheme to evaluate research output performance of Turkish institutions
specialized in astronomy and astrophysics in the period of 1998-2012. We
discuss the implications of the new metric, and emphasize the benefits of it
along with comparison to other proposed institutional performance indicators.
"
1382,"Excellence networks in science: A Web-based application based on
  Bayesian multilevel logistic regression (BMLR) for the identification of
  institutions collaborating successfully","  In this study we present an application which can be accessed via
www.excellence-networks.net and which represents networks of scientific
institutions worldwide. The application is based on papers (articles, reviews
and conference papers) published between 2007 and 2011. It uses (network) data,
on which the SCImago Institutions Ranking is based (Scopus data from Elsevier).
Using this data, institutional networks have been estimated with statistical
models (Bayesian multilevel logistic regression, BMLR) for a number of Scopus
subject areas. Within single subject areas, we have investigated and visualized
how successfully overall an institution (reference institution) has
collaborated (compared to all the other institutions in a subject area), and
with which other institutions (network institutions) a reference institution
has collaborated particularly successfully. The ""best paper rate""
(statistically estimated) was used as an indicator for evaluating the
collaboration success of an institution. This gives the proportion of highly
cited papers from an institution, and is considered generally as an indicator
for measuring impact in bibliometrics.
"
1383,"SFX Miscellaneous Free Ejournals Target: Usage Survey among the SFX
  Community","  The number of free or open access articles is increasing rapidly, and their
retrieval with library indexes and OpenURL link resolvers has been a challenge.
In June 2014, the SFX MISCELLANEOUS FREE EJOURNALS target contained more than
24,000 portfolios of all kinds. The SFX Knowledge Base Advisory Board (KBAB)
carried out an international survey to get an overview of the usage of this
target by the SFX community and to precisely identify what could be done to
improve it. The target is widely used among the community. However, many
respondents complained about three major problems: (a) incorrect links, (b)
full texts actually not free, and (c) incorrect or missing thresholds (years
and volumes information).
"
1384,nanopub-java: A Java Library for Nanopublications,"  The concept of nanopublications was first proposed about six years ago, but
it lacked openly available implementations. The library presented here is the
first one that has become an official implementation of the nanopublication
community. Its core features are stable, but it also contains unofficial and
experimental extensions: for publishing to a decentralized server network, for
defining sets of nanopublications with indexes, for informal assertions, and
for digitally signing nanopublications. Most of the features of the library can
also be accessed via an online validator interface.
"
1385,Quantifying the quality of peer reviewers through Zipf's law,"  This paper introduces a statistical and other analysis of peer reviewers in
order to approach their ""quality"" through some quantification measure, thereby
leading to some quality metrics. Peer reviewer reports for the Journal of the
Serbian Chemical Society are examined. The text of each report has first to be
adapted to word counting software in order to avoid jargon inducing confusion
when searching for the word frequency: e.g. C must be distinguished, depending
if it means Carbon or Celsius, etc. Thus, every report has to be carefully
""rewritten"". Thereafter, the quantity, variety and distribution of words are
examined in each report and compared to the whole set. Two separate months,
according when reports came in, are distinguished to observe any possible
hidden spurious effects. Coherence is found. An empirical distribution is
searched for through a Zipf-Pareto rank-size law. It is observed that peer
review reports are very far from usual texts in this respect. Deviations from
the usual (first) Zipf's law are discussed. A theoretical suggestion for the
""best (or worst) report"" and by extension ""good (or bad) reviewer"", within this
context, is provided from an entropy argument, through the concept of ""distance
to average"" behavior. Another entropy-based measure also allows to measure the
journal reviews (whence reviewers) for further comparison with other journals
through their own reviewer reports.
"
1386,"Semantic Publishing Challenge - Assessing the Quality of Scientific
  Output by Information Extraction and Interlinking","  The Semantic Publishing Challenge series aims at investigating novel
approaches for improving scholarly publishing using Linked Data technology. In
2014 we had bootstrapped this effort with a focus on extracting information
from non-semantic publications - computer science workshop proceedings volumes
and their papers - to assess their quality. The objective of this second
edition was to improve information extraction but also to interlink the 2014
dataset with related ones in the LOD Cloud, thus paving the way for
sophisticated end-user services.
"
1387,"Studies and analysis of reference management software: a literature
  review","  Reference management software is a well-known tool for scientific research
work. Since the 1980s, it has been the subject of reviews and evaluations in
library and information science literature. This paper presents a systematic
review of published studies that evaluate reference management software with a
comparative approach. The objective is to identify the types, models, and
evaluation criteria that authors have adopted, in order to determine whether
the methods used provide adequate methodological rigor and useful contributions
to the field of study.
"
1388,Ethnicity sensitive author disambiguation using semi-supervised learning,"  Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by the same person,
accounting for potential homonyms and/or synonyms. Among solutions to this
problem, digital libraries are increasingly offering tools for authors to
manually curate their publications and claim those that are theirs. Indirectly,
these tools allow for the inexpensive collection of large annotated training
data, which can be further leveraged to build a complementary automated
disambiguation system capable of inferring patterns for identifying
publications written by the same person. Building on more than 1 million
publicly released crowdsourced annotations, we propose an automated author
disambiguation solution exploiting this data (i) to learn an accurate
classifier for identifying coreferring authors and (ii) to guide the clustering
of scientific publications by distinct authors in a semi-supervised way. To the
best of our knowledge, our analysis is the first to be carried out on data of
this size and coverage. With respect to the state of the art, we validate the
general pipeline used in most existing solutions, and improve by: (i) proposing
phonetic-based blocking strategies, thereby increasing recall; and (ii) adding
strong ethnicity-sensitive features for learning a linkage function, thereby
tailoring disambiguation to non-Western author names whenever necessary.
"
1389,"Quantifying the impact of weak, strong, and super ties in scientific
  careers","  Scientists are frequently faced with the important decision to start or
terminate a creative partnership. This process can be influenced by strategic
motivations, as early career researchers are pursuers, whereas senior
researchers are typically attractors, of new collaborative opportunities.
Focusing on the longitudinal aspects of scientific collaboration, we analyzed
473 collaboration profiles using an ego-centric perspective which accounts for
researcher-specific characteristics and provides insight into a range of
topics, from career achievement and sustainability to team dynamics and
efficiency. From more than 166,000 collaboration records, we quantify the
frequency distributions of collaboration duration and tie-strength, showing
that collaboration networks are dominated by weak ties characterized by high
turnover rates. We use analytic extreme-value thresholds to identify a new
class of indispensable `super ties', the strongest of which commonly exhibit
>50% publication overlap with the central scientist. The prevalence of super
ties suggests that they arise from career strategies based upon cost, risk, and
reward sharing and complementary skill matching. We then use a combination of
descriptive and panel regression methods to compare the subset of publications
coauthored with a super tie to the subset without one, controlling for
pertinent features such as career age, prestige, team size, and prior group
experience. We find that super ties contribute to above-average productivity
and a 17% citation increase per publication, thus identifying these
partnerships - the analog of life partners - as a major factor in science
career development.
"
1390,Personalized Search,"  As the volume of electronically available information grows, relevant items
become harder to find. This work presents an approach to personalizing search
results in scientific publication databases. This work focuses on re-ranking
search results from existing search engines like Solr or ElasticSearch. This
work also includes the development of Obelix, a new recommendation system used
to re-rank search results. The project was proposed and performed at CERN,
using the scientific publications available on the CERN Document Server (CDS).
This work experiments with re-ranking using offline and online evaluation of
users and documents in CDS. The experiments conclude that the personalized
search result outperform both latest first and word similarity in terms of
click position in the search result for global search in CDS.
"
1391,"Provably Correct Systems: Community, connections, and citations","  The original European ESPRIT ProCoS I and II projects on Provably Correct
Systems} took place around a quarter of a century ago. Since then the legacy of
the initiative has spawned many researchers with careers in formal methods. One
of the leaders on the ProCoS projects was Ernst-R\""udiger Olderog. This paper
charts the influence of the ProCoS projects and the subsequent ProCoS-WG
Working Group, using Prof. Dr Olderog as an example. The community of
researchers surrounding an initiative such as ProCoS is considered in the
context of the social science concept of a Community of Practice (CoP) and the
collaborations undertaken through coauthorship of and citations to
publications. Consideration of citation metrics is also included.
"
1392,Empirical Big Data Research: A Systematic Literature Mapping,"  Background: Big Data is a relatively new field of research and technology,
and literature reports a wide variety of concepts labeled with Big Data. The
maturity of a research field can be measured in the number of publications
containing empirical results. In this paper we present the current status of
empirical research in Big Data. Method: We employed a systematic mapping method
with which we mapped the collected research according to the labels Variety,
Volume and Velocity. In addition, we addressed the application areas of Big
Data. Results: We found that 151 of the assessed 1778 contributions contain a
form of empirical result and can be mapped to one or more of the 3 V's and 59
address an application area. Conclusions: The share of publications containing
empirical results is well below the average compared to computer science
research as a whole. In order to mature the research on Big Data, we recommend
applying empirical methods to strengthen the confidence in the reported
results. Based on our trend analysis we consider Volume and Variety to be the
most promising uncharted area in Big Data.
"
1393,Accelerating Scientific Publication in Biology,"  Scientific publications enable results and ideas to be transmitted throughout
the scientific community. The number and type of journal publications also have
become the primary criteria used in evaluating career advancement. Our analysis
suggests that publication practices have changed considerably in the life
sciences over the past thirty years. More experimental data is now required for
publication, and the average time required for graduate students to publish
their first paper has increased and is approaching the desirable duration of
Ph.D. training. Since publication is generally a requirement for career
progression, schemes to reduce the time of graduate student and postdoctoral
training may be difficult to implement without also considering new mechanisms
for accelerating communication of their work. The increasing time to
publication also delays potential catalytic effects that ensue when many
scientists have access to new information. The time has come for life
scientists, funding agencies, and publishers to discuss how to communicate new
findings in a way that best serves the interests of the public and the
scientific community.
"
1394,"Emerging trends on the topic of Information Technology in the field of
  Educational Sciences: a bibliometric exploration","  The paper presents a bibliometric analysis on the topic of Information
Technology (IT) in the field of Educational Sciences, aimed at envisioning the
research emerging trends. The ERIC data base is used as a consultation source;
the results were subjected to productivity by authors, journals, and term
co-occurrence analysis indicators for the period 2009-2013. The productivity of
Computers & Education, and Turkish Online Journal of Educational
Technology-TOJET, as well as the preceding authors from Canada, have been
emphasized. The more used terms are the following: Information technology,
foreign countries, educational technology, technology integration, and student
attitudes. Researches performed here seem to have a largely qualitative
character, highlighting computers and internet as the mostly explored
technological objects. The largest subject matter trend refers to the
integration of IT in the higher education learning context, and its incidence
over the teaching methods.
"
1395,Collaborative Bibliographic System for Review/Survey Articles,"  This paper proposes a Bibliographic system intends to exchange bibliographic
information of survey/review articles by relying on Web service technology. It
allows researchers and university students to interact with system via single
service using platform-independent standard named Web service to add, search
and retrieve bibliographic information of review articles in various science
and technology fields and build-up a dedicated database for these articles in
each science and technology field. Additionally, different implementation
scenarios of the proposed system are presented and described, and rich features
that offered by such system are studied and described. However, this paper
explains the proposed system using computing area due to the existence of
detailed taxonomy of this area, which allows defining the system, their
functionalities and features provided. However, the proposed system is not only
confined to computing area, it can support any other science and technology
area without any need to modify this system.
"
1396,"Improvements in Google Scholar Citations are for the summer: creating an
  institutional affiliation link feature","  This report describes the feature introduced by Google to provide
standardized access to institutional affiliations within Google Scholar
Citations. First, this new tool is described, pointing out its main
characteristics and functioning. Next, the coverage and precision of the tool
are evaluated. Two special cases (Google Inc. and Spanish Universities) are
briefly treated with the purpose of illustrating some aspects about the
accuracy of the tool for the task of gathering authors within their appropriate
institution. Finally, some inconsistencies, errors and malfunctioning are
identified, categorized and described. The report finishes by providing some
suggestions to improve the feature. The general conclusion is that the
standardized institutional affiliation link provided by Google Scholar
Citations, despite working pretty well for a large number of institutions
(especially Anglo-Saxon universities) still has a number of shortcomings and
pitfalls which need to be addressed in order to make this authority control
tool fully useful worldwide, both for searching purposes and for metric tasks
"
1397,"Open Access and Discovery Tools: How do Primo Libraries Manage Green
  Open Access Collections?","  Scholarly Open Access repositories contain lots of treasures including rare
or otherwise unpublished materials and articles that scholars self-archive,
often as part of their institution's mandate. But it can be hard to discover
this material unless users know exactly where to look. Since the very
beginning, libraries have played a major role in supporting the OA movement.
Next to all services they can provide to support the deposit of research output
in the repositories, they can make Open Access materials widely discoverable by
their patrons through general search engines (Google, Bing...), specialized
search engines (like Google Scholar) and library discovery tools, thus
expanding their collection to include materials that they would not necessarily
pay for. In this paper, we intend to focus on two aspects regarding Open Access
and Primo discovery tool. In early 2013, Ex Libris Group started to add
institutional repositories to Primo Central Index (PCI), their mega-aggregation
of hundreds of millions of scholarly e-resources. After 2 years, it may be
interesting to take stock of the current situation of PCI regarding Open Access
institutional repositories. On basis of a survey to carry out among the Primo
community, the paper also shows how libraries using Primo discovery tool
integrate Green Open Access contents in their catalog. Two major ways are
possible for them: Firstly, they can directly harvest, index and manage any
repository in their Primo and display those free contents next to the more
traditional library collections; Secondly, if they are PCI subscribers, they
can quickly and easily activate any, if not all, of the Open Access
repositories contained PCI, making thus the contents of those directly
discoverable to their end users.
"
1398,"Integrating Research Data Management into Geographical Information
  Systems","  Ocean modelling requires the production of high-fidelity computational meshes
upon which to solve the equations of motion. The production of such meshes by
hand is often infeasible, considering the complexity of the bathymetry and
coastlines. The use of Geographical Information Systems (GIS) is therefore a
key component to discretising the region of interest and producing a mesh
appropriate to resolve the dynamics. However, all data associated with the
production of a mesh must be provided in order to contribute to the overall
recomputability of the subsequent simulation. This work presents the
integration of research data management in QMesh, a tool for generating meshes
using GIS. The tool uses the PyRDM library to provide a quick and easy way for
scientists to publish meshes, and all data required to regenerate them, to
persistent online repositories. These repositories are assigned unique
identifiers to enable proper citation of the meshes in journal articles.
"
1399,Measuring Verifiability in Online Information,"  The verifiability of online information is important, but difficult to assess
systematically. We examine verifiability in the case of Wikipedia, one of the
world's largest and most consulted online information sources. We extend prior
work about quality of Wikipedia articles, knowledge production, and sources to
consider the quality of Wikipedia references. We propose a multidimensional
measure of verifiability that takes into account technical accuracy and
practical accessibility of sources. We calculate article verifiability scores
for a sample of 5,000 articles and 295,800 citations, and compare differently
weighted models to illustrate effects of emphasizing particular elements of
verifiability over others. We find that, while the quality of references in the
overall sample is reasonably high, verifiability varies significantly by
article, particularly when emphasizing the use of standard digital identifiers
and taking into account the practical availability of referenced sources. We
discuss the implications of these findings for measuring verifiability in
online information more generally.
"
1400,Agent-based model for the h-index - Exact solution,"  The Hirsch's $h$-index is perhaps the most popular citation-based measure of
the scientific excellence. In 2013 G. Ionescu and B. Chopard proposed an
agent-based model for this index to describe a publications and citations
generation process in an abstract scientific community. With such an approach
one can simulate a single scientist's activity, and by extension investigate
the whole community of researchers. Even though this approach predicts quite
well the $h$-index from bibliometric data, only a solution based on simulations
was given. In this paper, we complete their results with exact, analytic
formulas. What is more, due to our exact solution we are able to simplify the
Ionescu-Chopard model which allows us to obtain a compact formula for
$h$-index. Moreover, a simulation study designed to compare both, approximated
and exact, solutions is included. The last part of this paper presents
evaluation of the obtained results on a real-word data set.
"
1401,More Precise Methods for National Research Citation Impact Comparisons,"  Governments sometimes need to analyse sets of research papers within a field
in order to monitor progress, assess the effect of recent policy changes, or
identify areas of excellence. They may compare the average citation impacts of
the papers by dividing them by the world average for the field and year. Since
citation data is highly skewed, however, simple averages may be too imprecise
to robustly identify differences within, rather than across, fields. In
response, this article introduces two new methods to identify national
differences in average citation impact, one based on linear modelling for
normalised data and the other using the geometric mean. Results from a sample
of 26 Scopus fields between 2009-2015 show that geometric means are the most
precise and so are recommended for smaller sample sizes, such as for individual
fields. The regression method has the advantage of distinguishing between
national contributions to internationally collaborative articles, but has
substantially wider confidence intervals than the geometric mean, undermining
its value for any except the largest sample sizes.
"
1402,"A Framework to Explore the Knowledge Structure of Multidisciplinary
  Research Fields","  Understanding emerging areas of a multidisciplinary research field is crucial
for researchers,policymakers and other stakeholders. For them a knowledge
structure based on longitudinal bibliographic data can be an effective
instrument. But with the vast amount of available online information it is
often hard to understand the knowledge structure for data. In this paper, we
present a novel approach for retrieving online bibliographic data and propose a
framework for exploring knowledge structure. We also present several
longitudinal analyses to interpret and visualize the last 20 years of published
obesity research data.
"
1403,"Analysis of the impact of studies published by Internext - Revista
  Eletr\^onica de Neg\'ocios Internacionais","  This paper presents a citation analysis of Internext-Review of International
Business to detect the impact caused by papers published for the period
2006-2013. The Publish or Perish (PoP) software is used, which retrieves
articles and citations from Google Scholar database. As part of the applied
indicators are: the distribution of authors by articles, citations per year,
citation vs. self-citation, journal's citable vs. non citable documents,
journal's cited vs. uncited documents, co-word analysis, and H Index. A total
of 131 articles were obtained for 153 citations made until June, 2014. Most
articles present multiple authorship. It is also detected an ascending line in
the citation. The Journal has very low levels of self-citation, showing that
most citing sources are Brazilian journals. The most cited articles have been
published in the early years (2006-2008); whose main topics are related with
the internationalization theory and strategy, the transaction analysis, and the
corporate governance. The Internext' H Index is 6 and the G Index is 9.
"
1404,"Exploration and Exploitation of Victorian Science in Darwin's Reading
  Notebooks","  Search in an environment with an uncertain distribution of resources involves
a trade-off between exploitation of past discoveries and further exploration.
This extends to information foraging, where a knowledge-seeker shifts between
reading in depth and studying new domains. To study this decision-making
process, we examine the reading choices made by one of the most celebrated
scientists of the modern era: Charles Darwin. From the full-text of books
listed in his chronologically-organized reading journals, we generate topic
models to quantify his local (text-to-text) and global (text-to-past) reading
decisions using Kullback-Liebler Divergence, a cognitively-validated,
information-theoretic measure of relative surprise. Rather than a pattern of
surprise-minimization, corresponding to a pure exploitation strategy, Darwin's
behavior shifts from early exploitation to later exploration, seeking unusually
high levels of cognitive surprise relative to previous eras. These shifts,
detected by an unsupervised Bayesian model, correlate with major intellectual
epochs of his career as identified both by qualitative scholarship and Darwin's
own self-commentary. Our methods allow us to compare his consumption of texts
with their publication order. We find Darwin's consumption more exploratory
than the culture's production, suggesting that underneath gradual societal
changes are the explorations of individual synthesis and discovery. Our
quantitative methods advance the study of cognitive search through a framework
for testing interactions between individual and collective behavior and between
short- and long-term consumption choices. This novel application of topic
modeling to characterize individual reading complements widespread studies of
collective scientific behavior.
"
1405,Mapping Technology Space by Normalizing Patent Networks,"  Technology is a complex system, with technologies relating to each other in a
space that can be mapped as a network. The technology network's structure can
reveal properties of technologies and of human behavior, if it can be mapped
accurately. Technology networks have been made from patent data, using several
measures of proximity. These measures, however, are influenced by factors of
the patenting system that do not reflect technologies or their proximity. We
introduce a method to precisely normalize out multiple impinging factors in
patent data and extract the true signal of technological proximity, by
comparing the empirical proximity measures with what they would be in random
situations that remove the impinging factors. With this method, we created
technology networks, using data from 3.9 million patents. After normalization,
different measures of proximity became more correlated with each other,
approaching a single dimension of technological proximity. The normalized
technology networks were sparse, with few pairs of technology domains being
significantly related. The normalized network corresponded with human behavior:
we analyzed the patenting histories of 2.8 million inventors and found they
were more likely to invent in two different technology domains if the pair was
closely related in the technology network. We also analyzed 250 thousand firms'
patents and found that, in contrast, firms' inventive activities were only
modestly associated with the technology network; firms' portfolios combined
pairs of technology domains about twice as often as inventors. These results
suggest that controlling for impinging factors provides meaningful measures of
technological proximity for patent-based mapping of the technology space, and
that this map can be used to aid in technology innovation planning and
management.
"
1406,"Analysis of a Planetary Scale Scientific Collaboration Dataset Reveals
  Novel Patterns","  Scientific collaboration networks are an important component of scientific
output and contribute significantly to expanding our knowledge and to the
economy and gross domestic product of nations. Here we examine a dataset from
the Mendeley scientific collaboration network. We analyze this data using a
combination of machine learning techniques and dynamical models. We find
interesting clusters of countries with different characteristics of
collaboration. Some of these clusters are dominated by developed countries that
have higher number of self connections compared with connections to other
countries. Another cluster is dominated by impoverished nations that have
mostly connections and collaborations with other countries but fewer self
connections. We also propose a complex systems dynamical model that explains
these characteristics. Our model explains how the scientific collaboration
networks of impoverished and developing nations change over time. We also find
interesting patterns in the behaviour of countries that may reflect past
foreign policies and contemporary geopolitics. Our model and analysis gives
insights and guidelines into how scientific development of developing countries
can be guided. This is intimately related to fostering economic development of
impoverished nations and creating a richer and more prosperous society.
"
1407,Fractional Authorship in Nuclear Physics,"  Large, multi-institutional groups or collaborations of scientists are engaged
in nuclear physics research projects, and the number of research facilities is
dwindling. These collaborations have their own authorship rules, and they
produce a large number of highly-cited papers. Multiple authorship of nuclear
physics publications creates a problem with the assessment of an individual
author's productivity relative to his/her colleagues and renders ineffective a
performance metrics solely based on annual publication and citation counts.
Many institutions are increasingly relying on the total number of first-author
papers; however, this approach becomes counterproductive for large research
collaborations with an alphabetical order of authors. A concept of fractional
authorship (the claiming of credit for authorship by more than one individual)
helps to clarify this issue by providing a more complete picture of research
activities. In the present work, nuclear physics fractional and total
authorships have been investigated using nuclear data mining techniques.
Historic total and fractional authorship averages have been extracted from the
Nuclear Science References (NSR) database, and the current range of fractional
contributions has been deduced. The results of this study and their
implications are discussed and conclusions presented.
"
1408,Replicability and the public/private divide,"  In a recent letter, Carlos Vilchez-Roman criticizes Bornmann et al. (2015)
for using data which cannot be reproduced without access to an in-house version
of the Web-of-Science (WoS) at the Max Planck Digital Libraries (MPDL, Munich).
We agree with the norm of replicability and therefore returned to our data. Is
the problem only a practical one of automation or does the in-house processing
add analytical value to the data? Is the newly emerging situation in any sense
different from a further professionalization of the field? In our opinion, a
political economy of science indicators has in the meantime emerged with a
competitive dynamic that affects the intellectual organization of the field.
"
1409,"Enduring Access to Rich Media Content: Understanding Use and Usability
  Requirements","  Through an NEH-funded initiative, Cornell University Library is creating a
technical, curatorial, and managerial framework for preserving access to
complex born-digital new media objects. The Library's Rose Goldsen Archive of
New Media Art provides the testbed for this project. This collection of complex
interactive born-digital artworks are used by students, faculty, and artists
from various disciplines. Interactive digital assets are far more complex to
preserve and manage than single uniform digital media files. The preservation
model developed will apply not merely to new media artworks, but to other rich
digital media environments. This article describes the project's findings and
discoveries, focusing on a user survey conducted with the aim of creating user
profiles and use cases for born-digital assets like those in the testbed
collection. The project's ultimate goal is to create a preservation and access
practice grounded in thorough and practical understanding of the
characteristics of digital objects and their access requirements, seen from the
perspectives of collection curators and users alike. We discuss how the survey
findings informed the development of an artist questionnaire to support
creation of user-centric and cost-efficient preservation strategies. Although
this project focuses on new media art, our methodologies and findings will
inform other kinds of complex born-digital collections.
"
1410,Sustainability: Scholarly Repository as an Enterprise,"  The expanding need for an open information sharing infrastructure to promote
scholarly communication led to the pioneering establishment of arXiv.org, now
maintained by the Cornell University Library. To be sustainable, the repository
requires careful, long term planning for services, management and funding. The
library is developing a sustainability model for arXiv.org, based on voluntary
contributions and the ongoing participation and support of 200 libraries and
research laboratories around the world. The sustainability initiative is based
on a membership model and builds on arXiv's technical, service, financial and
policy infrastructure. Five principles for sustainability drive development,
starting with deep integration into the scholarly community. Also key are a
clearly defined mandate and governance structure, a stable yet innovative
technology platform, systematic creation of content policies and strong
business planning strategies. Repositories like arXiv must consider usability
and life cycle alongside values and trends in scholarly communication. To
endure, they must also support and enhance their service by securing and
managing resources and demonstrating responsible stewardship.
"
1411,"Does Quantity Make a Difference? The importance of publishing many
  papers","  Do highly productive researchers have significantly higher probability to
produce top cited papers? Or does the increased productivity in science only
result in a sea of irrelevant papers as a perverse effect of competition and
the increased use of indicators for research evaluation and accountability
focus? We use a Swedish author disambiguated data set consisting of 48,000
researchers and their WoS-publications during the period of 2008 2011 with
citations until 2014 to investigate the relation between productivity and
production of highly cited papers. As the analysis shows, quantity does make a
difference.
"
1412,"Regional and Global Science: Latin American and Caribbean publications
  in the SciELO Citation Index and the Web of Science","  We compare the visibility of Latin American and Caribbean (LAC) publications
in the Core Collection indexes of the Web of Science (WoS)--Science Citation
Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation
Index--and the SciELO Citation Index (SciELO CI) which was integrated into the
larger WoS platform in 2014. The purpose of this comparison is to contribute to
our understanding of the communication of scientific knowledge produced in
Latin America and the Caribbean, and to provide some reflections on the
potential benefits of the articulation of regional indexing exercises into WoS
for a better understanding of geographic and disciplinary contributions. How is
the regional level of SciELO CI related to the global range of WoS? In WoS, LAC
authors are integrated at the global level in international networks, while
SciELO has provided a platform for interactions among LAC researchers. The
articulation of SciELO into WoS may improve the international visibility of the
regional journals, but at the cost of independent journal inclusion criteria.
"
1413,"A framework for the measurement and prediction of an individual
  scientist's performance","  Quantitative bibliometric indicators are widely used to evaluate the
performance of scientists. However, traditional indicators do not much rely on
the analysis of the processes intended to measure and the practical goals of
the measurement. In this study, I propose a simple framework to measure and
predict an individual researcher's scientific performance that takes into
account the main regularities of publication and citation processes and the
requirements of practical tasks. Statistical properties of the new indicator -
a scientist's personal impact rate - are illustrated by its application to a
sample of Estonian researchers.
"
1414,"Textual Analysis for Studying Chinese Historical Documents and Literary
  Novels","  We analyzed historical and literary documents in Chinese to gain insights
into research issues, and overview our studies which utilized four different
sources of text materials in this paper. We investigated the history of
concepts and transliterated words in China with the Database for the Study of
Modern China Thought and Literature, which contains historical documents about
China between 1830 and 1930. We also attempted to disambiguate names that were
shared by multiple government officers who served between 618 and 1912 and were
recorded in Chinese local gazetteers. To showcase the potentials and challenges
of computer-assisted analysis of Chinese literatures, we explored some
interesting yet non-trivial questions about two of the Four Great Classical
Novels of China: (1) Which monsters attempted to consume the Buddhist monk
Xuanzang in the Journey to the West (JTTW), which was published in the 16th
century, (2) Which was the most powerful monster in JTTW, and (3) Which major
role smiled the most in the Dream of the Red Chamber, which was published in
the 18th century. Similar approaches can be applied to the analysis and study
of modern documents, such as the newspaper articles published about the 228
incident that occurred in 1947 in Taiwan.
"
1415,"An indicator of journal impact that is based on calculating a journal's
  percentage of highly cited publications","  The two most used citation impact indicators in the assessment of scientific
journals are, nowadays, the impact factor and the h-index. However, both
indicators are not field normalized (vary heavily depending on the scientific
category) which makes them incomparable between categories. Furthermore, the
impact factor is not robust to the presence of articles with a large number of
citations, while the h-index depends on the journal size. These limitations are
very important when comparing journals of different sizes and categories. An
alternative citation impact indicator is the percentage of highly cited
articles in a journal. This measure is field normalized (comparable between
scientific categories), independent of the journal size and also robust to the
presence of articles with a high number of citations. This paper empirically
compares this indicator with the impact factor and the h-index, considering
different time windows and citation percentiles (levels of citation for
considering an article as highly cited compared to others in the same year and
category).
"
1416,"A Potential Approach to Overcome Data Limitation in Scientific
  Publication Recommendation","  Data are essential for the experiments of relevant scientific publication
recommendation methods but it is difficult to build ground truth data. A
naturally promising solution is using publications that are referenced by
researchers to build their ground truth data. Unfortunately, this approach has
not been explored in the literature, so its applicability is still a gap in our
knowledge. In this research, we systematically study this approach by
theoretical and empirical analyses. In general, the results show that this
approach is reasonable and has many advantages. However, the empirical analysis
shows both positive and negative results. We conclude that, in some situations,
this is a useful alternative approach toward overcoming data limitation. Based
on this approach, we build and publish a dataset in computer science domain to
help advancing other researches.
"
1417,The H-index Paradox: Your Coauthors Have a Higher H-index than You Do,"  One interesting phenomenon that emerges from the typical structure of social
networks is the friendship paradox. It states that your friends have on average
more friends than you do. Recent efforts have explored variations of it, with
numerous implications for the dynamics of social networks. However, the
friendship paradox and its variations consider only the topological structure
of the networks and neglect many other characteristics that are correlated with
node degree. In this article, we take the case of scientific collaborations to
investigate whether a similar paradox also arises in terms of a researcher's
scientific productivity as measured by her H-index. The H-index is a widely
used metric in academia to capture both the quality and the quantity of a
researcher's scientific output. It is likely that a researcher may use her
coauthors' H-indexes as a way to infer whether her own H-index is adequate in
her research area. Nevertheless, in this article, we show that the average
H-index of a researcher's coauthors is usually higher than her own H-index. We
present empirical evidence of this paradox and discuss some of its potential
consequences.
"
1418,"Bibliometric-Enhanced Information Retrieval: 3rd International BIR
  Workshop","  The BIR workshop brings together experts in Bibliometrics and Information
Retrieval. While sometimes perceived as rather loosely related, these research
areas share various interests and face similar challenges. Our motivation as
organizers of the BIR workshop stemmed from a twofold observation. First, both
communities only partly overlap, albeit sharing various interests. Second, it
will be profitable for both sides to tackle some of the emerging problems that
scholars face today when they have to identify relevant and high quality
literature in the fast growing number of electronic publications available
worldwide. Bibliometric techniques are not yet used widely to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. Information professionals working in libraries and archives, however,
are increasingly confronted with applying bibliometric techniques in their
services. The first BIR workshop in 2014 set the research agenda by introducing
each group to the other, illustrating state-of-the-art methods, reporting on
current research problems, and brainstorming about common interests. The second
workshop in 2015 further elaborated these themes. This third BIR workshop aims
to foster a common ground for the incorporation of bibliometric-enhanced
services into scholarly search engine interfaces. In particular we will address
specific communities, as well as studies on large, cross-domain collections
like Mendeley and ResearchGate. This third BIR workshop addresses explicitly
both scholarly and industrial researchers.
"
1419,"Comprehensive indicator comparisons intelligible to non-experts: The
  case of two SNIP versions","  A framework is proposed for comparing different types of bibliometric
indicators, introducing the notion of an Indicator Comparison Report. It
provides a comprehensive overview of the main differences and similarities of
indicators. The comparison shows both the strong points and the limitations of
each of the indicators at stake, rather than over-promoting one indicator and
ignoring the benefits of alternative constructs. It focuses on base notions,
assumptions, and application contexts, which makes it more intelligible to
non-experts. As an illustration, a comparison report is presented for the
original and the modified SNIP (Source Normalized Impact per Paper) indicator
of journal citation impact.
"
1420,"On full text download and citation distributions in scientific-scholarly
  journals","  A statistical analysis of full text downloads of articles in Elseviers
ScienceDirect covering all disciplines reveals large differences in download
frequencies, their skewness, and their correlation with Scopus-based citation
counts, between disciplines, journals, and document types. Download counts tend
to be two orders of magnitude higher and less skewedly distributed than
citations. A mathematical model based on the sum of two exponentials does not
adequately capture monthly download counts. The degree of correlation at the
article level within a journal is similar to that at the journal level in the
discipline covered by that journal, suggesting that the differences between
journals are to a large extent discipline specific. Despite the fact that in
all study journals download and citation counts per article positively
correlate, little overlap may exist between the set of articles appearing in
the top of the citation distribution and that with the most frequently
downloaded ones. Usage and citation leaks, bulk downloading, differences
between reader and author populations in a subject field, the type of document
or its content, differences in obsolescence patterns between downloads and
citations, different functions of reading and citing in the research process,
all provide possible explanations of differences between download and citation
distributions.
"
1421,Altmetrics as traces of the computerization of the research process,"  I propose a broad, multi-dimensional conception of altmetrics, namely as
traces of the computerization of the research process. Computerization should
be conceived in its broadest sense, including all recent developments in ICT
and software, taking place in society as a whole. I distinguish four aspects of
the research process: the collection of research data and development of
research methods; scientific information processing; communication and
organization; and, last but not least, research assessment. I will argue that
in each aspect, computerization plays a key role, and metrics are being
developed to describe this process. I propose to label the total collection of
such metrics as Altmetrics. I seek to provide a theoretical foundation of
altmetrics, based on notions developed by Michael Nielsen in his monograph
Reinventing Discovery: The New Era of Networked Science. Altmetrics can be
conceived as tools for the practical realization of the ethos of science and
scholarship in a computerized or digital age.
"
1422,A Historical Analysis of the Field of OR/MS using Topic Models,"  This study investigates the content of the published scientific literature in
the fields of operations research and management science (OR/MS) since the
early 1950s. Our study is based on 80,757 published journal abstracts from 37
of the leading OR/MS journals. We have developed a topic model, using Latent
Dirichlet Allocation (LDA), and extend this analysis to reveal the temporal
dynamics of the field, journals, and topics. Our analysis shows the generality
or specificity of each of the journals, and we identify groups of journals with
similar content, which are both consistent and inconsistent with intuition. We
also show how journals have become more or less unique in their scope. A more
detailed analysis of each journals' topics over time shows significant temporal
dynamics, especially for journals with niche content. This study presents an
observational, yet objective, view of the published literature from OR/MS that
would be of interest to authors, editors, journals, and publishers.
Furthermore, this work can be used by new entrants to the fields of OR/MS to
understand the content landscape, as a starting point for discussions and
inquiry of the field at large, or as a model for other fields to perform
similar analyses.
"
1423,"The Scaling Relationship between Citation-Based Performance and
  Scientific Collaboration in Natural Sciences","  The aim of this paper is to extend our knowledge about the power-law
relationship between citation-based performance and collaboration patterns for
papers of the Natural Sciences domain. We analyzed 829,924 articles that
received 16,490,346 citations. The number of articles published through
collaboration account for 89%. The citation-based performance and collaboration
patterns exhibit a power-law correlation with a scaling exponent of 1.20,
SD=0.07. We found that the Matthew effect is stronger for collaborated papers
than for single-authored. This means that the citations to a field research
areas articles increase 2.30 times each time it doubles the number of
collaborative papers. The scaling exponent for the power-law relationship for
single-authored papers was 0.85, SD=0.11. The citations to a field research
area single-authored articles increase 1.89 times each time the research area
doubles the number of non-collaborative papers.
"
1424,"Hierarchy of knowledge translation: from health problems to ad-hoc drug
  design","  An innovative approach to analyze the complexity of translating novel
molecular entities and nanomaterials into pharmaceutical alternatives (i.e.,
knowledge translation, KT) is discussed. First, some key concepts on the
organization and translation of the biomedical knowledge (paradigms, homophily,
power law distributions, hierarchy, modularity, and research fronts) are
reviewed. Then, we propose a model for the knowledge translation (KT) in Drug
Discovery that considers the complexity of interdisciplinary communication.
Specifically, we address two highly relevant aspects: 1) A successful KT
requires the emergence of organized bodies of inter-and transdisciplinary
research, and 2) The hierarchical and modular topological organization of these
bodies of knowledge. We focused on a set of previously-published studies on KT
which rely on a combination of network analysis and computer-assisted analysis
of the contents of scientific literature and patents. The selected studies
provide a duo of complementary perspectives: the demand of knowledge (cervical
cancer and Ebola hemorrhagic fever) and the supply of knowledge (liposomes and
nanoparticles to treat cancer and the paradigmatic Doxil, the first nanodrug to
be approved).
"
1425,"Prominent but Less Productive: The Impact of Interdisciplinarity on
  Scientists' Research","  Inter-disciplinary research (IDR) is being promoted by federal agencies and
universities nationwide because it presumably spurs transformative, innovative
science. In this paper we bring empirical data to assess whether IDR is indeed
beneficial, and whether costs accompany potential benefits.
"
1426,"The influence of time and discipline on the magnitude of correlations
  between citation counts and quality scores","  Although various citation-based indicators are commonly used to help research
evaluations, there are ongoing controversies about their value. In response,
they are often correlated with quality ratings or with other quantitative
indicators in order to partly assess their validity. When correlations are
calculated for sets of publications from multiple disciplines or years,
however, the magnitude of the correlation coefficient may be reduced, masking
the strength of the underlying correlation. In response, this article uses
simulations to systematically investigate the extent to which mixing years or
disciplines reduces correlations. The results show that mixing two sets of
articles with different correlation strengths can reduce the correlation for
the combined set to substantially below the average of the two. Moreover, even
mixing two sets of articles with the same correlation strength but different
mean citation counts can substantially reduce the correlation for the combined
set. The extent of the reduction in correlation also depends upon whether the
articles assessed have been pre-selected for being high quality and whether the
relationship between the quality ratings and citation counts is linear or
exponential. The results underline the importance of using homogeneous data
sets but also help to interpret correlation coefficients when this is
impossible.
"
1427,"Geometric journal impact factors correcting for individual highly cited
  articles","  Journal impact factors (JIFs) are widely used and promoted but have important
limitations. In particular, JIFs can be unduly influenced by individual highly
cited articles and hence are inherently unstable. A logical way to reduce the
impact of individual high citation counts is to use the geometric mean rather
than the standard mean in JIF calculations. Based upon journal rankings
2004-2014 in 50 sub-categories within 5 broad categories, this study shows that
journal rankings based on JIF variants tend to be more stable over time if the
geometric mean is used rather than the standard mean. The same is true for JIF
variants using Mendeley reader counts instead of citation counts. Thus,
although the difference is not large, the geometric mean is recommended instead
of the arithmetic mean for future JIF calculations. In addition, Mendeley
readership-based JIF variants are as stable as those using Scopus citations,
confirming the value of Mendeley readership as an academic impact indicator.
"
1428,Regression for citation data: An evaluation of different methods,"  Citations are increasingly used for research evaluations. It is therefore
important to identify factors affecting citation scores that are unrelated to
scholarly quality or usefulness so that these can be taken into account.
Regression is the most powerful statistical technique to identify these factors
and hence it is important to identify the best regression strategy for citation
data. Citation counts tend to follow a discrete lognormal distribution and, in
the absence of alternatives, have been investigated with negative binomial
regression. Using simulated discrete lognormal data (continuous lognormal data
rounded to the nearest integer) this article shows that a better strategy is to
add one to the citations, take their log and then use the general linear
(ordinary least squares) model for regression (e.g., multiple linear
regression, ANOVA), or to use the generalised linear model without the log.
Reasonable results can also be obtained if all the zero citations are
discarded, the log is taken of the remaining citation counts and then the
general linear model is used, or if the generalised linear model is used with
the continuous lognormal distribution. Similar approaches are recommended for
altmetric data, if it proves to be lognormally distributed.
"
1429,Distributions for cited articles from individual subjects and years,"  The citations to a set of academic articles are typically unevenly shared,
with many articles attracting few citations and few attracting many. It is
important to know more precisely how citations are distributed in order to help
statistical analyses of citations, especially for sets of articles from a
single discipline and a small range of years, as normally used for research
evaluation. This article fits discrete versions of the power law, the lognormal
distribution and the hooked power law to 20 different Scopus categories, using
citations to articles published in 2004 and ignoring uncited articles. The
results show that, despite its popularity, the power law is not a suitable
model for collections of articles from a single subject and year, even for the
purpose of estimating the slope of the tail of the citation data. Both the
hooked power law and the lognormal distributions fit best for some subjects but
neither is a universal optimal choice and parameter estimates for both seem to
be unreliable. Hence only the hooked power law and discrete lognormal
distributions should be considered for subject-and-year-based citation analysis
in future and parameter estimates should always be interpreted cautiously.
"
1430,National research impact indicators from Mendeley readers,"  National research impact indicators derived from citation counts are used by
governments to help assess their national research performance and to identify
the effect of funding or policy changes. Citation counts lag research by
several years, however, and so their information is somewhat out of date. Some
of this lag can be avoided by using readership counts from the social reference
sharing site Mendeley because these accumulate more quickly than citations.
This article introduces a method to calculate national research impact
indicators from Mendeley, using citation counts from older time periods to
partially compensate for international biases in Mendeley readership. A
refinement to accommodate recent national changes in Mendeley uptake makes
little difference, despite being theoretically more accurate. The Mendeley
patterns using the methods broadly reflect the results from similar
calculations with citations and seem to reflect impact trends about a year
earlier. Nevertheless, the reasons for the differences between the indicators
from the two data sources are unclear.
"
1431,"Measuring Metrics - A forty year longitudinal cross-validation of
  citations, downloads, and peer review in Astrophysics","  Citation measures, and newer altmetric measures such as downloads are now
commonly used to inform personnel decisions. How well do or can these measures
measure or predict the past, current of future scholarly performance of an
individual? Using data from the Smithsonian/NASA Astrophysics Data System we
analyze the publication, citation, download, and distinction histories of a
cohort of 922 individuals who received a U.S. PhD in astronomy in the period
1972-1976. By examining the same and different measures at the same and
different times for the same individuals we are able to show the capabilities
and limitations of each measure. Because the distributions are lognormal
measurement uncertainties are multiplicative; we show that in order to state
with 95% confidence that one person's citations and/or downloads are
significantly higher than another person's, the log difference in the ratio of
counts must be at least 0.3 dex, which corresponds to a multiplicative factor
of two.
"
1432,Quantifying the Cognitive Extent of Science,"  While the modern science is characterized by an exponential growth in
scientific literature, the increase in publication volume clearly does not
reflect the expansion of the cognitive boundaries of science. Nevertheless,
most of the metrics for assessing the vitality of science or for making funding
and policy decisions are based on productivity. Similarly, the increasing level
of knowledge production by large science teams, whose results often enjoy
greater visibility, does not necessarily mean that ""big science"" leads to
cognitive expansion. Here we present a novel, big-data method to quantify the
extents of cognitive domains of different bodies of scientific literature
independently from publication volume, and apply it to 20 million articles
published over 60-130 years in physics, astronomy, and biomedicine. The method
is based on the lexical diversity of titles of fixed quotas of research
articles. Owing to large size of quotas, the method overcomes the inherent
stochasticity of article titles to achieve <1% precision. We show that the
periods of cognitive growth do not necessarily coincide with the trends in
publication volume. Furthermore, we show that the articles produced by larger
teams cover significantly smaller cognitive territory than (the same quota of)
articles from smaller teams. Our findings provide a new perspective on the role
of small teams and individual researchers in expanding the cognitive boundaries
of science. The proposed method of quantifying the extent of the cognitive
territory can also be applied to study many other aspects of ""science of
science.""
"
1433,"The Distribution of the Asymptotic Number of Citations to Sets of
  Publications by a Researcher or From an Academic Department Are Consistent
  With a Discrete Lognormal Model","  How to quantify the impact of a researcher's or an institution's body of work
is a matter of increasing importance to scientists, funding agencies, and
hiring committees. The use of bibliometric indicators, such as the h-index or
the Journal Impact Factor, have become widespread despite their known
limitations. We argue that most existing bibliometric indicators are
inconsistent, biased, and, worst of all, susceptible to manipulation. Here, we
pursue a principled approach to the development of an indicator to quantify the
scientific impact of both individual researchers and research institutions
grounded on the functional form of the distribution of the asymptotic number of
citations. We validate our approach using the publication records of 1,283
researchers from seven scientific and engineering disciplines and the chemistry
departments at the 106 U.S. research institutions classified as ""very high
research activity"". Our approach has three distinct advantages. First, it
accurately captures the overall scientific impact of researchers at all career
stages, as measured by asymptotic citation counts. Second, unlike other
measures, our indicator is resistant to manipulation and rewards publication
quality over quantity. Third, our approach captures the time-evolution of the
scientific impact of research institutions.
"
1434,"Large-Scale Analysis of the Accuracy of the Journal Classification
  Systems of Web of Science and Scopus","  Journal classification systems play an important role in bibliometric
analyses. The two most important bibliographic databases, Web of Science and
Scopus, each provide a journal classification system. However, no study has
systematically investigated the accuracy of these classification systems. To
examine and compare the accuracy of journal classification systems, we define
two criteria on the basis of direct citation relations between journals and
categories. We use Criterion I to select journals that have weak connections
with their assigned categories, and we use Criterion II to identify journals
that are not assigned to categories with which they have strong connections. If
a journal satisfies either of the two criteria, we conclude that its assignment
to categories may be questionable. Accordingly, we identify all journals with
questionable classifications in Web of Science and Scopus. Furthermore, we
perform a more in-depth analysis for the field of Library and Information
Science to assess whether our proposed criteria are appropriate and whether
they yield meaningful results. It turns out that according to our
citation-based criteria Web of Science performs significantly better than
Scopus in terms of the accuracy of its journal classification system.
"
1435,Ranking scientists,"  Currently the ranking of scientists is based on the $h$-index, which is
widely perceived as an imprecise and simplistic though still useful metric. We
find that the $h$-index actually favours modestly performing researchers and
propose a simple criterion for proper ranking.
"
1436,"Mining Local Gazetteers of Literary Chinese with CRF and Pattern based
  Methods for Biographical Information in Chinese History","  Person names and location names are essential building blocks for identifying
events and social networks in historical documents that were written in
literary Chinese. We take the lead to explore the research on algorithmically
recognizing named entities in literary Chinese for historical studies with
language-model based and conditional-random-field based methods, and extend our
work to mining the document structures in historical documents. Practical
evaluations were conducted with texts that were extracted from more than 220
volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single
most important collection that contains information about officers who served
in local government in Chinese history. Our methods performed very well on
these realistic tests. Thousands of names and addresses were identified from
the texts. A good portion of the extracted names match the biographical
information currently recorded in the China Biographical Database (CBDB) of
Harvard University, and many others can be verified by historians and will
become as new additions to CBDB.
"
1437,"Color Aesthetics and Social Networks in Complete Tang Poems:
  Explorations and Discoveries","  The Complete Tang Poems (CTP) is the most important source to study Tang
poems. We look into CTP with computational tools from specific linguistic
perspectives, including distributional semantics and collocational analysis.
From such quantitative viewpoints, we compare the usage of ""wind"" and ""moon"" in
the poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,
and play a crucial role in the imageries of poems. Thus, words for colors are
studied, and ""white"" is the main focus because it is the most frequent color in
CTP. We also explore some cases of using colored words in antithesis pairs that
were central for fostering the imageries of the poems. CTP also contains useful
historical information, and we extract person names in CTP to study the social
networks of the Tang poets. Such information can then be integrated with the
China Biographical Database of Harvard University.
"
1438,ExpertSeer: a Keyphrase Based Expert Recommender for Digital Libraries,"  We describe ExpertSeer, a generic framework for expert recommendation based
on the contents of a digital library. Given a query term q, ExpertSeer
recommends experts of q by retrieving authors who published relevant papers
determined by related keyphrases and the quality of papers. The system is based
on a simple yet effective keyphrase extractor and the Bayes' rule for expert
recommendation. ExpertSeer is domain independent and can be applied to
different disciplines and applications since the system is automated and not
tailored to a specific discipline. Digital library providers can employ the
system to enrich their services and organizations can discover experts of
interest within an organization. To demonstrate the power of ExpertSeer, we
apply the framework to build two expert recommender systems. The first, CSSeer,
utilizes the CiteSeerX digital library to recommend experts primarily in
computer science. The second, ChemSeer, uses publicly available documents from
the Royal Society of Chemistry (RSC) to recommend experts in chemistry. Using
one thousand computer science terms as benchmark queries, we compared the top-n
experts (n=3, 5, 10) returned by CSSeer to two other expert recommenders --
Microsoft Academic Search and ArnetMiner -- and a simulator that imitates the
ranking function of Google Scholar. Although CSSeer, Microsoft Academic Search,
and ArnetMiner mostly return prestigious researchers who published several
papers related to the query term, it was found that different expert
recommenders return moderately different recommendations. To further study
their performance, we obtained a widely used benchmark dataset as the ground
truth for comparison. The results show that our system outperforms Microsoft
Academic Search and ArnetMiner in terms of Precision-at-k (P@k) for k=3, 5, 10.
We also conducted several case studies to validate the usefulness of our
system.
"
1439,The data sharing advantage in astrophysics,"  We present here evidence for the existence of a citation advantage within
astrophysics for papers that link to data. Using simple measures based on
publication data from NASA Astrophysics Data System we find a citation
advantage for papers with links to data receiving on the average significantly
more citations per paper than papers without links to data. Furthermore, using
INSPEC and Web of Science databases we investigate whether either papers of an
experimental or theoretical nature display different citation behavior.
"
1440,"Co-word Maps and Topic Modeling: A Comparison Using Small and
  Medium-Sized Corpora (n < 1000)","  Induced by ""big data,"" ""topic modeling"" has become an attractive alternative
to mapping co-words in terms of co-occurrences and co-absences using network
techniques. Does topic modeling provide an alternative for co-word mapping in
research practices using moderately sized document collections? We return to
the word/document matrix using first a single text with a strong argument (""The
Leiden Manifesto"") and then upscale to a sample of moderate size (n = 687) to
study the pros and cons of the two approaches in terms of the resulting
possibilities for making semantic maps that can serve an argument. The results
from co-word mapping (using two different routines) versus topic modeling are
significantly uncorrelated. Whereas components in the co-word maps can easily
be designated, the topic models provide sets of words that are very differently
organized. In these samples, the topic models seem to reveal similarities other
than semantic ones (e.g., linguistic ones). In other words, topic modeling does
not replace co-word mapping in small and medium-sized sets; but the paper
leaves open the possibility that topic modeling would work well for the
semantic mapping of large sets.
"
1441,An Index for SSRN Downloads,"  We propose a new index to quantify SSRN downloads. Unlike the SSRN downloads
rank, which is based on the total number of an author's SSRN downloads, our
index also reflects the author's productivity by taking into account the
download numbers for the papers. Our index is inspired by - but is not the same
as - Hirsch's h-index for citations, which cannot be directly applied to SSRN
downloads. We analyze data for about 30,000 authors and 367,000 papers. We find
a simple empirical formula for the SSRN author rank via a Gaussian function of
the log of the number of downloads.
"
1442,Context Sensitive Article Ranking with Citation Context Analysis,"  It is hard to detect important articles in a specific context. Information
retrieval techniques based on full text search can be inaccurate to identify
main topics and they are not able to provide an indication about the importance
of the article. Generating a citation network is a good way to find most
popular articles but this approach is not context aware.
  The text around a citation mark is generally a good summary of the referred
article. So citation context analysis presents an opportunity to use the wisdom
of crowd for detecting important articles in a context sensitive way. In this
work, we analyze citation contexts to rank articles properly for a given topic.
The model proposed uses citation contexts in order to create a directed and
weighted citation network based on the target topic. We create a directed and
weighted edge between two articles if citation context contains terms related
with the target topic. Then we apply common ranking algorithms in order to find
important articles in this newly created network. We showed that this method
successfully detects a good subset of most prominent articles in a given topic.
The biggest contribution of this approach is that we are able to identify
important articles for a given search term even though these articles do not
contain this search term. This technique can be used in other linked documents
including web pages, legal documents, and patents.
"
1443,"Which type of citation analysis generates the most accurate taxonomy of
  scientific and technical knowledge?","  In 1965, Derek de Solla Price foresaw the day when a citation-based taxonomy
of science and technology would be delineated and correspondingly used for
science policy. A taxonomy needs to be comprehensive and accurate if it is to
be useful for policy making, especially now that policy makers are utilizing
citation-based indicators to evaluate people, institutions and laboratories.
Determining the accuracy of a taxonomy, however, remains a challenge. Previous
work on the accuracy of partition solutions is sparse, and the results of those
studies, while useful, have not been definitive. In this study we compare the
accuracies of topic-level taxonomies based on the clustering of documents using
direct citation, bibliographic coupling, and co-citation. Using a set of new
gold standards - articles with at least 100 references - we find that direct
citation is better at concentrating references than either bibliographic
coupling or co-citation. Using the assumption that higher concentrations of
references denote more accurate clusters, direct citation thus provides a more
accurate representation of the taxonomy of scientific and technical knowledge
than either bibliographic coupling or co-citation. We also find that
discipline-level taxonomies based on journal schema are highly inaccurate
compared to topic-level taxonomies, and recommend against their use.
"
1444,Author Evaluation Based on H-index and Citation Response,"  An accurate and fair assessment of the efficiency and impact of scientific
work is, despite a lot of recent research effort, still an open problem. The
measurement of quality and success of individual scientists and research groups
can be approached from many different directions, none of which is universal. A
reason for this is inherently different behavior of different scientists within
the global research community. A complex evaluation of ones publication
activities requires a careful consideration of a wide variety of factors. The
well-known H-index is one of the most used bibliometric indices. Despite its
many imperfections, its simplicity and ease of interpretation make it a popular
scientometric method. This short paper uses the ideas behind the H-index ~to
analyze communities of authors who cite publishing scientists. A new author
evaluation measure named aH-index is proposed, and intuitive interpretations of
its properties and semantics are presented. Preliminary experiments with
authors with high H-index active in the area of computer science are presented
to demonstrate the properties of the proposed measure.
"
1445,"HistComp : bibliographic analysis and visualization of 'The Biological
  Bulletin'","  A collection of citation data, the HistComp, is available from the Internet
as a database of examples of real life citation networks. The purposes of this
approach is the analysis of these citation networks on learned literature by
presenting its typical steps and results. We have selected the bibliographic
insights into the ""The Biological Bulletin"", the journal published since 1897
by the Woods Hole Marine Biological Laboratory. Since the bibliographic
networks tend to be very scattered, their visualization requires of criteria of
convergence. To simplify, the main features in such a structure should include
the survey for authoritative sources in the hyperlinked environment and the
identification of thematic areas. By avoiding excessive loose connections and
too dense clustered layouts to be useful, a smooth presentation is obtained by
graphically depicting the citation patterns. HistComp computes 8884 articles
published by 'The Biological Bulletin' between 1945-2003. A two-dimensional
positioning of these papers that represent the extent of their bibliographic
coupling and co-citation is offered as a histograph. The criteria to construct
it is the adequateness of the visualization relative to the 8884 data set. The
spatial representation obtained optimizes the identification of the clusters or
topic areas. The thematic importance of marine science involves its
participation in 7 of the 7 presenting clusters. The mainstream subjects were
crustaceans and echinoderms, with some 60% of the material presented in the
graph. But sea anemone, with about 16% of the total, remains as the best
visualized topical area. A perspective of the highly relevant papers is readily
confirmed by the visual inspection of width of the glyphs used for nodes
representation. For user interaction, HistComp employs mouse-over labels.
"
1446,"Quantifying the evolution of a scientific topic: reaction of the
  academic community to the Chornobyl disaster","  We analyze the reaction of academic communities to a particular urgent topic
which abruptly arises as a scientific problem. To this end, we have chosen the
disaster that occurred in 1986 in Chornobyl (Chernobyl), Ukraine, considered as
one of the most devastating nuclear power plant accidents in history. The
academic response is evaluated using scientific-publication data concerning the
disaster using the Scopus database to present the picture on an international
scale and the bibliographic database ""Ukrainika naukova"" to consider it on a
national level. We measured distributions of papers in different scientific
fields, their growth rates and properties of co-authorship networks. {The
elements of descriptive statistics and the tools of the complex network theory
are used to highlight the interdisciplinary as well as international effects.}
Our analysis allows to compare contributions of the international community to
Chornobyl-related research as well as integration of Ukraine in the
international research on this subject. Furthermore, the content analysis of
titles and abstracts of the publications allowed to detect the most important
terms used for description of Chornobyl-related problems.
"
1447,Ranking library materials,"  Purpose: This paper discusses ranking factors suitable for library materials
and shows that ranking in general is a complex process and that ranking for
library materials requires a variety of techniques.
Design/methodology/approach: The relevant literature is reviewed to provide a
systematic overview of suitable ranking factors. The discussion is based on an
overview of ranking factors used in Web search engines. Findings: While there
are a wide variety of ranking factors applicable to library materials, todays
library systems use only some of them. When designing a ranking component for
the library catalogue, an individual weighting of applicable factors is
necessary. Research limitations/applications: While this article discusses
different factors, no particular ranking formula is given. However, this
article presents the argument that such a formula must always be individual to
a certain use case. Practical implications: The factors presented can be
considered when designing a ranking component for a librarys search system or
when discussing such a project with an ILS vendor. Originality/value: This
paper is original in that it is the first to systematically discuss ranking of
library materials based on the main factors used by Web search engines.
"
1448,Using Search Engine Technology to Improve Library Catalogs,"  This chapter outlines how search engine technology can be used in online
public access library catalogs (OPACs) to help improve users experiences, to
identify users intentions, and to indicate how it can be applied in the library
context, along with how sophisticated ranking criteria can be applied to the
online library catalog. A review of the literature and current OPAC
developments form the basis of recommendations on how to improve OPACs.
Findings were that the major shortcomings of current OPACs are that they are
not sufficiently user-centered and that their results presentations lack
sophistication. Further, these shortcomings are not addressed in current 2.0
developments. It is argued that OPAC development should be made search-centered
before additional features are applied. While the recommendations on ranking
functionality and the use of user intentions are only conceptual and not yet
applied to a library catalogue, practitioners will find recommendations for
developing better OPACs in this chapter. In short, readers will find a
systematic view on how the search engines strengths can be applied to improving
libraries online catalogs.
"
1449,"Google Scholar as a tool for discovering journal articles in library and
  information science","  Purpose: The purpose of this paper is to measure the coverage of Google
Scholar for the Library and Information Science (LIS) journal literature as
identified by a list of core LIS journals from a study by Schloegl and
Petschnig (2005).
  Methods: We checked every article from 35 major LIS journals from the years
2004 to 2006 for availability in Google Scholar (GS). We also collected
information on the type of availability-i.e., whether a certain article was
available as a PDF for a fee, as a free PDF, or as a preprint.
  Results: We found that only some journals are completely indexed by Google
Scholar, that the ratio of versions available depends on the type of publisher,
and that availability varies a lot from journal to journal. Google Scholar
cannot substitute for abstracting and indexing services in that it does not
cover the complete literature of the field. However, it can be used in many
cases to easily find available full texts of articles already found using
another tool.
  Originality/value: This study differs from other Google Scholar coverage
studies in that it takes into account not only whether an article is indexed in
GS at all, but also the type of availability.
"
1450,"National, disciplinary and temporal variations in the extent to which
  articles with more authors have more impact: Evidence from a geometric field
  normalised citation indicator","  The importance of collaboration in research is widely accepted, as is the
fact that articles with more authors tend to be more cited. Nevertheless,
although previous studies have investigated whether the apparent advantage of
collaboration varies by country, discipline, and number of co-authors, this
study introduces a more fine-grained method to identify differences: the
geometric Mean Normalized Citation Score (gMNCS). Based on comparisons between
disciplines, years and countries for two million journal articles, the average
citation impact of articles increases with the number of authors, even when
international collaboration is excluded. This apparent advantage of
collaboration varies substantially by discipline and country and changes a
little over time. Against the trend, however, in Russia solo articles have more
impact. Across the four broad disciplines examined, collaboration had by far
the strongest association with impact in the arts and humanities. Although
international comparisons are limited by the availability of systematic data
for author country affiliations, the new indicator is the most precise yet and
can give statistical evidence rather than estimates.
"
1451,Homophily and missing links in citation networks,"  Citation networks have been widely used to study the evolution of science
through the lenses of the underlying patterns of knowledge flows among academic
papers, authors, research sub-fields, and scientific journals. Here we focus on
citation networks to cast light on the salience of homophily, namely the
principle that similarity breeds connection, for knowledge transfer between
papers. To this end, we assess the degree to which citations tend to occur
between papers that are concerned with seemingly related topics or research
problems. Drawing on a large data set of articles published in the journals of
the American Physical Society between 1893 and 2009, we propose a novel method
for measuring the similarity between articles through the statistical
validation of the overlap between their bibliographies. Results suggest that
the probability of a citation made by one article to another is indeed an
increasing function of the similarity between the two articles. Our study also
enables us to uncover missing citations between pairs of highly related
articles, and may thus help identify barriers to effective knowledge flows. By
quantifying the proportion of missing citations, we conduct a comparative
assessment of distinct journals and research sub-fields in terms of their
ability to facilitate or impede the dissemination of knowledge. Findings
indicate that knowledge transfer seems to be more effectively facilitated by
journals of wide visibility, such as Physical Review Letters, than by
lower-impact ones. Our study has important implications for authors, editors
and reviewers of scientific journals, as well as public preprint repositories,
as it provides a procedure for recommending relevant yet missing references and
properly integrating bibliographies of papers.
"
1452,"Relative Citation Ratio (RCR): An empirical attempt to study a new
  field-normalized bibliometric indicator","  Hutchins, Yuan, M., and Santangelo (2015) proposed the Relative Citation
Ratio (RCR) as a new field-normalized impact indicator. This study investigates
the RCR by correlating it on the level of single publications with established
field-normalized indicators and assessments of the publications by peers. We
find that the RCR correlates highly with established field-normalized
indicators, but the correlation between RCR and peer assessments is only low to
medium.
"
1453,"The Journal Coverage of Web of Science and Scopus: a Comparative
  Analysis","  Bibliometric methods are used in multiple fields for a variety of purposes,
namely for research evaluation. Most bibliometric analyses have in common their
data sources: Thomson Reuters' Web of Science (WoS) and Elsevier's Scopus. This
research compares the journal coverage of both databases in terms of fields,
countries and languages, using Ulrich's extensive periodical directory as a
base for comparison. Results indicate that the use of either WoS or Scopus for
research evaluation may introduces biases that favor Natural Sciences and
Engineering as well as Biomedical Research to the detriment of Social Sciences
and Arts and Humanities. Similarly, English-language journals are
overrepresented to the detriment of other languages. While both databases share
these biases, their coverage differs substantially. As a consequence, the
results of bibliometric analyses may vary depending on the database used.
"
1454,"Sic Transit Gloria Manuscriptum: Two Views of the Aggregate Fate of
  Ancient Papers","  When PageRank began to be used for ranking in Web search, a concern soon
arose that older pages have an inherent --- and potentially unfair ---
advantage over emerging pages of high quality, because they have had more time
to acquire hyperlink citations. Algorithms were then proposed to compensate for
this effect. Curiously, in bibliometry, the opposite concern has often been
raised: that a growing body of recent papers crowds out older papers, resulting
in a collective amnesia in research communities, which potentially leads to
reinventions, redundancies, and missed opportunities to connect ideas. A recent
paper by Verstak et al. reported experiments on Google Scholar data, which
seemed to refute the amnesia, or aging, hypothesis. They claimed that more
recently written papers have a larger fraction of outbound citations targeting
papers that are older by a fixed number of years, indicating that ancient
papers are alive and well-loved and increasingly easily found, thanks in part
to Google Scholar. In this paper we show that the full picture is considerably
more nuanced. Specifically, the fate of a fixed sample of papers, as they age,
is rather different from what Verstak et al.'s study suggests: there is clear
and steady abandonment in favor of citations to newer papers. The two
apparently contradictory views are reconciled by the realization that, as time
passes, the number of papers older than a fixed number of years grows rapidly.
"
1455,"Academic research groups: evaluation of their quality and quality of
  their evaluation","  In recent years, evaluation of the quality of academic research has become an
increasingly important and influential business. It determines, often to a
large extent, the amount of research funding flowing into universities and
similar institutes from governmental agencies and it impacts upon academic
careers. Policy makers are becoming increasingly reliant upon, and influenced
by, the outcomes of such evaluations. In response, university managers are
increasingly attracted to simple indicators as guides to the dynamics of the
positions of their various institutions in league tables. However, these league
tables are frequently drawn up by inexpert bodies such as newspapers and
magazines, using rather arbitrary measures and criteria. Terms such as
""critical mass' and ""metrics"" are often bandied about without proper
understanding of what they actually mean. Rather than accepting the rise and
fall of universities, departments and individuals on a turbulent sea of
arbitrary measures, we suggest it is incumbent upon the scientific community
itself to clarify their nature. Here we report on recent attempts to do that by
properly defining critical mass and showing how group size influences research
quality. We also examine currently predominant metrics and show that these fail
as reliable indicators of group research quality.
"
1456,"GrantMed: a new, international system for tracking grants and funding
  trends in the life sciences","  Despite the success of PubMed and other search engines in managing the
massive volume of biomedical literature and the retrieval of individual
publications, grant-related data remains scattered and relatively inaccessible.
This is problematic, as project and funding data has significant analytical
value and could be integral to publication retrieval. Here, we introduce
GrantMed, a searchable international database of biomedical grants that
integrates some 20 million publications with the nearly 1.4 million research
projects and 650 billion dollars of funding that made them possible. For any
given topic in the life sciences, Grantmed provides instantaneous visualization
of the past 30 years of dollars spent and projects awarded, along with detailed
individual project descriptions, funding amounts, and links to investigators,
research organizations, and resulting publications. It summarizes trends in
funding and publication rates for areas of interest and merges data from
various national grant databases to create one international grant tracking
system. This information will benefit the research community and funding
entities alike. Users can view trends over time or current projects underway
and use this information to navigate the decision-making process in moving
forward. They can view projects prior to publication and records of previous
projects. Convenient access to this data for analytical purposes will be
beneficial in many ways, helping to prevent project overlap, reduce funding
redundancy, identify areas of success, accelerate dissemination of ideas, and
expose knowledge gaps in moving forward. It is our hope that this will be a
central resource for international life sciences research communities and the
funding organizations that support them, ultimately streamlining progress.
"
1457,"The cost of reading research. A study of Computer Science publication
  venues","  What does the cost of academic publishing look like to the common researcher
today? Our goal is to convey the current state of academic publishing,
specifically in regards to the field of computer science and provide analysis
and data to be used as a basis for future studies. We will focus on author and
reader costs as they are the primary points of interaction within the
publishing world. In this work, we restrict our focus to only computer science
in order to make the data collection more feasible (the authors are computer
scientists) and hope future work can analyze and collect data across all
academic fields.
"
1458,"Recommendations for the Technical Infrastructure for Standardized
  International Rights Statements","  This white paper is the product of a joint Digital Public Library of America
(DPLA)-Europeana working group organized to develop minimum rights statement
metadata standards for organizations that contribute to DPLA and Europeana.
This white paper deals specifically with the technical infrastructure of a
common namespace (rightsstatements.org) that hosts the rights statements to be
used by (at minimum) the DPLA and Europeana. These recommendations for a common
technical infrastructure for rights statements outline a simple, flexible, and
extensible framework to host the rights statements at rightsstatements.org.
This white paper specifically outlines the management of rights statements as
linked open data. The rights statements are published according to Best
Practices for Publishing RDF Vocabularies. They are encoded into
dereferenceable URIs, express further information encoded in RDF, and link to
existing vocabularies and standards. The rights statements adhere to
expressions of existing rights vocabularies. Furthermore the paper reviews the
publication and implementation to make the rights statements available through
human-readable web pages augmented with machine-readable formats.
"
1459,"Science Use in Regulatory Impact Analysis: The Effects of Political
  Attention and Controversy","  Scholars, policymakers, and research sponsors have long sought to understand
the conditions under which scientific research is used in the policymaking
process. Recent research has identified a resource that can be used to trace
the use of science across time and many policy domains. US federal agencies are
mandated by executive order to justify all economically significant regulations
by regulatory impact analyses (RIAs), in which they present evidence of the
scientific underpinnings and consequences of the proposed rule. To gain new
insight into when and how regulators invoke science in their policy
justifications, we ask: does the political attention and controversy
surrounding a regulation affect the extent to which science is utilized in
RIAs? We examine scientific citation activity in all 101 economically
significant RIAs from 2008-2012 and evaluate the effects of attention -- from
the public, policy elites and the media -- on the degree of science use in
RIAs. Our main finding is that regulators draw more heavily on scientific
research when justifying rules subject to a high degree of attention from
outside actors. These findings suggest that scientific research plays an
important role in the justification of regulations, especially those that are
highly salient to the public and other policy actors.
"
1460,"Using Google Ngram Viewer for Scientific Referencing and History of
  Science","  Today, several universal digital libraries exist such as Google Books,
Project Gutenberg, Internet Archive libraries, which possess texts from general
collections, and many other archives are available, concerning more specific
subjects. On the digitalized texts available from these libraries, we can
perform several analyses, from those typically used for time-series to those of
network theory. For what concerns time-series, an interesting tool provided by
Google Books exists, which can help us in bibliographical and reference
researches. This tool is the Ngram Viewer, based on yearly count of n-grams. As
we will show in this paper, although it seems suitable just for literary works,
it can be useful for scientific researches, not only for history of science,
but also for acquiring references often unknown to researchers.
"
1461,"Identifying potential breakthrough publications using refined citation
  analyses: Three related explorative approaches","  The article presents three advanced citation-based methods used to detect
potential breakthrough papers among very highly cited papers. We approach the
detection of such papers from three different perspectives in order to provide
different typologies of breakthrough papers. In all three cases we use the
classification of scientific publications developed at CWTS based on direct
citation relationships. This classification establishes clusters of papers at
three levels of aggregation. Papers are clustered based on their similar
citation orientations and it is assumed that they are focused on similar
research interests. We use the clustering as the context for detecting
potential breakthrough papers. We utilize the Characteristics Scores and Scales
(CSS) approach to partition citation distributions and implement a specific
filtering algorithm to sort out potential highly-cited followers, papers not
considered breakthroughs in themselves. After invoking thresholds and
filtering, three methods are explored: A very exclusive one where only the
highest cited paper in a micro-cluster is considered as a potential
breakthrough paper (M1); as well as two conceptually different methods, one
that detects potential breakthrough papers among the two percent highest cited
papers according to CSS (M2a), and finally a more restrictive version where, in
addition to the CSS two percent filter, knowledge diffusion is also taken in as
an extra parameter (M2b). The advance citation-based methods are explored and
evaluated using specifically validated publication sets linked to different
Danish funding instruments including centres of excellence.
"
1462,"A Comparative Study of Interdisciplinarity in Sciences in Brazil, South
  Korea, Turkey, and USA","  A comparative study is done of interdisciplinary citations in 2013 between
physics, chemistry, and molecular biology, in Brazil, South Korea, Turkey, and
USA. Several surprising conclusions emerge from our tabular and graphical
analysis: The cross-science citation rates are in general strikingly similar,
between Brazil, South Korea, Turkey, and USA. One apparent exception is the
comparatively more tenuous relation between molecular biology and physics in
Brazil and USA. Other slight exceptions are the higher amount of citing of
physicists by chemists in South Korea, of chemists by molecular biologists in
Turkey, and of molecular biologists by chemists in Brazil and USA. Chemists
are, by a sizable margin, the most cross-science citing scientists in this
group of three sciences. Physicist are, again by a sizable margin, the least
cross-science citing scientists in this group of three sciences. In all four
countries, the strongest cross-science citation is from chemistry to physics
and the weakest cross-science citation is from physics to molecular biology.
Our findings are consistent with a V-shaped backbone connectivity, as opposed
to a Delta connectivity, as also found in a previous study of earlier citation
years.
"
1463,"The precision of the arithmetic mean, geometric mean and percentiles for
  citation data: An experimental simulation modelling approach","  When comparing the citation impact of nations, departments or other groups of
researchers within individual fields, three approaches have been proposed:
arithmetic means, geometric means, and percentage in the top X%. This article
compares the precision of these statistics using 97 trillion experimentally
simulated citation counts from 6875 sets of different parameters (although all
having the same scale parameter) based upon the discretised lognormal
distribution with limits from 1000 repetitions for each parameter set. The
results show that the geometric mean is the most precise, closely followed by
the percentage of a country's articles in the top 50% most cited articles for a
field, year and document type. Thus the geometric mean citation count is
recommended for future citation-based comparisons between nations. The
percentage of a country's articles in the top 1% most cited is a particularly
imprecise indicator and is not recommended for international comparisons based
on individual fields. Moreover, whereas standard confidence interval formulae
for the geometric mean appear to be accurate, confidence interval formulae are
less accurate and consistent for percentile indicators. These recommendations
assume that the scale parameters of the samples are the same but the choice of
indicator is complex and partly conceptual if they are not.
"
1464,"Status Report of the DPHEP Collaboration: A Global Effort for
  Sustainable Data Preservation in High Energy Physics","  Data from High Energy Physics (HEP) experiments are collected with
significant financial and human effort and are mostly unique. An
inter-experimental study group on HEP data preservation and long-term analysis
was convened as a panel of the International Committee for Future Accelerators
(ICFA). The group was formed by large collider-based experiments and
investigated the technical and organizational aspects of HEP data preservation.
An intermediate report was released in November 2009 addressing the general
issues of data preservation in HEP and an extended blueprint paper was
published in 2012. In July 2014 the DPHEP collaboration was formed as a result
of the signature of the Collaboration Agreement by seven large funding agencies
(others have since joined or are in the process of acquisition) and in June
2015 the first DPHEP Collaboration Workshop and Collaboration Board meeting
took place.
  This status report of the DPHEP collaboration details the progress during the
period from 2013 to 2015 inclusive.
"
1465,Well-Stratified Linked Data for Well-Behaved Data Citation,"  In this paper we analyse the functional requirements of linked data citation
and identify a minimal set of operations and primitives needed to realize such
task. Citing linked data implies solving a series of data provenance issues and
finding a way to identify data subsets. Those two tasks can be handled defining
a simple type system inside data and verifying it with a type checker, which is
significantly less complex than interpreting reified RDF statements and can be
implemented in a non data invasive way. Finally we suggest that data citation
should be handled outside of the data, possibly with an ad-hoc language.
"
1466,"The Globalization of Academic Entrepreneurship? The Recent Growth
  (2009-2014) in University Patenting Decomposed","  The contribution of academia to US patents has become increasingly global.
Following a pause, with a relatively flat rate, from 1998 to 2008, the
long-term trend of university patenting rising as a share of all patenting has
resumed, driven by the internationalization of academic entrepreneurship and
the persistence of US university technology transfer. We disaggregate this
recent growth in university patenting at the US Patent and Trademark
Organization (USPTO) in terms of nations and patent classes. Foreign patenting
in the US has almost doubled during the period 2009-2014, mainly due to
patenting by universities in Taiwan, Korea, China, and Japan. These nations
compete with the US in terms of patent portfolios, whereas most European
countries--with the exception of the UK--have more specific portfolios, mainly
in the bio-medical fields. In the case of China, Tsinghua University holds 63%
of the university patents in USPTO, followed by King Fahd University with 55.2%
of the national portfolio.
"
1467,A Highly Literate Approach to Ontology Building,"  Ontologies present an attractive technology for describing bio-medicine,
because they can be shared, and have rich computational properties. However,
they lack the rich expressivity of English and fit poorly with the current
scientific ""publish or perish"" model. While, there have been attempts to
combine free text and ontologies, most of these perform \textit{post-hoc}
annotation of text. In this paper, we introduce our new environment which
borrows from literate programming, to allow an author to co-develop both text
and ontological description. We are currently using this environment to
document the Karyotype Ontology which allows rich descriptions of the
chromosomal complement in humans. We explore some of the advantages and
difficulties of this form of ontology development.
"
1468,"Towards Evaluation of Cultural-scale Claims in Light of Topic Model
  Sampling Effects","  Cultural-scale models of full text documents are prone to over-interpretation
by researchers making unintentionally strong socio-linguistic claims (Pechenick
et al., 2015) without recognizing that even large digital libraries are merely
samples of all the books ever produced. In this study, we test the sensitivity
of the topic models to the sampling process by taking random samples of books
in the Hathi Trust Digital Library from different areas of the Library of
Congress Classification Outline. For each classification area, we train several
topic models over the entire class with different random seeds, generating a
set of spanning models. Then, we train topic models on random samples of books
from the classification area, generating a set of sample models. Finally, we
perform a topic alignment between each pair of models by computing the
Jensen-Shannon distance (JSD) between the word probability distributions for
each topic. We take two measures on each model alignment: alignment distance
and topic overlap. We find that sample models with a large sample size
typically have an alignment distance that falls in the range of the alignment
distance between spanning models. Unsurprisingly, as sample size increases,
alignment distance decreases. We also find that the topic overlap increases as
sample size increases. However, the decomposition of these measures by sample
size differs by number of topics and by classification area. We speculate that
these measures could be used to find classes which have a common ""canon""
discussed among all books in the area, as shown by high topic overlap and low
alignment distance even in small sample sizes.
"
1469,"Does diversity of papers affect their citations? Evidence from American
  Physical Society Journals","  In this work, we study the correlation between interdisciplinarity of papers
within physical sciences and their citations by using meta data of articles
published in American Physical Society's Physical Review journals between 1985
to 2012. We use the Weitzman diversity index to measure the diversity of papers
and authors, exploiting the hierarchical structure of PACS (Physics and
Astronomy Classification Scheme) codes. We find that the fraction of authors
with high diversity is increasing with time, where as the fraction of least
diversity are decreasing, and moderate diversity authors have higher tendency
to switch over to other diversity groups. The diversity index of papers is
correlated with the citations they received in a given time period from their
publication year. Papers with lower and higher end of diversity index receive
lesser citations than the moderate diversity papers.
"
1470,Why scientific publications should be anonymous,"  Numerous studies have revealed biases within the scientific communication
system and across all scientific fields. For example, already prominent
researchers receive disproportional credit compared to their (almost) equally
qualified colleagues -- because of their prominence. However, none of those
studies has offered a solution as to how to decrease the incidence of these
biases. In this paper I argue that by publishing anonymously, we can decrease
the incidence of inaccurate heuristics in the current scientific communication
system. Specific suggestions are made as to how to implement the changes.
"
1471,A new methodology for comparing Google Scholar and Scopus,"  A new methodology is proposed for comparing Google Scholar (GS) with other
citation indexes. It focuses on the coverage and citation impact of sources,
indexing speed, and data quality, including the effect of duplicate citation
counts. The method compares GS with Elsevier's Scopus, and is applied to a
limited set of articles published in 12 journals from six subject fields, so
that its findings cannot be generalized to all journals or fields. The study is
exploratory, and hypothesis generating rather than hypothesis-testing. It
confirms findings on source coverage and citation impact obtained in earlier
studies. The ratio of GS over Scopus citation varies across subject fields
between 1.0 and 4.0, while Open Access journals in the sample show higher
ratios than their non-OA counterparts. The linear correlation between GS and
Scopus citation counts at the article level is high: Pearson's R is in the
range of 0.8-0.9. A median Scopus indexing delay of two months compared to GS
is largely though not exclusively due to missing cited references in articles
in press in Scopus. The effect of double citation counts in GS due to multiple
citations with identical or substantially similar meta-data occurs in less than
2 per cent of cases. Pros and cons of article-based and what is termed as
concept-based citation indexes are discussed.
"
1472,Quantifying Orphaned Annotations in Hypothes.is,"  Web annotation has been receiving increased attention recently with the
organization of the Open Annotation Collaboration and new tools for open
annotation, such as Hypothes.is. We investigate the prevalence of orphaned
annotations, where neither the live Web page nor an archived copy of the Web
page contains the text that had previously been annotated in the Hypothes.is
annotation system (containing 20,953 highlighted text annotations). We found
that about 22% of highlighted text annotations can no longer be attached to
their live Web pages. Unfortunately, only about 12% of these annotations can be
reattached using the holdings of current public web archives, leaving the
remaining 88% of these annotations orphaned. For those annotations that are
still attached, 53% are in danger of becoming orphans if the live Web page
changes. This points to the need for archiving the target of an annotation at
the time the annotation is created.
"
1473,"Using HistCite software to identify significant articles in subject
  searches of the Web of Science","  HistCite TM is a large-scale computer tool for mapping science. Its power of
visualization combines the production of historiographs on the basis of the
analysis of co-citations of documents, with the use of bibliometrics specific
indicators. The objective of this article is, to present the advantages of the
new bibliometrics configuration of HistCite TM (2004) when identifying
articles. The analysis of the histograms that produces HistCite TM , in terms
of cumulative advantage and aging of the citations. And the comparative study
of the results of HistCite TM , in its indicators of amplitude and recognition.
Also is examined its treatment of the sampling problems, by formalizing the
Kendall method of estimating the robust standard deviation.
"
1474,"Policy documents as sources for measuring societal impact: How often is
  climate change research mentioned in policy-related documents?","  In the current UK Research Excellence Framework (REF) and the Excellence in
Research for Australia (ERA) societal impact measurements are inherent parts of
the national evaluation systems. In this study, we deal with a relatively new
form of societal impact measurements. Recently, Altmetric - a start-up
providing publication level metrics - started to make data for publications
available which have been mentioned in policy documents. We regard this data
source as an interesting possibility to specifically measure the (societal)
impact of research. Using a comprehensive dataset with publications on climate
change as an example, we study the usefulness of the new data source for impact
measurement. Only 1.2% (n=2,341) out of 191,276 publications on climate change
in the dataset have at least one policy mention. We further reveal that papers
published in Nature and Science as well as from the areas ""Earth and related
environmental sciences"" and ""Social and economic geography"" are especially
relevant in the policy context. Given the low coverage of the climate change
literature in policy documents, this study can be only a first attempt to study
this new source of altmetric data. Further empirical studies are necessary in
upcoming years, because mentions in policy documents are of special interest in
the use of altmetric data for measuring target-oriented the broader impact of
research.
"
1475,"Comparison of full-text versus metadata searching in an institutional
  repository: Case study of the UNT Scholarly Works","  Authors in the library science field disagree about the importance of using
costly resources to create local metadata records, particularly for scholarly
materials that have full-text search alternatives. At the University of North
Texas (UNT) Libraries, we decided to test this concept by answering the
question: What percentage of search terms retrieved results based on full-text
versus metadata values for items in the UNT Scholarly Works institutional
repository? The analysis matched search query logs to indexes of the metadata
records and full text of the items in the collection. Results show the
distribution of item discoveries that were based on metadata exclusively, on
full text exclusively, and on the combination of both. This paper describes in
detail the methods and findings of this study.
"
1476,"A Triple Helix Model of Medical Innovation: Supply, Demand, and
  Technological Capabilities in terms of Medical Subject Headings","  We develop a model of innovation that enables us to trace the interplay among
three key dimensions of the innovation process: (i) demand of and (ii) supply
for innovation, and (iii) technological capabilities available to generate
innovation in the forms of products, processes, and services. Building on
triple helix research, we use entropy statistics to elaborate an indicator of
mutual information among these dimensions that can provide indication of
reduction of uncertainty. To do so, we focus on the medical context, where
uncertainty poses significant challenges to the governance of innovation. We
use the Medical Subject Headings (MeSH) of MEDLINE/PubMed to identify
publications classified within the categories ""Diseases"" (C), ""Drugs and
Chemicals"" (D), ""Analytic, Diagnostic, and Therapeutic Techniques and
Equipment"" (E) and use these as knowledge representations of demand, supply,
and technological capabilities, respectively. Three case-studies of medical
research areas are used as representative 'entry perspectives' of the medical
innovation process. These are: (i) human papilloma virus, (ii) RNA
interference, and (iii) magnetic resonance imaging. We find statistically
significant periods of synergy among demand, supply, and technological
capabilities (C-D-E) that point to three-dimensional interactions as a
fundamental perspective for the understanding and governance of the uncertainty
associated with medical innovation. Among the pairwise configurations in these
contexts, the demand-technological capabilities (C-E) provided the strongest
link, followed by the supply-demand (D-C) and the supply-technological
capabilities (D-E) channels.
"
1477,Improving Software Citation and Credit,"  The past year has seen movement on several fronts for improving software
citation, including the Center for Open Science's Transparency and Openness
Promotion (TOP) Guidelines, the Software Publishing Special Interest Group that
was started at January's AAS meeting in Seattle at the request of that
organization's Working Group on Astronomical Software, a Sloan-sponsored
meeting at GitHub in San Francisco to begin work on a cohesive research
software citation-enabling platform, the work of Force11 to ""transform and
improve"" research communication, and WSSSPE's ongoing efforts that include
software publication, citation, credit, and sustainability.
  Brief reports on these efforts were shared at the BoF, after which
participants discussed ideas for improving software citation, generating a list
of recommendations to the community of software authors, journal publishers,
ADS, and research authors. The discussion, recommendations, and feedback will
help form recommendations for software citation to those publishers represented
in the Software Publishing Special Interest Group and the broader community.
"
1478,"Biblioth\`eques num\'eriques et gamification : panorama et \'etat de
  l'art","  This article presents an overview of the main gamification projects for
digital libraries, either for tagging or OCR correction. This overview is
followed by a state of the art with functionalities, motivations, sociology of
contributors and the scope of gamification compared to the serious games and
explicit crowdsourcing. Finally a comparison of results between explicit
crowdsourcing and gamification is proposed.
  [English Title: Digital libraries and gamification: overview and state of the
art]
"
1479,Discrete power law with exponential cutoff and Lotka's Law,"  The first bibliometric law appeared in Alfred J. Lotka's 1926 examination of
author productivity in chemistry and physics. The result is that the
productivity distribution is thought to be described by a power law. In this
paper, Lotka's original data on author productivity in chemistry is
reconsidered by comparing the fit of the data to both a discrete power law and
a discrete power law with exponential cutoff.
"
1480,"Clustering scientific publications based on citation relations: A
  systematic comparison of different methods","  Clustering methods are applied regularly in the bibliometric literature to
identify research areas or scientific fields. These methods are for instance
used to group publications into clusters based on their relations in a citation
network. In the network science literature, many clustering methods, often
referred to as graph partitioning or community detection techniques, have been
developed. Focusing on the problem of clustering the publications in a citation
network, we present a systematic comparison of the performance of a large
number of these clustering methods. Using a number of different citation
networks, some of them relatively small and others very large, we extensively
study the statistical properties of the results provided by different methods.
In addition, we also carry out an expert-based assessment of the results
produced by different methods. The expert-based assessment focuses on
publications in the field of scientometrics. Our findings seem to indicate that
there is a trade-off between different properties that may be considered
desirable for a good clustering of publications. Overall, map equation methods
appear to perform best in our analysis, suggesting that these methods deserve
more attention from the bibliometric community.
"
1481,Repositories with Direct Representation,"  A new generation of digital repositories could be based on direct
representation of the contents with rich semantics and models rather than be
collections of documents. The contents of such repositories would be highly
structured which should help users to focus on meaningful relationships of the
contents. These repositories would implement earlier proposals for
model-oriented information organization by extending current work on ontologies
to cover state changes, instances, and scenarios. They could also apply other
approaches such as object-oriented design and frame semantics. In addition to
semantics, the representation needs to allow for discourse and repository
knowledge-support services and policies. For instance, the knowledge base would
need to be systematically updated as new findings and theories reshape it.
"
1482,"Detecting the historical roots of tribology research: a bibliometric
  analysis","  In this study, the historical roots of tribology are investigated using a
newly developed scientometric method called Referenced Publication Years
Spectroscopy. The study is based on cited references in tribology research
publications. The Science Citation Index Expanded is used as data source. The
results show that RPYS has the potential to identify the important publications
: Most of the publications which have been identified in this study as highly
cited (referenced) publications are landmark publications in the field of
tribology.
"
1483,Funding acknowledgment analysis:Queries and Caveats,"  Thomson Reuters' Web of Science (WoS) began systematically collecting
acknowledgment information in August 2008. Since then, bibliometric analysis of
funding acknowledgment (FA) has been growing and has aroused intense interest
and attention from both academia and policy makers. Examining the distribution
of FA by citation index database, by language, and by acknowledgment type, we
noted coverage limitations and potential biases in each analysis. We argue that
in spite of its great value, bibliometric analysis of FA should be used with
caution.
"
1484,"Identification of long-term concept-symbols among citations: Can
  documents be clustered in terms of common intellectual histories?","  ""Citation classics"" are not only highly cited, but also cited during several
decades. We test whether the peaks in the spectrograms generated by Reference
Publication Years Spectroscopy (RPYS) indicate such long-term impact by
comparing across RPYS for subsequent time intervals. Multi-RPYS enables us to
distinguish between short-term citation peaks at the research front that decay
within ten years versus historically constitutive (long-term) citations that
function as concept symbols (Small, 1978). Using these constitutive citations,
one is able to cluster document sets (e.g., journals) in terms of
intellectually shared histories. We test this premise by clustering 40 journals
in the Web of Science Category of Information and Library Science using
multi-RPYS. It follows that RPYS can not only be used for retrieving roots of
sets under study (cited), but also for algorithmic historiography of the citing
sets. Significant references are historically rooted symbols among other
citations that function as currency.
"
1485,"The discretised lognormal and hooked power law distributions for
  complete citation data: Best options for modelling and regression","  Identifying the statistical distribution that best fits citation data is
important to allow robust and powerful quantitative analyses. Whilst previous
studies have suggested that both the hooked power law and discretised lognormal
distributions fit better than the power law and negative binomial
distributions, no comparisons so far have covered all articles within a
discipline, including those that are uncited. Based on an analysis of 26
different Scopus subject areas in seven different years, this article reports
comparisons of the discretised lognormal and the hooked power law with citation
data, adding 1 to citation counts in order to include zeros. The hooked power
law fits better in two thirds of the subject/year combinations tested for
journal articles that are at least three years old, including most medical,
life and natural sciences, and for virtually all subject areas for younger
articles. Conversely, the discretised lognormal tends to fit best for arts,
humanities, social science and engineering fields. The difference between the
fits of the distributions is mostly small, however, and so either could
reasonably be used for modelling citation data. For regression analyses,
however, the best option is to use ordinary least squares regression applied to
the natural logarithm of citation counts plus one, especially for sets of
younger articles, because of the increased precision of the parameters.
"
1486,Scalable Models for Computing Hierarchies in Information Networks,"  Information hierarchies are organizational structures that often used to
organize and present large and complex information as well as provide a
mechanism for effective human navigation. Fortunately, many statistical and
computational models exist that automatically generate hierarchies; however,
the existing approaches do not consider linkages in information {\em networks}
that are increasingly common in real-world scenarios. Current approaches also
tend to present topics as an abstract probably distribution over words, etc
rather than as tangible nodes from the original network. Furthermore, the
statistical techniques present in many previous works are not yet capable of
processing data at Web-scale. In this paper we present the Hierarchical
Document Topic Model (HDTM), which uses a distributed vertex-programming
process to calculate a nonparametric Bayesian generative model. Experiments on
three medium size data sets and the entire Wikipedia dataset show that HDTM can
infer accurate hierarchies even over large information networks.
"
1487,Wikiometrics: A Wikipedia Based Ranking System,"  We present a new concept - Wikiometrics - the derivation of metrics and
indicators from Wikipedia. Wikipedia provides an accurate representation of the
real world due to its size, structure, editing policy and popularity. We
demonstrate an innovative mining methodology, where different elements of
Wikipedia - content, structure, editorial actions and reader reviews - are used
to rank items in a manner which is by no means inferior to rankings produced by
experts or other methods. We test our proposed method by applying it to two
real-world ranking problems: top world universities and academic journals. Our
proposed ranking methods were compared to leading and widely accepted
benchmarks, and were found to be extremely correlative but with the advantage
of the data being publically available.
"
1488,"Introducing CitedReferencesExplorer (CRExplorer): A program for
  Reference Publication Year Spectroscopy with Cited References Standardization","  We introduce a new tool - the CitedReferencesExplorer (CRExplorer,
www.crexplorer.net) - which can be used to disambiguate and analyze the cited
references (CRs) of a publication set downloaded from the Web of Science (WoS).
The tool is especially suitable to identify those publications which have been
frequently cited by the researchers in a field and thereby to study for example
the historical roots of a research field or topic. CRExplorer simplifies the
identification of key publications by enabling the user to work with both a
graph for identifying most frequently cited reference publication years (RPYs)
and the list of references for the RPYs which have been most frequently cited.
A further focus of the program is on the standardization of CRs. It is a
serious problem in bibliometrics that there are several variants of the same CR
in the WoS. In this study, CRExplorer is used to study the CRs of all papers
published in the Journal of Informetrics. The analyses focus on the most
important papers published between 1980 and 1990.
"
1489,"Publication boost in Web of Science journals and its effect on citation
  distributions","  In this paper we show that the dramatic increase in the number of research
articles indexed in the Web of Science database impacts the commonly observed
distributions of citations within these articles. First, we document that the
growing number of physics articles in recent years is due to existing journals
publishing more and more papers rather than more new journals coming into being
as it happens in computer science. And second, even though the references from
the more recent papers generally cover a longer time span, the newer papers are
cited more frequently than the older ones if the uneven paper growth is not
corrected for. Nevertheless, despite this change in the distribution of
citations, the citation behavior of scientists does not seem to have changed.
"
1490,"Highlighting Impact and the Impact of Highlighting: PRB Editors'
  Suggestions","  Associate Editor Manolis Antonoyiannakis discusses the highlighting, as
Editors' Suggestions, of a small percentage of the papers published each week.
We highlight papers primarily for their importance and impact in their
respective fields, or because we find them particularly interesting or elegant.
It turns out that the additional layer of scrutiny involved in the selection of
papers as Editors' Suggestions is associated with a significantly elevated and
sustained citation impact.
"
1491,Research Project: Text Engineering Tool for Ontological Scientometry,"  The number of scientific papers grows exponentially in many disciplines. The
share of online available papers grows as well. At the same time, the period of
time for a paper to loose at chance to be cited anymore shortens. The decay of
the citing rate shows similarity to ultradiffusional processes as for other
online contents in social networks. The distribution of papers per author shows
similarity to the distribution of posts per user in social networks. The rate
of uncited papers for online available papers grows while some papers 'go
viral' in terms of being cited. Summarized, the practice of scientific
publishing moves towards the domain of social networks. The goal of this
project is to create a text engineering tool, which can semi-automatically
categorize a paper according to its type of contribution and extract
relationships between them into an ontological database. Semi-automatic
categorization means that the mistakes made by automatic pre-categorization and
relationship-extraction will be corrected through a wikipedia-like front-end by
volunteers from general public. This tool should not only help researchers and
the general public to find relevant supplementary material and peers faster,
but also provide more information for research funding agencies.
"
1492,"Disambiguation of Patent Inventors and Assignees Using High-Resolution
  Geolocation Data","  Patent data represent a significant source of information on innovation and
the evolution of technology through networks of citations, co-invention and
co-assignment of new patents. A major obstacle to extracting useful information
from this data is the problem of name disambiguation: linking alternate
spellings of individuals or institutions to a single identifier to uniquely
determine the parties involved in the creation of a technology. In this paper,
we describe a new algorithm that uses high-resolution geolocation to
disambiguate both inventor and assignees on more than 3.6 million patents found
in the European Patent Office (EPO), under the Patent Cooperation treaty (PCT),
and in the US Patent and Trademark Office (USPTO). We show that our algorithm
has both high precision and recall in comparison to a manual disambiguation of
EPO assignee names in Boston and Paris, and show it performs well for a
benchmark of USPTO inventor names that can be linked to a high-resolution
address (but poorly for inventors that never provided a high quality address).
The most significant benefit of this work is the high quality assignee
disambiguation with worldwide coverage coupled with an inventor disambiguation
that is competitive with other state of the art approaches. To our knowledge
this is the broadest and most accurate simultaneous disambiguation and
cross-linking of the inventor and assignee names for a significant fraction of
patents in these three major patent collections.
"
1493,"Opening Scholarly Communication in Social Sciences: Supporting Open Peer
  Review with Fidus Writer","  Our system will initially provide readers, authors and reviewers with an
alternative, thus having the potential to gain wider acceptance and gradually
replace the old, incoherent publication process of our journals and of others
in related fields. It will make journals more ""open"" (in terms of reusability)
that are open access already, and it has the potential to serve as an incentive
for turning ""closed"" journals into open access ones. In this poster paper we
will present the framework of the OSCOSS system and highlight the reviewer use
case.
"
1494,Research infrastructures in the LHC era: a scientometric approach,"  When a research infrastructure is funded and implemented, new information and
new publications are created. This new information is the measurable output of
discovery process. In this paper, we describe the impact of infrastructure for
physics experiments in terms of publications and citations. In particular, we
consider the Large Hadron Collider (LHC) experiments (ATLAS, CMS, ALICE, LHCb)
and compare them to the Large Electron Positron Collider (LEP) experiments
(ALEPH, DELPHI, L3, OPAL) and the Tevatron experiments (CDF, D0). We provide an
overview of the scientific output of these projects over time and highlight the
role played by remarkable project results in the publication-citation
distribution trends. The methodological and technical contribution of this work
provides a starting point for the development of a theoretical model of modern
scientific knowledge propagation over time.
"
1495,"The challenge and promise of software citation for credit,
  identification, discovery, and reuse","  In this article, we present the challenge of software citation as a method to
ensure credit for and identification, discovery, and reuse of software in
scientific and engineering research. We discuss related work and key
challenges/research directions, including suggestions for metadata necessary
for software citation.
"
1496,"Lotka's Inverse Square Law of Scientific Productivity: Its Methods and
  Statistics","  This brief communication analyzes the statistics and methods Lotka used to
derive his inverse square law of scientific productivity from the standpoint of
modern theory. It finds that he violated the norms of this theory by extremely
truncating his data on the right. It also proves that Lotka himself played an
important role in establishing the commonly used method of identifying
power-law behavior by the R^2 fit to a regression line on a log-log plot that
modern theory considers unreliable by basing the derivation of his law on this
very method.
"
1497,ARL Libraries and Research: Correlates of Grant Funding,"  While providing the resources and tools that make advanced research possible
is a primary mission of academic libraries at large research universities, many
other elements also contribute to the success of the research enterprise, such
as institutional funding, staffing, labs, and equipment. This study focuses on
U.S. members of the ARL, the Association for Research Libraries. Research
success is measured by the total grant funding received by the University,
creating an ordered set of categories. Combining data from the NSF National
Center for Science and Engineering Statistics, ARL Statistics, and IPEDS, the
primary explanatory factors for research success are examined. Using linear
regression, logistic regression, and the cumulative logit model, the
best-fitting models generated by ARL data, NSF data, and the combined data set
for both nominal and per capita funding are compared. These models produce the
most relevant explanatory variables for research funding, which do not include
library-related variables in most cases.
"
1498,"Adapting the Hypercube Model to Archive Deferred Representations and
  Their Descendants","  The web is today's primary publication medium, making web archiving an
important activity for historical and analytical purposes. Web pages are
increasingly interactive, resulting in pages that are increasingly difficult to
archive. Client-side technologies (e.g., JavaScript) enable interactions that
can potentially change the client-side state of a representation. We refer to
representations that load embedded resources via JavaScript as deferred
representations. It is difficult to archive all of the resources in deferred
representations and the result is archives with web pages that are either
incomplete or that erroneously load embedded resources from the live web.
  We propose a method of discovering and crawling deferred representations and
their descendants (representation states that are only reachable through
client-side events). We adapt the Dincturk et al. Hypercube model to construct
a model for archiving descendants, and we measure the number of descendants and
requisite embedded resources discovered in a proof-of-concept crawl. Our
approach identified an average of 38.5 descendants per seed URI crawled, 70.9%
of which are reached through an onclick event. This approach also added 15.6
times more embedded resources than Heritrix to the crawl frontier, but at a
rate that was 38.9 times slower than simply using Heritrix. We show that our
dataset has two levels of descendants. We conclude with proposed crawl policies
and an analysis of the storage requirements for archiving descendants.
"
1499,"Tracing Digital Footprints to Academic Articles: An Investigation of
  PeerJ Publication Referral Data","  In this study, we propose a novel way to explore the patterns of people's
visits to academic articles. About 3.4 million links to referral source of
visitors of 1432 papers published in the journal of PeerJ are collected and
analyzed. We find that at least 57% visits are from external referral sources,
among which General Search Engine, Social Network, and News & Blog are the top
three categories of referrals. Academic Resource, including academic search
engines and academic publishers' sites, is the fourth largest category of
referral sources. In addition, our results show that Google contributes
significantly the most in directing people to scholarly articles. This
encompasses the usage of Google the search engine, Google Scholar the academic
search engine, and diverse specific country domains of them. Focusing on
similar disciplines to PeerJ's publication scope, NCBI is the academic search
engine on which people are the most frequently directed to PeerJ. Correlation
analysis and regression analysis indicates that papers with more mentions are
expected to have more visitors, and Facebook, Twitter and Reddit are the most
commonly used social networking tools that refer people to PeerJ.
"
1500,"Evaluating the impact of interdisciplinary research: a multilayer
  network approach","  Nowadays, scientific challenges usually require approaches that cross
traditional boundaries between academic disciplines, driving many researchers
towards interdisciplinarity. Despite its obvious importance, there is a lack of
studies on how to quantify the influence of interdisciplinarity on the research
impact, posing uncertainty in a proper evaluation for hiring and funding
purposes. Here we propose a method based on the analysis of bipartite
interconnected multilayer networks of citations and disciplines, to assess
scholars, institutions and countries interdisciplinary importance. Using data
about physics publications and US patents, we show that our method allows to
reward, using a quantitative approach, scholars and institutions that have
carried out interdisciplinary work and have had an impact in different
scientific areas. The proposed method could be used by funding agencies,
universities and scientific policy decision makers for hiring and funding
purposes, and to complement existing methods to rank universities and
countries.
"
1501,"Think before you collect: Setting up a data collection approach for
  social media studies","  This chapter discusses important challenges of designing the data collection
setup for social media studies. It outlines how it is necessary to carefully
think about which data to collect and to use, and to recognize the effects that
a specific data collection approach may have on the types of analyses that can
be carried out and the results that can be expected in a study. We will
highlight important questions one should ask before setting up a data
collection framework and relate them to the different options for accessing
social media data. The chapter will mainly be illustrated with examples from
studying Twitter and Facebook. A case study studying political communication
around the 2013 elections in Germany should serve as a practical application
scenario. In this case study we constructed several social media datasets based
on different collection approaches, using data from Facebook and Twitter.
"
1502,Font Identification in Historical Documents Using Active Learning,"  Identifying the type of font (e.g., Roman, Blackletter) used in historical
documents can help optical character recognition (OCR) systems produce more
accurate text transcriptions. Towards this end, we present an active-learning
strategy that can significantly reduce the number of labeled samples needed to
train a font classifier. Our approach extracts image-based features that
exploit geometric differences between fonts at the word level, and combines
them into a bag-of-word representation for each page in a document. We evaluate
six sampling strategies based on uncertainty, dissimilarity and diversity
criteria, and test them on a database containing over 3,000 historical
documents with Blackletter, Roman and Mixed fonts. Our results show that a
combination of uncertainty and diversity achieves the highest predictive
accuracy (89% of test cases correctly classified) while requiring only a small
fraction of the data (17%) to be labeled. We discuss the implications of this
result for mass digitization projects of historical documents.
"
1503,Aggregation and Linking of Observational Metadata in the ADS,"  We discuss current efforts behind the curation of observing proposals,
archive bibliographies, and data links in the NASA Astrophysics Data System
(ADS). The primary data in the ADS is the bibliographic content from scholarly
articles in Astronomy and Physics, which ADS aggregates from publishers, arXiv
and conference proceeding sites. This core bibliographic information is then
further enriched by ADS via the generation of citations and usage data, and
through the aggregation of external resources from astronomy data archives and
libraries. Important sources of such additional information are the metadata
describing observing proposals and high level data products, which, once
ingested in ADS, become easily discoverable and citeable by the science
community. Bibliographic studies have shown that the integration of links
between data archives and the ADS provides greater visibility to data products
and increased citations to the literature associated with them.
"
1504,"Individual Bibliometric Assessment @ University of Vienna: From Numbers
  to Multidimensional Profiles","  This paper shows how bibliometric assessment can be implemented at individual
level. This has been successfully done at the University of Vienna carried out
by the Bibliometrics and Publication Strategies Department of the Vienna
University Library. According to the department's philosophy, bibliometrics is
not only a helpful evaluation instrument in order to complement the peer review
system. It is also meant as a compass for researchers in the ""publish or
perish"" dilemma in order to increase general visibility and to optimize
publication strategies. The individual assessment comprises of an interview
with the researcher under evaluation, the elaboration of a bibliometric report
of the researcher's publication output, the discussion and validation of the
obtained results with the researcher under evaluation as well as further
optional analyses. The produced bibliometric reports are provided to the
researchers themselves and inform them about the quantitative aspects of their
research output. They also serve as a basis for further discussion concerning
their publication strategies. These reports are eventually intended for
informed peer review practices, and are therefore forwarded to the quality
assurance and the rector's office and finally sent to the peers. The most
important feature of the generated bibliometric report is its multidimensional
and individual character. It relies on a variety of basic indicators and
further control parameters in order to foster comprehensibility. Researchers,
administrative staff and peers alike have confirmed the usefulness of this
bibliometric approach. An increasing demand is noticeable. In total, 33
bibliometric reports have been delivered so far. Moreover, similar reports have
also been produced for the bibliometric assessment of two faculties with great
success.
"
1505,Computer-Assisted Processing of Intertextuality in Ancient Languages,"  The production of digital critical editions of texts using TEI is now a
widely-adopted procedure within digital humanities. The work described in this
paper extends this approach to the publication of gnomologia (anthologies of
wise sayings), which formed a widespread literary genre in many cultures of the
medieval Mediterranean. These texts are challenging because they were rarely
copied straightforwardly; rather, sayings were selected, reorganised, modified
or re-attributed between manuscripts, resulting in a highly interconnected
corpus for which a standard approach to digital publication is insufficient.
Focusing on Greek and Arabic collections, we address this challenge using
semantic web techniques to create an ecosystem of texts, relationships and
annotations, and consider a new model - organic, collaborative, interconnected,
and open-ended - of what constitutes an edition. This semantic web-based
approach allows scholars to add their own materials and annotations to the
network of information and to explore the conceptual networks that arise from
these interconnected sayings.
"
1506,Technical Report: Representing SES Cases Using Ontology,"  Socio-ecological System (SES) research studies the interaction between
environment, users, and governance of environment resources. Data produced
during the research cycle can be both long-tail (e.g. heterogeneous) and
longitudinal data. For example, the IFRI (International Forestry Resources and
Institutions) data set contains studies carried out over a period of 20 years.
Given the complexity of a SES system, case studies that are accumulated over
time and from different sites (e.g. site visit cases) are highly valuable in
the understanding of new SES system behavior for instance. We, as a group of
informatics researchers collaborating with personnel from the Workshop in
Political Theory and Policy Analysis at Indiana University, are developing
informatics approaches to facilitating SES scholars' research.
  Here we focus on presenting our work on representing SES cases using
ontology. An ontology for the SES field can help organize concepts in the
field, describe resources such as data and publications using a shared
vocabulary, and also facilitate data use and query for researchers. We develop
a core SES ontology, which contains core concepts and resources in the field
and can be used to describe actual concept and resource instances, and also a
tool for contributing instances by drawing graphs, called Cmap2SES.
"
1507,"Is collaboration among scientists related to the citation impact of
  papers because their quality increases with collaboration? An analysis based
  on data from F1000Prime and normalized citation scores","  In recent years, the relationship of collaboration among scientists and the
citation impact of papers have been frequently investigated. Most of the
studies show that the two variables are closely related: an increasing
collaboration activity (measured in terms of number of authors, number of
affiliations, and number of countries) is associated with an increased citation
impact. However, it is not clear whether the increased citation impact is based
on the higher quality of papers which profit from more than one scientist
giving expert input or other (citation-specific) factors. Thus, the current
study addresses this question by using two comprehensive datasets with
publications (in the biomedical area) including quality assessments by experts
(F1000Prime member scores) and citation data for the publications. The study is
based on nearly 10,000 papers. Robust regression models are used to investigate
the relationship between number of authors, number of affiliations, and number
of countries, respectively, and citation impact - controlling for the papers'
quality (measured by F1000Prime expert ratings). The results point out that the
effect of collaboration activities on impact is largely independent of the
papers' quality. The citation advantage is apparently not quality-related;
citation specific factors (e.g. self-citations) seem to be important here.
"
1508,Citations to articles citing Benford's law: a Benford analysis,"  The occurrence of first significant digits of numbers in large data is often
governed by a logarithmically decreasing distribution called Benford's law
(BL), reported first by S. Newcomb (SN) and many decades later independently by
F. Benford (FB). Due to its counter-intuitiveness the law was ignored for
decades as a mere curious observation. However, an indication of its remarkable
resurgence is the huge swell in the number of citations received by the papers
of SN/FB. The law has come a long way, from obscurity to now being a regular
subject of books, peer reviewed papers, patents, blogs and news. Here, we use
Google Scholar (GS) to collect the data on the number of citations received by
the articles citing the original paper of SN/FB and then investigate whether
the leading digits of this citations data are distributed according to the law
they discovered. We find that the citations data of literature on BL is in
remarkable agreement with the predictions of the law.
"
1509,On the Discovery of Success Trajectories of Authors,"  Understanding the qualitative patterns of research endeavor of scientific
authors in terms of publication count and their impact (citation) is important
in order to quantify success trajectories. Here, we examine the career profile
of authors in computer science and physics domains and discover at least six
different success trajectories in terms of normalized citation count in
longitudinal scale. Initial observations of individual trajectories lead us to
characterize the authors in each category. We further leverage this trajectory
information to build a two-stage stratification model to predict future success
of an author at the early stage of her career. Our model outperforms the
baseline with an average improvement of 15.68% for both the datasets.
"
1510,"RPYS i/o: A web-based tool for the historiography and visualization of
  citation classics, sleeping beauties, and research fronts","  Reference Publication Year Spectroscopy (RPYS) and Multi-RPYS provide
algorithmic approaches to reconstructing the intellectual histories of
scientific fields. With this brief communication, we describe a technical
advancement for developing research historiographies by introducing RPYS i/o,
an online tool for performing standard RPYS and Multi-RPYS analyses
interactively (at http://comins.leydesdorff.net/). The tool enables users to
explore seminal works underlying a research field and to plot the influence of
these seminal works over time. This suite of visualizations offers the
potential to analyze and visualize the myriad of temporal dynamics of
scientific influence, such as citation classics, sleeping beauties, and the
dynamics of research fronts. We demonstrate the features of the tool by
analyzing--as an example--the references in documents published in the journal
Philosophy of Science.
"
1511,"Economic and Technological Complexity: A Model Study of Indicators of
  Knowledge-based Innovation Systems","  The Economic Complexity Index (ECI; Hidalgo & Hausmann, 2009) measures the
complexity of national economies in terms of product groups. Analogously to
ECI, a Patent Complexity Index (PatCI) can be developed on the basis of a
matrix of nations versus patent classes. Using linear algebra, the three
dimensions: countries, product groups, and patent classes can be combined into
a measure of ""Triple Helix"" complexity (THCI) including the trilateral
interaction terms between knowledge production, wealth generation, and
(national) control. THCI can be expected to capture the extent of systems
integration between the global dynamics of markets (ECI) and technologies
(PatCI) in each national system of innovation. We measure ECI, PatCI, and THCI
during the period 2000-2014 for the 34 OECD member states, the BRICS countries,
and a group of emerging and affiliated economies (Argentina, Hong Kong,
Indonesia, Malaysia, Romania, and Singapore). The three complexity indicators
are correlated between themselves; but the correlations with GDP per capita are
virtually absent. Of the world's major economies, Japan scores highest on all
three indicators, while China has been increasingly successful in combining
economic and technological complexity. We could not reproduce the correlation
between ECI and average income that has been central to the argument about the
fruitfulness of the economic complexity approach.
"
1512,"The counting house: measuring those who count. Presence of
  Bibliometrics, Scientometrics, Informetrics, Webometrics and Altmetrics in
  the Google Scholar Citations, ResearcherID, ResearchGate, Mendeley & Twitter","  Following in the footsteps of the model of scientific communication, which
has recently gone through a metamorphosis (from the Gutenberg galaxy to the Web
galaxy), a change in the model and methods of scientific evaluation is also
taking place. A set of new scientific tools are now providing a variety of
indicators which measure all actions and interactions among scientists in the
digital space, making new aspects of scientific communication emerge. In this
work we present a method for capturing the structure of an entire scientific
community (the Bibliometrics, Scientometrics, Informetrics, Webometrics, and
Altmetrics community) and the main agents that are part of it (scientists,
documents, and sources) through the lens of Google Scholar Citations.
  Additionally, we compare these author portraits to the ones offered by other
profile or social platforms currently used by academics (ResearcherID,
ResearchGate, Mendeley, and Twitter), in order to test their degree of use,
completeness, reliability, and the validity of the information they provide. A
sample of 814 authors (researchers in Bibliometrics with a public profile
created in Google Scholar Citations was subsequently searched in the other
platforms, collecting the main indicators computed by each of them. The data
collection was carried out on September, 2015. The Spearman correlation was
applied to these indicators (a total of 31) , and a Principal Component
Analysis was carried out in order to reveal the relationships among metrics and
platforms as well as the possible existence of metric clusters
"
1513,Embedding Graphs in Lorentzian Spacetime,"  Geometric approaches to network analysis combine simply defined models with
great descriptive power. In this work we provide a method for embedding
directed acyclic graphs into Minkowski spacetime using Multidimensional scaling
(MDS). First we generalise the classical MDS algorithm, defined only for
metrics with a Euclidean signature, to manifolds of any metric signature. We
then use this general method to develop an algorithm to be used on networks
which have causal structure allowing them to be embedded in Lorentzian
manifolds. The method is demonstrated by calculating embeddings for both causal
sets and citation networks in Minkowski spacetime. We finally suggest a number
of applications in citation analysis such as paper recommendation, identifying
missing citations and fitting citation models to data using this geometric
approach.
"
1514,Climate Change Research in View of Bibliometrics,"  This bibliometric study of a large publication set dealing with research on
climate change aims at mapping the relevant literature from a bibliometric
perspective and presents a multitude of quantitative data: (1) The growth of
the overall publication output as well as (2) of some major subfields, (3) the
contributing journals and countries as well as their citation impact, and (4) a
title word analysis aiming to illustrate the time evolution and relative
importance of specific research topics. The study is based on 222,060 papers
published between 1980 and 2014. The total number of papers shows a strong
increase with a doubling every 5-6 years. Continental biomass related research
is the major subfield, closely followed by climate modeling. Research dealing
with adaptation, mitigation, risks, and vulnerability of global warming is
comparatively small, but their share of papers increased exponentially since
2005. Research on vulnerability and on adaptation published the largest
proportion of very important papers. Research on climate change is
quantitatively dominated by the USA, followed by the UK, Germany, and Canada.
The citation-based indicators exhibit consistently that the UK has produced the
largest proportion of high impact papers compared to the other countries
(having published more than 10,000 papers). The title word analysis shows that
the term climate change comes forward with time. Furthermore, the term impact
arises and points to research dealing with the various effects of climate
change. Finally, the term model and related terms prominently appear
independent of time, indicating the high relevance of climate modeling.
"
1515,Tracking the performance of an R&D programme in the Biomedical Sciences,"  This paper aims at offering an evaluation framework of an Research and
Development programme in the Biomedical Sciences. It showcases the Spanish
Biomedical Research Networking Centres initiative (CIBER) as an example of the
effect of research policy management on performance. For this it focuses on
three specific aspects: its role on the national research output in the
biomedical sciences, its effect on promoting translational research through
internal collaboration between research groups, and the perception of
researchers on the programme as defined by their inclusion of their CIBER
centres in the address field. Research output derived from this programme
represents around 25 per cent of the country's publications in the biomedical
fields. After analysing a seven year period, the programme has enhanced
collaborations between its members, but they do not seem to be sufficiently
strong. With regard to the credit given to the initiative, 54.5 per cent of the
publications mentioned this programme in their address, however an increase on
the share of papers mention it is observed two years after it was launched. We
suggest that by finding the point in which the share of mentions stabilises may
be a good strategy to identify the complete fulfilment of these types of
Research and Development policies.
"
1516,"Iran's scientific dominance and the emergence of South-East Asian
  countries as scientific collaborators in the Persian Gulf Region","  A longitudinal bibliometric analysis of publications indexed in Thomson
Reuters' Incites and Elsevier's Scopus, and published from Persian Gulf States
and neighbouring Middle East countries, shows clear effects of major political
events during the past 35 years. Predictions made in 2006 by the US diplomat
Richard N. Haass on political changes in the Middle East have come true in the
Gulf States' national scientific research systems, to the extent that Iran has
become in 2015 by far the leading country in the Persian Gulf, and South-East
Asian countries including China, Malaysia and South Korea have become major
scientific collaborators, displacing the USA and other large Western countries.
But collaborations patterns among Persian Gulf States show no apparent
relationship with differences in Islam denominations.
"
1517,"A note and a correction on measuring cognitive distance in multiple
  dimensions","  In a previous article (Rahman, Guns, Rousseau, and Engels, 2015) we described
several approaches to determine the cognitive distance between two units. One
of these approaches was based on what we called barycenters in N dimensions.
The present note corrects this terminology and introduces the more adequate
term 'similarity-adapted publication vectors'. Furthermore, we correct an error
in normalization and explain the importance of scale invariance in determining
cognitive distance. We also consider weighted cosine similarity as an
alternative approach to determine cognitive (dis)similarity. Overall, we find
that the three approaches (distance between barycenters, distance between
similarity-adapted publication vectors, and weighted cosine similarity) yield
very similar results.
"
1518,Rules of Acquisition for Mementos and Their Content,"  Text extraction from web pages has many applications, including web crawling
optimization and document clustering. Though much has been written about the
acquisition of content from live web pages, content acquisition of archived web
pages, known as mementos, remains a relatively new enterprise. In the course of
conducting a study with almost 700,000 web pages, we encountered issues
acquiring mementos and extracting text from them. The acquisition of memento
content via HTTP is expected to be a relatively painless exercise, but we have
found cases to the contrary. We also find that the parsing of HTML, already
known to be problematic, can be more complex when one attempts to extract the
text of mementos across many web archives, due to issues involving different
memento presentation behaviors, as well as the age of the HTML in their
mementos. For the benefit of others acquiring mementos across many web
archives, we document those experiences here.
"
1519,Concentration of research funding leads to decreasing marginal returns,"  In most countries, basic research is supported by research councils that
select, after peer review, the individuals or teams that are to receive
funding. Unfortunately, the number of grants these research councils can
allocate is not infinite and, in most cases, a minority of the researchers
receive the majority of the funds. However, evidence as to whether this is an
optimal way of distributing available funds is mixed. The purpose of this study
is to measure the relation between the amount of funding provided to 12,720
researchers in Quebec over a fifteen year period (1998-2012) and their
scientific output and impact from 2000 to 2013. Our results show that both in
terms of the quantity of papers produced and of their scientific impact, the
concentration of research funding in the hands of a so-called ""elite"" of
researchers generally produces diminishing marginal returns. Also, we find that
the most funded researchers do not stand out in terms of output and scientific
impact.
"
1520,"Recognition of Emerging Technology Trends. Class-selective study of
  citations in the U.S. Patent Citation Network","  By adopting a citation-based recursive ranking method for patents the
evolution of new fields of technology can be traced. Specifically, it is
demonstrated that the laser / inkjet printer technology emerged from the
recombination of two existing technologies: sequential printing and static
image production. The dynamics of the citations coming from the different
""precursor"" classes illuminates the mechanism of the emergence of new fields
and give the possibility to make predictions about future technological
development. For the patent network the optimal value of the PageRank damping
factor is close to 0.5; the application of d=0.85 leads to unacceptable ranking
results.
"
1521,"The Research Space: using the career paths of scholars to predict the
  evolution of the research output of individuals, institutions, and nations","  In recent years scholars have built maps of science by connecting the
academic fields that cite each other, are cited together, or that cite a
similar literature. But since scholars cannot always publish in the fields they
cite, or that cite them, these science maps are only rough proxies for the
potential of a scholar, organization, or country, to enter a new academic
field. Here we use a large dataset of scholarly publications disambiguated at
the individual level to create a map of science-or research space-where links
connect pairs of fields based on the probability that an individual has
published in both of them. We find that the research space is a significantly
more accurate predictor of the fields that individuals and organizations will
enter in the future than citation based science maps. At the country level,
however, the research space and citations based science maps are equally
accurate. These findings show that data on career trajectories-the set of
fields that individuals have previously published in-provide more accurate
predictors of future research output for more focalized units-such as
individuals or organizations-than citation based science maps.
"
1522,"Ground truth? Concept-based communities versus the external
  classification of physics manuscripts","  Community detection techniques are widely used to infer hidden structures
within interconnected systems. Despite demonstrating high accuracy on
benchmarks, they reproduce the external classification for many real-world
systems with a significant level of discrepancy. A widely accepted reason
behind such outcome is the unavoidable loss of non-topological information
(such as node attributes) encountered when the original complex system is
represented as a network. In this article we emphasize that the observed
discrepancies may also be caused by a different reason: the external
classification itself. For this end we use scientific publication data which i)
exhibit a well defined modular structure and ii) hold an expert-made
classification of research articles. Having represented the articles and the
extracted scientific concepts both as a bipartite network and as its unipartite
projection, we applied modularity optimization to uncover the inner thematic
structure. The resulting clusters are shown to partly reflect the author-made
classification, although some significant discrepancies are observed. A
detailed analysis of these discrepancies shows that they carry essential
information about the system, mainly related to the use of similar techniques
and methods across different (sub)disciplines, that is otherwise omitted when
only the external classification is considered.
"
1523,"Beams of particles and papers. How digital preprint archives shape
  authorship and credit","  In high energy physics, scholarly papers circulate primarily through online
preprint archives based on a centralized repository, arXiv, that physicists
simply refer to as ""the archive"". This is not just a tool for preservation and
memory, but also a space of flows where written objects are detected and their
authors made available for scrutiny. In this work I analyze the reading and
publishing practices of two subsets of high energy physicists: theorists and
experimentalists. In order to be recognized as legitimate and productive
members of their community, they need to abide by the temporalities and
authorial practices structured by the archive. Theorists live in a state of
accelerated time that shapes their reading and publishing practices around
precise cycles. Experimentalists turn to tactics that allow them to circumvent
the slowed-down time and invisibility they experience as members of large
collaborations. As digital platforms for the exchange of scholarly articles
emerge in other fields, high energy physics could help shed light on general
transformations of contemporary scholarly communication systems.
"
1524,Persistent URIs Must Be Used To Be Persistent,"  We quantify the extent to which references to papers in scholarly literature
use persistent HTTP URIs that leverage the Digital Object Identifier
infrastructure. We find a significant number of references that do not,
speculate why authors would use brittle URIs when persistent ones are
available, and propose an approach to alleviate the problem.
"
1525,"A Multi-dimensional Investigation of the Effects of Publication
  Retraction on Scholarly Impact","  Over the past few decades, the rate of publication retractions has increased
dramatically in academia. In this study, we investigate retractions from a
quantitative perspective, aiming to answer two fundamental questions. One, how
do retractions influence the scholarly impact of retracted papers, authors, and
institutions? Two, does this influence propagate to the wider academic
community through scholarly associations? Specifically, we analyzed a set of
retracted articles indexed in Thomson Reuters Web of Science (WoS), and ran
multiple experiments to compare changes in scholarly impact against a control
set of non-retracted articles, authors, and institutions. We further applied
the Granger Causality test to investigate whether different scientific topics
are dynamically affected by retracted papers occurring within those topics. Our
results show two key findings: first, the scholarly impact of retracted papers
and authors significantly decreases after retraction, and the most severe
impact decrease correlates to retractions based on proven purposeful scientific
misconduct; second, this retraction penalty does not seem to spread through the
broader scholarly social graph, but instead has a limited and localized effect.
Our findings may provide useful insights for scholars or science committees to
evaluate the scholarly value of papers, authors, or institutions related to
retractions.
"
1526,Grand Challenges in Measuring and Characterizing Scholarly Impact,"  The constantly growing body of scholarly knowledge of science, technology,
and humanities is an asset of the mankind. While new discoveries expand the
existing knowledge, they may simultaneously render some of it obsolete. It is
crucial for scientists and other stakeholders to keep their knowledge up to
date. Policy makers, decision makers, and the general public also need an
efficient communication of scientific knowledge. Several grand challenges
concerning the creation, adaptation, and diffusion of scholarly knowledge, and
advance quantitative and qualitative approaches to the study of scholarly
knowledge are identified.
"
1527,A Theory of Ambulance Chasing,"  Ambulance chasing is a common socio-scientific phenomenon in particle
physics. I argue that despite the seeming complexity, it is possible to gain
insight into both the qualitative and quantitative features of ambulance
chasing dynamics. Compound-Poisson statistics suffices to accommodate the time
evolution of the cumulative number of papers on a topic, where basic
assumptions that the interest in the topic as well as the number of available
ideas decrease with time appear to drive the time evolution. It follows that if
the interest scales as an inverse power law in time, the cumulative number of
papers on a topic is well described by a di-gamma function, with a distinct
logarithmic behavior at large times. In cases where the interest decreases
exponentially with time, the model predicts that the total number of papers on
the topic will converge to a fixed value as time goes to infinity. I
demonstrate that the two models are able to fit at least 9 specific instances
of ambulance chasing in particle physics using only two free parameters. In
case of the most recent ambulance chasing instance, the ATLAS {\gamma}{\gamma}
excess, fits to the current data predict that the total number of papers on the
topic will not exceed 310 papers by the June 1. 2016, and prior to the natural
cut-off for the validity of the theory.
"
1528,"From manuscript catalogues to a handbook of Syriac literature: Modeling
  an infrastructure for Syriaca.org","  Despite increasing interest in Syriac studies and growing digital
availability of Syriac texts, there is currently no up-to-date infrastructure
for discovering, identifying, classifying, and referencing works of Syriac
literature. The standard reference work (Baumstark's Geschichte) is over ninety
years old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be
accessed only through disparate catalogues and databases. The present article
proposes a tentative data model for Syriaca.org's New Handbook of Syriac
Literature, an open-access digital publication that will serve as both an
authority file for Syriac works and a guide to accessing their manuscript
representations, editions, and translations. The authors hope that by
publishing a draft data model they can receive feedback and incorporate
suggestions into the next stage of the project.
"
1529,"Simplified Relative Citation Ratio for Static Paper Ranking: UFMG/LATIN
  at WSDM Cup 2016","  Static rankings of papers play a key role in the academic search setting.
Many features are commonly used in the literature to produce such rankings,
some examples are citation-based metrics, distinct applications of PageRank,
among others. More recently, learning to rank techniques have been successfully
applied to combine sets of features producing effective results. In this work,
we propose the metric S-RCR, which is a simplified version of a metric called
Relative Citation Ratio --- both based on the idea of a co-citation network.
When compared to the classical version, our simplification S-RCR leads to
improved efficiency with a reasonable effectiveness. We use S-RCR to rank over
120 million papers in the Microsoft Academic Graph dataset. By using this
single feature, which has no parameters and does not need to be tuned, our team
was able to reach the 3rd position in the first phase of the WSDM Cup 2016.
"
1530,"Identifying and Improving Dataset References in Social Sciences Full
  Texts","  Scientific full text papers are usually stored in separate places than their
underlying research datasets. Authors typically make references to datasets by
mentioning them for example by using their titles and the year of publication.
However, in most cases explicit links that would provide readers with direct
access to referenced datasets are missing. Manually detecting references to
datasets in papers is time consuming and requires an expert in the domain of
the paper. In order to make explicit all links to datasets in papers that have
been published already, we suggest and evaluate a semi-automatic approach for
finding references to datasets in social sciences papers. Our approach does not
need a corpus of papers (no cold start problem) and it performs well on a small
test corpus (gold standard). Our approach achieved an F-measure of 0.84 for
identifying references in full texts and an F-measure of 0.83 for finding
correct matches of detected references in the da|ra dataset registry.
"
1531,"Two Tales of the World: Comparison of Widely Used World News Datasets
  GDELT and EventRegistry","  In this work, we compare GDELT and Event Registry, which monitor news
articles worldwide and provide big data to researchers regarding scale, news
sources, and news geography. We found significant differences in scale and news
sources, but surprisingly, we observed high similarity in news geography
between the two datasets.
"
1532,"Plagiarism Detection - State-of-the-art systems (2016) and evaluation
  methods","  Plagiarism detection systems comprise various approaches that aim to create a
fair environment for academic publications and appropriately acknowledge the
authors' works. While the need for a reliable and performant plagiarism
detection system increases with an increasing amount of publications, current
systems still have shortcomings. Particularly intelligent research plagiarism
detection still leaves room for improvement. An important factor for progress
in research is a suitable evaluation framework. In this paper, we give an
overview on the evaluation of plagiarism detection. We then use a taxonomy
provided in former research, to classify recent approaches of plagiarism
detection. Based on this, we asses the current research situation in the field
of plagiarism detection and derive further research questions and approaches to
be tackled in the future.
"
1533,Part-of-Speech Tagging for Historical English,"  As more historical texts are digitized, there is interest in applying natural
language processing tools to these archives. However, the performance of these
tools is often unsatisfactory, due to language change and genre differences.
Spelling normalization heuristics are the dominant solution for dealing with
historical texts, but this approach fails to account for changes in usage and
vocabulary. In this empirical paper, we assess the capability of domain
adaptation techniques to cope with historical texts, focusing on the classic
benchmark task of part-of-speech tagging. We evaluate several domain adaptation
methods on the task of tagging Early Modern English and Modern British English
texts in the Penn Corpora of Historical English. We demonstrate that the
Feature Embedding method for unsupervised domain adaptation outperforms word
embeddings and Brown clusters, showing the importance of embedding the entire
feature space, rather than just individual words. Feature Embeddings also give
better performance than spelling normalization, but the combination of the two
methods is better still, yielding a 5% raw improvement in tagging accuracy on
Early Modern English texts.
"
1534,Data fluidity in DARIAH -- pushing the agenda forward,"  This paper provides both an update concerning the setting up of the European
DARIAH infrastructure and a series of strong action lines related to the
development of a data centred strategy for the humanities in the coming years.
In particular we tackle various aspect of data management: data hosting, the
setting up of a DARIAH seal of approval, the establishment of a charter between
cultural heritage institutions and scholars and finally a specific view on
certification mechanisms for data.
"
1535,Automating the Horae: Boundary-work in the age of computers,"  This paper describes the intense software filtering that has allowed the
arXiv eprint repository to sort and process large numbers of submissions with
minimal human intervention, making it one of the most important and influential
cases of open access repositories to date. The paper narrates arXiv's
transformation, using sophisticated sorting-filtering algorithms to decrease
human workload, from a small mailing list used by a few hundred researchers to
a site that processes thousands of papers per month. However there are
significant negative consequences for authors who have been filtered out of the
main categories. There is thus a continued need to check and balance arXiv's
boundaries, based in the essential tension between stability and innovation.
"
1536,Academic Torrents: Scalable Data Distribution,"  As competitions get more popular, transferring ever-larger data sets becomes
infeasible and costly. For example, downloading the 157.3 GB 2012 ImageNet data
set incurs about $4.33 in bandwidth costs per download. Downloading the full
ImageNet data set takes 33 days. ImageNet has since become popular beyond the
competition, and many papers and models now revolve around this data set. For
sharing such an important resource to the machine learning community, the
sharers of ImageNet must shoulder a large bandwidth burden. Academic Torrents
reduces this burden for disseminating competition data, and also increases
download speeds for end users. Academic Torrents is run by a pending
nonprofit.. By augmenting an existing HTTP server with a peer-to-peer swarm,
requests get re-routed to get data from downloaders. While existing systems
slow down with more users, the benefits of Academic Torrents grow, with
noticeable effects even when only one other person is downloading.
"
1537,"Grand challenges in altmetrics: heterogeneity, data quality and
  dependencies","  As uptake among researchers is constantly increasing, social media are
finding their way into scholarly communication and, under the umbrella term
altmetrics, were introduced to research evaluation. Fueled by technological
possibilities and an increasing demand to demonstrate impact beyond the
scientific community, altmetrics received great attention as potential
democratizers of the scientific reward system and indicators of societal
impact. This paper focuses on current challenges of altmetrics. Heterogeneity,
data quality and particular dependencies are identified as the three major
issues and discussed in detail with a particular emphasis on past developments
in bibliometrics. The heterogeneity of altmetrics mirrors the diversity of the
types of underlying acts, most of which take place on social media platforms.
This heterogeneity has made it difficult to establish a common definition or
conceptual framework. Data quality issues become apparent in the lack of
accuracy, consistency and replicability of various altmetrics, which is largely
affected by the dynamic nature of social media events. It is further
highlighted that altmetrics are shaped by technical possibilities and depend
particularly on the availability of APIs and DOIs, are strongly dependent on
data providers and aggregators, and potentially influenced by technical
affordances of underlying platforms.
"
1538,"Are the discretised lognormal and hooked power law distributions
  plausible for citation data?","  There is no agreement over which statistical distribution is most appropriate
for modelling citation count data. This is important because if one
distribution is accepted then the relative merits of different citation-based
indicators, such as percentiles, arithmetic means and geometric means, can be
more fully assessed. In response, this article investigates the plausibility of
the discretised lognormal and hooked power law distributions for modelling the
full range of citation counts, with an offset of 1. The citation counts from 23
Scopus subcategories were fitted to hooked power law and discretised lognormal
distributions but both distributions failed a Kolmogorov-Smirnov goodness of
fit test in over three quarters of cases. The discretised lognormal
distribution also seems to have the wrong shape for citation distributions,
with too few zeros and not enough medium values for all subjects. The cause of
poor fits could be the impurity of the subject subcategories or the presence of
interdisciplinary research. Although it is possible to test for subject
subcategory purity indirectly through a goodness of fit test in theory with
large enough sample sizes, it is probably not possible in practice. Hence it
seems difficult to get conclusive evidence about the theoretically most
appropriate statistical distribution.
"
1539,Analysis of the Cuban journal Bibliotecas: Anales de Investigacion,"  The objective of this article is to describe the academic impact, the
editorial process quality, and the editorial and visibility strategies of
Bibliotecas. Anales de Investigacion (BAI), a scientific Cuban journal edited
by National Library of Cuba Jose Marti. The academic impact is determined
through a citation analysis, which considers Google Scholar database as
reference source. The bibliometric indicators applied are: citation per year,
citation vs. self-citation, citable journals vs. non-citable documents, Hirsch
Index, and impact factor. The editorial process quality and the visibility
strategies are determined through a self-evaluation which takes into account
the SciELO, Scopus, CLASE, Redalyc, Latindex, Dialnet, and ERIH PLUS
methodologies. The results reveal an ascending citation line that highlights
citing journals from the field of Library and Information Science, Medicine and
Health Sciences, and Education. Aspects related content and format have
negatively influenced on editorial process quality. Some strategies are
proposed to improve scientific visibility through the inclusion in databases,
directories, and social and academic networks. In general, this study
contributes to the editorial decision taking, an issue that could augment the
impact and scientific visibility of BAI.
"
1540,"A System for Probabilistic Linking of Thesauri and Classification
  Systems","  This paper presents a system which creates and visualizes probabilistic
semantic links between concepts in a thesaurus and classes in a classification
system. For creating the links, we build on the Polylingual Labeled Topic Model
(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in
the classification system by using information from the natural language text
of documents, their assigned thesaurus descriptors and their designated
classes. The links are then presented to users of the system in an interactive
visualization, providing them with an automatically generated overview of the
relations between the thesaurus and the classification system.
"
1541,"Enriching Ontologies with Encyclopedic Background Knowledge for Document
  Indexing","  The rapidly increasing number of scientific documents available publicly on
the Internet creates the challenge of efficiently organizing and indexing these
documents. Due to the time consuming and tedious nature of manual
classification and indexing, there is a need for better methods to automate
this process. This thesis proposes an approach which leverages encyclopedic
background knowledge for enriching domain-specific ontologies with textual and
structural information about the semantic vicinity of the ontologies' concepts.
The proposed approach aims to exploit this information for improving both
ontology-based methods for classifying and indexing documents and methods based
on supervised machine learning.
"
1542,"Profiling vs. Time vs. Content: What does Matter for Top-k Publication
  Recommendation based on Twitter Profiles? - An Extended Technical Report","  So far it is unclear how different factors of a scientific publication
recommender system based on users' tweets have an influence on the
recommendation performance. We examine three different factors, namely
profiling method, temporal decay, and richness of content. Regarding profiling,
we compare CF-IDF that replaces terms in TF-IDF by semantic concepts, HCF-IDF
as novel hierarchical variant of CF-IDF, and topic modeling. As temporal decay
functions, we apply sliding window and exponential decay. In terms of the
richness of content, we compare recommendations using both full-texts and
titles of publications and using only titles. Overall, the three factors make
twelve recommendation strategies. We have conducted an online experiment with
123 participants and compared the strategies in a within-group design. The best
recommendations are achieved by the strategy combining CF-IDF, sliding window,
and with full-texts. However, the strategies using the novel HCF-IDF profiling
method achieve similar results with just using the titles of the publications.
Therefore, HCF-IDF can make recommendations when only short and sparse data is
available.
"
1543,The Anatomy of a Search and Mining System for Digital Archives,"  Samtla (Search And Mining Tools with Linguistic Analysis) is a digital
humanities system designed in collaboration with historians and linguists to
assist them with their research work in quantifying the content of any textual
corpora through approximate phrase search and document comparison. The
retrieval engine uses a character-based n-gram language model rather than the
conventional word-based one so as to achieve great flexibility in language
agnostic query processing.
  The index is implemented as a space-optimised character-based suffix tree
with an accompanying database of document content and metadata. A number of
text mining tools are integrated into the system to allow researchers to
discover textual patterns, perform comparative analysis, and find out what is
currently popular in the research community.
  Herein we describe the system architecture, user interface, models and
algorithms, and data storage of the Samtla system. We also present several case
studies of its usage in practice together with an evaluation of the systems'
ranking performance through crowdsourcing.
"
1544,"CONDITOR1: Topic Maps and DITA labelling tool for textual documents with
  historical information","  Conditor is a software tool which works with textual documents containing
historical information. The purpose of this work two-fold: firstly to show the
validity of the developed engine to correctly identify and label the entities
of the universe of discourse with a labelled-combined XTM-DITA model. Secondly
to explain the improvements achieved in the information retrieval process
thanks to the use of a object-oriented database (JPOX) as well as its
integration into the Lucene-type database search process to not only accomplish
more accurate searches, but to also help the future development of a
recommender system. We finish with a brief demo in a 3D-graph of the results of
the aforementioned search.
"
1545,"Altmetrics of ""altmetrics"" using Google Scholar, Twitter, Mendeley,
  Facebook, Google-plus, CiteULike, Blogs and Wiki","  We measure the impact of ""altmetrics"" field by deploying altmetrics
indicators using the data from Google Scholar, Twitter, Mendeley, Facebook,
Google-plus, CiteULike, Blogs and Wiki during 2010- 2014. To capture the social
impact of scientific publications, we propose an index called alt-index,
analogues to h-index. Across the deployed indices, our results have shown high
correlation among the indicators that capture social impact. While we observe
medium Pearson's correlation (\r{ho}= .247) among the alt-index and h-index, a
relatively high correlation is observed between social citations and scholarly
citations (\r{ho}= .646). Interestingly, we find high turnover of social
citations in the field compared with the traditional scholarly citations, i.e.
social citations are 42.2 % more than traditional citations. The social mediums
such as Twitter and Mendeley appear to be the most effective channels of social
impact followed by Facebook and Google-plus. Overall, altmetrics appears to be
working well in the field of ""altmetrics"".
"
1546,"Measuring Book Impact Based on the Multi-granularity Online Review
  Mining","  As with articles and journals, the customary methods for measuring books'
academic impact mainly involve citations, which is easy but limited to
interrogating traditional citation databases and scholarly book reviews,
Researchers have attempted to use other metrics, such as Google Books,
libcitation, and publisher prestige. However, these approaches lack
content-level information and cannot determine the citation intentions of
users. Meanwhile, the abundant online review resources concerning academic
books can be used to mine deeper information and content utilizing altmetric
perspectives. In this study, we measure the impacts of academic books by
multi-granularity mining online reviews, and we identify factors that affect a
book's impact. First, online reviews of a sample of academic books on Amazon.cn
are crawled and processed. Then, multi-granularity review mining is conducted
to identify review sentiment polarities and aspects' sentiment values. Lastly,
the numbers of positive reviews and negative reviews, aspect sentiment values,
star values, and information regarding helpfulness are integrated via the
entropy method, and lead to the calculation of the final book impact scores.
The results of a correlation analysis of book impact scores obtained via our
method versus traditional book citations show that, although there are
substantial differences between subject areas, online book reviews tend to
reflect the academic impact. Thus, we infer that online reviews represent a
promising source for mining book impact within the altmetric perspective and at
the multi-granularity content level. Moreover, our proposed method might also
be a means by which to measure other books besides academic publications.
"
1547,Citations: Indicators of Quality? The Impact Fallacy,"  We argue that citation is a composed indicator: short-term citations can be
considered as currency at the research front, whereas long-term citations can
contribute to the codification of knowledge claims into concept symbols.
Knowledge claims at the research front are more likely to be transitory and are
therefore problematic as indicators of quality. Citation impact studies focus
on short-term citation, and therefore tend to measure not epistemic quality,
but involvement in current discourses in which contributions are positioned by
referencing. We explore this argument using three case studies: (1) citations
of the journal Soziale Welt as an example of a venue that tends not to publish
papers at a research front, unlike, for example, JACS; (2) Robert Merton as a
concept symbol across theories of citation; and (3) the Multi-RPYS
(""Multi-Referenced Publication Year Spectroscopy"") of the journals
Scientometrics, Gene, and Soziale Welt. We show empirically that the
measurement of ""quality"" in terms of citations can further be qualified:
short-term citation currency at the research front can be distinguished from
longer-term processes of incorporation and codification of knowledge claims
into bodies of knowledge. The recently introduced Multi-RPYS can be used to
distinguish between short-term and long-term impacts.
"
1548,Back to the past: on the shoulders of an academic search engine giant,"  A study released by the Google Scholar team found an apparently increasing
fraction of citations to old articles from studies published in the last 24
years (1990-2013). To demonstrate this finding we conducted a complementary
study using a different data source (Journal Citation Reports), metric
(aggregate cited half-life), time spam (2003-2013), and set of categories (53
Social Science subject categories and 167 Science subject categories). Although
the results obtained confirm and reinforce the previous findings, the possible
causes of this phenomenon keep unclear. We finally hypothesize that first page
results syndrome in conjunction with the fact that Google Scholar favours the
most cited documents are suggesting the growing trend of citing old documents
is partly caused by Google Scholar.
"
1549,"Day of the week effect in paper submission/acceptance/rejection to/in/by
  peer review journals","  This paper aims at providing an introduction to the behavior of authors
submitting a paper to a scientific journal. Dates of electronic submission of
papers to the Journal of the Serbian Chemical Society have been recorded from
the 1st January 2013 till the 31st December 2014, thus over 2 years.
  There is no Monday or Friday effect like in financial markets, but rather a
Tuesday-Wednesday effect occurs: papers are more often submitted on Wednesday;
however, the relative number of going to be accepted papers is larger if these
are submitted on Tuesday. On the other hand, weekend days (Saturday and Sunday)
are not the best days to finalize and submit manuscripts. An interpretation
based on the type of submitted work (""experimental chemistry"") and on the
influence of (senior) coauthors is presented. A thermodynamic connection is
proposed within an entropy context. A (new) entropic distance is defined in
order to measure the ""opaqueness"" = disorder) of the submission process.
"
1550,$C^3$-index: Revisiting Authors' Performance Measure,"  Author performance indices (such as h-index and its variants) fail to resolve
ties while ranking authors with low index values (majority in number) which
includes the young researchers. In this work we leverage the citations as well
as collaboration profile of an author in a novel way using a weighted
multi-layered network and propose a variant of page-rank algorithm to obtain a
new author performance measure, $C^3$-index. Experiments on a massive
publication dataset reveal several interesting characteristics of our metric:
(i) we observe that $C^3$-index is consistent over time, (ii) $C^3$-index has
high potential to break ties among low rank authors, (iii) $C^3$-index can
effectively be used to predict future achievers at the early stage of their
career.
"
1551,On the Composition of Scientific Abstracts,"  Scientific abstracts contain what is considered by the author(s) as
information that best describe documents' content. They represent a compressed
view of the informational content of a document and allow readers to evaluate
the relevance of the document to a particular information need. However, little
is known on their composition. This paper contributes to the understanding of
the structure of abstracts, by comparing similarity between scientific
abstracts and the text content of research articles. More specifically, using
sentence-based similarity metrics, we quantify the phenomenon of text re-use in
abstracts and examine the positions of the sentences that are similar to
sentences in abstracts in the IMRaD structure (Introduction, Methods, Results
and Discussion), using a corpus of over 85,000 research articles published in
the seven PLOS journals. We provide evidence that 84% of abstract have at least
one sentence in common with the body of the article. Our results also show that
the sections of the paper from which abstract sentence are taken are invariant
across the PLOS journals, with sentences mainly coming from the beginning of
the introduction and the end of the conclusion.
"
1552,"Construction of a Pragmatic Base Line for Journal Classifications and
  Maps Based on Aggregated Journal-Journal Citation Relations","  A number of journal classification systems have been developed in
bibliometrics since the launch of the Citation Indices by the Institute of
Scientific Information (ISI) in the 1960s. These systems are used to normalize
citation counts with respect to field-specific citation patterns. The best
known system is the so-called ""Web-of-Science Subject Categories"" (WCs). In
other systems papers are classified by algorithmic solutions. Using the Journal
Citation Reports 2014 of the Science Citation Index and the Social Science
Citation Index (n of journals = 11,149), we examine options for developing a
new system based on journal classifications into subject categories using
aggregated journal-journal citation data. Combining routines in VOSviewer and
Pajek, a tree-like classification is developed. At each level one can generate
a map of science for all the journals subsumed under a category. Nine major
fields are distinguished at the top level. Further decomposition of the social
sciences is pursued for the sake of example with a focus on journals in
information science (LIS) and science studies (STS). The new classification
system improves on alternative options by avoiding the problem of randomness in
each run that has made algorithmic solutions hitherto irreproducible.
Limitations of the new system are discussed (e.g. the classification of
multi-disciplinary journals). The system's usefulness for field-normalization
in bibliometrics should be explored in future studies.
"
1553,"DSRS: Estimation and Forecasting of Journal Influence in the Science and
  Technology Domain via a Lightweight Quantitative Approach","  The evaluation of journals based on their influence is of interest for
numerous reasons. Various methods of computing a score have been proposed for
measuring the scientific influence of scholarly journals. Typically the
computation of any of these scores involves compiling the citation information
pertaining to the journal under consideration. This involves significant
overhead since the article citation information of not only the journal under
consideration but also that of other journals for the recent few years need to
be stored. Our work is motivated by the idea of developing a computationally
lightweight approach that does not require any data storage, yet yields a score
which is useful for measuring the importance of journals. In this paper, a
regression analysis based method is proposed to calculate Journal Influence
Score. Proposed model is validated using historical data from the SCImago
portal. The results show that the error is small between rankings obtained
using the proposed method and the SCImago Journal Rank, thus proving that the
proposed approach is a feasible and effective method of calculating scientific
impact of journals.
"
1554,Growing Wikipedia Across Languages via Recommendation,"  The different Wikipedia language editions vary dramatically in how
comprehensive they are. As a result, most language editions contain only a
small fraction of the sum of information that exists across all Wikipedias. In
this paper, we present an approach to filling gaps in article coverage across
different Wikipedia editions. Our main contribution is an end-to-end system for
recommending articles for creation that exist in one language but are missing
in another. The system involves identifying missing articles, ranking the
missing articles according to their importance, and recommending important
missing articles to editors based on their interests. We empirically validate
our models in a controlled experiment involving 12,000 French Wikipedia
editors. We find that personalizing recommendations increases editor engagement
by a factor of two. Moreover, recommending articles increases their chance of
being created by a factor of 3.2. Finally, articles created as a result of our
recommendations are of comparable quality to organically created articles.
Overall, our system leads to more engaged editors and faster growth of
Wikipedia with no effect on its quality.
"
1555,"Liposomes versus metallic nanostructures: differences in the process of
  knowledge translation in cancer","  This research maps the knowledge translation process for two different types
of nanotechnologies applied to cancer: liposomes and metallic nanostructures
(MNs). We performed a structural analysis of citation networks and text mining
supported in controlled vocabularies. In the case of liposomes, our results
identify subnetworks (invisible colleges) associated with different therapeutic
strategies: nanopharmacology, hyperthermia, and gene therapy. Only in the
pharmacological strategy was an organized knowledge translation process
identified, which, however, is monopolized by the liposomal doxorubicins. In
the case of MNs, subnetworks are not differentiated by the type of therapeutic
strategy, and the content of the documents is still basic research. Research on
MNs is highly focused on developing a combination of molecular imaging and
photothermal therapy.
"
1556,"Referenced Publication Year Spectroscopy (RPYS) and Algorithmic
  Historiography: The Bibliometric Reconstruction of Andr\'as Schubert's
  {\OE}uvre","  Referenced Publication Year Spectroscopy (RPYS) was recently introduced as a
method to analyze the historical roots of research fields and groups or
institutions. RPYS maps the distribution of the publication years of the cited
references in a document set. In this study, we apply this methodology to the
{\oe}uvre of an individual researcher on the occasion of a Festschrift for
Andr\'as Schubert's 70th birthday. We discuss the different options of RPYS in
relation to one another (e.g. Multi-RPYS), and in relation to the longer-term
research program of algorithmic historiography (e.g., HistCite) based on
Schubert's publications (n=172) and cited references therein as a bibliographic
domain in scientometrics. Main path analysis and Multi-RPYS of the citation
network are used to show the changes and continuities in Schubert's
intellectual career. Diachronic and static decomposition of a document set can
lead to different results, while the analytically distinguishable lines of
research may overlap and interact over time, and intermittent.
"
1557,"Characterization, description, and considerations for the use of funding
  acknowledgement data in Web of Science","  Funding acknowledgements found in scientific publications have been used to
study the impact of funding on research since the 1970s. However, no broad
scale indexation of that paratextual element was done until 2008, when Thomson
Reuters Web of Science started to add funding acknowledgement information to
its bibliographic records. As this new information provides a new dimension to
bibliometric data that can be systematically exploited, it is important to
understand the characteristics of these data and the underlying implications
for their use. This paper analyses the presence and distribution of funding
acknowledgement data covered in Web of Science. Our results show that prior to
2009 funding acknowledgements coverage is extremely low and therefore not
reliable. Since 2008, funding information has been collected mainly for
publications indexed in the Science Citation Index Expanded (SCIE); more
recently (2015), inclusion of funding texts for publications indexed in the
Social Science Citation Index (SSCI) has been implemented. Arts & Humanities
Citation Index (AHCI) content is not indexed for funding acknowledgement data.
Moreover, English-language publications are the most reliably covered. Finally,
not all types of documents are equally covered for funding information
indexation and only articles and reviews show consistent coverage. The
characterization of the funding acknowledgement information collected by
Thomson Reuters can therefore help understand the possibilities offered by the
data but also their limitations.
"
1558,h-Index Manipulation by Undoing Merges,"  The h-index is an important bibliographic measure used to assess the
performance of researchers. Dutiful researchers merge different versions of
their articles in their Google Scholar profile even though this can decrease
their h-index. In this article, we study the manipulation of the h-index by
undoing such merges. In contrast to manipulation by merging articles (van
Bevern et al. [Artif. Intel. 240:19-35, 2016]) such manipulation is harder to
detect. We present numerous results on computational complexity (from
linear-time algorithms to parameterized computational hardness results) and
empirically indicate that at least small improvements of the h-index by
splitting merged articles are unfortunately easily achievable.
"
1559,"Funding Data from Publication Acknowledgements: Coverage, Uses and
  Limitations","  This article contributes to the development of methods for analysing research
funding systems by exploring the robustness and comparability of emerging
approaches to generate funding landscapes useful for policy making. We use a
novel dataset of manually extracted and coded data on the funding
acknowledgements of 7,510 publications representing UK cancer research in the
year 2011 and compare these 'reference data' with funding data provided by Web
of Science (WoS) and MEDLINE/PubMed. Findings show high recall (about 93%) of
WoS funding data. By contrast, MEDLINE/PubMed data retrieved less than half of
the UK cancer publications acknowledging at least one funder. Conversely, both
databases have high precision (+90%): i.e. few cases of publications with no
acknowledgement to funders are identified as having funding data. Nonetheless,
funders acknowledged in UK cancer publications were not correctly listed by
MEDLINE/PubMed and WoS in about 75% and 32% of the cases, respectively.
'Reference data' on the UK cancer research funding system are then used as a
case-study to demonstrate the utility of funding data for strategic
intelligence applications (e.g. mapping of funding landscape, comparison of
funders' research portfolios).
"
1560,"A Search/Crawl Framework for Automatically Acquiring Scientific
  Documents","  Despite the advancements in search engine features, ranking methods,
technologies, and the availability of programmable APIs, current-day
open-access digital libraries still rely on crawl-based approaches for
acquiring their underlying document collections. In this paper, we propose a
novel search-driven framework for acquiring documents for scientific portals.
Within our framework, publicly-available research paper titles and author names
are used as queries to a Web search engine. Next, research papers and sources
of research papers are identified from the search results using accurate
classification modules. Our experiments highlight not only the performance of
our individual classifiers but also the effectiveness of our overall
Search/Crawl framework. Indeed, we were able to obtain approximately 0.665
million research documents through our fully-automated framework using about
0.076 million queries. These prolific results position Web search as an
effective alternative to crawl methods for acquiring both the actual documents
and seed URLs for future crawls.
"
1561,Annual Journal citation indices: a comparative study,"  We study the statistics of citations made to the indexed Science journals in
the Journal Citation Reports during the period 2004-2013 using different
measures. We consider different measures which quantify the impact of the
journals. To our surprise, we find that the apparently uncorrelated measures,
even when defined in an arbitrary manner, show strong correlations. This is
checked over all the years considered. Impact factor being one of these
measures, the present work raises the question whether it is actually a nearly
perfect index as claimed often. In addition we study the distributions of the
different indices which also behave similarly.
"
1562,"Comparing Published Scientific Journal Articles to Their Pre-print
  Versions","  Academic publishers claim that they add value to scholarly communications by
coordinating reviews and contributing and enhancing text during publication.
These contributions come at a considerable cost: U.S. academic libraries paid
$1.7 billion for serial subscriptions in 2008 alone. Library budgets, in
contrast, are flat and not able to keep pace with serial price inflation. We
have investigated the publishers' value proposition by conducting a comparative
study of pre-print papers and their final published counterparts. This
comparison had two working assumptions: 1) if the publishers' argument is
valid, the text of a pre-print paper should vary measurably from its
corresponding final published version, and 2) by applying standard similarity
measures, we should be able to detect and quantify such differences. Our
analysis revealed that the text contents of the scientific papers generally
changed very little from their pre-print to final published versions. These
findings contribute empirical indicators to discussions of the added value of
commercial publishers and therefore should influence libraries' economic
decisions regarding access to scholarly publications.
"
1563,"Sleeping Beauties Cited in Patents: Is there also a Dormitory of
  Inventions?","  A Sleeping Beauty in Science is a publication that goes unnoticed (sleeps)
for a long time and then, almost suddenly, attracts a lot of attention (is
awakened by a prince). In our foregoing study we found that roughly half of the
Sleeping Beauties are application-oriented and thus are potential Sleeping
Innovations. In this paper we investigate a new topic: Sleeping Beauties that
are cited in patents. In this way we explore the existence of a dormitory of
inventions. We find that patent citation may occur before or after the
awakening and that the depth of the sleep, i.e., citation rate during the
sleeping period, is no predictor for later scientific or technological impact
of the Sleeping Beauty. Inventor-author self-citations occur only in a small
minority of the Sleeping Beauties that are cited in patents, but other types of
inventor-author links occur more frequently. We analyze whether they deal with
new topics by measuring the time-dependent evolution in the entire scientific
literature of the number of papers related to both the precisely defined topics
as well as the broader research theme of the Sleeping Beauty during and after
the sleeping time. We focus on the awakening by analyzing the first group of
papers that cites the Sleeping Beauty. Next, we create concept maps of the
topic-related and the citing papers for a time period immediately following the
awakening and for the most recent period. Finally, we make an extensive
assessment of the cited and citing relations of the Sleeping Beauty. We find
that tunable co-citation analysis is a powerful tool to discover the prince and
other important application-oriented work directly related to the Sleeping
Beauty, for instance papers written by authors who cite Sleeping Beauties in
both the patents of which they are the inventors, as well as in their
scientific papers.
"
1564,"Reconstructing a website's lost past: Methodological issues concerning
  the history of www.unibo.it","  This paper describes how born digital primary sources could be used to
reconstruct the recent history of scientific institutions. The case study is an
analysis of the first 25 years online of the University of Bologna. The focus
of this work is primarily methodological: several different issues are
presented, starting with the fact that the University of Bologna website has
been excluded for thirteen years from the Internet Archive's Wayback Machine,
and possible solutions are proposed and applied. The article is organised in
three parts: in the first one, some of the fundamental concepts on web archives
and the preservation of born digital sources are introduced. Then the
reconstruction of the University of Bologna web's past is presented. Finally
the future of this research is described, presenting a specific case study in
which the historian's craft will be challenged by a completely different issue,
namely the large amount of data available in the university digital library.
"
1565,"Decomposition and Analysis of Technological domains for better
  understanding of Technological Structure","  Patents represent one of the most complete sources of information related to
technological change. This paper presents three months of research on U.S.
patents in the field of patent analysis. The methodology consists of using
search terms to locate the most representative international and US patent
classes and determines the overlap of those classes to arrive at the final set
of patents and using the prediction model developed by Benson and Magee to
calculate the technological improvement rate for the technological domains. My
research focused on the Biochemical Pharmacology technological area and
selecting relevant patents for technological domains and sub-domains within
this area. The goal is to better understand structure of technology domain and
understand how fast the domains and their sub-domains progress. The method I
used is developed by Benson and Magee which is called the Classification
Overlap Method1, it provides a reliable and largely automated way to break the
patent database into understandable technological domains where progress can be
measured.
"
1566,"Are there too many uncited articles? Zero inflated variants of the
  discretised lognormal and hooked power law distributions","  Although statistical models fit many citation data sets reasonably well with
the best fitting models being the hooked power law and discretised lognormal
distribution, the fits are rarely close. One possible reason is that there
might be more uncited articles than would be predicted by any model if some
articles are inherently uncitable. Using data from 23 different Scopus
categories, this article tests the assumption that removing a proportion of
uncited articles from a citation dataset allows statistical distributions to
have much closer fits. It also introduces two new models, zero inflated
discretised lognormal distribution and the zero inflated hooked power law
distribution and algorithms to fit them. In all 23 cases, the zero inflated
version of the discretised lognormal distribution was an improvement on the
standard version and in 15 out of 23 cases the zero inflated version of the
hooked power law was an improvement on the standard version. Without zero
inflation the discretised lognormal models fit the data better than the hooked
power law distribution 6 out of 23 times and with it, the discretised lognormal
models fit the data better than the hooked power law distribution 9 out of 23
times. Apparently uncitable articles seem to occur due to the presence of
academic-related magazines in Scopus categories. In conclusion, future citation
analysis and research indicators should take into account uncitable articles,
and the best fitting distribution for sets of citation counts from a single
subject and year is either the zero inflated discretised lognormal or zero
inflated hooked power law.
"
1567,"Evaluating Journal Quality: A Review of Journal Citation Indicators and
  Ranking in Business and Management","  Evaluating the quality of academic journal is becoming increasing important
within the context of research performance evaluation. Traditionally, journals
have been ranked by peer review lists such as that of the Association of
Business Schools (UK) or though their journal impact factor (JIF). However,
several new indicators have been developed, such as the h-index, SJR, SNIP and
the Eigenfactor which take into account different factors and therefore have
their own particular biases. In this paper we evaluate these metrics both
theoretically and also through an empirical study of a large set of business
and management journals. We show that even though the indicators appear highly
correlated in fact they lead to large differences in journal rankings. We
contextualize our results in terms of the UK's large scale research assessment
exercise (the RAE/REF) and particularly the ABS journal ranking list. We
conclude that no one indicator is superior but that the h-index (which includes
the productivity of a journal) and SNIP (which aims to normalize for field
effects) may be the most effective at the moment.
"
1568,Knowledge Ratings in MetaboLights,"  This technical report presents an evaluation of the ontology annotations in
the metadata of a subset of entries of MetaboLights, a database for
Metabolomics experiments and derived information. The work includes a manual
analysis of the entries and a comprehensive qualitative evaluation of their
annotations, together with the evaluation guide and its rationale, that was
defined and followed. The approach was also implemented as a software script
that given a MetaboLights entry returns a quantitative evaluation of the
quality of its annotations (available on request).
"
1569,Universal Indexes for Highly Repetitive Document Collections,"  Indexing highly repetitive collections has become a relevant problem with the
emergence of large repositories of versioned documents, among other
applications. These collections may reach huge sizes, but are formed mostly of
documents that are near-copies of others. Traditional techniques for indexing
these collections fail to properly exploit their regularities in order to
reduce space.
  We introduce new techniques for compressing inverted indexes that exploit
this near-copy regularity. They are based on run-length, Lempel-Ziv, or grammar
compression of the differential inverted lists, instead of the usual practice
of gap-encoding them. We show that, in this highly repetitive setting, our
compression methods significantly reduce the space obtained with classical
techniques, at the price of moderate slowdowns. Moreover, our best methods are
universal, that is, they do not need to know the versioning structure of the
collection, nor that a clear versioning structure even exists.
  We also introduce compressed self-indexes in the comparison. These are
designed for general strings (not only natural language texts) and represent
the text collection plus the index structure (not an inverted index) in
integrated form. We show that these techniques can compress much further, using
a small fraction of the space required by our new inverted indexes. Yet, they
are orders of magnitude slower.
"
1570,"Connecting every bit of knowledge: The structure of Wikipedia's First
  Link Network","  Apples, porcupines, and the most obscure Bob Dylan song---is every topic a
few clicks from Philosophy? Within Wikipedia, the surprising answer is yes:
nearly all paths lead to Philosophy. Wikipedia is the largest, most
meticulously indexed collection of human knowledge ever amassed. More than
information about a topic, Wikipedia is a web of naturally emerging
relationships. By following the first link in each article, we algorithmically
construct a directed network of all 4.7 million articles: Wikipedia's First
Link Network. Here, we study the English edition of Wikipedia's First Link
Network for insight into how the many articles on inventions, places, people,
objects, and events are related and organized.
  By traversing every path, we measure the accumulation of first links, path
lengths, groups of path-connected articles, and cycles. We also develop a new
method, traversal funnels, to measure the influence each article exerts in
shaping the network. Traversal funnels provides a new measure of influence for
directed networks without spill-over into cycles, in contrast to traditional
network centrality measures. Within Wikipedia's First Link Network, we find
scale-free distributions describe path length, accumulation, and influence. Far
from dispersed, first links disproportionately accumulate at a few
articles---flowing from specific to general and culminating around fundamental
notions such as Community, State, and Science. Philosophy directs more paths
than any other article by two orders of magnitude. We also observe a
gravitation towards topical articles such as Health Care and Fossil Fuel. These
findings enrich our view of the connections and structure of Wikipedia's ever
growing store of knowledge.
"
1571,Quantifying the changing role of past publications,"  Our current societies increasingly rely on electronic repositories of
collective knowledge. An archetype of these databases is the Web of Science
(WoS) that stores scientific publications. In contrast to several other forms
of knowledge -- e.g., Wikipedia articles -- a scientific paper does not change
after its ""birth"". Nonetheless, from the moment a paper is published it exists
within the evolving web of other papers, thus, its actual meaning to the reader
changes. To track how scientific ideas (represented by groups of scientific
papers) appear and evolve, we apply a novel combination of algorithms
explicitly allowing for papers to change their groups. We (i) identify the
overlapping clusters of the undirected yearly co-citation networks of the WoS
(1975-2008) and (ii) match these yearly clusters (groups) to form group
timelines. After visualizing the longest lived groups of the entire data set we
assign topic labels to the groups. We find that in the entire Web of Science
multidisciplinarity is clearly over-represented among cutting edge ideas. In
addition, we provide detailed examples for papers that (i) change their topic
labels and (ii) move between groups.
"
1572,"A geometric graph model of citation networks with linearly growing
  node-increment","  Due to the fact that the numbers of annually published papers have witnessed
a linear growth in some citation networks, a geometric model is thus proposed
to predict some statistical features of those networks, in which the academic
influence scopes of the papers are denoted through specific geometric areas
related to time and space. In the model, nodes (papers) are uniformly and
randomly sprinkled onto a cluster of circles of the Minkowski space whose
centers are on the time axis. Edges (citations) are linked according to an
influence mechanism which indicates that an existing paper will be cited by a
new paper located in its influence zone. Considering the citations among papers
in different disciplines, an interdisciplinary citation mechanism is added to
the model in which some papers with a small probability of being chosen will
cite some existing papers randomly and uniformly. Different from most existing
models that only study the power-law tail of the in-degree distribution, this
model also characterizes the overall in-degree distribution. Moreover, it
presents the description of some other important statistical characteristics of
real networks, such as in- and out-assortativity, giant component and clear
community structure. Therefore, it is reasonable to believe that a good example
is provided in the paper to study real networks by geometric graphs.
"
1573,"Enhancing semantic expressivity in the cultural heritage domain:
  exposing the Zeri Photo Archive as Linked Open Data","  Describing cultural heritage objects from the perspective of Linked Open Data
(LOD) is not a trivial task. The process often requires not only choosing
pertinent ontologies, but also developing new models that preserve the most
information and express the semantic power of cultural heritage data. Indeed,
data managed in archives, libraries and museums are complex objects themselves,
which require a deep reflection on even non-conventional conceptual models.
Starting from these considerations, this paper describes a research project: to
expose the vastness of one of the most important collections of European
cultural heritage, the Zeri Photo Archive, as Linked Open Data. We describe
here the steps we undertook to this end: firstly, we developed two ad hoc
ontologies for describing all the issues not completely covered by existent
models (the F Entry and the OA Entry Ontology); then we mapped into RDF the
descriptive elements used in the current Zeri Photo Archive catalog, converting
into CIDOC-CRM and into the two new aforementioned models the source data based
on the Italian content standards Scheda F (Photography Entry, in English) and
Scheda OA (Work of Art Entry, in English); and finally, we created an RDF
dataset of the output of the mapping that could show a result capable of
demonstrating the complexity of our scenario.
"
1574,"ScientoBASE: A Framework and Model for Computing Scholastic Indicators
  of non-local influence of Journals via Native Data Acquisition algorithms","  Defining and measuring internationality as a function of influence diffusion
of scientific journals is an open problem. There exists no metric to rank
journals based on the extent or scale of internationality. Measuring
internationality is qualitative, vague, open to interpretation and is limited
by vested interests. With the tremendous increase in the number of journals in
various fields and the unflinching desire of academics across the globe to
publish in ""international"" journals, it has become an absolute necessity to
evaluate, rank and categorize journals based on internationality. Authors, in
the current work have defined internationality as a measure of influence that
transcends across geographic boundaries. There are concerns raised by the
authors about unethical practices reflected in the process of journal
publication whereby scholarly influence of a select few are artificially
boosted, primarily by resorting to editorial maneuvres. To counter the impact
of such tactics, authors have come up with a new method that defines and
measures internationality by eliminating such local effects when computing the
influence of journals. A new metric, Non-Local Influence Quotient(NLIQ) is
proposed as one such parameter for internationality computation along with
another novel metric, Other-Citation Quotient as the complement of the ratio of
self-citation and total citation. In addition, SNIP and International
Collaboration Ratio are used as two other parameters.
"
1575,"Mapping knowledge translation and innovation processes in Cancer Drug
  Development: the case of liposomal doxorubicin","  We explored how the knowledge translation and innovation processes are
structured when they result in innovations, as in the case of liposomal
doxorubicin research. In order to map the processes, a literature network
analysis was made through Cytoscape and semantic analysis was performed by
GOPubmed which is based in the controlled vocabularies MeSH (Medical Subject
Headings) and GO (Gene Ontology). We found clusters related to different stages
of the technological development (invention, innovation and imitation) and the
knowledge translation process (preclinical, translational and clinical
research), and we were able to map the historic emergence of Doxil as a
paradigmatic nanodrug. This research could be a powerful methodological tool
for decision-making and innovation management in drug delivery research.
"
1576,End-to-end evaluation of research organizations,"  End-to-end research evaluation needs to separate out the bibliometric part of
the chain from the econometric part. We first introduce the role of
size-dependent and size-independent indicators in the bibliometric part of the
evaluation chain. We show that performance can then be evaluated at various
levels, namely a zeroth-order, a first-order or even a second-order. To
complete the evaluation chain, we take up the econometric part where efficiency
of the research production process is represented in terms of output and
outcome productivities. Both size-dependent and size-independent terms play a
crucial role to combine quantity and quality (impact) in a meaningful way.
Output or outcome at the bibliometric level can be measured using zeroth, first
or second-order composite indicators, and the productivity terms follow
accordingly using the input to output or outcome factors.
"
1577,"Formation of subject area and the co-authors network by sounding of
  Google Scholar Citations service","  The suggested methodic is the way of formatting the subject areas models and
co-authors networks by sounding the content networks. The paper represents the
notion networks which match tags and authors of Google Scholar Citations
service. Models depicted in the work were built for the physical optics area,
and it can be applied for other domains. The proposed ways of defining
connections between science areas and authors depicts the collaborations
opportunities and versatility of interdisciplinary.
"
1578,"The elephant in the room: The problem of quantifying productivity in
  evaluative scientometrics","  In a critical and provocative paper, Abramo and D'Angelo claim that commonly
used scientometric indicators such as the mean normalized citation score (MNCS)
are completely inappropriate as indicators of scientific performance. Abramo
and D'Angelo argue that scientific performance should be quantified using
indicators that take into account the productivity of a research unit. We
provide a response to Abramo and D'Angelo, indicating where we believe they
raise important issues, but also pointing out where we believe their claims to
be too extreme.
"
1579,"The Normalization of Co-authorship Networks in the Bibliometric
  Evaluation: The Government Stimulation Programs of China and Korea","  Using co-authored publications between China and Korea in Web of Science
(WoS) during the one-year period of 2014, we evaluate the government
stimulation program for collaboration between China and Korea. In particular,
we apply dual approaches, full integer vs. fractional counting, to
collaborative publications in order to better examine both the patterns and
contents of Sino-Korean collaboration networks in terms of individual countries
and institutions. We first conduct a semi-automatic network analysis of
Sino-Korean publications based on the full-integer counting method, and then
compare our categorization with contextual rankings using the fractional
technique; routines for fractional counting of WoS data are made available at
http://www.leydesdorff.net/software/fraction . Increasing international
collaboration leads paradoxically to lower numbers of publications and
citations using fractional counting for performance measurement. However,
integer counting is not an appropriate measure for the evaluation of the
stimulation of collaborations. Both integer and fractional analytics can be
used to identify important countries and institutions, but with other research
questions.
"
1580,"Citation distribution of individual scientist: approximations of stretch
  exponential distribution with power law tails","  A multi-parametric family of stretch exponential distributions with various
power law tails is introduced and is shown to describe adequately the empirical
distributions of scientific citation of individual authors. The four-parametric
families are characterized by a normalization coefficient in the exponential
part, the power exponent in the power-law asymptotic part, and the coefficient
for the transition between the above two parts. The distribution of papers of
individual scientist over citations of these papers is studied. Scientists are
selected via total number of citations in three ranges: 102-103, 103-104, and
104-105 of total citations. We study these intervals for physicists in ISI Web
of Knowledge. The scientists who started their scientific publications after
1980 were taken into consideration only. It is detected that the power
coefficient in the stretch exponent starts from one for low-cited authors and
has to trend to smaller values for scientists with large number of citation. At
the same time, the power coefficient in tail drops for large-cited authors. One
possible explanation for the origin of the stretch-exponential distribution for
citation of individual author is done.
"
1581,Semantometrics: Towards Fulltext-based Research Evaluation,"  Over the recent years, there has been a growing interest in developing new
research evaluation methods that could go beyond the traditional citation-based
metrics. This interest is motivated on one side by the wider availability or
even emergence of new information evidencing research performance, such as
article downloads, views and Twitter mentions, and on the other side by the
continued frustrations and problems surrounding the application of purely
citation-based metrics to evaluate research performance in practice.
Semantometrics are a new class of research evaluation metrics which build on
the premise that full-text is needed to assess the value of a publication. This
paper reports on the analysis carried out with the aim to investigate the
properties of the semantometric contribution measure, which uses semantic
similarity of publications to estimate research contribution, and provides a
comparative study of the contribution measure with traditional bibliometric
measures based on citation counting.
"
1582,Viziometrics: Analyzing Visual Information in the Scientific Literature,"  Scientific results are communicated visually in the literature through
diagrams, visualizations, and photographs. These information-dense objects have
been largely ignored in bibliometrics and scientometrics studies when compared
to citations and text. In this paper, we use techniques from computer vision
and machine learning to classify more than 8 million figures from PubMed into 5
figure types and study the resulting patterns of visual information as they
relate to impact. We find that the distribution of figures and figure types in
the literature has remained relatively constant over time, but can vary widely
across field and topic. Remarkably, we find a significant correlation between
scientific impact and the use of visual information, where higher impact papers
tend to include more diagrams, and to a lesser extent more plots and
photographs. To explore these results and other ways of extracting this visual
information, we have built a visual browser to illustrate the concept and
explore design alternatives for supporting viziometric analysis and organizing
visual information. We use these results to articulate a new research agenda --
viziometrics -- to study the organization and presentation of visual
information in the scientific literature.
"
1583,Web Infrastructure to Support e-Journal Preservation (and More),"  E-journal preservation systems have to ingest millions of articles each year.
Ingest, especially of the ""long tail"" of journals from small publishers, is the
largest element of their cost. Cost is the major reason that archives contain
less than half the content they should. Automation is essential to minimize
these costs. This paper examines the potential for automation beyond the status
quo based on the API provided by CrossRef, ANSI/NISO Z39.99 ResourceSync, and
the provision of typed links in publishers' HTTP response headers. These
changes would not merely assist e-journal preservation and other cross-venue
scholarly applications, but would help remedy the gap that research has
revealed between DOIs' potential and actual benefits.
"
1584,Visualization of Publication Impact,"  Measuring scholarly impact has been a topic of much interest in recent years.
While many use the citation count as a primary indicator of a publications
impact, the quality and impact of those citations will vary. Additionally, it
is often difficult to see where a paper sits among other papers in the same
research area. Questions we wished to answer through this visualization were:
is a publication cited less than publications in the field?; is a publication
cited by high or low impact publications?; and can we visually compare the
impact of publications across a result set? In this work we address the above
questions through a new visualization of publication impact. Our technique has
been applied to the visualization of citation information in INSPIREHEP
(http://www.inspirehep.net), the largest high energy physics publication
repository.
"
1585,A note concerning Primary Source Knowledge,"  We add a small increment to understanding the notion of Primary Source
Knowledge, knowledge that the non-expert and the citizen can acquire by
assiduously reading the primary scientific journal literature without being
embedded in the cultural life of the corresponding technical specialty. This
comes from exposing four papers to the automated computer filters used by the
physics preprint server arXiv. These filters are used to flag papers in need of
further review by human assessors before being promulgated on the server;
papers not flagged by the algorithm are generally posted on arXiv without
further review. After the filtering, human moderators decide whether papers
should be posted based on a relatively low bar of whether they are of interest,
relevance and value to the research communities that populate arXiv.
"
1586,"Impact of lexical and sentiment factors on the popularity of scientific
  papers","  We investigate how textual properties of scientific papers relate to the
number of citations they receive. Our main finding is that correlations are
non-linear and affect differently most-cited and typical papers. For instance,
we find that in most journals short titles correlate positively with citations
only for the most cited papers, for typical papers the correlation is in most
cases negative. Our analysis of 6 different factors, calculated both at the
title and abstract level of 4.3 million papers in over 1500 journals, reveals
the number of authors, and the length and complexity of the abstract, as having
the strongest (positive) influence on the number of citations.
"
1587,Ruling Out Static Latent Homophily in Citation Networks,"  Citation and coauthor networks offer an insight into the dynamics of
scientific progress. We can also view them as representations of a causal
structure, a logical process captured in a graph. From a causal perspective, we
can ask questions such as whether authors form groups primarily due to their
prior shared interest, or if their favourite topics are 'contagious' and spread
through co-authorship. Such networks have been widely studied by the artificial
intelligence community, and recently a connection has been made to nonlocal
correlations produced by entangled particles in quantum physics -- the impact
of latent hidden variables can be analyzed by the same algebraic geometric
methodology that relies on a sequence of semidefinite programming (SDP)
relaxations. Following this trail, we treat our sample coauthor network as a
causal graph and, using SDP relaxations, rule out latent homophily as a
manifestation of prior shared interest leading to the observed patternedness.
By introducing algebraic geometry to citation studies, we add a new tool to
existing methods for the analysis of content-related social influences.
"
1588,"Models Coupling Urban Growth and Transportation Network Growth : An
  Algorithmic Systematic Review Approach","  A broad bibliographical study suggests a scarcity of quantitative models of
simulation integrating both network and urban growth. This absence may be due
to diverging interests of concerned disciplines, resulting in a lack of
communication. We propose to proceed to an algorithmic systematic review to
give quantitative elements of answer to this question. A formal iterative
algorithm to retrieve corpuses of references from initial keywords, based on
text-mining, is developed and implemented. We study its convergence properties
and do a sensitivity analysis. We then apply it on queries representative of
the specific question, for which results tend to confirm the assumption of
disciplines compartmentalisation.
"
1589,Efficiency of research performance and the glass researcher,"  Abramo and D'Angelo (in press) doubt the validity of established
size-independent indicators measuring citation impact and plead in favor of
measuring scientific efficiency (by using the Fractional Scientific Strength
indicator). This note is intended to comment on some questionable and a few
favorable approaches in the paper by Abramo and D'Angelo (in press).
"
1590,"Using almetrics for contextualised mapping of societal impact: From hits
  to networks","  In this article, we develop a method that uses altmetric data to analyse
researchers' interactions, as a way of mapping the contexts of potential
societal impact. In the face of an increasing policy demand for quantitative
methodologies to assess societal impact, social media data (altmetrics) has
been presented as a potential method to capture broader forms of impact.
However, current altmetric indicators were extrapolated from traditional
citation approaches and are seen as problematic for assessing societal impact.
In contrast, established qualitative methodologies for societal impact
assessment are based on interaction approaches. These argue that assessment
should focus on mapping the contexts in which engagement among researchers and
stakeholders take place, as a means to understand the pathways to societal
impact. Following these approaches, we propose to shift the use of altmetric
data towards network analysis of researchers and stakeholders. We carry out two
case studies, analysing researchers' networks with Twitter data. The comparison
illustrates the potential of Twitter networks to capture disparate degrees of
policy engagement. We propose that this mapping method can be used as an input
within broader methodologies in case studies of societal impact assessment.
"
1591,"Tracing scientific mobility of Early Career Researchers in Spain and The
  Netherlands through their publications","  International scientific mobility is acknowledged to be a key mechanism for
the diffusion of knowledge, particularly tacit or 'sticky' knowledge that
cannot be transferred without geographical proximity and personal contact, for
the incorporation of young researchers into elite transnational scientific
networks, and for accessing additional resources or infrastructures that are
essential to the research process but located in other places. The inadequacy
and lack of appropriate data to assess the phenomenon of researcher mobility
has been repeatedly pointed out by scholars and policy makers. This paper
presents an exploratory analysis of different typologies of researchers
according to their traceable mobility using scientific publications covered in
the Web of Science (WoS). We compare two populations of researchers, of the
same 'scientific age', based in Spain and The Netherlands. We observe
differences in the degree of mobility of Spain and Netherlands based
researchers. Factors associated with the different institutional conditions
characterizing the two national systems need to be taken into account. First,
the Spanish and Dutch university and research systems are different in many
ways. Second, there may be very different institutional incentives for mobility
in the two systems. More sophisticated bibliometric analyses and comparisons
with different 'generations' of researchers, possibly combined with qualitative
investigation, will be required to better understand the role and function of
national institutional context in both research mobility and research careers.
"
1592,"Not dead, just resting: The practical value of per publication citation
  indicators","  In the final analysis citation-based indicators are inferior to effective
peer review and even peer review is flawed. It is impossible to accurately
measure the value or impact of scientific research and a key task of
scientometricians should be to produce figures for policy makers and others
that are as informative as it is practical to make them and to ensure that
users are fully aware of their limitations. Although the Abramo and D'Angelo
(2016) suggestions make a lot of theoretical sense and so are a goal that is
worth aiming for, it is unrealistic in practice to advocate their universal use
in the contexts discussed above. This is because the indicators would still
have flaws in addition to the generic limitations of citation-based indicators
and would still be inadequate for replacing peer review. Thus, the expense of
the data gathering does not always justify the value in practice of the extra
accuracy. In the longer term, the restructuring of education needed in order to
get the homogeneity necessary for genuinely comparable statistics would be too
expensive and probably damaging to the research mission, in addition to being
out of proportion to the likely value of any citation-based indicator.
"
1593,"Can we use altmetrics at the institutional level? A case study analysing
  the coverage by research areas of four Spanish universities","  Social media based indicators or altmetrics have been under scrutiny for the
last seven years. Their promise as alternative metrics for measuring scholarly
impact is still far from becoming a reality. Up to now, most studies have
focused on the understanding of the nature and relation of altmetric indicators
with citation data. Few papers have analysed research profiles based on
altmetric data. Most of these have related to researcher profiles and the
expansion of these tools among researchers. This paper aims at exploring the
coverage of the Altmetric.com database and its potential use in order to show
universities' research profiles in relationship with other databases. We
analyse a sample of four different Spanish universities.First, we observe a low
coverage of altmetric indicators with only 36 percent of all documents
retrieved from the Web of Science having an 'altmetric' score. Second, we
observe that for the four universities analysed, the area of Science shows
higher 'altmetric' scores that the rest of the research areas. Finally,
considering the low coverage of altmetric data at the institutional level, it
could be interesting for research policy makers to consider the development of
guidelines and best practices guides to ensure that researchers disseminate
adequately their research findings through social media.
"
1594,"Using network centrality measures to improve national journal
  classification lists","  In countries like Denmark and Spain classified journal lists are now being
produced and used in the calculation of nationwide performance indicators. As a
result, Danish and Spanish scholars are advised to contribute to journals of
high 'authority' (as in the former) or those within a high class (as in the
latter). This can create a few problems. The aim of this paper is to analyse
the potential use of network centrality measures to identify possible
mismatches of journal categories. It analysis the Danish National Authority
List and the Spanish CIRC Classification. Based on a sample of Library and
Information Science publications, it analyses centrality measures that can
assess on the importance of journals to given fields, correcting mismatches in
these classifications. We conclude by emphasising the use of these measures to
better calibrate journal classifications as we observe a general bias in these
lists towards older journals. Centrality measures can allow to identify
periphery-to-core journals' transitions.
"
1595,New model for datasets citation and extraction reproducibility in VAMDC,"  In this paper we present a new paradigm for the identification of datasets
extracted from the Virtual Atomic and Molecular Data Centre (VAMDC) e-science
infrastructure. Such identification includes information on the origin and
version of the datasets, references associated to individual data in the
datasets, as well as timestamps linked to the extraction procedure. This
paradigm is described through the modifications of the language used to
exchange data within the VAMDC and through the services that will implement
those modifications. This new paradigm should enforce traceability of datasets,
favour reproducibility of datasets extraction, and facilitate the systematic
citation of the authors having originally measured and/or calculated the
extracted atomic and molecular data.
"
1596,"Maturity of the Internet of Things Research Field: Or Why Choose
  Rigorous Keywords","  Periodically researchers examine their fields of research in order to test
the fields maturity, to determine theories and methods used, and to uncover
areas of research emphasis. Research articles for these studies are generally
located through keyword search, then manually read and coded in order to
summarise the research field. This research tests using metadata analysis
alone, on the internet of things (IoT) research field, using data extracted
from three academic databases. The metadata analysis was proposed to reduce
time in analysis, and allow for testing of a much larger sample of articles.
Findings indicated that the IoT research field was immature, with experimental
methods dominating research outputs. However, difficulties with keyword
metadata included synonyms, abbreviations, lack of accuracy, and method and
theory not being included in metadata. A keyword schema is suggested in order
to relieve these problems in future articles, and to assist researchers in
locating relevant literature.
"
1597,"Cloud-Based Big Data Management and Analytics for Scholarly Resources:
  Current Trends, Challenges and Scope for Future Research","  With the shifting focus of organizations and governments towards digitization
of academic and technical documents, there has been an increasing need to use
this reserve of scholarly documents for developing applications that can
facilitate and aid in better management of research. In addition to this, the
evolving nature of research problems has made them essentially
interdisciplinary. As a result, there is a growing need for scholarly
applications like collaborator discovery, expert finding and research
recommendation systems. This research paper reviews the current trends and
identifies the challenges existing in the architecture, services and
applications of big scholarly data platform with a specific focus on directions
for future research.
"
1598,"Evaluating Co-Authorship Networks in Author Name Disambiguation for
  Common Names","  With the increasing size of digital libraries it has become a challenge to
identify author names correctly. The situation becomes more critical when
different persons share the same name (homonym problem) or when the names of
authors are presented in several different ways (synonym problem). This paper
focuses on homonym names in the computer science bibliography DBLP. The goal of
this study is to evaluate a method which uses co-authorship networks and
analyze the effect of common names on it. For this purpose we clustered the
publications of authors with the same name and measured the effectiveness of
the method against a gold standard of manually assigned DBLP records. The
results show that despite the good performance of implemented method for most
names, we should optimize for common names. Hence community detection was
employed to optimize the method. Results prove that the applied method improves
the performance for these names.
"
1599,"How marketing vocabulary was evolving from 2005 to 2014? An illustrative
  application of statistical methods on text mining","  Here a collection of 1169 abstracts, which corresponds to articles that the
Journal of Marketing Research has published from 2005 to 2014, are analysed
under a novel approach. We apply several statistical methods, such as Principal
Components Analysis and Correspondence Analysis to identify the way Marketing
vocabulary is evolving. Similarly those articles that introduce new vocabulary
are identified and the preferred words by authors are also detected. In order
to provide an easy-to-understand explanation, we provide our results
graphically. A word-cloud with the most frequent words is given first. Secondly
abstracts-words are represented on the factorial plane. Finally one
representation of word-years allows us to detect changes on the vocabulary
through the passing of time.
"
1600,"Credit allocation based on journal impact factor and coauthorship
  contribution","  Some research institutions demand researchers to distribute the incomes they
earn from publishing papers to their researchers and/or co-authors. In this
study, we deal with the Impact Factor-based ranking journal as a criteria for
the correct distribution of these incomes. We also include the Authorship
Credit factor for distribution of the incomes among authors, using the
geometric progression of Cantor's theory and the Harmonic Credit Index.
Depending on the ranking of the journal, the proposed model develops a proper
publication credit allocation among all authors. Moreover, our tool can be
deployed in the evaluation of an institution for a funding program, as well as
calculating the amounts necessary to incentivize research among personnel.
"
1601,Model-based evaluation of scientific impact indicators,"  Using bibliometric data artificially generated through a model of citation
dynamics calibrated on empirical data, we compare several indicators for the
scientific impact of individual researchers. The use of such a controlled setup
has the advantage of avoiding the biases present in real databases, and allows
us to assess which aspects of the model dynamics and which traits of individual
researchers a particular indicator actually reflects. We find that the simple
citation average performs well in capturing the intrinsic scientific ability of
researchers, whatever the length of their career. On the other hand, when
productivity complements ability in the evaluation process, the notorious $h$
and $g$ indices reveal their potential, yet their normalized variants do not
always yield a fair comparison between researchers at different career stages.
Notably, the use of logarithmic units for citation counts allows us to build
simple indicators with performance equal to that of $h$ and $g$. Our analysis
may provide useful hints for a proper use of bibliometric indicators.
Additionally, our framework can be extended by including other aspects of the
scientific production process and citation dynamics, with the potential to
become a standard tool for the assessment of impact metrics.
"
1602,Physical aging in article page views,"  Statistics of article page views is useful for measuring the impact of
individual articles. Analyzing the temporal evolution of article page views, we
find that article page views usually decay over time after reaching a peak,
especially exhibiting relaxation with nonexponentiality. This finding suggests
that relaxation in article page views resembles physical aging as frequently
found in complex systems.
"
1603,"Automatic quality evaluation and (semi-) automatic improvement of OCR
  models for historical printings","  Good OCR results for historical printings rely on the availability of
recognition models trained on diplomatic transcriptions as ground truth, which
is both a scarce resource and time-consuming to generate. Instead of having to
train a separate model for each historical typeface, we propose a strategy to
start from models trained on a combined set of available transcriptions in a
variety of fonts. These \emph{mixed models} result in character accuracy rates
over 90\% on a test set of printings from the same period of time, but without
any representation in the training data, demonstrating the possibility to
overcome the typography barrier by generalizing from a few typefaces to a
larger set of (similar) fonts in use over a period of time. The output of these
mixed models is then used as a baseline to be further improved by both fully
automatic methods and semi-automatic methods involving a minimal amount of
manual transcriptions. In order to evaluate the recognition quality of each
model in a series of models generated during the training process in the
absence of any ground truth, we introduce two readily observable quantities
that correlate well with true accuracy. These quantities are \emph{mean
character confidence C} (as given by the OCR engine OCRopus) and \emph{mean
token lexicality L} (a distance measure of OCR tokens from modern wordforms
taking historical spelling patterns into account, which can be calculated for
any OCR engine). Whereas the fully automatic method is able to improve upon the
result of a mixed model by only 1-2 percentage points, already 100-200
hand-corrected lines lead to much better OCR results with character error rates
of only a few percent. This procedure minimizes the amount of ground truth
production and does not depend on the previous construction of a specific
typographic model.
"
1604,"Proceedings Scholar Metrics: H Index of proceedings on Computer Science,
  Electrical & Electronic Engineering, and Communications according to Google
  Scholar Metrics (2010-2014)","  The objective of this report is to present a list of proceedings
(conferences, workshops, symposia, meetings) in the areas of Computer Science,
Electrical & Electronic Engineering, and Communications covered by Google
Scholar Metrics and ranked according to their h-index. Google Scholar Metrics
only displays publications that have published at least 100 papers and have
received at least one citation in the last five years (2010-2014). The searches
were conducted between the 8th and 10th of December, 2015. A total of 1501
proceedings have been identified
"
1605,Research on Wikipedia Vandalism: a brief literature review,"  Research on vandalism in Wikipedia has been of interest for the last decade.
This paper performs a literature review on the subject, with the goal of
identifying the main research topics and approaches, methods and techniques
used. 67 papers have been reviewed. Main topic is the detection of vandalism,
although there is a increasing interest about content quality. The most
commonly used technique is machine learning, based on feature analysis. It
draws attention to the lack of research on information behavior of vandals.
"
1606,"Connecting web-based mapping services with scientific data repositories:
  collaborative curation and retrieval of simulation data via a geospatial
  interface","  Increasing quantities of scientific data are becoming readily accessible via
online repositories such as those provided by Figshare and Zenodo.
Geoscientific simulations in particular generate large quantities of data, with
several research groups studying many, often overlapping areas of the world.
When studying a particular area, being able to keep track of one's own
simulations as well as those of collaborators can be challenging. This paper
describes the design, implementation, and evaluation of a new tool for visually
cataloguing and retrieving data associated with a given geographical location
through a web-based Google Maps interface. Each data repository is pin-pointed
on the map with a marker based on the geographical location that the dataset
corresponds to. By clicking on the markers, users can quickly inspect the
metadata of the repositories and download the associated data files. The crux
of the approach lies in the ability to easily query and retrieve data from
multiple sources via a common interface. While many advances are being made in
terms of scientific data repositories, the development of this new tool has
uncovered several issues and limitations of the current state-of-the-art which
are discussed herein, along with some ideas for the future.
"
1607,Identifying the Academic Rising Stars,"  Predicting the fast-rising young researchers (Academic Rising Stars) in the
future provides useful guidance to the research community, e.g., offering
competitive candidates to university for young faculty hiring as they are
expected to have success academic careers. In this work, given a set of young
researchers who have published the first first-author paper recently, we solve
the problem of how to effectively predict the top k% researchers who achieve
the highest citation increment in \Delta t years. We explore a series of
factors that can drive an author to be fast-rising and design a novel impact
increment ranking learning (IIRL) algorithm that leverages those factors to
predict the academic rising stars. Experimental results on the large ArnetMiner
dataset with over 1.7 million authors demonstrate the effectiveness of IIRL.
Specifically, it outperforms all given benchmark methods, with over 8% average
improvement. Further analysis demonstrates that the prediction models for
different research topics follow the similar pattern. We also find that
temporal features are the best indicators for rising stars prediction, while
venue features are less relevant.
"
1608,Can Scientific Impact Be Predicted?,"  A widely used measure of scientific impact is citations. However, due to
their heavy-tailed distribution, citations are fundamentally difficult to
predict. Instead, to characterize scientific impact, we address two analogous
questions asked by many scientific researchers: ""How will my h-index evolve
over time, and which of my previously or newly published papers will contribute
to it?"" To answer these questions, we perform two related tasks. First, we
develop a model to predict authors' future h-indices based on their current
scientific impact. Second, we examine the factors that drive papers---either
previously or newly published---to increase their authors' predicted future
h-indices. By leveraging relevant factors, we can predict an author's h-index
in five years with an R2 value of 0.92 and whether a previously (newly)
published paper will contribute to this future h-index with an F1 score of 0.99
(0.77). We find that topical authority and publication venue are crucial to
these effective predictions, while topic popularity is surprisingly
inconsequential. Further, we develop an online tool that allows users to
generate informed h-index predictions. Our work demonstrates the predictability
of scientific impact, and can help scholars to effectively leverage their
position of ""standing on the shoulders of giants.""
"
1609,Instruments on large optical telescopes -- A case study,"  In the distant past, telescopes were known, first and foremost, for the sizes
of their apertures. Advances in technology are now enabling astronomers to
build extremely powerful instruments to the extent that instruments have now
achieved importance comparable or even exceeding the usual importance accorded
to the apertures of the telescopes. However, the cost of successive generations
of instruments has risen at a rate noticeably above that of the rate of
inflation. Here, given the vast sums of money now being expended on optical
telescopes and their instrumentation, I argue that astronomers must undertake
""cost-benefit"" analysis for future planning. I use the scientific output of the
first two decades of the W. M. Keck Observatory as a laboratory for this
purpose. I find, in the absence of upgrades, that the time to reach peak paper
production for an instrument is about six years. The prime lifetime of
instruments (sans upgrades), as measured by citations returns, is about a
decade. Well thought out and timely upgrades increase and sometimes even double
the useful lifetime. I investigate how well instrument builders are rewarded. I
find acknowledgements ranging from almost 100% to as low as 60%. Next, given
the increasing cost of operating optical telescopes, the management of existing
observatories continue to seek new partnerships. This naturally raises the
question ""What is the cost of a single night of telescope time"". I provide a
rational basis to compute this quantity. I then end the paper with some
thoughts on the future of large ground-based optical telescopes, bearing in
mind the explosion of synoptic precision photometric, astrometric and imaging
surveys across the electromagnetic spectrum, the increasing cost of
instrumentation and the rise of mega instruments.
"
1610,"Static Ranking of Scholarly Papers using Article-Level Eigenfactor
  (ALEF)","  Microsoft Research hosted the 2016 WSDM Cup Challenge based on the Microsoft
Academic Graph. The goal was to provide static rankings for the articles that
make up the graph, with the rankings to be evaluated against those of human
judges. While the Microsoft Academic Graph provided metadata about many aspects
of each scholarly document, we focused more narrowly on citation data and used
this contest as an opportunity to test the Article Level Eigenfactor (ALEF), a
novel citation-based ranking algorithm, and evaluate its performance against
competing algorithms that drew upon multiple facets of the data from a large,
real world dataset (122M papers and 757M citations). Our final submission to
this contest was scored at 0.676, earning second place.
"
1611,Routing Memento Requests Using Binary Classifiers,"  The Memento protocol provides a uniform approach to query individual web
archives. Soon after its emergence, Memento Aggregator infrastructure was
introduced that supports querying across multiple archives simultaneously. An
Aggregator generates a response by issuing the respective Memento request
against each of the distributed archives it covers. As the number of archives
grows, it becomes increasingly challenging to deliver aggregate responses while
keeping response times and computational costs under control. Ad-hoc heuristic
approaches have been introduced to address this challenge and research has been
conducted aimed at optimizing query routing based on archive profiles. In this
paper, we explore the use of binary, archive-specific classifiers generated on
the basis of the content cached by an Aggregator, to determine whether or not
to query an archive for a given URI. Our results turn out to be readily
applicable and can help to significantly decrease both the number of requests
and the overall response times without compromising on recall. We find, among
others, that classifiers can reduce the average number of requests by 77%
compared to a brute force approach on all archives, and the overall response
time by 42% while maintaining a recall of 0.847.
"
1612,"Men Set Their Own Cites High: Gender and Self-citation across Fields and
  over Time","  How common is self-citation in scholarly publication, and does the practice
vary by gender? Using novel methods and a data set of 1.5 million research
papers in the scholarly database JSTOR published between 1779 and 2011, the
authors find that nearly 10 percent of references are self-citations by a
paper's authors. The findings also show that between 1779 and 2011, men cited
their own papers 56 percent more than did women. In the last two decades of
data, men self-cited 70 percent more than women. Women are also more than 10
percentage points more likely than men to not cite their own previous work at
all. While these patterns could result from differences in the number of papers
that men and women authors have published rather than gender-specific patterns
of self-citation behavior, this gender gap in self-citation rates has remained
stable over the last 50 years, despite increased representation of women in
academia. The authors break down self-citation patterns by academic field and
number of authors and comment on potential mechanisms behind these
observations. These findings have important implications for scholarly
visibility and cumulative advantage in academic careers.
"
1613,"Corporate system of monitoring network informational resources based on
  agent-based approach","  The paper provides a agent-based model, which describes distribution of
informative messages, containing links to informational resources in the
Internet. The results of modeling have been confirmed by studying a real
network of Twitter microblogs. The paper describes stages of building a
corporate system of monitoring network informational resources, the content of
which is determined by links in microblogs. The advantages of such approach are
set forth.
"
1614,"Citation score normalized by cited references (CSNCR): The introduction
  of a new citation impact indicator","  In this paper, a new field-normalized indicator is introduced, which is
rooted in early insights in bibliometrics, and is compared with several
established field-normalized indicators (e.g. the mean normalized citation
score, MNCS, and indicators based on percentile approaches). Garfield (1979)
emphasizes that bare citation counts from different fields cannot be compared
for evaluative purposes, because the ""citation potential"" can vary
significantly between the fields. Garfield (1979) suggests that ""the most
accurate measure of citation potential is the average number of references per
paper published in a given field"". Based on this suggestion, the new indicator
is basically defined as follows: the citation count of a focal paper is divided
by the mean number of cited references in a field to normalize citations. The
new indicator is called citation score normalized by cited references (CSNCR).
The theoretical analysis of the CSNCR shows that it has the properties of
consistency and homogeneous normalization. The close relation of the new
indicator to the MNCS is discussed. The empirical comparison of the CSNCR with
other field-normalized indicators shows that it is slightly poorer able to
field-normalize citation counts than other cited-side normalized indicators
(e.g. the MNCS), but its results are favorable compared to two citing-side
indicator variants (SNCS indicators). Taken as a whole, the results of this
study confirm the ability of established indicators to field-normalize
citations.
"
1615,New features of CitedReferencesExplorer (CRExplorer),"  CRExplorer version 1.6.7 was released on July 5, 2016. This version includes
the following new features and improvements: Scopus: Using ""File"" - ""Import"" -
""Scopus"", CRExplorer reads files from Scopus. The file format ""CSV"" (including
citations, abstracts and references) should be chosen in Scopus for downloading
records. Export facilities: Using ""File"" - ""Export"" - ""Scopus"", CRExplorer
exports files in the Scopus format. Using ""File"" - ""Export"" - ""Web of Science"",
CRExplorer exports files in the Web of Science format. These files can be
imported in other bibliometric programs (e.g. VOSviewer). Space bar: Select a
specific cited reference in the cited references table, press the space bar,
and all bibliographic details of the CR are shown. Internal file format: Using
""File"" - ""Save"", working files are saved in the internal file format ""*.cre"".
The files include all data including matching results and manual matching
corrections. The files can be opened by using ""File"" - ""Open"".
"
1616,"Constructing bibliometric networks: A comparison between full and
  fractional counting","  The analysis of bibliometric networks, such as co-authorship, bibliographic
coupling, and co-citation networks, has received a considerable amount of
attention. Much less attention has been paid to the construction of these
networks. We point out that different approaches can be taken to construct a
bibliometric network. Normally the full counting approach is used, but we
propose an alternative fractional counting approach. The basic idea of the
fractional counting approach is that each action, such as co-authoring or
citing a publication, should have equal weight, regardless of for instance the
number of authors, citations, or references of a publication. We present two
empirical analyses in which the full and fractional counting approaches yield
very different results. These analyses deal with co-authorship networks of
universities and bibliographic coupling networks of journals. Based on
theoretical considerations and on the empirical analyses, we conclude that for
many purposes the fractional counting approach is preferable over the full
counting one.
"
1617,"The Miracle of Peer Review and Development in Science: An Agent-Based
  Model","  It is not easy to rationalize how peer review, as the current grassroots of
science, can work based on voluntary contributions of reviewers. There is no
rationale to write impartial and thorough evaluations. Consequently, there is
no risk in submitting low-quality work by authors. As a result, scientists face
a social dilemma: if everyone acts according to his or her own self-interest,
low scientific quality is produced. Still, in practice, reviewers as well as
authors invest high effort in reviews and submissions.
  We examine how the increased relevance of public good benefits (journal
impact factor), the editorial policy of handling incoming reviews, and the
acceptance decisions that take into account reputational information can help
the evolution of high-quality contributions from authors. High effort from the
side of reviewers is problematic even if authors cooperate: reviewers are still
best off by producing low-quality reviews, which does not hinder scientific
development, just adds random noise and unnecessary costs to it. We show with
agent-based simulations that tacit agreements between authors that are based on
reciprocity might decrease these costs, but does not result in superior
scientific quality. Our study underlines why certain self-emerged current
practices, such as the increased importance of journal metrics, the
reputation-based selection of reviewers, and the reputation bias in acceptance
work efficiently for scientific development. Our results find no answers,
however, how the system of peer review with impartial and thorough evaluations
could be sustainable jointly with rapid scientific development.
"
1618,"A two-sided academic landscape: portrait of highly-cited documents in
  Google Scholar (1950-2013)","  The main objective of this paper is to identify the set of highly-cited
documents in Google Scholar and to define their core characteristics (document
types, language, free availability, source providers, and number of versions),
under the hypothesis that the wide coverage of this search engine may provide a
different portrait about this document set respect to that offered by the
traditional bibliographic databases. To do this, a query per year was carried
out from 1950 to 2013 identifying the top 1,000 documents retrieved from Google
Scholar and obtaining a final sample of 64,000 documents, of which 40% provided
a free full-text link. The results obtained show that the average highly-cited
document is a journal article or a book (62% of the top 1% most cited documents
of the sample), written in English (92.5% of all documents) and available
online in PDF format (86.0% of all documents). Yet, the existence of errors
especially when detecting duplicates and linking cites properly must be pointed
out. The fact of managing with highly cited papers, however, minimizes the
effects of these limitations. Given the high presence of books, and to a lesser
extend of other document types (such as proceedings or reports), the research
concludes that Google Scholar data offer an original and different vision of
the most influential academic documents (measured from the perspective of their
citation count), a set composed not only by strictly scientific material
(journal articles) but academic in its broad sense
"
1619,BioInfoBase : A Bioinformatics Resourceome,"  Over the past decade there has been a significant growth in bioinformatics
databases, tools and resources. Although, bioinformatics is becoming more
specific, increasing the number of bioinformatics-wares has made it difficult
for researchers to find the most appropriate databases, tools or methods which
match their needs. Our coordinated effort has been planned to establish a
reference website in Bioinformatics as a public repository of tools, databases,
directories and resources annotated with contextual information and organized
by functional relevance. Within the first phase of BioInfoBase development, 22
experts in different fields of molecular biology contributed and more than 2500
records were registered, which are increasing daily. For each record submitted
to the database of website almost all related data (40 features) has been
extracted. These include information from the biological category and
subcategory to the scientific article and developer information. Searching the
query keyword(s) returns links containing the entered keyword(s) found within
the different features of the records with more weights on the title, abstract
and application fields. The search results simply provide the users with the
most informative features of the records to select the most suitable ones. The
usefulness of the returned results is ranked according to the matching score
based on the Term Frequency-Inverse Document Frequency (TF-IDF) methods.
Therefore, this search engine will screen a comprehensive index of
bioinformatics tools, databases and resources and provide the best suited
records (links) to the researchers need. The BioInfoBase resource is available
at www.bioinfobase.info.
"
1620,"Citation success index - An intuitive pair-wise journal comparison
  metric","  In this paper we present ""citation success index"", a metric for comparing the
citation capacity of pairs of journals. Citation success index is the
probability that a random paper in one journal has more citations than a random
paper in another journal (50% means the two journals do equally well). Unlike
the journal impact factor (IF), the citation success index depends on the
broadness and the shape of citation distributions. Also, it is insensitive to
sporadic highly-cited papers that skew the IF. Nevertheless, we show, based on
16,000 journals containing ~2.4 million articles, that the citation success
index is a relatively tight function of the ratio of IFs of journals being
compared, due to the fact that journals with same IF have quite similar
citation distributions. The citation success index grows slowly as a function
of IF ratio. It is substantial (>90%) only when the ratio of IFs exceeds ~6,
whereas a factor of two difference in IF values translates into a modest
advantage for the journal with higher IF (index of ~70%). We facilitate the
wider adoption of this metric by providing an online calculator that takes as
input parameters only the IFs of the pair of journals.
"
1621,"Qualitative Judgement of Research Impact: Domain Taxonomy as a
  Fundamental Framework for Judgement of the Quality of Research","  The appeal of metric evaluation of research impact has attracted considerable
interest in recent times. Although the public at large and administrative
bodies are much interested in the idea, scientists and other researchers are
much more cautious, insisting that metrics are but an auxiliary instrument to
the qualitative peer-based judgement. The goal of this article is to propose
availing of such a well positioned construct as domain taxonomy as a tool for
directly assessing the scope and quality of research. We first show how
taxonomies can be used to analyse the scope and perspectives of a set of
research projects or papers. Then we proceed to define a research team or
researcher's rank by those nodes in the hierarchy that have been created or
significantly transformed by the results of the researcher. An experimental
test of the approach in the data analysis domain is described. Although the
concept of taxonomy seems rather simplistic to describe all the richness of a
research domain, its changes and use can be made transparent and subject to
open discussions.
"
1622,Citation count distributions for large monodisciplinary journals,"  Many different citation-based indicators are used by researchers and research
evaluators to help evaluate the impact of scholarly outputs. Although the
appropriateness of individual citation indicators depends in part on the
statistical properties of citation counts, there is no universally agreed
best-fitting statistical distribution against which to check them. The two
current leading candidates are the discretised lognormal and the hooked or
shifted power law. These have been mainly tested on sets of articles from a
single field and year but these collections can include multiple specialisms
that might dilute their properties. This article fits statistical distributions
to 50 large subject-specific journals in the belief that individual journals
can be purer than subject categories and may therefore give clearer findings.
The results show that in most cases the discretised lognormal fits
significantly better than the hooked power law, reversing previous findings for
entire subcategories. This suggests that the discretised lognormal is the more
appropriate distribution for modelling pure citation data. Thus future
analytical investigations of the properties of citation indicators can use the
lognormal distribution to analyse their basic properties. This article also
includes improved software for fitting the hooked power law.
"
1623,"Modeling the coevolution between citations and coauthorships in
  scientific papers","  Collaborations and citations within scientific research grow simultaneously
and interact dynamically. Modelling the coevolution between them helps to study
many phenomena that can be approached only through combining citation and
coauthorship data. A geometric graph for the coevolution is proposed, the
mechanism of which synthetically expresses the interactive impacts of authors
and papers in a geometrical way. The model is validated against a data set of
papers published on PNAS during 2007-2015. The validation shows the ability to
reproduce a range of features observed with citation and coauthorship data
combined and separately. Particularly, in the empirical distribution of
citations per author there exist two limits, in which the distribution appears
as a generalized Poisson and a power-law respectively. Our model successfully
reproduces the shape of the distribution, and provides an explanation for how
the shape emerges via the decisions of authors. The model also captures the
empirically positive correlations between the numbers of authors' papers,
citations and collaborators.
"
1624,"The Memory of Science: Inflation, Myopia, and the Knowledge Network","  Science is a growing system, exhibiting ~4% annual growth in publications and
~1.8% annual growth in the number of references per publication. Combined these
trends correspond to a 12-year doubling period in the total supply of
references, thereby challenging traditional methods of evaluating scientific
production, from researchers to institutions. Against this background, we
analyzed a citation network comprised of 837 million references produced by
32.6 million publications over the period 1965-2012, allowing for a temporal
analysis of the `attention economy' in science. Unlike previous studies, we
analyzed the entire probability distribution of reference ages - the time
difference between a citing and cited paper - thereby capturing previously
overlooked trends. Over this half-century period we observe a narrowing range
of attention - both classic and recent literature are being cited increasingly
less, pointing to the important role of socio-technical processes. To better
understand the impact of exponential growth on the underlying knowledge network
we develop a network-based model, featuring the redirection of scientific
attention via publications' reference lists, and validate the model against
several empirical benchmarks. We then use the model to test the causal impact
of real paradigm shifts, thereby providing guidance for science policy
analysis. In particular, we show how perturbations to the growth rate of
scientific output affects the reference age distribution and the functionality
of the vast science citation network as an aid for the search & retrieval of
knowledge. In order to account for the inflation of science, our study points
to the need for a systemic overhaul of the counting methods used to evaluate
citation impact - especially in the case of evaluating science careers, which
can span several decades and thus several doubling periods.
"
1625,A Supervised Authorship Attribution Framework for Bengali Language,"  Authorship Attribution is a long-standing problem in Natural Language
Processing. Several statistical and computational methods have been used to
find a solution to this problem. In this paper, we have proposed methods to
deal with the authorship attribution problem in Bengali.
"
1626,"2016 Google Scholar Metrics released: a matter of languages... and
  something else","  The 2016 edition of Google Scholar Metrics was released on July 15th 2016.
There haven't been any structural changes respect to previous versions, which
means that most of its limitations still persist. The biggest changes are the
addition of five new language rankings (Russian, Korean, Polish, Ukrainian, and
Indonesian) and elimination of two other language rankings (Italian and Dutch).
In addition, for reasons still unknown, this new edition doesn't include as
many working paper and discussion paper series as previous editions.
"
1627,"Cited References and Medical Subject Headings (MeSH) as Two Different
  Knowledge Representations: Clustering and Mappings at the Paper Level","  For the biomedical sciences, the Medical Subject Headings (MeSH) make
available a rich feature which cannot currently be merged properly with widely
used citing/cited data. Here, we provide methods and routines that make MeSH
terms amenable to broader usage in the study of science indicators: using
Web-of-Science (WoS) data, one can generate the matrix of citing versus cited
documents; using PubMed/MEDLINE data, a matrix of the citing documents versus
MeSH terms can be generated analogously. The two matrices can also be
reorganized into a 2-mode matrix of MeSH terms versus cited references. Using
the abbreviated journal names in the references, one can, for example, address
the question whether MeSH terms can be used as an alternative to WoS Subject
Categories for the purpose of normalizing citation data. We explore the
applicability of the routines in the case of a research program about the
amyloid cascade hypothesis in Alzheimer's disease (AD). One conclusion is that
referenced journals provide archival structures, whereas MeSH terms indicate
mainly variation (including novelty) at the research front. Furthermore, we
explore the option of using the citing/cited matrix for main-path analysis as a
by-product of the software.
"
1628,"Rightsstatements.org White Paper: Requirements for the Technical
  Infrastructure for Standardized International Rights Statements","  This document is part of the deliverables created by the RightsStatements.org
consortium. It provides the technical requirements for implementation of the
Standardized International Rights Statements. These requirements are based on
the principles and specifications found in the normative Recommendations for
Standardized International Rights Statements. This document replaces and
supersedes the previously released Recommendations for the Technical
Infrastructure for Standardized Rights Statements, released by this working
group. The Requirements for the Technical Infrastructure for Standardized
International Rights Statements describes the expected behaviours for a service
that enables the delivery of human and machine-readable representations of the
rights statements. It documents the fundamental decisions that informed the
development of a data model grounded in Linked Data approaches. This document
also provides proposed implementation guidelines and a non-normative set of
examples for incorporating rights statements into provider metadata.
"
1629,"How scientific literature has been evolving over the time? A novel
  statistical approach using tracking verbal-based methods","  This paper provides a global vision of the scientific publications related
with the Systemic Lupus Erythematosus (SLE), taking as starting point abstracts
of articles. Through the time, abstracts have been evolving towards higher
complexity on used terminology, which makes necessary the use of sophisticated
statistical methods and answering questions including: how vocabulary is
evolving through the time? Which ones are most influential articles? And which
one are the articles that introduced new terms and vocabulary? To answer these,
we analyze a dataset composed by 506 abstracts and downloaded from 115
different journals and cover a 18 year-period.
"
1630,"""Open Innovation"" and ""Triple Helix"" Models of Innovation: Can Synergy
  in Innovation Systems Be Measured?","  The model of ""Open Innovations"" (OI) can be compared with the ""Triple Helix
of University-Industry-Government Relations"" (TH) as attempts to find surplus
value in bringing industrial innovation closer to public R&D. Whereas the firm
is central in the model of OI, the TH adds multi-centeredness: in addition to
firms, universities and (e.g., regional) governments can take leading roles in
innovation eco-systems. In addition to the (transversal) technology transfer at
each moment of time, one can focus on the dynamics in the feedback loops. Under
specifiable conditions, feedback loops can be turned into feedforward ones that
drive innovation eco-systems towards self-organization and the auto-catalytic
generation of new options. The generation of options can be more important than
historical realizations (""best practices"") for the longer-term viability of
knowledge-based innovation systems. A system without sufficient options, for
example, is locked-in. The generation of redundancy -- the Triple Helix
indicator -- can be used as a measure of unrealized but technologically
feasible options given a historical configuration. Different coordination
mechanisms (markets, policies, knowledge) provide different perspectives on the
same information and thus generate redundancy. Increased redundancy not only
stimulates innovation in an eco-system by reducing the prevailing uncertainty;
it also enhances the synergy in and innovativeness of an innovation system.
"
1631,arXiv@25: Key findings of a user survey,"  As part of its 25th anniversary vision-setting process, the arXiv team at
Cornell University Library conducted a user survey in April 2016 to seek input
from the global user community about arXiv's current services and future
directions. We were heartened to receive 36,000 responses from 127 countries,
representing arXiv's diverse, global community. The prevailing message is that
users are happy with the service as it currently stands, with 95 percent of
survey respondents indicating they are very satisfied or satisfied with arXiv.
Furthermore, 72 percent of respondents indicated that arXiv should continue to
focus on its main purpose, which is to quickly make available scientific
papers, and this will be enough to sustain the value of arXiv in the future.
This theme was pervasively reflected in the open text comments; a significant
number of respondents suggested remaining focused on the core mission and
enabling arXiv's partners and related service providers to continue to build
new services and innovations on top of arXiv.
"
1632,"Growing complex network of citations of scientific papers --
  measurements and modeling","  To quantify the mechanism of a complex network growth we focus on the network
of citations of scientific papers and use a combination of the theoretical and
experimental tools to uncover microscopic details of this network growth.
Namely, we develop a stochastic model of citation dynamics based on
copying/redirection/triadic closure mechanism. In a complementary and coherent
way, the model accounts both for statistics of references of scientific papers
and for their citation dynamics. Originating in empirical measurements, the
model is cast in such a way that it can be verified quantitatively in every
aspect. Such verification is performed by measuring citation dynamics of
Physics papers. The measurements revealed nonlinear citation dynamics, the
nonlinearity being intricately related to network topology. The nonlinearity
has far-reaching consequences including non-stationary citation distributions,
diverging citation trajectory of similar papers, runaways or ""immortal papers""
with infinite citation lifetime etc. Thus, our most important finding is
nonlinearity in complex network growth. In a more specific context, our results
can be a basis for quantitative probabilistic prediction of citation dynamics
of individual papers and of the journal impact factor.
"
1633,"Hegemonic structure of basic, clinical and patented knowledge on Ebola
  research: a US army reductionist initiative","  Background: In this paper, we present an approach to understand how the
basic, clinical and patent knowledge on Ebola is organized and
intercommunicated and what leading factor could be shaping the evolution of the
knowledge translation process for this disease. Methodology: A combination of
citation network analysis; analysis of Medical heading Subject (MeSH) and Gene
Ontology (GO) terms, and quantitative content analysis for patents and
scientific literature, aimed to map the organization of Ebola research was
carried out. Results: We found six putative research fronts (i.e. clusters of
high interconnected papers). Three research fronts are basic research on Ebola
virus structural proteins: glycoprotein, VP40 and VP35, respectively. There is
a fourth research front of basic research papers on pathogenesis, which is the
organizing hub of Ebola research. A fifth research front is pre-clinical
research focused on vaccines and glycoproteins. Finally, a
clinical-epidemiology research front related to the disease outbreaks was
identified. The network structure of patent families shows that the dominant
design is the use of Ebola virus proteins as targets of vaccines and other
immunological treatments. Therefore, patents network organization resembles the
organization of the scientific literature. Specifically, the knowledge on Ebola
would flow from higher (clinical-epidemiology) to intermediated
(cellular-tissular pathogenesis) to lower (molecular interactions) levels of
organization. Conclusion: Our results suggest a strong reductionist approach
for Ebola research probably influenced by the lethality of the disease. On the
other hand, the ownership profile of the patent families network and the main
researches relationship with the United State Army suggest a strong involvement
of this military institution in Ebola research.
"
1634,Tracking the Digital Footprints to Scholarly Articles from Social Media,"  Scholarly articles are discussed and shared on social media, which generates
altmetrics. On the opposite side, what is the impact of social media on the
dissemination of scholarly articles and how to measure it? What are the
visiting patterns? Investigating these issues, the purpose of this study is to
seek a solution to fill the research gap, specifically, to explore the dynamic
visiting patterns directed by social media, and examine the effects of social
buzz on the article visits. Using the unique real referral data of 110
scholarly articles, which are daily updated in a 90-day period, this paper
proposes a novel method to make analysis. We find that visits from social media
are fast to accumulate but decay rapidly. Twitter and Facebook are the two most
important social referrals that directing people to scholarly articles, the two
are about the same and account for over 95% of the total social referral
directed visits. There is synchronism between tweets and tweets resulted
visits. Social media and open access are playing important roles in
disseminating scholarly articles and promoting public understanding science,
which are confirmed quantitatively for the first time with real data in this
study.
"
1635,"Is this conference a top-tier? ConfAssist: An assistive conflict
  resolution framework for conference categorization","  Classifying publication venues into top-tier or non top-tier is quite
subjective and can be debatable at times. In this paper, we propose ConfAssist,
a novel assisting framework for conference categorization that aims to address
the limitations in the existing systems and portals for venue classification.
We start with the hypothesis that top-tier conferences are much more stable
than other conferences and the inherent dynamics of these groups differs to a
very large extent. We identify various features related to the stability of
conferences that might help us separate a top-tier conference from the rest of
the lot. While there are many clear cases where expert agreement can be almost
immediately achieved as to whether a conference is a top-tier or not, there are
equally many cases that can result in a conflict even among the experts.
ConfAssist tries to serve as an aid in such cases by increasing the confidence
of the experts in their decision. An analysis of 110 conferences from 22
sub-fields of computer science clearly favors our hypothesis as the top-tier
conferences are found to exhibit much less fluctuations in the stability
related features than the non top-tier ones. We evaluate our hypothesis using
systems based on conference categorization. For the evaluation, we conducted
human judgment survey with 28 domain experts. The results are impressive with
85.18% classification accuracy. We also compare the dynamics of the newly
started conferences with the older conferences to identify the initial signals
of popularity. The system is applicable to any conference with atleast 5 years
of publication history.
"
1636,"OCR of historical printings with an application to building diachronic
  corpora: A case study using the RIDGES herbal corpus","  This article describes the results of a case study that applies Neural
Network-based Optical Character Recognition (OCR) to scanned images of books
printed between 1487 and 1870 by training the OCR engine OCRopus
[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].
Training specific OCR models was possible because the necessary *ground truth*
is available as error-corrected diplomatic transcriptions. The OCR results have
been evaluated for accuracy against the ground truth of unseen test sets.
Character and word accuracies (percentage of correctly recognized items) for
the resulting machine-readable texts of individual documents range from 94% to
more than 99% (character level) and from 76% to 97% (word level). This includes
the earliest printed books, which were thought to be inaccessible by OCR
methods until recently. Furthermore, OCR models trained on one part of the
corpus consisting of books with different printing dates and different typesets
*(mixed models)* have been tested for their predictive power on the books from
the other part containing yet other fonts, mostly yielding character accuracies
well above 90%. It therefore seems possible to construct generalized models
trained on a range of fonts that can be applied to a wide variety of historical
printings still giving good results. A moderate postcorrection effort of some
pages will then enable the training of individual models with even better
accuracies. Using this method, diachronic corpora including early printings can
be constructed much faster and cheaper than by manual transcription. The OCR
methods reported here open up the possibility of transforming our printed
textual cultural heritage into electronic text by largely automatic means,
which is a prerequisite for the mass conversion of scanned books.
"
1637,"Choosing Collaboration Partners. How Scientific Success in Physics
  Depends on Network Positions","  Physics is one of the most successful endeavors in science. Being a
prototypic big science it also reflects the growing tendency for scientific
collaborations. Utilizing 250,000 papers from ArXiv.org a prepublishing
platform prevalent in Physics we construct large coauthorship networks to
investigate how individual network positions influence scientific success. In
this context, success is seen as getting a paper published in high impact
journals of physical subdisciplines as compared to not getting it published at
all or in rather peripheral journals only. To control the nested levels of
authors and papers, and to consider the time elapsing between working paper and
prominent journal publication we employ multilevel eventhistory models with
various network measures as covariates. Our results show that the maintenance
of even a moderate number of persistent ties is crucial for scientific success.
Also, even with low volumes of social capital Physicists who occupy brokerage
positions enhance their chances of articles in high impact journals
significantly. Surprisingly, inter(sub)disciplinary collaborations decrease the
probability of getting a paper published in specialized journals for almost all
positions.
"
1638,"Anomalies in the peer-review system: A case study of the journal of High
  Energy Physics","  Peer-review system has long been relied upon for bringing quality research to
the notice of the scientific community and also preventing flawed research from
entering into the literature. The need for the peer-review system has often
been debated as in numerous cases it has failed in its task and in most of
these cases editors and the reviewers were thought to be responsible for not
being able to correctly judge the quality of the work. This raises a question
""Can the peer-review system be improved?"" Since editors and reviewers are the
most important pillars of a reviewing system, we in this work, attempt to
address a related question - given the editing/reviewing history of the editors
or re- viewers ""can we identify the under-performing ones?"", with citations
received by the edited/reviewed papers being used as proxy for quantifying
performance. We term such review- ers and editors as anomalous and we believe
identifying and removing them shall improve the performance of the peer- review
system. Using a massive dataset of Journal of High Energy Physics (JHEP)
consisting of 29k papers submitted between 1997 and 2015 with 95 editors and
4035 reviewers and their review history, we identify several factors which
point to anomalous behavior of referees and editors. In fact the anomalous
editors and reviewers account for 26.8% and 14.5% of the total editors and
reviewers respectively and for most of these anomalous reviewers the
performance degrades alarmingly over time.
"
1639,"End of Publication? Open access and a new scholarly communication
  technology","  At this time, developers of research information systems are experimenting
with new tools for research outputs usage that can expand the open access to
research. These tools allow researchers to record research as annotations,
nanopublications or other micro research outputs and link them by scientific
relationships. If these micro outputs and relationships are shared by their
creators publicly, these actions can initiate direct scholarly communication
between the creators and the authors of the used research outputs. Such direct
communication takes place while researchers are manipulating and organising
their research results, e.g. as manuscripts. Thus, researchers come to
communication before the manuscripts become traditional publications. In this
paper, we discuss how this pre-publication communication can affect existing
research practice. It can have important consequences for the research
community like the end of publication as a communication instrument, the higher
level of transparency in research, changes for the Open Access movement,
academic publishers, peer-reviewing and research assessment systems. We analyse
a background that exists in the economics discipline for experiments with the
pre-publication communication. We propose a set of experiments with already
existed and new tools, which can help with exploring the end of publication
possible impacts on the research community.
"
1640,"Detecting and Tracking The Real-time Hot Topics: A Study on
  Computational Neuroscience","  In this study, following the idea of our previous paper (Wang, et al.,
2013a), we improve the method to detect and track hot topics in a specific
field by using the real-time article usage data. With the ""usage count"" data
provided by Web of Science, we take the field of computational neuroscience as
an example to make analysis. About 10 thousand articles in the field of
Computational Neuroscience are queried in Web of Science, when the records,
including the usage count data of each paper, have been harvested and updated
weekly from October 19, 2015 to March 21, 2016. The hot topics are defined by
the most frequently used keywords aggregated from the articles. The analysis
reveals that hot topics in Computational Neuroscience are related to the key
technologies, like ""fmri"", ""eeg"", ""erp"", etc. Furthermore, using the weekly
updated data, we track the dynamical changes of the topics. The characteristic
of immediacy of usage data makes it possible to track the ""heat"" of hot topics
timely and dynamically.
"
1641,"A novel framework for assessing metadata quality in epidemiological and
  public health research settings","  Metadata are critical in epidemiological and public health research. However,
a lack of biomedical metadata quality frameworks and limited awareness of the
implications of poor quality metadata renders data analyses problematic. In
this study, we created and evaluated a novel framework to assess metadata
quality of epidemiological and public health research datasets. We performed a
literature review and surveyed stakeholders to enhance our understanding of
biomedical metadata quality assessment. The review identified 11 studies and
nine quality dimensions; none of which were specifically aimed at biomedical
metadata. 96 individuals completed the survey; of those who submitted data,
most only assessed metadata quality sometimes, and eight did not at all. Our
framework has four sections: a) general information; b) tools and technologies;
c) usability; and d) management and curation. We evaluated the framework using
three test cases and sought expert feedback. The framework can assess
biomedical metadata quality systematically and robustly.
"
1642,A Systematic Identification and Analysis of Scientists on Twitter,"  Metrics derived from Twitter and other social media---often referred to as
altmetrics---are increasingly used to estimate the broader social impacts of
scholarship. Such efforts, however, may produce highly misleading results, as
the entities that participate in conversations about science on these platforms
are largely unknown. For instance, if altmetric activities are generated mainly
by scientists, does it really capture broader social impacts of science? Here
we present a systematic approach to identifying and analyzing scientists on
Twitter. Our method can identify scientists across many disciplines, without
relying on external bibliographic data, and be easily adapted to identify other
stakeholder groups in science. We investigate the demographics, sharing
behaviors, and interconnectivity of the identified scientists. We find that
Twitter has been employed by scholars across the disciplinary spectrum, with an
over-representation of social and computer and information scientists;
under-representation of mathematical, physical, and life scientists; and a
better representation of women compared to scholarly publishing. Analysis of
the sharing of URLs reveals a distinct imprint of scholarly sites, yet only a
small fraction of shared URLs are science-related. We find an assortative
mixing with respect to disciplines in the networks between scientists,
suggesting the maintenance of disciplinary walls in social media. Our work
contributes to the literature both methodologically and conceptually---we
provide new methods for disambiguating and identifying particular actors on
social media and describing the behaviors of scientists, thus providing
foundational information for the construction and use of indicators on the
basis of social media metrics.
"
1643,Coauthorship and citation networks for statisticians: Comment,"  This is a comment on the paper arXiv:1410.2840 by Ji and Jin, to appear in
the AOAS.
"
1644,"The specific shapes of gender imbalance in scientific authorships: a
  network approach","  Gender differences in collaborative research have received little attention
when compared with the growing importance that women hold in academia and
research. Unsurprisingly, most of bibliometric databases have a strong lack of
directly available information by gender. Although empirical-based network
approaches are often used in the study of research collaboration, the studies
about the influence of gender dissimilarities on the resulting topological
outcomes are still scarce. Here, networks of scientific subjects are used to
characterize patterns that might be associated to five categories of
authorships which were built based on gender. We find enough evidence that
gender imbalance in scientific authorships brings a peculiar trait to the
networks induced from papers published in Web of Science (WoS) indexed journals
of Economics over the period 2010-2015 and having at least one author
affiliated to a Portuguese institution. Our results show the emergence of a
specific pattern when the network of co-occurring subjects is induced from a
set of papers exclusively authored by men. Such a male-exclusive authorship
condition is found to be the solely responsible for the emergence that
particular shape in the network structure. This peculiar trait might facilitate
future network analyses of research collaboration and interdisciplinarity.
"
1645,"Tracing technological development trajectories: A genetic knowledge
  persistence-based main path approach","  The aim of this paper is to propose a new method to identify main paths in a
technological domain using patent citations. Previous approaches for using main
path analysis have greatly improved our understanding of actual technological
trajectories but nonetheless have some limitations. They have high potential to
miss some dominant patents from the identified main paths; nonetheless, the
high network complexity of their main paths makes qualitative tracing of
trajectories problematic. The proposed method searches backward and forward
paths from the high-persistence patents which are identified based on a
standard genetic knowledge persistence algorithm. We tested the new method by
applying it to the desalination and the solar photovoltaic domains and compared
the results to output from the same domains using a prior method. The empirical
results show that the proposed method overcomes the aforementioned drawbacks
defining main paths that are almost 10x less complex while containing more of
the relevant important knowledge than the main path networks defined by the
existing method.
"
1646,"Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties:
  Using Changing Colors and Innovative Terms as Examples","  Tang (618-907 AD) and Song (960-1279) dynasties are two very important
periods in the development of Chinese literary. The most influential forms of
the poetry in Tang and Song were Shi and Ci, respectively. Tang Shi and Song Ci
established crucial foundations of the Chinese literature, and their influences
in both literary works and daily lives of the Chinese communities last until
today.
  We can analyze and compare the Complete Tang Shi and the Complete Song Ci
from various viewpoints. In this presentation, we report our findings about the
differences in their vocabularies. Interesting new words that started to appear
in Song Ci and continue to be used in modern Chinese were identified. Colors
are an important ingredient of the imagery in poetry, and we discuss the most
frequent color words that appeared in Tang Shi and Song Ci.
"
1647,TrueReview: A Platform for Post-Publication Peer Review,"  In post-publication peer review, scientific contributions are first published
in open-access forums, such as arXiv or other digital libraries, and are
subsequently reviewed and possibly ranked and/or evaluated. Compared to the
classical process of scientific publishing, in which review precedes
publication, post-publication peer review leads to faster dissemination of
ideas, and publicly-available reviews. The chief concern in post-publication
reviewing consists in eliciting high-quality, insightful reviews from
participants.
  We describe the mathematical foundations and structure of TrueReview, an
open-source tool we propose to build in support of post-publication review. In
TrueReview, the motivation to review is provided via an incentive system that
promotes reviews and evaluations that are both truthful (they turn out to be
correct in the long run) and informative (they provide significant new
information). TrueReview organizes papers in venues, allowing different
scientific communities to set their own submission and review policies. These
venues can be manually set-up, or they can correspond to categories in
well-known repositories such as arXiv. The review incentives can be used to
form a reviewer ranking that can be prominently displayed alongside papers in
the various disciplines, thus offering a concrete benefit to reviewers. The
paper evaluations, in turn, reward the authors of the most significant papers,
both via an explicit paper ranking, and via increased visibility in search.
"
1648,"Which early works are cited most frequently in climate change research
  literature? A bibliometric approach based on Reference Publication Year
  Spectroscopy","  This bibliometric analysis focuses on the general history of climate change
research and, more specifically, on the discovery of the greenhouse effect.
First, the Reference Publication Year Spectroscopy (RPYS) is applied to a large
publication set on climate change of 222,060 papers published between 1980 and
2014. The references cited therein were extracted and analyzed with regard to
publications, which are cited most frequently. Second, a new method for
establishing a more subject-specific publication set for applying RPYS (based
on the co-citations of a marker reference) is proposed (RPYS-CO). The RPYS of
the climate change literature focuses on the history of climate change research
in total. We identified 35 highly-cited publications across all disciplines,
which include fundamental early scientific works of the 19th century (with a
weak connection to climate change) and some cornerstones of science with a
stronger connection to climate change. By using the Arrhenius (1896) paper as a
RPYS-CO marker paper, we selected only publications specifically discussing the
discovery of the greenhouse effect and the role of carbon dioxide. Also, we
focused on the time period 1800-1850 to reveal the contributions of J.B.J
Fourier in terms of cited references. Using different RPYS approaches in this
study, we were able to identify the complete range of works of the celebrated
icons as well as many less known works relevant for the history of climate
change research. The analyses confirmed the potential of the RPYS method for
historical studies: Seminal papers are detected on the basis of the references
cited by the overall community without any further assumptions.
"
1649,Scholarly use of social media and altmetrics: a review of the literature,"  Social media has become integrated into the fabric of the scholarly
communication system in fundamental ways: principally through scholarly use of
social media platforms and the promotion of new indicators on the basis of
interactions with these platforms. Research and scholarship in this area has
accelerated since the coining and subsequent advocacy for altmetrics -- that
is, research indicators based on social media activity. This review provides an
extensive account of the state-of-the art in both scholarly use of social media
and altmetrics. The review consists of two main parts: the first examines the
use of social media in academia, examining the various functions these
platforms have in the scholarly communication process and the factors that
affect this use. The second part reviews empirical studies of altmetrics,
discussing the various interpretations of altmetrics, data collection and
methodological limitations, and differences according to platform. The review
ends with a critical discussion of the implications of this transformation in
the scholarly communication system.
"
1650,"Bibliometrics and Information Retrieval: Creating Knowledge through
  Research Synergies","  This panel brings together experts in bibliometrics and information retrieval
to discuss how each of these two important areas of information science can
help to inform the research of the other. There is a growing body of literature
that capitalizes on the synergies created by combining methodological
approaches of each to solve research problems and practical issues related to
how information is created, stored, organized, retrieved and used. The session
will begin with an overview of the common threads that exist between IR and
metrics, followed by a summary of findings from the BIR workshops and examples
of research projects that combine aspects of each area to benefit IR or metrics
research areas, including search results ranking, semantic indexing and
visualization. The panel will conclude with an engaging discussion with the
audience to identify future areas of research and collaboration.
"
1651,"Identification of milestone papers through time-balanced network
  centrality","  Citations between scientific papers and related bibliometric indices, such as
the $h$-index for authors and the impact factor for journals, are being
increasingly used - often in controversial ways - as quantitative tools for
research evaluation. Yet, a fundamental research question remains still open:
to which extent do quantitative metrics capture the significance of scientific
works? We analyze the network of citations among the $449,935$ papers published
by the American Physical Society (APS) journals between 1893 and 2009, and
focus on the comparison of metrics built on the citation count with
network-based metrics. We contrast five article-level metrics with respect to
the rankings that they assign to a set of fundamental papers, called Milestone
Letters, carefully selected by the APS editors for ""making long-lived
contributions to physics, either by announcing significant discoveries, or by
initiating new areas of research"". A new metric, which combines PageRank
centrality with the explicit requirement that paper score is not biased by
paper age, is the best-performing metric overall in identifying the Milestone
Letters. The lack of time bias in the new metric makes it also possible to use
it to compare papers of different age on the same scale. We find that
network-based metrics identify the Milestone Letters better than metrics based
on the citation count, which suggests that the structure of the citation
network contains information that can be used to improve the ranking of
scientific publications. The methods and results presented here are relevant
for all evolving systems where network centrality metrics are applied, for
example the World Wide Web and online social networks. An interactive Web
platform where it is possible to view the ranking of the APS papers by rescaled
PageRank is available at the address \url{http://www.sciencenow.info}.
"
1652,"All Fingers are not Equal: Intensity of References in Scientific
  Articles","  Research accomplishment is usually measured by considering all citations with
equal importance, thus ignoring the wide variety of purposes an article is
being cited for. Here, we posit that measuring the intensity of a reference is
crucial not only to perceive better understanding of research endeavor, but
also to improve the quality of citation-based applications. To this end, we
collect a rich annotated dataset with references labeled by the intensity, and
propose a novel graph-based semi-supervised model, GraLap to label the
intensity of references. Experiments with AAN datasets show a significant
improvement compared to the baselines to achieve the true labels of the
references (46% better correlation). Finally, we provide four applications to
demonstrate how the knowledge of reference intensity leads to design better
real-world applications.
"
1653,Citation Classification for Behavioral Analysis of a Scientific Field,"  Citations are an important indicator of the state of a scientific field,
reflecting how authors frame their work, and influencing uptake by future
scholars. However, our understanding of citation behavior has been limited to
small-scale manual citation analysis. We perform the largest behavioral study
of citations to date, analyzing how citations are both framed and taken up by
scholars in one entire field: natural language processing. We introduce a new
dataset of nearly 2,000 citations annotated for function and centrality, and
use it to develop a state-of-the-art classifier and label the entire ACL
Reference Corpus. We then study how citations are framed by authors and use
both papers and online traces to track how citations are followed by readers.
We demonstrate that authors are sensitive to discourse structure and
publication venue when citing, that online readers follow temporal links to
previous and future work rather than methodological links, and that how a paper
cites related work is predictive of its citation count. Finally, we use changes
in citation roles to show that the field of NLP is undergoing a significant
increase in consensus.
"
1654,"Do Mathematicians, Economists and Biomedical Scientists Trace Large
  Topics More Strongly Than Physicists?","  In this work, we extend our previous work on largeness tracing among
physicists to other fields, namely mathematics, economics and biomedical
science. Overall, the results confirm our previous discovery, indicating that
scientists in all these fields trace large topics. Surprisingly, however, it
seems that researchers in mathematics tend to be more likely to trace large
topics than those in the other fields. We also find that on average, papers in
top journals are less largeness-driven. We compare researchers from the USA,
Germany, Japan and China and find that Chinese researchers exhibit consistently
larger exponents, indicating that in all these fields, Chinese researchers
trace large topics more strongly than others. Further correlation analyses
between the degree of largeness tracing and the numbers of authors,
affiliations and references per paper reveal positive correlations -- papers
with more authors, affiliations or references are likely to be more
largeness-driven, with several interesting and noteworthy exceptions: in
economics, papers with more references are not necessary more largeness-driven,
and the same is true for papers with more authors in biomedical science. We
believe that these empirical discoveries may be valuable to science
policy-makers.
"
1655,"Public scientists contributing to local literary fiction. An exploratory
  analysis","  Public scientists (scientists only from now onwards), understood as a member
of the teaching and/or research staff of a public university or a public
research organization (including humanities and social sciences), benefit the
academic community, industry and other social collectives through teaching and
research. Active involvement of scientists in culture is part of the richness
of developed societies. Some voices in current debates on the evaluation of
societal impact and the role of universities towards social development are
claiming a refocus from a socioeconomic perspective to also including
sociocultural benefits from academiaIn this paper we will focus in one facet of
cultural engagement; writing literary fiction. We will narrow our general
objective to local activities, due to the interest in the engagement of
scientist on this geographic dimension. Do local publishers include the
literary work of scientists? Are works written by scientists more likely to be
local than works not written by scientists?
"
1656,"In the Name of the Name: RDF literals, ER Attributes and the Potential
  to Rethink the Structures and Visualizations of Catalogs","  The aim of this study is to contribute to the field of machine-processable
bibliographic data that is suitable for the Semantic Web. We examine the Entity
Relationship (ER) model, which has been selected by IFLA as a ""conceptual
framework"" in order to model the FR family (FRBR, FRAD, and RDA), and the
problems ER causes as we move towards the Semantic Web. Subsequently, while
maintaining the semantics of the aforementioned standards but rejecting the ER
as a conceptual framework for bibliographic data, this paper builds on the RDF
(Resource Description Framework) potential and documents how both the RDF and
Linked Data's rationale can affect the way we model bibliographic data. In this
way, a new approach to bibliographic data emerges where the distinction between
description and authorities is obsolete. Instead, the integration of the
authorities with descriptive information becomes fundamental so that a network
of correlations can be established between the entities and the names by which
the entities are known. Naming is a vital issue for human cultures because
names are not random sequences of characters or sounds that stand just as
identifiers for the entities; they also have socio-cultural meanings and
interpretations. Thus, instead of describing indivisible resources, we could
describe entities that appear in a variety of names on various resources. In
this study, a method is proposed to connect the names with the entities they
represent and, in this way, to document the provenance of these names by
connecting specific resources with specific names.
"
1657,"Predicting the future relevance of research institutions - The winning
  solution of the KDD Cup 2016","  The world's collective knowledge is evolving through research and new
scientific discoveries. It is becoming increasingly difficult to objectively
rank the impact research institutes have on global advancements. However, since
the funding, governmental support, staff and students quality all mirror the
projected quality of the institution, it becomes essential to measure the
affiliation's rating in a transparent and widely accepted way. We propose and
investigate several methods to rank affiliations based on the number of their
accepted papers at future academic conferences. We carry out our investigation
using publicly available datasets such as the Microsoft Academic Graph, a
heterogeneous graph which contains various information about academic papers.
We analyze several models, starting with a simple probabilities-based method
and then gradually expand our training dataset, engineer many more features and
use mixed models and gradient boosted decision trees models to improve our
predictions.
"
1658,"Libraries' Metadata as Data in the Era of the Semantic Web: Modeling a
  Repository of Master Theses and PhD Dissertations for the Web of Data","  This study argues that metadata of library catalogs can stand autonomously,
providing valuable information detached from the resources they point to and,
therefore, could be used as data in the context of the Semantic Web. We present
an analysis of this perception followed by an implementation proposal for a
Master's thesis and PhD dissertation repository. The analysis builds on the
flexibility of the Resource Description Framework (RDF) and takes into account
the Functional Requirements for Bibliographic Records (FRBR) and Functional
Requirements for Authority Data (FRAD) in order to reveal the latent academic
network by linking its entities to a meaningful and computationally processable
set. Current library catalogs retrieve documents to find answers, whereas in
our approach catalogs can provide answers that could not be found in any
specific document.
"
1659,Math-Aware Search Engines: Physics Applications and Overview,"  Search engines for equations now exist, which return results matching the
query's mathematical meaning or structural presentation. Operating over
scientific papers, online encyclopedias, and math discussion forums, their
content includes physics, math, and other sciences. They enable physicists to
avoid jargon and more easily target mathematical content within and across
disciplines. As a natural extension of keyword-based search, they open up a new
world for discovering both exact and approximate mathematical solutions;
physical systems' analogues and alternative models; and physics' patterns.
  This review presents the existing math-aware search engines, discusses
methods for maximizing their search success, and overviews their math-matching
capabilities. Proposed applications to physics are also given, to contribute
towards developers' and physicists' exploration of the newly available search
horizons.
"
1660,"Quantitative identification of technological discontinuities using
  simulation modeling","  The aim of this paper is to develop and test metrics to quantitatively
identify technological discontinuities in a knowledge network. We developed
five metrics based on innovation theories and tested the metrics by a
simulation model-based knowledge network and hypothetically designed
discontinuity. The designed discontinuity is modeled as a node which combines
two different knowledge streams and whose knowledge is dominantly persistent in
the knowledge network. The performances of the proposed metrics were evaluated
by how well the metrics can distinguish the designed discontinuity from other
nodes on the knowledge network. The simulation results show that the
persistence times # of converging main paths provides the best performance in
identifying the designed discontinuity: the designed discontinuity was
identified as one of the top 3 patents with 96~99% probability by Metric 5 and
it is, according to the size of a domain, 12~34% better than the performance of
the second best metric. Beyond the simulation analysis, we tested the metrics
using a patent set representative of the Magnetic information storage domain.
The three representative patents associated with a well-known breakthrough
technology in the domain, the giant magneto-resistance (GMR) spin valve sensor,
were selected based on the qualitative studies, and the metrics were tested by
how well the metrics identify the selected patents as top-ranked patents. The
empirical results fully support the simulation results and therefore the
persistence times # of converging main paths is recommended for identifying
technological discontinuities for any technology.
"
1661,"Professional and Citizen Bibliometrics: Complementarities and
  ambivalences in the development and use of indicators","  Bibliometric indicators such as journal impact factors, h-indices, and total
citation counts are algorithmic artifacts that can be used in research
evaluation and management. These artifacts have no meaning by themselves, but
receive their meaning from attributions in institutional practices. We
distinguish four main stakeholders in these practices: (1) producers of
bibliometric data and indicators; (2) bibliometricians who develop and test
indicators; (3) research managers who apply the indicators; and (4) the
scientists being evaluated with potentially competing career interests. These
different positions may lead to different and sometimes conflicting
perspectives on the meaning and value of the indicators. The indicators can
thus be considered as boundary objects which are socially constructed in
translations among these perspectives. This paper proposes an analytical
clarification by listing an informed set of (sometimes unsolved) problems in
bibliometrics which can also shed light on the tension between simple but
invalid indicators that are widely used (e.g., the h-index) and more
sophisticated indicators that are not used or cannot be used in evaluation
practices because they are not transparent for users, cannot be calculated, or
are difficult to interpret.
"
1662,"Indicators as judgment devices: Citizen bibliometrics in biomedicine and
  economics","  The number of publications has been a fundamental merit in the competition
for academic positions since the late 18th century. Today, the simple counting
of publications has been supplemented with a whole range of bibliometric
measures, which supposedly not only measures the volume of research but also
its impact. In this study, we investigate how bibliometrics are used for
evaluating the impact and quality of publications in two specific settings:
biomedicine and economics. Our study exposes the extent and type of metrics
used in external evaluations of candidates for academic positions at Swedish
universities. Moreover, we show how different bibliometric indicators, both
explicitly and implicitly, are employed to value and rank candidates. Our
findings contribute to a further understanding of bibliometric indicators as
judgment devices that are employed in evaluating individuals and their
published works within specific fields. We also show how expertise in using
bibliometrics for evaluative purposes is negotiated at the interface between
domain knowledge and skills in using indicators. In line with these results we
propose that the use of metrics in this context is best described as a form of
citizen bibliometrics - an underspecified term which we build upon in the
paper.
"
1663,A simple centrality index for scientific social recognition,"  We introduce a new centrality index for bipartite network of papers and
authors that we call $K$-index. The $K$-index grows with the citation
performance of the papers that cite a given researcher and can seen as a
measure of scientific social recognition. Indeed, the $K$-index measures the
number of hubs, defined in a self-consistent way in the bipartite network, that
cites a given author. We show that the $K$-index can be computed by simple
inspection of the Web of Science platform and presents several advantages over
other centrality indexes, in particular Hirsch $h$-index. The $K$-index is
robust to self-citations, is not limited by the total number of papers
published by a researcher as occurs for the $h$-index and can distinguish in a
consistent way researchers that have the same $h$-index but very different
scientific social recognition. The $K$-index easily detects a known case of a
researcher with inflated number of papers, citations and $h$-index due to
scientific misconduct. Finally, we show that, in a sample of twenty-eight
physics Nobel laureates and twenty-eight highly cited non-Nobel-laureate
physicists, the $K$-index correlates better to the achievement of the prize
than the number of papers, citations, citations per paper, citing articles or
the $h$-index. Clustering researchers in a $K$ versus $h$ plot reveals
interesting outliers that suggest that these two indexes can present
complementary independent information.
"
1664,Citation Analysis with Microsoft Academic,"  We explore if and how Microsoft Academic (MA) could be used for bibliometric
analyses. First, we examine the Academic Knowledge API (AK API), an interface
to access MA data, and compare it to Google Scholar (GS). Second, we perform a
comparative citation analysis of researchers by normalizing data from MA and
Scopus. We find that MA offers structured and rich metadata, which facilitates
data retrieval, handling and processing. In addition, the AK API allows
retrieving frequency distributions of citations. We consider these features to
be a major advantage of MA over GS. However, we identify four main limitations
regarding the available metadata. First, MA does not provide the document type
of a publication. Second, the 'fields of study' are dynamic, too specific and
field hierarchies are incoherent. Third, some publications are assigned to
incorrect years. Fourth, the metadata of some publications did not include all
authors. Nevertheless, we show that an average-based indicator (i.e. the
journal normalized citation score; JNCS) as well as a distribution-based
indicator (i.e. percentile rank classes; PR classes) can be calculated with
relative ease using MA. Hence, normalization of citation counts is feasible
with MA. The citation analyses in MA and Scopus yield uniform results. The JNCS
and the PR classes are similar in both databases, and, as a consequence, the
evaluation of the researchers' publication impact is congruent in MA and
Scopus. Given the fast development in the last year, we postulate that MA has
the potential to be used for full-fledged bibliometric analyses.
"
1665,Patterns of authors contribution in scientific manuscripts,"  Science is becoming increasingly more interdisciplinary, giving rise to more
diversity in the areas of expertise within research labs and groups. This also
have brought changes to the role researchers in scientific works. As a
consequence, multi-authored scientific papers have now became a norm for high
quality research. Unfortunately, such a phenomenon induces bias to existing
metrics employed to evaluate the productivity and success of researchers. While
some metrics were adapted to account for the rank of authors in a paper, many
journals are now requiring a description of the specific roles of each author
in a publication. Surprisingly, the investigation of the relationship between
the rank of authors and their contributions has been limited to a few studies.
By analyzing such kind of data, here we show, quantitatively, that the
regularity in the authorship contributions decreases with the number of authors
in a paper. Furthermore, we found that the rank of authors and their roles in
papers follows three general patterns according to the nature of their
contributions, such as writing, data analysis, and the conduction of
experiments. This was accomplished by collecting and analyzing the data
retrieved from PLoS ONE and by devising an entropy-based measurement to
quantify the effective number of authors in a paper according to their
contributions. The analysis of such patterns confirms that some aspects of the
author ranking are in accordance with the expected convention, such as the fact
that the first and last authors are more likely to contribute more in a
scientific work. Conversely, such analysis also revealed that authors in the
intermediary positions of the rank contribute more in certain specific roles,
such as the task of collecting data. This indicates that the an unbiased
evaluation of researchers must take into account the distinct types of
scientific contributions.
"
1666,"A globally-applicable disease ontology for biosurveillance; Anthology of
  Biosurveillance Diseases (ABD)","  Biosurveillance, a relatively young field, has recently increased in
importance because of its relevance to national security and global health.
Databases and tools describing particular subsets of disease are becoming
increasingly common in the field. However, a common method to describe those
diseases is lacking. Here, we present the Anthology of Biosurveillance Diseases
(ABD), an ontology of infectious diseases of biosurveillance relevance.
"
1667,"OCR++: A Robust Framework For Information Extraction from Scholarly
  Articles","  This paper proposes OCR++, an open-source framework designed for a variety of
information extraction tasks from scholarly articles including metadata (title,
author names, affiliation and e-mail), structure (section headings and body
text, table and figure headings, URLs and footnotes) and bibliography (citation
instances and references). We analyze a diverse set of scientific articles
written in English language to understand generic writing patterns and
formulate rules to develop this hybrid framework. Extensive evaluations show
that the proposed framework outperforms the existing state-of-the-art tools
with huge margin in structural information extraction along with improved
performance in metadata and bibliography extraction tasks, both in terms of
accuracy (around 50% improvement) and processing time (around 52% improvement).
A user experience study conducted with the help of 30 researchers reveals that
the researchers found this system to be very helpful. As an additional
objective, we discuss two novel use cases including automatically extracting
links to public datasets from the proceedings, which would further accelerate
the advancement in digital libraries. The result of the framework can be
exported as a whole into structured TEI-encoded documents. Our framework is
accessible online at http://cnergres.iitkgp.ac.in/OCR++/home/.
"
1668,"Towards a global scientific brain: Indicators of researcher mobility
  using co-affiliation data","  This paper analyses the potential use of bibliometric data for mapping and
applying network analysis to mobility flows. We show case mobility networks at
three different levels of aggregation: at the country level, at the city level
and at the institutional level. We reflect on the potential uses of
bibliometric data to inform research policies with regard to scientific
mobility.
"
1669,"Bibliographic Analysis on Research Publications using Authors,
  Categorical Labels and the Citation Network","  Bibliographic analysis considers the author's research areas, the citation
network and the paper content among other things. In this paper, we combine
these three in a topic model that produces a bibliographic model of authors,
topics and documents, using a nonparametric extension of a combination of the
Poisson mixed-topic link model and the author-topic model. This gives rise to
the Citation Network Topic Model (CNTM). We propose a novel and efficient
inference algorithm for the CNTM to explore subsets of research publications
from CiteSeerX. The publication datasets are organised into three corpora,
totalling to about 168k publications with about 62k authors. The queried
datasets are made available online. In three publicly available corpora in
addition to the queried datasets, our proposed model demonstrates an improved
performance in both model fitting and document clustering, compared to several
baselines. Moreover, our model allows extraction of additional useful knowledge
from the corpora, such as the visualisation of the author-topics network.
Additionally, we propose a simple method to incorporate supervision into topic
modelling to achieve further improvement on the clustering task.
"
1670,Bibliographic Analysis with the Citation Network Topic Model,"  Bibliographic analysis considers author's research areas, the citation
network and paper content among other things. In this paper, we combine these
three in a topic model that produces a bibliographic model of authors, topics
and documents using a non-parametric extension of a combination of the Poisson
mixed-topic link model and the author-topic model. We propose a novel and
efficient inference algorithm for the model to explore subsets of research
publications from CiteSeerX. Our model demonstrates improved performance in
both model fitting and a clustering task compared to several baselines.
"
1671,"Measuring the match between evaluators and evaluees: Cognitive distances
  between panel members and research groups at the journal level","  When research groups are evaluated by an expert panel, it is an open question
how one can determine the match between panel and research groups. In this
paper, we outline two quantitative approaches that determine the cognitive
distance between evaluators and evaluees, based on the journals they have
published in. We use example data from four research evaluations carried out
between 2009 and 2014 at the University of Antwerp.
  While the barycenter approach is based on a journal map, the
similarity-adapted publication vector (SAPV) approach is based on the full
journal similarity matrix. Both approaches determine an entity's profile based
on the journals in which it has published. Subsequently, we determine the
Euclidean distance between the barycenter or SAPV profiles of two entities as
an indicator of the cognitive distance between them. Using a bootstrapping
approach, we determine confidence intervals for these distances. As such, the
present article constitutes a refinement of a previous proposal that operates
on the level of Web of Science subject categories.
"
1672,"Corrigendum to ""Is the expertise of evaluation panels congruent with the
  research interests of the research groups: a quantitative approach based on
  barycenters"" [Journal of Informetrics 9(4) (2015) 704-721]","  In Rahman, Guns, Rousseau, and Engels (2015) we described several approaches
to determine the cognitive distance between two units. One of these approaches
was based on what we called barycenters in N dimensions. This note corrects
this terminology and introduces the more adequate term 'similarity-adapted
publication vectors'.
"
1673,"Towards Evidence-Based Ontology for Supporting Systematic Literature
  Review","  [Background]: Systematic Literature Review (SLR) has become an important
software engineering research method but costs tremendous efforts. [Aim]: This
paper proposes an approach to leverage on empirically evolved ontology to
support automating key SLR activities. [Method]: First, we propose an ontology,
SLRONT, built on SLR experiences and best practices as a groundwork to capture
common terminologies and their relationships during SLR processes; second, we
present an extended version of SLRONT, the COSONT and instantiate it with the
knowledge and concepts extracted from structured abstracts. Case studies
illustrate the details of applying it for supporting SLR steps. [Results]:
Results show that through using COSONT, we acquire the same conclusion compared
with sheer manual works, but the efforts involved is significantly reduced.
[Conclusions]: The approach of using ontology could effectively and efficiently
support the conducting of systematic literature review.
"
1674,Linking Behavior in the PER Coauthorship Network,"  There is considerable long-term interest in understanding the dynamics of
collaboration networks, and how these networks form and evolve over time. Most
of the work done on the dynamics of social networks focuses on well-established
communities. Work examining emerging social networks is rarer, simply because
data is difficult to obtain in real time. In this paper, we use thirty years of
data from an emerging scientific community to look at that crucial early stage
in the development of a social network. We show that when the field was very
young, islands of individual researchers labored in relative isolation, and the
coauthorship network was disconnected. Thirty years later, rather than a
cluster of individuals, we find a true collaborative community, bound together
by a robust collaboration network. However, this change did not take place
gradually\textemdash the network remained a loose assortment of isolated
individuals until the mid-2000s, when those smaller parts suddenly knit
themselves together into a single whole. In the rest of this paper, we consider
the role of three factors in these observed structural changes: growth, changes
in social norms, and the introduction of institutions such as field-specific
conferences and journals. We have data from the very earliest years of the
field, a period which includes the introduction of two different institutions:
the first field-specific conference, and the first field-specific journals. We
also identify two relevant behavioral shifts: a discrete increase in
coauthorship coincident with the first conference, and a shift among
established authors away from collaborating with outsiders, towards
collaborating with each other. The interaction of these factors gives us
insight into the formation of collaboration networks more broadly.
"
1675,"Who is Who in Phylogenetic Networks: Articles, Authors and Programs","  The phylogenetic network emerged in the 1990s as a new model to represent the
evolution of species in the case where coexisting species transfer genetic
information through hybridization, recombination, lateral gene transfer, etc.
As is true for many rapidly evolving fields, there is considerable
fragmentation and diversity in methodologies, standards and vocabulary in
phylogenetic network research, thus creating the need for an integrated
database of articles, authors, techniques, keywords and software. We describe
such a database, ""Who is Who in Phylogenetic Networks"", available at
http://phylnet.univ-mlv.fr. ""Who is Who in Phylogenetic Networks"" comprises
more than 600 publications and 500 authors interlinked with a rich set of more
than 200 keywords related to phylogenetic networks. The database is integrated
with web-based tools to visualize authorship and collaboration networks and
analyze these networks using common graph and social network metrics such as
centrality (betweenness, eigenvector, degree and closeness) and clustering. We
provide downloads of raw information about entries in the database, and a
facility to suggest modifications and contribute new information to the
database. We also present in this article common use cases of the database and
identify trends in the research on phylogenetic networks using the information
in the database and textual analysis.
"
1676,"A categorization of arguments for counting methods for publication and
  citation indicators","  Most publication and citation indicators are based on datasets with
multi-authored publications and thus a change in counting method will often
change the value of an indicator. Therefore it is important to know why a
specific counting method has been applied. I have identified arguments for
counting methods in a sample of 32 bibliometric studies published in 2016 and
compared the result with discussions of arguments for counting methods in three
older studies. Based on the underlying logics of the arguments I have arranged
the arguments in four groups. Group 1 focuses on arguments related to what an
indicator measures, Group 2 on the additivity of a counting method, Group 3 on
pragmatic reasons for the choice of counting method, and Group 4 on an
indicator's influence on the research community or how it is perceived by
researchers. This categorization can be used to describe and discuss how
bibliometric studies with publication and citation indicators argue for
counting methods.
"
1677,"Ranking academic institutions on potential paper acceptance in upcoming
  conferences","  The crux of the problem in KDD Cup 2016 involves developing data mining
techniques to rank research institutions based on publications. Rank importance
of research institutions are derived from predictions on the number of full
research papers that would potentially get accepted in upcoming top-tier
conferences, utilizing public information on the web. This paper describes our
solution to KDD Cup 2016. We used a two step approach in which we first
identify full research papers corresponding to each conference of interest and
then train two variants of exponential smoothing models to make predictions.
Our solution achieves an overall score of 0.7508, while the winning submission
scored 0.7656 in the overall results.
"
1678,Bibliometric Index for Academic Leadership,"  Academic leadership is essential for research innovation and impact. Until
now, there has been no dedicated measure of leadership by bibliometrics.
Popular bibliometric indices are mainly based on academic output, such as the
journal impact factor and the number of citations. Here we develop an academic
leadership index based on readily available bibliometric data that is sensitive
to not only academic output but also research efficiency. Our leadership index
was tested in two studies on peer-reviewed journal papers by
extramurally-funded principal investigators in the field of life sciences from
China and the USA, respectively. The leadership performance of these principal
investigators was quantified and compared relative to university rank and other
factors. As a validation measure, we show that the highest average leadership
index was achieved by principal investigators at top national universities in
both countries. More interestingly, our results also indicate that on an
individual basis, strong leadership and high efficiency are not necessarily
associated with those at top-tier universities nor with the most funding. This
leadership index may become the basis of a comprehensive merit system,
facilitating academic evaluation and resource management.
"
1679,"Generating Clustered Journal Maps: An Automated System for Hierarchical
  Classification","  Journal maps and classifications for 11,359 journals listed in the combined
Journal Citation Reports 2015 of the Science and Social Sciences Citation
Indexes are provided at http://www.leydesdorff.net/jcr15. A routine using
VOSviewer for integrating the journal mapping and their hierarchical clustering
is also made available. In this short communication, we provide background on
the journal mapping/clustering and an explanation and instructions about the
routine. We compare 2015 journal maps with those for 2014 and show the
delineations among fields and subfields to be sensitive to fluctuations. Labels
for fields and sub-fields are not provided by the routine, but can be added by
an analyst for pragmatic or intellectual reasons. The routine provides a means
for testing one's assumptions against a baseline without claiming authority,
clusters of related journals can be visualized to understand communities. The
routine is generic and can be used for any 1-mode network.
"
1680,"Gender differences in scientific collaborations: Women are more
  egalitarian than men","  By analyzing a unique dataset of more than 270,000 scientists, we discovered
substantial gender differences in scientific collaborations. While men are more
likely to collaborate with other men, women are more egalitarian. This is
consistently observed over all fields and regardless of the number of
collaborators a scientist has. The only exception is observed in the field of
engineering, where this gender bias disappears with increasing number of
collaborators. We also found that the distribution of the number of
collaborators follows a truncated power law with a cut-off that is gender
dependent and related to the gender differences in the number of published
papers. Considering interdisciplinary research, our analysis shows that men and
women behave similarly across fields, except in the case of natural sciences,
where women with many collaborators are more likely to have collaborators from
other fields.
"
1681,"ERMrest: an entity-relationship data storage service for web-based,
  data-oriented collaboration","  Scientific discovery is increasingly dependent on a scientist's ability to
acquire, curate, integrate, analyze, and share large and diverse collections of
data. While the details vary from domain to domain, these data often consist of
diverse digital assets (e.g. image files, sequence data, or simulation outputs)
that are organized with complex relationships and context which may evolve over
the course of an investigation. In addition, discovery is often collaborative,
such that sharing of the data and its organizational context is highly
desirable. Common systems for managing file or asset metadata hide their
inherent relational structures, while traditional relational database systems
do not extend to the distributed collaborative environment often seen in
scientific investigations. To address these issues, we introduce ERMrest, a
collaborative data management service which allows general entity-relationship
modeling of metadata manipulated by RESTful access methods. We present the
design criteria, architecture, and service implementation, as well as describe
an ecosystem of tools and services that we have created to integrate metadata
into an end-to-end scientific data life cycle. ERMrest has been deployed to
hundreds of users across multiple scientific research communities and projects.
We present two representative use cases: an international consortium and an
early-phase, multidisciplinary research project.
"
1682,Ten Blue Links on Mars,"  This paper explores a simple question: How would we provide a high-quality
search experience on Mars, where the fundamental physical limit is
speed-of-light propagation delays on the order of tens of minutes? On Earth,
users are accustomed to nearly instantaneous response times from search
engines. Is it possible to overcome orders-of-magnitude longer latency to
provide a tolerable user experience on Mars? In this paper, we formulate the
searching from Mars problem as a tradeoff between ""effort"" (waiting for
responses from Earth) and ""data transfer"" (pre-fetching or caching data on
Mars). The contribution of our work is articulating this design space and
presenting two case studies that explore the effectiveness of baseline
techniques, using publicly available data from the TREC Total Recall and
Sessions Tracks. We intend for this research problem to be aspirational and
inspirational - even if one is not convinced by the premise of Mars
colonization, there are Earth-based scenarios such as searching from a rural
village in India that share similar constraints, thus making the problem worthy
of exploration and attention from researchers.
"
1683,"$C^3$-index: A PageRank based multi-faceted metric for authors'
  performance measurement","  Ranking scientific authors is an important but challenging task, mostly due
to the dynamic nature of the evolving scientific publications. The basic
indicators of an author's productivity and impact are still the number of
publications and the citation count (leading to the popular metrics such as
h-index, g-index etc.). H-index and its popular variants are mostly effective
in ranking highly-cited authors, thus fail to resolve ties while ranking
medium-cited and low-cited authors who are majority in number. Therefore, these
metrics are inefficient to predict the ability of promising young researchers
at the beginning of their career. In this paper, we propose $C^3$-index that
combines the effect of citations and collaborations of an author in a
systematic way using a weighted multi-layered network to rank authors. We
conduct our experiments on a massive publication dataset of Computer Science
and show that - (i) $C^3$-index is consistent over time, which is one of the
fundamental characteristics of a ranking metric, (ii) $C^3$-index is as
efficient as h-index and its variants to rank highly-cited authors, (iii)
$C^3$-index can act as a conflict resolution metric to break ties in the
ranking of medium-cited and low-cited authors, (iv) $C^3$-index can also be
used to predict future achievers at the early stage of their career.
"
1684,From Ontology to Structured Applied Epistemology,"  Developing and organizing new knowledge is a core activity for scholars.
Recently, ontologies have been introduced as an approach for organizing
knowledge. However, most ontologies do not readily support the development and
organization of new knowledge. By comparison, to ontology, epistemology is the
study of what can be known. Aspects of epistemology include the acquisition of
and justification for new knowledge. Thus, we need to coordinate ontology with
epistemology. Because we are developing frameworks for capturing knowledge
across several scholarly domains, we describe the work in this paper as
exploring structured applied epistemology. Unlike other recent proposals for
new approaches to scholarly publishing, we propose an integrated and
comprehensive approach. We have explored direct representation based on the
rigorous Basic Formal Ontology and in this paper, we consider how epistemology
can be incorporated with that. In addition to highly-structured scientific
research reports, we also consider how to develop highly-structured
descriptions of historical events on which historical analyses can be based.
"
1685,"The poor altmetric performance of publications authored by researchers
  in mainland China","  China's scientific output has risen precipitously over the past decade; it is
now the world's second-largest producer of scientific papers, behind only the
United States. The quality of China's research is also on the rise (Van
Noorden, 2016). The online visibility and impact of China's research are also
important issues worth exploring. In this study, we investigate the altmetric
performance of publications in the field of Biotechnology and Applied
Microbiology and published by authors from Chinese affiliations. We find that
papers published by those authors from Chinese affiliations have much lower
visibility on the social web than articles from other countries, when there is
no significant difference for the citations. Fewer of China's publications get
tweeted, and those tweeted publications attract less social attention. A
geographical analysis of tweeters shows that scholarly articles get most of
their social attention from the authors' home countries, a finding that is also
confirmed by correlation and regression analysis. This situation, which is
unfavorable for researchers from Chinese affiliations, is caused, in part, by
the inaccessibility of mainstream social networking platforms in mainland
China.
"
1686,"4D Specialty Approximation: Ability to Distinguish between Related
  Specialties","  Publication and citation patterns can vary significantly between related
disciplines or more narrow specialties, even when sharing journals.
Journal-based structures are therefore not accurate enough to approximate
certain specialties, neither subject categories in global citation indices, nor
cell sub-structures (Rons, 2012). This paper presents first test results of a
new methodology that approximates the specialty of a highly specialized seed
record by combining criteria for four publication metadata-fields, thereby
broadly covering conceptual components defining disciplines and scholarly
communication. To offer added value compared to journal-based structures, the
methodology needs to generate sufficiently distinct results for seed
directories in related specialties (sharing subject categories, cells, or even
sources) with significantly different characteristics. This is tested
successfully for the sub-domains of theoretical and experimental particle
physics. In particular analyses of specialties with characteristics deviating
from those of a broader discipline embedded in can benefit from an approach
discerning down to specialty level. Such specialties are potentially present in
all disciplines, for instance as cases of peripheral, emerging, frontier, or
strategically prioritized research areas.
"
1687,"Quantitative Evaluation of Gender Bias in Astronomical Publications from
  Citation Counts","  We analyze the role of first (leading) author gender on the number of
citations that a paper receives, on the publishing frequency and on the
self-citing tendency. We consider a complete sample of over 200,000
publications from 1950 to 2015 from five major astronomy journals. We determine
the gender of the first author for over 70% of all publications. The fraction
of papers which have a female first author has increased from less than 5% in
the 1960s to about 25% today. We find that the increase of the fraction of
papers authored by females is slowest in the most prestigious journals such as
Science and Nature. Furthermore, female authors write 19$\pm$7% fewer papers in
seven years following their first paper than their male colleagues. At all
times papers with male first authors receive more citations than papers with
female first authors. This difference has been decreasing with time and amounts
to $\sim$6% measured over the last 30 years. To account for the fact that the
properties of female and male first author papers differ intrinsically, we use
a random forest algorithm to control for the non-gender specific properties of
these papers which include seniority of the first author, number of references,
total number of authors, year of publication, publication journal, field of
study and region of the first author's institution. We show that papers
authored by females receive 10.4$\pm$0.9% fewer citations than what would be
expected if the papers with the same non-gender specific properties were
written by the male authors. Finally, we also find that female authors in our
sample tend to self-cite more, but that this effect disappears when controlled
for non-gender specific variables.
"
1688,"On the relationship between set-based and network-based measures of
  gender homophily in scholarly publications","  There is an increased interest in the scientific community in the problem of
measuring gender homophily in co-authorship on scholarly publications (Eisen,
2016). For a given set of publications and co-authorships, we assume that
author identities have not been disambiguated in that we do not know when one
person is an author on more than one paper. In this case, one way to think
about measuring gender homophily is to consider all observed co-authorship
pairs and obtain a set-based gender homophily coefficient (e.g., Bergstrom et
al., 2016). Another way is to consider papers as observed disjoint networks of
co-authors and use a network-based assortativity coefficient (e.g., Newman,
2003). In this note, we review both metrics and show that the gender homophily
set-based index is equivalent to the gender assortativity network-based
coefficient with properly weighted edges.
"
1689,"How Users Explore Ontologies on the Web: A Study of NCBO's BioPortal
  Usage Logs","  Ontologies in the biomedical domain are numerous, highly specialized and very
expensive to develop. Thus, a crucial prerequisite for ontology adoption and
reuse is effective support for exploring and finding existing ontologies.
Towards that goal, the National Center for Biomedical Ontology (NCBO) has
developed BioPortal---an online repository designed to support users in
exploring and finding more than 500 existing biomedical ontologies. In 2016,
BioPortal represents one of the largest portals for exploration of semantic
biomedical vocabularies and terminologies, which is used by many researchers
and practitioners. While usage of this portal is high, we know very little
about how exactly users search and explore ontologies and what kind of usage
patterns or user groups exist in the first place. Deeper insights into user
behavior on such portals can provide valuable information to devise strategies
for a better support of users in exploring and finding existing ontologies, and
thereby enable better ontology reuse. To that end, we study and group users
according to their browsing behavior on BioPortal using data mining techniques.
Additionally, we use the obtained groups to characterize and compare
exploration strategies across ontologies. In particular, we were able to
identify seven distinct browsing-behavior types, which all make use of
different functionality provided by BioPortal. For example, Search Explorers
make extensive use of the search functionality while Ontology Tree Explorers
mainly rely on the class hierarchy to explore ontologies. Further, we show that
specific characteristics of ontologies influence the way users explore and
interact with the website. Our results may guide the development of more
user-oriented systems for ontology exploration on the Web.
"
1690,Effects of Social Ties in Knowledge Diffusion: case study on PLOS ONE,"  In order to capture the effects of social ties in knowledge diffusion, this
paper examines the publication network that emerges from the collaboration of
researchers, using citation information as means to estimate knowledge flow.
For this purpose, we analyzed the papers published in the PLOS ONE journal
finding strong evidence to support that the closer two authors are in the
co-authorship network, the larger the probability that knowledge flow will
occur between them. Moreover, we also found that when it comes to knowledge
diffusion, strong co-authorship proximity is more determinant than geographic
proximity.
"
1691,"Crowdsourced science: sociotechnical epistemology in the e-research
  paradigm","  Recent years have seen a surge in online collaboration between experts and
amateurs on scientific research. In this article, we analyse the
epistemological implications of these crowdsourced projects, with a focus on
Zooniverse, the world's largest citizen science web portal. We use quantitative
methods to evaluate the platform's success in producing large volumes of
observation statements and high impact scientific discoveries relative to more
conventional means of data processing. Through empirical evidence, Bayesian
reasoning, and conceptual analysis, we show how information and communication
technologies enhance the reliability, scalability, and connectivity of
crowdsourced e-research, giving online citizen science projects powerful
epistemic advantages over more traditional modes of scientific investigation.
These results highlight the essential role played by technologically mediated
social interaction in contemporary knowledge production. We conclude by calling
for an explicitly sociotechnical turn in the philosophy of science that
combines insights from statistics and logic to analyse the latest developments
in scientific research.
"
1692,"A Bibliometric Study of Asia Pacific Software Engineering Conference
  from 2010 to 2015","  The Asia-Pacific Software Engineering Conference (APSEC) is a reputed and a
long-running conference which has successfully completed more than two decades
as of year 2015. We conduct a bibliometric and scientific publication mining
based study to how the conference has evolved over the recent past six years
(year 2010 to 2015). Our objective is to perform in-depth examination of the
state of APSEC so that the APSEC community can identify strengths, areas of
improvements and future directions for the conference. Our empirical analysis
is based on various perspectives such as: paper submission acceptance rate
trends, conference location, scholarly productivity and contributions from
various countries, analysis of keynotes, workshops, conference organizers and
sponsors, tutorials, identification of prolific authors, computation of
citation impact of papers and contributing authors, internal and external
collaboration, university and industry participation and collaboration,
measurement of gender imbalance, topical analysis, yearly author churn and
program committee characteristics.
"
1693,"Capturing the ""Whole Tale"" of Computational Research: Reproducibility in
  Computing Environments","  We present an overview of the recently funded ""Merging Science and
Cyberinfrastructure Pathways: The Whole Tale"" project (NSF award #1541450). Our
approach has two nested goals: 1) deliver an environment that enables
researchers to create a complete narrative of the research process including
exposure of the data-to-publication lifecycle, and 2) systematically and
persistently link research publications to their associated digital scholarly
objects such as the data, code, and workflows. To enable this, Whole Tale will
create an environment where researchers can collaborate on data, workspaces,
and workflows and then publish them for future adoption or modification.
Published data and applications will be consumed either directly by users using
the Whole Tale environment or can be integrated into existing or future domain
Science Gateways.
"
1694,"The Durability and Fragility of Knowledge Infrastructures: Lessons
  Learned from Astronomy","  Infrastructures are not inherently durable or fragile, yet all are fragile
over the long term. Durability requires care and maintenance of individual
components and the links between them. Astronomy is an ideal domain in which to
study knowledge infrastructures, due to its long history, transparency, and
accumulation of observational data over a period of centuries. Research
reported here draws upon a long-term study of scientific data practices to ask
questions about the durability and fragility of infrastructures for data in
astronomy. Methods include interviews, ethnography, and document analysis. As
astronomy has become a digital science, the community has invested in shared
instruments, data standards, digital archives, metadata and discovery services,
and other relatively durable infrastructure components. Several features of
data practices in astronomy contribute to the fragility of that infrastructure.
These include different archiving practices between ground- and space-based
missions, between sky surveys and investigator-led projects, and between
observational and simulated data. Infrastructure components are tightly
coupled, based on international agreements. However, the durability of these
infrastructures relies on much invisible work - cataloging, metadata, and other
labor conducted by information professionals. Continual investments in care and
maintenance of the human and technical components of these infrastructures are
necessary for sustainability.
"
1695,Scientific Literature Text Mining and the Case for Open Access,"  ""Open access"" has become a central theme of journal reform in academic
publishing. In this article, I examine the relationship between open access
publishing and an important infrastructural element of a modern research
enterprise, scientific literature text mining, or the use of data analytic
techniques to conduct meta-analyses and investigations into the scientific
corpus. I give a brief history of the open access movement, discuss novel
journalistic practices, and an overview of data-driven investigation of the
scientific corpus. I argue that particularly in an era where the veracity of
many research studies has been called into question, scientific literature text
mining should be one of the key motivations for open access publishing, not
only in the basic sciences, but in the engineering and applied sciences as
well. The enormous benefits of unrestricted access to the research literature
should prompt scholars from all disciplines to lend their vocal support to
enabling legal, wholesale access to the scientific literature as part of a data
science pipeline.
"
1696,Sources of Change for Modern Knowledge Organization Systems,"  Knowledge Organization Systems (e.g. taxonomies and ontologies) continue to
contribute benefits in the design of information systems by providing a shared
conceptual underpinning for developers, users, and automated systems. However,
the standard mechanisms for the management of KOSs changes are inadequate for
systems built on top of thousands of data sources or with the involvement of
hundreds of individuals. In this work, we review standard sources of change for
KOSs (e.g. institutional shifts; standards cycles; cultural and political;
distribution, etc) and then proceed to catalog new sources of change for KOSs
ranging from massively cooperative development to always-on automated
extraction systems. Finally, we reflect on what this means for the design and
management of KOSs.
"
1697,"Could freely available articles reduce faculty reliance on library for
  access? An analysis of items cited by faculty from Singapore Management
  University","  Various studies have attempted to assess the amount of free full text
available on the web and recent work have suggested that we are close to the
50% mark for freely available articles (Archambault et al. 2013; Bjork et al.
2010; Jamali and Nabavi 2015). It is natural to wonder if this might reduce
researchers' reliance on library subscriptions for access. To do so, we need to
determine not just what papers researchers are citing to that are free today,
but to estimate if the papers they were citing were freely available at the
time they were citing it. We attempt to do so for a sample of citations made by
researchers in the Singapore Management University in the field of Economics.
"
1698,Learning to Rank Scientific Documents from the Crowd,"  Finding related published articles is an important task in any science, but
with the explosion of new work in the biomedical domain it has become
especially challenging. Most existing methodologies use text similarity metrics
to identify whether two articles are related or not. However biomedical
knowledge discovery is hypothesis-driven. The most related articles may not be
ones with the highest text similarities. In this study, we first develop an
innovative crowd-sourcing approach to build an expert-annotated
document-ranking corpus. Using this corpus as the gold standard, we then
evaluate the approaches of using text similarity to rank the relatedness of
articles. Finally, we develop and evaluate a new supervised model to
automatically rank related scientific articles. Our results show that authors'
ranking differ significantly from rankings by text-similarity-based models. By
training a learning-to-rank model on a subset of the annotated corpus, we found
the best supervised learning-to-rank model (SVM-Rank) significantly surpassed
state-of-the-art baseline systems.
"
1699,"Web of Science: showing a bug today that can mislead scientific research
  output's prediction","  As it happened in all domains of human activities, economic issues and the
increase of people working in scientific research have altered the way
scientific production is evaluated so as the objectives of performing the
evaluation. Introduced in 2005 by J. E. Hirsch as an indicator able to measure
individual scientific output not only in terms of quantity, but also in terms
of quality, h index has spread throughout the world. In 2007, Hirsch proposed
its adoption also as the best to predict future scientific achievement and,
consequently, a useful guide for investments in research and for institutions
when hiring members for their scientific staff. Since then, several authors
have also been using the Thomson ISI Web of Science database to develop their
proposals for evaluating research output. Here, using a software we have
developed, we analyse more than 100 thousand articles and show that a subtle
flaw in Web of Science can inflate the results of info collected, therefore
compromising the exactness and, consequently, the effectiveness of Hirsch's
proposal and its variations.
"
1700,"Citation algorithms for identifying research milestones driving
  biomedical innovation","  Scientific activity plays a major role in innovation for biomedicine and
healthcare. For instance, fundamental research on disease pathologies and
mechanisms can generate potential targets for drug therapy. This co-evolution
is punctuated by papers which provide new perspectives and open new domains.
Despite the relationship between scientific discovery and biomedical
advancement, identifying these research milestones that truly impact biomedical
innovation can be difficult and is largely based solely on the opinions of
subject matter experts. Here, we consider whether a new class of citation
algorithms that identify seminal scientific works in a field, Reference
Publication Year Spectroscopy (RPYS) and multi-RPYS, can identify the
connections between innovation (e.g. therapeutic treatments) and the
foundational research underlying them. Specifically, we assess whether the
results of these analytic techniques converge with expert opinions on research
milestones driving biomedical innovation in the treatment of Basal Cell
Carcinoma. Our results show that these algorithms successfully identify the
majority of milestone papers detailed by experts (Wong and Dlugosz 2014)
thereby validating the power of these algorithms to converge on independent
opinions of seminal scientific works derived by subject matter experts. These
advances offer an opportunity to identify scientific activities enabling
innovation in biomedicine.
"
1701,"A Semi-Automatic Approach for Detecting Dataset References in Social
  Science Texts","  Today, full-texts of scientific articles are often stored in different
locations than the used datasets. Dataset registries aim at a closer
integration by making datasets citable but authors typically refer to datasets
using inconsistent abbreviations and heterogeneous metadata (e.g. title,
publication year). It is thus hard to reproduce research results, to access
datasets for further analysis, and to determine the impact of a dataset.
Manually detecting references to datasets in scientific articles is
time-consuming and requires expert knowledge in the underlying research
domain.We propose and evaluate a semi-automatic three-step approach for finding
explicit references to datasets in social sciences articles.We first extract
pre-defined special features from dataset titles in the da|ra registry, then
detect references to datasets using the extracted features, and finally match
the references found with corresponding dataset titles. The approach does not
require a corpus of articles (avoiding the cold start problem) and performs
well on a test corpus. We achieved an F-measure of 0.84 for detecting
references in full-texts and an F-measure of 0.83 for finding correct matches
of detected references in the da|ra dataset registry.
"
1702,"Skewness of citation impact data and covariates of citation
  distributions: A large-scale empirical analysis based on Web of Science data","  Using percentile shares, one can visualize and analyze the skewness in
bibliometric data across disciplines and over time. The resulting figures can
be intuitively interpreted and are more suitable for detailed analysis of the
effects of independent and control variables on distributions than regression
analysis. We show this by using percentile shares to analyze so-called ""factors
influencing citation impact"" (FICs; e.g., the impact factor of the publishing
journal) across year and disciplines. All articles (n= 2,961,789) covered by
WoS in 1990 (n= 637,301), 2000 (n= 919,485), and 2010 (n= 1,405,003) are used.
In 2010, nearly half of the citation impact is accounted for by the 10%
most-frequently cited papers; the skewness is largest in the humanities (68.5%
in the top-10% layer) and lowest in agricultural sciences (40.6%). The
comparison of the effects of the different FICs (the number of cited
references, number of authors, number of pages, and JIF) on citation impact
shows that JIF has indeed the strongest correlations with the citation scores.
However, the correlation between FICs and citation impact is lower, if
citations are normalized instead of using raw citation counts.
"
1703,"Bots, Seeds and People: Web Archives as Infrastructure","  The field of web archiving provides a unique mix of human and automated
agents collaborating to achieve the preservation of the web. Centuries old
theories of archival appraisal are being transplanted into the sociotechnical
environment of the World Wide Web with varying degrees of success. The work of
the archivist and bots in contact with the material of the web present a
distinctive and understudied CSCW shaped problem. To investigate this space we
conducted semi-structured interviews with archivists and technologists who were
directly involved in the selection of content from the web for archives. These
semi-structured interviews identified thematic areas that inform the appraisal
process in web archives, some of which are encoded in heuristics and
algorithms. Making the infrastructure of web archives legible to the archivist,
the automated agents and the future researcher is presented as a challenge to
the CSCW and archival community.
"
1704,1.5 billion words Arabic Corpus,"  This study is an attempt to build a contemporary linguistic corpus for Arabic
language. The corpus produced, is a text corpus includes more than five million
newspaper articles. It contains over a billion and a half words in total, out
of which, there is about three million unique words. The data were collected
from newspaper articles in ten major news sources from eight Arabic countries,
over a period of fourteen years. The corpus was encoded with two types of
encoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two
mark-up languages, namely: SGML, and XML.
"
1705,"Day of the week effect in paper submission/acceptance/rejection to/in/by
  peer review journals. II. An ARCH econometric-like modeling","  This paper aims at providing a statistical model for the preferred behavior
of authors submitting a paper to a scientific journal. The electronic
submission of (about 600) papers to the Journal of the Serbian Chemical Society
has been recorded for every day from Jan. 01, 2013 till Dec. 31, 2014, together
with the acceptance or rejection paper fate. Seasonal effects and editor roles
(through desk rejection and subfield editors) are examined. An ARCH-like
econometric model is derived stressing the main determinants of the favorite
day-of-week process.
"
1706,The Opening Scholarly Communication in Social Sciences project OSCOSS,"  The OSCOSS project (Opening Scholarly Communication in Social Sciences),
which will be outlined, aims at providing integrated support for all steps of
the scholarly communication process. Incl. collaborative writing of a
scientific paper, collecting data related to existing publications,
interpreting and including data in a paper, submitting the paper for peer
review, reviewing the paper, publishing an article, and, finally, facilitating
its consumption by readers. The OSCOSS project will support this process
considering in particular the perspective of three main actors detailed in the
use case descriptions below: readers, authors and reviewers.
"
1707,"Fame and Obsolescence: Disentangling growth and ageing dynamics of
  patent citations","  We present an analysis of citations accrued over time by patents granted by
the United States Patent and Trademark Office in 1998. In contrast to previous
studies, a disaggregation by technology category is performed, and exogenously
caused citation-number growth is controlled for. Our approach reveals an
intrinsic citation rate that clearly separates into an -- in the long run,
exponentially time-dependent -- ageing function and a completely
time-independent preferential-attachment-type growth kernel. For the general
case of such a separable citation rate, we obtain the time-dependent citation
distribution analytically in a form that is valid for any functional form of
its ageing and growth parts. Good agreement between theory and long-time
characteristics of patent-citation data establishes our work as a useful
framework for addressing still open questions about knowledge-propagation
dynamics, such as the observed excess of citations at short times.
"
1708,The emergence and evolution of the research fronts in HIV/AIDS research,"  In this paper, we have identified and analyzed the emergence, structure and
dynamics of the paradigmatic research fronts that established the fundamentals
of the biomedical knowledge on HIV/AIDS. A search of papers with the
identifiers ""HIV/AIDS"", ""Human Immunodeficiency Virus"", ""HIV-1"" and ""Acquired
Immunodeficiency Syndrome"" in the Web of Science (Thomson Reuters), was carried
out. A citation network of those papers was constructed. Then, a sub-network of
the papers with the highest number of inter-citations (with a minimal in-degree
of 28) was selected to perform a combination of network clustering and text
mining to identify the paradigmatic research fronts and analyze their dynamics.
Thirteen research fronts were identified in this sub-network. The biggest and
oldest front is related to the clinical knowledge on the disease in the
patient. Nine of the fronts are related to the study of specific molecular
structures and mechanisms and two of these fronts are related to the
development of drugs. The rest of the fronts are related to the study of the
disease at the cellular level. Interestingly, the emergence of these fronts
occurred in successive ""waves"" over the time which suggest a transition in the
paradigmatic focus. The emergence and evolution of the biomedical fronts in
HIV/AIDS research is explained not just by the partition of the problem in
elements and interactions leading to increasingly specialized communities, but
also by changes in the technological context of this health problem and the
dramatic changes in the epidemiological reality of HIV/AIDS that occurred
between 1993 and 1995.
"
1709,Expected values in percentile indicators,"  PP(top x%) is the proportion of papers of a unit (e.g. an institution or a
group of researchers), which belongs to the x% most frequently cited papers in
the corresponding fields and publication years. It has been proposed that x% of
papers can be expected which belongs to the x% most frequently cited papers. In
this Letter to the Editor we will present the results of an empirical test
whether we can really have this expectation and how strong the deviations from
the expected values are when many random samples are drawn from the database.
"
1710,"Simple Yet Effective Methods for Large-Scale Scholarly Publication
  Ranking","  With the growing amount of published research, automatic evaluation of
scholarly publications is becoming an important task. In this paper we address
this problem and present a simple and transparent approach for evaluating the
importance of scholarly publications. Our method has been ranked among the top
performers in the WSDM Cup 2016 Challenge. The first part of this paper
describes our method. In the second part we present potential improvements to
the method and analyse the evaluation setup which was provided during the
challenge. Finally, we discuss future challenges in automatic evaluation of
papers including the use of full-texts based evaluation methods.
"
1711,Implementing Ideas for Improving Software Citation and Credit,"  Improving software citation and credit continues to be a topic of interest
across and within many disciplines, with numerous efforts underway. In this
Birds of a Feather (BoF) session, we started with a list of actionable ideas
from last year's BoF and other similar efforts and worked alone or in small
groups to begin implementing them. Work was captured in a common Google
document; the session organizers will disseminate or otherwise put this
information to use in or for the community in collaboration with those who
contributed.
"
1712,A critical comparative analysis of five world university rankings,"  To provide users insight into the value and limits of world university
rankings, a comparative analysis is conducted of 5 ranking systems: ARWU,
Leiden, THE, QS and U-Multirank. It links these systems with one another at the
level of individual institutions, and analyses the overlap in institutional
coverage, geographical coverage, how indicators are calculated from raw data,
the skewness of indicator distributions, and statistical correlations between
indicators. Four secondary analyses are presented investigating national
academic systems and selected pairs of indicators. It is argued that current
systems are still one-dimensional in the sense that they provide finalized,
seemingly unrelated indicator values rather than offering a data set and tools
to observe patterns in multi-faceted data. By systematically comparing
different systems, more insight is provided into how their institutional
coverage, rating methods, the selection of indicators and their normalizations
influence the ranking positions of given institutions.
"
1713,Full and Fractional Counting in Bibliometric Networks,"  In their study entitled ""Constructing bibliometric networks: A comparison
between full and fractional counting,"" Perianes-Rodriguez, Waltman, & van Eck
(2016; henceforth abbreviated as PWvE) provide arguments for the use of
fractional counting at the network level as different from the level of
publications. Whereas fractional counting in the latter case divides the credit
among co-authors (countries, institutions, etc.), fractional counting at the
network level can normalize the relative weights of links and thereby clarify
the structures in the network. PWvE, however, propose a counting scheme for
fractional counting that is one among other possible ones. Alternative schemes
proposed by Batagelj and Cerin\v{s}ek (2013) and Park, Yoon, & Leydesdorff
(2016; henceforth abbreviated as PYL) are discussed in an appendix. However,
our approach is not correctly identified as identical to their Equation A3.
Here below, we distinguish three approaches analytically; routines for applying
these approaches to bibliometric data are also provided.
"
1714,Leveraging Citation Networks to Visualize Scholarly Influence Over Time,"  Assessing the influence of a scholar's work is an important task for funding
organizations, academic departments, and researchers. Common methods, such as
measures of citation counts, can ignore much of the nuance and
multidimensionality of scholarly influence. We present an approach for
generating dynamic visualizations of scholars' careers. This approach uses an
animated node-link diagram showing the citation network accumulated around the
researcher over the course of the career in concert with key indicators,
highlighting influence both within and across fields. We developed our design
in collaboration with one funding organization---the Pew Biomedical Scholars
program---but the methods are generalizable to visualizations of scholarly
influence. We applied the design method to the Microsoft Academic Graph, which
includes more than 120 million publications. We validate our abstractions
throughout the process through collaboration with the Pew Biomedical Scholars
program officers and summative evaluations with their scholars.
"
1715,"How I Stopped Worrying about the Twitter Archive at the Library of
  Congress and Learned to Build a Little One for Myself","  Twitter is among the commonest sources of data employed in social media
research mainly because of its convenient APIs to collect tweets. However, most
researchers do not have access to the expensive Firehose and Twitter Historical
Archive, and they must rely on data collected with free APIs whose
representativeness has been questioned. In 2010 the Library of Congress
announced an agreement with Twitter to provide researchers access to the whole
Twitter Archive. However, such a task proved to be daunting and, at the moment
of this writing, no researcher has had the opportunity to access such
materials. Still, there have been experiences that proved that smaller
searchable archives are feasible and, therefore, amenable for academics to
build with relatively little resources. In this paper I describe my efforts to
build one of such archives, covering the first three years of Twitter (actually
from March 2006 to July 2009) and containing 1.48 billion tweets. If you
carefully follow my directions you may have your very own little Twitter
Historical Archive and you may forget about paying for historical tweets.
Please note that to achieve that you should be proficient in some programming
language, knowable about Twitter APIs, and have some basic knowledge on
ElasticSearch; moreover, you may very well get disappointed by the quality of
the contents of the final dataset.
"
1716,"The sum of it all: revealing collaboration patterns by combining
  authorship and acknowledgements","  Acknowledgments are one of many conventions by which researchers publicly
bestow recognition towards individuals, organizations and institutions that
contributed in some way to the work that led to publication. Combining data on
both co-authors and acknowledged individuals, the present study analyses
disciplinary differences in researchers credit attribution practices in
collaborative context. Our results show that the important differences
traditionally observed between disciplines in terms of team size are greatly
reduced when acknowledgees are taken into account. Broadening the measurement
of collaboration beyond co-authorship by including individuals credited in the
acknowledgements allows for an assessment of collaboration practices and team
work that might be closer to the reality of contemporary research, especially
in the social sciences and humanities.
"
1717,A conservation rule for constructing bibliometric network matrices,"  The social network analysis of bibliometric data needs matrices to be recast
in a network framework. In this paper we argue that a simple conservation rule
requires that this should be done only using fractional counting so that
conservation at the paper level will be faithfully reproduced at higher levels
ofaggregation (i.e. author, institute, country, journal etc.) of the complex
network.
"
1718,"""Smart Girls"" versus ""Sleeping Beauties"" in the Sciences: The
  Identification of Instant and Delayed Recognition by Using the Citation Angle","  In recent years, a number of studies have introduced methods for identifying
papers with delayed recognition (so called ""sleeping beauties"", SBs) or have
presented single publications as cases of SBs. Most recently, Ke et al. (2015)
proposed the so called ""beauty coefficient"" (denoted as B) to quantify how much
a given paper can be considered as a paper with delayed recognition. In this
study, the new term ""smart girl"" (SG) is suggested to differentiate instant
credit or ""flashes in the pan"" from SBs. While SG and SB are qualitatively
defined, the dynamic citation angle \b{eta} is introduced in this study as a
simple way for identifying SGs and SBs quantitatively - complementing the
beauty coefficient B. The citation angles for all articles from 1980 (n=166870)
in natural sciences are calculated for identifying SGs and SBs and their
extent. We reveal that about 3% of the articles are typical SGs and about 0.1%
typical SBs. The potential advantages of the citation angle approach are
explained.
"
1719,The Effect of Gender in the Publication Patterns in Mathematics,"  Despite the increasing number of women graduating in mathematics, a systemic
gender imbalance persists and is signified by a pronounced gender gap in the
distribution of active researchers and professors. Especially at the level of
university faculty, women mathematicians continue being drastically
underrepresented, decades after the first affirmative action measures have been
put into place. A solid publication record is of paramount importance for
securing permanent positions. Thus, the question arises whether the publication
patterns of men and women mathematicians differ in a significant way. Making
use of the zbMATH database, one of the most comprehensive metadata sources on
mathematical publications, we analyze the scholarly output of ~150,000
mathematicians from the past four decades whose gender we algorithmically
inferred. We focus on development over time, collaboration through
coautorships, presumed journal quality and distribution of research topics --
factors known to have a strong impact on job perspectives. We report
significant differences between genders which may put women at a disadvantage
when pursuing an academic career in mathematics.
"
1720,"Achieving ""space of physics journals"": topological structure and the
  Journal Impact Factor","  The empirical distribution function of citations to journal articles (EDF for
short) can become the fundamental tool for analyzing the scientific journals.
Endeavors at making bibliometric analysis independent of the intuition of
average citation levels have led us to the study of qualitative properties of
physics journals in the functional space of EDFs. We show that the structure of
this space establishes the connections and relationships that determine the
essential features of physics journals. The research provides an analysis of
240 physics journals indexed in Journal Citation Reports 2015. The relevance of
EDFs clustering is discussed. Our findings reveal four-cluster space of physics
journals. The space brings to light the essential distinctions between physics
journals and shows different level of influence of scientific publishers
belonging to different types (professional physics societies, transnational and
local publishers). The study of EDFs grouped by publishers reveals two binary
oppositions that structure relations between them: ""global\,--- local""
publishers and ""high cited\,--- low cited"" publishers.
"
1721,A biology journal that can teach physicists a lesson in peer review,"  This is a Commentary in $Physics~Today$ on the novel review process developed
by the biology journal $eLife$, with the suggestion that it be adopted by
physics journals.
"
1722,"Three practical field normalised alternative indicator formulae for
  research evaluation","  Although altmetrics and other web-based alternative indicators are now
commonplace in publishers' websites, they can be difficult for research
evaluators to use because of the time or expense of the data, the need to
benchmark in order to assess their values, the high proportion of zeros in some
alternative indicators, and the time taken to calculate multiple complex
indicators. These problems are addressed here by (a) a field normalisation
formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows
simple confidence limits to be calculated and is similar to a proposal of
Lundberg, (b) field normalisation formulae for the proportion of cited articles
in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the
Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data
sets, (c) a sampling strategy to minimise data collection costs, and (d) free
unified software to gather the raw data, implement the sampling strategy, and
calculate the indicator formulae and confidence limits. The approach is
demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley
readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in
three large fields for 2013-2016. Within the results, statistically significant
differences in both citation counts and Mendeley reader counts were found even
for sets of articles that were less than six months old. Mendeley reader counts
were more precise than Scopus citations for the most recent articles and all
three funders could be demonstrated to have an impact in Wikipedia that was
significantly above the world average.
"
1723,"The Evolution of Sentiment Analysis - A Review of Research Topics,
  Venues, and Top Cited Papers","  Sentiment analysis is one of the fastest growing research areas in computer
science, making it challenging to keep track of all the activities in the area.
We present a computer-assisted literature review, where we utilize both text
mining and qualitative coding, and analyze 6,996 papers from Scopus. We find
that the roots of sentiment analysis are in the studies on public opinion
analysis at the beginning of 20th century and in the text subjectivity analysis
performed by the computational linguistics community in 1990's. However, the
outbreak of computer-based sentiment analysis only occurred with the
availability of subjective texts on the Web. Consequently, 99% of the papers
have been published after 2004. Sentiment analysis papers are scattered to
multiple publication venues, and the combined number of papers in the top-15
venues only represent ca. 30% of the papers in total. We present the top-20
cited papers from Google Scholar and Scopus and a taxonomy of research topics.
In recent years, sentiment analysis has shifted from analyzing online product
reviews to social media texts from Twitter and Facebook. Many topics beyond
product reviews like stock markets, elections, disasters, medicine, software
engineering and cyberbullying extend the utilization of sentiment analysis
"
1724,"Measuring field-normalized impact of papers on specific societal groups:
  An altmetrics study based on Mendeley data","  Bibliometrics is successful in measuring impact, because the target is
clearly defined: the publishing scientist who is still active and working.
Thus, citations are a target-oriented metric which measures impact on science.
In contrast, societal impact measurements based on altmetrics are as a rule
intended to measure impact in a broad sense on all areas of society (e.g.
science, culture, politics, and economics). This tendency is especially
reflected in the efforts to design composite indicators (e.g. the Altmetric
attention score). We deem appropriate that not only the impact measurement
using citations is target-oriented (citations measure the impact of papers on
scientists), but also the measurement of impact using altmetrics. Impact
measurements only make sense, if the target group - the recipient of academic
papers - is clearly defined. Thus, we extend in this study the field-normalized
reader impact indicator proposed by us in an earlier study, which is based on
Mendeley data (the mean normalized reader score, MNRS), to a target-oriented
field-normalized impact indicator (e.g., MNRS_ED measures reader impact on the
sector of educational donation, i.e., teaching). This indicator can show - as
demonstrated in empirical examples - the ability of journals, countries, and
academic institutions to publish papers which are below or above the average
impact of papers on a specific sector in society (e.g., the educational or
teaching sector). For example, the method allows to measure the impact of
scientific papers on students - controlling for the field in which the papers
have been published and their publication year.
"
1725,"How many scientific papers are mentioned in policy-related documents? An
  empirical investigation using Web of Science and Altmetric data","  In this short communication, we provide an overview of a relatively newly
provided source of altmetrics data which could possibly be used for societal
impact measurements in scientometrics. Recently, Altmetric - a start-up
providing publication level metrics - started to make data for publications
available which have been mentioned in policy-related documents. Using data
from Altmetric, we study how many papers indexed in the Web of Science (WoS)
are mentioned in policy-related documents. We find that less than 0.5% of the
papers published in different subject categories are mentioned at least once in
policy-related documents. Based on our results, we recommend that the analysis
of (WoS) publications with at least one policy-related mention is repeated
regularly (annually). Mentions in policy-related documents should not be used
for impact measurement until new policy-related sites are tracked.
"
1726,"Conceptual difficulties in the use of statistical inference in citation
  analysis","  In this comment, I discuss the use of statistical inference in citation
analysis. In a recent paper, Williams and Bornmann argue in favor of the use of
statistical inference in citation analysis. I present a critical analysis of
their arguments and of similar arguments provided elsewhere in the literature.
My conclusion is that the use of statistical inference in citation analysis
involves major conceptual difficulties and, consequently, that the usefulness
of statistical inference in citation analysis is highly questionable.
"
1727,Quantifying perceived impact of scientific publications,"  Citations are commonly held to represent scientific impact. To date, however,
there is no empirical evidence in support of this postulate that is central to
research assessment exercises and Science of Science studies. Here, we report
on the first empirical verification of the degree to which citation numbers
represent scientific impact as it is actually perceived by experts in their
respective field. We run a large-scale survey of about 2000 corresponding
authors who performed a pairwise impact assessment task across more than 20000
scientific articles. Results of the survey show that citation data and
perceived impact do not align well, unless one properly accounts for strong
psychological biases that affect the opinions of experts with respect to their
own papers vs. those of others. First, researchers tend to largely prefer their
own publications to the most cited papers in their field of research. Second,
there is only a mild positive correlation between the number of citations of
top-cited papers in given research areas and expert preference in pairwise
comparisons. This also applies to pairs of papers with several orders of
magnitude differences in their total number of accumulated citations. However,
when researchers were asked to choose among pairs of their own papers, thus
eliminating the bias favouring one's own papers over those of others, they did
systematically prefer the most cited article. We conclude that, when scientists
have full information and are making unbiased choices, expert opinion on impact
is congruent with citation numbers.
"
1728,The Journal Impact Factor Should Not Be Discarded,"  The Journal Impact Factor (JIF) has been heavily criticized over decades.
This opinion piece argues that the JIF should not be demonized. It still can be
employed for research evaluation purposes by carefully considering the context
and academic environment.
"
1729,"Prerequisites for International Exchanges of Health Information:
  Comparison of Australian, Austrian, Finnish, Swiss, and US Privacy Policies","  Capabilities to exchange health information are critical to accelerate
discovery and its diffusion to healthcare practice. However, the same ethical
and legal policies that protect privacy hinder these data exchanges, and the
issues accumulate if moving data across geographical or organizational borders.
This can be seen as one of the reasons why many health technologies and
research findings are limited to very narrow domains. In this paper, we compare
how using and disclosing personal data for research purposes is addressed in
Australian, Austrian, Finnish, Swiss, and US policies with a focus on text data
analytics. Our goal is to identify approaches and issues that enable or hinder
international health information exchanges. As expected, the policies within
each country are not as diverse as across countries. Most policies apply the
principles of accountability and/or adequacy and are thereby fundamentally
similar. Their following requirements create complications with re-using and
re-disclosing data and even secondary data: 1) informing data subjects about
the purposes of data collection and use, before the dataset is collected; 2)
assurance that the subjects are no longer identifiable; and 3) destruction of
data when the research activities are finished. Using storage and compute cloud
services as well as other exchange technologies on the Internet without proper
permissions is technically not allowed if the data are stored in another
country. Both legislation and technologies are available as vehicles for
overcoming these barriers. The resulting richness in information variety will
contribute to the development and evaluation of new clinical hypotheses and
technologies.
"
1730,"Research performance of UNU - A bibliometric analysis of the United
  Nations University","  The scientific paper output of the United Nations University (UNU) was
bibliometrically analysed. It was found that (i) a noticeable continous paper
output starts in 1995, (ii) about 65% of the research papers have been
published as international cooperations and 18% as single-authored papers, (iv)
the research papers rank above world average according to Pudovkin-Garfield
Percentile Rank Index, and (v) paper content indicate the wide variety of
scientific topics UNU has been and is working on.
"
1731,The CENDARI Infrastructure,"  The CENDARI infrastructure is a research supporting platform designed to
provide tools for transnational historical research, focusing on two topics:
Medieval culture and World War I. It exposes to the end users modern web-based
tools relying on a sophisticated infrastructure to collect, enrich, annotate,
and search through large document corpora. Supporting researchers in their
daily work is a novel concern for infrastructures. We describe how we gathered
requirements through multiple methods to understand the historians' needs and
derive an abstract workflow to support them. We then outline the tools we have
built, tying their technical descriptions to the user requirements. The main
tools are the Note Taking Environment and its faceted search capabilities, the
Data Integration platform including the Data API, supporting semantic
enrichment through entity recognition, and the environment supporting the
software development processes throughout the project to keep both technical
partners and researchers in the loop. The outcomes are technical together with
new resources developed and gathered, and the research workflow that has been
described and documented.
"
1732,Analyzing Web Archives Through Topic and Event Focused Sub-collections,"  Web archives capture the history of the Web and are therefore an important
source to study how societal developments have been reflected on the Web.
However, the large size of Web archives and their temporal nature pose many
challenges to researchers interested in working with these collections. In this
work, we describe the challenges of working with Web archives and propose the
research methodology of extracting and studying sub-collections of the archive
focused on specific topics and events. We discuss the opportunities and
challenges of this approach and suggest a framework for creating
sub-collections.
"
1733,"Patent Portfolio Analysis of Cities: Statistics and Maps of
  Technological Inventiveness","  Cities are engines of the knowledge-based economy, because they are the
primary sites of knowledge production activities that subsequently shape the
rate and direction of technological change and economic growth. Patents provide
a wealth of information to analyse the knowledge specialization at specific
places, such as technological details and information on inventors and entities
involved, including address information. The technology codes on each patent
document indicate the specialization and scope of the underlying technological
knowledge of a given invention. In this paper we introduce tools for portfolio
analysis in terms of patents that provide insights into the technological
specialization of cities. The mapping and analysis of patent portfolios of
cities using data of the Unites States Patent and Trademark Office (USPTO)
website (at http://www.uspto.gov) and dedicated tools (at
http://www.leydesdorff.net/portfolio) can be used to analyse the specialisation
patterns of inventive activities among cities. The results allow policy makers
and other stakeholders to identify promising areas of further knowledge
development and 'smart specialisation' strategies.
"
1734,Recommendation of Scholarly Venues Based on Dynamic User Interests,"  The ever-growing number of venues publishing academic work makes it difficult
for researchers to identify venues that publish data and research most in line
with their scholarly interests. A solution is needed, therefore, whereby
researchers can identify information dissemination pathways in order to both
access and contribute to an existing body of knowledge. In this study, we
present a system to recommend scholarly venues rated in terms of relevance to a
given researcher's current scholarly pursuits and interests. We collected our
data from an academic social network and modeled researchers' scholarly reading
behavior in order to propose a new and adaptive implicit rating technique for
venues. We present a way to recommend relevant, specialized scholarly venues
using these implicit ratings that can provide quick results, even for new
researchers without a publication history and for emerging scholarly venues
that do not yet have an impact factor. We performed a large-scale experiment
with real data to evaluate the current scholarly recommendation system and
showed that our proposed system achieves better results than the baseline. The
results provide important up-to-the-minute signals that compared with
post-publication usage-based metrics represent a closer reflection of a
researcher's interests.
"
1735,An empirical and theoretical critique of the Euclidean index,"  The recently proposed Euclidean index offers a novel approach to measure the
citation impact of academic authors, in particular as an alternative to the
h-index. We test if the index provides new, robust information, not covered by
existing bibliometric indicators, discuss the measurement scale and the degree
of distinction between analytical units the index offers. We find that the
Euclidean index does not outperform existing indicators on these topics and
that the main application of the index would be solely for ranking, which is
not seen as a recommended practice.
"
1736,The iCrawl Wizard -- Supporting Interactive Focused Crawl Specification,"  Collections of Web documents about specific topics are needed for many areas
of current research. Focused crawling enables the creation of such collections
on demand. Current focused crawlers require the user to manually specify
starting points for the crawl (seed URLs). These are also used to describe the
expected topic of the collection. The choice of seed URLs influences the
quality of the resulting collection and requires a lot of expertise. In this
demonstration we present the iCrawl Wizard, a tool that assists users in
defining focused crawls efficiently and semi-automatically. Our tool uses major
search engines and Social Media APIs as well as information extraction
techniques to find seed URLs and a semantic description of the crawl intent.
Using the iCrawl Wizard even non-expert users can create semantic
specifications for focused crawlers interactively and efficiently.
"
1737,"iCrawl: Improving the Freshness of Web Collections by Integrating Social
  Web and Focused Web Crawling","  Researchers in the Digital Humanities and journalists need to monitor,
collect and analyze fresh online content regarding current events such as the
Ebola outbreak or the Ukraine crisis on demand. However, existing focused
crawling approaches only consider topical aspects while ignoring temporal
aspects and therefore cannot achieve thematically coherent and fresh Web
collections. Especially Social Media provide a rich source of fresh content,
which is not used by state-of-the-art focused crawlers. In this paper we
address the issues of enabling the collection of fresh and relevant Web and
Social Web content for a topic of interest through seamless integration of Web
and Social Media in a novel integrated focused crawler. The crawler collects
Web and Social Media content in a single system and exploits the stream of
fresh Social Media content for guiding the crawler.
"
1738,"Growth of International Cooperation in Science: Revisiting Six Case
  Studies","  International collaboration in science continues to grow at a remarkable
rate, but little agreement exists about dynamics of growth and organization at
the discipline level. Some suggest that disciplines differ in their
collaborative tendencies, reflecting their epistemic culture. This study
examines collaborative patterns in six previously studied specialties to add
new data and conduct analyses over time. Our findings show that the global
network of collaboration continues to add new nations and new participants;
each specialty has added many new nations to its lists of collaborating
partners since 1990. We also find that the scope of international collaboration
is positively related to impact. Network characteristics for the six
specialties are notable in that instead of reflecting underlying culture, they
tend towards convergence. This observation suggests that the global level may
represent next-order dynamics that feed back to the national and local levels
(as subsystems) in a complex, networked hierarchy.
"
1739,ScienceWISE: Topic Modeling over Scientific Literature Networks,"  We provide an up-to-date view on the knowledge management system ScienceWISE
(SW) and address issues related to the automatic assignment of articles to
research topics. So far, SW has been proven to be an effective platform for
managing large volumes of technical articles by means of ontological
concept-based browsing. However, as the publication of research articles
accelerates, the expressivity and the richness of the SW ontology turns into a
double-edged sword: a more fine-grained characterization of articles is
possible, but at the cost of introducing more spurious relations among them. In
this context, the challenge of continuously recommending relevant articles to
users lies in tackling a network partitioning problem, where nodes represent
articles and co-occurring concepts create edges between them. In this paper, we
discuss the three research directions we have taken for solving this issue: i)
the identification of generic concepts to reinforce inter-article similarities;
ii) the adoption of a bipartite network representation to improve scalability;
iii) the design of a clustering algorithm to identify concepts for
cross-disciplinary articles and obtain fine-grained topics for all articles.
"
1740,"Anatomy of Scholarly Information Behavior Patterns in the Wake of
  Academic Social Media Platforms","  As more scholarly content is born digital or converted to a digital format,
digital libraries are becoming increasingly vital to researchers seeking to
leverage scholarly big data for scientific discovery. Although scholarly
products are available in abundance-especially in environments created by the
advent of social networking services-little is known about international
scholarly information needs, information-seeking behavior, or information use.
The purpose of this paper is to address these gaps via an in-depth analysis of
the information needs and information-seeking behavior of researchers, both
students and faculty, at two universities, one in the U.S. and the other in
Qatar. Based on this analysis, the study identifies and describes new behavior
patterns on the part of researchers as they engage in the information-seeking
process. The analysis reveals that the use of academic social networks has
notable effects on various scholarly activities. Further, this study identifies
differences between students and faculty members in regard to their use of
academic social networks, and it identifies differences between researchers
according to discipline. Although the researchers who participated in the
present study represent a range of disciplinary and cultural backgrounds, the
study reports a number of similarities in terms of the researchers' scholarly
activities.
"
1741,"The misleading narrative of the canonical faculty productivity
  trajectory","  A scientist may publish tens or hundreds of papers over a career, but these
contributions are not evenly spaced in time. Sixty years of studies on career
productivity patterns in a variety of fields suggest an intuitive and universal
pattern: productivity tends to rise rapidly to an early peak and then gradually
declines. Here, we test the universality of this conventional narrative by
analyzing the structures of individual faculty productivity time series,
constructed from over 200,000 publications and matched with hiring data for
2453 tenure-track faculty in all 205 Ph.D-granting computer science departments
in the U.S. and Canada. Unlike prior studies, which considered only some
faculty or some institutions, or lacked common career reference points, here we
combine a large bibliographic dataset with comprehensive information on career
transitions that covers an entire field of study. We show that the conventional
narrative confidently describes only one fifth of faculty, regardless of
department prestige or researcher gender, and the remaining four fifths of
faculty exhibit a rich diversity of productivity patterns. To explain this
diversity, we introduce a simple model of productivity trajectories, and
explore correlations between its parameters and researcher covariates, showing
that departmental prestige predicts overall individual productivity and the
timing of the transition from first- to last-author publications. These results
demonstrate the unpredictability of productivity over time, and open the door
for new efforts to understand how environmental and individual factors shape
scientific productivity.
"
1742,Citation indices and dimensional homogeneity,"  The importance of dimensional analysis and dimensional homogeneity in
bibliometric studies is always overlooked. In this paper, we look at this issue
systematically and show that most h-type indices have the dimensions of [P],
where [P] is the basic dimensional unit in bibliometrics which is the unit
publication or paper. The newly introduced Euclidean index, based on the
Euclidean length of the citation vector has the dimensions [P3/2]. An empirical
example is used to illustrate the concepts.
"
1743,"Toward a Calculus of Redundancy: The feedback arrow of expectations in
  knowledge-based systems","  This paper considers the relationships among meaning generation, selection,
and the dynamics of discourse from a variety of perspectives ranging from
information theory and biology to sociology. Following Husserl's idea of a
horizon of meaning in intersubjective communication, we propose a way in which,
using Shannon's equations, the generation and selection of meanings from a
horizon of possibilities can be considered probabilistically. The
information-theoretical dynamics we articulate considers a process of meaning
generation within cultural evolution: information is imbued with meaning, and
through this process, the number of options for the selection of meaning in
discourse proliferates. The redundancy of possible meanings contributes to a
codification of expectations within the discourse. Unlike hard-wired DNA, the
codes of non-biological systems can co-evolve with the variations. Spanning
horizons of meaning, the codes structure the communications as selection
environments that shape discourses. Discursive knowledge can be considered as
meta-coded communication which enables us to translate among differently coded
communications. The dynamics of discursive knowledge production can thus infuse
the historical dynamics with a cultural evolution by adding options, that is,
by increasing redundancy. A calculus of redundancy is presented as an indicator
whereby these dynamics of discourse and meaning may be explored empirically.
"
1744,"Can scientists and their institutions become their own open access
  publishers?","  This article offers a personal perspective on the current state of academic
publishing, and posits that the scientific community is beset with journals
that contribute little valuable knowledge, overload the community's capacity
for high-quality peer review, and waste resources. Open access publishing can
offer solutions that benefit researchers and other information users, as well
as institutions and funders, but commercial journal publishers have influenced
open access policies and practices in ways that favor their economic interests
over those of other stakeholders in knowledge creation and sharing. One way to
free research from constraints on access is the diamond route of open access
publishing, in which institutions and funders that produce new knowledge
reclaim responsibility for publication via institutional journals or other open
platforms. I argue that research journals (especially those published for
profit) may no longer be fit for purpose, and hope that readers will consider
whether the time has come to put responsibility for publishing back into the
hands of researchers and their institutions. The potential advantages and
challenges involved in a shift away from for-profit journals in favor of
institutional open access publishing are explored.
"
1745,On Low Overlap Among Search Results of Academic Search Engines,"  Number of published scholarly articles is growing exponentially. To tackle
this information overload, researchers are increasingly depending on niche
academic search engines. Recent works have shown that two major general web
search engines: Google and Bing, have high level of agreement in their top
search results. In contrast, we show that various academic search engines have
low degree of agreement among themselves. We performed experiments using 2500
queries over four academic search engines. We observe that overlap in search
result sets of any pair of academic search engines is significantly low and in
most of the cases the search result sets are mutually exclusive. We also
discuss implications of this low overlap.
"
1746,"Quantifying the distribution of editorial power and manuscript decision
  bias at the mega-journal PLOS ONE","  We analyzed the longitudinal activity of nearly 7,000 editors at the
mega-journal PLOS ONE over the 10-year period 2006-2015. Using the
article-editor associations, we develop editor-specific measures of power,
activity, article acceptance time, citation impact, and editorial renumeration
(an analogue to self-citation). We observe remarkably high levels of power
inequality among the PLOS ONE editors, with the top-10 editors responsible for
3,366 articles -- corresponding to 2.4% of the 141,986 articles we analyzed.
Such high inequality levels suggest the presence of unintended incentives,
which may reinforce unethical behavior in the form of decision-level biases at
the editorial level. Our results indicate that editors may become apathetic in
judging the quality of articles and susceptible to modes of power-driven
misconduct. We used the longitudinal dimension of editor activity to develop
two panel regression models which test and verify the presence of editor-level
bias. In the first model we analyzed the citation impact of articles, and in
the second model we modeled the decision time between an article being
submitted and ultimately accepted by the editor. We focused on two variables
that represent social factors that capture potential conflicts-of-interest: (i)
we accounted for the social ties between editors and authors by developing a
measure of repeat authorship among an editor's article set, and (ii) we
accounted for the rate of citations directed towards the editor's own
publications in the reference list of each article he/she oversaw. Our results
indicate that these two factors play a significant role in the editorial
decision process. Moreover, these two effects appear to increase with editor
age, which is consistent with behavioral studies concerning the evolution of
misbehavior and response to temptation in power-driven environments.
"
1747,Science and Facebook: the same popularity law!,"  The distribution of scientific citations for publications selected with
different rules (author, topic, institution, country, journal, etc.) collapse
on a single curve if one plots the citations relative to their mean value. We
find that the distribution of shares for the Facebook posts re-scale in the
same manner to the very same curve with scientific citations. This finding
suggests that citations are subjected to the same growth mechanism with
Facebook popularity measures, being influenced by a statistically similar
social environment and selection mechanism. In a simple master-equation
approach the exponential growth of the number of publications and a
preferential selection mechanism leads to a Tsallis-Pareto distribution
offering an excellent description for the observed statistics. Based on our
model and on the data derived from PubMed we predict that according to the
present trend the average citations per scientific publications exponentially
relaxes to about 4.
"
1748,Profiling of OCR'ed Historical Texts Revisited,"  In the absence of ground truth it is not possible to automatically determine
the exact spectrum and occurrences of OCR errors in an OCR'ed text. Yet, for
interactive postcorrection of OCR'ed historical printings it is extremely
useful to have a statistical profile available that provides an estimate of
error classes with associated frequencies, and that points to conjectured
errors and suspicious tokens. The method introduced in Reffle (2013) computes
such a profile, combining lexica, pattern sets and advanced matching techniques
in a specialized Expectation Maximization (EM) procedure. Here we improve this
method in three respects: First, the method in Reffle (2013) is not adaptive:
user feedback obtained by actual postcorrection steps cannot be used to compute
refined profiles. We introduce a variant of the method that is open for
adaptivity, taking correction steps of the user into account. This leads to
higher precision with respect to recognition of erroneous OCR tokens. Second,
during postcorrection often new historical patterns are found. We show that
adding new historical patterns to the linguistic background resources leads to
a second kind of improvement, enabling even higher precision by telling
historical spellings apart from OCR errors. Third, the method in Reffle (2013)
does not make any active use of tokens that cannot be interpreted in the
underlying channel model. We show that adding these uninterpretable tokens to
the set of conjectured errors leads to a significant improvement of the recall
for error detection, at the same time improving precision.
"
1749,"Novel processes and metrics for a scientific evaluation rooted in the
  principles of science - Version 1","  Scientific evaluation is a determinant of how scientists, institutions and
funders behave, and as such is a key element in the making of science. In this
article, we propose an alternative to the current norm of evaluating research
with journal rank. Following a well-defined notion of scientific value, we
introduce qualitative processes that can also be quantified and give rise to
meaningful and easy-to-use article-level metrics. In our approach, the goal of
a scientist is transformed from convincing an editorial board through a
vertical process to convincing peers through an horizontal one. We argue that
such an evaluation system naturally provides the incentives and logic needed to
constantly promote quality, reproducibility, openness and collaboration in
science. The system is legally and technically feasible and can gradually lead
to the self-organized reappropriation of the scientific process by the
scholarly community and its institutions. We propose an implementation of our
evaluation system with the platform ""the Self-Journals of Science""
(www.sjscience.org).
"
1750,How to Search the Internet Archive Without Indexing It,"  Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.
"
1751,"ArchiveWeb: Collaboratively Extending and Exploring Web Archive
  Collections","  Curated web archive collections contain focused digital contents which are
collected by archiving organizations to provide a representative sample
covering specific topics and events to preserve them for future exploration and
analysis. In this paper, we discuss how to best support collaborative
construction and exploration of these collections through the ArchiveWeb
system. ArchiveWeb has been developed using an iterative evaluation-driven
design-based research approach, with considerable user feedback at all stages.
This paper describes the functionalities of our current prototype for
searching, constructing, exploring and discussing web archive collections, as
well as feedback on this prototype from seven archiving organizations, and our
plans for improving the next release of the system.
"
1752,"ArchiveWeb: collaboratively extending and exploring web archive
  collections - How would you like to work with your collections?","  Curated web archive collections contain focused digital content which is
collected by archiving organizations, groups, and individuals to provide a
representative sample covering specific topics and events to preserve them for
future exploration and analysis. In this paper, we discuss how to best support
collaborative construction and exploration of these collections through the
ArchiveWeb system. ArchiveWeb has been developed using an iterative
evaluation-driven design-based research approach, with considerable user
feedback at all stages. The first part of this paper describes the important
insights we gained from our initial requirements engineering phase during the
first year of the project and the main functionalities of the current
ArchiveWeb system for searching, constructing, exploring, and discussing web
archive collections. The second part summarizes the feedback we received on
this version from archiving organizations and libraries, as well as our
corresponding plans for improving and extending the system for the next
release.
"
1753,Single versus Double Blind Reviewing at WSDM 2017,"  In this paper we study the implications for conference program committees of
using single-blind reviewing, in which committee members are aware of the names
and affiliations of paper authors, versus double-blind reviewing, in which this
information is not visible to committee members. WSDM 2017, the 10th ACM
International ACM Conference on Web Search and Data Mining, performed a
controlled experiment in which each paper was reviewed by four committee
members. Two of these four reviewers were chosen from a pool of committee
members who had access to author information; the other two were chosen from a
disjoint pool who did not have access to this information. This information
asymmetry persisted through the process of bidding for papers, reviewing
papers, and entering scores. Reviewers in the single-blind condition typically
bid for 22% fewer papers, and preferentially bid for papers from top
institutions. Once papers were allocated to reviewers, single-blind reviewers
were significantly more likely than their double-blind counterparts to
recommend for acceptance papers from famous authors and top institutions. The
estimated odds multipliers are 1.63 for famous authors and 1.58 and 2.10 for
top universities and companies respectively, so the result is tangible. For
female authors, the associated odds multiplier of 0.78 is not statistically
significant in our study. However, a meta-analysis places this value in line
with that of other experiments, and in the context of this larger aggregate the
gender effect is also statistically significant.
"
1754,"Benford's law: a 'sleeping beauty' sleeping in the dirty pages of
  logarithmic tables","  Benford's law is an empirical observation, first reported by Simon Newcomb in
1881 and then independently by Frank Benford in 1938: the first significant
digits of numbers in large data are often distributed according to a
logarithmically decreasing function. Being contrary to intuition, the law was
forgotten as a mere curious observation. However, in the last two decades,
relevant literature has grown exponentially, - an evolution typical of
""Sleeping Beauties"" (SBs) publications that go unnoticed (sleep) for a long
time and then suddenly become center of attention (are awakened). Thus, in the
present study, we show that Newcomb (1881) and Benford (1938) papers are
clearly SBs. The former was in deep sleep for 110 years whereas the latter was
in deep sleep for a comparatively lesser period of 31 years up to 1968, and in
a state of less deep sleep for another 27 years up to 1995. Both SBs were
awakened in the year 1995 by Hill (1995a). In so doing, we show that the waking
prince (Hill, 1995a) is more often quoted than the SB whom he kissed, - in this
Benford's law case, wondering whether this is a general effect, - to be
usefully studied.
"
1755,Topic Modeling the H\`an di\u{a}n Ancient Classics,"  Ancient Chinese texts present an area of enormous challenge and opportunity
for humanities scholars interested in exploiting computational methods to
assist in the development of new insights and interpretations of culturally
significant materials. In this paper we describe a collaborative effort between
Indiana University and Xi'an Jiaotong University to support exploration and
interpretation of a digital corpus of over 18,000 ancient Chinese documents,
which we refer to as the ""Handian"" ancient classics corpus (H\`an di\u{a}n
g\u{u} j\'i, i.e, the ""Han canon"" or ""Chinese classics""). It contains classics
of ancient Chinese philosophy, documents of historical and biographical
significance, and literary works. We begin by describing the Digital Humanities
context of this joint project, and the advances in humanities computing that
made this project feasible. We describe the corpus and introduce our
application of probabilistic topic modeling to this corpus, with attention to
the particular challenges posed by modeling ancient Chinese documents. We give
a specific example of how the software we have developed can be used to aid
discovery and interpretation of themes in the corpus. We outline more advanced
forms of computer-aided interpretation that are also made possible by the
programming interface provided by our system, and the general implications of
these methods for understanding the nature of meaning in these texts.
"
1756,"ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation","  Web archives are a valuable resource for researchers of various disciplines.
However, to use them as a scholarly source, researchers require a tool that
provides efficient access to Web archive data for extraction and derivation of
smaller datasets. Besides efficient access we identify five other objectives
based on practical researcher needs such as ease of use, extensibility and
reusability.
  Towards these objectives we propose ArchiveSpark, a framework for efficient,
distributed Web archive processing that builds a research corpus by working on
existing and standardized data formats commonly held by Web archiving
institutions. Performance optimizations in ArchiveSpark, facilitated by the use
of a widely available metadata index, result in significant speed-ups of data
processing. Our benchmarks show that ArchiveSpark is faster than alternative
approaches without depending on any additional data stores while improving
usability by seamlessly integrating queries and derivations with external
tools.
"
1757,"Multi-level computational methods for interdisciplinary research in the
  HathiTrust Digital Library","  We show how faceted search using a combination of traditional classification
systems and mixed-membership topic models can go beyond keyword search to
inform resource discovery, hypothesis formulation, and argument extraction for
interdisciplinary research. Our test domain is the history and philosophy of
scientific work on animal mind and cognition. The methods can be generalized to
other research areas and ultimately support a system for semi-automatic
identification of argument structures. We provide a case study for the
application of the methods to the problem of identifying and extracting
arguments about anthropomorphism during a critical period in the development of
comparative psychology. We show how a combination of classification systems and
mixed-membership models trained over large digital libraries can inform
resource discovery in this domain. Through a novel approach of ""drill-down""
topic modeling---simultaneously reducing both the size of the corpus and the
unit of analysis---we are able to reduce a large collection of fulltext volumes
to a much smaller set of pages within six focal volumes containing arguments of
interest to historians and philosophers of comparative psychology. The volumes
identified in this way did not appear among the first ten results of the
keyword search in the HathiTrust digital library and the pages bear the kind of
""close reading"" needed to generate original interpretations that is the heart
of scholarly work in the humanities. Zooming back out, we provide a way to
place the books onto a map of science originally constructed from very
different data and for different purposes. The multilevel approach advances
understanding of the intellectual and societal contexts in which writings are
interpreted.
"
1758,"The Dawn of Today's Popular Domains: A Study of the Archived German Web
  over 18 Years","  The Web has been around and maturing for 25 years. The popular websites of
today have undergone vast changes during this period, with a few being there
almost since the beginning and many new ones becoming popular over the years.
This makes it worthwhile to take a look at how these sites have evolved and
what they might tell us about the future of the Web. We therefore embarked on a
longitudinal study spanning almost the whole period of the Web, based on data
collected by the Internet Archive starting in 1996, to retrospectively analyze
how the popular Web as of now has evolved over the past 18 years.
  For our study we focused on the German Web, specifically on the top 100 most
popular websites in 17 categories. This paper presents a selection of the most
interesting findings in terms of volume, size as well as age of the Web. While
related work in the field of Web Dynamics has mainly focused on change rates
and analyzed datasets spanning less than a year, we looked at the evolution of
websites over 18 years. We found that around 70% of the pages we investigated
are younger than a year, with an observed exponential growth in age as well as
in size up to now. If this growth rate continues, the number of pages from the
popular domains will almost double in the next two years. In addition, we give
insights into our data set, provided by the Internet Archive, which hosts the
largest and most complete Web archive as of today.
"
1759,Archiving Software Surrogates on the Web for Future Reference,"  Software has long been established as an essential aspect of the scientific
process in mathematics and other disciplines. However, reliably referencing
software in scientific publications is still challenging for various reasons. A
crucial factor is that software dynamics with temporal versions or states are
difficult to capture over time. We propose to archive and reference surrogates
instead, which can be found on the Web and reflect the actual software to a
remarkable extent. Our study shows that about a half of the webpages of
software are already archived with almost all of them including some kind of
documentation.
"
1760,Linking Mathematical Software in Web Archives,"  The Web is our primary source of all kinds of information today. This
includes information about software as well as associated materials, like
source code, documentation, related publications and change logs. Such data is
of particular importance in research in order to conduct, comprehend and
reconstruct scientific experiments that involve software. swMATH, a
mathematical software directory, attempts to identify software mentions in
scientific articles and provides additional information as well as links to the
Web. However, just like software itself, the Web is dynamic and most likely the
information on the Web has changed since it was referenced in a scientific
publication. Therefore, it is crucial to preserve the resources of a software
on the Web to capture its states over time.
  We found that around 40% of the websites in swMATH are already included in an
existing Web archive. Out of these, 60% of contain some kind of documentation
and around 45% even provide downloads of software artifacts. Hence, already
today links can be established based on the publication dates of corresponding
articles. The contained data enable enriching existing information with a
temporal dimension. In the future, specialized infrastructure will improve the
coverage of software resources and allow explicit references in scientific
publications.
"
1761,Insights into Entity Name Evolution on Wikipedia,"  Working with Web archives raises a number of issues caused by their temporal
characteristics. Depending on the age of the content, additional knowledge
might be needed to find and understand older texts. Especially facts about
entities are subject to change. Most severe in terms of information retrieval
are name changes. In order to find entities that have changed their name over
time, search engines need to be aware of this evolution. We tackle this problem
by analyzing Wikipedia in terms of entity evolutions mentioned in articles
regardless the structural elements. We gathered statistics and automatically
extracted minimum excerpts covering name changes by incorporating lists
dedicated to that subject. In future work, these excerpts are going to be used
to discover patterns and detect changes in other sources. In this work we
investigate whether or not Wikipedia is a suitable source for extracting the
required knowledge.
"
1762,Named Entity Evolution Analysis on Wikipedia,"  Accessing Web archives raises a number of issues caused by their temporal
characteristics. Additional knowledge is needed to find and understand older
texts. Especially entities mentioned in texts are subject to change. Most
severe in terms of information retrieval are name changes. In order to find
entities that have changed their name over time, search engines need to be
aware of this evolution. We tackle this problem by analyzing Wikipedia in terms
of entity evolutions mentioned in articles. We present statistical data on
excerpts covering name changes, which will be used to discover similar text
passages and extract evolution knowledge in future work.
"
1763,Extraction of Evolution Descriptions from the Web,"  The evolution of named entities affects exploration and retrieval tasks in
digital libraries. An information retrieval system that is aware of name
changes can actively support users in finding former occurrences of evolved
entities. However, current structured knowledge bases, such as DBpedia or
Freebase, do not provide enough information about evolutions, even though the
data is available on their resources, like Wikipedia. Our \emph{Evolution Base}
prototype will demonstrate how excerpts describing name evolutions can be
identified on these websites with a promising precision. The descriptions are
classified by means of models that we trained based on a recent analysis of
named entity evolutions on Wikipedia.
"
1764,Named Entity Evolution Recognition on the Blogosphere,"  Advancements in technology and culture lead to changes in our language. These
changes create a gap between the language known by users and the language
stored in digital archives. It affects user's possibility to firstly find
content and secondly interpret that content. In previous work we introduced our
approach for Named Entity Evolution Recognition~(NEER) in newspaper
collections. Lately, increasing efforts in Web preservation lead to increased
availability of Web archives covering longer time spans. However, language on
the Web is more dynamic than in traditional media and many of the basic
assumptions from the newspaper domain do not hold for Web data. In this paper
we discuss the limitations of existing methodology for NEER. We approach these
by adapting an existing NEER method to work on noisy data like the Web and the
Blogosphere in particular. We develop novel filters that reduce the noise and
make use of Semantic Web resources to obtain more information about terms. Our
evaluation shows the potentials of the proposed approach.
"
1765,"A bibliometric approach to Systematic Mapping Studies: The case of the
  evolution and perspectives of community detection in complex networks","  Critical analysis of the state of the art is a necessary task when
identifying new research lines worthwhile to pursue. To such an end, all the
available work related to the field of interest must be taken into account. The
key point is how to organize, analyze, and make sense of the huge amount of
scientific literature available today on any topic. To tackle this problem, we
present here a bibliometric approach to Systematic Mapping Studies (SMS). Thus,
a modify SMS protocol is used relying on the scientific references metadata to
extract, process and interpret the wealth of information contained in nowadays
research literature. As a test case, the procedure is applied to determine the
current state and perspectives of community detection in complex networks. Our
results show that community detection is a still active, far from exhausted, in
development, field. In addition, we find that, by far, the most exploited
methods are those related to determining hierarchical community structures. On
the other hand, the results show that fuzzy clustering techniques, despite
their interest, are underdeveloped as well as the adaptation of existing
algorithms to parallel or, more specifically, distributed, computational
systems.
"
1766,"Memetic search for overlapping topics based on a local evaluation of
  link communities","  In spite of recent advances in field delineation methods, bibliometricians
still don't know the extent to which their topic detection algorithms
reconstruct `ground truths', i.e. thematic structures in the scientific
literature. In this paper, we demonstrate a new approach to the delineation of
thematic structures that attempts to match the algorithm to theoretically
derived and empirically observed properties all thematic structures have in
common. We cluster citation links rather than publication nodes, use
predominantly local information and search for communities of links starting
from seed subgraphs in order to allow for pervasive overlaps of topics. We
evaluate sets of links with a new cost function and assume that local minima in
the cost landscape correspond to link communities. Because this cost landscape
has many local minima we define a valid community as the community with the
lowest minimum within a certain range. Since finding all valid communities is
impossible for large networks, we designed a memetic algorithm that combines
probabilistic evolutionary strategies with deterministic local searches. We
apply our approach to a network of about 15,000 Astronomy & Astrophysics papers
published 2010 and their cited sources, and to a network of about 100,000
Astronomy & Astrophysics papers (published 2003--2010) which are linked through
direct citations.
"
1767,"Citation-based clustering of publications using CitNetExplorer and
  VOSviewer","  Clustering scientific publications in an important problem in bibliometric
research. We demonstrate how two software tools, CitNetExplorer and VOSviewer,
can be used to cluster publications and to analyze the resulting clustering
solutions. CitNetExplorer is used to cluster a large set of publications in the
field of astronomy and astrophysics. The publications are clustered based on
direct citation relations. CitNetExplorer and VOSviewer are used together to
analyze the resulting clustering solutions. Both tools use visualizations to
support the analysis of the clustering solutions, with CitNetExplorer focusing
on the analysis at the level of individual publications and VOSviewer focusing
on the analysis at an aggregate level. The demonstration provided in this paper
shows how a clustering of publications can be created and analyzed using freely
available software tools. Using the approach presented in this paper,
bibliometricians are able to carry out sophisticated cluster analyses without
the need to have a deep knowledge of clustering techniques and without
requiring advanced computer skills.
"
1768,"Content-Based Video Retrieval in Historical Collections of the German
  Broadcasting Archive","  The German Broadcasting Archive (DRA) maintains the cultural heritage of
radio and television broadcasts of the former German Democratic Republic (GDR).
The uniqueness and importance of the video material stimulates a large
scientific interest in the video content. In this paper, we present an
automatic video analysis and retrieval system for searching in historical
collections of GDR television recordings. It consists of video analysis
algorithms for shot boundary detection, concept classification, person
recognition, text recognition and similarity search. The performance of the
system is evaluated from a technical and an archival perspective on 2,500 hours
of GDR television recordings.
"
1769,Google Scholar and the gray literature: A reply to Bonato's review,"  Recently, a review concluded that Google Scholar (GS) is not a suitable
source of information ""for identifying recent conference papers or other gray
literature publications"". The goal of this letter is to demonstrate that GS can
be an effective tool to search and find gray literature, as long as appropriate
search strategies are used. To do this, we took as examples the same two case
studies used by the original review, describing first how GS processes
original's search strategies, then proposing alternative search strategies, and
finally generalizing each case study to compose a general search procedure
aimed at finding gray literature in Google Scholar for two wide selected case
studies: a) all contributions belonging to a congress (the ASCO Annual
Meeting); and b) indexed guidelines as well as gray literature within medical
institutions (National Institutes of Health) and governmental agencies (U.S.
Department of Health & Human Services). The results confirm that original
search strategies were undertrained offering misleading results and erroneous
conclusions. Google Scholar lacks many of the advanced search features
available in other bibliographic databases (such as Pubmed), however, it is one
thing to have a friendly search experience, and quite another to find gray
literature. We finally conclude that Google Scholar is a powerful tool for
searching gray literature, as long as the users are familiar with all the
possibilities it offers as a search engine. Poorly formulated searches will
undoubtedly return misleading results.
"
1770,"Developing an ontology for the access to the contents of an archival
  fonds: the case of the Catasto Gregoriano","  The research was proposed to exploit and extend the relational and contextual
nature of the information assets of the Catasto Gregoriano, kept at the
Archivio di Stato in Rome. Developed within the MODEUS project (Making Open
Data Effectively Usable), this study originates from the following key ideas of
MODEUS: to require Open Data to be expressed in terms of an ontology, and to
include such an ontology as a documentation of the data themselves. Thus, Open
Data are naturally linked by means of the ontology, which meets the
requirements of the Linked Open Data vision.
"
1771,Wiki-index of authors popularity,"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
"
1772,Clustering articles based on semantic similarity,"  Document clustering is generally the first step for topic identification.
Since many clustering methods operate on the similarities between documents, it
is important to build representations of these documents which keep their
semantics as much as possible and are also suitable for efficient similarity
calculation. The metadata of articles in the Astro dataset contribute to a
semantic matrix, which uses a vector space to capture the semantics of entities
derived from these articles and consequently supports the contextual
exploration of these entities in LittleAriadne. However, this semantic matrix
does not allow to calculate similarities between articles directly. In this
paper, we will describe in detail how we build a semantic representation for an
article from the entities that are associated with it. Base on such semantic
representations of articles, we apply two standard clustering methods, K-Means
and the Louvain community detection algorithm, which leads to our two
clustering solutions labelled as OCLC-31 (standing for K-Means) and
OCLC-Louvain (standing for Louvain). In this paper, we will give the
implementation details and a basic comparison with other clustering solutions
that are reported in this special issue.
"
1773,"OntoMath Digital Ecosystem: Ontologies, Mathematical Knowledge Analytics
  and Management","  In this article we consider the basic ideas, approaches and results of
developing of mathematical knowledge management technologies based on
ontologies. These solutions form the basis of a specialized digital ecosystem
OntoMath which consists of the ontology of the logical structure of
mathematical documents Mocassin and ontology of mathematical knowledge
OntoMathPRO, tools of text analysis, recommender system and other applications
to manage mathematical knowledge. The studies are in according to the ideas of
creating a distributed system of interconnected repositories of digitized
versions of mathematical documents and project to create a World Digital
Mathematical Library.
"
1774,Why We Read Wikipedia,"  Wikipedia is one of the most popular sites on the Web, with millions of users
relying on it to satisfy a broad range of information needs every day. Although
it is crucial to understand what exactly these needs are in order to be able to
meet them, little is currently known about why users visit Wikipedia. The goal
of this paper is to fill this gap by combining a survey of Wikipedia readers
with a log-based analysis of user activity. Based on an initial series of user
surveys, we build a taxonomy of Wikipedia use cases along several dimensions,
capturing users' motivations to visit Wikipedia, the depth of knowledge they
are seeking, and their knowledge of the topic of interest prior to visiting
Wikipedia. Then, we quantify the prevalence of these use cases via a
large-scale user survey conducted on live Wikipedia with almost 30,000
responses. Our analyses highlight the variety of factors driving users to
Wikipedia, such as current events, media coverage of a topic, personal
curiosity, work or school assignments, or boredom. Finally, we match survey
responses to the respondents' digital traces in Wikipedia's server logs,
enabling the discovery of behavioral patterns associated with specific use
cases. For instance, we observe long and fast-paced page sequences across
topics for users who are bored or exploring randomly, whereas those using
Wikipedia for work or school spend more time on individual articles focused on
topics such as science. Our findings advance our understanding of reader
motivations and behavior on Wikipedia and can have implications for developers
aiming to improve Wikipedia's user experience, editors striving to cater to
their readers' needs, third-party services (such as search engines) providing
access to Wikipedia content, and researchers aiming to build tools such as
recommendation engines.
"
1775,Allometric Scaling in Scientific Fields,"  Allometric scaling can reflect underlying mechanisms, dynamics and structures
in complex systems; examples include typical scaling laws in biology, ecology
and urban development. In this work, we study allometric scaling in scientific
fields. By performing an analysis of the outputs/inputs of various scientific
fields, including the numbers of publications, citations, and references, with
respect to the number of authors, we find that in all fields that we have
studied thus far, including physics, mathematics and economics, there are
allometric scaling laws relating the outputs/inputs and the sizes of scientific
fields. Furthermore, the exponents of the scaling relations have remained quite
stable over the years. We also find that the deviations of individual subfields
from the overall scaling laws are good indicators for ranking subfields
independently of their sizes.
"
1776,MOLIERE: Automatic Biomedical Hypothesis Generation System,"  Hypothesis generation is becoming a crucial time-saving technique which
allows biomedical researchers to quickly discover implicit connections between
important concepts. Typically, these systems operate on domain-specific
fractions of public medical data. MOLIERE, in contrast, utilizes information
from over 24.5 million documents. At the heart of our approach lies a
multi-modal and multi-relational network of biomedical objects extracted from
several heterogeneous datasets from the National Center for Biotechnology
Information (NCBI). These objects include but are not limited to scientific
papers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses
using Latent Dirichlet Allocation applied on abstracts found near shortest
paths discovered within this network, and demonstrate the effectiveness of
MOLIERE by performing hypothesis generation on historical data. Our network,
implementation, and resulting data are all publicly available for the broad
scientific community.
"
1777,"Mapping Patent Classifications: Portfolio and Statistical Analysis, and
  the Comparison of Strengths and Weaknesses","  The Cooperative Patent Classifications (CPC) jointly developed by the
European and US Patent Offices provide a new basis for mapping and portfolio
analysis. This update provides an occasion for rethinking the parameter
choices. The new maps are significantly different from previous ones, although
this may not always be obvious on visual inspection. Since these maps are
statistical constructs based on index terms, their quality--as different from
utility--can only be controlled discursively. We provide nested maps online and
a routine for portfolio overlays and further statistical analysis. We add a new
tool for ""difference maps"" which is illustrated by comparing the portfolios of
patents granted to Novartis and MSD in 2016.
"
1778,"Citation personal display: A case study of personal websites by
  physicists in 11 well-known universities","  This paper aims to investigate the extent to which researchers display
citation, and wants to examine whether there are researcher differences in
citation personal display at the level of university, country, and academic
rank. Physicists in 11 well-known universities in USA, Britain, and China were
chosen as the object of study. It was manually identified if physicists had
mentioned citation counts, citation-based indices, or a link to Google Scholar
Citations (GSC) on the personal websites. A chi-square test is constructed to
test researcher differences in citation personal display. Results showed that
the overall proportion of citation personal display is not high (14.8%), with
129 of 870 physicists displaying citation. And physicists from different
well-known universities indeed had a significant difference in citation
personal display. Moreover, at the national level, it was noticed that
physicists in well-known Chinese universities had the highest level of citation
personal display, followed by Britain and the USA. Further, this study also
found that researchers who had the academic rank of professor had the highest
citation personal display. In addition, the differences in h-index personal
display by university, country or academic rank were analyzed, and the results
showed that they were not statistically significant.
"
1779,PubTree: A Hierarchical Search Tool for the MEDLINE Database,"  Keeping track of the ever-increasing body of scientific literature is an
escalating challenge. We present PubTree a hierarchical search tool that
efficiently searches the PubMed/MEDLINE dataset based upon a decision tree
constructed using >26 million abstracts. The tool is implemented as a webpage,
where users are asked a series of eighteen questions to locate pertinent
articles. The implementation of this hierarchical search tool highlights issues
endemic with document retrieval. However, the construction of this tree
indicates that with future developments hierarchical search could become an
effective tool (or adjunct) in the mining of biological literature.
"
1780,Mutual Information based labelling and comparing clusters,"  After a clustering solution is generated automatically, labelling these
clusters becomes important to help understanding the results. In this paper, we
propose to use a Mutual Information based method to label clusters of journal
articles. Topical terms which have the highest Normalised Mutual Information
(NMI) with a certain cluster are selected to be the labels of the cluster.
Discussion of the labelling technique with a domain expert was used as a check
that the labels are discriminating not only lexical-wise but also semantically.
Based on a common set of topical terms, we also propose to generate lexical
fingerprints as a representation of individual clusters. Eventually, we
visualise and compare these fingerprints of different clusters from either one
clustering solution or different ones.
"
1781,"Contextualization of topics: Browsing through the universe of
  bibliographic information","  This paper describes how semantic indexing can help to generate a contextual
overview of topics and visually compare clusters of articles. The method was
originally developed for an innovative information exploration tool, called
Ariadne, which operates on bibliographic databases with tens of millions of
records. In this paper, the method behind Ariadne is further developed and
applied to the research question of the special issue ""Same data, different
results"" - the better understanding of topic (re-)construction by different
bibliometric approaches. For the case of the Astro dataset of 111,616 articles
in astronomy and astrophysics, a new instantiation of the interactive exploring
tool, LittleAriadne, has been created. This paper contributes to the overall
challenge to delineate and define topics in two different ways. First, we
produce two clustering solutions based on vector representations of articles in
a lexical space. These vectors are built on semantic indexing of entities
associated with those articles. Second, we discuss how LittleAriadne can be
used to browse through the network of topical terms, authors, journals,
citations and various cluster solutions of the Astro dataset. More
specifically, we treat the assignment of an article to the different clustering
solutions as an additional element of its bibliographic record. Keeping the
principle of semantic indexing on the level of such an extended list of
entities of the bibliographic record, LittleAriadne in turn provides a
visualization of the context of a specific clustering solution. It also conveys
the similarity of article clusters produced by different algorithms, hence
representing a complementary approach to other possible means of comparison.
"
1782,"\'Etude sur les portails et agr\'egateurs des ressources p\'edagogiques
  universitaires francophones en acc\`es libre","  This study responds to the first measure undertaken on July 17, 2015 by
IDNEUF prject, that of an exploratory analysis of the existing portals and
aggregators of free French-language academic resources. The idea is to provide
an overview of the most common trends and practices in the constitution and
organization of digital online learning resource portals. The study of these
trends would help to define the appropriate choices and conditions for
designing the future common French-language portal and to optimize its services
for the conservation, exchange, integration and pooling of educational
resources within the distributed technological framework of French-language
universities. This framework should therefore be interconnected, transparent
and interoperable, reflecting both the linguistic and cultural specificities of
partner institutions and their ambitions for technological and economic
developments. The development of this first exploratory study of portals would
take into account the two technological solutions discussed at the task force
meeting on 17 July 2015.1. The alternative of extending the capabilities of
France's Digital University's search engine to French-language academic
institutions will require that the experience of the UNT (Numerical Thematic
University) be taken into account as a key player in the digital portal In
higher education (supnumerique.gouv.fr). Thus, the present study should first
target the portals of the UNT as models replicable or extensible to the French
context by analyzing their technological choices, their modes of organization
and their modes of use and communication;2. Next, the study will not be limited
to analyzing UNT portals. The proposed hypothesis to create a common portal of
French portals from the existing university portals also requires exploring
this possibility and proposing an analysis of existing portals that function
according to other organizational models.
"
1783,Coverage of Author Identifiers in Web of Science and Scopus,"  As digital collections of scientific literature are widespread and used
frequently in knowledge-intense working environments, it has become a challenge
to identify author names correctly. The treatment of homonyms is crucial for
the reliable resolution of author names. Apart from varying handling of first,
middle and last names, vendors as well as the digital library community created
tools to address the problem of author name disambiguation. This technical
report focuses on two widespread collections of scientific literature, Web of
Science (WoS) and Scopus, and the coverage with author identification
information such as Researcher ID, ORCID and Scopus Author Identifier in the
period 1996 - 2014. The goal of this study is to describe the significant
differences of the two collections with respect to overall distribution of
author identifiers and its use across different subject domains. We found that
the STM disciplines show the best coverage of author identifiers in our dataset
of 6,032,000 publications which are both covered by WoS and Scopus. In our
dataset we found 184,823 distinct ResearcherIDs and 70,043 distinct ORCIDs. In
the appendix of this report we list a complete overview of all WoS subject
areas and the amount of author identifiers in these subject areas.
"
1784,Scientific wealth and inequality within nations,"  We show that the greater the scientific wealth of a nation, the more likely
that it will tend to concentrate this excellence in a few premier institutions.
That is, great wealth implies great inequality of distribution. The scientific
wealth is interpreted in terms of citation data harvested by Google Scholar
Citations for profiled institutions from all countries in the world.
"
1785,Tracing Networks of Knowledge in the Digital Age,"  The emergence of new digital technologies has allowed the study of human
behaviour at a scale and at level of granularity that were unthinkable just a
decade ago. In particular, by analysing the digital traces left by people
interacting in the online and offline worlds, we are able to trace the
spreading of knowledge and ideas at both local and global scales. In this
article we will discuss how these digital traces can be used to map knowledge
across the world, outlining both the limitations and the challenges in
performing this type of analysis. We will focus on data collected from social
media platforms, large-scale digital repositories and mobile data. Finally, we
will provide an overview of the tools that are available to scholars and
practitioners for understanding these processes using these emerging forms of
data.
"
1786,"Doing Things Twice (Or Differently): Strategies to Identify Studies for
  Targeted Validation","  The ""reproducibility crisis"" has been a highly visible source of scientific
controversy and dispute. Here, I propose and review several avenues for
identifying and prioritizing research studies for the purpose of targeted
validation. Of the various proposals discussed, I identify scientific data
science as being a strategy that merits greater attention among those
interested in reproducibility. I argue that the tremendous potential of
scientific data science for uncovering high-value research studies is a
significant and rarely discussed benefit of the transition to a fully
open-access publishing model.
"
1787,"Use of the journal impact factor for assessing individual articles need
  not be statistically wrong","  Most scientometricians reject the use of the journal impact factor for
assessing individual articles and their authors. The well-known San Francisco
Declaration on Research Assessment also strongly objects against this way of
using the impact factor. Arguments against the use of the impact factor at the
level of individual articles are often based on statistical considerations. The
skewness of journal citation distributions typically plays a central role in
these arguments. We present a theoretical analysis of statistical arguments
against the use of the impact factor at the level of individual articles. Our
analysis shows that these arguments do not support the conclusion that the
impact factor should not be used for assessing individual articles. In fact,
our computer simulations demonstrate the possibility that the impact factor is
a more accurate indicator of the value of an article than the number of
citations the article has received. It is important to critically discuss the
dominant role of the impact factor in research evaluations, but the discussion
should not be based on misplaced statistical arguments. Instead, the primary
focus should be on the socio-technical implications of the use of the impact
factor.
"
1788,"Reconsidering the gold open access citation advantage postulate in a
  multidisciplinary context: an analysis of the subject categories in the Web
  of Science database 2009-2014","  Since Lawrence in 2001 proposed the open access (OA) citation advantage, the
potential benefit of OA in relation to the citation impact has been discussed
in depth. The methodology to test this postulate ranges from comparing the
impact factors of OA journals versus traditional ones, to comparing citations
of OA versus non-OA articles published in the same non-OA journals. However,
conclusions are not entirely consistent among fields, and two possible
explications have been suggested in those fields where a citation advantage has
been observed for OA: the early view and the selection bias postulates. In this
study, a longitudinal and multidisciplinary analysis of the gold OA citation
advantage is developed. All research articles in all journals for all subject
categories in the multidisciplinary database Web of Science are considered. A
total of 1,137,634 articles - 86,712 OA articles (7.6%) and 1,050,922 non-OA
articles (92.4%)- published in 2009 are analysed. The citation window
considered goes from 2009 to 2014, and data are aggregated for the 249
disciplines (subject categories). At journal level, we also study the evolution
of journal impact factors for OA and non-OA journals in those disciplines whose
OA prevalence is higher (top 36 subject categories). As the main conclusion,
there is no generalizable gold OA citation advantage, neither at article nor at
journal level.
"
1789,Impact of URI Canonicalization on Memento Count,"  Quantifying the captures of a URI over time is useful for researchers to
identify the extent to which a Web page has been archived. Memento TimeMaps
provide a format to list mementos (URI-Ms) for captures along with brief
metadata, like Memento-Datetime, for each URI-M. However, when some URI-Ms are
dereferenced, they simply provide a redirect to a different URI-M (instead of a
unique representation at the datetime), often also present in the TimeMap. This
infers that confidently obtaining an accurate count quantifying the number of
non-forwarding captures for a URI-R is not possible using a TimeMap alone and
that the magnitude of a TimeMap is not equivalent to the number of
representations it identifies. In this work we discuss this particular
phenomena in depth. We also perform a breakdown of the dynamics of counting
mementos for a particular URI-R (google.com) and quantify the prevalence of the
various canonicalization patterns that exacerbate attempts at counting using
only a TimeMap. For google.com we found that 84.9% of the URI-Ms result in an
HTTP redirect when dereferenced. We expand on and apply this metric to TimeMaps
for seven other URI-Rs of large Web sites and thirteen academic institutions.
Using a ratio metric DI for the number of URI-Ms without redirects to those
requiring a redirect when dereferenced, five of the eight large web sites' and
two of the thirteen academic institutions' TimeMaps had a ratio of ratio less
than one, indicating that more than half of the URI-Ms in these TimeMaps result
in redirects when dereferenced.
"
1790,The Accuracy of Confidence Intervals for Field Normalised Indicators,"  When comparing the average citation impact of research groups, universities
and countries, field normalisation reduces the influence of discipline and
time. Confidence intervals for these indicators can help with attempts to infer
whether differences between sets of publications are due to chance factors.
Although both bootstrapping and formulae have been proposed for these, their
accuracy is unknown. In response, this article uses simulated data to
systematically compare the accuracy of confidence limits in the simplest
possible case, a single field and year. The results suggest that the MNLCS
(Mean Normalised Log-transformed Citation Score) confidence interval formula is
conservative for large groups but almost always safe, whereas bootstrap MNLCS
confidence intervals tend to be accurate but can be unsafe for smaller world or
group sample sizes. In contrast, bootstrap MNCS (Mean Normalised Citation
Score) confidence intervals can be very unsafe, although their accuracy
increases with sample sizes.
"
1791,Scholia and scientometrics with Wikidata,"  Scholia is a tool to handle scientific bibliographic information in Wikidata.
The Scholia Web service creates on-the-fly scholarly profiles for researchers,
organizations, journals, publishers, individual scholarly works, and for
research topics. To collect the data, it queries the SPARQL-based Wikidata
Query Service. Among several display formats available in Scholia are lists of
publications for individual researchers and organizations, publications per
year, employment timelines, as well as co-author networks and citation graphs.
The Python package implementing the Web service is also able to format Wikidata
bibliographic entries for use in LaTeX/BIBTeX.
"
1792,"Opening Scholarly Communication in Social Sciences by Connecting
  Collaborative Authoring to Peer Review","  The objective of the OSCOSS research project on ""Opening Scholarly
Communication in the Social Sciences"" is to build a coherent collaboration
environment that facilitates scholarly communication workflows of social
scientists in the roles of authors, reviewers, editors and readers. This paper
presents the implementation of the core of this environment: the integration of
the Fidus Writer academic word processor with the Open Journal Systems (OJS)
submission and review management system.
"
1793,"Evaluation of 50 Greek Science and Engineering University Departments
  using Google Scholar","  In this paper, the scientometric evaluation of faculty members of 50 Greek
Science and Engineering University Departments is presented. 1978 academics
were examined in total. The number of papers, citations, h-index and i10-index
have been collected for each academic, department, school and university using
Google Scholar and the citations analysis program Publish or Perish. Analysis
of the collected data showed that departments of the same academic discipline
are characterized by significant differences on the scientific outcome. In
addition, in the majority of the evaluated departments a significant difference
in h-index between academics who report scientific activity on the departments
website and those who do not, was observed. Moreover, academics who earned
their PhD title in the USA demonstrate higher indices in comparison to scholars
who obtained their PhD title in Europe or in Greece. Finally, the correlation
between the academic rank and the scholars h-index (or the number of their
citations) is quite low in some departments, which, under specific
circumstances, could be an indication of the lack of meritocracy.
"
1794,"Citation histories of papers: sometimes the rich get richer, sometimes
  they don't","  We describe a simple model of how a publication's citations change over time,
based on pure-birth stochastic processes with a linear cumulative advantage
effect. The model is applied to citation data from the Physical Review corpus
provided by APS. Our model reveals that papers fall into three different
clusters: papers that have rapid initial citations and ultimately high impact
(fast-hi), fast to rise but quick to plateau (fast-flat), or late bloomers
(slow-late), which may either never achieve many citations, or do so many years
after publication. In ""fast-hi"" and ""slow-late"", there is a rich-get-richer
effect: papers that have many citations accumulate additional citations more
rapidly while the ""fast-flat"" papers do not display this effect. We conclude by
showing that only a few years of post-publication statistics are needed to
identify high impact (""fast-hi"") papers.
"
1795,"A modification to Hirsch index allowing comparisons across different
  scientific fields","  The aim of this paper is to propose a simple modification to the original
measure, the relative Hirsch index, which assigns each researcher a value
between 0 (the bottom) and 1 (the top), expressing his/her distance to the top
in a given field. By this normalization scholars from different scientific
disciplines can be compared.
"
1796,"The coverage of Microsoft Academic: Analyzing the publication output of
  a university","  This is the first detailed study on the coverage of Microsoft Academic (MA).
Based on the complete and verified publication list of a university, the
coverage of MA was assessed and compared with two benchmark databases, Scopus
and Web of Science (WoS), on the level of individual publications. Citation
counts were analyzed, and issues related to data retrieval and data quality
were examined. A Perl script was written to retrieve metadata from MA based on
publication titles. The script is freely available on GitHub. We find that MA
covers journal articles, working papers, and conference items to a substantial
extent and indexes more document types than the benchmark databases (e.g.,
working papers, dissertations). MA clearly surpasses Scopus and WoS in covering
book-related document types and conference items but falls slightly behind
Scopus in journal articles. The coverage of MA is favorable for evaluative
bibliometrics in most research fields, including economics/business,
computer/information sciences, and mathematics. However, MA shows biases
similar to Scopus and WoS with regard to the coverage of the humanities,
non-English publications, and open-access publications. Rank correlations of
citation counts are high between MA and the benchmark databases. We find that
the publication year is correct for 89.5% of all publications and the number of
authors is correct for 95.1% of the journal articles. Given the fast and
ongoing development of MA, we conclude that MA is on the verge of becoming a
bibliometric superpower. However, comprehensive studies on the quality of MA
metadata are still lacking.
"
1797,"What makes papers visible on social media? An analysis of various
  document characteristics","  In this study we have investigated the relationship between different
document characteristics and the number of Mendeley readership counts, tweets,
Facebook posts, mentions in blogs and mainstream media for 1.3 million papers
published in journals covered by the Web of Science (WoS). It aims to
demonstrate that how factors affecting various social media-based indicators
differ from those influencing citations and which document types are more
popular across different platforms. Our results highlight the heterogeneous
nature of altmetrics, which encompasses different types of uses and user groups
engaging with research on social media.
"
1798,"A Preferential Attachment Paradox: How Preferential Attachment Combines
  with Growth to Produce Networks with Log-normal In-degree Distributions","  Every network scientist knows that preferential attachment combines with
growth to produce networks with power-law in-degree distributions. How, then,
is it possible for the network of American Physical Society journal collection
citations to enjoy a log-normal citation distribution when it was found to have
grown in accordance with preferential attachment? This anomalous result, which
we exalt as the preferential attachment paradox, has remained unexplained since
the physicist Sidney Redner first made light of it over a decade ago. Here we
propose a resolution. The chief source of the mischief, we contend, lies in
Redner having relied on a measurement procedure bereft of the accuracy required
to distinguish preferential attachment from another form of attachment that is
consistent with a log-normal in-degree distribution. There was a high-accuracy
measurement procedure in use at the time, but it would have have been difficult
to use it to shed light on the paradox, due to the presence of a systematic
error inducing design flaw. In recent years the design flaw had been recognised
and corrected. We show that the bringing of the newly corrected measurement
procedure to bear on the data leads to a resolution of the paradox.
"
1799,"Mendeley readership as a filtering tool to identify highly cited
  publications","  This study presents a large scale analysis of the distribution and presence
of Mendeley readership scores over time and across disciplines. We study
whether Mendeley readership scores (RS) can identify highly cited publications
more effectively than journal citation scores (JCS). Web of Science (WoS)
publications with DOIs published during the period 2004-2013 and across 5 major
scientific fields have been analyzed. The main result of this study shows that
readership scores are more effective (in terms of precision/recall values) than
journal citation scores to identify highly cited publications across all fields
of science and publication years. The findings also show that 86.5% of all the
publications are covered by Mendeley and have at least one reader. Also the
share of publications with Mendeley readership scores is increasing from 84% in
2004 to 89% in 2009, and decreasing from 88% in 2010 to 82% in 2013. However,
it is noted that publications from 2010 onwards exhibit on average a higher
density of readership vs. citation scores. This indicates that compared to
citation scores, readership scores are more prevalent for recent publications
and hence they could work as an early indicator of research impact. These
findings highlight the potential and value of Mendeley as a tool for
scientometric purposes and particularly as a relevant tool to identify highly
cited publications.
"
1800,Historical collaborative geocoding,"  The latest developments in digital have provided large data sets that can
increasingly easily be accessed and used. These data sets often contain
indirect localisation information, such as historical addresses. Historical
geocoding is the process of transforming the indirect localisation information
to direct localisation that can be placed on a map, which enables spatial
analysis and cross-referencing. Many efficient geocoders exist for current
addresses, but they do not deal with the temporal aspect and are based on a
strict hierarchy (..., city, street, house number) that is hard or impossible
to use with historical data. Indeed historical data are full of uncertainties
(temporal aspect, semantic aspect, spatial precision, confidence in historical
source, ...) that can not be resolved, as there is no way to go back in time to
check. We propose an open source, open data, extensible solution for geocoding
that is based on the building of gazetteers composed of geohistorical objects
extracted from historical topographical maps. Once the gazetteers are
available, geocoding an historical address is a matter of finding the
geohistorical object in the gazetteers that is the best match to the historical
address. The matching criteriae are customisable and include several dimensions
(fuzzy semantic, fuzzy temporal, scale, spatial precision ...). As the goal is
to facilitate historical work, we also propose web-based user interfaces that
help geocode (one address or batch mode) and display over current or historical
topographical maps, so that they can be checked and collaboratively edited. The
system is tested on Paris city for the 19-20th centuries, shows high returns
rate and is fast enough to be used interactively.
"
1801,Randomizing growing networks with a time-respecting null model,"  Complex networks are often used to represent systems that are not static but
grow with time: people make new friendships, new papers are published and refer
to the existing ones, and so forth. To assess the statistical significance of
measurements made on such networks, we propose a randomization methodology---a
time-respecting null model---that preserves both the network's degree sequence
and the time evolution of individual nodes' degree values. By preserving the
temporal linking patterns of the analyzed system, the proposed model is able to
factor out the effect of the system's temporal patterns on its structure. We
apply the model to the citation network of Physical Review scholarly papers and
the citation network of US movies. The model reveals that the two datasets are
strikingly different with respect to their degree-degree correlations, and we
discuss the important implications of this finding on the information provided
by paradigmatic node centrality metrics such as indegree and Google's PageRank.
The randomization methodology proposed here can be used to assess the
significance of any structural property in growing networks, which could bring
new insights into the problems where null models play a critical role, such as
the detection of communities and network motifs.
"
1802,Quantifying and suppressing ranking bias in a large citation network,"  It is widely recognized that citation counts for papers from different fields
cannot be directly compared because different scientific fields adopt different
citation practices. Citation counts are also strongly biased by paper age since
older papers had more time to attract citations. Various procedures aim at
suppressing these biases and give rise to new normalized indicators, such as
the relative citation count. We use a large citation dataset from Microsoft
Academic Graph and a new statistical framework based on the Mahalanobis
distance to show that the rankings by well known indicators, including the
relative citation count and Google's PageRank score, are significantly biased
by paper field and age. We propose a general normalization procedure motivated
by the $z$-score which produces much less biased rankings when applied to
citation count and PageRank score.
"
1803,Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia,"  Only few digital libraries and reference managers offer recommender systems,
although such systems could assist users facing information overload. In this
paper, we introduce Mr. DLib's recommendations-as-a-service, which allows third
parties to easily integrate a recommender system into their products. We
explain the recommender approaches implemented in Mr. DLib (content-based
filtering among others), and present details on 57 million recommendations,
which Mr. DLib delivered to its partner GESIS Sowiport. Finally, we outline our
plans for future development, including integration into JabRef, establishing a
living lab, and providing personalized recommendations.
"
1804,Discovering Scholarly Orphans Using ORCID,"  Archival efforts such as (C)LOCKSS and Portico are in place to ensure the
longevity of traditional scholarly resources like journal articles. At the same
time, researchers are depositing a broad variety of other scholarly artifacts
into emerging online portals that are designed to support web-based
scholarship. These web-native scholarly objects are largely neglected by
current archival practices and hence they become scholarly orphans. We
therefore argue for a novel paradigm that is tailored towards archiving these
scholarly orphans. We are investigating the feasibility of using Open
Researcher and Contributor ID (ORCID) as a supporting infrastructure for the
process of discovery of web identities and scholarly orphans for active
researchers. We analyze ORCID in terms of coverage of researchers, subjects,
and location and assess the richness of its profiles in terms of web identities
and scholarly artifacts. We find that ORCID currently lacks in all considered
aspects and hence can only be considered in conjunction with other discovery
sources. However, ORCID is growing fast so there is potential that it could
achieve a satisfactory level of coverage and richness in the near future.
"
1805,"Important New Developments in Arabographic Optical Character Recognition
  (OCR)","  The OpenITI team has achieved Optical Character Recognition (OCR) accuracy
rates for classical Arabic-script texts in the high nineties. These numbers are
based on our tests of seven different Arabic-script texts of varying quality
and typefaces, totaling over 7,000 lines. These accuracy rates not only
represent a distinct improvement over the actual accuracy rates of the various
proprietary OCR options for classical Arabic-script texts, but, equally
important, they are produced using an open-source OCR software, thus enabling
us to make this Arabic-script OCR technology freely available to the broader
Islamic, Persian, and Arabic Studies communities.
"
1806,"A scientists' view of scientometrics: Not everything that counts can be
  counted","  Like it or not, attempts to evaluate and monitor the quality of academic
research have become increasingly prevalent worldwide. Performance reviews
range from at the level of individuals, through research groups and
departments, to entire universities. Many of these are informed by, or
functions of, simple scientometric indicators and the results of such exercises
impact onto careers, funding and prestige. However, there is sometimes a
failure to appreciate that scientometrics are, at best, very blunt instruments
and their incorrect usage can be misleading. Rather than accepting the rise and
fall of individuals and institutions on the basis of such imprecise measures,
calls have been made for indicators be regularly scrutinised and for
improvements to the evidence base in this area. It is thus incumbent upon the
scientific community, especially the physics, complexity-science and
scientometrics communities, to scrutinise metric indicators. Here, we review
recent attempts to do this and show that some metrics in widespread use cannot
be used as reliable indicators research quality.
"
1807,"Knowledge Evolution in Physics Research: An Analysis of Bibliographic
  Coupling Networks","  Even as we advance the frontiers of physics knowledge, our understanding of
how this knowledge evolves remains at the descriptive levels of Popper and
Kuhn. Using the APS publications data sets, we ask in this letter how new
knowledge is built upon old knowledge. We do so by constructing year-to-year
bibliographic coupling networks, and identify in them validated communities
that represent different research fields. We then visualize their evolutionary
relationships in the form of alluvial diagrams, and show how they remain intact
through APS journal splits. Quantitatively, we see that most fields undergo
weak Popperian mixing, and it is rare for a field to remain isolated/undergo
strong mixing. The sizes of fields obey a simple linear growth with
recombination. We can also reliably predict the merging between two fields, but
not for the considerably more complex splitting. Finally, we report a case
study of two fields that underwent repeated merging and splitting around 1995,
and how these Kuhnian events are correlated with breakthroughs on BEC, quantum
teleportation, and slow light. This impact showed up quantitatively in the
citations of the BEC field as a larger proportion of references from during and
shortly after these events.
"
1808,"Normalization of zero-inflated data: An empirical analysis of a new
  indicator family","  Recently, two new indicators (Equalized Mean-based Normalized Proportion
Cited, EMNPC, and Mean-based Normalized Proportion Cited, MNPC) were proposed
which are intended for sparse data. We propose a third indicator
(Mantel-Haenszel quotient, MHq) belonging to the same indicator family. The MHq
is based on the MH analysis - an established method for polling the data from
multiple 2x2 contingency tables based on different subgroups. We test (using
citations and assessments by peers) if the three indicators can distinguish
between different quality levels as defined on the basis of the assessments by
peers (convergent validity). We find that the indicator MHq is able to
distinguish between the quality levels in most cases while MNPC and EMNPC are
not.
"
1809,"What Constitutes Peer Review of Data: A survey of published peer review
  guidelines","  Since a number of journals specifically focus on the review and publication
of data sets, reviewing their policies seems an appropriate place to start in
assessing what existing practice looks like in the 'real world' of reviewing
and publishing data. This article outlines a study of the publicly available
peer review policies of 39 scientific publications that publish data papers to
discern which criteria are most and least frequently referenced. It also
compares current practice with proposed criteria published in 2012.
"
1810,"The scientific influence of nations on global scientific and
  technological development","  Determining how scientific achievements influence the subsequent process of
knowledge creation is a fundamental step in order to build a unified ecosystem
for studying the dynamics of innovation and competitiveness. Relying separately
on data about scientific production on one side, through bibliometric
indicators, and about technological advancements on the other side, through
patents statistics, gives only a limited insight on the key interplay between
science and technology which, as a matter of fact, move forward together within
the innovation space. In this paper, using citation data of both research
papers and patents, we quantify the direct influence of the scientific outputs
of nations on further advancements in science and on the introduction of new
technologies. Our analysis highlights the presence of geo-cultural clusters of
nations with similar innovation system features, and unveils the heterogeneous
coupled dynamics of scientific and technological advancements. This study
represents a step forward in the buildup of an inclusive framework for
knowledge creation and innovation.
"
1811,Lago Distributed Network Of Data Repositories,"  We describe a set of tools, services and strategies of the Latin American
Giant Observatory (LAGO) data repository network, to implement Data
Accessibility, Reproducibility and Trustworthiness.
"
1812,The Time Dimension of Science: Connecting the Past to the Future,"  A central question in science of science concerns how time affects citations.
Despite the long-standing interests and its broad impact, we lack systematic
answers to this simple yet fundamental question. By reviewing and classifying
prior studies for the past 50 years, we find a significant lack of consensus in
the literature, primarily due to the coexistence of retrospective and
prospective approaches to measuring citation age distributions. These two
approaches have been pursued in parallel, lacking any known connections between
the two. Here we developed a new theoretical framework that not only allows us
to connect the two approaches through precise mathematical relationships, it
also helps us reconcile the interplay between temporal decay of citations and
the growth of science, helping us uncover new functional forms characterizing
citation age distributions. We find retrospective distribution follows a
lognormal distribution with exponential cutoff, while prospective distribution
is governed by the interplay between a lognormal distribution and the growth in
the number of references. Most interestingly, the two approaches can be
connected once rescaled by the growth of publications and citations. We further
validate our framework using both large-scale citation datasets and analytical
models capturing citation dynamics. Together this paper presents a
comprehensive analysis of the time dimension of science, representing a new
empirical and theoretical basis for all future studies in this area.
"
1813,"Designing a Web-based interactive audio library automation system for
  visually-impaired people and evaluation of its usability","  The aim of this study is to introduce an application that enables information
sharing and communication between visually-impaired individuals and
able-bodied. For the purposes of the study, web-based audio library automation
was designed and the usability of the system was analyzed regarding the
volunteers who record audio books and the visually-impaired individuals. The
visually-impaired individuals who took part in the test procedures in order to
make a general evaluation of the system reported that the system was
theoretically necessary and successful. As for the usability aspect, positive
comments were received regarding the automation system developed. The authors
believe that the current study is likely to be an alternative reference source
for the related literature and further research studies to be conducted in the
field.
"
1814,"A Century of Science: Globalization of Scientific Collaborations,
  Citations, and Innovations","  Progress in science has advanced the development of human society across
history, with dramatic revolutions shaped by information theory, genetic
cloning, and artificial intelligence, among the many scientific achievements
produced in the 20th century. However, the way that science advances itself is
much less well-understood. In this work, we study the evolution of scientific
development over the past century by presenting an anatomy of 89 million
digitalized papers published between 1900 and 2015. We find that science has
benefited from the shift from individual work to collaborative effort, with
over 90% of the world-leading innovations generated by collaborations in this
century, nearly four times higher than they were in the 1900s. We discover that
rather than the frequent myopic- and self-referencing that was common in the
early 20th century, modern scientists instead tend to look for literature
further back and farther around. Finally, we also observe the globalization of
scientific development from 1900 to 2015, including 25-fold and 7-fold
increases in international collaborations and citations, respectively, as well
as a dramatic decline in the dominant accumulation of citations by the US, the
UK, and Germany, from ~95% to ~50% over the same period. Our discoveries are
meant to serve as a starter for exploring the visionary ways in which science
has developed throughout the past century, generating insight into and an
impact upon the current scientific innovations and funding policies.
"
1815,HEPData: a repository for high energy physics data,"  The Durham High Energy Physics Database (HEPData) has been built up over the
past four decades as a unique open-access repository for scattering data from
experimental particle physics papers. It comprises data points underlying
several thousand publications. Over the last two years, the HEPData software
has been completely rewritten using modern computing technologies as an overlay
on the Invenio v3 digital library framework. The software is open source with
the new site available at https://hepdata.net now replacing the previous site
at http://hepdata.cedar.ac.uk. In this write-up, we describe the development of
the new site and explain some of the advantages it offers over the previous
platform.
"
1816,Ranking in evolving complex networks,"  Complex networks have emerged as a simple yet powerful framework to represent
and analyze a wide range of complex systems. The problem of ranking the nodes
and the edges in complex networks is critical for a broad range of real-world
problems because it affects how we access online information and products, how
success and talent are evaluated in human activities, and how scarce resources
are allocated by companies and policymakers, among others. This calls for a
deep understanding of how existing ranking algorithms perform, and which are
their possible biases that may impair their effectiveness. Well-established
ranking algorithms (such as the popular Google's PageRank) are static in nature
and, as a consequence, they exhibit important shortcomings when applied to real
networks that rapidly evolve in time. The recent advances in the understanding
and modeling of evolving networks have enabled the development of a wide and
diverse range of ranking algorithms that take the temporal dimension into
account. The aim of this review is to survey the existing ranking algorithms,
both static and time-aware, and their applications to evolving networks. We
emphasize both the impact of network evolution on well-established static
algorithms and the benefits from including the temporal dimension for tasks
such as prediction of real network traffic, prediction of future links, and
identification of highly-significant nodes.
"
1817,"Stochastic Block Model Reveals the Map of Citation Patterns and Their
  Evolution in Time","  In this study we map out the large-scale structure of citation networks of
science journals and follow their evolution in time by using stochastic block
models (SBMs). The SBM fitting procedures are principled methods that can be
used to find hierarchical grouping of journals into blocks that show similar
incoming and outgoing citations patterns. These methods work directly on the
citation network without the need to construct auxiliary networks based on
similarity of nodes. We fit the SBMs to the networks of journals we have
constructed from the data set of around 630 million citations and find a
variety of different types of blocks, such as clusters, bridges, sources, and
sinks. In addition we use a recent generalization of SBMs to determine how much
a manually curated classification of journals into subfields of science is
related to the block structure of the journal network and how this relationship
changes in time. The SBM method tries to find a network of blocks that is the
best high-level representation of the network of journals, and we illustrate
how these block networks (at various levels of resolution) can be used as maps
of science.
"
1818,"Testing the science/technology relationship by analysis of patent
  citations of scientific papers after decomposition of both science and
  technology","  The relationship of scientific knowledge development to technological
development is widely recognized as one of the most important and complex
aspects of technological evolution. This paper adds to our understanding of the
relationship through use of a more rigorous structure for differentiating among
technologies based upon technological domains (defined as consisting of the
artifacts over time that fulfill a specific generic function using a specific
body of technical knowledge).
"
1819,Search for Evergreens in Science: A Functional Data Analysis,"  Evergreens in science are papers that display a continual rise in annual
citations without decline, at least within a sufficiently long time period.
Aiming to better understand evergreens in particular and patterns of citation
trajectory in general, this paper develops a functional data analysis method to
cluster citation trajectories of a sample of 1699 research papers published in
1980 in the American Physical Society (APS) journals. We propose a functional
Poisson regression model for individual papers' citation trajectories, and fit
the model to the observed 30-year citations of individual papers by functional
principal component analysis and maximum likelihood estimation. Based on the
estimated paper-specific coefficients, we apply the K-means clustering
algorithm to cluster papers into different groups, for uncovering general types
of citation trajectories. The result demonstrates the existence of an evergreen
cluster of papers that do not exhibit any decline in annual citations over 30
years.
"
1820,Towards effective research recommender systems for repositories,"  In this paper, we argue why and how the integration of recommender systems
for research can enhance the functionality and user experience in repositories.
We present the latest technical innovations in the CORE Recommender, which
provides research article recommendations across the global network of
repositories and journals. The CORE Recommender has been recently redeveloped
and released into production in the CORE system and has also been deployed in
several third-party repositories. We explain the design choices of this unique
system and the evaluation processes we have in place to continue raising the
quality of the provided recommendations. By drawing on our experience, we
discuss the main challenges in offering a state-of-the-art recommender solution
for repositories. We highlight two of the key limitations of the current
repository infrastructure with respect to developing research recommender
systems: 1) the lack of a standardised protocol and capabilities for exposing
anonymised user-interaction logs, which represent critically important input
data for recommender systems based on collaborative filtering and 2) the lack
of a voluntary global sign-on capability in repositories, which would enable
the creation of personalised recommendation and notification solutions based on
past user interactions.
"
1821,"Influence of Reviewer Interaction Network on Long-term Citations: A Case
  Study of the Scientific Peer-Review System of the Journal of High Energy
  Physics","  A `peer-review system' in the context of judging research contributions, is
one of the prime steps undertaken to ensure the quality of the submissions
received, a significant portion of the publishing budget is spent towards
successful completion of the peer-review by the publication houses.
Nevertheless, the scientific community is largely reaching a consensus that
peer-review system, although indispensable, is nonetheless flawed. A very
pertinent question therefore is ""could this system be improved?"". In this
paper, we attempt to present an answer to this question by considering a
massive dataset of around $29k$ papers with roughly $70k$ distinct review
reports together consisting of $12m$ lines of review text from the Journal of
High Energy Physics (JHEP) between 1997 and 2015. In specific, we introduce a
novel \textit{reviewer-reviewer interaction network} (an edge exists between
two reviewers if they were assigned by the same editor) and show that
surprisingly the simple structural properties of this network such as degree,
clustering coefficient, centrality (closeness, betweenness etc.) serve as
strong predictors of the long-term citations (i.e., the overall scientific
impact) of a submitted paper. These features, when plugged in a regression
model, alone achieves a high $R^2$ of \0.79 and a low $RMSE$ of 0.496 in
predicting the long-term citations. In addition, we also design a set of
supporting features built from the basic characteristics of the submitted
papers, the authors and the referees (e.g., the popularity of the submitting
author, the acceptance rate history of a referee, the linguistic properties
laden in the text of the review reports etc.), which further results in overall
improvement with $R^2$ of 0.81 and $RMSE$ of 0.46.
"
1822,"Improving fitness: Mapping research priorities against societal needs on
  obesity","  Science policy is increasingly shifting towards an emphasis in societal
problems or grand challenges. As a result, new evaluative tools are needed to
help assess not only the knowledge production side of research programmes or
organisations, but also the articulation of research agendas with societal
needs. In this paper, we present an exploratory investigation of science supply
and societal needs on the grand challenge of obesity -an emerging health
problem with enormous social costs. We illustrate a potential approach that
uses topic modelling to explore: (a) how scientific publications can be used to
describe existing priorities in science production; (b) how records of
questions posed in the European parliament can be used as an instance of
mapping discourse of social needs; (c) how the comparison between the two may
show (mis)alignments between societal concerns and scientific outputs. While
this is a technical exercise, we propose that this type of mapping methods can
be useful for informing strategic planning and evaluation in funding agencies.
"
1823,"Quantitative and Qualitative Analysis of Editor Behavior through
  Potentially Coercive Citations","  How much is the h-index of an editor of a well ranked journal improved due to
citations which occur after his or her appointment? Scientific recognition
within academia is widely measured nowadays by the number of citations or
h-index. Our dataset is based on a sample of four editors from a well ranked
journal (impact factor - IF - greater than 2). The target group consists of two
editors who seem to benefit by their position through an increased citation
number (and subsequently h-index) within journal. The total amount of citations
for the target group is bigger than 600. The control group is formed by another
set of two editors from the same journal whose relations between their
positions and their citation records remain neutral. The total amount of
citations for the control group is more than 1200. The timespan for which
pattern of citations has been studied is 1975-2015. Previous coercive citations
for a journal benefit (increase its IF) has been signaled. To the best of our
knowledge, this is a pioneering work on coercive citations for personal (or
editors) benefit. Editorial teams should be aware about this type of
potentially unethical behavior and act accordingly.
"
1824,A measure of authorship by publications,"  Measuring publication success of a researcher is a complicated task as
publications are often co-authored by multiple authors, and so, require
comparison of solo publications with joint publications. In this paper, like
\cite{price1981multiple}, we argue for an egalitarian perspective in
accomplishing this task.
  More specifically, we justify the need for an ethical perspective in
quantifying academic author by identifying certain ethical difficulties of some
popular contemporary indices used for this purpose. And then we show that for
any given dataset of research papers, the unique method satisfying the ethical
notions of {\it identity independence} and performance invariance must be the
egaliatarian E-index proposed by \cite{bps} and \cite{price1981multiple}. In
our setting, this egalitarian method divides authorship of joint projects
equally among authors and sums across all publications of each author.
"
1825,"Analysis of Computational Science Papers from ICCS 2001-2016 using Topic
  Modeling and Graph Theory","  This paper presents results of topic modeling and network models of topics
using the International Conference on Computational Science corpus, which
contains domain-specific (computational science) papers over sixteen years (a
total of 5695 papers). We discuss topical structures of International
Conference on Computational Science, how these topics evolve over time in
response to the topicality of various problems, technologies and methods, and
how all these topics relate to one another. This analysis illustrates
multidisciplinary research and collaborations among scientific communities, by
constructing static and dynamic networks from the topic modeling results and
the keywords of authors. The results of this study give insights about the past
and future trends of core discussion topics in computational science. We used
the Non-negative Matrix Factorization topic modeling algorithm to discover
topics and labeled and grouped results hierarchically.
"
1826,"Citation sentence reuse behavior of scientists: A case study on massive
  bibliographic text dataset of computer science","  Our current knowledge of scholarly plagiarism is largely based on the
similarity between full text research articles. In this paper, we propose an
innovative and novel conceptualization of scholarly plagiarism in the form of
reuse of explicit citation sentences in scientific research articles. Note that
while full-text plagiarism is an indicator of a gross-level behavior, copying
of citation sentences is a more nuanced micro-scale phenomenon observed even
for well-known researchers. The current work poses several interesting
questions and attempts to answer them by empirically investigating a large
bibliographic text dataset from computer science containing millions of lines
of citation sentences. In particular, we report evidences of massive copying
behavior. We also present several striking real examples throughout the paper
to showcase widespread adoption of this undesirable practice. In contrast to
the popular perception, we find that copying tendency increases as an author
matures. The copying behavior is reported to exist in all fields of computer
science; however, the theoretical fields indicate more copying than the applied
fields.
"
1827,Understanding the Impact of Early Citers on Long-Term Scientific Impact,"  This paper explores an interesting new dimension to the challenging problem
of predicting long-term scientific impact (LTSI) usually measured by the number
of citations accumulated by a paper in the long-term. It is well known that
early citations (within 1-2 years after publication) acquired by a paper
positively affects its LTSI. However, there is no work that investigates if the
set of authors who bring in these early citations to a paper also affect its
LTSI. In this paper, we demonstrate for the first time, the impact of these
authors whom we call early citers (EC) on the LTSI of a paper. Note that this
study of the complex dynamics of EC introduces a brand new paradigm in citation
behavior analysis. Using a massive computer science bibliographic dataset we
identify two distinct categories of EC - we call those authors who have high
overall publication/citation count in the dataset as influential and the rest
of the authors as non-influential. We investigate three characteristic
properties of EC and present an extensive analysis of how each category
correlates with LTSI in terms of these properties. In contrast to popular
perception, we find that influential EC negatively affects LTSI possibly owing
to attention stealing. To motivate this, we present several representative
examples from the dataset. A closer inspection of the collaboration network
reveals that this stealing effect is more profound if an EC is nearer to the
authors of the paper being investigated. As an intuitive use case, we show that
incorporating EC properties in the state-of-the-art supervised citation
prediction models leads to high performance margins. At the closing, we present
an online portal to visualize EC statistics along with the prediction results
for a given query paper.
"
1828,"Measuring Social Media Activity of Scientific Literature: An Exhaustive
  Comparison of Scopus and Novel Altmetrics Big Data","  This paper measures social media activity of 15 broad scientific disciplines
indexed in Scopus database using Altmetric.com data. First, the presence of
Altmetric.com data in Scopus database is investigated, overall and across
disciplines. Second, the correlation between the bibliometric and altmetric
indices is examined using Spearman correlation. Third, a zero-truncated
negative binomial model is used to determine the association of various factors
with increasing or decreasing citations. Lastly, the effectiveness of altmetric
indices to identify publications with high citation impact is comprehensively
evaluated by deploying Area Under the Curve (AUC) - an application of receiver
operating characteristic. Results indicate a rapid increase in the presence of
Altmetric.com data in Scopus database from 10.19% in 2011 to 20.46% in 2015. A
zero-truncated negative binomial model is implemented to measure the extent to
which different bibliometric and altmetric factors contribute to citation
counts. Blog count appears to be the most important factor increasing the
number of citations by 38.6% in the field of Health Professions and Nursing,
followed by Twitter count increasing the number of citations by 8% in the field
of Physics and Astronomy. Interestingly, both Blog count and Twitter count
always show positive increase in the number of citations across all fields.
While there was a positive weak correlation between bibliometric and altmetric
indices, the results show that altmetric indices can be a good indicator to
discriminate highly cited publications, with an encouragingly AUC= 0.725
between highly cited publications and total altmetric count. Overall, findings
suggest that altmetrics could better distinguish highly cited publications.
"
1829,"Betweenness and Diversity in Journal Citation Networks as Measures of
  Interdisciplinarity -- A Tribute to Eugene Garfield --","  Journals were central to Eugene Garfield's research interests. Among other
things, journals are considered as units of analysis for bibliographic
databases such as the Web of Science (WoS) and Scopus. In addition to
disciplinary classifications of journals, journal citation patterns span
networks across boundaries to variable extents. Using betweenness centrality
(BC) and diversity, we elaborate on the question of how to distinguish and rank
journals in terms of interdisciplinarity. Interdisciplinarity, however, is
difficult to operationalize in the absence of an operational definition of
disciplines, the diversity of a unit of analysis is sample-dependent. BC can be
considered as a measure of multi-disciplinarity. Diversity of co-citation in a
citing document has been considered as an indicator of knowledge integration,
but an author can also generate trans-disciplinary--that is,
non-disciplined--variation by citing sources from other disciplines. Diversity
in the bibliographic coupling among citing documents can analogously be
considered as diffusion of knowledge across disciplines. Because the citation
networks in the cited direction reflect both structure and variation, diversity
in this direction is perhaps the best available measure of interdisciplinarity
at the journal level. Furthermore, diversity is based on a summation and can
therefore be decomposed, differences among (sub)sets can be tested for
statistical significance. In an appendix, a general-purpose routine for
measuring diversity in networks is provided.
"
1830,Do ResearchGate Scores create ghost academic reputations?,"  The academic social network site ResearchGate (RG) has its own indicator, RG
Score, for its members. The high profile nature of the site means that the RG
score may be used for recruitment, promotion and other tasks for which
researchers are evaluated. In response, this study investigates whether it is
reasonable to employ the RG Score as evidence of scholarly reputation. For
this, three different author samples were investigated. An outlier sample
includes 104 authors with high values. A Nobel sample comprises 73 Nobel
winners from Medicine & Physiology, Chemistry, Physics and Economics (from 1975
to 2015). A longitudinal sample includes weekly data on 4 authors with
different RG Scores. The results suggest that high RG Scores are built
primarily from activity related to asking and answering questions in the site.
In particular, it seems impossible to get a high RG Score solely through
publications. Within RG it is possible to distinguish between (passive)
academics that interact little in the site and active platform users, who can
get high RG Scores through engaging with others inside the site (questions,
answers, social networks with influential researchers). Thus, RG Scores should
not be mistaken for academic reputation indicators.
"
1831,"Is there agreement on the prestige of scholarly book publishers in the
  Humanities? DELPHI over survey results","  Despite having an important role supporting assessment processes, criticism
towards evaluation systems and the categorizations used are frequent.
Considering the acceptance by the scientific community as an essential issue
for using rankings or categorizations in research evaluation, the aim of this
paper is testing the results of rankings of scholarly book publishers'
prestige, Scholarly Publishers Indicators (SPI hereafter). SPI is a public,
survey-based ranking of scholarly publishers' prestige (among other
indicators). The latest version of the ranking (2014) was based on an expert
consultation with a large number of respondents. In order to validate and
refine the results for Humanities' fields as proposed by the assessment
agencies, a Delphi technique was applied with a panel of randomly selected
experts over the initial rankings. The results show an equalizing effect of the
technique over the initial rankings as well as a high degree of concordance
between its theoretical aim (consensus among experts) and its empirical results
(summarized with Gini Index). The resulting categorization is understood as
more conclusive and susceptible of being accepted by those under evaluation.
"
1832,"Using Titles vs. Full-text as Source for Automated Semantic Document
  Annotation","  A significant part of the largest Knowledge Graph today, the Linked Open Data
cloud, consists of metadata about documents such as publications, news reports,
and other media articles. While the widespread access to the document metadata
is a tremendous advancement, it is yet not so easy to assign semantic
annotations and organize the documents along semantic concepts. Providing
semantic annotations like concepts in SKOS thesauri is a classical research
topic, but typically it is conducted on the full-text of the documents. For the
first time, we offer a systematic comparison of classification approaches to
investigate how far semantic annotations can be conducted using just the
metadata of the documents such as titles published as labels on the Linked Open
Data cloud. We compare the classifications obtained from analyzing the
documents' titles with semantic annotations obtained from analyzing the
full-text. Apart from the prominent text classification baselines kNN and SVM,
we also compare recent techniques of Learning to Rank and neural networks and
revisit the traditional methods logistic regression, Rocchio, and Naive Bayes.
The results show that across three of our four datasets, the performance of the
classifications using only titles reaches over 90% of the quality compared to
the classification performance when using the full-text. Thus, conducting
document classification by just using the titles is a reasonable approach for
automated semantic annotation and opens up new possibilities for enriching
Knowledge Graphs.
"
1833,"Knowledge discovery through text-based similarity searches for astronomy
  literature","  The increase in the number of researchers coupled with the ease of publishing
and distribution of scientific papers (due to technological advancements) has
resulted in a dramatic increase in astronomy literature. This has likely led to
the predicament that the body of the literature is too large for traditional
human consumption and that related and crucial knowledge is not discovered by
researchers. In addition to the increased production of astronomical
literature, recent decades have also brought several advancements in
computational linguistics. Especially, the machine-aided processing of
literature dissemination might make it possible to convert this stream of
papers into a coherent knowledge set. In this paper, we present the application
of computational linguistics techniques to astronomy literature. In particular,
we developed a tool that will find similar articles purely based on text
content from an input paper. We find that our technique performs robustly in
comparison with other tools recommending articles given a reference paper
(known as recommender system). Our novel tool shows the great power in
combining computational linguistics with astronomy literature and suggests that
additional research in this endeavor will likely produce even better tools that
will help researchers cope with the vast amounts of knowledge being produced.
"
1834,Applied Evaluative Informetrics: Part 1,"  This manuscript is a preprint version of Part 1 (General Introduction and
Synopsis) of the book Applied Evaluative Informetrics, to be published by
Springer in the summer of 2017. This book presents an introduction to the field
of applied evaluative informetrics, and is written for interested scholars and
students from all domains of science and scholarship. It sketches the field's
history, recent achievements, and its potential and limits. It explains the
notion of multi-dimensional research performance, and discusses the pros and
cons of 28 citation-, patent-, reputation- and altmetrics-based indicators. In
addition, it presents quantitative research assessment as an evaluation
science, and focuses on the role of extra-informetric factors in the
development of indicators, and on the policy context of their application. It
also discusses the way forward, both for users and for developers of
informetric tools.
"
1835,Stories From the Past Web,"  Archiving Web pages into themed collections is a method for ensuring these
resources are available for posterity. Services such as Archive-It exists to
allow institutions to develop, curate, and preserve collections of Web
resources. Understanding the contents and boundaries of these archived
collections is a challenge for most people, resulting in the paradox of the
larger the collection, the harder it is to understand. Meanwhile, as the sheer
volume of data grows on the Web, ""storytelling"" is becoming a popular technique
in social media for selecting Web resources to support a particular narrative
or ""story"". There are multiple stories that can be generated from an archived
collection with different perspectives about the collection. For example, a
user may want to see a story that is composed of the key events from a specific
Web site, a story that is composed of the key events of the story regardless of
the sources, or how a specific event at a specific point in time was covered by
different Web sites, etc. In this paper, we provide different case studies for
possible types of stories that can be extracted from a collection. We also
provide the definitions and models of these types of stories.
"
1836,"Entropic selection of concepts unveils hidden topics in documents
  corpora","  The organization and evolution of science has recently become itself an
object of scientific quantitative investigation, thanks to the wealth of
information that can be extracted from scientific documents, such as citations
between papers and co-authorship between researchers. However, only few studies
have focused on the concepts that characterize full documents and that can be
extracted and analyzed, revealing the deeper organization of scientific
knowledge. Unfortunately, several concepts can be so common across documents
that they hinder the emergence of the underlying topical structure of the
document corpus, because they give rise to a large amount of spurious and
trivial relations among documents. To identify and remove common concepts, we
introduce a method to gauge their relevance according to an objective
information-theoretic measure related to the statistics of their occurrence
across the document corpus. After progressively removing concepts that,
according to this metric, can be considered as generic, we find that the topic
organization displays a correspondingly more refined structure.
"
1837,Nucleus: A Pilot Project,"  Early in 2016, an environmental scan was conducted by the Research Library
Data Working Group for three purposes:
  1.) Perform a survey of the data management landscape at Los Alamos National
Laboratory in order to identify local gaps in data management services.
  2.) Conduct an environmental scan of external institutions to benchmark
budgets, infrastructure, and personnel dedicated to data management.
  3.) Draft a research data infrastructure model that aligns with the current
workflow and classification restrictions at Los Alamos National Laboratory.
  This report is a summary of those activities and the draft for a pilot data
management project.
"
1838,Reference String Extraction Using Line-Based Conditional Random Fields,"  The extraction of individual reference strings from the reference section of
scientific publications is an important step in the citation extraction
pipeline. Current approaches divide this task into two steps by first detecting
the reference section areas and then grouping the text lines in such areas into
reference strings. We propose a classification model that considers every line
in a publication as a potential part of a reference string. By applying
line-based conditional random fields rather than constructing the graphical
model based on the individual words, dependencies and patterns that are typical
in reference sections provide strong features while the overall complexity of
the model is reduced.
"
1839,"Calidad en repositorios digitales en Argentina, estudio comparativo y
  cualitativo","  Numerous institutions and organizations need not only to preserve the
material and publications they produce, but also have as their task (although
it would be desirable it was an obligation) to publish, disseminate and make
publicly available all the results of the research and any other
scientific/academic material. The Open Archives Initiative (OAI) and the
introduction of Open Archives Initiative Protocol for Metadata Harvesting
(OAI-PMH), make this task much easier. The main objective of this work is to
make a comparative and qualitative study of the data -metadata specifically-
contained in the whole set of Argentine repositories listed in the ROAR portal,
focusing on the functional perspective of the quality of this metadata. Another
objective is to offer an overview of the status of these repositories, in an
attempt to detect common failures and errors institutions incur when storing
the metadata of the resources contained in these repositories, and thus be able
to suggest measures to be able to improve the load and further retrieval
processes. It was found that the eight most used Dublin Core fields are:
identifier, type, title, date, subject, creator, language and description. Not
all repositories fill all the fields, and the lack of normalization, or the
excessive use of fields like language, type, format and subject is somewhat
striking, and in some cases even alarming
"
1840,On Patterns and Re-Use in Bioinformatics Databases,"  As the quantity of data being depositing into biological databases continues
to increase, it becomes ever more vital to develop methods that enable us to
understand this data and ensure that the knowledge is correct. It is
widely-held that data percolates between different databases, which causes
particular concerns for data correctness; if this percolation occurs, incorrect
data in one database may eventually affect many others while, conversely,
corrections in one database may fail to percolate to others.
  In this paper, we test this widely-held belief by directly looking for
sentence reuse both within and between databases. Further, we investigate
patterns of how sentences are reused over time. Finally, we consider the
limitations of this form of analysis and the implications that this may have
for bioinformatics database design.
  We show that reuse of annotation is common within many different databases,
and that also there is a detectable level of reuse between databases. In
addition, we show that there are patterns of reuse that have previously been
shown to be associated with percolation errors.
"
1841,"Analysing Timelines of National Histories across Wikipedia Editions: A
  Comparative Computational Approach","  Portrayals of history are never complete, and each description inherently
exhibits a specific viewpoint and emphasis. In this paper, we aim to
automatically identify such differences by computing timelines and detecting
temporal focal points of written history across languages on Wikipedia. In
particular, we study articles related to the history of all UN member states
and compare them in 30 language editions. We develop a computational approach
that allows to identify focal points quantitatively, and find that Wikipedia
narratives about national histories (i) are skewed towards more recent events
(recency bias) and (ii) are distributed unevenly across the continents with
significant focus on the history of European countries (Eurocentric bias). We
also establish that national historical timelines vary across language
editions, although average interlingual consensus is rather high. We hope that
this paper provides a starting point for a broader computational analysis of
written history on Wikipedia and elsewhere.
"
1842,"On the ""persistency"" of scientific publications: introducing an h-index
  for journals","  What do we really mean by a ""good"" scientific journal? Do we care more about
the short-time impact of our papers, or about the chance that they will still
be read and cited on the long run? Here I show that, by regarding a journal as
a ""virtual scientist"" that can be attributed a time-dependent Hirsch h-index,
we can introduce a parameter that, arguably, better captures the ""persistency""
of a scientific publication. Curiously, however, this parameter seems to depend
above all on the ""thickness"" of a journal.
"
1843,"Joint Modeling of Topics, Citations, and Topical Authority in Academic
  Corpora","  Much of scientific progress stems from previously published findings, but
searching through the vast sea of scientific publications is difficult. We
often rely on metrics of scholarly authority to find the prominent authors but
these authority indices do not differentiate authority based on research
topics. We present Latent Topical-Authority Indexing (LTAI) for jointly
modeling the topics, citations, and topical authority in a corpus of academic
papers. Compared to previous models, LTAI differs in two main aspects. First,
it explicitly models the generative process of the citations, rather than
treating the citations as given. Second, it models each author's influence on
citations of a paper based on the topics of the cited papers, as well as the
citing papers. We fit LTAI to four academic corpora: CORA, Arxiv Physics, PNAS,
and Citeseer. We compare the performance of LTAI against various baselines,
starting with the latent Dirichlet allocation, to the more advanced models
including author-link topic model and dynamic author citation topic model. The
results show that LTAI achieves improved accuracy over other similar models
when predicting words, citations and authors of publications.
"
1844,"A Complete Year of User Retrieval Sessions in a Social Sciences Academic
  Search Engine","  In this paper, we present an open data set extracted from the transaction log
of the social sciences academic search engine sowiport. The data set includes a
filtered set of 484,449 retrieval sessions which have been carried out by
sowiport users in the period from April 2014 to April 2015. We propose a
description of interactions performed by the academic search engine users that
can be used in different applications such as result ranking improvement, user
modeling, query reformulation analysis, search pattern recognition.
"
1845,"One look at the rating of scientific publications and corresponding
  toy-model","  A toy-model of publications and citations processes is proposed. The model
shows that the role of randomness in the processes is essential and cannot be
ignored. Some other aspects of scientific publications rating are discussed.
"
1846,Publication Trends in Physics Education: A Bibliometric study,"  A publication trend in Physics Education by employing bibliometric analysis
leads the researchers to describe current scientific movement. This paper tries
to answer ""What do Physics education scientists concentrate in their
publications?"" by analyzing the productivity and development of publications on
the subject category of Physics Education in the period 1980--2013. The Web of
Science databases in the research areas of ""EDUCATION - EDUCATIONAL RESEARCH""
was used to extract the publication trends. The study involves 1360
publications, including 840 articles, 503 proceedings paper, 22 reviews, 7
editorial material, 6 Book review, and one Biographical item. Number of
publications with ""Physical Education"" in topic increased from 0.14 % (n = 2)
in 1980 to 16.54 % (n = 225) in 2011. Total number of receiving citations is
8071, with approximately citations per papers of 5.93. The results show the
publication and citations in Physic Education has increased dramatically while
the Malaysian share is well ranked.
"
1847,"A two-dimensional index to quantify both scientific research impact and
  scope","  Modern management of research is increasingly based on quantitative
bibliometric indices. Nowadays, the h-index is a major measure of research
output that has supplanted all other citation-based indices. In this context,
indicators that complement the h-index by evaluating different facets of
research achievement are compelling. As an additional bibliometric source that
can be easily extracted from available databases, we propose to use the number
of distinct journals Nj in which an individual's papers were published. We show
that Nj is independent of citation counts, and argue that it is a relevant
indicator of research scope, since it quantifies readership extent and
scientific multidisciplinarity. Combining the h-index and Nj, we define a
two-dimensional index (H,M) that measures both the output (through H) and the
outreach (through M) of individual's research. In order to probe the relevance
of this two dimensional index, we have analysed the scientific production of a
panel of physicists belonging to the same Department but with different
research themes. The analysis of bibliometric data confirms that the two
indices are uncorrelated and shows that while H reliably ranks the impact of
researchers, M accurately sorts multidisciplinary or readership aspects. We
conclude that the two indices together offer a more complete picture of
research performance and can be applied to either individuals, research groups
or institutions.
"
1848,"Analyzing the disciplinary focus of universities: Can rankings be a
  one-size-fits-all?","  The phenomenon of rankings is intimately related with the government interest
in fiscalizing the research outputs of universities. New forms of managerialism
have been introduced into the higher education system, leading to an increasing
interest from funding bodies in developing external evaluation tools to
allocate funds. Rankings rely heavily on bibliometric indicators. But
bibliometricians have been very critical with their use. Among other, they have
pointed out the over-simplistic view rankings represent when analyzing the
research output of universities, as they consider them as homogeneous ignoring
disciplinary differences. Although many university rankings now include league
tables by fields, reducing the complex framework of universities' research
activity to a single dimension leads to poor judgment and decision making. This
is partly because of the influence disciplinary specialization has on research
evaluation. This chapter analyzes from a methodological perspective how
rankings suppress disciplinary differences which are key factors to interpret
correctly these rankings.
"
1849,Usage Bibliometrics as a Tool to Measure Research Activity,"  Measures for research activity and impact have become an integral ingredient
in the assessment of a wide range of entities (individual researchers,
organizations, instruments, regions, disciplines). Traditional bibliometric
indicators, like publication and citation based indicators, provide an
essential part of this picture, but cannot describe the complete picture. Since
reading scholarly publications is an essential part of the research life cycle,
it is only natural to introduce measures for this activity in attempts to
quantify the efficiency, productivity and impact of an entity. Citations and
reads are significantly different signals, so taken together, they provide a
more complete picture of research activity. Most scholarly publications are now
accessed online, making the study of reads and their patterns possible.
Click-stream logs allow us to follow information access by the entire research
community, real-time. Publication and citation datasets just reflect activity
by authors. In addition, download statistics will help us identify publications
with significant impact, but which do not attract many citations. Click-stream
signals are arguably more complex than, say, citation signals. For one, they
are a superposition of different classes of readers. Systematic downloads by
crawlers also contaminate the signal, as does browsing behavior. We discuss the
complexities associated with clickstream data and how, with proper filtering,
statistically significant relations and conclusions can be inferred from
download statistics. We describe how download statistics can be used to
describe research activity at different levels of aggregation, ranging from
organizations to countries. These statistics show a correlation with
socio-economic indicators. A comparison will be made with traditional
bibliometric indicators. We will argue that astronomy is representative of more
general trends.
"
1850,"Joint Workshop on Bibliometric-enhanced Information Retrieval and
  Natural Language Processing for Digital Libraries (BIRNDL 2017)","  The large scale of scholarly publications poses a challenge for scholars in
information seeking and sensemaking. Bibliometrics, information retrieval (IR),
text mining and NLP techniques could help in these search and look-up
activities, but are not yet widely used. This workshop is intended to stimulate
IR researchers and digital library professionals to elaborate on new approaches
in natural language processing, information retrieval, scientometrics, text
mining and recommendation techniques that can advance the state-of-the-art in
scholarly document understanding, analysis, and retrieval at scale. The BIRNDL
workshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the
third edition of the Computational Linguistics (CL) Scientific Summarization
Shared Task.
"
1851,"Developmental tendencies in the Academic Field of Intellectual Property
  through the Identification of Invisible Colleges","  The emergence of intellectual property as an academic issue opens a big gate
to a cross-disciplinary field. Different disciplines start a dialogue in the
framework of the international multilateral treaties in the early 90's. As
global economy demands new knowledge on intellectual property, Science grows at
its own pace. However, the degree of consolidation of cross-disciplinary
academic communities is not clear. In order to know how closely related are
these communities, this paper proposes a mixed methodology to find invisible
colleges in the production about intellectual property. The articles examined
in this paper were extracted from Web of Science. The analyzed period was from
1994 to 2016, taking into account the signature of the agreement on
Trade-Related Aspects of Intellectual Property Rights in the early 90's. A
total amount of 1580 papers were processed through co-citation network
analysis. An especial technique, which combine algorithms of community
detection and defining population of articles through thresholds of shared
references, was applied. In order to contrast the invisible colleges that
emerged with the existence of formal institutional relations, it was made a
qualitative tracking of the authors with respect to their institutional
affiliation, lines of research and meeting places. Both methods show that the
subjects of interest can be grouped into 13 different issues related to
intellectual property field. Even though most of them are related to Laws and
Economics, there are weak linkages between disciplines which could indicate the
construction of a cross-disciplinary field.
"
1852,"Scientific document summarization via citation contextualization and
  scientific discourse","  The rapid growth of scientific literature has made it difficult for the
researchers to quickly learn about the developments in their respective fields.
Scientific document summarization addresses this challenge by providing
summaries of the important contributions of scientific papers. We present a
framework for scientific summarization which takes advantage of the citations
and the scientific discourse structure. Citation texts often lack the evidence
and context to support the content of the cited paper and are even sometimes
inaccurate. We first address the problem of inaccuracy of the citation texts by
finding the relevant context from the cited paper. We propose three approaches
for contextualizing citations which are based on query reformulation, word
embeddings, and supervised learning. We then train a model to identify the
discourse facets for each citation. We finally propose a method for summarizing
scientific papers by leveraging the faceted citations and their corresponding
contexts. We evaluate our proposed method on two scientific summarization
datasets in the biomedical and computational linguistics domains. Extensive
evaluation results show that our methods can improve over the state of the art
by large margins.
"
1853,Predicting Research that will be Cited in Policy Documents,"  Scientific publications and other genres of research output are increasingly
being cited in policy documents. Citations in documents of this nature could be
considered a critical indicator of the significance and societal impact of the
research output. In this study, we built classification models that predict
whether a particular research work is likely to be cited in a public policy
document based on the attention it received online, primarily on social media
platforms. We evaluated the classifiers based on their accuracy, precision, and
recall values. We found that Random Forest and Multinomial Naive Bayes
classifiers performed better overall.
"
1854,"Decentralized creation of academic documents using a Network Attached
  Storage (NAS) server","  Scholarly document creation continues to face various obstacles. Scholarly
text production requires more complex word processors than other forms of texts
because of the complex structures of citations, formulas and figures. The need
for peer review, often single-blind or double-blind, creates needs for document
management that other texts do not require. Additionally, the need for
collaborative editing, security and strict document access rules means that
many existing word processors are imperfect solutions for academics.
Nevertheless, most papers continue to be written using Microsoft Word (Sadeghi
et al. 2017). We here analyze some of the problems with existing academic
solutions and then present an argument why we believe that running an open
source academic writing solution for academic purposes, such as Fidus Writer,
on a Network Attached Storage (NAS) server could be a viable alternative.
"
1855,Preprint D\'ej\`a Vu: an FAQ,"  I give a brief overview of arXiv history, and describe the current state of
arXiv practice, both technical and sociological. This commentary originally
appeared in the EMBO Journal, 19 Oct 2016. It was intended as an update on
comments from the late 1990s regarding use of preprints by biologists (or lack
thereof), but may be of interest to practitioners of other disciplines. It is
based largely on a keynote presentation I gave to the ASAPbio inaugural meeting
in Feb 2016, and responds as well to some follow-up questions.
"
1856,Research Topics Map: rtopmap,"  In this paper we describe a system for visualizing and analyzing worldwide
research topics, {\tt rtopmap}. We gather data from google scholar academic
research profiles, putting together a weighted topics graph, consisting of over
35,000 nodes and 646,000 edges. The nodes correspond to self-reported research
topics, and edges correspond to co-occurring topics in google scholar profiles.
The {\tt rtopmap} system supports zooming/panning/searching and other
google-maps-based interactive features. With the help of map overlays, we also
visualize the strengths and weaknesses of different academic institutions in
terms of human resources (e.g., number of researchers in different areas), as
well as scholarly output (e.g., citation counts in different areas). Finally,
we also visualize what parts of the map are associated with different academic
departments, or with specific documents (such as research papers, or calls for
proposals). The system itself is available at
\url{http://rtopmap.arl.arizona.edu/}.
"
1857,Number game,"  CLARIN (Common Language Resources and Technology Infrastructure) is regarded
as one of the most important European research infrastructures, offering and
promoting a wide array of useful services for (digital) research in linguistics
and humanities. However, the assessment of the users for its core technical
development has been highly limited, therefore, it is unclear if the community
is thoroughly aware of the status-quo of the growing infrastructure. In
addition, CLARIN does not seem to be fully materialised marketing and business
plans and strategies despite its strong technical assets. This article analyses
the web traffic of the Virtual Language Observatory, one of the main web
applications of CLARIN and a symbol of pan-European re-search cooperation, to
evaluate the users and performance of the service in a transparent and
scientific way. It is envisaged that the paper can raise awareness of the
pressing issues on objective and transparent operation of the infrastructure
though Open Evaluation, and the synergy between marketing and technical
development. It also investigates the ""science of web analytics"" in an attempt
to document the research process for the purpose of reusability and
reproducibility, thus to find universal lessons for the use of a web analytics,
rather than to merely produce a statistical report of a particular website
which loses its value outside its context.
"
1858,"Big Missing Data: are scientific memes inherited differently from
  gendered authorship?","  This paper seeks to build upon the previous literature on gender aspects in
research collaboration and knowledge diffusion. Our approach adds the meme
inheritance notion to traditional citation analysis, as we investigate if
scientific memes are inherited differently from gendered authorship. Since
authors of scientific papers inherit knowledge from their cited authors, once
authorship is gendered we are able to characterize the inheritance process with
respect to the frequencies of memes and their propagation scores depending on
the gender of the authors. By applying methodologies that enable the gender
disambiguation of authors, big missing data on the gender of citing and cited
authors is dealt with. Our empirically based approach allows for investigating
the combined effect of meme inheritance and gendered transmission. Results show
that scientific memes do not spread differently from either male or female
cited authors. Likewise, the memes that we analyse were not found to propagate
more easily via male or female inheritance.
"
1859,"Feature analysis of multidisciplinary scientific collaboration patterns
  based on PNAS","  The features of collaboration patterns are often considered to be different
from discipline to discipline. Meanwhile, collaborating among disciplines is an
obvious feature emerged in modern scientific research, which incubates several
interdisciplines. The features of collaborations in and among the disciplines
of biological, physical and social sciences are analyzed based on 52,803 papers
published in a multidisciplinary journal PNAS during 1999 to 2013. From those
data, we found similar transitivity and assortativity of collaboration patterns
as well as the identical distribution type of collaborators per author and that
of papers per author, namely a mixture of generalized Poisson and power-law
distributions. In addition, we found that interdisciplinary research is
undertaken by a considerable fraction of authors, not just those with many
collaborators or those with many papers. This case study provides a window for
understanding aspects of multidisciplinary and interdisciplinary collaboration
patterns.
"
1860,Leveraging web resources for keyword assignment to short text documents,"  Assigning relevant keywords to documents is very important for efficient
retrieval, clustering and management of the documents. Especially with the web
corpus deluged with digital documents, automation of this task is of prime
importance. Keyword assignment is a broad topic of research which refers to
tagging of document with keywords, key-phrases or topics. For text documents,
the keyword assignment techniques have been developed under two sub-topics:
automatic keyword extraction (AKE) and automatic key-phrase abstraction.
However, the approaches developed in the literature for full text documents
cannot be used to assign keywords to low text content documents like twitter
feeds, news clips, product reviews or even short scholarly text. In this work,
we point out several practical challenges encountered in tagging such low text
content documents. As a solution to these challenges, we show that the proposed
approaches which leverage knowledge from several open source web resources
enhance the quality of the tags (keywords) assigned to the low text content
documents. The performance of the proposed approach is tested on real world
corpus consisting of scholarly documents with text content ranging from only
the text in the title of the document (5-10 words) to the summary text/abstract
(100- 150 words). We find that the proposed approach not just improves the
accuracy of keyword assignment but offer a computationally efficient solution
which can be used in real world applications.
"
1861,"Aztec: A Platform to Render Biomedical Software Findable, Accessible,
  Interoperable, and Reusable","  Precision medicine and health requires the characterization and phenotyping
of biological systems and patient datasets using a variety of data formats.
This scenario mandates the centralization of various tools and resources in a
unified platform to render them Findable, Accessible, Interoperable, and
Reusable (FAIR Principles). Leveraging these principles, Aztec provides the
scientific community with a new platform that promotes a long-term, sustainable
ecosystem of biomedical research software. Aztec is available at
https://aztec.bio and its source code is hosted at
https://github.com/BD2K-Aztec.
"
1862,"Making visible the invisible through the analysis of acknowledgements in
  the humanities","  Purpose: Science is subject to a normative structure that includes how the
contributions and interactions between scientists are rewarded. Authorship and
citations have been the key elements within the reward system of science,
whereas acknowledgements, despite being a well-established element in scholarly
communication, have not received the same attention. This paper aims to put
forward the bearing of acknowledgements in the humanities to bring to the
foreground contributions and interactions that, otherwise, would remain
invisible through traditional indicators of research performance.
  Design/methodology/approach: The study provides a comprehensive framework to
understanding acknowledgements as part of the reward system with a special
focus on its value in the humanities as a reflection of intellectual
indebtedness. The distinctive features of research in the humanities are
outlined and the role of acknowledgements as a source of contributorship
information is reviewed to support these assumptions.
  Findings: Peer interactive communication is the prevailing support thanked in
the acknowledgements of humanities, so the notion of acknowledgements as
super-citations can make special sense in this area. Since single-authored
papers still predominate as publishing pattern in this domain, the study of
acknowledgements might help to understand social interactions and intellectual
influences that lie behind a piece of research and are not visible through
authorship.
  Originality/value: Previous works have proposed and explored the prevailing
acknowledgement types by domain. This paper focuses on the humanities to show
the role of acknowledgements within the reward system and highlight publication
patterns and inherent research features which make acknowledgements
particularly interesting in the area as reflection of the socio-cognitive
structure of research.
"
1863,"Investigating Exploratory Search Activities based on the Stratagem Level
  in Digital Libraries","  In this paper we present the results of a user study on exploratory search
activities in a social science digital library. We conducted a user study with
32 participants with a social sciences background -- 16 postdoctoral
researchers and 16 students -- who were asked to solve a task on searching
related work to a given topic. The exploratory search task was performed in a
10-minutes time slot. The use of certain search activities is measured and
compared to gaze data recorded with an eye tracking device. We use a novel tree
graph representation to visualise the users' search patterns and introduce a
way to combine multiple search session trees. The tree graph representation is
capable to create one single tree for multiple users and to identify common
search patterns. In addition, the information behaviour of students and
postdoctoral researchers is being compared. The results show that search
activities on the stratagem level are frequently utilised by both user groups.
The most heavily used search activities were keyword search, followed by
browsing through references and citations, and author searching. The eye
tracking results showed an intense examination of documents metadata,
especially on the level of citations and references. When comparing the group
of students and postdoctoral researchers we found significant differences
regarding gaze data on the area of the journal name of the seed document. In
general, we found a tendency of the postdoctoral researchers to examine the
metadata records more intensively with regards to dwell time and the number of
fixations.
"
1864,"Can the Journal Impact Factor Be Used as a Criterion for the Selection
  of Junior Researchers? A Large-Scale Empirical Study Based on ResearcherID
  Data","  Early in researchers' careers, it is difficult to assess how good their work
is or how important or influential the scholars will eventually be. Hence,
funding agencies, academic departments, and others often use the Journal Impact
Factor (JIF) of where the authors have published to assess their work and
provide resources and rewards for future work. The use of JIFs in this way has
been heavily criticized, however. Using a large data set with many thousands of
publication profiles of individual researchers, this study tests the ability of
the JIF (in its normalized variant) to identify, at the beginning of their
careers, those candidates who will be successful in the long run. Instead of
bare JIFs and citation counts, the metrics used here are standardized according
to Web of Science subject categories and publication years. The results of the
study indicate that the JIF (in its normalized variant) is able to discriminate
between researchers who published papers later on with a citation impact above
or below average in a field and publication year - not only in the short term,
but also in the long term. However, the low to medium effect sizes of the
results also indicate that the JIF (in its normalized variant) should not be
used as the sole criterion for identifying later success: other criteria, such
as the novelty and significance of the specific research, academic
distinctions, and the reputation of previous institutions, should also be
considered.
"
1865,"Dynamic patterns of knowledge flows across technological domains:
  empirical results and link prediction","  The purpose of this study is to investigate the structure and evolution of
knowledge spillovers across technological domains. Specifically, dynamic
patterns of knowledge flow among 29 technological domains, measured by patent
citations for eight distinct periods, are identified and link prediction is
tested for capability for forecasting the evolution in these cross-domain
patent networks. The overall success of the predictions using the Katz metric
implies that there is a tendency to generate increased knowledge flows mostly
within the set of previously linked technological domains. This study
contributes to innovation studies by characterizing the structural change and
evolutionary behaviors in dynamic technology networks and by offering the basis
for predicting the emergence of future technological knowledge flows.
"
1866,"A Community's Perspective on the Status and Future of Peer Review in
  Software Engineering","  Context: Pre-publication peer review of scientific articles is considered a
key element of the research process in software engineering, yet it is often
perceived as not to work fully well.
  Objective: We aim at understanding the perceptions of and attitudes towards
peer review of authors and reviewers at one of software engineering's most
prestigious venues, the International Conference on Software Engineering
(ICSE).
  Method: We invited 932 ICSE 2014/15/16 authors and reviewers to participate
in a survey with 10 closed and 9 open questions.
  Results: We present a multitude of results, such as: Respondents perceive
only one third of all reviews to be good, yet one third as useless or
misleading; they propose double-blind or zero-blind reviewing regimes for
improvement; they would like to see showable proofs of (good) reviewing work be
introduced; attitude change trends are weak.
  Conclusion: The perception of the current state of software engineering peer
review is fairly negative. Also, we found hardly any trend that suggests
reviewing will improve by itself over time; the community will have to make
explicit efforts. Fortunately, our (mostly senior) respondents appear more open
for trying different peer reviewing regimes than we had expected.
"
1867,"Comparing Neural and Attractiveness-based Visual Features for Artwork
  Recommendation","  Advances in image processing and computer vision in the latest years have
brought about the use of visual features in artwork recommendation. Recent
works have shown that visual features obtained from pre-trained deep neural
networks (DNNs) perform very well for recommending digital art. Other recent
works have shown that explicit visual features (EVF) based on attractiveness
can perform well in preference prediction tasks, but no previous work has
compared DNN features versus specific attractiveness-based visual features
(e.g. brightness, texture) in terms of recommendation performance. In this
work, we study and compare the performance of DNN and EVF features for the
purpose of physical artwork recommendation using transactional data from
UGallery, an online store of physical paintings. In addition, we perform an
exploratory analysis to understand if DNN embedded features have some relation
with certain EVF. Our results show that DNN features outperform EVF, that
certain EVF features are more suited for physical artwork recommendation and,
finally, we show evidence that certain neurons in the DNN might be partially
encoding visual features such as brightness, providing an opportunity for
explaining recommendations based on visual neural models.
"
1868,"Challenges of facet analysis and concept placement in universal
  classifications: the example of architecture in UDC","  The paper discusses the challenges of faceted vocabulary organization in
universal classifications which treat the universe of knowledge as a coherent
whole and in which the concepts and subjects in different disciplines are
shared, related and combined. The authors illustrate the challenges of the
facet analytical approach using, as an example, the revision of class 72 in
UDC. The paper reports on the research undertaken in 2013 as preparation for
the revision. This consisted of analysis of concept organization in the UDC
schedules in comparison with the Art & Architecture Thesaurus and class W of
the Bliss Bibliographic Classification. The paper illustrates how such research
can contribute to a better understanding of the field and may lead to
improvements in the facet structure of this segment of the UDC vocabulary.
"
1869,Theory and Practice of Data Citation,"  Citations are the cornerstone of knowledge propagation and the primary means
of assessing the quality of research, as well as directing investments in
science. Science is increasingly becoming ""data-intensive"", where large volumes
of data are collected and analyzed to discover complex patterns through
simulations and experiments, and most scientific reference works have been
replaced by online curated datasets. Yet, given a dataset, there is no
quantitative, consistent and established way of knowing how it has been used
over time, who contributed to its curation, what results have been yielded or
what value it has.
  The development of a theory and practice of data citation is fundamental for
considering data as first-class research objects with the same relevance and
centrality of traditional scientific products. Many works in recent years have
discussed data citation from different viewpoints: illustrating why data
citation is needed, defining the principles and outlining recommendations for
data citation systems, and providing computational methods for addressing
specific issues of data citation.
  The current panorama is many-faceted and an overall view that brings together
diverse aspects of this topic is still missing. Therefore, this paper aims to
describe the lay of the land for data citation, both from the theoretical (the
why and what) and the practical (the how) angle.
"
1870,"Using text analysis to quantify the similarity and evolution of
  scientific disciplines","  We use an information-theoretic measure of linguistic similarity to
investigate the organization and evolution of scientific fields. An analysis of
almost 20M papers from the past three decades reveals that the linguistic
similarity is related but different from experts and citation-based
classifications, leading to an improved view on the organization of science. A
temporal analysis of the similarity of fields shows that some fields (e.g.,
computer science) are becoming increasingly central, but that on average the
similarity between pairs has not changed in the last decades. This suggests
that tendencies of convergence (e.g., multi-disciplinarity) and divergence
(e.g., specialization) of disciplines are in balance.
"
1871,"The Individual Impact Index ($i^3$) Statistic: A Novel Article-Level
  Citation Metric","  Citation metrics are analytic measures used to evaluate the usage, impact and
dissemination of scientific research. Traditionally, citation metrics have been
independently measured at each level of the publication pyramid, namely at the
article-level, at the author-level, and at the journal-level. The most commonly
used metrics have been focused on journal-level measurements, such as the
Impact Factor and the Eigenfactor, as well as on researcher-level metrics like
the Hirsch index (h-index) and i10 index. On the other hand, reliable
article-level metrics are less widespread, and are often reserved to
non-standardized and non-scientific characteristics of individual articles,
such as views, citations, downloads, and mentions in social and news media.
These characteristics are known as 'altmetrics'. However, when the number of
views and citations are similar between two articles, no discriminating measure
currently exists with which to assess and compare each articles' individual
impact. Given the modern, exponentially growing scientific literature,
scientists and readers of Science need optimized, reliable, objective methods
for managing, measuring and comparing research outputs and individual
publications. To this end, I hereby describe and propose a new standardized
article-level metric henceforth known as the 'Individual Impact Index
Statistic', or $i^3$ for short. The $i^3$ is a weighted algorithm that takes
advantage of the peer-review process, and considers a number of characteristics
of individual scientific publications in order to yield a standardized and
readily comparable measure of impact and dissemination. The strengths,
limitations, and potential uses of this novel metric are also discussed.
"
1872,"Classic papers: d\'ej\`a vu, a step further in the bibliometric
  exploitation of Google Scholar","  After giving a brief overview of Eugene Garfield contributions to the issue
of identifying and studying the most cited scientific articles, manifested in
the creation of his Citation Classics, the main characteristics and features of
Google Scholar new service Classic Papers, as well as its main strengths and
weaknesses, are addressed. This product currently displays the most cited
English-language original research articles by fields and published in 2006
"
1873,"A Datamining Approach to the Short Title Catalogue Flanders: the Case of
  Early Modern Quiring Practices","  This paper contains a data mining approach to the Short Title Catalogue
Flanders (http://www.stcv.be/), which aims to record all books printed in
Flanders up to 1801 (24.850 editions, per 31/08/2018). More specifically, it
aims to analyse the Early Modern practice of 'quiring' gatherings in handpress
book production
"
1874,CASCONet: A Conference dataset,"  Knowledge mobilization and translation describes the process of moving
knowledge from research and development (R&D) labs into environments where it
can be put to use. There is increasing interest in understanding mechanisms for
knowledge mobilization, specifically with respect to academia and industry
collaborations. These mechanisms include funding programs, research centers,
and conferences, among others. In this paper, we focus on one specific
knowledge mobilization mechanism, the CASCON conference, the annual conference
of the IBM Centre for Advanced Studies (CAS). The mandate of CAS when it was
established in 1990 was to foster collaborative work between the IBM Toronto
Lab and university researchers from around the world. The first CAS Conference
(CASCON) was held one year after CAS was formed in 1991. The focus of this
annual conference was, and continues to be, bringing together academic
researchers, industry practitioners, and technology users in a forum for
sharing ideas and showcasing the results of the CAS collaborative work. We
collected data about CASCON for the past 25 years including information about
papers, technology showcase demos, workshops, and keynote presentations. The
resulting dataset, called ""CASCONet"" is available for analysis and integration
with related datasets. Using CASCONet, we analyzed interactions between R&D
topics and changes in those topics over time. Results of our analysis show how
the domain of knowledge being mobilized through CAS had evolved over time. By
making CASCONet available to others, we hope that the data can be used in
additional ways to understand knowledge mobilization and translation in this
unique context.
"
1875,A selectional auto-encoder approach for document image binarization,"  Binarization plays a key role in the automatic information retrieval from
document images. This process is usually performed in the first stages of
documents analysis systems, and serves as a basis for subsequent steps. Hence
it has to be robust in order to allow the full analysis workflow to be
successful. Several methods for document image binarization have been proposed
so far, most of which are based on hand-crafted image processing strategies.
Recently, Convolutional Neural Networks have shown an amazing performance in
many disparate duties related to computer vision. In this paper we discuss the
use of convolutional auto-encoders devoted to learning an end-to-end map from
an input image to its selectional output, in which activations indicate the
likelihood of pixels to be either foreground or background. Once trained,
documents can therefore be binarized by parsing them through the model and
applying a threshold. This approach has proven to outperform existing
binarization strategies in a number of document domains.
"
1876,"Complex-network approach for visualizing and quantifying the evolution
  of a scientific topic","  Tracing the evolution of specific topics is a subject area which belongs to
the general problem of mapping the structure of scientific knowledge. Often
bibliometric data bases are used to study the history of scientific topic
evolution from its appearance to its extinction or merger with other topics. In
this chapter the authors present an analysis of the academic response to the
disaster that occurred in 1986 in Chornobyl (Chernobyl), Ukraine, considered as
one of the most devastating nuclear power plant accidents in history. Using a
bibliographic database the distributions of Chornobyl-related papers in
different scientific fields are analysed, as are their growth rates and
properties of co-authorship networks. Elements of descriptive statistics and
tools of complex-network theory are used to highlight interdisciplinary as well
as international effects. In particular, tools of complex-network science
enable information visualization complemented by further quantitative analysis.
A further goal of the chapter is to provide a simple pedagogical introduction
to the application of complex-network analysis for visual data representation
and interdisciplinary communication.
"
1877,Towards Understanding the Evolution of the WWW Conference,"  The World Wide Web conference is a well-established and mature venue with an
already long history. Over the years it has been attracting papers reporting
many important research achievements centered around the Web. In this work we
aim at understanding the evolution of WWW conference series by detecting
crucial years and important topics. We propose a simple yet novel approach
based on tracking the classification errors of the conference papers according
to their predicted publication years.
"
1878,"Publish or impoverish: An investigation of the monetary reward system of
  science in China (1999-2016)","  Purpose: The purpose of this study is to present the landscape of the
cash-per-publication reward policy in China and reveal its trend since the late
1990s.
  Design/methodology/approach: This study is based on the analysis of 168
university documents regarding the cash-per-publication reward policy at 100
Chinese universities.
  Findings: Chinese universities offer cash rewards from 30 to 165,000 USD for
papers published in journals indexed by Web of Science (WoS), and the average
reward amount has been increasing for the past 10 years.
  Originality/value: The cash-per-publication reward policy in China has never
been systematically studied and investigated before except for in some case
studies. This is the first paper that reveals the landscape of the
cash-per-publication reward policy in China.
"
1879,"Determining sentiment in citation text and analyzing its impact on the
  proposed ranking index","  Whenever human beings interact with each other, they exchange or express
opinions, emotions, and sentiments. These opinions can be expressed in text,
speech or images. Analysis of these sentiments is one of the popular research
areas of present day researchers. Sentiment analysis, also known as opinion
mining tries to identify or classify these sentiments or opinions into two
broad categories - positive and negative. In recent years, the scientific
community has taken a lot of interest in analyzing sentiment in textual data
available in various social media platforms. Much work has been done on social
media conversations, blog posts, newspaper articles and various narrative
texts. However, when it comes to identifying emotions from scientific papers,
researchers have faced some difficulties due to the implicit and hidden nature
of opinion. By default, citation instances are considered inherently positive
in emotion. Popular ranking and indexing paradigms often neglect the opinion
present while citing. In this paper, we have tried to achieve three objectives.
First, we try to identify the major sentiment in the citation text and assign a
score to the instance. We have used a statistical classifier for this purpose.
Secondly, we have proposed a new index (we shall refer to it hereafter as
M-index) which takes into account both the quantitative and qualitative factors
while scoring a paper. Thirdly, we developed a ranking of research papers based
on the M-index. We also try to explain how the M-index impacts the ranking of
scientific papers.
"
1880,Probing Multivariate Indicators for Academic Evaluation,"  We combine the Integrated Impact Indicator (I3) and the h-index into the
I3-type framework and introduce the publication vector X = (X1, X2, X3) and the
citation vector Y = (Y1, Y2, Y3) , the publication score I3X=X1+X2+X3 and the
citation score I3Y=Y1+Y2+Y3, and alternative indicators based on percentile
classes generated by the h-index. These multivariate indicators can be used for
academic evaluation. The empirical studies show that the h-core distribution is
suitable to evaluate scholars, the X1 and Y1 are applied to measure core impact
power of universities, and I3X and I3Y are alternatives of journal impact
factor (JIF). The multivariate indicators provide a multidimensional view of
academic evaluation with using the advantages of both the h-index and I3.
"
1881,The structural role of the core literature in history,"  The intellectual landscapes of the humanities are mostly uncharted territory.
Little is known on the ways published research of humanist scholars defines
areas of intellectual activity. An open question relates to the structural role
of core literature: highly cited sources, naturally playing a disproportionate
role in the definition of intellectual landscapes. We introduce four indicators
in order to map the structural role played by core sources into connecting
different areas of the intellectual landscape of citing publications (i.e.
communities in the bibliographic coupling network). All indicators factor out
the influence of degree distributions by internalizing a null configuration
model. By considering several datasets focused on history, we show that two
distinct structural actions are performed by the core literature: a global one,
by connecting otherwise separated communities in the landscape, or a local one,
by rising connectivity within communities. In our study, the global action is
mainly performed by small sets of scholarly monographs, reference works and
primary sources, while the rest of the core, and especially most journal
articles, acts mostly locally.
"
1882,Journal of Open Source Software (JOSS): design and first-year review,"  This article describes the motivation, design, and progress of the Journal of
Open Source Software (JOSS). JOSS is a free and open-access journal that
publishes articles describing research software. It has the dual goals of
improving the quality of the software submitted and providing a mechanism for
research software developers to receive credit. While designed to work within
the current merit system of science, JOSS addresses the dearth of rewards for
key contributions to science made in the form of software. JOSS publishes
articles that encapsulate scholarship contained in the software itself, and its
rigorous peer review targets the software components: functionality,
documentation, tests, continuous integration, and the license. A JOSS article
contains an abstract describing the purpose and functionality of the software,
references, and a link to the software archive. The article is the entry point
of a JOSS submission, which encompasses the full set of software artifacts.
Submission and review proceed in the open, on GitHub. Editors, reviewers, and
authors work collaboratively and openly. Unlike other journals, JOSS does not
reject articles requiring major revision; while not yet accepted, articles
remain visible and under review until the authors make adequate changes (or
withdraw, if unable to meet requirements). Once an article is accepted, JOSS
gives it a DOI, deposits its metadata in Crossref, and the article can begin
collecting citations on indexers like Google Scholar and other services.
Authors retain copyright of their JOSS article, releasing it under a Creative
Commons Attribution 4.0 International License. In its first year, starting in
May 2016, JOSS published 111 articles, with more than 40 additional articles
under review. JOSS is a sponsored project of the nonprofit organization
NumFOCUS and is an affiliate of the Open Source Initiative.
"
1883,"Core Elements in the Process of Citing Publications: A Conceptual
  Overview of the Literature","  This study provides a conceptual overview of the literature dealing with the
process of citing documents (focusing on the literature from the recent
decade). It presents theories, which have been proposed for explaining the
citation process, and studies having empirically analyzed this process. The
overview is referred to as conceptual, because it is structured based on core
elements in the citation process: the context of the cited document, processes
from selection to citation of documents, and the context of the citing
document. The core elements are presented in a schematic representation. The
overview can be used to find answers on basic questions about the practice of
citing documents. Besides understanding of the process of citing, it delivers
basic information for the proper application of citations in research
evaluation.
"
1884,"Analysis of Footnote Chasing and Citation Searching in an Academic
  Search Engine","  In interactive information retrieval, researchers consider the user behavior
towards systems and search tasks in order to adapt search results by analyzing
their past interactions. In this paper, we analyze the user behavior towards
Marcia Bates' search stratagems such as 'footnote chasing' and 'citation
search' in an academic search engine. We performed a preliminary analysis of
their frequency and stage of use in the social sciences search engine sowiport.
In addition, we explored the impact of these stratagems on the whole search
process performance. We can conclude that the appearance of these two search
features in real retrieval sessions lead to an improvement of the precision in
terms of positive interactions with 16% when using footnote chasing and 17% for
the citation search stratagem.
"
1885,"Guidelines for including grey literature and conducting multivocal
  literature reviews in software engineering","  Context: A Multivocal Literature Review (MLR) is a form of a Systematic
Literature Review (SLR) which includes the grey literature (e.g., blog posts
and white papers) in addition to the published (formal) literature (e.g.,
journal and conference papers). MLRs are useful for both researchers and
practitioners since they provide summaries both the state-of-the art and
-practice in a given area. Objective: There are several guidelines to conduct
SLR studies in SE. However, given the facts that several phases of MLRs differ
from those of traditional SLRs, for instance with respect to the search process
and source quality assessment. Therefore, SLR guidelines are only partially
useful for conducting MLR studies. Our goal in this paper is to present
guidelines on how to conduct MLR studies in SE. Method: To develop the MLR
guidelines, we benefit from three inputs: (1) existing SLR guidelines in SE,
(2), a literature survey of MLR guidelines and experience papers in other
fields, and (3) our own experiences in conducting several MLRs in SE. All
derived guidelines are discussed in the context of three examples MLRs as
running examples (two from SE and one MLR from the medical sciences). Results:
The resulting guidelines cover all phases of conducting and reporting MLRs in
SE from the planning phase, over conducting the review to the final reporting
of the review. In particular, we believe that incorporating and adopting a vast
set of recommendations from MLR guidelines and experience papers in other
fields have enabled us to propose a set of guidelines with solid foundations.
Conclusion: Having been developed on the basis of three types of solid
experience and evidence, the provided MLR guidelines support researchers to
effectively and efficiently conduct new MLRs in any area of SE.
"
1886,"Exploring the position of cities in global corporate research and
  development: a bibliometric analysis by two different geographical approaches","  Global cities are defined, on the one hand, as the major command and control
centres of the world economy and, on the other hand, as the most significant
sites of the production of innovation. As command and control centres, they are
home to the headquarters of the most powerful MNCs of the global economy, while
as sites for the production of innovation they are supposed to be the most
important sites of corporate research and development (R&D) activities. In this
paper, we conduct a bibliometric analysis of the data located in the Scopus and
Forbes 2000 databases to reveal the correlation between the characteristics of
the above global city definitions. We explore which cities are the major
control points of the global corporate R&D (home city approach), and which
cities are the most important sites of corporate R&D activities (host city
approach). According to the home city approach we assign articles produced by
companies to cities where the decision-making headquarters are located (i.e. to
cities that control the companies' R&D activities), while according to the host
city approach we assign articles to cities where the R&D activities are
actually conducted. Given Sassen's global city concept, we expect global cities
to be both the leading home cities and host cities.
"
1887,"The Closer the Better: Similarity of Publication Pairs at Different
  Co-Citation Levels","  We investigate the similarities of pairs of articles which are co-cited at
the different co-citation levels of the journal, article, section, paragraph,
sentence and bracket. Our results indicate that textual similarity,
intellectual overlap (shared references), author overlap (shared authors),
proximity in publication time all rise monotonically as the co-citation level
gets lower (from journal to bracket). While the main gain in similarity happens
when moving from journal to article co-citation, all level changes entail an
increase in similarity, especially section to paragraph and paragraph to
sentence/bracket levels. We compare results from four journals over the years
2010-2015: Cell, the European Journal of Operational Research, Physics Letters
B and Research Policy, with consistent general outcomes and some interesting
differences. Our findings motivate the use of granular co-citation information
as defined by meaningful units of text, with implications for, among others,
the elaboration of maps of science and the retrieval of scholarly literature.
"
1888,"Look Who's Talking: Bipartite Networks as Representations of a Topic
  Model of New Zealand Parliamentary Speeches","  Quantitative methods to measure the participation to parliamentary debate and
discourse of elected Members of Parliament (MPs) and the parties they belong to
are lacking. This is an exploratory study in which we propose the development
of a new approach for a quantitative analysis of such participation. We utilize
the New Zealand government's digital Hansard database to construct a topic
model of parliamentary speeches consisting of nearly 40 million words in the
period 2003-2016. A Latent Dirichlet Allocation topic model is implemented in
order to reveal the thematic structure of our set of documents. This generative
statistical model enables the detection of major themes or topics that are
publicly discussed in the New Zealand parliament, as well as permitting their
classification by MP. Information on topic proportions is subsequently analyzed
using a combination of statistical methods. We observe patterns arising from
time-series analysis of topic frequencies which can be related to specific
social, economic and legislative events. We then construct a bipartite network
representation, linking MPs to topics, for each of four parliamentary terms in
this time frame. We build projected networks (onto the set of nodes represented
by MPs) and proceed to the study of the dynamical changes of their topology,
including community structure. By performing this longitudinal network
analysis, we can observe the evolution of the New Zealand parliamentary topic
network and its main parties in the period studied.
"
1889,"Mapping spatial and temporal changes of global corporate research and
  development activities by conducting a bibliometric analysis","  Corporate research and development (R&D) activities have long been highly
concentrated in a handful of world cities. This is due to the fact that these
cities (e.g., Tokyo, New York, London, and Paris) are home to the largest and
most powerful transnational corporations and are globally important sites for
innovative start-up firms that operate in the fastest growing industries.
However, in tandem with the rapid technological changes of our age, corporate
R&D activities have shifted towards newly emerging and now globally significant
R&D centres, like San Jose, San Francisco, and Boston in the United States, and
Beijing, Seoul, and Shenzhen in East Asia. In this paper, I will conduct a
bibliometric analysis to define which cities are centres of corporate R&D
activities, how different industries influence their performance, and what
spatial tendencies characterise the period from 1980 to 2014. The bibliometric
analysis is based upon an assumption that implies there is a close connection
between the number of scientific articles published by a given firm and the
volume of its R&D activity. Results show that firms headquartered in Tokyo, New
York, London, and Paris published the largest combined number of scientific
articles in the period from 1980 to 2014, but that the growth rate of the
annual output of scientific articles was much greater in Boston, San Jose,
Beijing, and Seoul, as well as some Taiwanese cities. Furthermore, it can also
be seen that those cities that have the largest number of articles; i.e., that
can be considered as the most significant sites of corporate R&D in which firms
operate in fast-growing industries, are primarily in the pharmaceutical and
information technology industries. For these reasons, some mid-sized cities
that are home to globally significant pharmaceutical or information technology
firms are also top corporate R&D hubs.
"
1890,"PlumX As a Potential Tool to Assess the Macroscopic Multidimensional
  Impact of Books","  The main purpose of this macro-study is to shed light on the broad impact of
books. For this purpose, the impact of a very large collection of books has
been analyzed by using PlumX, an analytical tool providing a great number of
different metrics provided by various tools.
"
1891,VMEXT: A Visualization Tool for Mathematical Expression Trees,"  Mathematical expressions can be represented as a tree consisting of terminal
symbols, such as identifiers or numbers (leaf nodes), and functions or
operators (non-leaf nodes). Expression trees are an important mechanism for
storing and processing mathematical expressions as well as the most frequently
used visualization of the structure of mathematical expressions. Typically,
researchers and practitioners manually visualize expression trees using
general-purpose tools. This approach is laborious, redundant, and error-prone.
Manual visualizations represent a user's notion of what the markup of an
expression should be, but not necessarily what the actual markup is. This paper
presents VMEXT - a free and open source tool to directly visualize expression
trees from parallel MathML. VMEXT simultaneously visualizes the presentation
elements and the semantic structure of mathematical expressions to enable users
to quickly spot deficiencies in the Content MathML markup that does not affect
the presentation of the expression. Identifying such discrepancies previously
required reading the verbose and complex MathML markup. VMEXT also allows one
to visualize similar and identical elements of two expressions. Visualizing
expression similarity can support support developers in designing retrieval
approaches and enable improved interaction concepts for users of mathematical
information retrieval systems. We demonstrate VMEXT's visualizations in two
web-based applications. The first application presents the visualizations
alone. The second application shows a possible integration of the
visualizations in systems for mathematical knowledge management and
mathematical information retrieval. The application converts LaTeX input to
parallel MathML, computes basic similarity measures for mathematical
expressions, and visualizes the results using VMEXT.
"
1892,A Bibliometric Model for Identifying Emerging Research Topics,"  Detecting emerging research topics is essential, not only for research
agencies but also for individual researchers. Previous studies have created
various bibliographic indicators for the identification of emerging research
topics. However, as indicated by Rotolo et al. (2015), the most serious
problems are the lack of an acknowledged definition of emergence and incomplete
elaboration of the linkages between the definitions that are used and the
indicators that are created. With these issues in mind, this study first
adjusts the definition of an emerging technology that Rotolo et al. (2015) have
proposed in order to accommodate the analysis. Next, a set of criteria for the
identification of emerging topics is proposed according to the adjusted
definition and attributes of emergence. By the use of two sets of parameter
values, several emerging research topics are identified. Finally, evaluation
tests are conducted by demonstration of the proposed approach and comparison
with previous studies. The strength of the present methodology lies in the fact
that it is fully transparent, straightforward, and flexible.
"
1893,"Plots for visualizing paper impact and journal impact of single
  researchers in a single graph","  In research evaluation of single researchers, the assessment of paper and
journal impact is of interest. High journal impact reflects the ability of
researchers to convince strict reviewers, and high paper impact reflects the
usefulness of papers for future research. In many bibliometric studies, metrics
for journal and paper impact are separately presented. In this paper, we
introduce two graph types, which combine both metrics in a single graph. The
graphs can be used in research evaluation to visualize the performance of
single researchers comprehensively.
"
1894,"Classifying document types to enhance search and recommendations in
  digital libraries","  In this paper, we address the problem of classifying documents available from
the global network of (open access) repositories according to their type. We
show that the metadata provided by repositories enabling us to distinguish
research papers, thesis and slides are missing in over 60% of cases. While
these metadata describing document types are useful in a variety of scenarios
ranging from research analytics to improving search and recommender (SR)
systems, this problem has not yet been sufficiently addressed in the context of
the repositories infrastructure. We have developed a new approach for
classifying document types using supervised machine learning based exclusively
on text specific features. We achieve 0.96 F1-score using the random forest and
Adaboost classifiers, which are the best performing models on our data. By
analysing the SR system logs of the CORE [1] digital library aggregator, we
show that users are an order of magnitude more likely to click on research
papers and thesis than on slides. This suggests that using document types as a
feature for ranking/filtering SR results in digital libraries has the potential
to improve user experience.
"
1895,"Incidental or influential? - Challenges in automatically detecting
  citation importance using publication full texts","  This work looks in depth at several studies that have attempted to automate
the process of citation importance classification based on the publications
full text. We analyse a range of features that have been previously used in
this task. Our experimental results confirm that the number of in text
references are highly predictive of influence. Contrary to the work of
Valenzuela et al. we find abstract similarity one of the most predictive
features. Overall, we show that many of the features previously described in
literature are not particularly predictive. Consequently, we discuss challenges
and potential improvements in the classification pipeline, provide a critical
review of the performance of individual features and address the importance of
constructing a large scale gold standard reference dataset.
"
1896,Sustainable computational science: the ReScience initiative,"  Computer science offers a large set of tools for prototyping, writing,
running, testing, validating, sharing and reproducing results, however
computational science lags behind. In the best case, authors may provide their
source code as a compressed archive and they may feel confident their research
is reproducible. But this is not exactly true. James Buckheit and David Donoho
proposed more than two decades ago that an article about computational results
is advertising, not scholarship. The actual scholarship is the full software
environment, code, and data that produced the result. This implies new
workflows, in particular in peer-reviews. Existing journals have been slow to
adapt: source codes are rarely requested, hardly ever actually executed to
check that they produce the results advertised in the article. ReScience is a
peer-reviewed journal that targets computational research and encourages the
explicit replication of already published research, promoting new and
open-source implementations in order to ensure that the original research can
be replicated from its description. To achieve this goal, the whole publishing
chain is radically different from other traditional scientific journals.
ReScience resides on GitHub where each new implementation of a computational
study is made available together with comments, explanations, and software
tests.
"
1897,"Medical Theses and Derivative Articles: Dissemination Of Contents and
  Publication Patterns","  Doctoral theses are an important source of publication in universities,
although little research has been carried out on the publications resulting
from theses, on so-called derivative articles. This study investigates how
derivative articles can be identified through a text analysis based on the
full-text of a set of medical theses and the full-text of articles, with which
they shared authorship. The text similarity analysis methodology applied
consisted in exploiting the full-text articles according to organization of
scientific discourse (IMRaD) using the TurnItIn plagiarism tool. The study
found that the text similarity rate in the Discussion section can be used to
discriminate derivative articles from non-derivative articles. Additional
findings were: the first position of the thesis's author dominated in 85% of
derivative articles, the participation of supervisors as coauthors occurred in
100% of derivative articles, the authorship credit retained by the thesis's
author was 42% in derivative articles, the number of coauthors by article was 5
in derivative articles versus 6.4 coauthors, as average, in non-derivative
articles and the time differential regarding the year of thesis completion
showed that 87.5% of derivative articles were published before or in the same
year of thesis completion.
"
1898,Sciunits: Reusable Research Objects,"  Science is conducted collaboratively, often requiring knowledge sharing about
computational experiments. When experiments include only datasets, they can be
shared using Uniform Resource Identifiers (URIs) or Digital Object Identifiers
(DOIs). An experiment, however, seldom includes only datasets, but more often
includes software, its past execution, provenance, and associated
documentation. The Research Object has recently emerged as a comprehensive and
systematic method for aggregation and identification of diverse elements of
computational experiments. While a necessary method, mere aggregation is not
sufficient for the sharing of computational experiments. Other users must be
able to easily recompute on these shared research objects. In this paper, we
present the sciunit, a reusable research object in which aggregated content is
recomputable. We describe a Git-like client that efficiently creates, stores,
and repeats sciunits. We show through analysis that sciunits repeat
computational experiments with minimal storage and processing overhead.
Finally, we provide an overview of sharing and reproducible cyberinfrastructure
based on sciunits gaining adoption in the domain of geosciences.
"
1899,"Impact and Productivity of PhD Graduates of Computer Science/Engineering
  Departments of Hellenic Universities","  This article presents an anatomy of PhD programmes in Hellenic universities'
departments of computer science/engineering from the perspective of research
productivity and impact. The study aims at showing the dynamics of research
conducted in computer science/engineering departments, and after recognizing
weaknesses, to motivate the stakeholders to take actions that will improve
competition and excellence. Beneficiaries of this investigation are the
following entities: a) the departments themselves can assess their performance
relative to that of other departments and then set strategic goals and design
procedures to achieve them, b) supervisors can assess the part of their
research conducted with PhDs and set their own goals, c) former PhDs who can
identify their relative success, and finally d) prospective PhD students who
can consider the efficacy of departments and supervisors in conducting
high-impact research as one more significant factor in designing the doctoral
studies they will follow.
"
1900,"DataCite as a novel bibliometric source: Coverage, strengths and
  limitations","  This paper explores the characteristics of DataCite to determine its
possibilities and potential as a new bibliometric data source to analyze the
scholarly production of open data. Open science and the increasing data sharing
requirements from governments, funding bodies, institutions and scientific
journals has led to a pressing demand for the development of data metrics. As a
very first step towards reliable data metrics, we need to better comprehend the
limitations and caveats of the information provided by sources of open data. In
this paper, we critically examine records downloaded from the DataCite's OAI
API and elaborate a series of recommendations regarding the use of this source
for bibliometric analyses of open data. We highlight issues related to metadata
incompleteness, lack of standardization, and ambiguous definitions of several
fields. Despite these limitations, we emphasize DataCite's value and potential
to become one of the main sources for data metrics development.
"
1901,Open Source Software for Digital Preservation Repositories: a Survey,"  In the digital age, the amount of data produced is growing exponentially.
Governments and institutions can no longer rely on old methods for storing data
and passing on the knowledge to future generations. Digital data preservation
is a mandatory issue that needs proper strategies and tools. With this
awareness, efforts are being made to create and perfect software solutions
capable of responding to the challenge of properly preserving digital
information. This paper focuses on the state-of-the-art in open-source software
solutions for the digital preservation and curation field used to assimilate
and disseminate information to designated audiences. Eleven open source
projects for digital preservation are surveyed in areas such as supported
standards and protocols, strategies for preservation, methodologies for
reporting, dynamic of development, targeted operating systems, multilingual
support and open source license. Furthermore, five of these open source
projects, are further analysed, with focus on features deemed important for the
area. Along open source solutions, the paper also briefly surveys the standards
and protocols relevant for digital data preservation. The area of digital data
preservation repositories has several open source solutions, which can form the
base to overcome the challenges to reach mature and reliable digital data
preservation.
"
1902,"Predicting Personality from Book Preferences with User-Generated Content
  Labels","  Psychological studies have shown that personality traits are associated with
book preferences. However, past findings are based on questionnaires focusing
on conventional book genres and are unrepresentative of niche content. For a
more comprehensive measure of book content, this study harnesses a massive
archive of content labels, also known as 'tags', created by users of an online
book catalogue, Goodreads.com. Combined with data on preferences and
personality scores collected from Facebook users, the tag labels achieve high
accuracy in personality prediction by psychological standards. We also group
tags into broader genres, to check their validity against past findings. Our
results are robust across both tag and genre levels of analyses, and consistent
with existing literature. Moreover, user-generated tag labels reveal unexpected
insights, such as cultural differences, book reading behaviors, and other
non-content factors affecting preferences. To our knowledge, this is currently
the largest study that explores the relationship between personality and book
content preferences.
"
1903,"Tweeting about journal articles: Engagement, marketing or just
  gibberish?","  This paper presents preliminary results on the analysis of tweets to journal
articles in the field of Dentistry. We present two case studies in which we
critically examine the contents and context that motivate the tweeting of
journal articles. We then focus on a specific aspect, the role played by
journals on self-promoting their contents and the effect this has on the total
number of tweets their papers produce. In a context where many are pushing to
the use of altmetrics as an alternative or complement to traditional
bibliometric indicators. We find a lack of evidence (and interest) on
critically examining the many claims that are being made as to their capability
to trace evidences of 'broader forms of impact'. Our first results are not
promising and question current approaches being made in the field of
altmetrics.
"
1904,ShortScience.org - Reproducing Intuition,"  We present ShortScience.org, a platform for post-publication discussion of
research papers. On ShortScience.org, the research community can read and write
summaries of papers in order to increase accessible and reproducibility.
Summaries contain the perspective and insight of other readers, why they liked
or disliked it, and their attempt to demystify complicated sections.
ShortScience.org has over 600 paper summaries, all of which are searchable and
organized by paper, conference, and year. Many regular contributors are expert
machine learning researchers. We present statistics from the last year of
operation, user demographics, and responses from a usage survey. Results
indicate that ShortScience benefits students most, by providing short,
understandable summaries reflecting expert opinions.
"
1905,"Errors and secret data in the Italian research assessment exercise. A
  comment to a reply","  Italy adopted a performance-based system for funding universities that is
centered on the results of a national research assessment exercise, realized by
a governmental agency (ANVUR). ANVUR evaluated papers by using 'a dual system
of evaluation', that is by informed peer review or by bibliometrics. In view of
validating that system, ANVUR performed an experiment for estimating the
agreement between informed review and bibliometrics. Ancaiani et al. (2015)
presents the main results of the experiment. Baccini and De Nicolao (2017)
documented in a letter, among other critical issues, that the statistical
analysis was not realized on a random sample of articles. A reply to the letter
has been published by Research Evaluation (Benedetto et al. 2017). This note
highlights that in the reply there are (1) errors in data, (2) problems with
'representativeness' of the sample, (3) unverifiable claims about weights used
for calculating kappas, (4) undisclosed averaging procedures; (5) a statement
about 'same protocol in all areas' contradicted by official reports. Last but
not least: the data used by the authors continue to be undisclosed. A general
warning concludes: many recently published papers use data originating from
Italian research assessment exercise. These data are not accessible to the
scientific community and consequently these papers are not reproducible. They
can be hardly considered as containing sound evidence at least until authors or
ANVUR disclose the data necessary for replication.
"
1906,"Searching Data: A Review of Observational Data Retrieval Practices in
  Selected Disciplines","  A cross-disciplinary examination of the user behaviours involved in seeking
and evaluating data is surprisingly absent from the research data discussion.
This review explores the data retrieval literature to identify commonalities in
how users search for and evaluate observational research data. Two analytical
frameworks rooted in information retrieval and science technology studies are
used to identify key similarities in practices as a first step toward
developing a model describing data retrieval.
"
1907,"Data, Science and Society","  Reflections on the Concept of Data and its Implications for Science and
Society
"
1908,Extracting Core Claims from Scientific Articles,"  The number of scientific articles has grown rapidly over the years and there
are no signs that this growth will slow down in the near future. Because of
this, it becomes increasingly difficult to keep up with the latest developments
in a scientific field. To address this problem, we present here an approach to
help researchers learn about the latest developments and findings by extracting
in a normalized form core claims from scientific articles. This normalized
representation is a controlled natural language of English sentences called
AIDA, which has been proposed in previous work as a method to formally
structure and organize scientific findings and discourse. We show how such AIDA
sentences can be automatically extracted by detecting the core claim of an
article, checking for AIDA compliance, and - if necessary - transforming it
into a compliant sentence. While our algorithm is still far from perfect, our
results indicate that the different steps are feasible and they support the
claim that AIDA sentences might be a promising approach to improve scientific
communication in the future.
"
1909,"Extracting Event-Centric Document Collections from Large-Scale Web
  Archives","  Web archives are typically very broad in scope and extremely large in scale.
This makes data analysis appear daunting, especially for non-computer
scientists. These collections constitute an increasingly important source for
researchers in the social sciences, the historical sciences and journalists
interested in studying past events. However, there are currently no access
methods that help users to efficiently access information, in particular about
specific events, beyond the retrieval of individual disconnected documents.
Therefore we propose a novel method to extract event-centric document
collections from large scale Web archives. This method relies on a specialized
focused extraction algorithm. Our experiments on the German Web archive
(covering a time period of 19 years) demonstrate that our method enables the
extraction of event-centric collections for different event types.
"
1910,Comparing People with Bibliometrics,"  Bibliometric indicators, citation counts and/or download counts are
increasingly being used to inform personnel decisions such as hiring or
promotions. These statistics are very often misused. Here we provide a guide to
the factors which should be considered when using these so-called quantitative
measures to evaluate people. Rules of thumb are given for when begin to use
bibliometric measures when comparing otherwise similar candidates.
"
1911,"A Graph Analytics Framework for Ranking Authors, Papers and Venues","  A lot of scientific works are published in different areas of science,
technology, engineering and mathematics. It is not easy, even for experts, to
judge the quality of authors, papers and venues (conferences and journals). An
objective measure to assign scores to these entities and to rank them is very
useful. Although, several metrics and indexes have been proposed earlier, they
suffer from various problems. In this paper, we propose a graph-based analytics
framework to assign scores and to rank authors, papers and venues. Our
algorithm considers only the link structures of the underlying graphs. It does
not take into account other aspects, such as the associated texts and the
reputation of these entities. In the limit of large number of iterations, the
solution of the iterative equations gives the unique entity scores. This
framework can be easily extended to other interdependent networks.
"
1912,Emerging Topics in Internet Technology: A Complex Networks Approach,"  Communication networks, in general, and internet technology, in particular,
is a fast-evolving area of research. While it is important to keep track of
emerging trends in this domain, it is such a fast-growing area that it can be
very difficult to keep track of literature. The problem is compounded by the
fast-growing number of citation databases. While other databases are gradually
indexing a large set of reliable content, currently the Web of Science
represents one of the most highly valued databases. Research indexed in this
database is known to highlight key advancements in any domain. In this paper,
we present a Complex Network-based analytical approach to analyze recent data
from the Web of Science in communication networks. Taking bibliographic records
from the recent period of 2014 to 2017, we model and analyze complex
scientometric networks. Using bibliometric coupling applied over complex
citation data we present answers to co-citation patterns of documents,
co-occurrence patterns of terms, as well as the most influential articles,
among others, We also present key pivot points and intellectual turning points.
Complex network analysis of the data demonstrates a considerably high level of
interest in two key clusters labeled descriptively as ""social networks"" and
""computer networks"". In addition, key themes in highly cited literature were
clearly identified as ""communication networks,"" ""social networks,"" and ""complex
networks"".
"
1913,Network Community Detection: A Review and Visual Survey,"  Community structure is an important area of research. It has received a
considerable attention from the scientific community. Despite its importance,
one of the key problems in locating information about community detection is
the diverse spread of related articles across various disciplines. To the best
of our knowledge, there is no current comprehensive review of recent literature
which uses a scientometric analysis using complex networks analysis covering
all relevant articles from the Web of Science (WoS). Here we present a visual
survey of key literature using CiteSpace. The idea is to identify emerging
trends besides using network techniques to examine the evolution of the domain.
Towards that end, we identify the most influential, central, as well as active
nodes using scientometric analyses. We examine authors, key articles, cited
references, core subject categories, key journals, institutions, as well as
countries. The exploration of the scientometric literature of the domain
reveals that Yong Wang is a pivot node with the highest centrality.
Additionally, we have observed that Mark Newman is the most highly cited author
in the network. We have also identified that the journal, ""Reviews of Modern
Physics"" has the strongest citation burst. In terms of cited documents, an
article by Andrea Lancichinetti has the highest centrality score. We have also
discovered that the origin of the key publications in this domain is from the
United States. Whereas Scotland has the strongest and longest citation burst.
Additionally, we have found that the categories of ""Computer Science"" and
""Engineering"" lead other categories based on frequency and centrality
respectively.
"
1914,Research Activity Classification based on Time Series Bibliometrics,"  Bibliometrics such as the number of papers and times cited are often used to
compare researchers based on specific criteria. The criteria, however, are
different in each research domain and are set by empirical laws. Moreover,
there are arguments, such that the simple sum of metric values works to the
advantage of elders. Therefore, this paper attempts to constitute features from
time series data of bibliometrics, and then classify the researchers according
to the features. In detail, time series patterns are extracted from
bibliographic data sets, and then a model to classify whether the researchers
are ""distinguished"" or not is created by a machine learning technique. The
experiments achieved an F-measure of 80.0% in the classification of 114
researchers in two research domains based on the data sets of Japan Science and
Technology Agency and Elsevier's Scopus. In the future, we will conduct
verification on a number of researchers in several domains, and then make use
of discovering ""distinguished"" researchers, who are not widely known.
"
1915,Exploring Features for Predicting Policy Citations,"  In this study we performed an initial investigation and evaluation of
altmetrics and their relationship with public policy citation of research
papers. We examined methods for using altmetrics and other data to predict
whether a research paper is cited in public policy and applied receiver
operating characteristic curve on various feature groups in order to evaluate
their potential usefulness. From the methods we tested, classifying based on
tweet count provided the best results, achieving an area under the ROC curve of
0.91.
"
1916,Power-law citation distributions are not scale-free,"  We analyze time evolution of statistical distributions of citations to
scientific papers published in one year. While these distributions can be
fitted by a power-law dependence we find that they are nonstationary and the
exponent of the power law fit decreases with time and does not come to
saturation. We attribute the nonstationarity of citation distributions to
different longevity of the low-cited and highly-cited papers. By measuring
citation trajectories of papers we found that citation careers of the low-cited
papers come to saturation after 10-15 years while those of the highly-cited
papers continue to increase indefinitely: the papers that exceed some citation
threshold become runaways. Thus, we show that although citation distribution
can look as a power-law, it is not scale-free and there is a hidden dynamic
scale associated with the onset of runaways. We compare our measurements to our
recently developed model of citation dynamics based on
copying/redirection/triadic closure and find explanations to our empirical
observations.
"
1917,"LitStoryTeller: An Interactive System for Visual Exploration of
  Scientific Papers Leveraging Named entities and Comparative Sentences","  The present study proposes LitStoryTeller, an interactive system for visually
exploring the semantic structure of a scientific article. We demonstrate how
LitStoryTeller could be used to answer some of the most fundamental research
questions, such as how a new method was built on top of existing methods, based
on what theoretical proof and experimental evidences. More importantly,
LitStoryTeller can assist users to understand the full and interesting story a
scientific paper, with a concise outline and important details. The proposed
system borrows a metaphor from screen play, and visualizes the storyline of a
scientific paper by arranging its characters (scientific concepts or
terminologies) and scenes (paragraphs/sentences) into a progressive and
interactive storyline. Such storylines help to preserve the semantic structure
and logical thinking process of a scientific paper. Semantic structures, such
as scientific concepts and comparative sentences, are extracted using existing
named entity recognition APIs and supervised classifiers, from a scientific
paper automatically. Two supplementary views, ranked entity frequency view and
entity co-occurrence network view, are provided to help users identify the
""main plot"" of such scientific storylines. When collective documents are ready,
LitStoryTeller also provides a temporal entity evolution view and entity
community view for collection digestion.
"
1918,"Visualizing the context of citations referencing papers published by
  Eugene Garfield: A new type of keyword co-occurrence analysis","  During Eugene Garfield's (EG's) lengthy career as information scientist, he
published about 1,500 papers. In this study, we use the impressive oeuvre of EG
to introduce a new type of bibliometric networks: keyword co-occurrences
networks based on the context of citations, which are referenced in a certain
paper set (here: the papers published by EG). The citation context is defined
by the words which are located around a specific citation. We retrieved the
citation context from Microsoft Academic. To interpret and compare the results
of the new network type, we generated two further networks: co-occurrence
networks which are based on title and abstract keywords from (1) EG's papers
and (2) the papers citing EG's publications. The comparison of the three
networks suggests that papers of EG and citation contexts of papers citing EG
are semantically more closely related to each other than to titles and
abstracts of papers citing EG. This result accords with the use of citations in
research evaluation that is based on the premise that citations reflect the
cognitive influence of the cited on the citing publication.
"
1919,"Creating an A Cappella Singing Audio Dataset for Automatic Jingju
  Singing Evaluation Research","  The data-driven computational research on automatic jingju (also known as
Beijing or Peking opera) singing evaluation lacks a suitable and comprehensive
a cappella singing audio dataset. In this work, we present an a cappella
singing audio dataset which consists of 120 arias, accounting for 1265 melodic
lines. This dataset is also an extension our existing CompMusic jingju corpus.
Both professional and amateur singers were invited to the dataset recording
sessions, and the most common jingju musical elements have been covered. This
dataset is also accompanied by metadata per aria and melodic line annotated for
automatic singing evaluation research purpose. All the gathered data is openly
available online.
"
1920,"Reference Publication Year Spectroscopy (RPYS) of Eugene Garfield's
  publications","  Which studies, theories, and ideas have influenced Eugene Garfield's
scientific work? Recently, the method reference publication year spectroscopy
(RPYS) has been introduced, which can be used to answer this and related
questions. Since then, several studies have been published dealing with the
historical roots of research fields and scientists. The program CRExplorer
(http://www.crexplorer.net) was specifically developed for RPYS. In this study,
we use this program to investigate the historical roots of Eugene Garfield's
oeuvre.
"
1921,"Faculty citation measures are highly correlated with peer assessment of
  computer science doctoral programs","  We study relationship between peer assessment of quality of U.S. Computer
Science (CS) doctoral programs and objective measures of research strength of
those programs. In Fall 2016 we collected Google Scholar citation data for
4,352 tenure-track CS faculty from 173 U.S. universities. The citations are
measured by the t10 index, which represents the number of citations received by
the 10th highest cited paper of a faculty. To measure the research strength of
a CS doctoral program we use 2 groups of citation measures. The first group of
measures averages t10 of faculty in a program. Pearson correlation of those
measures with the peer assessment of U.S. CS doctoral programs published by the
U.S. News in 2014 is as high as 0.890. The second group of measures counts the
number of well cited faculty in a program. Pearson correlation of those
measures with the peer assessment is as high as 0.909. By combining those two
groups of measures using linear regression, we create the Scholar score whose
Pearson correlation with the peer assessment is 0.933 and which explains 87.2%
of the variance in the peer assessment. Our evaluation shows that the highest
62 ranked CS doctoral programs by the U.S. News peer assessment are much higher
correlated with the Scholar score than the next 57 ranked programs, indicating
the deficiencies of peer assessment of less-known CS programs. Our results also
indicate that university reputation might have a sizeable impact on peer
assessment of CS doctoral programs. To promote transparency, the raw data and
the codes used in this study are made available to research community at
http://www.dabi.temple.edu/~vucetic/CSranking/.
"
1922,"University Twitter Engagement: Using Twitter Followers to Rank
  Universities","  We examine and rank a set of 264 U.S. universities extracted from the
National Collegiate Athletic Association (NCAA) Division I membership and
global lists published in U.S. News, Times Higher Education, Academic Ranking
of World Universities, and Money Magazine. Our University Twitter Engagement
(UTE) rank is based on the friend and extended follower network of primary and
affiliated secondary Twitter accounts referenced on a university's home page.
In rank-to-rank comparisons we observed a significant, positive rank
correlation ({\tau}=0.6018) between UTE and an aggregate reputation ranking
which indicates that UTE could be a viable proxy for ranking atypical
institutions normally excluded from traditional lists. In addition, we
significantly reduce the cost of data collection needed to rank each
institution by using only web-based artifacts and a publicly accessible Twitter
application programming interface (API).
"
1923,"Agent-based computing from multi-agent systems to agent-based Models: a
  visual survey","  Agent-Based Computing is a diverse research domain concerned with the
building of intelligent software based on the concept of ""agents"". In this
paper, we use Scientometric analysis to analyze all sub-domains of agent-based
computing. Our data consists of 1,064 journal articles indexed in the ISI web
of knowledge published during a twenty year period: 1990-2010. These were
retrieved using a topic search with various keywords commonly used in
sub-domains of agent-based computing. In our proposed approach, we have
employed a combination of two applications for analysis, namely Network
Workbench and CiteSpace - wherein Network Workbench allowed for the analysis of
complex network aspects of the domain, detailed visualization-based analysis of
the bibliographic data was performed using CiteSpace. Our results include the
identification of the largest cluster based on keywords, the timeline of
publication of index terms, the core journals and key subject categories. We
also identify the core authors, top countries of origin of the manuscripts
along with core research institutes. Finally, our results have interestingly
revealed the strong presence of agent-based computing in a number of
non-computing related scientific domains including Life Sciences, Ecological
Sciences and Social Sciences.
"
1924,"Prevalence and citation advantage of gold open access in the subject
  areas of the Scopus database","  The potential benefit of open access (OA) in relation to citation impact has
been discussed in the literature in depth. The methodology used to test the OA
citation advantage includes comparing OA vs. non-OA journal impact factors and
citations of OA versus non-OA articles published in the same non-OA journals.
However, one problem with many studies is that they are small -restricted to a
discipline or set of journals-. Moreover, conclusions are not entirely
consistent among research areas and 'early view' and 'selection bias' have been
suggested as possible explications. In the present paper, an analysis of gold
OA from across all areas of research -the 27 subject areas of the Scopus
database- is realized. As a novel contribution, this paper takes a
journal-level approach to assessing the OA citation advantage, whereas many
others take a paper-level approach. Data were obtained from Scimago Lab, sorted
using Scopus database, and tagged as OA/non-OA using the DOAJ list. Jointly
with the OA citation advantage, the OA prevalence as well as the differences
between access types (OA vs. non-OA) in production and referencing are tested.
A total of 3,737 OA journals (16.8%) and 18,485 non-OA journals (83.2%)
published in 2015 are considered. As the main conclusion, there is no
generalizable gold OA citation advantage at journal level.
"
1925,"Towards a Formal, Visual Framework of Emergent Cognitive Development of
  Scholars","  Understanding the cognitive evolution of researchers as they progress in the
academia is an important but complex problem, a problem belonging to a class of
problems, which often require the development of models for gaining further
understanding in the intricacies of the domain. The research question that we
address in this paper is how to effectively model this temporal cognitive
mental development of prolific researchers. Our proposed solution to this
problem is based on noting that the academic progression and notability of a
researcher are linked with a progressive increase in the citation count for the
scholar's refereed publications quantified using indices such as the Hirsch
index. In other words, we propose the use of yearly cognitive increment of a
scholar's cognition to be quantifiable using a function of the scholar's
citation index, thereby considering the index as an indicator of the discrete
approximation of the scholar's cognitive development. Using validated
agent-based modeling, a paradigm presented as part of our previous work i.e.
Cognitive Agent-based Computing framework, we present both formal as well as
visual agent-based complex network representations for this cognitive evolution
in the form of a Temporal Cognitive Level Network (TCLN) model. As a proof of
the effectiveness of this approach, we demonstrate validation of the model
using historic data of citations.
"
1926,"Some variations on the standard theoretical models for the h-index: A
  comparative analysis","  There are various mathematical models proposed in the recent literature for
estimating the h-index through bibliometric measures, such as number of
articles (P) and citations received (C). These models have been previously
empirically tested assuming a mathematical model and predetermining the models
parameter values at some fixed constant. Here, by adopting a statistical
modelling view I investigate alternative distributions commonly used for this
type of point data. I also show that the typical assumptions for the parameters
of the h-index mathematical models in such representations are not always
realistic, with more suitable specifications being favorable. Prediction of the
hindex is also demonstrated.
"
1927,"Comparison of the h-index for Different Fields of Research Using
  Bootstrap Methodology","  An important disadvantage of the h-index is that typically it cannot take
into account the specific field of research of a researcher. Usually sample
point estimates of the average and median h-index values for the various fields
are reported that are highly variable and dependent of the specific samples and
it would be useful to provide confidence intervals of prediction accuracy. In
this paper we apply the non-parametric bootstrap technique for constructing
confidence intervals for the h-index for different fields of research. In this
way no specific assumptions about the distribution of the empirical hindex are
required as well as no large samples since that the methodology is based on
resampling from the initial sample. The results of the analysis showed
important differences between the various fields. The performance of the
bootstrap intervals for the mean and median h-index for most fields seems to be
rather satisfactory as revealed by the performed simulation.
"
1928,Carnot Efficiency of Publication,"  This paper analyzes publication efficiency in terms of Hirsch-index or
h-index and total citations, with an analogy to the Carnot efficiency used in
thermodynamics. Such publication efficiency, with typical value of 30%, can be
utilized to normalize the research output judgment, favoring quality outputs in
reduced quantity, which is currently lacking in many discipline.
"
1929,Categorizing Hirsch Index Variants,"  Utilizing the Hirsch index h and some of its variants for an exploratory
factor analysis we discuss whether one of the most important Hirsch-type
indices, namely the g-index comprises information about not only the size of
the productive core but also the impact of the papers in the core. We also
study the effect of logarithmic and square-root transformation of the data
utilized in the factor analysis. To demonstrate our approach we use a real data
example analysing the citation records of 26 physicists compiled from the Web
of Science.
"
1930,"Unraveling the dynamics of growth, aging and inflation for citations to
  scientific articles from specific research fields","  We analyze the time evolution of citations acquired by articles from journals
of the American Physical Society (PRA, PRB, PRC, PRD, PRE and PRL). The
observed change over time in the number of papers published in each journal is
considered an exogenously caused variation in citability that is accounted for
by a normalization. The appropriately inflation-adjusted citation rates are
found to be separable into a preferential-attachment-type growth kernel and a
purely obsolescence-related (i.e., monotonously decreasing as a function of
time since publication) aging function. Variations in the empirically extracted
parameters of the growth kernels and aging functions associated with different
journals point to research-field-specific characteristics of citation intensity
and knowledge flow. Comparison with analogous results for the citation dynamics
of technology-disaggregated cohorts of patents provides deeper insight into the
basic principles of information propagation as indicated by citing behavior.
"
1931,"Rich Semantic Models and Knowledgebases for Highly-Structured Scientific
  Communication","  Rather than using text for scientific research reports, we have proposed
developing highly-structured reports with rich semantic models. In this paper,
we consider detailed structures for the components of research reports using a
modeling framework based on a rigorous upper ontology. For instance, we
consider the use of structured descriptions of Research Designs to support
evaluation of internal and external validity. In addition, collections of
highly-structured scientific research reports would be the key component of a
set of evolving and interlocking highly-structured scientific knowledgebases.
"
1932,"How the notion of ACCESS guides the organization of a European research
  infrastructure: the example of DARIAH","  This contribution will show how Access play a strong role in the creation and
structuring of DARIAH, a European Digital Research Infrastructure in Arts and
Humanities.To achieve this goal, this contribution will develop the concept of
Access from five examples: Interdisciplinarity point of view, Manage
contradiction between national and international perspectives, Involve
different communities (not only researchers stakeholders), Manage tools and
services, Develop and use new collaboration tools. We would like to demonstrate
that speaking about Access always implies a selection, a choice, even in the
perspective of ""Open Access"".
"
1933,"Global picture of OAI-PMH repositories through the analysis of 6 key
  open archive meta-catalogs","  By the end of the late 90's the Open Archives Initiative needed direction to
insure its improvement and thus, created the Open Archives Initiative Protocol
for Metadata Harvesting (OAI-PMH) standard. The movement showed a rise in
popularity, followed by a decline then a relative stabilization. This process
was essentially a way to ensure the viability of open archive repositories.
However, a meta-catalog containing an ensemble of repositories was never
established, which lead to confusion of what could be found in said catalogs.
  This study ultimately aims to find out what repository content can be found
and where with the use of the 6 key meta-catalogs. Although they undoubtedly
have numerous limitations pertaining to the available data, this article seeks
to compare the common data in each meta-catalog and estimates which
repositories are found within them (with approx. less than 1% in common within
the 6 meta-catalog).
  Decisively, this paper identifies the need to collate this data (with a total
of 42.3% OAI-PMH repositories specific to each meta-catalog) and improve
current search tools, hence portraying the benefits of a comprehensible single
unifying meta-catalog for end users.
"
1934,Reliable Granular References to Changing Linked Data,"  Nanopublications are a concept to represent Linked Data in a granular and
provenance-aware manner, which has been successfully applied to a number of
scientific datasets. We demonstrated in previous work how we can establish
reliable and verifiable identifiers for nanopublications and sets thereof.
Further adoption of these techniques, however, was probably hindered by the
fact that nanopublications can lead to an explosion in the number of triples
due to auxiliary information about the structure of each nanopublication and
repetitive provenance and metadata. We demonstrate here that this significant
overhead disappears once we take the version history of nanopublication
datasets into account, calculate incremental updates, and allow users to deal
with the specific subsets they need. We show that the total size and overhead
of evolving scientific datasets is reduced, and typical subsets that
researchers use for their analyses can be referenced and retrieved efficiently
with optimized precision, persistence, and reliability.
"
1935,The PCI Ontology,"  In this paper, an ontology for the description and indexing of the contents
of audiovisual resources will be presented. The concerned domain of reference
(or: knowledge domain) is the cultural heritage of minorities and indigenous
people, hence the name or title of this ontology, PCI ontology (standing for
the French ""Patrimoine Culturel de Minorit\'es et Peuples Indig\`enes"").
"
1936,Clinical Decision Support Systems: A Visual Survey,"  Clinical Decision Support Systems (CDSS) form an important area of research.
In spite of its importance, it is difficult for researchers to evaluate the
domain primarily because of a considerable spread of relevant literature in
interdisciplinary domains. Previous surveys of CDSS have examined the domain
from the perspective of individual disciplines. However, to the best of our
knowledge, no visual scientometric survey of CDSS has previously been conducted
which provides a broader spectrum of the domain with a horizon covering
multiple disciplines. While traditional systematic literature surveys focus on
analyzing literature using arbitrary results, visual surveys allow for the
analysis of domains by using complex network-based analytical models. In this
paper, we present a detailed visual survey of CDSS literature using important
papers selected from highly cited sources in the Thomson Reuters web of
science. We analyze the entire set of relevant literature indexed in the Web of
Science database. Our key results include the discovery of the articles which
have served as key turning points in literature. Additionally, we have
identified highly cited authors and the key country of origin of top
publications. We also present the Universities with the strongest citation
bursts. Finally, our network analysis has also identified the key journals and
subject categories both in terms of centrality and frequency. It is our belief
that this paper will thus serve as an important role for researchers as well as
clinical practitioners interested in identifying key literature and resources
in the domain of clinical decision support.
"
1937,Design and Analysis of the NIPS 2016 Review Process,"  Neural Information Processing Systems (NIPS) is a top-tier annual conference
in machine learning. The 2016 edition of the conference comprised more than
2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents
a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and
over 100% in terms of attendees as compared to the previous year. The massive
scale as well as rapid growth of the conference calls for a thorough quality
assessment of the peer-review process and novel means of improvement. In this
paper, we analyze several aspects of the data collected during the review
process, including an experiment investigating the efficacy of collecting
ordinal rankings from reviewers. Our goal is to check the soundness of the
review process, and provide insights that may be useful in the design of the
review process of subsequent conferences.
"
1938,"Implementation and Evaluation of a Framework to calculate Impact
  Measures for Wikipedia Authors","  Wikipedia, an open collaborative website, can be edited by anyone, even
anonymously, thus becoming victim to ill-intentioned changes. Therefore,
ranking Wikipedia authors by calculating impact measures based on the edit
history can help to identify reputational users or harmful activity such as
vandalism \cite{Adler:2008:MAC:1822258.1822279}. However, processing millions
of edits on one system can take a long time. The author implements an open
source framework to calculate such rankings in a distributed way (MapReduce)
and evaluates its performance on various sized datasets. A reimplementation of
the contribution measures by \citeauthor{Adler:2008:MAC:1822258.1822279}
demonstrates its extensibility and usability, as well as problems of handling
huge datasets and their possible resolutions. The results put different
performance optimizations into perspective and show that horizontal scaling can
decrease the total processing time.
"
1939,A Collaborative Approach to Computational Reproducibility,"  Although a standard in natural science, reproducibility has been only
episodically applied in experimental computer science. Scientific papers often
present a large number of tables, plots and pictures that summarize the
obtained results, but then loosely describe the steps taken to derive them. Not
only can the methods and the implementation be complex, but also their
configuration may require setting many parameters and/or depend on particular
system configurations. While many researchers recognize the importance of
reproducibility, the challenge of making it happen often outweigh the benefits.
Fortunately, a plethora of reproducibility solutions have been recently
designed and implemented by the community. In particular, packaging tools
(e.g., ReproZip) and virtualization tools (e.g., Docker) are promising
solutions towards facilitating reproducibility for both authors and reviewers.
To address the incentive problem, we have implemented a new publication model
for the Reproducibility Section of Information Systems Journal. In this
section, authors submit a reproducibility paper that explains in detail the
computational assets from a previous published manuscript in Information
Systems.
"
1940,Effectiveness of Anonymization in Double-Blind Review,"  Double-blind review relies on the authors' ability and willingness to
effectively anonymize their submissions. We explore anonymization effectiveness
at ASE 2016, OOPSLA 2016, and PLDI 2016 by asking reviewers if they can guess
author identities. We find that 74%-90% of reviews contain no correct guess and
that reviewers who self-identify as experts on a paper's topic are more likely
to attempt to guess, but no more likely to guess correctly. We present our
findings, summarize the PC chairs' comments about administering double-blind
review, discuss the advantages and disadvantages of revealing author identities
part of the way through the process, and conclude by advocating for the
continued use of double-blind review.
"
1941,On-the-fly Historical Handwritten Text Annotation,"  The performance of information retrieval algorithms depends upon the
availability of ground truth labels annotated by experts. This is an important
prerequisite, and difficulties arise when the annotated ground truth labels are
incorrect or incomplete due to high levels of degradation. To address this
problem, this paper presents a simple method to perform on-the-fly annotation
of degraded historical handwritten text in ancient manuscripts. The proposed
method aims at quick generation of ground truth and correction of inaccurate
annotations such that the bounding box perfectly encapsulates the word, and
contains no added noise from the background or surroundings. This method will
potentially be of help to historians and researchers in generating and
correcting word labels in a document dynamically. The effectiveness of the
annotation method is empirically evaluated on an archival manuscript collection
from well-known publicly available datasets.
"
1942,Extracting data from vector figures in scholarly articles,"  It is common for authors to communicate their results in graphical figures,
but those data are frequently unavailable for reanalysis. Reconstructing data
points from a figure manually requires the author to measure the coordinates
either on printed pages using a ruler, or from the display screen using a
cursor. This is time-consuming (often hours) and error-prone, and limited by
the precision of the display or ruler. What is often not realised is that the
data themselves are held in the PDF document to much higher precision (usually
0.0-0.01 pixels), if the figure is stored in vector format. We developed alpha
software to automatically reconstruct data from vector figures and tested it on
funnel plots in the meta-analysis literature. Our results indicate that
reconstructing data from vector based figures is promising, where we correctly
extracted data for 12 out of 24 funnel plots with extracted data (50%).
However, we observed that vector based figures are relatively sparse (15 out of
136 papers with funnel plots) and strongly insist publishers to provide more
vector based data figures in the near future for the benefit of the scholarly
community.
"
1943,"Large Teams Have Developed Science and Technology; Small Teams Have
  Disrupted It","  Teams dominate the production of high-impact science and technology.
Analyzing teamwork from more than 50 million papers, patents, and software
products, 1954-2014, we demonstrate across this period that larger teams
developed recent, popular ideas, while small teams disrupted the system by
drawing on older and less prevalent ideas. Attention to work from large teams
came immediately, while advances by small teams succeeded further into the
future. Differences between small and large teams magnify with impact - small
teams have become known for disruptive work and large teams for developing
work. Differences in topic and re- search design account for part of the
relationship between team size and disruption, but most of the effect occurs
within people, controlling for detailed subject and article type. Our findings
suggest the importance of supporting both small and large teams for the
sustainable vitality of science and technology.
"
1944,"Analysing Scientific Collaborations of New Zealand Institutions using
  Scopus Bibliometric Data","  Scientific collaborations are among the main enablers of development in small
national science systems. Although analysing scientific collaborations is a
well-established subject in scientometrics, evaluations of scientific
collaborations within a country remain speculative with studies based on a
limited number of fields or using data too inadequate to be representative of
collaborations at a national level. This study represents a unique view on the
collaborative aspect of scientific activities in New Zealand. We perform a
quantitative study based on all Scopus publications in all subjects for more
than 1500 New Zealand institutions over a period of 6 years to generate an
extensive mapping of scientific collaboration at a national level. The
comparative results reveal the level of collaboration between New Zealand
institutions and business enterprises, government institutions, higher
education providers, and private not for profit organisations in 2010-2015.
Constructing a collaboration network of institutions, we observe a power-law
distribution indicating that a small number of New Zealand institutions account
for a large proportion of national collaborations. Network centrality concepts
are deployed to identify the most central institutions of the country in terms
of collaboration. We also provide comparative results on 15 universities and
Crown research institutes based on 27 subject classifications.
"
1945,"Algorithmically generated subject categories based on citation
  relations: An empirical micro study using papers on overall water splitting","  One important reason for the use of field categorization in bibliometrics is
the necessity to make citation impact of papers published in different
scientific fields comparable with each other. Raw citations are normalized by
using field-normalization schemes to achieve comparable citation scores. There
are different approaches to field categorization available. They can be broadly
classified as intellectual and algorithmic approaches. A paper-based
algorithmically constructed classification system (ACCS) was proposed which is
based on citation relations. Using a few ACCS field-specific clusters, we
investigate the discriminatory power of the ACCS. The micro study focusses on
the topic ""overall water splitting"" and related topics. The first part of the
study investigates intellectually whether the ACCS is able to identify papers
on overall water splitting reliably and validly. Next, we compare the ACCS with
(1) a paper-based intellectual (INSPEC) classification and (2) a journal-based
intellectual classification (Web of Science, WoS, subject categories). In the
last part of our case study, we compare the average number of citations in
selected ACCS clusters (on overall water splitting and related topics) with the
average citation count of publications in WoS subject categories related to
these clusters. The results of this micro study question the discriminatory
power of the ACCS. We recommend larger follow-up studies on broad datasets.
"
1946,"Matrix and Graph Operations for Relationship Inference: An Illustration
  with the Kinship Inference in the China Biographical Database","  Biographical databases contain diverse information about individuals. Person
names, birth information, career, friends, family and special achievements are
some possible items in the record for an individual. The relationships between
individuals, such as kinship and friendship, provide invaluable insights about
hidden communities which are not directly recorded in databases. We show that
some simple matrix and graph-based operations are effective for inferring
relationships among individuals, and illustrate the main ideas with the China
Biographical Database (CBDB).
"
1947,Quantifying patterns of research interest evolution,"  Our quantitative understanding of how scientists choose and shift their
research focus over time is highly consequential, because it affects the ways
in which scientists are trained, science is funded, knowledge is organized and
discovered, and excellence is recognized and rewarded. Despite extensive
investigations of various factors that influence a scientist's choice of
research topics, quantitative assessments of mechanisms that give rise to
macroscopic patterns characterizing research interest evolution of individual
scientists remain limited. Here we perform a large-scale analysis of
publication records, finding that research interest change follows a
reproducible pattern characterized by an exponential distribution. We identify
three fundamental features responsible for the observed exponential
distribution, which arise from a subtle interplay between exploitation and
exploration in research interest evolution. We develop a random walk based
model, allowing us to accurately reproduce the empirical observations. This
work presents a quantitative analysis of macroscopic patterns governing
research interest change, discovering a high degree of regularity underlying
scientific research and individual careers.
"
1948,Research Portfolio Analysis and Topic Prominence,"  Stakeholders in the science system need to decide where to place their bets.
Example questions include: Which areas of research should get more funding? Who
should we hire? Which projects should we abandon and which new projects should
we start? Making informed choices requires knowledge about these research
options. Unfortunately, to date research portfolio options have not been
defined in a consistent, transparent and relevant manner. Furthermore, we don't
know how to define demand for these options. In this article, we address the
issues of consistency, transparency, relevance and demand by using a model of
science consisting of 91,726 topics (or research options) that contain over 58
million documents. We present a new indicator of topic prominence - a measure
of visibility, momentum and, ultimately, demand. We assign over $203 billion of
project-level funding data from STAR METRICS to individual topics in science,
and show that the indicator of topic prominence, explains over one-third of the
variance in current (or future) funding by topic. We also show that highly
prominent topics receive far more funding per researcher than topics that are
not prominent. Implications of these results for research planning and
portfolio analysis by institutions and researchers are emphasized.
"
1949,Network Classification and Categorization,"  To the best of our knowledge, this paper presents the first large-scale study
that tests whether network categories (e.g., social networks vs. web graphs)
are distinguishable from one another (using both categories of real-world
networks and synthetic graphs). A classification accuracy of $94.2\%$ was
achieved using a random forest classifier with both real and synthetic
networks. This work makes two important findings. First, real-world networks
from various domains have distinct structural properties that allow us to
predict with high accuracy the category of an arbitrary network. Second,
classifying synthetic networks is trivial as our models can easily distinguish
between synthetic graphs and the real-world networks they are supposed to
model.
"
1950,"SKOS Concepts and Natural Language Concepts: an Analysis of Latent
  Relationships in KOSs","  The vehicle to represent Knowledge Organization Systems (KOSs) in the
environment of the Semantic Web and linked data is the Simple Knowledge
Organization System (SKOS). SKOS provides a way to assign a URI to each
concept, and this URI functions as a surrogate for the concept. This fact makes
of main concern the need to clarify the URIs' ontological meaning. The aim of
this study is to investigate the relation between the ontological substance of
KOS concepts and concepts revealed through the grammatical and syntactic
formalisms of natural language. For this purpose, we examined the dividableness
of concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a
classification scheme) by applying Natural Language Processing (NLP) techniques
(i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF
literals) of SKOS concepts. The results of the comparative analysis reveal
that, despite the use of multi-word units, thesauri tend to represent concepts
in a way that can hardly be further divided conceptually, while Subject
Headings and Classification Schemes - to a certain extent - comprise terms that
can be decomposed into more conceptual constituents. Consequently, SKOS
concepts deriving from thesauri are more likely to represent atomic conceptual
units and thus be more appropriate tools for inference and reasoning. Since
identifiers represent the meaning of a concept, complex concepts are neither
the most appropriate nor the most efficient way of modelling a KOS for the
Semantic Web.
"
1951,"Character Distributions of Classical Chinese Literary Texts: Zipf's Law,
  Genres, and Epochs","  We collect 14 representative corpora for major periods in Chinese history in
this study. These corpora include poetic works produced in several dynasties,
novels of the Ming and Qing dynasties, and essays and news reports written in
modern Chinese. The time span of these corpora ranges between 1046 BCE and 2007
CE. We analyze their character and word distributions from the viewpoint of the
Zipf's law, and look for factors that affect the deviations and similarities
between their Zipfian curves. Genres and epochs demonstrated their influences
in our analyses. Specifically, the character distributions for poetic works of
between 618 CE and 1644 CE exhibit striking similarity. In addition, although
texts of the same dynasty may tend to use the same set of characters, their
character distributions still deviate from each other.
"
1952,"Flexible Computing Services for Comparisons and Analyses of Classical
  Chinese Poetry","  We collect nine corpora of representative Chinese poetry for the time span of
1046 BCE and 1644 CE for studying the history of Chinese words, collocations,
and patterns. By flexibly integrating our own tools, we are able to provide new
perspectives for approaching our goals. We illustrate the ideas with two
examples. The first example show a new way to compare word preferences of
poets, and the second example demonstrates how we can utilize our corpora in
historical studies of the Chinese words. We show the viability of the tools for
academic research, and we wish to make it helpful for enriching existing
Chinese dictionary as well.
"
1953,"The geography of references in elite articles: What countries contribute
  to the archives of knowledge","  This study is intended to find an answer for the question on which national
""shoulders"" the worldwide top-level research stands. Traditionally, national
scientific standings are evaluated in terms of the number of citations to their
papers. We raise a different question: instead of analyzing the citations to
the countries' articles (the forward view), we examine referenced publications
from specific countries cited in the most elite publications (the
backward-citing-view). ""Elite publications"" are operationalized as the top-1%
most-highly cited articles. Using the articles published during the years 2004
to 2013, we examine the research referenced in these works. Our results confirm
the well-known fact that China has emerged to become a major player in science.
However, China still belongs to the low contributors when countries are ranked
as contributors to the cited references in top-1% articles. Using this
perspective, the results do not point to a decreasing trend for the USA; in
fact, the USA exceeds expectations (compared to its publication share) in terms
of contributions to cited references in the top-1% articles. Switzerland,
Sweden, and the Netherlands also are shown at the top of the list. However, the
results for Germany are lower than statistically expected.
"
1954,Robust clustering of languages across Wikipedia growth,"  Wikipedia is the largest existing knowledge repository that is growing on a
genuine crowdsourcing support. While the English Wikipedia is the most
extensive and the most researched one with over five million articles,
comparatively little is known about the behavior and growth of the remaining
283 smaller Wikipedias, the smallest of which, Afar, has only one article. Here
we use a subset of this data, consisting of 14962 different articles, each of
which exists in 26 different languages, from Arabic to Ukrainian. We study the
growth of Wikipedias in these languages over a time span of 15 years. We show
that, while an average article follows a random path from one language to
another, there exist six well-defined clusters of Wikipedias that share common
growth patterns. The make-up of these clusters is remarkably robust against the
method used for their determination, as we verify via four different clustering
methods. Interestingly, the identified Wikipedia clusters have little
correlation with language families and groups. Rather, the growth of Wikipedia
across different languages is governed by different factors, ranging from
similarities in culture to information literacy.
"
1955,The arXiv of the future will not look like the arXiv,"  The arXiv is the most popular preprint repository in the world. Since its
inception in 1991, the arXiv has allowed researchers to freely share
publication-ready articles prior to formal peer review. The growth and the
popularity of the arXiv emerged as a result of new technologies that made
document creation and dissemination easy, and cultural practices where
collaboration and data sharing were dominant. The arXiv represents a unique
place in the history of research communication and the Web itself, however it
has arguably changed very little since its creation. Here we look at the
strengths and weaknesses of arXiv in an effort to identify what possible
improvements can be made based on new technologies not previously available.
Based on this, we argue that a modern arXiv might in fact not look at all like
the arXiv of today.
"
1956,"A spatial scientometric analysis of the publication output of cities
  worldwide","  In tandem with the rapid globalisation of science, spatial scientometrics has
become an important research sub-field in scientometric studies. Recently,
numerous spatial scientometric contributions have focused on the examination of
cities' scientific output by using various scientometric indicators. In this
paper, I analyse cities' scientific output worldwide in terms of the number of
journal articles indexed by the Scopus database, in the period from 1986 to
2015. Furthermore, I examine which countries are the most important
collaborators of cities. Finally, I identify the most productive disciplines in
each city. I use GPS Visualizer to illustrate the scientometric data of nearly
2,200 cities on maps. Results show that cities with the highest scientific
output are mostly located in developed countries and China. Between 1986 and
2015, the greatest number of scientific articles were created in Beijing. The
international hegemony of the United States in science has been described by
many studies, and is also reinforced by the fact that the United States is the
most important collaborator to more than 75 percent of all cities. Medicine is
the most productive discipline in two-thirds of cities. Furthermore, cities
having the highest scientific output in specific disciplines show well-defined
geographical patterns.
"
1957,Mining User Queries with Information Extraction Methods and Linked Data,"  Purpose: Advanced usage of Web Analytics tools allows to capture the content
of user queries. Despite their relevant nature, the manual analysis of large
volumes of user queries is problematic. This paper demonstrates the potential
of using information extraction techniques and Linked Data to gather a better
understanding of the nature of user queries in an automated manner.
  Design/methodology/approach: The paper presents a large-scale case-study
conducted at the Royal Library of Belgium consisting of a data set of 83 854
queries resulting from 29 812 visits over a 12 month period of the historical
newspapers platform BelgicaPress. By making use of information extraction
methods, knowledge bases and various authority files, this paper presents the
possibilities and limits to identify what percentage of end users are looking
for person and place names.
  Findings: Based on a quantitative assessment, our method can successfully
identify the majority of person and place names from user queries. Due to the
specific character of user queries and the nature of the knowledge bases used,
a limited amount of queries remained too ambiguous to be treated in an
automated manner.
  Originality/value: This paper demonstrates in an empirical manner both the
possibilities and limits of gaining more insights from user queries extracted
from a Web Analytics tool and analysed with the help of information extraction
tools and knowledge bases. Methods and tools used are generalisable and can be
reused by other collection holders.
"
1958,"Trends in scientific research in Online Information Review. Part 1.
  Production, impact and research collaboration","  The study, based on the Web of Science, analyses 758 articles published from
2000 to 2014. Our analysis includes the publications' output, authorship,
institutional and country patterns of production, citations and collaboration.
A Social Network Analysis was conducted to identify primary groups of
researchers and institutions and the collaboration between countries. The study
reveals that 1097 authors and 453 Institutions have contributed to the journal.
The collaboration index has increased progressively, and the average degree of
collaboration during the study period was 1.98. The majority of the papers were
contributed by professionals affiliated with a university. Highly cited papers
address online and digital environments, e-learning systems, mobile services,
web 2.0 and citation analyses. This work is a bibliometric analysis of a
leading journal in library and information science, Online Information Review.
"
1959,"Trends in scientific research in Online Information Review. Part 2.
  Mapping the scientific knowledge through bibliometric and social network
  analyses","  Objective. The purpose of this work is to analyse the knowledge structure and
trends in scientific research in the Online Information Reviews journal by
bibliometric analysis of key words and social network analysis of co-words.
Methods. Key words included in a set of 758 papers included in the Web of
Science database from 2000 to 2014 were analysed. We conducted a subject
analysis considering the key words assigned to papers. A social network
analysis was also conducted to identify the number of co-occurrences between
key words (co-words). The Pajek software was used to create and graphically
visualize the networks. Results. Internet is the most frequent key word (n=219)
and the most central in the network of co-words, strongly associated with
Information retrieval, search engines, the World Wide Web, libraries and users
Conclusions. Information science, as represented by Online Information Review
in the present study, is an evolving discipline that draws on literature from a
relatively wide range of subjects. Although Online Information Review appears
to have well-defined and established research topics, the journal also changes
rapidly to embrace new lines of research.
"
1960,Identitas: A Better Way To Be Meaningless,"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
"
1961,Integration of Japanese Papers Into the DBLP Data Set,"  If someone is looking for a certain publication in the field of computer
science, the searching person is likely to use the DBLP to find the desired
publication. The DBLP data set is continuously extended with new publications,
or rather their metadata, for example the names of involved authors, the title
and the publication date. While the size of the data set is already remarkable,
specific areas can still be improved. The DBLP offers a huge collection of
English papers because most papers concerning computer science are published in
English. Nevertheless, there are official publications in other languages which
are supposed to be added to the data set. One kind of these are Japanese
papers. This diploma thesis will show a way to automatically process
publication lists of Japanese papers and to make them ready for an import into
the DBLP data set. Especially important are the problems along the way of
processing, such as transcription handling and Personal Name Matching with
Japanese names.
"
1962,A Longitudinal Assessment of the Persistence of Twitter Datasets,"  With social media datasets being increasingly shared by researchers, it also
presents the caveat that those datasets are not always completely replicable.
Having to adhere to requirements of platforms like Twitter, researchers cannot
release the raw data and instead have to release a list of unique identifiers,
which others can then use to recollect the data from the platform themselves.
This leads to the problem that subsets of the data may no longer be available,
as content can be deleted or user accounts deactivated. To quantify the impact
of content deletion in the replicability of datasets in a long term, we perform
a longitudinal analysis of the persistence of 30 Twitter datasets, which
include over 147 million tweets. Having the original datasets collected between
2012 and 2016, and recollecting them later by using the tweet IDs, we look at
four different factors that quantify the extent to which recollected datasets
resemble original ones: completeness, representativity, similarity and
changingness. Even though the ratio of available tweets keeps decreasing as the
dataset gets older, we find that the textual content of the recollected subset
is still largely representative of the whole dataset that was originally
collected. The representativity of the metadata, however, keeps decreasing over
time, both because the dataset shrinks and because certain metadata, such as
the users' number of followers, keeps changing. Our study has important
implications for researchers sharing and using publicly shared Twitter datasets
in their research.
"
1963,"A Literature Based Approach to Define the Scope of Biomedical
  Ontologies: A Case Study on a Rehabilitation Therapy Ontology","  In this article, we investigate our early attempts at building an ontology
describing rehabilitation therapies following brain injury. These therapies are
wide-ranging, involving interventions of many different kinds. As a result,
these therapies are hard to describe. As well as restricting actual practice,
this is also a major impediment to evidence-based medicine as it is hard to
meaningfully compare two treatment plans.
  Ontology development requires significant effort from both ontologists and
domain experts. Knowledge elicited from domain experts forms the scope of the
ontology. The process of knowledge elicitation is expensive, consumes experts'
time and might have biases depending on the selection of the experts. Various
methodologies and techniques exist for enabling this knowledge elicitation,
including community groups and open development practices. A related problem is
that of defining scope. By defining the scope, we can decide whether a concept
(i.e. term) should be represented in the ontology. This is the opposite of
knowledge elicitation, in the sense that it defines what should not be in the
ontology. This can be addressed by pre-defining a set of competency questions.
  These approaches are, however, expensive and time-consuming. Here, we
describe our work toward an alternative approach, bootstrapping the ontology
from an initially small corpus of literature that will define the scope of the
ontology, expanding this to a set covering the domain, then using information
extraction to define an initial terminology to provide the basis and the
competencies for the ontology. Here, we discuss four approaches to building a
suitable corpus that is both sufficiently covering and precise.
"
1964,Scaling Author Name Disambiguation with CNF Blocking,"  An author name disambiguation (AND) algorithm identifies a unique author
entity record from all similar or same publication records in scholarly or
similar databases. Typically, a clustering method is used that requires
calculation of similarities between each possible record pair. However, the
total number of pairs grows quadratically with the size of the author database
making such clustering difficult for millions of records. One remedy for this
is a blocking function that reduces the number of pairwise similarity
calculations. Here, we introduce a new way of learning blocking schemes by
using a conjunctive normal form (CNF) in contrast to the disjunctive normal
form (DNF). We demonstrate on PubMed author records that CNF blocking reduces
more pairs while preserving high pairs completeness compared to the previous
methods that use a DNF with the computation time significantly reduced. Thus,
these concepts in scholarly data can be better represented with CNFs. Moreover,
we also show how to ensure that the method produces disjoint blocks so that the
rest of the AND algorithm can be easily paralleled. Our CNF blocking tested on
the entire PubMed database of 80 million author mentions efficiently removes
82.17% of all author record pairs in 10 minutes.
"
1965,Bounded Rationality in Scholarly Knowledge Discovery,"  In an information-rich world, people's time and attention must be divided
among rapidly changing information sources and the diverse tasks demanded of
them. How people decide which of the many sources, such as scientific articles
or patents, to read and use in their own work affects dissemination of
scholarly knowledge and adoption of innovation. We analyze the choices people
make about what information to propagate on the citation networks of Physical
Review journals, US patents and legal opinions. We observe regularities in
behavior consistent with human bounded rationality: rather than evaluate all
available choices, people rely on simply cognitive heuristics to decide what
information to attend to. We demonstrate that these heuristics bias choices, so
that people preferentially propagate information that is easier to discover,
often because it is newer or more popular. However, we do not find evidence
that popular sources help to amplify the spread of information beyond making it
more salient. Our paper provides novel evidence of the critical role that
bounded rationality plays in the decisions to allocate attention in social
communication.
"
1966,Towards Open Data for the Citation Content Analysis,"  The paper presents first results of the CitEcCyr project funded by RANEPA.
The project aims to create a source of open citation data for research papers
written in Russian. Compared to existing sources of citation data, CitEcCyr is
working to provide the following added values: a) a transparent and distributed
architecture of a technology that generates the citation data; b) an openness
of all built/used software and created citation data; c) an extended set of
citation data sufficient for the citation content analysis; d) services for
public control over a quality of the citation data and a citing activity of
researchers.
"
1967,"Filling the citation gap: Measuring the multidimensional impact of the
  academic book at institutional level with PlumX","  More than five years after their emergence, altmetrics are still seen as a
promise to complement traditional citation-based indicators. However, no study
has focused on their potential usefulness to capture the impact of scholarly
books. While recent literature shows that citation indicators cannot fully
capture the impact of books, other studies have suggested alternative
indicators such as usage, publishers' prestige or library holdings. In this
paper, we calculate 18 indicators which range from altmetrics to library
holdings, views, downloads or citations to the production of monographs of a
Spanish university using the bibliometric suite PlumX from EBSCO. The objective
of the study is to adopt a multidimensional perspective on the analysis of
books and understand the level of complementarity between these different
indicators. Also, we compare the overview offered by this range of indicators
when applied to monographs with the traditional bibliometric perspective
focused on journal articles and citation impact. We observe a low presence of
altmetric indicators for monographs, even lower than for journal articles and a
predominance of library holdings, confirming this indicator as the most
promising one towards the analysis of the impact of books.
"
1968,Finding Talk About the Past in the Discourse of Non-Historians,"  A heightened interest in the presence of the past has given rise to the new
field of memory studies, but there is a lack of search and research tools to
support studying how and why the past is evoked in diachronic discourses.
Searching for temporal references is not straightforward. It entails bridging
the gap between conceptually-based information needs on one side, and
term-based inverted indexes on the other.
  Our approach enables the search for references to (intersubjective)
historical periods in diachronic corpora. It consists of a
semantically-enhanced search engine that is able to find references to many
entities at a time, which is combined with a novel interface that invites its
user to actively sculpt the search result set. Until now we have been concerned
mostly with user-friendly retrieval and selection of sources, but our tool can
also contribute to existing efforts to create reusable linked data from and for
research in the humanities.
"
1969,Automatic Structural Scene Digitalization,"  In this paper, we present an automatic system for the analysis and labeling
of structural scenes, floor plan drawings in Computer-aided Design (CAD)
format. The proposed system applies a fusion strategy to detect and recognize
various components of CAD floor plans, such as walls, doors, windows and other
ambiguous assets. Technically, a general rule-based filter parsing method is
fist adopted to extract effective information from the original floor plan.
Then, an image-processing based recovery method is employed to correct
information extracted in the first step. Our proposed method is fully automatic
and real-time. Such analysis system provides high accuracy and is also
evaluated on a public website that, on average, archives more than ten
thousands effective uses per day and reaches a relatively high satisfaction
rate.
"
1970,Eugene Garfield's Scholarly Impact: A Scientometric Review,"  The concept of citation indexing has become deeply involved in many parts of
research itself and the broad environment in which research plays an integral
role, ranging from research evaluation, numerous indicators, to an increasingly
wider range of scientific disciplines. In this article, we pay tribute to
Eugene Garfield and present a scientometric review of the intellectual assets
that he brought to us. In addition, we explore the intellectual landscape that
has subsequently evolved in connection to many of his ideas. We illustrate what
systematic reviews of the scientific literature may reveal and what we may
learn from the rich information conveyed through citation-induced patterns. The
study is conducted with CiteSpace, one of many science mapping tools based on
data from the Web of Science and Scopus. Without Garfield's inventions, none of
these would be possible.
"
1971,"Quantitative Perspectives on Fifty Years of the Journal of the History
  of Biology","  Journal of the History of Biology provides a fifty-year long record for
examining the evolution of the history of biology as a scholarly discipline. In
this paper, we present a new dataset and preliminary quantitative analysis of
the thematic content of JHB from the perspectives of geography, organisms, and
thematic fields. The geographic diversity of authors whose work appears in JHB
has increased steadily since 1968, but the geographic coverage of the content
of JHB articles remains strongly lopsided toward the United States, United
Kingdom, and western Europe and has diversified much less dramatically over
time. The taxonomic diversity of organisms discussed in JHB increased steadily
between 1968 and the late 1990s but declined in later years, mirroring broader
patterns of diversification previously reported in the biomedical research
literature. Finally, we used a combination of topic modeling and nonlinear
dimensionality reduction techniques to develop a model of multi-article fields
within JHB. We found evidence for directional changes in the representation of
fields on multiple scales. The diversity of JHB with regard to the
representation of thematic fields has increased overall, with most of that
diversification occurring in recent years. Drawing on the dataset generated in
the course of this analysis, as well as web services in the emerging digital
history and philosophy of science ecosystem, we have developed an interactive
web platform for exploring the content of JHB, and we provide a brief overview
of the platform in this article. As a whole, the data and analyses presented
here provide a starting-place for further critical reflection on the evolution
of the history of biology over the past half-century.
"
1972,"Characterizing in-text citations in scientific articles: A large-scale
  analysis","  We report characteristics of in-text citations in over five million full text
articles from two large databases - the PubMed Central Open Access subset and
Elsevier journals - as functions of time, textual progression, and scientific
field. The purpose of this study is to understand the characteristics of
in-text citations in a detailed way prior to pursuing other studies focused on
answering more substantive research questions. As such, we have analyzed
in-text citations in several ways and report many findings here. Perhaps most
significantly, we find that there are large field-level differences that are
reflected in position within the text, citation interval (or reference age),
and citation counts of references. In general, the fields of Biomedical and
Health Sciences, Life and Earth Sciences, and Physical Sciences and Engineering
have similar reference distributions, although they vary in their specifics.
The two remaining fields, Mathematics and Computer Science and Social Science
and Humanities, have different reference distributions from the other three
fields and between themselves. We also show that in all fields the numbers of
sentences, references, and in-text mentions per article have increased over
time, and that there are field-level and temporal differences in the numbers of
in-text mentions per reference. A final finding is that references mentioned
only once tend to be much more highly cited than those mentioned multiple
times.
"
1973,"Patent Citation Spectroscopy (PCS): Algorithmic retrieval of landmark
  patents","  One essential component in the construction of patent landscapes in
biomedical research and development (R&D) is identifying the most seminal
patents. Hitherto, the identification of seminal patents required subject
matter experts within biomedical areas. In this brief communication, we report
an analytical method and tool, Patent Citation Spectroscopy (PCS), for rapidly
identifying landmark patents in user-specified areas of biomedical innovation.
PCS mines the cited references within large sets of patents and provides an
estimate of the most historically impactful prior work. The efficacy of PCS is
shown in two case studies of biomedical innovation with clinical relevance: (1)
RNA interference and (2) cholesterol. PCS mined and analyzed 4,065 cited
references related to patents on RNA interference and correctly identified the
foundational patent of this technology, as independently reported by subject
matter experts on RNAi intellectual property. Secondly, PCS was applied to a
broad set of patents dealing with cholesterol - a case study chosen to reflect
a more general, as opposed to expert, patent search query. PCS mined through
11,326 cited references and identified the seminal patent as that for Lipitor,
the groundbreaking medication for treating high cholesterol as well as the pair
of patents underlying Repatha. These cases suggest that PCS provides a useful
method for identifying seminal patents in areas of biomedical innovation and
therapeutics. The interactive tool is free-to-use at: www.leydesdorff.net/pcs/.
"
1974,"Science and its significant other: Representing the humanities in
  bibliometric scholarship","  Bibliometrics offers a particular representation of science. Through
bibliometric methods a bibliometrician will always highlight particular
elements of publications, and through these elements operationalize particular
representations of science, while obscuring other possible representations from
view. Understanding bibliometrics as representation implies that a bibliometric
analysis is always performative: a bibliometric analysis brings a particular
representation of science into being that potentially influences the science
system itself. In this review we analyze the ways the humanities have been
represented throughout the history of bibliometrics, often in comparison to
other scientific domains or to a general notion of the sciences. Our review
discusses bibliometric scholarship between 1965 and 2016 that studies the
humanities empirically. We distinguish between two periods of bibliometric
scholarship. The first period, between 1965 and 1989, is characterized by a
sociological theoretical framework, the development and use of the Price index,
and small samples of journal publications as data sources. The second period,
from the mid-1980s up until the present day, is characterized by a new
hinterland, that of science policy and research evaluation, in which
bibliometric methods become embedded.
"
1975,"The number of linked references of publications in Microsoft Academic in
  comparison with the Web of Science","  In the context of a comprehensive Microsoft Academic (MA) study, we explored
in an initial step the quality of linked references data in MA in comparison
with Web of Science (WoS). Linked references are the backbone of bibliometrics,
because they are the basis of the times cited information in citation indexes.
We found that the concordance of linked references between MA and WoS ranges
from weak to non-existent for the full sample (publications of the University
of Zurich with less than 50 linked references in MA). An analysis with a sample
restricted to less than 50 linked references in WoS showed a strong agreement
between linked references in MA and WoS.
"
1976,"Is it reasonable to limit scientific coauthorship? There is no inflation
  of co-authors in Social Sciences and Education in Spain","  This paper analyzes the evolution of coauthorship in Spain in the social
sciences between 2000 and 2013. The goal is to explore to which extent
limitations on the number of coauthors established by Spanish national
evaluation agencies are justified. The analysis of 11681 papers authored by
researchers affiliated to Spanish institutions in 20 subject categories of the
social sciences reveals that there is no inflation in the number of authors,
team size is similar to that found in foreign papers and the number of authors
is dependent on international and institutional collaboration. With the
exception of Anthropology and Special Education, in no area the average of
authors by paper is higher than four. However, papers with a higher number of
authors receive more citations. Overall, our results suggest that there is no
justification on limiting the number of coauthors in publications,
acknowledging that their inclusion in the criteria employed by Spanish
evaluation agencies is to prevent honorary authors. Such limitation endangers
institutional and international collaboration, and consequently, high impact
research.
"
1977,Popularity of arXiv.org within Computer Science,"  It may seem surprising that, out of all areas of science, computer scientists
have been slow to post electronic versions of papers on sites like arXiv.org.
Instead, computer scientists have tended to place papers on our individual home
pages, but this loses the benefits of aggregation, namely notification and
browsing.
  But this is changing. More and more computer scientists are now using the
arXiv. At the same time, there is ongoing discussion and controversy about how
prepublication affects peer review, especially for double-blind conferences.
This discussion is often carried out with precious little evidence of how
popular prepublication is.
  We measure what percentage of papers in computer science are placed on the
arXiv, by cross-referencing published papers in DBLP with e-prints on arXiv. We
found:
  * Usage of arXiv.org has risen dramatically among the most selective
conferences in computer science. In 2017, fully 23% of papers had e-prints on
arXiv, compared to only 1% ten years ago.
  * Areas of computer science vary widely in e-print prevalence. In theoretical
computer science and machine learning, over 60% of published papers are on
arXiv, while other areas are essentially zero. In most areas, arXiv usage is
rising.
  * Many researchers use arXiv for posting preprints. Of the 2017 published
papers with arXiv e-prints, 56% were preprints that were posted before or
during peer review.
  Our paper describes these results as well as policy implications for
researchers and practitioners.
"
1978,"Categorization of an emerging discipline in the world publication system
  (SCOPUS): E-learning","  E-learning has been continuously present in current educational discourse,
thanks to technological advances, learning methodologies and public or
organizational policies, among other factors. However, despite its boom and
dominance in various subject areas, this thematic does not yet exist in the
world system of publications. Therefore, works in this thematic end up being
published under related categories, particularly Education or categories within
subject area Computer Science, thus fragmenting and make invisible the existing
knowledge. This work is based on the hypothesis that the scientific
communication of e-learning has a sufficient degree of cohesion to be
considered as a thematic category in itself. From a bibliometric approach, its
scientific production was analyzed, obtaining the bibliographic data of SCOPUS
and SCImago, selecting its main descriptors and generating visualizations
through VOSViewer with the mapping overlay technique, to represent its set and
proximity. As a result, it was determined that a set of 219 publications show a
high bibliometric interrelation among its articles and these are presented
transversely between the social sciences, computer science and health. This set
serves as a channel of scientific communication and structure of knowledge on
the thematic and can therefore be considered as the basis for establishing the
""e-learning"" thematic category in the world system of scientific publications,
contributing to the consolidation of the discipline, to its access and
development by researchers.
"
1979,"Role of Interdisciplinarity in Computer Sciences: Quantification, Impact
  and Life Trajectory","  The tremendous advances in computer science in the last few decades have
provided the platform to address and solve complex problems using
interdisciplinary research. In this paper, we investigate how the extent of
interdisciplinarity in computer science domain (which is further divided into
24 research fields) has changed over the last 50 years. To this end, we collect
a massive bibliographic dataset with rich metadata information. We start with
quantifying interdisciplinarity of a field in terms of the diversity of topics
and citations. We then analyze the effect of interdisciplinary research on the
scientific impact of individual fields and observe that highly disciplinary and
highly interdisciplinary papers in general have a low scientific impact;
remarkably those that are able to strike a balance between the two extremes
eventually land up having the highest impact. Further, we study the reciprocity
among fields through citation interactions and notice that links from one field
to related and citation-intensive fields (fields producing large number of
citations) are reciprocated heavily. A systematic analysis of the citation
interactions reveals the life trajectory of a research field, which generally
undergoes three phases -- a growing phase, a matured phase and an
interdisciplinary phase. The combination of metrics and empirical observations
presented here provides general benchmarks for future studies of
interdisciplinary research activities in other domains of science.
"
1980,MEDOC: a Python wrapper to load MEDLINE into a local MySQL database,"  Since the MEDLINE database was released, the number of documents indexed by
this entity has risen every year. Several tools have been developed by the
National Institutes of Health (NIH) to query this corpus of scientific
publications. However, in terms of advances in big data, text-mining and data
science, an option to build a local relational database containing all metadata
available on MEDLINE would be truly useful to optimally exploit these
resources. MEDOC (MEdline DOwnloading Contrivance) is a Python program designed
to download data on an FTP and to load all extracted information into a local
MySQL database. It took MEDOC 4 days and 17 hours to load the 26 million
documents available on this server onto a standard computer. This indexed
relational database allows the user to build complex and rapid queries. All
fields can thus be searched for desired information, a task that is difficult
to accomplish through the PubMed graphical interface. MEDOC is free and
publicly available at https://github.com/MrMimic/MEDOC.
"
1981,Maximum Value Matters: Finding Hot Topics in Scholarly Fields,"  Finding hot topics in scholarly fields can help researchers to keep up with
the latest concepts, trends, and inventions in their field of interest. Due to
the rarity of complete large-scale scholarly data, earlier studies target this
problem based on manual topic extraction from a limited number of domains, with
their focus solely on a single feature such as coauthorship, citation
relations, and etc. Given the compromised effectiveness of such predictions, in
this paper we use a real scholarly dataset from Microsoft Academic Graph, which
provides more than 12000 topics in the field of Computer Science (CS),
including 1200 venues, 14.4 million authors, 30 million papers and their
citation relations over the period of 1950 till now. Aiming to find the topics
that will trend in CS area, we innovatively formalize a hot topic prediction
problem where, with joint consideration of both inter- and intra-topical
influence, 17 different scientific features are extracted for comprehensive
description of topic status. By leveraging all those 17 features, we observe
good accuracy of topic scale forecasting after 5 and 10 years with R2 values of
0.9893 and 0.9646, respectively. Interestingly, our prediction suggests that
the maximum value matters in finding hot topics in scholarly fields, primarily
from three aspects: (1) the maximum value of each factor, such as authors'
maximum h-index and largest citation number, provides three times the amount of
information than the average value in prediction; (2) the mutual influence
between the most correlated topics serve as the most telling factor in
long-term topic trend prediction, interpreting that those currently exhibiting
the maximum growth rates will drive the correlated topics to be hot in the
future; (3) we predict in the next 5 years the top 100 fastest growing (maximum
growth rate) topics that will potentially get the major attention in CS area.
"
1982,"A Scalable and Adaptive Method for Finding Semantically Equivalent Cue
  Words of Uncertainty","  Scientific knowledge is constantly subject to a variety of changes due to new
discoveries, alternative interpretations, and fresh perspectives. Understanding
uncertainties associated with various stages of scientific inquiries is an
integral part of scientists' domain expertise and it serves as the core of
their meta-knowledge of science. Despite the growing interest in areas such as
computational linguistics, systematically characterizing and tracking the
epistemic status of scientific claims and their evolution in scientific
disciplines remains a challenge. We present a unifying framework for the study
of uncertainties explicitly and implicitly conveyed in scientific publications.
The framework aims to accommodate a wide range of uncertain types, from
speculations to inconsistencies and controversies. We introduce a scalable and
adaptive method to recognize semantically equivalent cues of uncertainty across
different fields of research and accommodate individual analysts' unique
perspectives. We demonstrate how the new method can be used to expand a small
seed list of uncertainty cue words and how the validity of the expanded
candidate cue words are verified. We visualize the mixture of the original and
expanded uncertainty cue words to reveal the diversity of expressions of
uncertainty. These cue words offer a novel resource for the study of
uncertainty in scientific assertions.
"
1983,New ADS Functionality for the Curator,"  In this paper we provide an update concerning the operations of the NASA
Astrophysics Data System (ADS), its services and user interface, and the
content currently indexed in its database. As the primary information system
used by researchers in Astronomy, the ADS aims to provide a comprehensive index
of all scholarly resources appearing in the literature. With the current effort
in our community to support data and software citations, we discuss what steps
the ADS is taking to provide the needed infrastructure in collaboration with
publishers and data providers. A new API provides access to the ADS search
interface, metrics, and libraries allowing users to programmatically automate
discovery and curation tasks. The new ADS interface supports a greater
integration of content and services with a variety of partners, including ORCID
claiming, indexing of SIMBAD objects, and article graphics from a variety of
publishers. Finally, we highlight how librarians can facilitate the ingest of
gray literature that they curate into our system.
"
1984,"Implementing Recommendation Algorithms in a Large-Scale Biomedical
  Science Knowledge Base","  The number of biomedical research articles published has doubled in the past
20 years. Search engine based systems naturally center around searching, but
researchers may not have a clear goal in mind, or the goal may be expressed in
a query that a literature search engine cannot easily answer, such as
identifying the most prominent authors in a given field of research. The
discovery process can be improved by providing researchers with recommendations
for relevant papers or for researchers who are dealing with related bodies of
work. In this paper we describe several recommendation algorithms that were
implemented in the Meta platform. The Meta platform contains over 27 million
articles and continues to grow daily. It provides an online map of science that
organizes, in real time, all published biomedical research. The ultimate goal
is to make it quicker and easier for researchers to: filter through scientific
papers; find the most important work and, keep up with emerging research
results. Meta generates and maintains a semantic knowledge network consisting
of these core entities: authors, papers, journals, institutions, and concepts.
We implemented several recommendation algorithms and evaluated their efficiency
in this large-scale biomedical knowledge base. We selected recommendation
algorithms that could take advantage of the unique environment of the Meta
platform such as those that make use of diverse datasets such as a citation
networks, text content, semantic tag content, and co-authorship information and
those that can scale to very large datasets. In this paper, we describe the
recommendation algorithms that were implemented and report on their relative
efficiency and the challenges associated with developing and deploying a
production recommendation engine system.
"
1985,"On the differences between citations and altmetrics: An investigation of
  factors driving altmetrics vs. citations for Finnish articles","  This study examines a range of factors associating with future citation and
altmetric counts to a paper. The factors include journal impact factor,
individual collaboration, international collaboration, institution prestige,
country prestige, research funding, abstract readability, abstract length,
title length, number of cited references, field size, and field type and will
be modelled in association with citation counts, Mendeley readers, Twitter
posts, Facebook posts, blog posts, and news posts. The results demonstrate that
eight factors are important for increased citation counts, seven different
factors are important for increased Mendeley readers, eight factors are
important for increased Twitter posts, three factors are important for
increased Facebook posts, six factors are important for increased blog posts,
and five factors are important for increased news posts. Journal impact factor
and international collaboration are the two factors that significantly
associate with increased citation counts and with all altmetric scores.
Moreover, it seems that the factors driving Mendeley readership are similar to
those driving citation counts. However, the altmetric events differ from each
other in terms of a small number of factors; for instance, institution prestige
and country prestige associate with increased Mendeley readers and blog and
news posts, but it is an insignificant factor for Twitter and Facebook posts.
The findings contribute to the continued development of theoretical models and
methodological developments associated with capturing, interpreting, and
understanding altmetric events.
"
1986,"The Evolutions of the Rich get Richer and the Fit get Richer Phenomena
  in Scholarly Networks: The Case of the Strategic Management Journal","  Understanding how a scientist develops new scientific collaborations or how
their papers receive new citations is a major challenge in scientometrics. The
approach being proposed simultaneously examines the growth processes of the
co-authorship and citation networks by analyzing the evolutions of the rich get
richer and the fit get richer phenomena. In particular, the preferential
attachment function and author fitnesses, which govern the two phenomena, are
estimated non-parametrically in each network. The approach is applied to the
co-authorship and citation networks of the flagship journal of the strategic
management scientific community, namely the Strategic Management Journal. The
results suggest that the abovementioned phenomena have been consistently
governing both temporal networks. The average of the attachment exponents in
the co-authorship network is 0.30 while it is 0.29 in the citation network.
This suggests that the rich get richer phenomenon has been weak in both
networks. The right tails of the distributions of author fitness in both
networks are heavy, which imply that the intrinsic scientific quality of each
author has been playing a crucial role in getting new citations and new
co-authorships. Since the total competitiveness in each temporal network is
founded to be rising with time, it is getting harder to receive a new citation
or to develop a new collaboration. Analyzing the average competency, it was
found that on average, while the veterans tend to be more competent at
developing new collaborations, the newcomers are likely better at acquiring new
citations. Furthermore, the author fitness in both networks has been consistent
with the history of the strategic management scientific community. This
suggests that coupling node fitnesses throughout different networks might be a
promising new direction in analyzing simultaneously multiple networks.
"
1987,"Confidence intervals for normalised citation counts: Can they delimit
  underlying research capability?","  Normalised citation counts are routinely used to assess the average impact of
research groups or nations. There is controversy over whether confidence
intervals for them are theoretically valid or practically useful. In response,
this article introduces the concept of a group's underlying research capability
to produce impactful research. It then investigates whether confidence
intervals could delimit the underlying capability of a group in practice. From
123120 confidence interval comparisons for the average citation impact of the
national outputs of ten countries within 36 individual large monodisciplinary
journals, moderately fewer than 95% of subsequent indicator values fall within
95% confidence intervals from prior years, with the percentage declining over
time. This is consistent with confidence intervals effectively delimiting the
research capability of a group, although it does not prove that this is the
cause of the results. The results are unaffected by whether internationally
collaborative articles are included.
"
1988,Early identification of important patents through network centrality,"  One of the most challenging problems in technological forecasting is to
identify as early as possible those technologies that have the potential to
lead to radical changes in our society. In this paper, we use the US patent
citation network (1926-2010) to test our ability to early identify a list of
historically significant patents through citation network analysis. We show
that in order to effectively uncover these patents shortly after they are
issued, we need to go beyond raw citation counts and take into account both the
citation network topology and temporal information. In particular, an
age-normalized measure of patent centrality, called rescaled PageRank, allows
us to identify the significant patents earlier than citation count and PageRank
score. In addition, we find that while high-impact patents tend to rely on
other high-impact patents in a similar way as scientific papers, the patents'
citation dynamics is significantly slower than that of papers, which makes the
early identification of significant patents more challenging than that of
significant papers.
"
1989,"Creating a Linked Data-Friendly Metadata Application Profile for
  Archival Description","  We provide an overview of efforts to apply and extend Schema.org for archives
and archival description. The authors see the application of Schema.org and
extensions as a low barrier means to publish easily consumable linked data
about archival resources, institutions that hold them, and contextual entities
such as people and organizations responsible for their creation.
"
1990,"The relationship between the number of editorial board members and the
  scientific output of universities in the chemistry field","  Editorial board members, who are considered the gatekeepers of scientific
journals, play an important role in academia, and may directly or indirectly
affect the scientific output of a university. In this article, we used the
quantile regression method among a sample of 1,387 university in chemistry to
characterize the correlation between the number of editorial board members and
the scientific output of their universities. Furthermore, we used time-series
data and the Granger causality test to explore the causal relationship between
the number of editorial board members and the number of articles of some top
universities. Our results suggest that the number of editorial board members is
positively and significantly related to the scientific output (as measured by
the number of articles, total number of citations, citations per paper, and h
index) of their universities. However, the Granger causality test results
suggest that the causal relationship between the number of editorial board
members and the number of articles of some top universities is not obvious.
Combining these findings with the results of qualitative interviews with
editorial board members, we discuss the causal relationship between the number
of editorial board members and the scientific output of their universities.
"
1991,New Methods for Metadata Extraction from Scientific Literature,"  Within the past few decades we have witnessed digital revolution, which moved
scholarly communication to electronic media and also resulted in a substantial
increase in its volume. Nowadays keeping track with the latest scientific
achievements poses a major challenge for the researchers. Scientific
information overload is a severe problem that slows down scholarly
communication and knowledge propagation across the academia. Modern research
infrastructures facilitate studying scientific literature by providing
intelligent search tools, proposing similar and related documents, visualizing
citation and author networks, assessing the quality and impact of the articles,
and so on. In order to provide such high quality services the system requires
the access not only to the text content of stored documents, but also to their
machine-readable metadata. Since in practice good quality metadata is not
always available, there is a strong demand for a reliable automatic method of
extracting machine-readable metadata directly from source documents. This
research addresses these problems by proposing an automatic, accurate and
flexible algorithm for extracting wide range of metadata directly from
scientific articles in born-digital form. Extracted information includes basic
document metadata, structured full text and bibliography section. Designed as a
universal solution, proposed algorithm is able to handle a vast variety of
publication layouts with high precision and thus is well-suited for analyzing
heterogeneous document collections. This was achieved by employing supervised
and unsupervised machine-learning algorithms trained on large, diverse
datasets. The evaluation we conducted showed good performance of proposed
metadata extraction algorithm. The comparison with other similar solutions also
proved our algorithm performs better than competition for most metadata types.
"
1992,A Distributed Parallel Model to Analyze Journal Impact Factors,"  A simple abstract model is developed as a parallel experimental basis for the
aim of exploring the differences of journal impact factors, particularly
between different disciplines. Our model endeavors to simulate the publication
and citation behaviors of the articles in the journals belonging to a similar
discipline, in a distributed manner. Based on simulation experiments, the
mechanism of influence from several fundamental factors to the trend of impact
factor is revealed. These factors include the average review cycle, average
number of references and yearly distribution of references. Moreover,
satisfactory approximation could possibly be observed between certain actual
data and simulation results.
"
1993,"Synergy in the Knowledge Base of U.S. Innovation Systems at National,
  State, and Regional Levels: The Contributions of High-Tech Manufacturing and
  Knowledge-Intensive Services","  Using information theory, we measure innovation systemness as synergy among
size-classes, zip-codes, and technological classes (NACE-codes) for 8.5 million
American companies. The synergy at the national level is decomposed at the
level of states, Core-Based Statistical Areas (CBSA), and Combined Statistical
Areas (CSA). We zoom in to the state of California and in more detail to
Silicon Valley. Our results do not support the assumption of a national system
of innovations in the U.S.A. Innovation systems appear to operate at the level
of the states; the CBSA are too small, so that systemness spills across their
borders. Decomposition of the sample in terms of high-tech manufacturing (HTM),
medium-high-tech manufacturing (MHTM), knowledge-intensive services (KIS), and
high-tech services (HTKIS) does not change this pattern, but refines it. The
East Coast -- New Jersey, Boston, and New York -- and California are the major
players, with Texas a third one in the case of HTKIS. Chicago and industrial
centers in the Midwest also contribute synergy. Within California, Los Angeles
contributes synergy in the sectors of manufacturing, the San Francisco area in
KIS. Knowledge-intensive services in Silicon Valley and the Bay area -- a CSA
composed of seven CBSA -- spill over to other regions and even globally.
"
1994,"Statistical Significance and Effect Sizes of Differences among Research
  Universities at the Level of Nations and Worldwide based on the Leiden
  Rankings","  The Leiden Rankings can be used for grouping research universities by
considering universities which are not statistically significantly different as
homogeneous sets. The groups and intergroup relations can be analyzed and
visualized using tools from network analysis. Using the so-called ""excellence
indicator"" PPtop-10%--the proportion of the top-10% most-highly-cited papers
assigned to a university--we pursue a classification using (i) overlapping
stability intervals, (ii) statistical-significance tests, and (iii) effect
sizes of differences among 902 universities in 54 countries; we focus on the
UK, Germany, Brazil, and the USA as national examples. Although the groupings
remain largely the same using different statistical significance levels or
overlapping stability intervals, these classifications are uncorrelated with
those based on effect sizes. Effect sizes for the differences between
universities are small (w <.2). The more detailed analysis of universities at
the country level suggests that distinctions beyond three or perhaps four
groups of universities (high, middle, low) may not be meaningful. Given similar
institutional incentives, isomorphism within each eco-system of universities
should not be underestimated. Our results suggest that networks based on
overlapping stability intervals can provide a first impression of the relevant
groupings among universities. However, the clusters are not well-defined
divisions between groups of universities.
"
1995,"Bibliometric-Enhanced Information Retrieval: 5th International BIR
  Workshop","  Bibliometric-enhanced Information Retrieval (BIR) workshops serve as the
annual gathering of IR researchers who address various information-related
tasks on scientific corpora and bibliometrics. The workshop features original
approaches to search, browse, and discover value-added knowledge from
scientific documents and related information networks (e.g., terms, authors,
institutions, references). We welcome contributions elaborating on dedicated IR
systems, as well as studies revealing original characteristics on how
scientific knowledge is created, communicated, and used. In this paper we
introduce the BIR workshop series and discuss some selected papers presented at
previous BIR workshops.
"
1996,Scientific co-authorship networks,"  The paper addresses the stability of the co-authorship networks in time. The
analysis is done on the networks of Slovenian researchers in two time periods
(1991-2000 and 2001-2010). Two researchers are linked if they published at
least one scientific bibliographic unit in a given time period. As proposed by
Kronegger et al. (2011), the global network structures are examined by
generalized blockmodeling with the assumed
multi-core--semi-periphery--periphery blockmodel type. The term core denotes a
group of researchers who published together in a systematic way with each
other.
  The obtained blockmodels are comprehensively analyzed by visualizations and
through considering several statistics regarding the global network structure.
To measure the stability of the obtained blockmodels, different adjusted
modified Rand and Wallace indices are applied. Those enable to distinguish
between the splitting and merging of cores when operationalizing the stability
of cores. Also, the adjusted modified indices can be used when new researchers
occur in the second time period (newcomers) and when some researchers are no
longer present in the second time period (departures). The research disciplines
are described and clustered according to the values of these indices.
Considering the obtained clusters, the sources of instability of the research
disciplines are studied (e.g., merging or splitting of cores, newcomers or
departures). Furthermore, the differences in the stability of the obtained
cores on the level of scientific disciplines are studied by linear regression
analysis where some personal characteristics of the researchers (e.g., age,
gender), are also considered.
"
1997,"Assessing the level of merging errors for coauthorship data: a Bayesian
  model","  Robust analysis of coauthorship networks is based on high quality data.
However, ground-truth data are usually unavailable. Empirical data suffer
several types of errors, a typical one of which is called merging error,
identifying different persons as one entity. Specific features of authors have
been used to reduce these errors. We proposed a Bayesian model to calculate the
information of any given features of authors. Based on the features, the model
can be utilized to calculate the rate of merging errors for entities.
Therefore, the model helps to find informative features for detecting heavily
compromised entities. It has potential contributions to improving the quality
of empirical data.
"
1998,A Foundry of Human Activities and Infrastructures,"  Direct representation knowledgebases can enhance and even provide an
alternative to document-centered digital libraries. Here we consider realist
semantic modeling of everyday activities and infrastructures in such
knowledgebases. Because we want to integrate a wide variety of topics, a
collection of ontologies (a foundry) and a range of other knowledge resources
are needed. We first consider modeling the routine procedures that support
human activities and technologies. Next, we examine the interactions of
technologies with aspects of social organization. Then, we consider approaches
and issues for developing and validating explanations of the relationships
among various entities.
"
1999,"Evaluation of research activities of universities of Ukraine and
  Belarus: a set of bibliometric indicators and its implementation","  Monitoring bibliometric indicators of University rankings is considered as a
subject of a University library activity. In order to fulfill comparative
assessment of research activities of the universities of Ukraine and Belarus
the authors introduced a set of bibliometric indicators. A comparative
assessment of the research activities of corresponding universities was
fulfilled; the data on the leading universities are presented. The sensitivity
of the one of the indicators to rapid changes of the research activity of
universities and the fact that the other one is normalized across the fields of
science condition advantage of the proposed set over the one that was used in
practice of the corresponding national rankings.
"
2000,"Measuring Influence in Science: Standing on the Shoulders of Which
  Giants?","  I study the measurement of the influence of scientists based on bibliographic
data. I propose a new measure that accounts for indirect influence and allows
to compare scientists across different fields of science. By contrast, common
measures of influence that ""count citations"", such as the h-index, are unable
to satisfy either of these two properties. I use the axiomatic method in two
opposite ways: to highlight the two limitations of citation-counting schemes
and their independence, and to carefully justify the assumptions made in the
construction of the proposed measure.
"
2001,"Discovery of potential collaboration networks from open knowledge
  sources","  Scientific publishing conveys the outputs of an academic or research
activity, in this sense; it also reflects the efforts and issues in which
people engage. To identify potential collaborative networks one of the simplest
approaches is to leverage the co-authorship relations. In this approach,
semantic and hierarchic relationships defined by a Knowledge Organization
System are used in order to improve the system's ability to recommend potential
networks beyond the lexical or syntactic analysis of the topics or concepts
that are of interest to academics.
"
2002,"Digitising Cultural Complexity: Representing Rich Cultural Data in a Big
  Data environment","  One of the major terminological forces driving ICT integration in research
today is that of ""big data."" While the phrase sounds inclusive and integrative,
""big data"" approaches are highly selective, excluding input that cannot be
effectively structured, represented, or digitised. Data of this complex sort is
precisely the kind that human activity produces, but the technological
imperative to enhance signal through the reduction of noise does not
accommodate this richness. Data and the computational approaches that
facilitate ""big data"" have acquired a perceived objectivity that belies their
curated, malleable, reactive, and performative nature. In an input environment
where anything can ""be data"" once it is entered into the system as ""data,"" data
cleaning and processing, together with the metadata and information
architectures that structure and facilitate our cultural archives acquire a
capacity to delimit what data are. This engenders a process of simplification
that has major implications for the potential for future innovation within
research environments that depend on rich material yet are increasingly
mediated by digital technologies. This paper presents the preliminary findings
of the European-funded KPLEX (Knowledge Complexity) project which investigates
the delimiting effect digital mediation and datafication has on rich, complex
cultural data. The paper presents a systematic review of existing implicit
definitions of data, elaborating on the implications of these definitions and
highlighting the ways in which metadata and computational technologies can
restrict the interpretative potential of data. It sheds light on the gap
between analogue or augmented digital practices and fully computational ones,
and the strategies researchers have developed to deal with this gap. The paper
proposes a reconceptualisation of data as it is functionally employed within
digitally-mediated research so as to incorporate and acknowledge the richness
and complexity of our source materials.
"
2003,"Towards a Cloud-Based Service for Maintaining and Analyzing Data About
  Scientific Events","  We propose the new cloud-based service OpenResearch for managing and
analyzing data about scientific events such as conferences and workshops in a
persistent and reliable way. This includes data about scientific articles,
participants, acceptance rates, submission numbers, impact values as well as
organizational details such as program committees, chairs, fees and sponsors.
OpenResearch is a centralized repository for scientific events and supports
researchers in collecting, organizing, sharing and disseminating information
about scientific events in a structured way. An additional feature currently
under development is the possibility to archive web pages along with the
extracted semantic data in order to lift the burden of maintaining new and old
conference web sites from public research institutions. However, the main
advantage is that this cloud-based repository enables a comprehensive analysis
of conference data. Based on extracted semantic data, it is possible to
determine quality estimations, scientific communities, research trends as well
the development of acceptance rates, fees, and number of participants in a
continuous way complemented by projections into the future. Furthermore, data
about research articles can be systematically explored using a content-based
analysis as well as citation linkage. All data maintained in this
crowd-sourcing platform is made freely available through an open SPARQL
endpoint, which allows for analytical queries in a flexible and user-defined
way.
"
2004,Web Robot Detection in Academic Publishing,"  Recent industry reports assure the rise of web robots which comprise more
than half of the total web traffic. They not only threaten the security,
privacy and efficiency of the web but they also distort analytics and metrics,
doubting the veracity of the information being promoted. In the academic
publishing domain, this can cause articles to be faulty presented as prominent
and influential. In this paper, we present our approach on detecting web robots
in academic publishing websites. We use different supervised learning
algorithms with a variety of characteristics deriving from both the log files
of the server and the content served by the website. Our approach relies on the
assumption that human users will be interested in specific domains or articles,
while web robots crawl a web library incoherently. We experiment with features
adopted in previous studies with the addition of novel semantic characteristics
which derive after performing a semantic analysis using the Latent Dirichlet
Allocation (LDA) algorithm. Our real-world case study shows promising results,
pinpointing the significance of semantic features in the web robot detection
problem.
"
2005,"War and Peace: The Peculiarities of Ukrainian-Russian Scientific
  Cooperation Dynamics Against the Background of Russian Military Aggression in
  Ukraine, in 2014-2016","  The paper presents the results of bibliometric analysis of publications that
were co-written by authors affiliated with Ukrainian and Russian institutions
in 2007-2016 according to Scopus. Results of the study show that Ukrainian and
Russian scientists have not refused to carry out joint research in major
international projects, but a decrease in the number of works, written by
Ukrainian and Russian scientific institutions staff members in 2016, provides
evidence on the threat and negative impact the Russian military intervention
brings to cooperation in science. The findings are important for generating the
science development programs in Ukraine.
"
2006,"Understanding the Changing Roles of Scientific Publications via Citation
  Embeddings","  Researchers may describe different aspects of past scientific publications in
their publications and the descriptions may keep changing in the evolution of
science. The diverse and changing descriptions (i.e., citation context) on a
publication characterize the impact and contributions of the past publication.
In this article, we aim to provide an approach to understanding the changing
and complex roles of a publication characterized by its citation context. We
described a method to represent the publications' dynamic roles in science
community in different periods as a sequence of vectors by training temporal
embedding models. The temporal representations can be used to quantify how much
the roles of publications changed and interpret how they changed. Our study in
the biomedical domain shows that our metric on the changes of publications'
roles is stable over time at the population level but significantly distinguish
individuals. We also show the interpretability of our methods by a concrete
example.
"
2007,"Do altmetrics correlate with the quality of papers? A large-scale
  empirical study based on F1000Prime data","  In this study, we address the question whether (and to what extent,
respectively) altmetrics are related to the scientific quality of papers (as
measured by peer assessments). Only a few studies have previously investigated
the relationship between altmetrics and assessments by peers. In the first
step, we analyse the underlying dimensions of measurement for traditional
metrics (citation counts) and altmetrics - by using principal component
analysis (PCA) and factor analysis (FA). In the second step, we test the
relationship between the dimensions and quality of papers (as measured by the
post-publication peer-review system of F1000Prime assessments) - using
regression analysis. The results of the PCA and FA show that altmetrics operate
along different dimensions, whereas Mendeley counts are related to citation
counts, and tweets form a separate dimension. The results of the regression
analysis indicate that citation-based metrics and readership counts are
significantly more related to quality, than tweets. This result on the one hand
questions the use of Twitter counts for research evaluation purposes and on the
other hand indicates potential use of Mendeley reader counts.
"
2008,"Automated Analysis of Topic-Actor Networks on Twitter: New approach to
  the analysis of socio-semantic networks","  Social-media data provides increasing opportunities for automated analysis of
large sets of textual documents. So far, automated tools have been developed to
account for either the social networks between the participants of the debates,
or to analyze the content of those debates. Less attention has been paid to
mapping co-occurring actors (participants) and topics (content) in online
debates that form socio-semantic networks. We propose a new, automated approach
that uses a whole matrix approach of co-addressed topics and the actors. We
show the advantages of the new approach with the analysis of a large set of
English-language Twitter messages at the Rio+20 meeting, in June 2012 (72,077
tweets), and a smaller data set of Dutch-language Twitter messages on bird flu
related to poultry farming in 2015-2017 (2,139 tweets). We discuss the
theoretical, methodological and substantive implications of our approach, also
for the analysis of other social-media data.
"
2009,"The Research Production of Nations and Departments: A Statistical Model
  for the Share of Publications","  Policy makers and managers sometimes assess the share of research produced by
a group (country, department, institution). This takes the form of the
percentage of publications in a journal, field or broad area that has been
published by the group. This quantity is affected by essentially random
influences that obscure underlying changes over time and differences between
groups. A model of research production is needed to help identify whether
differences between two shares indicate underlying differences. This article
introduces a simple production model for indicators that report the share of
the world's output in a journal or subject category, assuming that every new
article has the same probability to be authored by a given group. With this
assumption, confidence limits can be calculated for the underlying production
capability (i.e., probability to publish). The results of a time series
analysis of national contributions to 36 large monodisciplinary journals
1996-2016 are broadly consistent with this hypothesis. Follow up tests of
countries and institutions in 26 Scopus subject categories support the
conclusions but highlight the importance of ensuring consistent subject
category coverage.
"
2010,"Microsoft Academic: A multidisciplinary comparison of citation counts
  with Scopus and Mendeley for 29 journals","  Microsoft Academic is a free citation index that allows large scale data
collection. This combination makes it useful for scientometric research.
Previous studies have found that its citation counts tend to be slightly larger
than those of Scopus but smaller than Google Scholar, with disciplinary
variations. This study reports the largest and most systematic analysis so far,
of 172,752 articles in 29 large journals chosen from different specialisms.
From Scopus citation counts, Microsoft Academic citation counts and Mendeley
reader counts for articles published 2007-2017, Microsoft Academic found a
slightly more (6%) citations than Scopus overall and especially for the current
year (51%). It found fewer citations than Mendeley readers overall (59%), and
only 7% as many for the current year. Differences between journals were
probably due to field preprint sharing cultures or journal policies rather than
broad disciplinary differences.
"
2011,"Microsoft Academic Automatic Document Searches: Accuracy for Journal
  Articles and Suitability for Citation Analysis","  Microsoft Academic is a free academic search engine and citation index that
is similar to Google Scholar but can be automatically queried. Its data is
potentially useful for bibliometric analysis if it is possible to search
effectively for individual journal articles. This article compares different
methods to find journal articles in its index by searching for a combination of
title, authors, publication year and journal name and uses the results for the
widest published correlation analysis of Microsoft Academic citation counts for
journal articles so far. Based on 126,312 articles from 323 Scopus subfields in
2012, the optimal strategy to find articles with DOIs is to search for them by
title and filter out those with incorrect DOIs. This finds 90% of journal
articles. For articles without DOIs, the optimal strategy is to search for them
by title and then filter out matches with dissimilar metadata. This finds 89%
of journal articles, with an additional 1% incorrect matches. The remaining
articles seem to be mainly not indexed by Microsoft Academic or indexed with a
different language version of their title. From the matches, Scopus citation
counts and Microsoft Academic counts have an average Spearman correlation of
0.95, with the lowest for any single field being 0.63. Thus, Microsoft Academic
citation counts are almost universally equivalent to Scopus citation counts for
articles that are not recent but there are national biases in the results.
"
2012,Digital Encyclopedia of Scientific Results,"  This study describes a vision, how technology can help improving the
efficiency in research. We propose a new clean-slate design, where more
emphasis is given on the correctness and up-to-dateness of the scientific
results, it is more open to new ideas and better utilize the research efforts
worldwide by providing personalized interface for every researcher. The key
idea is to reveal the structure and connections of the problems solved in the
scientific studies. We will build the system with the main focus on the solved
problems itself, and treat the studies only as one presentation form. By
utilizing artificial intelligence and machine learning on the network of the
solved problems we could coordinate individual research activities in a large
scale, that has never been seen before.
"
2013,"Emerging basic, clinical and translational research fronts in dental
  biomaterials R&D","  The current (2007-2007) structure and content of dental materials research
has been investigated by identifying and describing the emergent research
fronts which can be related to basic, translational and clinical observation
research. By a combination of network analysis and text mining of the
literature on dental materials indexed in the Web of Science, we have
identified eleven emerging research fronts. These fronts are related to
different dental materials applications which are at different levels in the
knowledge translation and biomedical innovation process. We identified fronts
related to dominant designs like titanium implants, competing technologies like
ceramics and composites applications to prothesis and restauration, and
disruptive technologies like nanomaterials and mineral trioxide aggregates. Our
results suggest the possible relation between the technological complexity of
the dental materials and the level of advance in terms of knowledge
translation. This is the first time the structure and content of research on
dental materials research is analyzed.
"
2014,Research assessment by percentile-based double rank analysis,"  In the double rank analysis of research publications, the local rank position
of a country or institution publication is expressed as a function of the world
rank position. Excluding some highly or lowly cited publications, the double
rank plot fits well with a power law, which can be explained because citations
for local and world publications follow lognormal distributions. We report here
that the distribution of the number of country or institution publications in
world percentiles is a double rank distribution that can be fitted to a power
law. Only the data points in high percentiles deviate from it when the local
and world $\mu$ parameters of the lognormal distributions are very different.
The likelihood of publishing very highly cited papers can be calculated from
the power law that can be fitted either to the upper tail of the citation
distribution or to the percentile-based double rank distribution. The great
advantage of the latter method is that it has universal application, because it
is based on all publications and not just on highly cited publications.
Furthermore, this method extends the application of the well-established
percentile approach to very low percentiles where breakthroughs are reported
but paper counts cannot be performed.
"
2015,Exploration of an Interdisciplinary Scientific Landscape,"  Patterns of interdisciplinarity in science can be quantified through diverse
complementary dimensions. This paper studies as a case study the scientific
environment of a generalist journal in Geography, Cybergeo, in order to
introduce a novel methodology combining citation network analysis and semantic
analysis. We collect a large corpus of around 200,000 articles with their
abstracts and the corresponding citation network that provides a first citation
classification. Relevant keywords are extracted for each article through
text-mining, allowing us to construct a semantic classification. We study the
qualitative patterns of relations between endogenous disciplines within each
classification, and finally show the complementarity of classifications and of
their associated interdisciplinarity measures. The tools we develop accordingly
are open and reusable for similar large scale studies of scientific
environments.
"
2016,"Artificial intelligence in peer review: How can evolutionary computation
  support journal editors?","  With the volume of manuscripts submitted for publication growing every year,
the deficiencies of peer review (e.g. long review times) are becoming more
apparent. Editorial strategies, sets of guidelines designed to speed up the
process and reduce editors workloads, are treated as trade secrets by
publishing houses and are not shared publicly. To improve the effectiveness of
their strategies, editors in small publishing groups are faced with undertaking
an iterative trial-and-error approach. We show that Cartesian Genetic
Programming, a nature-inspired evolutionary algorithm, can dramatically improve
editorial strategies. The artificially evolved strategy reduced the duration of
the peer review process by 30%, without increasing the pool of reviewers (in
comparison to a typical human-developed strategy). Evolutionary computation has
typically been used in technological processes or biological ecosystems. Our
results demonstrate that genetic programs can improve real-world social systems
that are usually much harder to understand and control than physical systems.
"
2017,"Normalization of zero-inflated data: An empirical analysis of a new
  indicator family and its use with altmetrics data","  Recently, two new indicators (Equalized Mean-based Normalized Proportion
Cited, EMNPC; Mean-based Normalized Proportion Cited, MNPC) were proposed which
are intended for sparse scientometrics data. The indicators compare the
proportion of mentioned papers (e.g. on Facebook) of a unit (e.g., a researcher
or institution) with the proportion of mentioned papers in the corresponding
fields and publication years (the expected values). In this study, we propose a
third indicator (Mantel-Haenszel quotient, MHq) belonging to the same indicator
family. The MHq is based on the MH analysis - an established method in
statistics for the comparison of proportions. We test (using citations and
assessments by peers, i.e. F1000Prime recommendations) if the three indicators
can distinguish between different quality levels as defined on the basis of the
assessments by peers. Thus, we test their convergent validity. We find that the
indicator MHq is able to distinguish between the quality levels in most cases
while MNPC and EMNPC are not. Since the MHq is shown in this study to be a
valid indicator, we apply it to six types of zero-inflated altmetrics data and
test whether different altmetrics sources are related to quality. The results
for the various altmetrics demonstrate that the relationship between altmetrics
(Wikipedia, Facebook, blogs, and news data) and assessments by peers is not as
strong as the relationship between citations and assessments by peers.
Actually, the relationship between citations and peer assessments is about two
to three times stronger than the association between altmetrics and assessments
by peers.
"
2018,Difficulties of Timestamping Archived Web Pages,"  We show that state-of-the-art services for creating trusted timestamps in
blockchain-based networks do not adequately allow for timestamping of web
pages. They accept data by value (e.g., images and text), but not by reference
(e.g., URIs of web pages). Also, we discuss difficulties in repeatedly
generating the same cryptographic hash value of an archived web page. We then
introduce several requirements to be fulfilled in order to produce repeatable
hash values for archived web pages.
"
2019,Sketch Layer Separation in Multi-Spectral Historical Document Images,"  High-resolution imaging has delivered new prospects for detecting the
material composition and structure of cultural treasures. Despite the various
techniques for analysis, a significant diagnostic gap remained in the range of
available research capabilities for works on paper. Old master drawings were
mostly composed in a multi-step manner with various materials. This resulted in
the overlapping of different layers which made the subjacent strata difficult
to differentiate. The separation of stratified layers using imaging methods
could provide insights into the artistic work processes and help answer
questions about the object, its attribution, or in identifying forgeries. The
pattern recognition procedure was tested with mock replicas to achieve the
separation and the capability of displaying concealed red chalk under ink. In
contrast to RGB-sensor based imaging, the multi- or hyperspectral technology
allows accurate layer separation by recording the characteristic signatures of
the material's reflectance. The risk of damage to the artworks as a result of
the examination can be reduced by using combinations of defined spectra for
lightning and image capturing. By guaranteeing the maximum level of
readability, our results suggest that the technique can be applied to a broader
range of objects and assist in diagnostic research into cultural treasures in
the future.
"
2020,"The effect of publishing a highly cited paper on journal's impact
  factor: a case study of the Review of Particle Physics","  A single highly cited article can give a big but temporary lift in its host
journal's impact factor evidenced by the striking example of ""A short history
of SHELX"" published in Acta Crystallographica Section A. By using Journal
Citation Reports and Web of Science's citation analysis tool, we find a more
general and continuous form of this phenomenon in the Particle Physics field.
The highly-cited ""Review of Particle Physics"" series have been published in one
of the major Particle Physics journals biennially. This study analyses the
effect of these articles on the Impact Factor (IF) of the host journals. The
results show that the publication of Review of Particle Physics articles has a
direct effect of lifting the IF of its host journal. However the effect on the
IF varies according to whether the host journal already has a relatively high
or low IF, and the number of articles that it publishes. The impact of these
highly cited articles clearly demonstrates the limitations of journal impact
factor, and endorses the need to use it more wisely when deciding where to
publish and how to evaluate the relative impact of a journal.
"
2021,Image Registration for the Alignment of Digitized Historical Documents,"  In this work, we conducted a survey on different registration algorithms and
investigated their suitability for hyperspectral historical image registration
applications. After the evaluation of different algorithms, we choose an
intensity based registration algorithm with a curved transformation model. For
the transformation model, we select cubic B-splines since they should be
capable to cope with all non-rigid deformations in our hyperspectral images.
From a number of similarity measures, we found that residual complexity and
localized mutual information are well suited for the task at hand. In our
evaluation, both measures show an acceptable performance in handling all
difficulties, e.g., capture range, non-stationary and spatially varying
intensity distortions or multi-modality that occur in our application.
"
2022,"The relative influences of government funding and international
  collaboration on citation impact","  In a recent publication in Nature, Wagner & Jonkers (2017) report that public
R&D funding is only weakly correlated with the citation impact of a nation's
papers as measured by the field-weighted citation index (FWCI; defined by
Scopus). On the basis of the supplementary data, we upscaled the design using
Web-of-Science data for the decade 2003-2013 and OECD funding data for the
corresponding decade assuming a two-year delay (2001-2011). Using negative
binomial regression analysis, we find very small coefficients, but the effects
of international collaboration are positive and statistically significant,
whereas the effects of government funding are negative, an order of magnitude
smaller, and statistically non-significant (in two of three analyses). In other
words, international collaboration improves the impact of average research
papers, whereas more government funding tends to have a small adverse effect
when comparing OECD countries.
"
2023,Scholars on Twitter: who and how many are they?,"  In this paper we present a novel methodology for identifying scholars with a
Twitter account. By combining bibliometric data from Web of Science and Twitter
users identified by Altmetric.com we have obtained the largest set of
individual scholars matched with Twitter users made so far. Our methodology
consists of a combination of matching algorithms, considering different
linguistic elements of both author names and Twitter names; followed by a
rule-based scoring system that weights the common occurrence of several
elements related with the names, individual elements and activities of both
Twitter users and scholars matched. Our results indicate that about 2% of the
overall population of scholars in the Web of Science is active on Twitter. By
domain we find a strong presence of researchers from the Social Sciences and
the Humanities. Natural Sciences is the domain with the lowest level of
scholars on Twitter. Researchers on Twitter also tend to be younger than those
that are not on Twitter. As this is a bibliometric-based approach, it is
important to highlight the reliance of the method on the number of publications
produced and tweeted by the scholars, thus the share of scholars on Twitter
ranges between 1% and 5% depending on their level of productivity. Further
research is suggested in order to improve and expand the methodology.
"
2024,The Wisdom of Polarized Crowds,"  As political polarization in the United States continues to rise, the
question of whether polarized individuals can fruitfully cooperate becomes
pressing. Although diversity of individual perspectives typically leads to
superior team performance on complex tasks, strong political perspectives have
been associated with conflict, misinformation and a reluctance to engage with
people and perspectives beyond one's echo chamber. It is unclear whether
self-selected teams of politically diverse individuals will create higher or
lower quality outcomes. In this paper, we explore the effect of team political
composition on performance through analysis of millions of edits to Wikipedia's
Political, Social Issues, and Science articles. We measure editors' political
alignments by their contributions to conservative versus liberal articles. A
survey of editors validates that those who primarily edit liberal articles
identify more strongly with the Democratic party and those who edit
conservative ones with the Republican party. Our analysis then reveals that
polarized teams---those consisting of a balanced set of politically diverse
editors---create articles of higher quality than politically homogeneous teams.
The effect appears most strongly in Wikipedia's Political articles, but is also
observed in Social Issues and even Science articles. Analysis of article ""talk
pages"" reveals that politically polarized teams engage in longer, more
constructive, competitive, and substantively focused but linguistically diverse
debates than political moderates. More intense use of Wikipedia policies by
politically diverse teams suggests institutional design principles to help
unleash the power of politically polarized teams.
"
2025,Global research collaboration: Networks and partners in South East Asia,"  This is an empirical paper that addresses the role of bilateral and
multilateral international co-authorships in the six leading science systems
among the ASEAN group of countries (ASEAN6). The paper highlights the different
ways that bilateral and multilateral co-authorships structure global networks
and the collaborations of the ASEAN6. The paper looks at the influence of the
collaboration styles of major collaborating countries of the ASEAN6,
particularly the USA and Japan. It also highlights the role of bilateral and
multilateral co-authorships in the production of knowledge in the leading
specialisations of the ASEAN6. The discussion section offers some tentative
explanations for major dynamics evident in the results and summarises the next
steps in this research.
"
2026,"Bibliometric Approximation of a Scientific Specialty by Combining Key
  Sources, Title Words, Authors and References","  Bibliometric methods for the analysis of highly specialized subjects are
increasingly investigated and debated. Information and assessments well-focused
at the specialty level can help make important decisions in research and
innovation policy. This paper presents a novel method to approximate the
specialty to which a given publication record belongs. The method partially
combines sets of key values for four publication data fields: source, title,
authors and references. The approach is founded in concepts defining research
disciplines and scholarly communication, and in empirically observed
regularities in publication data. The resulting specialty approximation
consists of publications associated to the investigated publication record via
key values for at least three of the four data fields. This paper describes the
method and illustrates it with an application to publication records of
individual scientists. The illustration also successfully tests the focus of
the specialty approximation in terms of its ability to connect and help
identify peers. Potential tracks for further investigation include analyses
involving other kinds of specialized publication records, studies for a broader
range of specialties, and exploration of the potential for diverse applications
in research and research policy context.
"
2027,A proposal for a quantitative indicator of original research output,"  The use of quantitative indicators of scientific productivity seems now quite
widespread for assessing researchers and research institutions. There is a
general perception, however, that these indicators are not necessarily
representative of the originality of the research carried out, being primarily
indicative of a more or less prolific scientific activity and of the size of
the targeted scientific subcommunity. We first discuss some of the drawbacks of
the broadly adopted $h$-index and of the fact that it represents, in an average
sense, an indicator derivable from the total number of citations. Then we
propose an indicator which, although not immune from biases, seems more in line
with the general expectations for quantifying what is typically considered
original work. Qualitative arguments on how different indicators may shape the
future of science are finally discussed.
"
2028,"On the relationships between bibliographic characteristics of scientific
  documents and citation and Mendeley readership counts: A large-scale analysis
  of Web of Science publications","  In this paper we present a first large-scale analysis of the relationship
between Mendeley readership and citation counts with particular documents
bibliographic characteristics. A data set of 1.3 million publications from
different fields published in journals covered by the Web of Science (WoS) has
been analyzed. This work reveals that document types that are often excluded
from citation analysis due to their lower citation values, like editorial
materials, letters, or news items, are strongly covered and saved in Mendeley,
suggesting that Mendeley readership can reliably inform the analysis of these
document types. Findings show that collaborative papers are frequently saved in
Mendeley, which is similar to what is observed for citations. The relationship
between readership and the length of titles and number of pages, however, is
weaker than for the same relationship observed for citations. The analysis of
different disciplines also points to different patterns in the relationship
between several document characteristics, readership, and citation counts.
Overall, results highlight that although disciplinary differences exist,
readership counts are related to similar bibliographic characteristics as those
related to citation counts, reinforcing the idea that Mendeley readership and
citations capture a similar concept of impact, although they cannot be
considered as equivalent indicators.
"
2029,"Field- and time-normalization of data with many zeros: An empirical
  analysis using citation and Twitter data","  Thelwall (2017a, 2017b) proposed a new family of field- and time-normalized
indicators, which is intended for sparse data. These indicators are based on
units of analysis (e.g., institutions) rather than on the paper level. They
compare the proportion of mentioned papers (e.g., on Twitter) of a unit with
the proportion of mentioned papers in the corresponding fields and publication
years (the expected values). We propose a new indicator (Mantel-Haenszel
quotient, MHq) for the indicator family. The MHq goes back to the MH analysis.
This analysis is an established method, which can be used to pool the data from
several 2x2 cross tables based on different subgroups. We investigate (using
citations and assessments by peers, i.e., F1000Prime recommendations) whether
the indicator family (including the MHq) can distinguish between quality levels
defined by the assessments of peers. Thus, we test the convergent validity. We
find that the MHq is able to distinguish between quality levels (in most cases)
while other indicators of the family are not. Since our study approves the MHq
as a convergent valid indicator, we apply the MHq to four different Twitter
groups as defined by the company Altmetric (e.g., science communicators). Our
results show that there is a weak relationship between all four Twitter groups
and scientific quality, much weaker than between citations and scientific
quality. Therefore, our results discourage the use of Twitter counts in
research evaluation.
"
2030,Building the Brazilian Academic Genealogy Tree,"  Along the history, many researchers provided remarkable contributions to
science, not only advancing knowledge but also in terms of mentoring new
scientists. Currently, identifying and studying the formation of researchers
over the years is a challenging task as current repositories of theses and
dissertations are cataloged in a decentralized way through many local digital
libraries. Following our previous work in which we created and analyzed a large
collection of genealogy trees extracted from NDLTD, in this paper we focus our
attention on building such trees for the Brazilian research community. For
this, we use data from the Lattes Platform, an internationally renowned
initiative from CNPq, the Brazilian National Council for Scientific and
Technological Development, for managing information about individual
researchers and research groups in Brazil.
"
2031,"A Model for Data Citation in Astronomical Research using Digital Object
  Identifiers (DOIs)","  Standardizing and incentivizing the use of digital object identifiers (DOIs)
to aggregate and identify both data analyzed and data generated by a research
project will advance the field of astronomy to match best practices in other
research fields like geosciences and medicine. Increase in the use of DOIs will
prepare the discipline for changing expectations among funding agencies and
publishers, who increasingly expect accurate and thorough data citation to
accompany scientific outputs. The use of DOIs ensures a robust, sustainable,
and interoperable approach to data citation in which due credit is given to
researchers and institutions who produce and maintain the primary data. We
describe in this work the advantages of DOIs for data citation and best
practices for integrating a DOI service in an astronomical archive. We report
on a pilot project carried out in collaboration with AAS Journals. During the
course of the 1.5 year pilot, over 75% of submitting authors opted to use the
integrated DOI service to clearly identify data analyzed during their research
project when prompted at the time of paper submission.
"
2032,Semantic Modeling with Foundries,"  We analyze challenges for the development of the Human Activities and
Infrastructures Foundry. We explore a rich semantic modeling approach to
describe two Korean ceramic water droppers used to mix ink for calligraphy, how
they were produced and the reasons for their differing aesthetic. Our modeling
supports schema and allows for transitions of Entities based on the
relationships to other Entities with which they are associated. We explore the
similarity of our approach to object-oriented analysis and modeling.
"
2033,How the Taiwanese Do China Studies: Applications of Text Mining,"  With the rapid evolution of cross-strait situation, ""Mainland China"" as a
subject of social science study has evoked the voice of ""Rethinking China
Study"" among intelligentsia recently. This essay tried to apply an automatic
content analysis tool (CATAR) to the journal ""Mainland China Studies""
(1998-2015) in order to observe the research trends based on the clustering of
text from the title and abstract of each paper in the journal. The results
showed that the 473 articles published by the journal were clustered into seven
salient topics. From the publication number of each topic over time (including
""volume of publications"", ""percentage of publications""), there are two major
topics of this journal while other topics varied over time widely. The
contribution of this study includes: 1. We could group each ""independent"" study
into a meaningful topic, as a small scale experiment verified that this topic
clustering is feasible. 2. This essay reveals the salient research topics and
their trends for the Taiwan journal ""Mainland China Studies"". 3. Various
topical keywords were identified, providing easy access to the past study. 4.
The yearly trends of the identified topics could be viewed as signature of
future research directions.
"
2034,"The Unified Astronomy Thesaurus: Semantic Metadata for Astronomy and
  Astrophysics","  Several different controlled vocabularies have been developed and used by the
astronomical community, each designed to serve a specific need and a specific
group. The Unified Astronomy Thesaurus (UAT) attempts to provide a highly
structured controlled vocabulary that will be relevant and useful across the
entire discipline, regardless of content or platform. As two major use cases
for the UAT include classifying articles and data, we examine the UAT in
comparison with the Astronomical Subject Keywords used by major publications
and the JWST Science Keywords used by STScI's Astronomer's Proposal Tool.
"
2035,"Text Extraction and Retrieval from Smartphone Screenshots: Building a
  Repository for Life in Media","  Daily engagement in life experiences is increasingly interwoven with mobile
device use. Screen capture at the scale of seconds is being used in behavioral
studies and to implement ""just-in-time"" health interventions. The increasing
psychological breadth of digital information will continue to make the actual
screens that people view a preferred if not required source of data about life
experiences. Effective and efficient Information Extraction and Retrieval from
digital screenshots is a crucial prerequisite to successful use of screen data.
In this paper, we present the experimental workflow we exploited to: (i)
pre-process a unique collection of screen captures, (ii) extract unstructured
text embedded in the images, (iii) organize image text and metadata based on a
structured schema, (iv) index the resulting document collection, and (v) allow
for Image Retrieval through a dedicated vertical search engine application. The
adopted procedure integrates different open source libraries for traditional
image processing, Optical Character Recognition (OCR), and Image Retrieval. Our
aim is to assess whether and how state-of-the-art methodologies can be applied
to this novel data set. We show how combining OpenCV-based pre-processing
modules with a Long short-term memory (LSTM) based release of Tesseract OCR,
without ad hoc training, led to a 74% character-level accuracy of the extracted
text. Further, we used the processed repository as baseline for a dedicated
Image Retrieval system, for the immediate use and application for behavioral
and prevention scientists. We discuss issues of Text Information Extraction and
Retrieval that are particular to the screenshot image case and suggest
important future work.
"
2036,"Schroedinger's code: A preliminary study on research source code
  availability and link persistence in astrophysics","  We examined software usage in a sample set of astrophysics research articles
published in 2015 and searched for source code for the software mentioned in
these research papers. We categorized the software to indicate whether source
code is available for download and whether there are restrictions to accessing
it, and if source code is not available, whether some other form of the
software, such as a binary, is. We also extracted hyperlinks from one journal's
2015 research articles, as links in articles can serve as an acknowledgment of
software use and lead to data used in the research, and tested them to
determine which of these URLs are still accessible. For our sample of 715
software instances in the 166 articles we examined, we were able to categorize
418 records as to availability of source code and found that 285 unique codes
were used, 58% of which offer source code available online for download. Of the
2,558 hyperlinks extracted from 1,669 research articles, at best, 90% of them
were available over our testing period.
"
2037,"Social Media Attention Increases Article Visits: An Investigation on
  Article-Level Referral Data of PeerJ","  In order to better understand the effect of social media in the dissemination
of scholarly articles, employing the daily updated referral data of 110 PeerJ
articles collected over a period of 345 days, we analyze the relationship
between social media attention and article visitors directed by social media.
Our results show that social media presence of PeerJ articles is high. About
68.18% of the papers receive at least one tweet from Twitter accounts other
than @PeerJ, the official account of the journal. Social media attention
increases the dissemination of scholarly articles. Altmetrics could not only
act as the complement of traditional citation measures but also play an
important role in increasing the article downloads and promoting the impacts of
scholarly articles. There also exists a significant correlation among the
online attention from different social media platforms. Articles with more
Facebook shares tend to get more tweets. The temporal trends show that social
attention comes immediately following publication but does not last long, so do
the social media directed article views.
"
2038,"Granularity of algorithmically constructed publication-level
  classifications of research publications: Identification of topics","  The purpose of this study is to find a theoretically grounded, practically
applicable and useful granularity level of an algorithmically constructed
publication-level classification of research publications (ACPLC). The level
addressed is the level of research topics. The methodology we propose uses
synthesis papers and their reference articles to construct a baseline
classification. A dataset of about 31 million publications, and their mutual
citations relations, is used to obtain several ACPLCs of different granularity.
Each ACPLC is compared to the baseline classification and the best performing
ACPLC is identified. The results of two case studies show that the topics of
the cases are closely associated with different classes of the identified
ACPLC, and that these classes tend to treat only one topic. Further, the class
size variation is moderate, and only a small proportion of the publications
belong to very small classes. For these reasons, we conclude that the proposed
methodology is suitable to determine the topic granularity level of an ACPLC
and that the ACPLC identified by this methodology is useful for bibliometric
analyses.
"
2039,"Why informatics and general science need a conjoint basic definition of
  information","  First the basic definition of information as a selection from a set of
possibilities resp. domain is recalled. This also applies to digital
information. The bits of digital information are parts of number sequences
which represent a selection from a set of possibilities resp. domain. For
faultless conversation sender and receiver of information must have the same
definition of the domain (e.g. of language vocabulary). Up to now the
definition of the domain and of its elements is derived from context and
knowledge. The internet provides an additional important possibility: A link to
a conjoint uniform definition of the domain at unique location on the internet.
The associated basic information structure is called ""Domain Vector"" (DV) and
has the structure ""UL (of the domain definition) plus sequence of numbers"". The
""UL"" is not only ""Uniform Locator"" of the domain definition. It also identifies
a certain kind of information for later comparison and search. It can be a
Uniform Resource Locator (URL) or an abbreviated equivalent, e.g. a hierarchic
numeric pointer or a short local pointer to a table with global internet
pointers. The DV structure can be used as general carrier of information which
is language independent and more precise than language. A domain which contains
DVs is called ""Domain Space"" (DS) and is defined as metric space. This allows
similarity search according to user defined criteria, so that any kind of
definable information can be made comparable and searchable according to user
selected (relevant) and objectifiable (globally uniform) criteria. DS
definitions can be reused in new DS definitions. Their elements, the DVs, are
automatically globally uniformly identified and defined. Obviously such
conjoint definition of comparable information has great potential. It also can
avoid interoperability problems and redundant programming and so save high
costs.
"
2040,On the Availability of ESO Data Papers on arXiv/astro-ph,"  Using the ESO Telescope Bibliography database telbib, we have investigated
the percentage of ESO data papers that were submitted to the arXiv/astro-ph
e-print server and that are therefore free to read. Our study revealed an
availability of up to 96% of telbib papers on arXiv over the years 2010 to
2017. We also compared the citation counts of arXiv vs. non-arXiv papers and
found that on average, papers submitted to arXiv are cited 2.8 times more often
than those not on arXiv. While simulations suggest that these findings are
statistically significant, we cannot yet draw firm conclusions as to the main
cause of these differences.
"
2041,Edge Factors: Scientific Frontier Positions of Nations,"  A key decision in scientific work is whether to build on novel or
well-established ideas. Because exploiting new ideas is often harder than more
conventional science, novel work can be especially dependent on interactions
with colleagues, the training environment, and ready access to potential
collaborators. Location may thus influence the tendency to pursue work that is
close to the edge of the scientific frontier in the sense that it builds on
recent ideas. We calculate for each nation its position relative to the edge of
the scientific frontier by measuring its propensity to build on relatively new
ideas in biomedical research. Text analysis of 20+ million publications shows
that the United States and South Korea have the highest tendencies for novel
science. China has become a leader in favoring newer ideas when working with
basic science ideas and research tools, but is still slow to adopt new clinical
ideas. Many locations remain far behind the leaders in terms of their tendency
to work with novel ideas, indicating that the world is far from flat in this
regard.
"
2042,"The archive solution for distributed workflow management agents of the
  CMS experiment at LHC","  The CMS experiment at the CERN LHC developed the Workflow Management Archive
system to persistently store unstructured framework job report documents
produced by distributed workflow management agents. In this paper we present
its architecture, implementation, deployment, and integration with the CMS and
CERN computing infrastructures, such as central HDFS and Hadoop Spark cluster.
The system leverages modern technologies such as a document oriented database
and the Hadoop eco-system to provide the necessary flexibility to reliably
process, store, and aggregate $\mathcal{O}$(1M) documents on a daily basis. We
describe the data transformation, the short and long term storage layers, the
query language, along with the aggregation pipeline developed to visualize
various performance metrics to assist CMS data operators in assessing the
performance of the CMS computing system.
"
2043,The efficiency of community detection by most similar node pairs,"  Community analysis is an important way to ascertain whether or not a complex
system consists of sub-structures with different properties. In this paper, we
give a two level community structure analysis for the SSCI journal system by
most similar co-citation pattern. Five different strategies for the selection
of most similar node (journal) pairs are introduced. The efficiency is checked
by the normalized mutual information technique. Statistical properties and
comparisons of the community results show that both of the two level detection
could give instructional information for the community structure of complex
systems. Further comparisons of the five strategies indicates that, the most
efficient strategy is to assign nodes with maximum similarity into the same
community whether the similarity information is complete or not, while random
selection generates small world local community with no inside order. These
results give valuable indication for efficient community detection by most
similar node pairs.
"
2044,"Towards the social media studies of science: social media metrics,
  present and future","  In this paper we aim at providing a general reflection around the present and
future of social media metrics (or altmetrics) and how they could evolve into a
new discipline focused on the study of the relationships and interactions
between science and social media, in what could be seen as the social media
studies of science.
"
2045,"Knowledge Organization Systems (KOS) in the Semantic Web: A
  Multi-Dimensional Review","  Since the Simple Knowledge Organization System (SKOS) specification and its
SKOS eXtension for Labels (SKOS-XL) became formal W3C recommendations in 2009 a
significant number of conventional knowledge organization systems (KOS)
(including thesauri, classification schemes, name authorities, and lists of
codes and terms, produced before the arrival of the ontology-wave) have made
their journeys to join the Semantic Web mainstream. This paper uses ""LOD KOS""
as an umbrella term to refer to all of the value vocabularies and lightweight
ontologies within the Semantic Web framework. The paper provides an overview of
what the LOD KOS movement has brought to various communities and users. These
are not limited to the colonies of the value vocabulary constructors and
providers, nor the catalogers and indexers who have a long history of applying
the vocabularies to their products. The LOD dataset producers and LOD service
providers, the information architects and interface designers, and researchers
in sciences and humanities, are also direct beneficiaries of LOD KOS. The paper
examines a set of the collected cases (experimental or in real applications)
and aims to find the usages of LOD KOS in order to share the practices and
ideas among communities and users. Through the viewpoints of a number of
different user groups, the functions of LOD KOS are examined from multiple
dimensions. This paper focuses on the LOD dataset producers, vocabulary
producers, and researchers (as end-users of KOS).
"
2046,Understanding Data Search as a Socio-technical Practice,"  Open research data are heralded as having the potential to increase
effectiveness, productivity, and reproducibility in science, but little is
known about the actual practices involved in data search. The socio-technical
problem of locating data for reuse is often reduced to the technological
dimension of designing data search systems. We combine a bibliometric study of
the current academic discourse around data search with interviews with data
seekers. In this article, we explore how adopting a contextual, socio-technical
perspective can help to understand user practices and behavior and ultimately
help to improve the design of data discovery systems.
"
2047,"The dynamically changing publication universe as a reference point in
  national impact evaluation: A counterfactual case study on the Chinese
  publication growth","  National bibliometric performance is commonly measured via relative impact
indicators which appraise absolute national values through a global
environment. Consequenty the resulting impact values mirror changes in the
national performance as well as in its embedding. In order to assess the
importance of the environment in this ratio, we analyse the increase in Chinese
publications as an example for a structural change altering the whole database.
Via a counterfactual comparison we quantify how Chinese publications benefit a
large set of countries on their impact values, identify explanatory factors and
describe the underelying mechanism due to longer reference lists and a
non-uniform citation distribution among recipient countries. We argue that such
structural changes in the environment have to be taken into account for an
unbiased measurement of national bibliometric performance.
"
2048,"TexT - Text Extractor Tool for Handwritten Document Transcription and
  Annotation","  This paper presents a framework for semi-automatic transcription of
large-scale historical handwritten documents and proposes a simple
user-friendly text extractor tool, TexT for transcription. The proposed
approach provides a quick and easy transcription of text using computer
assisted interactive technique. The algorithm finds multiple occurrences of the
marked text on-the-fly using a word spotting system. TexT is also capable of
performing on-the-fly annotation of handwritten text with automatic generation
of ground truth labels, and dynamic adjustment and correction of user generated
bounding box annotations with the word being perfectly encapsulated. The user
can view the document and the found words in the original form or with
background noise removed for easier visualization of transcription results. The
effectiveness of TexT is demonstrated on an archival manuscript collection from
well-known publicly available dataset.
"
2049,Structure and Evolution of Indian Physics Co-authorship Networks,"  We trace the evolution of Indian physics community from 1919 to 2013 by
analysing the coauthorship network constructed from papers published by authors
in India in American Physical Society journals. We make inferences on Indias
contribution to different branches of Physics and identify the most influential
Indian physicists at different time periods. The relative contribution of India
to global physics publication(research) and its variation across subfields of
Physics is assessed. We extract the changing collaboration pattern of authors
between Indian physicists through various network measures. We study the
evolution of Indian physics communities and trace the mean life and
stationarity of communities by size in different APS journals. We map the
transition of authors between communities of different sizes from 1970 to 2013,
capturing their birth, growth, merger and collapse. We find that Indian-Foreign
collaborations are increasing at a faster pace compared to the Indian-Indian.
We observe that the degree distribution of Indian collaboration networks
follows the power law, with distinct patterns between Physical Review A, B and
E, and high energy physics journals Physical Review C and D, and Physical
Review Letters. In almost every measure, we observe strong structural
differences between low-energy and high-energy physics journals.
"
2050,"Citation Analysis of Innovative ICT and Advances of Governance
  (2008-2017)","  This paper opens by introducing the Internet Plus Government (IPG), a new
government initiative emerging in the last decade. To understand benefits and
challenges associated with this initiative worldwide, we conducted analyses on
research articles published in the e-governance area between 2008 and 2017.
Content analysis and citation analysis were performed on 2105 articles to
address three questions: (1) What types of new ICT have been adopted in the IPG
initiative in the past decade? (2) How did scholars investigate interactions
between the new ICTs and governance core to IPG? (3) How did the new ICTs
interact and shape while also being shaped by the evolution of governance in
the past decade? Our analysis suggests that IPG initiative has enriched the
government information infrastructure. It presented opportunities to accumulate
and use huge volume of data for better decision making and proactive
government-citizen interaction. At the same time, the advance of open data, the
widespread use of social media and the potential of data analytics also
generated great pressure to address challenging questions and issues in the
domain of e-democracy.
"
2051,"The Bibliotelemetry of Information and Environment: an Evaluation of
  IoT-Powered Recommender Systems","  Internet of Things (IoT) infrastructure within the physical library
environment is the basis for an integrative, hybrid approach to digital
resource recommenders. The IoT infrastructure provides mobile, dynamic
wayfinding support for items in the collection, which includes features for
location-based recommendations. A modular evaluation and analysis herein
clarified the nature of users' requests for recommendations based on their
location and describes subject areas of the library for which users request
recommendations. The modular mobile design allowed for deep exploration of
users' bibliographic identifiers throughout the global module system, serving
to provide context to the browsing data that are the focus of this study.
Bibliotelemetry is introduced as an evaluation method for IoT middleware within
library collections.
"
2052,"Ontology-based Adaptive e-Textbook Platform for Student and Machine
  Co-Learning","  The use of electronic textbooks (e-book) has been heavily studied over the
years due to their flexibility, accessibility, interactivity and extensibility.
Yet current shortcomings of e-book, which is often just a digitized version of
the original book, does not encourage adoption. Consequently, this leads to a
rethinking of e-book that should incorporate current technologies to augment
its capabilities, where inclusion of information search and organization tools
have shown to be favorable. This paper is on a preliminary work to add
intelligence into such tools in terms of information retrieval. Construction of
knowledge graph for e-book material with little overhead is first introduced.
Information retrieval through typed similarity query is then performed via
random walk. Case study demonstrate the applicability of the e-book platform,
with promising application and advancement in the area of electronic textbooks.
"
2053,"Using Deep Learning for Title-Based Semantic Subject Indexing to Reach
  Competitive Performance to Full-Text","  For (semi-)automated subject indexing systems in digital libraries, it is
often more practical to use metadata such as the title of a publication instead
of the full-text or the abstract. Therefore, it is desirable to have good text
mining and text classification algorithms that operate well already on the
title of a publication. So far, the classification performance on titles is not
competitive with the performance on the full-texts if the same number of
training samples is used for training. However, it is much easier to obtain
title data in large quantities and to use it for training than full-text data.
In this paper, we investigate the question how models obtained from training on
increasing amounts of title training data compare to models from training on a
constant number of full-texts. We evaluate this question on a large-scale
dataset from the medical domain (PubMed) and from economics (EconBiz). In these
datasets, the titles and annotations of millions of publications are available,
and they outnumber the available full-texts by a factor of 20 and 15,
respectively. To exploit these large amounts of data to their full potential,
we develop three strong deep learning classifiers and evaluate their
performance on the two datasets. The results are promising. On the EconBiz
dataset, all three classifiers outperform their full-text counterparts by a
large margin. The best title-based classifier outperforms the best full-text
method by 9.4%. On the PubMed dataset, the best title-based method almost
reaches the performance of the best full-text classifier, with a difference of
only 2.9%.
"
2054,"Identifying single influential publications in a research field: New
  analysis opportunities of the CRExplorer","  Reference Publication Year Spectroscopy (RPYS) has been developed for
identifying the cited references (CRs) with the greatest influence in a given
paper set (mostly sets of papers on certain topics or fields). The program
CRExplorer (see www.crexplorer.net) was specifically developed by Thor, Marx,
Leydesdorff, and Bornmann (2016a, 2016b) for applying RPYS to publication sets
downloaded from Scopus or Web of Science. In this study, we present some
advanced methods which have been newly developed for CRExplorer. These methods
are able to identify and characterize the CRs which have been influential
across a longer period (many citing years). The new methods are demonstrated in
this study using all the papers published in Scientometrics between 1978 and
2016. The indicators N_TOP50, N_TOP25, and N_TOP10 can be used to identify
those CRs which belong to the 50%, 25%, or 10% most frequently cited
publications (CRs) over many citing publication years. In the Scientometrics
dataset, for example, Lotka's (1926) paper on the distribution of scientific
productivity belongs to the top 10% publications (CRs) in 36 citing years.
Furthermore, the new version of CRExplorer analyzes the impact sequence of CRs
across citing years. CRs can have below average (-), average (0), or above
average (+) impact in citing years (whereby average is meant in the sense of
expected values). The sequence (e.g. 00++---0--00) is used by the program to
identify papers with typical impact distributions. For example, CRs can have
early, but not late impact (""hot papers"", e.g. +++---) or vice versa (""sleeping
beauties"", e.g. ---0000---++).
"
2055,"The Journal Impact Factor: A brief history, critique, and discussion of
  adverse effects","  The Journal Impact Factor (JIF) is, by far, the most discussed bibliometric
indicator. Since its introduction over 40 years ago, it has had enormous
effects on the scientific ecosystem: transforming the publishing industry,
shaping hiring practices and the allocation of resources, and, as a result,
reorienting the research activities and dissemination practices of scholars.
Given both the ubiquity and impact of the indicator, the JIF has been widely
dissected and debated by scholars of every disciplinary orientation. Drawing on
the existing literature as well as on original research, this chapter provides
a brief history of the indicator and highlights well-known limitations-such as
the asymmetry between the numerator and the denominator, differences across
disciplines, the insufficient citation window, and the skewness of the
underlying citation distributions. The inflation of the JIF and the weakening
predictive power is discussed, as well as the adverse effects on the behaviors
of individual actors and the research enterprise. Alternative journal-based
indicators are described and the chapter concludes with a call for responsible
application and a commentary on future developments in journal indicators.
"
2056,"Predictive Effects of Novelty Measured by Temporal Embeddings on the
  Growth of Scientific Literature","  Novel scientific knowledge is constantly produced by the scientific
community. Understanding the level of novelty characterized by scientific
literature is key for modeling scientific dynamics and analyzing the growth
mechanisms of scientific knowledge. Metrics derived from bibliometrics and
citation analysis were effectively used to characterize the novelty in
scientific development. However, time is required before we can observe links
between documents such as citation links or patterns derived from the links,
which makes these techniques more effective for retrospective analysis than
predictive analysis. In this study, we present a new approach to measuring the
novelty of a research topic in a scientific community over a specific period by
tracking semantic changes of the terms and characterizing the research topic in
their usage context. The semantic changes are derived from the text data of
scientific literature by temporal embedding learning techniques. We validated
the effects of the proposed novelty metric on predicting the future growth of
scientific publications and investigated the relations between novelty and
growth by panel data analysis applied in a large-scale publication dataset
(MEDLINE/PubMed). Key findings based on the statistical investigation indicate
that the novelty metric has significant predictive effects on the growth of
scientific literature and the predictive effects may last for more than ten
years. We demonstrated the effectiveness and practical implications of the
novelty metric in three case studies.
"
2057,"Hyper-Hue and EMAP on Hyperspectral Images for Supervised Layer
  Decomposition of Old Master Drawings","  Old master drawings were mostly created step by step in several layers using
different materials. To art historians and restorers, examination of these
layers brings various insights into the artistic work process and helps to
answer questions about the object, its attribution and its authenticity.
However, these layers typically overlap and are oftentimes difficult to
differentiate with the unaided eye. For example, a common layer combination is
red chalk under ink.
  In this work, we propose an image processing pipeline that operates on
hyperspectral images to separate such layers. Using this pipeline, we show that
hyperspectral images enable better layer separation than RGB images, and that
spectral focus stacking aids the layer separation. In particular, we propose to
use two descriptors in hyperspectral historical document analysis, namely
hyper-hue and extended multi-attribute profile (EMAP). Our comparative results
with other features underline the efficacy of the three proposed improvements.
"
2058,"Data-mining the Foundational Patents of Photovoltaic Materials: An
  application of Patent Citation Spectroscopy","  We apply Patent Citation Spectroscopy (PCS)--originally developed as
Reference Publication Year Spectroscopy for studying landmarks and milestones
in scientific literature--to patent literature classified into the nine
Y-subclasses of the Cooperative Patent Classification (CPC) that describe
material photovoltaic technologies. For this study we extended the routine with
the option to use the advanced search queries at PatentsView. On the basis of
two normalizations of the longitudinal distribution of the publication years of
the patents cited by the retrieved patents, the routine (at
http://www.leydesdorff.net/comins/pcs/index.html) provides a best guess of the
foundational patent for the subject specified in the string. In five of the
nine cases, we found corroborating evidence for the foundational character of
the patent indicated by the routine.
"
2059,Improving Active Learning in Systematic Reviews,"  Systematic reviews are essential to summarizing the results of different
clinical and social science studies. The first step in a systematic review task
is to identify all the studies relevant to the review. The task of identifying
relevant studies for a given systematic review is usually performed manually,
and as a result, involves substantial amounts of expensive human resource.
Lately, there have been some attempts to reduce this manual effort using active
learning. In this work, we build upon some such existing techniques, and
validate by experimenting on a larger and comprehensive dataset than has been
attempted until now. Our experiments provide insights on the use of different
feature extraction models for different disciplines. More importantly, we
identify that a naive active learning based screening process is biased in
favour of selecting similar documents. We aimed to improve the performance of
the screening process using a novel active learning algorithm with success.
Additionally, we propose a mechanism to choose the best feature extraction
method for a given review.
"
2060,Field normalization of scientometric indicators,"  When scientometric indicators are used to compare research units active in
different scientific fields, there often is a need to make corrections for
differences between fields, for instance differences in publication,
collaboration, and citation practices. Field-normalized indicators aim to make
such corrections. The design of these indicators is a significant challenge. We
discuss the main issues in the design of field-normalized indicators, and we
present an overview of different approaches that have been developed for
dealing with the problem of field normalization. We also discuss how
field-normalized indicators can be evaluated, and we consider the sensitivity
of scientometric analyses to the choice of a field normalization approach.
"
2061,A Machine Learning Approach to Quantitative Prosopography,"  Prosopography is an investigation of the common characteristics of a group of
people in history, by a collective study of their lives. It involves a study of
biographies to solve historical problems. If such biographies are unavailable,
surviving documents and secondary biographical data are used. Quantitative
prosopography involves analysis of information from a wide variety of sources
about ""ordinary people"". In this paper, we present a machine learning framework
for automatically designing a people gazetteer which forms the basis of
quantitative prosopographical research. The gazetteer is learnt from the noisy
text of newspapers using a Named Entity Recognizer (NER). It is capable of
identifying influential people from it by making use of a custom designed
Influential Person Index (IPI). Our corpus comprises of 14020 articles from a
local newspaper, ""The Sun"", published from New York in 1896. Some influential
people identified by our algorithm include Captain Donald Hankey (an English
soldier), Dame Nellie Melba (an Australian operatic soprano), Hugh Allan (a
Canadian shipping magnate) and Sir Hugh John McDonald (the first Prime Minister
of Canada).
"
2062,"Could scientists use Altmetric.com scores to predict longer term
  citation counts?","  Altmetrics from Altmetric.com are widely used by publishers and researchers
to give earlier evidence of attention than citation counts. This article
assesses whether Altmetric.com scores are reliable early indicators of likely
future impact and whether they may also reflect non-scholarly impacts. A
preliminary factor analysis suggests that the main altmetric indicator of
scholarly impact is Mendeley reader counts, with weaker news, informational and
social network discussion/promotion dimensions in some fields. Based on a
regression analysis of Altmetric.com data from November 2015 and Scopus
citation counts from October 2017 for articles in 30 narrow fields, only
Mendeley reader counts are consistent predictors of future citation impact.
Most other Altmetric.com scores can help predict future impact in some fields.
Overall, the results confirm that early Altmetric.com scores can predict later
citation counts, although less well than journal impact factors, and the
optimal strategy is to consider both Altmetric.com scores and journal impact
factors. Altmetric.com scores can also reflect dimensions of non-scholarly
impact in some fields.
"
2063,Understanding Web Archiving Services and Their (Mis)Use on Social Media,"  Web archiving services play an increasingly important role in today's
information ecosystem, by ensuring the continuing availability of information,
or by deliberately caching content that might get deleted or removed. Among
these, the Wayback Machine has been proactively archiving, since 2001, versions
of a large number of Web pages, while newer services like archive.is allow
users to create on-demand snapshots of specific Web pages, which serve as time
capsules that can be shared across the Web. In this paper, we present a
large-scale analysis of Web archiving services and their use on social media,
shedding light on the actors involved in this ecosystem, the content that gets
archived, and how it is shared. We crawl and study: 1) 21M URLs from
archive.is, spanning almost two years, and 2) 356K archive.is plus 391K Wayback
Machine URLs that were shared on four social networks: Reddit, Twitter, Gab,
and 4chan's Politically Incorrect board (/pol/) over 14 months. We observe that
news and social media posts are the most common types of content archived,
likely due to their perceived ephemeral and/or controversial nature. Moreover,
URLs of archiving services are extensively shared on ""fringe"" communities
within Reddit and 4chan to preserve possibly contentious content. Lastly, we
find evidence of moderators nudging or even forcing users to use archives,
instead of direct links, for news sources with opposing ideologies, potentially
depriving them of ad revenue.
"
2064,"Best Practices for a Future Open Code Policy: Experiences and Vision of
  the Astrophysics Source Code Library","  We are members of the Astrophysics Source Code Library's Advisory Committee
and its editor-in-chief. The Astrophysics Source Code Library (ASCL, ascl.net)
is a successful initiative that advocates for open research software and
provides an infrastructure for registering, discovering, sharing, and citing
this software. Started in 1999, the ASCL has been expanding in recent years,
with an average of over 200 codes added each year, and now houses over 1,600
code entries.
"
2065,Allegation of scientific misconduct increases Twitter attention,"  The web-based microblogging system Twitter is a very popular altmetrics
source for measuring the broader impact of science. In this case study, we
demonstrate how problematic the use of Twitter data for research evaluation can
be, even though the aspiration of measurement is degraded from impact to
attention measurement. We collected the Twitter data for the paper published by
Yamamizu et al. (2017). An investigative committee found that the main figures
in the paper are fraudulent.
"
2066,"The negative effects of citing with a national orientation in terms of
  recognition: national and international citations in natural-sciences papers
  from Germany, the Netherlands, and the UK","  Nations can be distinguished in terms of whether domestic or international
research is cited. We analyzed the research output in natural sciences of three
leading European research economies (Germany, the Netherlands, and the UK) and
ask where their researchers look for the knowledge that underpins their most
highly-cited papers. Is one internationally oriented or is citation limited to
national resources? Do the citation patterns reflect a growing differentiation
between the domestic and international research enterprise? To evaluate change
over time, we include natural-sciences papers published in the countries from
three publication years: 2004, 2009, and 2014. The results show that articles
co-authored by researchers from Germany or the Netherlands are less likely to
be among the globally most highly-cited articles if they also cite ""domestic""
research (i.e. research authored by authors from the same country). To put this
another way, less well-cited research is more likely to stand on domestic
shoulders and research that becomes more highly-cited is more likely to stand
on international shoulders. A possible reason for the results is that
researchers ""over-cite"" the papers from their own country - lacking the focus
on quality in citing. However, these differences between domestic and
international shoulders are not visible for the UK.
"
2067,"Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An
  Evaluation of Open-Source Bibliographic Reference and Citation Parsers","  Bibliographic reference parsing refers to extracting machine-readable
metadata, such as the names of the authors, the title, or journal name, from
bibliographic reference strings. Many approaches to this problem have been
proposed so far, including regular expressions, knowledge bases and supervised
machine learning. Many open source reference parsers based on various
algorithms are also available. In this paper, we apply, evaluate and compare
ten reference parsing tools in a specific business use case. The tools are
Anystyle-Parser, Biblio, CERMINE, Citation, Citation-Parser, GROBID, ParsCit,
PDFSSA4MET, Reference Tagger and Science Parse, and we compare them in both
their out-of-the-box versions and versions tuned to the project-specific data.
According to our evaluation, the best performing out-of-the-box tool is GROBID
(F1 0.89), followed by CERMINE (F1 0.83) and ParsCit (F1 0.75). We also found
that even though machine learning-based tools and tools based on rules or
regular expressions achieve on average similar precision (0.77 for ML-based
tools vs. 0.76 for non-ML-based tools), applying machine learning-based tools
results in a recall three times higher than in the case of non-ML-based tools
(0.66 vs. 0.22). Our study also confirms that tuning the models to the
task-specific data results in the increase in the quality. The retrained
versions of reference parsers are in all cases better than their out-of-the-box
counterparts; for GROBID F1 increased by 3% (0.92 vs. 0.89), for CERMINE by 11%
(0.92 vs. 0.83), and for ParsCit by 16% (0.87 vs. 0.75).
"
2068,"A Method for Discovering and Extracting Author Contributions Information
  from Scientific Biomedical Publications","  Creating scientific publications is a complex process, typically composed of
a number of different activities, such as designing the experiments, data
preparation, programming software and writing and editing the manuscript. The
information about the contributions of individual authors of a paper is
important in the context of assessing authors' scientific achievements. Some
publications in biomedical disciplines contain a description of authors' roles
in the form of a short section written in natural language, typically entitled
""Authors' contributions"". In this paper, we present an analysis of roles
commonly appearing in the content of these sections, and propose an algorithm
for automatic extraction of authors' roles from natural language text in
scientific publications. During the first part of the study, we used clustering
techniques, as well as Open Information Extraction (OpenIE), to
semi-automatically discover the most popular roles within a corpus of 2,000
contributions sections obtained from PubMed Central resources. The roles
discovered by our approach include: experimenting (1,743 instances, 17% of the
entire role set within the corpus), analysis (1,343, 16%), study design (1,132,
13%), interpretation (879, 10%), conceptualization (865, 10%), paper reading
(823, 10%), paper writing (724, 8%), paper review (501, 6%), paper drafting
(351, 4%), coordination (319, 4%), data collection (76, 1%), paper review (41,
0.5%) and literature review (41, 0.5%). Discovered roles were then used to
automatically build a training set for the supervised role extractor, based on
Naive Bayes algorithm. According to the evaluation we performed, the proposed
role extraction algorithm is able to extract the roles from the text with
precision 0.71, recall 0.49 and F1 0.58.
"
2069,The Social Structure of Consensus in Scientific Review,"  Personal connections between creators and evaluators of scientific works are
ubiquitous, and the possibility of bias ever-present. Although connections have
been shown to bias prospective judgments of (uncertain) future performance, it
is unknown whether such biases occur in the much more concrete task of
assessing the scientific validity of already completed work, and if so, why.
This study presents evidence that personal connections between authors and
reviewers of neuroscience manuscripts are associated with biased judgments and
explores the mechanisms driving the effect. Using reviews from 7,981
neuroscience manuscripts submitted to the journal PLOS ONE, which instructs
reviewers to evaluate manuscripts only on scientific validity, we find that
reviewers favored authors close in the co-authorship network by ~0.11 points on
a 1.0 - 4.0 scale for each step of proximity. PLOS ONE's validity-focused
review and the substantial amount of favoritism shown by distant vs. very
distant reviewers, both of whom should have little to gain from nepotism, point
to the central role of substantive disagreements between scientists in
different ""schools of thought."" The results suggest that removing bias from
peer review cannot be accomplished simply by recusing the closely-connected
reviewers, and highlight the value of recruiting reviewers embedded in diverse
professional networks.
"
2070,"An AI aid to the editors. Exploring the possibility of an AI assisted
  article classification system","  This work is a preliminary exploratory study of how we could progress a step
towards an AI assisted article classification sys- tem in academia. The
proposed system aims to aid the journal editors in their decisions by
pinpointing the potential weaknesses or strengths of a submitted manuscript.
From a large collection of articles and corresponding author-editor
interactions we explore the possible reasons that lead to a paper being not
forwarded for review. Our investigation reveals that in most cases either it is
be- cause the prospective manuscript is out of scope of the journal or the
manuscript does not satisfy the minimum quality requirements to maintain the
standard of the journal. We extract several features to quantify the quality of
a paper and the degree of in-scope explor- ing keyword search, citation
analysis, reputations of authors and affiliations, similarity with respect to
accepted papers. With these features we train standard machine learning
classifiers to develop a classification system. On a decent set of test data
our approach yields promising results across 3 different journals. We believe
that our approach is generic and could be adapted to other journals with
appropriate adjustments.
"
2071,"Can Common Crawl reliably track persistent identifier (PID) use over
  time?","  We report here on the results of two studies using two and four monthly web
crawls respectively from the Common Crawl (CC) initiative between 2014 and
2017, whose initial goal was to provide empirical evidence for the changing
patterns of use of so-called persistent identifiers. This paper focusses on the
tooling needed for dealing with CC data, and the problems we found with it. The
first study is based on over $10^{12}$ URIs from over $5 * 10^9$ pages crawled
in April 2014 and April 2017, the second study adds a further $3 * 10^9$ pages
from the April 2015 and April 2016 crawls. We conclude with suggestions on
specific actions needed to enable studies based on CC to give reliable
longitudinal information.
"
2072,"Astrophysicists and physicists as creators of ArXiv-based commenting
  resources for their research communities. An initial survey","  This paper conveys the outcomes of what results to be the first, though
initial, overview of commenting platforms and related 2.0 resources born within
and for the astrophysical community (from 2004 to 2016). Experiences were
added, mainly in the physics domain, for a total of 22 major items, including
four epijournals, and four supplementary resources, thus casting some light
onto an unexpected richness and consonance of endeavours. These experiences
rest almost entirely on the contents of the database ArXiv, which adds to its
merits that of potentially setting the grounds for web 2.0 resources, and
research behaviours, to be explored.
  Most of the experiences retrieved are UK and US based, but the resulting
picture is international, as various European countries, China and Australia
have been actively involved.
  Final remarks about creation patterns and outcome of these resources are
outlined. The results integrate the previous studies according to which the web
2.0 is presently of limited use for communication in astrophysics and vouch for
a role of researchers in the shaping of their own professional communication
tools that is greater than expected. Collaterally, some aspects of ArXiv s
recent pathway towards partial inclusion of web 2.0 features are touched upon.
Further investigation is hoped for.
"
2073,"Uptake and outcome of manuscripts in Nature journals by review model and
  author characteristics","  Double-blind peer review has been proposed as a possible solution to avoid
implicit referee bias in academic publishing. The aims of this study are to
analyse the demographics of corresponding authors choosing double blind peer
review, and to identify differences in the editorial outcome of manuscripts
depending on their review model. Data includes 128,454 manuscripts received
between March 2015 and February 2017 by 25 Nature-branded journals. Author
uptake for double-blind was 12%. We found a small but significant association
between journal tier and review type. We found no statistically significant
difference in the distribution of peer review model between males and females.
We found that corresponding authors from the less prestigious institutions are
more likely to choose double-blind review. In the ten countries with the
highest number of submissions, we found a small but significant association
between country and review type. The outcome at both first decision and post
review is significantly more negative (i.e. a higher likelihood for rejection)
for double than single-blind papers. Authors choose double-blind review more
frequently when they submit to more prestigious journals, they are affiliated
with less prestigious institutions or they are from specific countries; the
double-blind option is also linked to less successful editorial outcomes.
"
2074,"The paradigm of complexity. Contributions for hypertext's formal
  approaches","  This article argues for a return to formal approaches of hypertext and builds
on the paradigm of complexity to develop the idea of ""hypermediator website"". A
hypermediator website is an intermediate device between a digitalization of
book culture and a ""real"" hypertext writing. If our thinking on the
hypermediator website joined the hypertext's notions and the databases, it
differs by the relationship reader-device no longer based on information search
query but using the visualization of the information.
"
2075,"Digital Data Archives as Knowledge Infrastructures: Mediating Data
  Sharing and Reuse","  Digital archives are the preferred means for open access to research data.
They play essential roles in knowledge infrastructures - robust networks of
people, artifacts, and institutions - but little is known about how they
mediate information exchange between stakeholders. We open the ""black box"" of
data archives by studying DANS, the Data Archiving and Networked Services
institute of The Netherlands, which manages 50+ years of data from the social
sciences, humanities, and other domains. Our interviews, weblogs, ethnography,
and document analyses reveal that a few large contributors provide a steady
flow of content, but most are academic researchers who submit datasets
infrequently and often restrict access to their files. Consumers are a diverse
group that overlaps minimally with contributors. Archivists devote about half
their time to aiding contributors with curation processes and half to assisting
consumers. Given the diversity and infrequency of usage, human assistance in
curation and search remains essential. DANS' knowledge infrastructure
encompasses public and private stakeholders who contribute, consume, harvest,
and serve their data - many of whom did not exist at the time the DANS
collections originated - reinforcing the need for continuous investment in
digital data archives as their communities, technologies, and services evolve.
"
2076,Book reviews in academic journals: patterns and dynamics,"  Book reviews play important roles in scholarly communication especially in
arts and humanities disciplines. By using Web of Science's Science Citation
Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation
Index, this study probed the patterns and dynamics of book reviews within these
three indexes empirically during the past decade (2006-2015). We found that the
absolute numbers of book reviews among all the three indexes were relatively
stable but the relative shares were decreasing. Book reviews were very common
in arts and humanities, common in social sciences, but rare in natural
sciences. Book reviews are mainly contributed by authors from developed
economies such as the USA and the UK. Oppositely, scholars from China and Japan
are unlikely to contribute to book reviews.
"
2077,"Developing indicators on Open Access by combining evidence from diverse
  data sources","  In the last couple of years, the role of Open Access (OA) publishing has
become central in science management and research policy. In the UK and the
Netherlands, national OA mandates require the scientific community to seriously
consider publishing research outputs in OA forms. At the same time, other
elements of Open Science are becoming also part of the debate, thus including
not only publishing research outputs but also other related aspects of the
chain of scientific knowledge production such as open peer review and open
data. From a research management point of view, it is important to keep track
of the progress made in the OA publishing debate. Until now, this has been
quite problematic, given the fact that OA as a topic is hard to grasp by
bibliometric methods, as most databases supporting bibliometric data lack
exhaustive and accurate open access labelling of scientific publications. In
this study, we present a methodology that systematically creates OA labels for
large sets of publications processed in the Web of Science database. The
methodology is based on the combination of diverse data sources that provide
evidence of publications being OA
"
2078,"Open Data, Grey Data, and Stewardship: Universities at the Privacy
  Frontier","  As universities recognize the inherent value in the data they collect and
hold, they encounter unforeseen challenges in stewarding those data in ways
that balance accountability, transparency, and protection of privacy, academic
freedom, and intellectual property. Two parallel developments in academic data
collection are converging: (1) open access requirements, whereby researchers
must provide access to their data as a condition of obtaining grant funding or
publishing results in journals; and (2) the vast accumulation of 'grey data'
about individuals in their daily activities of research, teaching, learning,
services, and administration. The boundaries between research and grey data are
blurring, making it more difficult to assess the risks and responsibilities
associated with any data collection. Many sets of data, both research and grey,
fall outside privacy regulations such as HIPAA, FERPA, and PII. Universities
are exploiting these data for research, learning analytics, faculty evaluation,
strategic decisions, and other sensitive matters. Commercial entities are
besieging universities with requests for access to data or for partnerships to
mine them. The privacy frontier facing research universities spans open access
practices, uses and misuses of data, public records requests, cyber risk, and
curating data for privacy protection. This paper explores the competing values
inherent in data stewardship and makes recommendations for practice, drawing on
the pioneering work of the University of California in privacy and information
security, data governance, and cyber risk.
"
2079,The ESO Survey of Non-Publishing Programmes,"  One of the classic ways to measure the success of a scientific facility is
the publication return, which is defined as the number of refereed papers
produced per unit of allocated resources (for example, telescope time or
proposals). The recent studies by Sterzik et al. (2015, 2016) have shown that
30-50 % of the programmes allocated time at ESO do not produce a refereed
publication. While this may be inherent to the scientific process, this finding
prompted further investigation. For this purpose, ESO conducted a Survey of
Non-Publishing Programmes (SNPP) within the activities of the Time Allocation
Working Group, similar to the monitoring campaign that was recently implemented
at ALMA (Stoehr et al. 2016). The SNPP targeted 1278 programmes scheduled
between ESO Periods 78 and 90 (October 2006 to March 2013) that had not
published a refereed paper as of April 2016. The poll was launched on 6 May
2016, remained open for four weeks, and returned 965 valid responses. This
article summarises and discusses the results of this survey, the first of its
kind at ESO.
"
2080,Terminologies for Reproducible Research,"  Reproducible research---by its many names---has come to be regarded as a key
concern across disciplines and stakeholder groups. Funding agencies and
journals, professional societies and even mass media are paying attention,
often focusing on the so-called ""crisis"" of reproducibility. One big problem
keeps coming up among those seeking to tackle the issue: different groups are
using terminologies in utter contradiction with each other. Looking at a broad
sample of publications in different fields, we can classify their terminology
via decision tree: they either, A---make no distinction between the words
reproduce and replicate, or B---use them distinctly. If B, then they are
commonly divided in two camps. In a spectrum of concerns that starts at a
minimum standard of ""same data+same methods=same results,"" to ""new data and/or
new methods in an independent study=same findings,"" group 1 calls the minimum
standard reproduce, while group 2 calls it replicate. This direct swap of the
two terms aggravates an already weighty issue. By attempting to inventory the
terminologies across disciplines, I hope that some patterns will emerge to help
us resolve the contradictions.
"
2081,"To the problem of ""The Instrumental complex for ontological engineering
  purpose"" software system design","  The given work describes methodological principles of design instrumental
complex of ontological purpose. Instrumental complex intends for the
implementation of the integrated information technologies automated build of
domain ontologies. Results focus on enhancing the effectiveness of the
automatic analysis and understanding of natural-language texts, building of
knowledge description of subject areas (primarily in the area of science and
technology) and for interdisciplinary research in conjunction with the solution
of complex problems.
"
2082,"Astrolabe: Curating, Linking and Computing Astronomy's Dark Data","  Where appropriate repositories are not available to support all relevant
astronomical data products, data can fall into darkness: unseen and unavailable
for future reference and re-use. Some data in this category are legacy or old
data, but newer datasets are also often uncurated and could remain ""dark"". This
paper provides a description of the design motivation and development of
Astrolabe, a cyberinfrastructure project that addresses a set of community
recommendations for locating and ensuring the long-term curation of dark or
otherwise at-risk data and integrated computing. This paper also describes the
outcomes of the series of community workshops that informed creation of
Astrolabe. According to participants in these workshops, much astronomical dark
data currently exist that are not curated elsewhere, as well as software that
can only be executed by a few individuals and therefore becomes unusable
because of changes in computing platforms. Astronomical research questions and
challenges would be better addressed with integrated data and computational
resources that fall outside the scope of existing observatory and space mission
projects. As a solution, the design of the Astrolabe system is aimed at
developing new resources for management of astronomical data. The project is
based in CyVerse cyberinfrastructure technology and is a collaboration between
the University of Arizona and the American Astronomical Society. Overall the
project aims to support open access to research data by leveraging existing
cyberinfrastructure resources and promoting scientific discovery by making
potentially-useful data in a computable format broadly available to the
astronomical community.
"
2083,Automated Early Leaderboard Generation From Comparative Tables,"  A leaderboard is a tabular presentation of performance scores of the best
competing techniques that address a specific scientific problem. Manually
maintained leaderboards take time to emerge, which induces a latency in
performance discovery and meaningful comparison. This can delay dissemination
of best practices to non-experts and practitioners. Regarding papers as proxies
for techniques, we present a new system to automatically discover and maintain
leaderboards in the form of partial orders between papers, based on performance
reported therein. In principle, a leaderboard depends on the task, data set,
other experimental settings, and the choice of performance metrics. Often there
are also tradeoffs between different metrics. Thus, leaderboard discovery is
not just a matter of accurately extracting performance numbers and comparing
them. In fact, the levels of noise and uncertainty around performance
comparisons are so large that reliable traditional extraction is infeasible. We
mitigate these challenges by using relatively cleaner, structured parts of the
papers, e.g., performance tables. We propose a novel performance improvement
graph with papers as nodes, where edges encode noisy performance comparison
information extracted from tables. Every individual performance edge is
extracted from a table with citations to other papers. These extractions
resemble (noisy) outcomes of 'matches' in an incomplete tournament. We propose
several approaches to rank papers from these noisy 'match' outcomes. We show
that our ranking scheme can reproduce various manually curated leaderboards
very well. Using widely-used lists of state-of-the-art papers in 27 areas of
Computer Science, we demonstrate that our system produces very reliable
rankings.
"
2084,Do Citations and Readership Identify Seminal Publications?,"  In this paper, we show that citation counts work better than a random
baseline (by a margin of 10%) in distinguishing excellent research, while
Mendeley reader counts don't work better than the baseline. Specifically, we
study the potential of these metrics for distinguishing publications that
caused a change in a research field from those that have not. The experiment
has been conducted on a new dataset for bibliometric research called
TrueImpactDataset. TrueImpactDataset is a collection of research publications
of two types -- research papers which are considered seminal works in their
area and papers which provide a literature review of a research area. We
provide overview statistics of the dataset and propose to use it for validating
research evaluation metrics. Using the dataset, we conduct a set of experiments
to study how citation and reader counts perform in distinguishing these
publication types, following the intuition that causing a change in a field
signifies research contribution. We show that citation counts help in
distinguishing research that strongly influenced later developments from works
that predominantly discuss the current state of the art with a degree of
accuracy (63%, i.e. 10% over the random baseline). In all setups, Mendeley
reader counts perform worse than a random baseline.
"
2085,"'Getting out of the closet': Scientific authorship of literary fiction
  and knowledge transfer","  Some scientists write literary fiction books in their spare time. If these
books contain scientific knowledge, literary fiction becomes a mechanism of
knowledge transfer. In this case, we could conceptualize literary fiction as
non-formal knowledge transfer. We model knowledge transfer via literary fiction
as a function of the type of scientist (academic or non-academic) and his/her
scientific field. Academic scientists are those employed in academia and public
research organizations whereas non-academic scientists are those with a
scientific background employed in other sectors. We also distinguish between
direct knowledge transfer (the book includes the scientist's research topics),
indirect knowledge transfer (scientific authors talk about their research with
cultural agents) and reverse knowledge transfer (cultural agents give
scientists ideas for future research). Through mixed-methods research and a
sample from Spain, we find that scientific authorship accounts for a
considerable percentage of all literary fiction authorship. Academic scientists
do not transfer knowledge directly so often as non-academic scientists, but the
former engage into indirect and reverse transfer knowledge more often than the
latter. Scientists from History stand out in direct knowledge transfer. We draw
propositions about the role of the academic logic and scientific field on
knowledge transfer via literary fiction. We advance some tentative conclusions
regarding the consideration of scientific authorship of literary fiction as a
valuable knowledge transfer mechanism.
"
2086,Classification of Scientific Papers With Big Data Technologies,"  Data sizes that cannot be processed by conventional data storage and analysis
systems are named as Big Data.It also refers to nex technologies developed to
store, process and analyze large amounts of data. Automatic information
retrieval about the contents of a large number of documents produced by
different sources, identifying research fields and topics, extraction of the
document abstracts, or discovering patterns are some of the topics that have
been studied in the field of big data.In this study, Naive Bayes classification
algorithm, which is run on a data set consisting of scientific articles, has
been tried to automatically determine the classes to which these documents
belong. We have developed an efficient system that can analyze the Turkish
scientific documents with the distributed document classification algorithm run
on the Cloud Computing infrastructure. The Apache Mahout library is used in the
study. The servers required for classifying and clustering distributed
documents are
"
2087,"The influence of online posting dates on the bibliometric indicators of
  scientific articles","  This article analyses the difference in timing between the online
availability of articles and their corresponding print publication and how it
affects two bibliometric indicators: Journal Impact Factor (JIF) and Immediacy
Index. This research examined 18,526 articles, the complete collection of
articles and reviews published by a set of 61 journals on Urology and
Nephrology in 2013 and 2014. The findings suggest that Advance Online
Publication (AOP) accelerates the citation of articles and affects the JIF and
Immediacy Index values. Regarding the JIF values, the comparison between
journals with or without AOP showed statistically significant differences
(P=0.001, Mann-Whitney U test). The Spearman's correlation between the JIF and
the median online-to-print publication delay was not statistically significant.
As to the Immediacy Index, a significant Spearman's correlation (rs=0.280,
P=0.029) was found regarding the median online-to-print publication delays for
journals published in 2014, although no statistically significant correlation
was found for those published in 2013. Most journals examined (n=52 out of 61)
published their articles in AOP. The analysis also showed different publisher
practices: eight journals did not include the online posting dates in the
full-text and nine journals published articles showing two different online
posting dates--the date provided on the journal website and another provided by
Elsevier's Science Direct. These practices suggest the need for transparency
and standardization of the AOP dates of scientific articles for calculating
bibliometric indicators for journals.
"
2088,Point systems in Games for Health: A bibliometric scoping study,"  Very few details about point systems used in games for health are reported in
scientific literature. To shed some light on this topic a bibliometric study,
analyzing the papers containing terms related to games for health and point
systems was performed and a mini taxonomy was derived. The search string game*
AND health AND (point* OR score) AND system* in a Scopus bibliographic database
was used to produce the corpus. We limited the search to articles, reviews and
conference papers written in English and to topics related to medical, health
and social subjects. The corpus papers abstracts and titles were analysed by
VOSviewer and a scientific landscape was generated. The search resulted in a
corpus consisting of 354 papers. The derived taxonomy contains three objects;
video games, serious games and educational games. The biblimetric mapping and
taxonomy revealed some interesting conclusions: (1) the video games have mostly
negative effects on health, (2) the serious games might have both a direct
positive health effects on users and also indirect effects by improved
competencies of health professionals, and (3) the research is concerned not
only to computer based educational games, but also to traditional table games
and sporting games. Based on the derived taxonomy we can conclude that point
systems should reward physical activity and healthy living style and punish
sedentary activities.
"
2089,"Accumulation of Knowledge in Para-Scientific Areas. The Case of Analytic
  Philosophy","  This study analyzes how the accumulation of knowledge takes place in
para-scientific areas, focusing on the case of Analytic Philosophy. The
theoretical framework chosen for the analysis is Kuhn's theory of normal
science. The methodology employed is qualitative citation context analysis. A
sample of 60 papers published in leading Analytic Philosophy journals between
1950 and 2009 is analyzed, and a specific classsificatory scheme is developed
to classify citations according to their epistemological function. Compared to
previous studies of citation context, this is the first paper that includes the
temporal dimension into the analysis of citation context, in order to gain
insights into the process of knowledge accumulation. Interestingly, the results
show that Analytic Philosophy started accumulating after Second World War, but
in a peculiar way. The accumulation was not matched by a corresponding rising
consensus. This can be explained by the hypothesis that AP underwent a process
of fragmentation in sub-fields during the second half of the century.
"
2090,"How does undone science get funded? A bibliometric analysis linking rare
  diseases publications to national and European funding sources","  One of the notable features of undone science debates is how formation of new
interest groups becomes pivotal in mobilizing and championing emerging research
on undone topics. Clearly money is one of the most important mediums through
which different types of actors can support and steer scientists to work on
undone topics. Yet which actors are more visible in their support for
scientific research is something which has seldom been measured. This study
delves into research funding in the context of rare diseases research, a topic
which has evolved from the margins of medical research into a priority area
articulated by many contemporary funding agencies. Rare diseases refer to
conditions affecting relatively few people in a population. Given low
incidences, interest groups have articulated a lack of attention within medical
research compared to more common conditions. The rise to prominence of rare
diseases in research funding policies is often explained in the science studies
literature in terms of effective lobbying by social movements Likewise,
innovative fundraising initiatives, infrastructure building, and close
partnerships with research groups are other means through which interested
actors have sought to build capacity for research into rare medical conditions.
To date however systematic empirical evidence to compare the relative
importance of different actors in funding rare disease research has not been
produced. Building on interest in undone science in STS and science policy
studies, our study hopes to map-out different kinds of funding actors and their
influence on leading scientific research on rare diseases, by use of
bibliometric tools. The approach we are developing relies on the use of Funding
Acknowledgement data provided in Web of Science database.
"
2091,Authorship Attribution Using the Chaos Game Representation,"  The Chaos Game Representation, a method for creating images from nucleotide
sequences, is modified to make images from chunks of text documents. Machine
learning methods are then applied to train classifiers based on authorship.
Experiments are conducted on several benchmark data sets in English, including
the widely used Federalist Papers, and one in Portuguese. Validation results
for the trained classifiers are competitive with the best methods in prior
literature. The methodology is also successfully applied for text
categorization with encouraging results. One classifier method is moreover seen
to hold promise for the task of digital fingerprinting.
"
2092,Diversity from the Topology of Citation Networks,"  We study transitivity in directed acyclic graphs and its usefulness in
capturing nodes that act as bridges between more densely interconnected parts
in such type of network. In transitively reduced citation networks degree
centrality could be used as a measure of interdisciplinarity or diversity. We
study the measure's ability to capture ""diverse"" nodes in random directed
acyclic graphs and citation networks. We show that transitively reduced degree
centrality is capable of capturing ""diverse"" nodes, thus this measure could be
a timely alternative to text analysis techniques for retrieving papers,
influential in a variety of research fields.
"
2093,A Study of Position Bias in Digital Library Recommender Systems,"  ""Position bias"" describes the tendency of users to interact with items on top
of a list with higher probability than with items at a lower position in the
list, regardless of the items' actual relevance. In the domain of recommender
systems, particularly recommender systems in digital libraries, position bias
has received little attention. We conduct a study in a real-world recommender
system that delivered ten million related-article recommendations to the users
of the digital library Sowiport, and the reference manager JabRef.
Recommendations were randomly chosen to be shuffled or non-shuffled, and we
compared click-through rate (CTR) for each rank of the recommendations.
According to our analysis, the CTR for the highest rank in the case of Sowiport
is 53% higher than expected in a hypothetical non-biased situation (0.189% vs.
0.123%). Similarly, in the case of Jabref the highest rank received a CTR of
1.276%, which is 87% higher than expected (0.683%). A chi-squared test confirms
the strong relationship between the rank of the recommendation shown to the
user and whether the user decided to click it (p < 0.01 for both Jabref and
Sowiport). Our study confirms the findings from other domains, that
recommendations in the top positions are more often clicked, regardless of
their actual relevance.
"
2094,"Technological research in the EU is less efficient than the world
  average. EU research policy risks Europeans' future","  We have studied the efficiency of research in the EU by a percentile-based
citation approach that analyzes the distribution of country papers among the
world papers. Going up in the citation scale, the frequency of papers from
efficient countries increases while the frequency from inefficient countries
decreases. In the percentile-based approach, this trend, which is permanent at
any citation level, is measured by the ep index that equals the Ptop 1%/Ptop
10% ratio. By using the ep index we demonstrate that EU research on
fast-evolving technological topics is less efficient than the world average and
that the EU is far from being able to compete with the most advanced countries.
The ep index also shows that the USA is well ahead of the EU in both fast- and
slow-evolving technologies, which suggests that the advantage of the USA over
the EU in innovation is due to low research efficiency in the EU. In accord
with some previous studies, our results show that the European Commission's
ongoing claims about the excellence of EU research are based on a wrong
diagnosis. The EU must focus its research policy on the improvement of its
inefficient research. Otherwise, the future of Europeans is at risk.
"
2095,"Developing a system for securely time-stamping and visualizing the
  changes made to online news content","  Nowadays, the Internet is indispensable when it comes to information
dissemination. People rely on the Internet to inform themselves on current news
events, as well as to verify facts. We, as a community, are quickly approaching
an 'electronic information age' where the majority of information will be
distributed electronically and tools to preserve this information will become
essential. While archiving online digital information is a good way to preserve
online information for future generations, it has many disadvantages including
the easy manipulation of archived information, e.g. by the archiving authority.
Online information is also prone to getting hacked or being taken offline.
Therefore, it is necessary that archived online news information is securely
time-stamped with the date and time when it was first archived in a way that
cannot be manipulated. The process of 'trusted timestamping' is an established
approach for claiming that particular digital information existed at a
particular 'point in time' in the past. However, traditional approaches for
trusted timestamping depend on the time-stamping authority's fidelity. Directly
embedding the hash of a digital file into the blockchain of a cryptocurrency is
a more recent method that allows for secure time-stamping, since digital
information is stored as part of the transaction information in, e.g.
Bitcoin's, blockchain, and not stored at a centralized time-stamping authority.
However, there is no system yet available, which uses this approach for
archiving and time-stamping online news articles. Therefore, the aim of this
thesis is to develop a system that 1) enables decentralized trusted
time-stamping of web and news articles as a means of making future manipulation
of online information identifiable, and 2) allows users to determine the
authenticity of articles by checking different versions of the same article
online.
"
2096,"Personal research information system. About developing the methods for
  searching patent analogs of invention","  The article describes information model and the method for searching patent
analogs for Personal Research Information System.
"
2097,"Can Microsoft Academic assess the early citation impact of in-press
  articles? A multi-discipline exploratory analysis","  Many journals post accepted articles online before they are formally
published in an issue. Early citation impact evidence for these articles could
be helpful for timely research evaluation and to identify potentially important
articles that quickly attract many citations. This article investigates whether
Microsoft Academic can help with this task. For over 65,000 Scopus in-press
articles from 2016 and 2017 across 26 fields, Microsoft Academic found 2-5
times as many citations as Scopus, depending on year and field. From manual
checks of 1,122 Microsoft Academic citations not found in Scopus, Microsoft
Academic's citation indexing was faster but not much wider than Scopus for
journals. It achieved this by associating citations to preprints with their
subsequent in-press versions and by extracting citations from in-press
articles. In some fields its coverage of scholarly digital libraries, such as
arXiv.org, was also an advantage. Thus, Microsoft Academic seems to be a more
comprehensive automatic source of citation counts for in-press articles than
Scopus.
"
2098,Connecting KOSs and the LOD Cloud,"  This paper describes a specific project, the current situation leading to it,
its project design and first results. In particular, we will examine the
terminology employed in the Linked Open Data cloud and compare this to the
terminology employed in both the Universal Decimal Classification and the Basic
Concepts Classification. We will explore whether these classifications can
encourage greater consistency in LOD terminology. We thus hope to link the
largely distinct scholarly literatures that address LOD and KOSs.
"
2099,Content-Based Citation Recommendation,"  We present a content-based method for recommending citations in an academic
paper draft. We embed a given query document into a vector space, then use its
nearest neighbors as candidates, and rerank the candidates using a
discriminative model trained to distinguish between observed and unobserved
citations. Unlike previous work, our method does not require metadata such as
author names which can be missing, e.g., during the peer review process.
Without using metadata, our method outperforms the best reported results on
PubMed and DBLP datasets with relative improvements of over 18% in F1@20 and
over 22% in MRR. We show empirically that, although adding metadata improves
the performance on standard metrics, it favors self-citations which are less
useful in a citation recommendation setup. We release an online portal
(http://labs.semanticscholar.org/citeomatic/) for citation recommendation based
on our method, and a new dataset OpenCorpus of 7 million research articles to
facilitate future research on this task.
"
2100,Digital Archives as Big Data,"  Digital archives contribute to Big data. Combining social network analysis,
coincidence analysis, data reduction, and visual analytics leads to better
characterize topics over time, publishers' main themes and best authors of all
times, according to the British newspaper The Guardian and from the 3 million
records of the British National Bibliography.
"
2101,The Development of Darwin's Origin of Species,"  From 1837, when he returned to England aboard the $\textit{HMS Beagle}$, to
1860, just after publication of $\textit{The Origin of Species}$, Charles
Darwin kept detailed notes of each book he read or wanted to read. His notes
and manuscripts provide information about decades of individual scientific
practice. Previously, we trained topic models on the full texts of each
reading, and applied information-theoretic measures to detect that changes in
his reading patterns coincided with the boundaries of his three major
intellectual projects in the period 1837-1860. In this new work we apply the
reading model to five additional documents, four of them by Darwin: the first
edition of $\textit{The Origin of Species}$, two private essays stating
intermediate forms of his theory in 1842 and 1844, a third essay of disputed
dating, and Alfred Russel Wallace's essay, which Darwin received in 1858. We
address three historical inquiries, previously treated qualitatively: 1) the
mythology of ""Darwin's Delay,"" that despite completing an extensive draft in
1844, Darwin waited until 1859 to publish $\textit{The Origin of Species}$ due
to external pressures; 2) the relationship between Darwin and Wallace's
contemporaneous theories, especially in light of their joint presentation; and
3) dating of the ""Outline and Draft"" which was rediscovered in 1975 and
postulated first as an 1839 draft preceding the Sketch of 1842, then as an
interstitial draft between the 1842 and 1844 essays.
"
2102,"Improving OCR Accuracy on Early Printed Books using Deep Convolutional
  Networks","  This paper proposes a combination of a convolutional and a LSTM network to
improve the accuracy of OCR on early printed books. While the standard model of
line based OCR uses a single LSTM layer, we utilize a CNN- and Pooling-Layer
combination in advance of an LSTM layer. Due to the higher amount of trainable
parameters the performance of the network relies on a high amount of training
examples to unleash its power. Hereby, the error is reduced by a factor of up
to 44%, yielding a CER of 1% and below. To further improve the results we use a
voting mechanism to achieve character error rates (CER) below $0.5%$. The
runtime of the deep model for training and prediction of a book behaves very
similar to a shallow network.
"
2103,"The concordance of field-normalized scores based on Web of Science and
  Microsoft Academic data: A case study in computer sciences","  In order to assess Microsoft Academic as a useful data source for evaluative
bibliometrics it is crucial to know, if citation counts from Microsoft Academic
could be used in common normalization procedures and whether the normalized
scores agree with the scores calculated on the basis of established databases.
To this end, we calculate the field-normalized citation scores of the
publications of a computer science institute based on Microsoft Academic and
the Web of Science and estimate the statistical concordance of the scores. Our
results suggest that field-normalized citation scores can be calculated with
Microsoft Academic and that these scores are in good agreement with the
corresponding scores from the Web of Science.
"
2104,"Discontinuities in Citation Relations among Journals: Self-organized
  Criticality as a Model of Scientific Revolutions and Change","  Using three-year moving averages of the complete Journal Citation Reports
1994-2016 of the Science Citation Index and the Social Sciences Citation Index
(combined), we analyze links between citing and cited journals in terms of (1)
whether discontinuities among the networks of consecutive years have occurred;
(2) are these discontinuities relatively isolated or networked? (3) Can these
discontinuities be used as indicators of novelty, change, and innovation in the
sciences? We examine each of the N2 links among the N journals across the
years. We find power-laws for the top 10,000 instances of change, which we
suggest interpreting in terms of ""self-organized criticality"": co-evolutions of
avalanches in aggregated citation relations and meta-stable states in the
knowledge base can be expected to drive the sciences towards the edges of
chaos. The flux of journal-journal citations in new manuscripts may generate an
avalanche in the meta-stable networks, but one can expect the effects to remain
local (for example, within a specialty). The avalanches can be of any size;
they reorient the relevant citation environments by inducing a rewrite of
history in the affected partitions.
"
2105,"Highly Cited Papers of Ukrainian Scientists Written in Collaboration: A
  Bibliometric Analysis (2011-2015)","  The paper presents the results of the study of international and national
cooperation of Ukrainian scientists from different scientific fields using
citation analysis data from Scopus in the period of 2011-2015. The results show
that during the period under study, the number of documents of highly cited
Ukrainian scientists that have received enough citations to be included to the
top 1%, 5% and 10% most cited documents in the world, evidenced an increase and
were significantly different in different subjects areas. Papers written by a
group of co-authors predominate among highly cited documents of Ukrainian
scientists. Consequently, international cooperation plays an important role in
Ukrainian scientists research results visibility and impact. At the same time,
up to 16%-27% highly cited articles of Ukrainian scientists have been written
without partnering with foreign colleagues, that means a significant part of
important scientific results in Ukraine are carried out by oneself. Therefore,
for generating an optimal policy of science development in Ukraine it is
important to provide a balanced view of the expectations of the results of the
international cooperation of Ukrainian scientists and achieving a required
balance between the inter-country and international collaboration.
"
2106,"Data Sustainability and Reuse Pathways of Natural Resources and
  Environmental Scientists","  This paper presents a multifarious examination of natural resources and
environmental scientists' adventures navigating the policy change towards open
access and cultural shift in data management, sharing, and reuse. Situated in
the institutional context of Virginia Tech, a focus group and multiple
individual interviews were conducted exploring the domain scientists'
all-around experiences, performances, and perspectives on their collection,
adoption, integration, preservation, and management of data. The results reveal
the scientists' struggles, concerns, and barriers encountered, as well as their
shared values, beliefs, passions, and aspirations when working with data. Based
on these findings, this study provides suggestions on data modeling and
knowledge representation strategies to support the long-term viability,
stewardship, accessibility, and sustainability of scientific data. It also
discusses the art of curation as creative scholarship and new opportunities for
data librarians and information professionals to mobilize the data revolution.
"
2107,"Burgeoning Data Repository Systems, Characteristics and Development
  Strategies: Insights of Natural Resources and Environmental Scientists","  Nowadays, we have the emergence and abundance of many different data
repositories and archival systems for scientific data discovery, use, and
analysis. With the burgeoning data sharing platforms available, this study
addresses how natural resources and environmental scientists navigate these
diverse data sources, what their concerns and value propositions are towards
multiple data discovery channels, and most importantly, how they perceive the
characteristics and compare the functionalities of different types of data
repository systems. Through a user community research of domain scientists on
their data use dynamics and insights, this research provides strategies and
discusses ideas on how to leverage these different platforms. Further, it
proposes a top-down, novel approach to search, browsing, and visualization for
dynamic exploration of environmental data.
"
2108,Ethnic Diversity Increases Scientific Impact,"  Inspired by the numerous social and economic benefits of diversity, we
analyze over 9 million papers and 6 million scientists spanning 24 fields of
study, to understand the relationship between research impact and five types of
diversity, reflecting (i) ethnicity, (ii) discipline, (iii) gender, (iv)
affiliation and (v) academic age. For each type, we study group diversity
(i.e., the heterogeneity of a paper's set of authors) and individual diversity
(i.e., the heterogeneity of a scientist's entire set of collaborators).
Remarkably, of all the types considered, we find that ethnic diversity is the
strongest predictor of a field's scientific impact (r is 0.77 and 0.55 for
group and individual ethnic diversity, respectively). Moreover, to isolate the
effect of ethnic diversity from other confounding factors, we analyze a
baseline model in which author ethnicities are randomized while preserving all
other characteristics. We find that the relationship between ethnic diversity
and impact is stronger in the empirical data compared to the randomized
baseline model, regardless of publication year, number of authors per paper,
and number of collaborators per scientist. Finally, we use coarsened exact
matching to infer causality, whereby the scientific impact of ethnically
diverse papers and scientists are compared with closely-matched control groups.
Impact gains of 11.64% and 55.45% were observed between the top and bottom 10%
group and individual diversities, respectively. This provides further evidence
that ethnic diversity leads to higher scientific impact.
"
2109,"Genealogy tree: understanding academic lineage of authors via
  algorithmic and visual analysis","  Ancestry and genealogy tree are proven tools to determine the lineage of any
person and establish dependencies among individuals. Genealogy tree can be
exploited further to gain information about the researcher and his scholastic
lineage which is of paramount importance in today's world of computer
technology. this insight into academic genealogy could be a way of helping PHD
students achieve academic socialization within the discipline, by making
explicit connections that may be influential. Awareness of his scientific
heritage gives the user a broader perspective of his own research project. This
paper also highlights and investigates how this academic network is exploited
by certain researchers using various visualization tools. It was observed
during this work that the credibility and influence factor is determined by the
various citations obtained by an author and to improve their rankings in
various forms, they tend to collaborate in their academic circle and boost
their citation count. A recent trend among researchers is to form communities
based on their academic relationships and rely on copious citations for their
mutual benefit. Tracing the genealogical relationships can be helpful in
detecting such communities and also create a more quality aware metric using a
lineage independent model for computation of author level metrics.
"
2110,"Impact Factors and the Central Limit Theorem: Why Citation Averages Are
  Scale Dependent","  Citation averages, and Impact Factors (IFs) in particular, are sensitive to
sample size. We apply the Central Limit Theorem (CLT) to IFs to understand
their scale-dependent behavior. For a journal of $n$ randomly selected papers
from a population of all papers, we expect from the CLT that its IF fluctuates
around the population average $\mu$, and spans a range of values proportional
to $\sigma/\sqrt[]{n}$, where $\sigma^2$ is the variance of the population's
citation distribution. The $1/\sqrt[]{n}$ dependence has profound implications
for IF rankings: The larger a journal, the narrower the range around $\mu$
where its IF lies. IF rankings therefore allocate an unfair advantage to
smaller journals in the high IF ranks, and to larger journals in the low IF
ranks. We expect a scale-dependent stratification of journals in IF rankings,
whereby small journals occupy top, middle, and bottom ranks; mid-sized journals
occupy middle ranks; and very large journals have IFs that asymptotically
approach $\mu$. We confirm these predictions by analyzing (i) 166,498 IF \&
journal-size data pairs in the 1997--2016 Journal Citation Reports of Clarivate
Analytics, (ii) the top-cited portion of 276,000 physics papers published in
2014--2015, and (iii) the citation distributions of an arbitrarily sampled list
of physics journals. We conclude that the CLT is a good predictor of the IF
range of actual journals, while sustained deviations from its predictions are a
mark of true, non-random, citation impact. IF rankings are thus misleading
unless one compares like-sized journals or adjusts for these effects. We
propose the $\Phi$ index, a rescaled IF adjusted for size, which can be
generalized to account also for different citation practices across research
fields. Our methodology applies also to citation averages used to compare
research fields, university departments or countries in various rankings.
"
2111,Knowledge Graphs in the Libraries and Digital Humanities Domain,"  Knowledge graphs represent concepts (e.g., people, places, events) and their
semantic relationships. As a data structure, they underpin a digital
information system, support users in resource discovery and retrieval, and are
useful for navigation and visualization purposes. Within the libaries and
humanities domain, knowledge graphs are typically rooted in knowledge
organization systems, which have a century-old tradition and have undergone
their digital transformation with the advent of the Web and Linked Data. Being
exposed to the Web, metadata and concept definitions are now forming an
interconnected and decentralized global knowledge network that can be curated
and enriched by community-driven editorial processes. In the future, knowledge
graphs could be vehicles for formalizing and connecting findings and insights
derived from the analysis of possibly large-scale corpora in the libraries and
digital humanities domain.
"
2112,"Towards Knowledge Discovery from the Vatican Secret Archives. In Codice
  Ratio -- Episode 1: Machine Transcription of the Manuscripts","  In Codice Ratio is a research project to study tools and techniques for
analyzing the contents of historical documents conserved in the Vatican Secret
Archives (VSA). In this paper, we present our efforts to develop a system to
support the transcription of medieval manuscripts. The goal is to provide
paleographers with a tool to reduce their efforts in transcribing large
volumes, as those stored in the VSA, producing good transcriptions for
significant portions of the manuscripts. We propose an original approach based
on character segmentation. Our solution is able to deal with the dirty
segmentation that inevitably occurs in handwritten documents. We use a
convolutional neural network to recognize characters and language models to
compose word transcriptions. Our approach requires minimal training efforts,
making the transcription process more scalable as the production of training
sets requires a few pages and can be easily crowdsourced. We have conducted
experiments on manuscripts from the Vatican Registers, an unreleased corpus
containing the correspondence of the popes. With training data produced by 120
high school students, our system has been able to produce good transcriptions
that can be used by paleographers as a solid basis to speedup the transcription
process at a large scale.
"
2113,"The many faces of mobility: Using bibliometric data to measure the
  movement of scientists","  This paper presents a methodological framework for developing scientific
mobility indicators based on bibliometric data. We identify nearly 16 million
individual authors from publications covered in the Web of Science for the
2008-2015 period. Based on the information provided across individuals'
publication records, we propose a general classification for analyzing
scientific mobility using institutional affiliation changes. We distinguish
between migrants--authors who have ruptures with their country of origin--and
travelers--authors who gain additional affiliations while maintaining
affiliation with their country of origin. We find that 3.7 percent of
researchers who have published at least one paper over the period are mobile.
Travelers represent 72.7 percent of all mobile scholars, but migrants have
higher scientific impact. We apply this classification at the country level,
expanding the classification to incorporate the directionality of scientists'
mobility (i.e., incoming and outgoing). We provide a brief analysis to
highlight the utility of the proposed taxonomy to study scholarly mobility and
discuss the implications for science policy.
"
2114,Merging the Astrophysics and Planetary Science Information Systems,"  Conceptually exoplanet research has one foot in the discipline of
Astrophysics and the other foot in Planetary Science. Research strategies for
exoplanets will require efficient access to data and information from both
realms. Astrophysics has a sophisticated, well integrated, distributed
information system with archives and data centers which are interlinked with
the technical literature via the Astrophysics Data System (ADS). The
information system for Planetary Science does not have a central component
linking the literature with the observational and theoretical data. Here we
propose that the Committee on an Exoplanet Science Strategy recommend that this
linkage be built, with the ADS playing the role in Planetary Science which it
already plays in Astrophysics. This will require additional resources for the
ADS, and the Planetary Data System (PDS), as well as other international
collaborators
"
2115,"Analyzing the network structure and gender differences among the members
  of the Networked Knowledge Organization Systems (NKOS) community","  In this paper, we analyze a major part of the research output of the
Networked Knowledge Organization Systems (NKOS) community in the period 2000 to
2016 from a network analytical perspective. We focus on the papers presented at
the European and U.S. NKOS workshops and in addition four special issues on
NKOS in the last 16 years. For this purpose, we have generated an open dataset,
the ""NKOS bibliography"" which covers the bibliographic information of the
research output. We analyze the co-authorship network of this community which
results in 123 papers with a sum of 256 distinct authors. We use standard
network analytic measures such as degree, betweenness and closeness centrality
to describe the co-authorship network of the NKOS dataset. First, we
investigate global properties of the network over time. Second, we analyze the
centrality of the authors in the NKOS network. Lastly, we investigate gender
differences in collaboration behavior in this community. Our results show that
apart from differences in centrality measures of the scholars, they have higher
tendency to collaborate with those in the same institution or the same
geographic proximity. We also find that homophily is higher among women in this
community. Apart from small differences in closeness and clustering among men
and women, we do not find any significant dissimilarities with respect to other
centralities.
"
2116,Linking ImageNet WordNet Synsets with Wikidata,"  The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep
learning algorithm with access to a rich multilingual knowledge graph. Here I
will describe our on-going efforts in linking the two resources and issues
faced in matching the Wikidata and WordNet knowledge graphs. I show an example
on how the linkage can be used in a deep learning setting with real-time image
classification and labeling in a non-English language and discuss what
opportunities lies ahead.
"
2117,"Co-occurrence simplicial complexes in mathematics: identifying the holes
  of knowledge","  In the last years complex networks tools contributed to provide insights on
the structure of research, through the study of collaboration, citation and
co-occurrence networks. The network approach focuses on pairwise relationships,
often compressing multidimensional data structures and inevitably losing
information. In this paper we propose for the first time a simplicial complex
approach to word co-occurrences, providing a natural framework for the study of
higher-order relations in the space of scientific knowledge. Using topological
methods we explore the conceptual landscape of mathematical research, focusing
on homological holes, regions with low connectivity in the simplicial
structure. We find that homological holes are ubiquitous, which suggests that
they capture some essential feature of research practice in mathematics. Holes
die when a subset of their concepts appear in the same article, hence their
death may be a sign of the creation of new knowledge, as we show with some
examples. We find a positive relation between the dimension of a hole and the
time it takes to be closed: larger holes may represent potential for important
advances in the field because they separate conceptually distant areas. We also
show that authors' conceptual entropy is positively related with their
contribution to homological holes, suggesting that polymaths tend to be on the
frontier of research.
"
2118,"Text Data Mining from the Author's Perspective: Whose Text, Whose
  Mining, and to Whose Benefit?","  Given the many technical, social, and policy shifts in access to scholarly
content since the early days of text data mining, it is time to expand the
conversation about text data mining from concerns of the researcher wishing to
mine data to include concerns of researcher-authors about how their data are
mined, by whom, for what purposes, and to whose benefits.
"
2119,"Model Visualization in understanding rapid growth of a journal in an
  emerging area","  A recent independent study resulted in a ranking system which ranked
Astronomy and Computing (ASCOM) much higher than most of the older journals
highlighting the niche prominence of the particular journal. We investigate the
remarkable ascendancy in reputation of ASCOM by proposing a novel differential
equation based modeling. The Modeling is a consequence of knowledge discovery
from big data-centric methods, namely L1-SVD. The inadequacy of the ranking
method in explaining the reason behind the growth in reputation of ASCOM is
reasonable to understand given that the study was post-facto. Thus, we propose
a growth model by accounting for the behavior of parameters that contribute to
the growth of a field. It is worthwhile to spend some time in analysing the
cause and control variables behind rapid rise in reputation of a journal in a
niche area. We intent to probe and bring out parameters responsible for its
growing influence. Delay differential equations are used to model the change of
influence on a journal's status by exploiting the effects of historical data.
"
2120,"Which differences can be expected when two universities in the Leiden
  Ranking are compared? Some benchmarks for institutional research evaluations","  The comparison of two universities in terms of bibliometric indicators
frequently faces the problem of assessing the differences as meaningful or not.
This Letter to the Editor proposes some benchmarks which can be used for
supporting the interpretation of institutional differences.
"
2121,"Sharing and Preserving Computational Analyses for Posterity with
  encapsulator","  Open data and open-source software may be part of the solution to science's
""reproducibility crisis"", but they are insufficient to guarantee
reproducibility. Requiring minimal end-user expertise, encapsulator creates a
""time capsule"" with reproducible code in a self-contained computational
environment. encapsulator provides end-users with a fully-featured desktop
environment for reproducible research.
"
2122,"Evidence of Open Access of scientific publications in Google Scholar: a
  large-scale analysis","  This article uses Google Scholar (GS) as a source of data to analyse Open
Access (OA) levels across all countries and fields of research. All articles
and reviews with a DOI and published in 2009 or 2014 and covered by the three
main citation indexes in the Web of Science (2,269,022 documents) were selected
for study. The links to freely available versions of these documents displayed
in GS were collected. To differentiate between more reliable (sustainable and
legal) forms of access and less reliable ones, the data extracted from GS was
combined with information available in DOAJ, CrossRef, OpenDOAR, and ROAR. This
allowed us to distinguish the percentage of documents in our sample that are
made OA by the publisher (23.1%, including Gold, Hybrid, Delayed, and Bronze
OA) from those available as Green OA (17.6%), and those available from other
sources (40.6%, mainly due to ResearchGate). The data shows an overall free
availability of 54.6%, with important differences at the country and subject
category levels. The data extracted from GS yielded very similar results to
those found by other studies that analysed similar samples of documents, but
employed different methods to find evidence of OA, thus suggesting a relative
consistency among methods.
"
2123,"Link prediction for interdisciplinary collaboration via co-authorship
  network","  We analyse the Publication and Research (PURE) data set of University of
Bristol collected between $2008$ and $2013$. Using the existing co-authorship
network and academic information thereof, we propose a new link prediction
methodology, with the specific aim of identifying potential interdisciplinary
collaboration in a university-wide collaboration network.
"
2124,Dimensions: A Competitor to Scopus and the Web of Science?,"  Dimensions is a partly free scholarly database launched by Digital Science in
January 2018. Dimensions includes journal articles and citation counts, making
it a potential new source of impact data. This article explores the value of
Dimensions from an impact assessment perspective with an examination of Food
Science research 2008-2018 and a random sample of 10,000 Scopus articles from
2012. The results include high correlations between citation counts from Scopus
and Dimensions (0.96 by narrow field in 2012) as well as similar average
counts. Almost all Scopus articles with DOIs were found in Dimensions (97% in
2012). Thus, the scholarly database component of Dimensions seems to be a
plausible alternative to Scopus and the Web of Science for general citation
analyses and for citation data in support of some types of research
evaluations.
"
2125,"Long term availability of raw experimental data in experimental fracture
  mechanics","  Experimental data availability is a cornerstone for reproducibility in
experimental fracture mechanics, which is crucial to the scientific method.
This short communication focuses on the accessibility and long term
availability of raw experimental data. The corresponding authors of the eleven
most cited papers, related to experimental fracture mechanics, for every year
from 2000 up to 2016, were kindly asked about the availability of the raw
experimental data associated with each publication. For the 187 e-mails sent:
22.46% resulted in outdated contact information, 57.75% of the authors did
received our request and did not reply, and 19.79 replied to our request. The
availability of data is generally low with only $11$ available data sets
(5.9%). The authors identified two main issues for the lacking availability of
raw experimental data. First, the ability to retrieve data is strongly attached
to the the possibility to contact the corresponding author. This study suggests
that institutional e-mail addresses are insufficient means for obtaining
experimental data sets. Second, lack of experimental data is also due that
submission and publication does not require to make the raw experimental data
available. The following solutions are proposed: (1) Requirement of unique
identifiers, like ORCID or ResearcherID, to detach the author(s) from their
institutional e-mail address, (2) Provide DOIs, like Zenodo or Dataverse, to
make raw experimental data citable, and (3) grant providing organizations
should ensure that experimental data by public funded projects is available to
the public.
"
2126,Measuring the academic reputation through citation networks via PageRank,"  The objective assessment of the prestige of an academic institution is a
difficult and hotly debated task. In the last few years, different types of
University Rankings have been proposed to quantify the excellence of different
research institutions in the world. Albeit met with criticism in some cases,
the relevance of university rankings is being increasingly acknowledged:
indeed, rankings are having a major impact on the design of research policies,
both at the institutional and governmental level. Yet, the debate on what
rankings are {\em exactly} measuring is enduring. Here, we address the issue by
measuring a quantitive and reliable proxy of the academic reputation of a given
institution and by evaluating its correlation with different university
rankings. Specifically, we study citation patterns among universities in five
different Web of Science Subject Categories and use the \pr~algorithm on the
five resulting citation networks. The rationale behind our work is that
scientific citations are driven by the reputation of the reference so that the
PageRank algorithm is expected to yield a rank which reflects the reputation of
an academic institution in a specific field. Our results allow to quantifying
the prestige of a set of institutions in a certain research field based only on
hard bibliometric data. Given the volume of the data analysed, our findings are
statistically robust and less prone to bias, at odds with ad--hoc surveys often
employed by ranking bodies in order to attain similar results. Because our
findings are found to correlate extremely well with the ARWU Subject rankings,
the approach we propose in our paper may open the door to new, Academic Ranking
methodologies that go beyond current methods by reconciling the qualitative
evaluation of Academic Prestige with its quantitative measurements via
publication impact.
"
2127,"Diversity and Interdisciplinarity: How Can One Distinguish and Recombine
  Disparity, Variety, and Balance?","  The dilemma which remained unsolved using Rao-Stirling diversity, namely of
how variety and balance can be combined into ""dual concept diversity""
(Stirling, 1998, pp. 48f.) can be clarified by using Nijssen et al.'s (1998)
argument that the Gini coefficient is a perfect indicator of balance. However,
the Gini coefficient is not an indicator of variety; this latter term can be
operationalized independently as relative variety. The three components of
diversity--variety, balance, and disparity--can thus be clearly distinguished
and independently operationalized as measures varying between zero and one. The
new diversity indicator ranges with more resolving power in the empirical case.
"
2128,"Comparing Published Scientific Journal Articles to Their Pre-Print
  Versions -- Extended Version","  Academic publishers claim that they add value to scholarly communications by
coordinating reviews and contributing and enhancing text during publication.
These contributions come at a considerable cost: U.S. academic libraries paid
$1.7 billion for serial subscriptions in 2008 alone. Library budgets, in
contrast, are flat and not able to keep pace with serial price inflation. We
have investigated the publishers' value proposition by conducting a comparative
study of pre-print papers from two distinct science, technology, and medicine
(STM) corpora and their final published counterparts. This comparison had two
working assumptions: 1) if the publishers' argument is valid, the text of a
pre-print paper should vary measurably from its corresponding final published
version, and 2) by applying standard similarity measures, we should be able to
detect and quantify such differences. Our analysis revealed that the text
contents of the scientific papers generally changed very little from their
pre-print to final published versions. These findings contribute empirical
indicators to discussions of the added value of commercial publishers and
therefore should influence libraries' economic decisions regarding access to
scholarly publications.
"
2129,Crowd-based Multi-Predicate Screening of Papers in Literature Reviews,"  Systematic literature reviews (SLRs) are one of the most common and useful
form of scientific research and publication. Tens of thousands of SLRs are
published each year, and this rate is growing across all fields of science.
Performing an accurate, complete and unbiased SLR is however a difficult and
expensive endeavor. This is true in general for all phases of a literature
review, and in particular for the paper screening phase, where authors lter a
set of potentially in-scope papers based on a number of exclusion criteria. To
address the problem, in recent years the research community has began to
explore the use of the crowd to allow for a faster, accurate, cheaper and
unbiased screening of papers. Initial results show that crowdsourcing can be
effective, even for relatively complex reviews. In this paper we derive and
analyze a set of strategies for crowd-based screening, and show that an
adaptive strategy, that continuously re-assesses the statistical properties of
the problem to minimize the number of votes needed to take decisions for each
paper, significantly outperforms a number of non-adaptive approaches in terms
of cost and accuracy. We validate both applicability and results of the
approach through a set of crowdsourcing experiments, and discuss properties of
the problem and algorithms that we believe to be generally of interest for
classification problems where items are classified via a series of successive
tests (as it often happens in medicine).
"
2130,Biblioranking fundamental physics,"  We propose measures of the impact of research that improve on existing ones
such as counting of number of papers, citations and $h$-index. Since different
papers and different fields have largely different average number of co-authors
and of references we replace citations with individual citations, shared among
co-authors. Next, we improve on citation counting applying the PageRank
algorithm to citations among papers. Being time-ordered, this reduces to a
weighted counting of citation descendants that we call PaperRank. Similarly, we
compute an AuthorRank applying the PageRank algorithm to citations among
authors. These metrics quantify the impact of an author or paper taking into
account the impact of those authors that cite it. Finally, we show how self-
and circular- citations can be eliminated by defining a closed market of
citation-coins. We apply these metrics to the InSpire database that covers
fundamental physics, ranking papers, authors, journals, institutes, towns,
countries, continents, genders, for all-time and in recent time periods.
"
2131,Build up of a subject classification system from collective intelligence,"  Systematized subject classification is essential for funding and assessing
scientific projects. Conventionally, classification schemes are founded on the
empirical knowledge of the group of experts; thus, the experts' perspectives
have influenced the current systems of scientific classification. Those systems
archived the current state-of-art in practice, yet the global effect of the
accelerating scientific change over time has made the updating of the
classifications system on a timely basis vertually impossible. To overcome the
aforementioned limitations, we propose an unbiased classification scheme that
takes advantage of collective knowledge; Wikipedia, an Internet encyclopedia
edited by millions of users, sets a prompt classification in a collective
fashion. We construct a Wikipedia network for scientific disciplines and
extract the backbone of the network. This structure displays a landscape of
science and technology that is based on a collective intelligence and that is
more unbiased and adaptable than conventional classifications.
"
2132,Is science driven by principal investigators?,"  In this paper we consider the scientific and career performance of principal
investigators (PI's) of publicly funded research projects compared to
scientific performance of all researchers. Our study is based on high quality
data about (i) research projects awarded in Slovenia in the period 1994-2016
(7508 projects with 2725 PI's in total) and (ii) about scientific productivity
of all researchers in Slovenia that were active in the period 1970-2016 - there
are 19598 such researchers in total, including the PI's. We compare average
productivity, collaboration, internationality and interdisciplinarity of PI's
and of all active researchers. Our analysis shows that for all four indicators
the average performance of PI's is much higher compared to average performance
of all active researchers. Additionally, we analyze careers of both groups of
researchers. The results show that the PI's have on average longer and more
fruitful career compared to all active researchers, with regards to all career
indicators. The PI's that have received a postdoc grant have at the beginning
outstanding scientific performance, but later deviate towards average. On long
run, the PI's leading the research programs (the most prestigious grants) on
average demonstrate the best scientific performance. In the last part of the
paper we study 23 co-authorship networks, spanned by all active researchers in
the periods 1970-1994, ..., 1970-2016. We find out that they are well connected
and that PI's are well distributed across these networks forming their
backbones. Even more, PI's generate new PI's, since more than 90% of new PI's
are connected (have at least one joint scientific publication) with existing
PI's. We believe that our study sheds new light to the relations between the
public funding of the science and the scientific output and can be considered
as an affirmative answer to the question posed in the title.
"
2133,Focused Crawl of Web Archives to Build Event Collections,"  Event collections are frequently built by crawling the live web on the basis
of seed URIs nominated by human experts. Focused web crawling is a technique
where the crawler is guided by reference content pertaining to the event. Given
the dynamic nature of the web and the pace with which topics evolve, the timing
of the crawl is a concern for both approaches. We investigate the feasibility
of performing focused crawls on the archived web. By utilizing the Memento
infrastructure, we obtain resources from 22 web archives that contribute to
building event collections. We create collections on four events and compare
the relevance of their resources to collections built from crawling the live
web as well as from a manually curated collection. Our results show that
focused crawling on the archived web can be done and indeed results in highly
relevant collections, especially for events that happened further in the past.
"
2134,Extracting Scientific Figures with Distantly Supervised Neural Networks,"  Non-textual components such as charts, diagrams and tables provide key
information in many scientific documents, but the lack of large labeled
datasets has impeded the development of data-driven methods for scientific
figure extraction. In this paper, we induce high-quality training labels for
the task of figure extraction in a large number of scientific documents, with
no human intervention. To accomplish this we leverage the auxiliary data
provided in two large web collections of scientific documents (arXiv and
PubMed) to locate figures and their associated captions in the rasterized PDF.
We share the resulting dataset of over 5.5 million induced labels---4,000 times
larger than the previous largest figure extraction dataset---with an average
precision of 96.8%, to enable the development of modern data-driven methods for
this task. We use this dataset to train a deep neural network for end-to-end
figure detection, yielding a model that can be more easily extended to new
domains compared to previous work. The model was successfully deployed in
Semantic Scholar, a large-scale academic search engine, and used to extract
figures in 13 million scientific documents.
"
2135,"Can We Count on Social Media Metrics? First Insights into the Active
  Scholarly Use of Social Media","  Measuring research impact is important for ranking publications in academic
search engines and for research evaluation. Social media metrics or altmetrics
measure the impact of scientific work based on social media activity.
Altmetrics are complementary to traditional, citation-based metrics, e.g.
allowing the assessment of new publications for which citations are not yet
available. Despite the increasing importance of altmetrics, their
characteristics are not well understood: Until now it has not been researched
what kind of researchers are actively using which social media services and why
- important questions for scientific impact prediction. Based on a survey among
3,430 scientists, we uncover previously unknown and significant differences
between social media services: We identify services which attract young and
experienced researchers, respectively, and detect differences in usage
motivations. Our findings have direct implications for the future design of
altmetrics for scientific impact prediction.
"
2136,Automatically assembling a full census of an academic field,"  The composition of the scientific workforce shapes the direction of
scientific research, directly through the selection of questions to
investigate, and indirectly through its influence on the training of future
scientists. In most fields, however, complete census information is difficult
to obtain, complicating efforts to study workforce dynamics and the effects of
policy. This is particularly true in computer science, which lacks a single,
all-encompassing directory or professional organization. A full census of
computer science would serve many purposes, not the least of which is a better
understanding of the trends and causes of unequal representation in computing.
Previous academic census efforts have relied on narrow or biased samples, or on
professional society membership rolls. A full census can be constructed
directly from online departmental faculty directories, but doing so by hand is
prohibitively expensive and time-consuming. Here, we introduce a topical web
crawler for automating the collection of faculty information from web-based
department rosters, and demonstrate the resulting system on the 205
PhD-granting computer science departments in the U.S. and Canada. This method
constructs a complete census of the field within a few minutes, and achieves
over 99% precision and recall. We conclude by comparing the resulting 2017
census to a hand-curated 2011 census to quantify turnover and retention in
computer science, in general and for female faculty in particular,
demonstrating the types of analysis made possible by automated census
construction.
"
2137,"Novelty and Foreseeing Research Trends; The Case of Astrophysics and
  Astronomy","  Metrics based on reference lists of research articles or on keywords have
been used to predict citation impact. The concept behind such metrics is that
original ideas stem from the reconfiguration of the structure of past
knowledge, and therefore atypical combinations in the reference lists,
keywords, or classification codes indicate future high impact research. The
current paper serves as an introduction to this line of research for
astronomers and also addresses some methodological questions of this field of
innovation studies. It is still not clear if the choice of particular indexes,
such as references to journals, articles, or specific bibliometric
classification codes would affect the relationship between atypical
combinations and citation impact. To understand more aspects of the innovation
process, a new metric has been devised to measure to what extent researchers
are able to anticipate the changing combinatorial trends of the future. Results
show that the variant of the latter anticipation scores that is based on paper
combinations is a good predictor of future citation impact of scholarly works.
The study also shows that the effect of tested indexes vary with the
aggregation level that was used to construct them. A detailed analysis of
combinatorial novelty in the field reveals that certain sub-fields of astronomy
and astrophysics have different roles in the reconfiguration in past knowledge.
"
2138,"Methodological considerations on the use of the normalized impact in the
  Severo Ochoa and Mar\'ia de Maetzu programmes","  In 2011, the programme for Severo Ochoa Centers of Excellence and Mar\'ia de
Maeztu Units of Excellence was launched for the first time. Since this
programme has become one of the axes of the Spanish scientific policy. 186
million euros have been distributed and 26 centers and 16 units have been
accredited. One of the most relevant criteria for submission is the need for
guarantor researchers to have a Normalized Impact of 1.5. In this work, we
critically analyze the origin of this bibliometric indicator rooted in the
1980s, the different variants that have been proposed and the limitations its
use in this programme have. Finally, we offer a series of practical
recommendations for a more accurate use of normalized impact indicators for
evaluative purposes.
"
2139,"Report on the 7th International Workshop on Bibliometric-enhanced
  Information Retrieval (BIR 2018)","  The Bibliometric-enhanced Information Retrieval (BIR) workshop series has
started at ECIR in 2014 and serves as the annual gathering of IR researchers
who address various information-related tasks on scientific corpora and
bibliometrics. We welcome contributions elaborating on dedicated IR systems, as
well as studies revealing original characteristics on how scientific knowledge
is created, communicated, and used. This report presents all accepted papers at
the 7th BIR workshop at ECIR 2018 in Grenoble, France.
"
2140,Analyzing the activities of visitors of the Leiden Ranking website,"  To provide a better understanding of the way in which university rankings are
used, we present a detailed analysis of the activities of visitors of a
university ranking website. We use the website of the CWTS Leiden Ranking for
this purpose. We for instance study the countries from which visitors
originate, the specific pages on the Leiden Ranking website that they visit,
the countries or the universities that they find of special interest, and the
indicators that they focus on. In addition, we also discuss two experiments
that were carried out on the Leiden Ranking website. Our analysis does not only
provide new insights into the use of university rankings, but it also suggests
possible ways in which these rankings can be improved.
"
2141,Comparing scientific and technological impact of biomedical research,"  Traditionally, the number of citations that a scholarly paper receives from
other papers is used as the proxy of its scientific impact. Yet citations can
come from domains outside the scientific community, and one such example is
through patented technologies---paper can be cited by patents, achieving
technological impact. While the scientific impact of papers has been
extensively studied, the technological aspect remains less known in the
literature. Here we aim to fill this gap by presenting a comparative study on
how 919 thousand biomedical papers are cited by U.S. patents and by other
papers over time. We observe a positive correlation between citations from
patents and from papers, but there is little overlap between the two domains in
either the most cited papers, or papers with the most delayed recognition. We
also find that the two types of citations exhibit distinct temporal variations,
with patent citations lagging behind paper citations for a median of 6 years
for the majority of papers. Our work contributes to the understanding of the
technological impact of papers.
"
2142,"Improving the Representation and Conversion of Mathematical Formulae by
  Considering their Textual Context","  Mathematical formulae represent complex semantic information in a concise
form. Especially in Science, Technology, Engineering, and Mathematics,
mathematical formulae are crucial to communicate information, e.g., in
scientific papers, and to perform computations using computer algebra systems.
Enabling computers to access the information encoded in mathematical formulae
requires machine-readable formats that can represent both the presentation and
content, i.e., the semantics, of formulae. Exchanging such information between
systems additionally requires conversion methods for mathematical
representation formats. We analyze how the semantic enrichment of formulae
improves the format conversion process and show that considering the textual
context of formulae reduces the error rate of such conversions. Our main
contributions are: (1) providing an openly available benchmark dataset for the
mathematical format conversion task consisting of a newly created test
collection, an extensive, manually curated gold standard and task-specific
evaluation metrics; (2) performing a quantitative evaluation of
state-of-the-art tools for mathematical format conversions; (3) presenting a
new approach that considers the textual context of formulae to reduce the error
rate for mathematical format conversions. Our benchmark dataset facilitates
future research on mathematical format conversions as well as research on many
problems in mathematical information retrieval. Because we annotated and linked
all components of formulae, e.g., identifiers, operators and other entities, to
Wikidata entries, the gold standard can, for instance, be used to train methods
for formula concept discovery and recognition. Such methods can then be applied
to improve mathematical information retrieval systems, e.g., for semantic
formula search, recommendation of mathematical content, or detection of
mathematical plagiarism.
"
2143,"Exploration of reproducibility issues in scientometric research Part 1:
  Direct reproducibility","  This is the first part of a small-scale explorative study in an effort to
start assessing reproducibility issues specific to scientometrics research.
This effort is motivated by the desire to generate empirical data to inform
debates about reproducibility in scientometrics. Rather than attempt to
reproduce studies, we explore how we might assess ""in principle""
reproducibility based on a critical review of the content of published papers.
The first part of the study focuses on direct reproducibility - that is the
ability to reproduce the specific evidence produced by an original study using
the same data, methods, and procedures. The second part (Velden et al. 2018) is
dedicated to conceptual reproducibility - that is the robustness of knowledge
claims towards verification by an alternative approach using different data,
methods and procedures. The study is exploratory: it investigates only a very
limited number of publications and serves us to develop instruments for
identifying potential reproducibility issues of published studies: These are a
categorization of study types and a taxonomy of threats to reproducibility. We
work with a select sample of five publications in scientometrics covering a
variation of study types of theoretical, methodological, and empirical nature.
Based on observations made during our exploratory review, we conclude this
paper with open questions on how to approach and assess the status of direct
reproducibility in scientometrics, intended for discussion at the special track
on ""Reproducibility in Scientometrics"" at STI2018 in Leiden.
"
2144,"Exploration of Reproducibility Issues in Scientometric Research Part 2:
  Conceptual Reproducibility","  This is the second part of a small-scale explorative study in an effort to
assess reproducibility issues specific to scientometrics research. This effort
is motivated by the desire to generate empirical data to inform debates about
reproducibility in scientometrics. Rather than attempt to reproduce studies, we
explore how we might assess ""in principle"" reproducibility based on a critical
review of the content of published papers. While the first part of the study
(Waltman et al. 2018) focuses on direct reproducibility - that is the ability
to reproduce the specific evidence produced by an original study using the same
data, methods, and procedures, this second part is dedicated to conceptual
reproducibility - that is the robustness of knowledge claims towards
verification by an alternative approach using different data, methods and
procedures. The study is exploratory: it investigates only a very limited
number of publications and serves us to develop instruments for identifying
potential reproducibility issues of published studies: These are a
categorization of study types and a taxonomy of threats to reproducibility. We
work with a select sample of five publications in scientometrics covering a
variation of study types of theoretical, methodological, and empirical nature.
Based on observations made during our exploratory review, we conclude with open
questions on how to approach and assess the status of conceptual
reproducibility in scientometrics intended for discussion at the special track
on ""Reproducibility in Scientometrics"" at STI2018 in Leiden.
"
2145,Dimensions: re-discovering the ecosystem of scientific information,"  The overarching aim of this work is to provide a detailed description of the
free version of Dimensions (new bibliographic database produced by Digital
Science and launched in January 2018). To do this, the work is divided into two
differentiated blocks. First, its characteristics, operation and features are
described, focusing on its main strengths and weaknesses. Secondly, an analysis
of its coverage is carried out (comparing it Scopus and Google Scholar) in
order to determine whether the bibliometric indicators offered by Dimensions
have an order of magnitude significant enough to be used. To this end, an
analysis is carried out at three levels: journals (sample of 20 publications in
'Library & Information Science'), documents (276 articles published by the
Journal of informetrics between 2013 and 2015) and authors (28 people awarded
with the Derek de Solla Price prize). Preliminary results indicate that
Dimensions has coverage of the recent literature superior to Scopus although
inferior to Google Scholar. With regard to the number of citations received,
Dimensions offers slightly lower figures than Scopus. Despite this, the number
of citations in Dimensions exhibits a strong correlation with Scopus and
somewhat less (although still significant) with Google Scholar. For this
reason, it is concluded that Dimensions is an alternative for carrying out
citation studies, being able to rival Scopus (greater coverage and free of
charge) and with Google Scholar (greater functionalities for the treatment and
data export).
"
2146,"Using the Jupyter Notebook as a Tool for Open Science: An Empirical
  Study","  As scientific work becomes more computational and data intensive, research
processes and results become more difficult to interpret and reproduce. In this
poster, we show how the Jupyter notebook, a tool originally designed as a free
version of Mathematica notebooks, has evolved to become a robust tool for
scientists to share code, associated computation, and documentation.
"
2147,CL Scholar: The ACL Anthology Knowledge Graph Miner,"  We present CL Scholar, the ACL Anthology knowledge graph miner to facilitate
high-quality search and exploration of current research progress in the
computational linguistics community. In contrast to previous works,
periodically crawling, indexing and processing of new incoming articles is
completely automated in the current system. CL Scholar utilizes both textual
and network information for knowledge graph construction. As an additional
novel initiative, CL Scholar supports more than 1200 scholarly natural language
queries along with standard keyword-based search on constructed knowledge
graph. It answers binary, statistical and list based natural language queries.
The current system is deployed at http://cnerg.iitkgp.ac.in/aclakg. We also
provide REST API support along with bulk download facility. Our code and data
are available at https://github.com/CLScholar.
"
2148,"Author-level metrics in the new academic profile platforms: The online
  behaviour of the Bibliometrics community","  The new web-based academic communication platforms do not only enable
researchers to better advertise their academic outputs, making them more
visible than ever before, but they also provide a wide supply of metrics to
help authors better understand the impact their work is making. This study has
three objectives: a) to analyse the uptake of some of the most popular
platforms (Google Scholar Citations, ResearcherID, ResearchGate, Mendeley and
Twitter) by a specific scientific community (bibliometrics, scientometrics,
informetrics, webometrics, and altmetrics); b) to compare the metrics available
from each platform; and c) to determine the meaning of all these new metrics.
To do this, the data available in these platforms about a sample of 811 authors
(researchers in bibliometrics for whom a public profile Google Scholar
Citations was found) were extracted. A total of 31 metrics were analysed. The
results show that a high number of the analysed researchers only had a profile
in Google Scholar Citations (159), or only in Google Scholar Citations and
ResearchGate (142). Lastly, we find two kinds of metrics of online impact.
First, metrics related to connectivity (followers), and second, all metrics
associated to academic impact. This second group can further be divided into
usage metrics (reads, views), and citation metrics. The results suggest that
Google Scholar Citations is the source that provides more comprehensive
citation-related data, whereas Twitter stands out in connectivity-related
metrics.
"
2149,Are Abstracts Enough for Hypothesis Generation?,"  The potential for automatic hypothesis generation (HG) systems to improve
research productivity keeps pace with the growing set of publicly available
scientific information. But as data becomes easier to acquire, we must
understand the effect different textual data sources have on our resulting
hypotheses. Are abstracts enough for HG, or does it need full-text papers? How
many papers does an HG system need to make valuable predictions? How sensitive
is a general-purpose HG system to hyperparameter values or input quality? What
effect does corpus size and document length have on HG results? To answer these
questions we train multiple versions of knowledge network-based HG system,
Moliere, on varying corpora in order to compare challenges and trade offs in
terms of result quality and computational requirements. Moliere generalizes
main principles of similar knowledge network-based HG systems and reinforces
them with topic modeling components. The corpora include the abstract and
full-text versions of PubMed Central, as well as iterative halves of MEDLINE,
which allows us to compare the effect document length and count has on the
results. We find that, quantitatively, corpora with a higher median document
length result in marginally higher quality results, yet require substantially
longer to process. However, qualitatively, full-length papers introduce a
significant number of intruder terms to the resulting topics, which decreases
human interpretability. Additionally, we find that the effect of document
length is greater than that of document count, even if both sets contain only
paper abstracts. Reproducibility: Our code and data are available at
github.com/jsybran/moliere, and bit.ly/2GxghpM respectively.
"
2150,"PMC text mining subset in BioC: 2.3 million full text articles and
  growing","  Interest in full text mining biomedical research articles is growing. NCBI
provides the PMC Open Access and Author Manuscript sets of articles which are
available for text mining. We have made all of these articles available in
BioC, an XML and JSON format which is convenient for sharing text, annotations,
and relations. These articles are available both via ftp for bulk download and
via a Web API for updates or more focused collection. Availability:
https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/
"
2151,Prioritizing and Scheduling Conferences for Metadata Harvesting in dblp,"  Maintaining literature databases and online bibliographies is a core
responsibility of metadata aggregators such as digital libraries. In the
process of monitoring all the available data sources the question arises which
data source should be prioritized. Based on a broad definition of information
quality we are looking for different ways to find the best fitting and most
promising conference candidates to harvest next. We evaluate different
conference ranking features by using a pseudo-relevance assessment and a
component-based evaluation of our approach.
"
2152,Contextualised Browsing in a Digital Library's Living Lab,"  Contextualisation has proven to be effective in tailoring \linebreak search
results towards the users' information need. While this is true for a basic
query search, the usage of contextual session information during exploratory
search especially on the level of browsing has so far been underexposed in
research. In this paper, we present two approaches that contextualise browsing
on the level of structured metadata in a Digital Library (DL), (1) one variant
bases on document similarity and (2) one variant utilises implicit session
information, such as queries and different document metadata encountered during
the session of a users. We evaluate our approaches in a living lab environment
using a DL in the social sciences and compare our contextualisation approaches
against a non-contextualised approach. For a period of more than three months
we analysed 47,444 unique retrieval sessions that contain search activities on
the level of browsing. Our results show that a contextualisation of browsing
significantly outperforms our baseline in terms of the position of the first
clicked item in the result set. The mean rank of the first clicked document
(measured as mean first relevant - MFR) was 4.52 using a non-contextualised
ranking compared to 3.04 when re-ranking the result lists based on similarity
to the previously viewed document. Furthermore, we observed that both
contextual approaches show a noticeably higher click-through rate. A
contextualisation based on document similarity leads to almost twice as many
document views compared to the non-contextualised ranking.
"
2153,ArXiv and the REF open access policy,"  HEFCE's Policy for open access in the post-2014 Research Excellence Framework
states ""authors' outputs must have been deposited in an institutional or
subject repository"". There is no definition of a subject repository in the
policy: however, there is a footnote stating: ""Individuals depositing their
outputs in a subject repository are advised to ensure that their chosen
repository meets the requirements set out in this policy."" The longest standing
subject repository (or repository of any kind) is arXiv.org, established in
1991. arXiv is an open access repository of scientific research available to
authors and researchers worldwide and acts as a scholarly communications forum
informed and guided by scientists. Content held on arXiv is free to the end
user and researchers can deposit their content freely. As of April 2018, arXiv
held over 1,377,000 eprints. In some disciplines arXiv is considered essential
to the sharing and publication of research. The HEFCE requirements on
repositories are defined in the Information and Audit Requirements which lists
the ""Accepted date"", the ""Version of deposited file"" and ""available open access
immediately after the publisher embargo"" are expected as part of the REF
submission. However, while many records in arXiv have multiple versions of
work, the Author's Accepted Manuscript is not identified and there is no field
to record the acceptance date of the work. Because arXiv does not capture these
two specific information points it does not meet the technical requirements to
be a compliant subject repository for the purposes of REF. This paper is
presenting the case that articles deposited to arXiv are, in general, compliant
with the requirements of the HEFCE policy. The paper summarises some work
undertaken by Jisc to establish if there are other factors that can indicate
the likelihood of formal compliance to the HEFCE policy.
"
2154,Do neutrons publish? A neutron publication survey 2005-2015,"  Publication in scientific journals is the main product of scientific
research. The amount of papers published, their placement in high impact
journals, and their citations are used as a measure of the productivity of
individual scientists, institutes or fields of science. To give a profound
basis on the publication record and the quality of the publication efforts in
neutron scattering, a survey has been done following the approach to use
bibliographic databases. Questions to be addressed by this survey are: Is the
productivity of research with neutrons changing over the years? Which is the
geographic distribution in this field of research? Which ones are leading
facilities? Is the quality of publications changing? The main results found are
presented.
"
2155,"The lost academic home: institutional affiliation links in Google
  Scholar Citations","  This paper analyzes the new affiliation feature available in Google-Scholar
Citations revealing that the affiliation-tool works well for most-institutions,
it is unable to detect all existing institutions in database, and it is not
always able to create unique-standardized entry for each-institution.
"
2156,"A Survey of User Expectations and Tool Limitations in Collaborative
  Scientific Authoring and Reviewing","  Collaborative scientific authoring is increasingly being supported by
software tools. Traditionally, desktop-based authoring tools had the most
advanced editing features, allowed for more formatting options, and included
more import/export filters. Web-based tools have excelled in their
collaboration support. Recently, developers on both sides have been trying to
close this gap by extending desktop-based tools to better support collaboration
and by making web-based tools richer in functionality. To verify to what extent
these developments actually meet the needs of researchers, we gathered precise
requirements towards better tool support for scientific authoring and reviewing
workflows by interviewing 213 users and studying a corpus of 27 documents. We
present the design of the survey and interpret its results. The conclusion is
that WYSIWYG and offline desktop authoring tools continue to be more popular
among academics than text-based and online editors.
"
2157,ARCHANGEL: Trusted Archives of Digital Public Documents,"  We present ARCHANGEL; a de-centralised platform for ensuring the long-term
integrity of digital documents stored within public archives. Document
integrity is fundamental to public trust in archives. Yet currently that trust
is built upon institutional reputation --- trust at face value in a centralised
authority, like a national government archive or University. ARCHANGEL proposes
a shift to a technological underscoring of that trust, using distributed ledger
technology (DLT) to cryptographically guarantee the provenance, immutability
and so the integrity of archived documents. We describe the ARCHANGEL
architecture, and report on a prototype of that architecture build over the
Ethereum infrastructure. We report early evaluation and feedback of ARCHANGEL
from stakeholders in the research data archives space.
"
2158,"Evaluation of research publications and publication channels in
  astronomy and astrophysics","  The astronomy community usually turns to the Astrophysics Data System for
bibliometrics. When the context is cross-disciplinary, commercial products like
Web of Science and Scopus are used along with related analytics tools instead.
The results are often tainted by inherent problems in the chosen classification
system. A review of the most common challenges and pitfalls is given.
  Commercial altmetrics products could be added to the evaluation toolbox in
the near future despite the fact that they are best suited for promotion
instead of evaluation.
  Norway, Denmark, and Finland have created journal and publisher ranking
systems that are used in national funding models. Differences in how astronomy
journals are weighed in these systems night be related to the volume of papers
published on a national level.
"
2159,"Black Open Access in Ukraine: Analysis of Downloading Sci-Hub
  Publications by Ukrainian Internet Users","  Introduction. High subscription fees to scholarly research journals provoke
researchers to use illegal channels of access to scientific information.
Analysis of statistical data on downloads of scholarly research papers by
Ukrainian Internet users from illegal web resource can help to define gaps in
information provision at the institutional or the state level for each
scientific field. Problem Statement. To conduct an analysis of behavior and
geography of downloads of scholarly research publications from illegal web
resource Sci-Hub by Ukrainian Internet users within the period from September
1, 2015 to February 29, 2016. Purpose. To assess the information needs of
Ukrainian researchers who download scientific papers from Sci-Hub. Materials
and Methods. The used file is available at public domain and contains complete
data of downloads of scholarly research articles from Sci-Hub for the period
from September 1, 2015 to February 29, 2016. Inquiries of users with Ukrainian
IP-addresses have been selected. Using DOI of downloaded articles enables
finding the publishers and journal brands with the help of CrossRef API,
whereas using the All Science Journal Classification (ASJC) codes makes it
possible to identify the subject. Results. The study has shown that the most
documents downloaded related to natural sciences (primarily, chemistry,
physics, and astronomy), with Elsevier publications being the most frequently
inquired by Ukrainian users of Sci-Hub and Internet users from Kyiv downloading
the papers most actively. Conclusion. The obtained data are important for
understanding the information needs of Ukrainian researchers and can be used to
formulate an optimal subscription policy for providing access to information
resources at Ukrainian R&D institutions.
"
2160,False Information on Web and Social Media: A Survey,"  False information can be created and spread easily through the web and social
media platforms, resulting in widespread real-world impact. Characterizing how
false information proliferates on social platforms and why it succeeds in
deceiving readers are critical to develop efficient detection algorithms and
tools for early detection. A recent surge of research in this area has aimed to
address the key issues using methods based on feature engineering, graph
mining, and information modeling. Majority of the research has primarily
focused on two broad categories of false information: opinion-based (e.g., fake
reviews), and fact-based (e.g., false news and hoaxes). Therefore, in this
work, we present a comprehensive survey spanning diverse aspects of false
information, namely (i) the actors involved in spreading false information,
(ii) rationale behind successfully deceiving readers, (iii) quantifying the
impact of false information, (iv) measuring its characteristics across
different dimensions, and finally, (iv) algorithms developed to detect false
information. In doing so, we create a unified framework to describe these
recent methods and highlight a number of important directions for future
research.
"
2161,"Coverage of highly-cited documents in Google Scholar, Web of Science,
  and Scopus: a multidisciplinary comparison","  This study explores the extent to which bibliometric indicators based on
counts of highly-cited documents could be affected by the choice of data
source. The initial hypothesis is that databases that rely on journal selection
criteria for their document coverage may not necessarily provide an accurate
representation of highly-cited documents across all subject areas, while
inclusive databases, which give each document the chance to stand on its own
merits, might be better suited to identify highly-cited documents. To test this
hypothesis, an analysis of 2,515 highly-cited documents published in 2006 that
Google Scholar displays in its Classic Papers product is carried out at the
level of broad subject categories, checking whether these documents are also
covered in Web of Science and Scopus, and whether the citation counts offered
by the different sources are similar. The results show that a large fraction of
highly-cited documents in the Social Sciences and Humanities (8.6%-28.2%) are
invisible to Web of Science and Scopus. In the Natural, Life, and Health
Sciences the proportion of missing highly-cited documents in Web of Science and
Scopus is much lower. Furthermore, in all areas, Spearman correlation
coefficients of citation counts in Google Scholar, as compared to Web of
Science and Scopus citation counts, are remarkably strong (.83-.99). The main
conclusion is that the data about highly-cited documents available in the
inclusive database Google Scholar does indeed reveal significant coverage
deficiencies in Web of Science and Scopus in several areas of research.
Therefore, using these selective databases to compute bibliometric indicators
based on counts of highly-cited documents might produce biased assessments in
poorly covered areas.
"
2162,"Characterizing the highly cited articles: a large-scale bibliometric
  analysis of the top 1% most cited research","  We conducted a large-scale analysis of around 10,000 scientific articles,
from the period 2007-2016, to study the bibliometric or formal aspects
influencing citations. A transversal analysis was conducted disaggregating the
articles into more than one hundred scientific areas and two groups, one
experimental and one control, each with a random sample of around five thousand
documents. The experimental group comprised a random sample of the top 1% most
cited articles in each field and year of publication (highly cited articles),
and the control group a random sample of the remaining articles in the Journal
Citation Reports (science and social science citation indexes in the Web of
Science database). As the main result, highly cited articles differ from
non-highly cited articles in most of the bibliometric aspects considered. There
are significant differences, below the 0.01 level, between the groups of
articles in many variables and areas. The highly cited articles are published
in journals of higher impact factor (33 percentile points above) and have 25%
higher co-authorship. The highly cited articles are also longer in terms of
number of pages (10% higher) and bibliographical references (35% more).
Finally, highly cited articles have slightly shorter titles (3% lower) but,
contrastingly, longer abstracts (10% higher).
"
2163,Can we use Google Scholar to identify highly-cited documents?,"  The main objective of this paper is to empirically test whether the
identification of highly-cited documents through Google Scholar is feasible and
reliable. To this end, we carried out a longitudinal analysis (1950 to 2013),
running a generic query (filtered only by year of publication) to minimise the
effects of academic search engine optimisation. This gave us a final sample of
64,000 documents (1,000 per year). The strong correlation between a document's
citations and its position in the search results (r= -0.67) led us to conclude
that Google Scholar is able to identify highly-cited papers effectively. This,
combined with Google Scholar's unique coverage (no restrictions on document
type and source), makes the academic search engine an invaluable tool for
bibliometric research relating to the identification of the most influential
scientific documents. We find evidence, however, that Google Scholar ranks
those documents whose language (or geographical web domain) matches with the
user's interface language higher than could be expected based on citations.
Nonetheless, this language effect and other factors related to the Google
Scholar's operation, i.e. the proper identification of versions and the date of
publication, only have an incidental impact. They do not compromise the ability
of Google Scholar to identify the highly-cited papers.
"
2164,Abstract Mining,"  We have developed an application that will take a ""MEDLINE"" output from the
PubMed database and allows the user to cluster all non-trivial words of the
abstracts of the PubMed output. The number of clusters to use can be selected
by the user.
  A specific cluster may be selected, and the PMIDs and dates for all
publications in the selected cluster are displayed underneath. See figure 2,
where cluster 12 is selected.
  The application also has an ""Abstracts"" tab, where the abstracts for the
selected cluster can be perused. Here, it is also possible to download a HTML
file containing the PMID, date, title, and abstract for each publication in the
selected cluster.
  A third tab is called ""Titles"", where all the titles for the selected cluster
are displayed.
  Via a ""Use Cluster"" button, the selected Cluster can itself be clustered. A
""Back"" button allows the user to return to any previous state.
  Finally, it is also possible to exclude documents whose abstracts contain
certain words (see figure 3).
  The application will allow researchers to enter general search terms in the
PubMed search engine, then use the application to search for publications of
special interest within those search terms.
"
2165,"A novel method for depicting academic disciplines through Google Scholar
  Citations: The case of Bibliometrics","  This article describes a procedure to generate a snapshot of the structure of
a specific scientific community and their outputs based on the information
available in Google Scholar Citations (GSC). We call this method MADAP
(Multifaceted Analysis of Disciplines through Academic Profiles). The
international community of researchers working in Bibliometrics,
Scientometrics, Informetrics, Webometrics, and Altmetrics was selected as a
case study. The records of the top 1,000 most cited documents by these authors
according to GSC were manually processed to fill any missing information and
deduplicate fields like the journal titles and book publishers. The results
suggest that it is feasible to use GSC and the MADAP method to produce an
accurate depiction of the community of researchers working in Bibliometrics
(both specialists and occasional researchers) and their publication habits
(main publication venues such as journals and book publishers). Additionally,
the wide document coverage of Google Scholar (specially books and book
chapters) enables more comprehensive analyses of the documents published in a
specific discipline than were previously possible with other citation indexes,
finally shedding light on what until now had been a blind spot in most citation
analyses.
"
2166,Opium in science and society: Numbers,"  In science and beyond, numbers are omnipresent when it comes to justifying
different kinds of judgments. Which scientific author, hiring committee-member,
or advisory board panelist has not been confronted with page-long ""publication
manuals"", ""assessment reports"", ""evaluation guidelines"", calling for p-values,
citation rates, h-indices, or other statistics in order to motivate judgments
about the ""quality"" of findings, applicants, or institutions? Yet, many of
those relying on and calling for statistics do not even seem to understand what
information those numbers can actually convey, and what not. Focusing on the
uninformed usage of bibliometrics as worrysome outgrowth of the increasing
quantification of science and society, we place the abuse of numbers into
larger historical contexts and trends. These are characterized by a
technology-driven bureaucratization of science, obsessions with control and
accountability, and mistrust in human intuitive judgment. The ongoing digital
revolution increases those trends. We call for bringing sanity back into
scientific judgment exercises. Despite all number crunching, many judgments -
be it about scientific output, scientists, or research institutions - will
neither be unambiguous, uncontroversial, or testable by external standards, nor
can they be otherwise validated or objectified. Under uncertainty, good human
judgment remains, for the better, indispensable, but it can be aided, so we
conclude, by a toolbox of simple judgment tools, called heuristics. In the best
position to use those heuristics are research evaluators (1) who have expertise
in the to-be-evaluated area of research, (2) who have profound knowledge in
bibliometrics, and (3) who are statistically literate.
"
2167,"Use of NoSQL database and visualization techniques to analyze massive
  scholarly article data from journals","  Visualization of the massive data is a challenging endeavor. Extracting data
and providing graphical representations can aid in its effective utilization in
terms of interpretation and knowledge discovery. Publishing research articles
has become a way of life for academicians. The scholarly publications can
shape-up the professional growth of authors and also expand the research and
technological growth of a country, continent and other demographic regions.
Scholarly articles have grown in gigantic numbers that are published in
different domains by various journals. Information related to articles,
authors, their affiliations, number of citations, country, publisher,
references and other information is like a gold mine for statisticians and data
analysts. This data when used skillfully, via visual analysis tool, can provide
valuable understanding and can aid in deeper exposition for researchers working
in domains like scientometrics and bibliometrics. Since the data is not readily
available, we used Google scholar, a comprehensive and free repository of
scholarly articles, as data source for our study. Data was scraped from Google
scholar and stored as a graph and later visualized in the form of nodes and its
relationships, which offered discerning and concealed information of growing
impact of articles, journals and authors in their domains. Not only this,
evident domain shift of an author, various research domains spread for an
author, predicting emerging domain and subdomains, detecting cartel behavior at
Journal and author-level was also depicted by graphical analysis. Neo4j graph
database was used in the background to help store the data in structured
manner.
"
2168,Trends in Russian research output indexed in Scopus and Web of Science,"  Trends are analysed in the annual number of documents published by Russian
institutions and indexed in Scopus and Web of Science, giving special attention
to the time period starting in the year 2013 in which the Project 5-100 was
launched by the Russian Government. Numbers are broken down by document type,
publication language, type of source, research discipline, country and source.
It is concluded that Russian publication counts strongly depend upon the
database used, and upon changes in database coverage, and that one should be
cautious when using indicators derived from WoS, and especially from Scopus, as
tools in the measurement of research performance and international orientation
of the Russian science system.
"
2169,"Wisdom in Sum of Parts: Multi-Platform Activity Prediction in Social
  Collaborative Sites","  In this paper, we proposed a novel framework which uses user interests
inferred from activities (a.k.a., activity interests) in multiple social
collaborative platforms to predict users' platform activities. Included in the
framework are two prediction approaches: (i) direct platform activity
prediction, which predicts a user's activities in a platform using his or her
activity interests from the same platform (e.g., predict if a user answers a
given Stack Overflow question using the user's interests inferred from his or
her prior answer and favorite activities in Stack Overflow), and (ii)
cross-platform activity prediction, which predicts a user's activities in a
platform using his or her activity interests from another platform (e.g.,
predict if a user answers a given Stack Overflow question using the user's
interests inferred from his or her fork and watch activities in GitHub). To
evaluate our proposed method, we conduct prediction experiments on two widely
used social collaborative platforms in the software development community:
GitHub and Stack Overflow. Our experiments show that combining both direct and
cross-platform activity prediction approaches yield the best accuracies for
predicting user activities in GitHub (AUC=0.75) and Stack Overflow (AUC=0.89).
"
2170,Factors Influencing Cities' Publishing Efficiency,"  Recently, a vast number of scientific publications have been produced in
cities in emerging countries. It has long been observed that the publication
output of Beijing has exceeded that of any other city in the world, including
such leading centres of science as Boston, New York, London, Paris, and Tokyo.
Researchers have suggested that, instead of focusing on cities' total
publication output, the quality of the output in terms of the number of highly
cited papers should be examined. However, in the period from 2014 to 2016,
Beijing produced as many highly cited papers as Boston, London, or New York. In
this paper, I propose another method to measure cities' publishing performance;
I focus on cities' publishing efficiency (i.e., the ratio of highly cited
articles to all articles produced in that city). First, I rank 554 cities based
on their publishing efficiency, then I reveal some general factors influencing
cities' publishing efficiency. The general factors examined in this paper are
as follows: the linguistic environment, cities' economic development level, the
location of excellent organisations, cities' international collaboration
patterns, and the productivity of scientific disciplines.
"
2171,"Time Reversed Delay Differential Equation Based Modeling Of Journal
  Influence In An Emerging Area","  A recent independent study resulted in a ranking system which ranked
Astronomy and Computing (ASCOM) much higher than most of the older journals
highlighting its niche prominence. We investigate the notable ascendancy in
reputation of ASCOM by proposing a novel differential equation based modeling.
The modeling is a consequence of knowledge discovery from big data methods,
namely L1-SVD. We propose a growth model by accounting for the behavior of
parameters that contribute to the growth of a field. It is worthwhile to spend
some time in analyzing the cause and control variables behind rapid rise in the
reputation of a journal in a niche area. We intend to identify and probe the
parameters responsible for its growing influence. Delay differential equations
are used to model the change of influence on a journal's status by exploiting
the effects of historical data. The manuscript justifies the use of implicit
control variables and models those accordingly that demonstrate certain
behavior in the journal influence.
"
2172,"OK Google, What Is Your Ontology? Or: Exploring Freebase Classification
  to Understand Google's Knowledge Graph","  This paper reconstructs the Freebase data dumps to understand the underlying
ontology behind Google's semantic search feature. The Freebase knowledge base
was a major Semantic Web and linked data technology that was acquired by Google
in 2010 to support the Google Knowledge Graph, the backend for Google search
results that include structured answers to queries instead of a series of links
to external resources. After its shutdown in 2016, Freebase is contained in a
data dump of 1.9 billion Resource Description Format (RDF) triples. A
recomposition of the Freebase ontology will be analyzed in relation to concepts
and insights from the literature on classification by Bowker and Star. This
paper will explore how the Freebase ontology is shaped by many of the forces
that also shape classification systems through a deep dive into the ontology
and a small correlational study. These findings will provide a glimpse into the
proprietary blackbox Knowledge Graph and what is meant by Google's mission to
""organize the world's information and make it universally accessible and
useful"".
"
2173,"Query for Architecture, Click through Military: Comparing the Roles of
  Search and Navigation on Wikipedia","  As one of the richest sources of encyclopedic information on the Web,
Wikipedia generates an enormous amount of traffic. In this paper, we study
large-scale article access data of the English Wikipedia in order to compare
articles with respect to the two main paradigms of information seeking, i.e.,
search by formulating a query, and navigation by following hyperlinks. To this
end, we propose and employ two main metrics, namely (i) searchshare -- the
relative amount of views an article received by search --, and (ii) resistance
-- the ability of an article to relay traffic to other Wikipedia articles -- to
characterize articles. We demonstrate how articles in distinct topical
categories differ substantially in terms of these properties. For example,
architecture-related articles are often accessed through search and are
simultaneously a ""dead end"" for traffic, whereas historical articles about
military events are mainly navigated. We further link traffic differences to
varying network, content, and editing activity features. Lastly, we measure the
impact of the article properties by modeling access behavior on articles with a
gradient boosting approach. The results of this paper constitute a step towards
understanding human information seeking behavior on the Web.
"
2174,Research Curation on Knowledge Management,"  The notions of knowledge and its management have been at the core of the
information systems (IS) field almost since its inception. Knowledge has been
viewed in several ways in the prior literature, including as a state of mind,
an object, a process, access to information, and a capability. A commonly-used
definition characterizes knowledge as a justified belief that increases an
entity's capacity for effective action (Alavi and Leidner 2001, p. 109).
Relatedly, knowledge management (KM) has been defined as a systemic process to
acquire, organize, and communicate individual knowledge so that others may make
use of it (Beck et al. 2014). Knowledge-management systems (KMSs) support these
processes for creating, exchanging, and storing knowledge (Beck et al. 2014),
and have been viewed as being either repository- based or network-based
(Kankanhalli et al. 2005). In an attempt to provide a useful resource for
scholars interested in KM, we take stock of the pertinent research published in
MISQ. More specifically, the goal of this curation is to serve as a living
document that will offer a starting point for future KM research. This curation
highlights the 44 articles with a primary focus on KM (Table 1). The articles
address theoretical and conceptual issues, provide methodological guidance, and
use a wide range of quantitative and qualitative research methods. To define
the scope of this curation, we excluded: (1) articles in which KM is used as
part of another construct; (2) some early articles that were practice- oriented
with limited scholarly orientation; and (3) articles that focus on knowledge
(such as the knowledge requirements of IS professionals) but not on KM.
"
2175,Measuring Scientific Broadness,"  Who has not read letters of recommendations that comment on a student's
`broadness' and wondered what to make of it? We here propose a way to quantify
scientific broadness by a semantic analysis of researchers' publications. We
apply our methods to papers on the open-access server arXiv.org and report our
findings.
"
2176,"Citation Data-set for Machine Learning Citation Styles and Entity
  Extraction from Citation Strings","  Citation parsing is fundamental for search engines within academia and the
protection of intellectual property. Meticulous extraction is further needed
when evaluating the similarity of documents and calculating their citation
impact. Citation parsing involves the identification and dissection of citation
strings into their bibliography components such as ""Author"", ""Volume"",""Title"",
etc. This meta-data can be difficult to acquire accurately due to the thousands
of different styles and noise that can be applied to a bibliography to create
the citation string. Many approaches exist already to accomplish accurate
parsing of citation strings. This dissertation describes the creation of a
large data-set which can be used to aid in the training of these approaches
which have limited data.It also describes the investigation into if the
downfall of these approaches to citation parsing and in particular the machine
learning based approaches is because of the lack of size associated to the data
used to train them.
"
2177,"Interdisciplinary collaboration in research networks: Empirical analysis
  of energy-related research in Greece","  Technological innovation is intimately related to knowledge creation and
recombination. In this work we introduce a combined statistical and
network-based approach to study collaboration in scientific authorship. We
apply it to characterize recent research efforts in renewable energy technology
and its intersections with the domains of nanoscience and nanotechnology with
focus on materials, and electrical engineering and computer science in Greece
and its broader European and international environment as a case study. Using
our methods we attempt to illuminate the processes which underlie knowledge
creation and diversification in these research networks: a (positive)
relationship between expenditure on research and development and the extent and
diversity of team-based research at the intersections of the three domains is
established. Our specific findings collectively provide insights into the
collaboration structure and evolution of energy-related research activity in
Greece, while our methodology can be used for evidence-based design,
monitoring, and evaluation of interdisciplinary research programs.
"
2178,Citation Count Analysis for Papers with Preprints,"  We explore the degree to which papers prepublished on arXiv garner more
citations, in an attempt to paint a sharper picture of fairness issues related
to prepublishing. A paper's citation count is estimated using a
negative-binomial generalized linear model (GLM) while observing a binary
variable which indicates whether the paper has been prepublished. We control
for author influence (via the authors' h-index at the time of paper writing),
publication venue, and overall time that paper has been available on arXiv. Our
analysis only includes papers that were eventually accepted for publication at
top-tier CS conferences, and were posted on arXiv either before or after the
acceptance notification. We observe that papers submitted to arXiv before
acceptance have, on average, 65\% more citations in the following year compared
to papers submitted after. We note that this finding is not causal, and discuss
possible next steps.
"
2179,"Critical rationalism and the search for standard (field-normalized)
  indicators in bibliometrics","  Bibliometrics plays an increasingly important role in research evaluation.
However, no gold standard exists for a set of reliable and valid
(field-normalized) impact indicators in research evaluation. This opinion paper
recommends that bibliometricians develop and analyze these impact indicators
against the backdrop of Popper's critical rationalism. The studies critically
investigating the indicators should publish the results in such a way that they
can be included in meta-analyses. The results of meta-analyses give guidance on
which indicators can then be part of a set of indicators used as standard in
bibliometrics. The generation and continuous revision of the standard set could
be handled by the International Society for Informetrics and Scientometrics
(ISSI).
"
2180,"Peer-review under review - A statistical study on proposal ranking at
  ESO. Part I: the pre-meeting phase","  Peer review is the most common mechanism in place for assessing requests for
resources in a large variety of scientific disciplines. One of the strongest
criticisms to this paradigm is the limited reproducibility of the process,
especially at largely oversubscribed facilities. In this and in a subsequent
paper we address this specific aspect in a quantitative way, through a
statistical study on proposal ranking at the European Southern Observatory. For
this purpose we analysed a sample of about 15000 proposals, submitted by more
than 3000 Principal Investigators over 8 years. The proposals were reviewed by
more than 500 referees, who assigned over 140000 grades in about 200 panel
sessions. After providing a detailed analysis of the statistical properties of
the sample, the paper presents an heuristic model based on these findings,
which is then used to provide quantitative estimates of the reproducibility of
the pre-meeting process. On average, about one third of the proposals ranked in
the top quartile by one referee are ranked in the same quartile by any other
referee of the panel. A similar value is observed for the bottom quartile. In
the central quartiles, the agreement fractions are very marginally above the
value expected for a fully aleatory process (25%). The agreement fraction
between two panels composed by 6 referees is 55+/-5% (50% confidence level) for
the top and bottom quartiles. The corresponding fraction for the central
quartiles is 33+/-5%. The model predictions are confirmed by the results
obtained from boot-strapping the data for sub-panels composed by 3 referees,
and fully consistent with the NIPS experiment. The post-meeting phase will be
presented and discussed in a forthcoming paper.
"
2181,"Italian center for Astronomical Archives publishing solution: modular
  and distributed","  The Italian center for Astronomical Archives tries to provide astronomical
data resources as interoperable services based on IVOA standards. Its VO
expertise and knowledge comes from active participation within IVOA and VO at
European and international level, with a double-fold goal: learn from the
collaboration and provide inputs to the community. The first solution to build
an easy to configure and maintain resource publisher conformant to VO standards
proved to be too optimistic. For this reason it has been necessary to re-think
the architecture with a modular system built around the messaging concept,
where each modular component speaks to the other interested parties through a
system of broker-managed queues. The first implemented protocol, the Simple
Cone Search, shows the messaging task architecture connecting the parametric
HTTP interface to the database backend access module, the logging module, and
allows multiple cone search resources to be managed together through a
configuration manager module. Even if relatively young, it already proved the
flexibility required by the overall system when the database backend changed
from MySQL to PostgreSQL+PgSphere. Another implementation test has been made to
leverage task distribution over multiple servers to serve simultaneously: FITS
cubes direct linking, cubes cutout and cubes positional merging. Currently the
implementation of the SIA-2.0 standard protocol is ongoing while for TAP we
will be adapting the TAPlib library. Alongside these tools a first
administration tool (TASMAN) has been developed to ease the build up and
maintenance of TAP_SCHEMA-ta including also ObsCore maintenance capability.
Future work will be devoted at widening the range of VO protocols covered by
the set of available modules, improve the configuration management and develop
specific purpose modules common to all the service components.
"
2182,"Peer review and citation data in predicting university rankings, a
  large-scale analysis","  Most Performance-based Research Funding Systems (PRFS) draw on peer review
and bibliometric indicators, two different methodologies which are sometimes
combined. A common argument against the use of indicators in such research
evaluation exercises is their low correlation at the article level with peer
review judgments. In this study, we analyse 191,000 papers from 154 higher
education institutes which were peer reviewed in a national research evaluation
exercise. We combine these data with 6.95 million citations to the original
papers. We show that when citation-based indicators are applied at the
institutional or departmental level, rather than at the level of individual
papers, surprisingly large correlations with peer review judgments can be
observed, up to r <= 0.802, n = 37, p < 0.001 for some disciplines. In our
evaluation of ranking prediction performance based on citation data, we show we
can reduce the mean rank prediction error by 25% compared to previous work.
This suggests that citation-based indicators are sufficiently aligned with peer
review results at the institutional level to be used to lessen the overall
burden of peer review on national evaluation exercises leading to considerable
cost savings.
"
2183,"Corpus Conversion Service: A machine learning platform to ingest
  documents at scale [Poster abstract]","  Over the past few decades, the amount of scientific articles and technical
literature has increased exponentially in size. Consequently, there is a great
need for systems that can ingest these documents at scale and make their
content discoverable. Unfortunately, both the format of these documents (e.g.
the PDF format or bitmap images) as well as the presentation of the data (e.g.
complex tables) make the extraction of qualitative and quantitive data
extremely challenging. We present a platform to ingest documents at scale which
is powered by Machine Learning techniques and allows the user to train custom
models on document collections. We show precision/recall results greater than
97% with regard to conversion to structured formats, as well as scaling
evidence for each of the microservices constituting the platform.
"
2184,Scraping SERPs for Archival Seeds: It Matters When You Start,"  Event-based collections are often started with a web search, but the search
results you find on Day 1 may not be the same as those you find on Day 7. In
this paper, we consider collections that originate from extracting URIs
(Uniform Resource Identifiers) from Search Engine Result Pages (SERPs).
Specifically, we seek to provide insight about the retrievability of URIs of
news stories found on Google, and to answer two main questions: first, can one
""refind"" the same URI of a news story (for the same query) from Google after a
given time? Second, what is the probability of finding a story on Google over a
given period of time? To answer these questions, we issued seven queries to
Google every day for over seven months (2017-05-25 to 2018-01-12) and collected
links from the first five SERPs to generate seven collections for each query.
The queries represent public interest stories: ""healthcare bill,"" ""manchester
bombing,"" ""london terrorism,"" ""trump russia,"" ""travel ban,"" ""hurricane harvey,""
and ""hurricane irma."" We tracked each URI in all collections over time to
estimate the discoverability of URIs from the first five SERPs. Our results
showed that the daily average rate at which stories were replaced on the
default Google SERP ranged from 0.21 -0.54, and a weekly rate of 0.39 - 0.79,
suggesting the fast replacement of older stories by newer stories. The
probability of finding the same URI of a news story after one day from the
initial appearance on the SERP ranged from 0.34 - 0.44. After a week, the
probability of finding the same news stories diminishes rapidly to 0.01 - 0.11.
Our findings suggest that due to the difficulty in retrieving the URIs of news
stories from Google, collection building that originates from search engines
should begin as soon as possible in order to capture the first stages of
events, and should persist in order to capture the evolution of the events...
"
2185,Simplified Graph-based Visualization for Scientific Publication,"  Understanding citations to scientific publications is a task of vital
importance in the academic world. This task can be supported by appropriate
data structures and visualization mechanisms. One challenge is the amount of
existing relationships and the difficulty of determining which of the
references of a document are considered the most potentially relevant to it. In
this paper, we propose a simplified visualization of the relationships between
scientific publications, in the form of a directed acyclic graph. From a given
document, it is possible to visualize a path of references in which each step
corresponds to the main citation of the previous one. A methodology is proposed
in order to build this graph based in the opinion of the authors of scientific
articles and an editorial board.
"
2186,The value and credits of n-authors publications,"  Collaboration among researchers is becoming increasingly common, which raises
a large number of scientometrics questions for which there is not a clear and
generally accepted answer. For instance, what value should be given to a
two-author or three-author publication with respect to a single-author
publication? This paper uses axiomatic analysis and proposes a practical method
to compute the expected value of an n-authors publication that takes into
consideration the added value induced by collaboration in contexts in which
there is no prior or ex-ante information about the publication's potential
merits or scientific impact. The only information required is the number of
authors. We compared the obtained theoretical values with the empirical values
based on a large dataset from the Web of Science database. We found that the
theoretical values are very close to the empirical values for some disciplines,
but not for all. This observation provides support in favor of the method
proposed in this paper. We expect that our findings can help researchers and
decision-makers to choose more effective and fair counting methods that take
into account the benefits of collaboration.
"
2187,XFO: Toward Programming Rich Semantic Models,"  We have proposed that ontologies and programming languages should be more
closely aligned. Specifically, we have argued that the Basic Formal Ontology
(BFO2) has many features that are consistent with object-oriented analysis,
design, and modeling. Here, we describe the eXtended Formal Ontology (XFO), a
programming environment we developed to support semantic modeling. We then use
XFO to implement a Traffic Light Microworld and discuss more complex
applications.
"
2188,"Regions, Innovation Systems, and the North-South Divide in Italy","  Using firm-level data collected by Statistics Italy for 2008, 2011, and 2015,
we examine the Triple-Helix synergy among geographical and size distributions
of firms, and the NACE codes attributed to these firms, at the different levels
of regional and national government. At which levels is innovation-systemness
indicated? The contributions of regions to the Italian innovation system have
increased, but synergy generation between regions and supra-regionally has
remained at almost 45%. As against the statistical classification of Italy into
twenty regions or into Northern, Central, and Southern Italy, the greatest
synergy is retrieved by considering the country in terms of Northern and
Southern Italy as two sub-systems, with Tuscany included as part of Northern
Italy. We suggest that separate innovation strategies should be developed for
these two parts of the country. The current focus on regions for innovation
policies may to some extent be an artifact of the statistics and EU policies.
In terms of sectors, both medium- and high-tech manufacturing (MHTM) and
knowledge-intensive services (KIS) are proportionally integrated in the various
regions.
"
2189,Better Metrics for Ranking SE Researchers,"  This paper studies how SE researchers are ranked using a variety of metrics
and data from 35,406 authors of 35,391 papers from 34 top SE venues in the
period 1992-2016. Based on that analysis, we: deprecate the widely used
""h-index"", favoring instead an alternate Weighted Page Rank(PR_W) metric that
is somewhat analogous to the PageRank(PR) metric developed at Google. Unlike
the h-index, PR_W rewards not just citation counts but also how often authors
collaborate. Using PR_W, we offer a ranking of the top 20 SE authors in the
last decade.
"
2190,A Web-scale system for scientific knowledge exploration,"  To enable efficient exploration of Web-scale scientific knowledge, it is
necessary to organize scientific publications into a hierarchical concept
structure. In this work, we present a large-scale system to (1) identify
hundreds of thousands of scientific concepts, (2) tag these identified concepts
to hundreds of millions of scientific publications by leveraging both text and
graph structure, and (3) build a six-level concept hierarchy with a
subsumption-based model. The system builds the most comprehensive cross-domain
scientific concept ontology published to date, with more than 200 thousand
concepts and over one million relationships.
"
2191,CrowdRev: A platform for Crowd-based Screening of Literature Reviews,"  In this paper and demo we present a crowd and crowd+AI based system, called
CrowdRev, supporting the screening phase of literature reviews and achieving
the same quality as author classification at a fraction of the cost, and
near-instantly. CrowdRev makes it easy for authors to leverage the crowd, and
ensures that no money is wasted even in the face of difficult papers or
criteria: if the system detects that the task is too hard for the crowd, it
just gives up trying (for that paper, or for that criteria, or altogether),
without wasting money and never compromising on quality.
"
2192,Cascading Citation Expansion,"  Digital Science's Dimensions is envisaged as a next-generation research and
discovery platform for a better and more efficient access to cross-referenced
scholarly publications, grants, patents, and clinical trials. As a new addition
to the growing open citation resources, it offers opportunities that may
benefit a wide variety of stakeholders of scientific publications from
researchers, policy makers, and the general public. In this article, we explore
and demonstrate some of the practical potentials in terms of cascading citation
expansions. Given a set of publications, the cascading citation expansion
process can be successively applied to a set of articles so as to extend the
coverage to more and more relevant articles through citation links. Although
the conceptual origin can be traced back to Garfield's citation indexing, it
has been largely limited, until recently, to the few who have unrestricted
access to a citation database that is large enough to sustain such iterative
expansions. Building on the open API of Dimensions, we integrate cascading
citation expansion functions in CiteSpace and demonstrate how one may benefit
from these new capabilities. In conclusion, cascading citation expansion has
the potential to improve our understanding of the structure and dynamics of
scientific knowledge.
"
2193,"Creativity in Science and the Link to Cited References: Is the Creative
  Potential of Papers Reflected in their Cited References?","  Several authors have proposed that a large number of unusual combinations of
cited references in a paper point to its high creative potential (or novelty).
However, it is still not clear whether the number of unusual combinations can
really measure the creative potential of papers. The current study addresses
this question on the basis of several case studies from the field of
scientometrics. We identified some landmark papers in this field. Study
subjects were the corresponding authors of these papers. We asked them where
the ideas for the papers came from and which role the cited publications
played. The results revealed that the creative ideas might not necessarily have
been inspired by past publications. The literature seems to be important for
the contextualization of the idea in the field of scientometrics. Instead, we
found that creative ideas are the result of finding solutions to practical
problems, result from discussions with colleagues, and profit from
interdisciplinary exchange. The roots of the studied landmark papers are
discussed in detail.
"
2194,"On Performance of Peer Review for Academic Journals: Analysis Based on
  Distributed Parallel System","  A simulation model based on parallel systems is established, aiming to
explore the relation between the number of submissions and the overall standard
of academic journals within a similar discipline under peer review. The model
can effectively simulate the submission, review, and acceptance behaviors of
academic journals in a distributed manner. According to the simulation
experiments, it could possibly happen that the overall standard of academic
journals deteriorates due to excessive submissions.
"
2195,A Framework for Aggregating Private and Public Web Archives,"  Personal and private Web archives are proliferating due to the increase in
the tools to create them and the realization that Internet Archive and other
public Web archives are unable to capture personalized (e.g., Facebook) and
private (e.g., banking) Web pages. We introduce a framework to mitigate issues
of aggregation in private, personal, and public Web archives without
compromising potential sensitive information contained in private captures. We
amend Memento syntax and semantics to allow TimeMap enrichment to account for
additional attributes to be expressed inclusive of the requirements for
dereferencing private Web archive captures. We provide a method to involve the
user further in the negotiation of archival captures in dimensions beyond time.
We introduce a model for archival querying precedence and short-circuiting, as
needed when aggregating private and personal Web archive captures with those
from public Web archives through Memento. Negotiation of this sort is novel to
Web archiving and allows for the more seamless aggregation of various types of
Web archives to convey a more accurate picture of the past Web.
"
2196,Using the AIDA Language to Formally Organize Scientific Claims,"  Scientific communication still mainly relies on natural language written in
scientific papers, which makes the described knowledge very difficult to access
with automatic means. We can therefore only make limited use of formal
knowledge organization methods to support researchers and other interested
parties with features such as automatic aggregations, fact checking,
consistency checking, question answering, and powerful semantic search.
Existing approaches to solve this problem by improving the scientific
communication methods have either very restricted coverage, require formal
logic skills on the side of the researchers, or depend on unreliable machine
learning for the formalization of knowledge. Here, I propose an approach to
this problem that is general, intuitive, and flexible. It is based on a unique
kind of controlled natural language, called AIDA, consisting of English
sentences that are atomic, independent, declarative, and absolute. Such
sentences can then serve as nodes in a network of scientific claims linked to
publications, researchers, and domain elements. I present here some small
studies on preliminary applications of this language. The results indicate that
it is well accepted by users and provides a good basis for the creation of a
knowledge graph of scientific findings.
"
2197,"Corpus Conversion Service: A Machine Learning Platform to Ingest
  Documents at Scale","  Over the past few decades, the amount of scientific articles and technical
literature has increased exponentially in size. Consequently, there is a great
need for systems that can ingest these documents at scale and make the
contained knowledge discoverable. Unfortunately, both the format of these
documents (e.g. the PDF format or bitmap images) as well as the presentation of
the data (e.g. complex tables) make the extraction of qualitative and
quantitive data extremely challenging. In this paper, we present a modular,
cloud-based platform to ingest documents at scale. This platform, called the
Corpus Conversion Service (CCS), implements a pipeline which allows users to
parse and annotate documents (i.e. collect ground-truth), train
machine-learning classification algorithms and ultimately convert any type of
PDF or bitmap-documents to a structured content representation format. We will
show that each of the modules is scalable due to an asynchronous microservice
architecture and can therefore handle massive amounts of documents.
Furthermore, we will show that our capability to gather ground-truth is
accelerated by machine-learning algorithms by at least one order of magnitude.
This allows us to both gather large amounts of ground-truth in very little time
and obtain very good precision/recall metrics in the range of 99\% with regard
to content conversion to structured output. The CCS platform is currently
deployed on IBM internal infrastructure and serving more than 250 active users
for knowledge-engineering project engagements.
"
2198,"Content-Based Quality Estimation for Automatic Subject Indexing of Short
  Texts under Precision and Recall Constraints","  Semantic annotations have to satisfy quality constraints to be useful for
digital libraries, which is particularly challenging on large and diverse
datasets. Confidence scores of multi-label classification methods typically
refer only to the relevance of particular subjects, disregarding indicators of
insufficient content representation at the document-level. Therefore, we
propose a novel approach that detects documents rather than concepts where
quality criteria are met. Our approach uses a deep, multi-layered regression
architecture, which comprises a variety of content-based indicators. We
evaluated multiple configurations using text collections from law and
economics, where the available content is restricted to very short texts.
Notably, we demonstrate that the proposed quality estimation technique can
determine subsets of the previously unseen data where considerable gains in
document-level recall can be achieved, while upholding precision at the same
time. Hence, the approach effectively performs a filtering that ensures high
data quality standards in operative information retrieval systems.
"
2199,Collaboration Diversity and Scientific Impact,"  The shift from individual effort to collaborative output has benefited
science, with scientific work pursued collaboratively having increasingly led
to more highly impactful research than that pursued individually. However,
understanding of how the diversity of a collaborative team influences the
production of knowledge and innovation is sorely lacking. Here, we study this
question by breaking down the process of scientific collaboration of 32.9
million papers over the last five decades. We find that the probability of
producing a top-cited publication increases as a function of the diversity of a
team of collaborators---namely, the distinct number of institutions represented
by the team. We discover striking phenomena where a smaller, yet more diverse
team is more likely to generate highly innovative work than a relatively larger
team within one institution. We demonstrate that the synergy of collaboration
diversity is universal across different generations, research fields, and tiers
of institutions and individual authors. Our findings suggest that collaboration
diversity strongly and positively correlates with the production of scientific
innovation, giving rise to the potential revolution of the policies used by
funding agencies and authorities to fund research projects, and broadly the
principles used to organize teams, organizations, and societies.
"
2200,"PubMed Labs: An experimental platform for improving biomedical
  literature search","  PubMed is a freely accessible system for searching the biomedical literature,
with approximately 2.5 million users worldwide on an average workday. We have
recently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform
for users to test new features/tools and provide feedback, which enables us to
make more informed decisions about potential changes to improve the search
quality and overall usability of PubMed. In doing so, we hope to better meet
our user needs in an era of information overload. Another novel aspect of
PubMed Labs lies in its mobile-first and responsive layout, which offers better
support for accessing PubMed on the increasingly popular use of mobile and
small-screen devices. Currently, PubMed Labs only includes a core subset of
PubMed functionalities, e.g. search, facets. We encourage users to test PubMed
Labs and share their experience with us, based on which we expect to
continuously improve PubMed Labs with more advanced features and better user
experience.
"
2201,"Bipartite graph analysis as an alternative to reveal clusterization in
  complex systems","  We demonstrate how analysis of co-clustering in bipartite networks may be
used as a bridge to connect, compare and complement clustering results about
community structure in two different spaces: single-mode bipartite network
projections. As a case study we consider scientific knowledge, which is
represented as a complex bipartite network of articles and related concepts.
Connecting clusters of articles and clusters of concepts via article-to-concept
bipartite co-clustering, we demonstrate how concept features (e.g. subject
classes) may be inferred from the article ones.
"
2202,Google Scholar as a data source for research assessment,"  The launch of Google Scholar (GS) marked the beginning of a revolution in the
scientific information market. This search engine, unlike traditional
databases, automatically indexes information from the academic web. Its ease of
use, together with its wide coverage and fast indexing speed, have made it the
first tool most scientists currently turn to when they need to carry out a
literature search. Additionally, the fact that its search results were
accompanied from the beginning by citation counts, as well as the later
development of secondary products which leverage this citation data (such as
Google Scholar Metrics and Google Scholar Citations), made many scientists
wonder about its potential as a source of data for bibliometric analyses. The
goal of this chapter is to lay the foundations for the use of GS as a
supplementary source (and in some disciplines, arguably the best alternative)
for scientific evaluation. First, we present a general overview of how GS
works. Second, we present empirical evidences about its main characteristics
(size, coverage, and growth rate). Third, we carry out a systematic analysis of
the main limitations this search engine presents as a tool for the evaluation
of scientific performance. Lastly, we discuss the main differences between GS
and other more traditional bibliographic databases in light of the correlations
found between their citation data. We conclude that Google Scholar presents a
broader view of the academic world because it has brought to light a great
amount of sources that were not previously visible.
"
2203,Predicting Citation Counts with a Neural Network,"  We here describe and present results of a simple neural network that predicts
individual researchers' future citation counts based on a variety of data from
the researchers' past. For publications available on the open access-server
arXiv.org we find a higher predictability than previous studies.
"
2204,"Unbundling Open Access dimensions: a conceptual discussion to reduce
  terminology inconsistencies","  The current ways in which documents are made freely accessible in the Web no
longer adhere to the models established Budapest/Bethesda/Berlin (BBB)
definitions of Open Access (OA). Since those definitions were established,
OA-related terminology has expanded, trying to keep up with all the variants of
OA publishing that are out there. However, the inconsistent and arbitrary
terminology that is being used to refer to these variants are complicating
communication about OA-related issues. This study intends to initiate a
discussion on this issue, by proposing a conceptual model of OA. Our model
features six different dimensions (prestige, user rights, stability, immediacy,
peer-review, and cost). Each dimension allows for a range of different options.
We believe that by combining the options in these six dimensions, we can arrive
at all the current variants of OA, while avoiding ambiguous and/or arbitrary
terminology. This model can be an useful tool for funders and policy makers who
need to decide exactly which aspects of OA are necessary for each specific
scenario.
"
2205,Analysis of Search Stratagem Utilisation,"  In Interactive IR, researchers consider the user behaviour towards systems
and search tasks in order to adapt search results and to improve the search
experience of users. Analysing the users' past interactions with the system is
one typical approach. In this paper, we analyse the user behaviour in retrieval
sessions towards Marcia Bates' search stratagems such as Footnote Chasing,
Citation Searching, Keyword Searching, Author Searching and Journal Run in a
real-life academic search engine. In fact, search stratagems represent
high-level search behaviour as the users go beyond simple execution of queries
and investigate more of the system functionalities. We performed analyses of
these five search stratagems using two datasets extracted from the social
sciences search engine sowiport. A specific focus was the detection of the
search phase and frequency of the usage of these stratagems. In addition, we
explored the impact of these stratagems on the whole search process
performance. We addressed mainly the usage patterns' observation of the
stratagems, their impact on the conduct of retrieval sessions and explore
whether they are used similarly in both datasets. From the observation and
metrics proposed, we can conclude that the utilisation of search stratagems in
real retrieval sessions leads to an improvement of the precision in terms of
positive interactions. However, the difference is that Footnote Chasing,
Citation Searching and Journal Run appear mostly at the end of a session while
Keyword and Author Searching appear typically at the beginning. Thus, we can
conclude from the log analysis that the improvement of search functionalities
including personalisation and/or recommendation could be achieved by
considering references, citations, and journals in the ranking process.
"
2206,"Homonym Detection in Curated Bibliographies: Learning from dblp's
  Experience (full version)","  Identifying (and fixing) homonymous and synonymous author profiles is one of
the major tasks of curating personalized bibliographic metadata repositories
like the dblp computer science bibliography. In this paper, we present and
evaluate a machine learning approach to identify homonymous author
bibliographies using a simple multilayer perceptron setup. We train our model
on a novel gold-standard data set derived from the past years of active, manual
curation at the dblp computer science bibliography.
"
2207,Google Scholar: the 'big data' bibliographic tool,"  The launch of Google Scholar back in 2004 meant a revolution not only in the
scientific information search market but also in research evaluation processes.
Its dynamism, unparalleled coverage, and uncontrolled indexing make of Google
Scholar an unusual product, especially when compared to traditional
bibliographic databases. Conceived primarily as a discovery tool for academic
information, it presents a number of limitations as a bibliometric tool. The
main objective of this chapter is to show how Google Scholar operates and how
its core database may be used for bibliometric purposes. To do this, the
general features of the search engine (in terms of document typologies,
disciplines, and coverage) are analysed. Lastly, several bibliometric tools
based on Google Scholar data, both official (Google Scholar Metrics, Google
Scholar Citations), and some developed by third parties (H Index Scholar,
Publishers Scholar Metrics, Proceedings Scholar Metrics, Journal Scholar
Metrics, Scholar Mirrors), as well as software to collect and process data from
this source (Publish or Perish, Scholarometer) are introduced, aiming to
illustrate the potential bibliometric uses of this source.
"
2208,Utilizing Provenance in Reusable Research Objects,"  Science is conducted collaboratively, often requiring the sharing of
knowledge about computational experiments. When experiments include only
datasets, they can be shared using Uniform Resource Identifiers (URIs) or
Digital Object Identifiers (DOIs). An experiment, however, seldom includes only
datasets, but more often includes software, its past execution, provenance, and
associated documentation. The Research Object has recently emerged as a
comprehensive and systematic method for aggregation and identification of
diverse elements of computational experiments. While a necessary method, mere
aggregation is not sufficient for the sharing of computational experiments.
Other users must be able to easily recompute on these shared research objects.
Computational provenance is often the key to enable such reuse. In this paper,
we show how reusable research objects can utilize provenance to correctly
repeat a previous reference execution, to construct a subset of a research
object for partial reuse, and to reuse existing contents of a research object
for modified reuse. We describe two methods to summarize provenance that aid in
understanding the contents and past executions of a research object. The first
method obtains a process-view by collapsing low-level system information, and
the second method obtains a summary graph by grouping related nodes and edges
with the goal to obtain a graph view similar to application workflow. Through
detailed experiments, we show the efficacy and efficiency of our algorithms.
"
2209,TIB-arXiv: An Alternative Search Portal for the arXiv Pre-print Server,"  arXiv is a popular pre-print server focusing on natural science disciplines
(e.g. physics, computer science, quantitative biology). As a platform with
focus on easy publishing services it does not provide enhanced search
functionality -- but offers programming interfaces which allow external parties
to add these services. This paper presents extensions of the open source
framework arXiv Sanity Preserver (SP). With respect to the original framework,
it derestricts the topical focus and allows for text-based search and
visualisation of all papers in arXiv. To this end, all papers are stored in a
unified back-end; the extension provides enhanced search and ranking facilities
and allows the exploration of arXiv papers by a novel user interface.
"
2210,The Off-Topic Memento Toolkit,"  Web archive collections are created with a particular purpose in mind. A
curator selects seeds, or original resources, which are then captured by an
archiving system and stored as archived web pages, or mementos. The systems
that build web archive collections are often configured to revisit the same
original resource multiple times. This is incredibly useful for understanding
an unfolding news story or the evolution of an organization. Unfortunately,
over time, some of these original resources can go off-topic and no longer suit
the purpose for which the collection was originally created. They can go
off-topic due to web site redesigns, changes in domain ownership, financial
issues, hacking, technical problems, or because their content has moved on from
the original topic. Even though they are off-topic, the archiving system will
still capture them, thus it becomes imperative to anyone performing research on
these collections to identify these off-topic mementos. Hence, we present the
Off-Topic Memento Toolkit, which allows users to detect off-topic mementos
within web archive collections. The mementos identified by this toolkit can
then be separately removed from a collection or merely excluded from downstream
analysis. The following similarity measures are available: byte count, word
count, cosine similarity, Jaccard distance, S{\o}rensen-Dice distance, Simhash
using raw text content, Simhash using term frequency, and Latent Semantic
Indexing via the gensim library. We document the implementation of each of
these similarity measures. We possess a gold standard dataset generated by
manual analysis, which contains both off-topic and on-topic mementos. Using
this gold standard dataset, we establish a default threshold corresponding to
the best F1 score for each measure. We also provide an overview of potential
future directions that the toolkit may take.
"
2211,The Many Shapes of Archive-It,"  Web archives, a key area of digital preservation, meet the needs of
journalists, social scientists, historians, and government organizations. The
use cases for these groups often require that they guide the archiving process
themselves, selecting their own original resources, or seeds, and creating
their own web archive collections. We focus on the collections within
Archive-It, a subscription service started by the Internet Archive in 2005 for
the purpose of allowing organizations to create their own collections of
archived web pages, or mementos. Understanding these collections could be done
via their user-supplied metadata or via text analysis, but the metadata is
applied inconsistently between collections and some Archive-It collections
consist of hundreds of thousands of seeds, making it costly in terms of time to
download each memento. Our work proposes using structural metadata as an
additional way to understand these collections. We explore structural features
currently existing in these collections that can unveil curation and crawling
behaviors. We adapt the concept of the collection growth curve for
understanding Archive-It collection curation and crawling behavior. We also
introduce several seed features and come to an understanding of the diversity
of resources that make up a collection. Finally, we use the descriptions of
each collection to identify four semantic categories of Archive-It collections.
Using the identified structural features, we reviewed the results of runs with
20 classifiers and are able to predict the semantic category of a collection
using a Random Forest classifier with a weighted average F1 score of 0.720,
thus bridging the structural to the descriptive. Our method is useful because
it saves the researcher time and bandwidth. Identifying collections by their
semantic category allows further downstream processing to be tailored to these
categories.
"
2212,"Cross-country comparisons of scientific performance by focusing on
  post-apartheid South Africa","  This paper examines the scientific performance of South Africa since 1994
(post-apartheid) until 2014 in comparisons with the rest of the world,
utilizing relative indicator. It provides a view over current standing of South
Africa in the scientific world as well as its temporal evolution after the
apartheid. This study focuses on four major aspects of scientific performance,
namely quantity, productivity, impact and quality, as the main attributes of
scientific perfomance on national level. These are measured by re-based
(relative) publication, publication per population or GDP, citations and
citations per publication respectively. The study focuses on scientific outputs
(in the form of papers published in peer-reviewed journals) and their impact
(measured by the citations they have received) to bring into a light a
homogeneous comprehension of South Africa's scientific performance in all these
four aspects. Indicators are adopted cautiously by considering the measures put
forward recently for scientometrics indicators and their usage in the long-term
comparisons studies. The temporal evolution of these indicators for South
Africa are discussed in the context of three major groups of countries, namely
African countries, developing countries, and developed (including BRICS)
countries. It aims to examine the process of transition of South Africa from a
developing world economy system into a knowledge-based and innovation-driven
one of the developed world. The study reveals that South Africa has shown
steady increase in its scientific performance during the studied period when
compared to the rest of the world. However, due to the increasing competition
from the other developing countries, South Africa's position stands the same
during this period, while countries such as China, Iran, Turkey and Malaysia
have shown great jump at least in the quantity of their scientific performance.
"
2213,"Recommending Scientific Videos based on Metadata Enrichment using Linked
  Open Data","  The amount of available videos in the Web has significantly increased not
only for entertainment etc., but also to convey educational or scientific
information in an effective way. There are several web portals that offer
access to the latter kind of video material. One of them is the TIB AV-Portal
of the Leibniz Information Centre for Science and Technology (TIB), which hosts
scientific and educational video content. In contrast to other video portals,
automatic audiovisual analysis (visual concept classification, optical
character recognition, speech recognition) is utilized to enhance metadata
information and semantic search. In this paper, we propose to further exploit
and enrich this automatically generated information by linking it to the
Integrated Authority File (GND) of the German National Library. This
information is used to derive a measure to compare the similarity of two videos
which serves as a basis for recommending semantically similar videos. A user
study demonstrates the feasibility of the proposed approach.
"
2214,Developing a Temporal Bibliographic Data Set for Entity Resolution,"  Entity resolution is the process of identifying groups of records within or
across data sets where each group represents a real-world entity. Novel
techniques that consider temporal features to improve the quality of entity
resolution have recently attracted significant attention. However, there are
currently no large data sets available that contain both temporal information
as well as ground truth information to evaluate the quality of temporal entity
resolution approaches. In this paper, we describe the preparation of a temporal
data set based on author profiles extracted from the Digital Bibliography and
Library Project (DBLP). We completed missing links between publications and
author profiles in the DBLP data set using the DBLP public API. We then used
the Microsoft Academic Graph (MAG) to link temporal affiliation information for
DBLP authors. We selected around 80K (1%) of author profiles that cover 2
million (50%) publications using information in DBLP such as alternative author
names and personal web profile to improve the reliability of the resulting
ground truth, while at the same time keeping the data set challenging for
temporal entity resolution research.
"
2215,"An Analytics Tool for Exploring Scientific Software and Related
  Publications","  Scientific software is one of the key elements for reproducible research.
However, classic publications and related scientific software are typically not
(sufficiently) linked, and it lacks tools to jointly explore these artefacts.
In this paper, we report on our work on developing an analytics tool for
jointly exploring software and publications. The presented prototype, a concept
for automatic code discovery, and two use cases demonstrate the feasibility and
usefulness of the proposal.
  Submitted to TPDL 2018 as Demonstration Paper.
"
2216,"Scientific mobility indicators in practice: International mobility
  profiles at the country level","  This paper presents and describes the methodological opportunities offered by
bibliometric data to produce indicators of scientific mobility. Large
bibliographic datasets of disambiguated authors and their affiliations allow
for the possibility of tracking the affiliation changes of scientists. Using
the Web of Science as data source, we analyze the distribution of types of
mobile scientists for a selection of countries. We explore the possibility of
creating profiles of international mobility at the country level, and discuss
potential interpretations and caveats. Five countries (Canada, The Netherlands,
South Africa, Spain, and the United States) are used as examples. These
profiles enable us to characterize these countries in terms of their strongest
links with other countries. This type of analysis reveals circulation among and
between countries with strong policy implications.
"
2217,"Gamification for Education of the Digitally Native Generation by Means
  of Virtual Reality, Augmented Reality, Machine Learning, and Brain-Computing
  Interfaces in Museums","  Particularly close attention is being paid today among researchers in social
science disciplines to aspects of learning in the digital age, especially for
the Digitally Native Generation. In the context of museums, the question is:
how can rich learning experiences be provided for increasingly technologically
advanced young visitors in museums? Which high-tech platforms and solutions do
museums need to focus on? At the same time, the software games business is
growing fast and now finding its way into non-entertainment contexts, helping
to deliver substantial benefits, particularly in education, training, research,
and health. This article outlines some aspects facing Digitally Native learners
in museums through an analysis of several radically new key technologies:
Interactivity, Wearables, Virtual Reality, and Augmented Reality. Special
attention is paid to use cases for application of games-based scenarios via
these technologies in non-leisure contexts and specifically for educational
purposes in museums.
"
2218,"Metadata Enrichment of Multi-Disciplinary Digital Library: A
  Semantic-based Approach","  In the scientific digital libraries, some papers from different research
communities can be described by community-dependent keywords even if they share
a semantically similar topic. Articles that are not tagged with enough keyword
variations are poorly indexed in any information retrieval system which limits
potentially fruitful exchanges between scientific disciplines. In this paper,
we introduce a novel experimentally designed pipeline for multi-label
semantic-based tagging developed for open-access metadata digital libraries.
The approach starts by learning from a standard scientific categorization and a
sample of topic tagged articles to find semantically relevant articles and
enrich its metadata accordingly. Our proposed pipeline aims to enable
researchers reaching articles from various disciplines that tend to use
different terminologies. It allows retrieving semantically relevant articles
given a limited known variation of search terms. In addition to achieving an
accuracy that is higher than an expanded query based method using a topic
synonym set extracted from a semantic network, our experiments also show a
higher computational scalability versus other comparable techniques. We created
a new benchmark extracted from the open-access metadata of a scientific digital
library and published it along with the experiment code to allow further
research in the topic.
"
2219,Finding Person Relations in Image Data of the Internet Archive,"  The multimedia content in the World Wide Web is rapidly growing and contains
valuable information for many applications in different domains. For this
reason, the Internet Archive initiative has been gathering billions of
time-versioned web pages since the mid-nineties. However, the huge amount of
data is rarely labeled with appropriate metadata and automatic approaches are
required to enable semantic search. Normally, the textual content of the
Internet Archive is used to extract entities and their possible relations
across domains such as politics and entertainment, whereas image and video
content is usually neglected. In this paper, we introduce a system for person
recognition in image content of web news stored in the Internet Archive. Thus,
the system complements entity recognition in text and allows researchers and
analysts to track media coverage and relations of persons more precisely. Based
on a deep learning face recognition approach, we suggest a system that
automatically detects persons of interest and gathers sample material, which is
subsequently used to identify them in the image data of the Internet Archive.
We evaluate the performance of the face recognition system on an appropriate
standard benchmark dataset and demonstrate the feasibility of the approach with
two use cases.
"
2220,Simulation Study on a New Peer Review Approach,"  The increasing volume of scientific publications and grant proposals has
generated an unprecedentedly high workload to scientific communities.
Consequently, review quality has been decreasing and review outcomes have
become less correlated with the real merits of the papers and proposals. A
novel distributed peer review (DPR) approach has recently been proposed to
address these issues. The new approach assigns principal investigators (PIs)
who submitted proposals (or papers) to the same program as reviewers. Each PI
reviews and ranks a small number (such as seven) of other PIs' proposals. The
individual rankings are then used to estimate a global ranking of all proposals
using the Modified Borda Count (MBC). In this study, we perform simulation
studies to investigate several parameters important for the decision making
when adopting this new approach. We also propose a new method called
Concordance Index-based Global Ranking (CIGR) to estimate global ranking from
individual rankings. An efficient simulated annealing algorithm is designed to
search the optimal Concordance Index (CI). Moreover, we design a new balanced
review assignment procedure, which can result in significantly better
performance for both MBC and CIGR methods. We found that CIGR performs better
than MBC when the review quality is relatively high. As review quality and
review difficulty are tightly correlated, we constructed a boundary in the
space of review quality vs review difficulty that separates the CIGR-superior
and MBC-superior regions. Finally, we propose a multi-stage DPR strategy based
on CIGR, which has the potential to substantially improve the overall review
performance while reducing the review workload.
"
2221,"ESO telbib: learning from experience, preparing for the future","  The ESO telescope bibliography (telbib) dates back to 1996. During the 20+
years of its existence, it has undergone many changes. Most importantly, the
telbib system has been enhanced to cater to new use cases and demands from its
stakeholders. Based on achievements of the past, we will show how a system like
telbib can not only stay relevant through the decades, but gain importance, and
provide an essential tool for the observatory's management and the wider user
community alike.
"
2222,Measuring News Similarity Across Ten U.S. News Sites,"  News websites make editorial decisions about what stories to include on their
website homepages and what stories to emphasize (e.g., large font size for main
story). The emphasized stories on a news website are often highly similar to
many other news websites (e.g, a terrorist event story). The selective emphasis
of a top news story and the similarity of news across different news
organizations are well-known phenomena but not well-measured. We provide a
method for identifying the top news story for a select set of U.S.-based news
websites and then quantify the similarity across them. To achieve this, we
first developed a headline and link extractor that parses select websites, and
then examined ten United States based news website homepages during a three
month period, November 2016 to January 2017. Using archived copies, retrieved
from the Internet Archive (IA), we discuss the methods and difficulties for
parsing these websites, and how events such as a presidential election can lead
news websites to alter their document representation just for these events. We
use our parser to extract k = 1, 3, 10 maximum number of stories for each news
site. Second, we used the cosine similarity measure to calculate news
similarity at 8PM Eastern Time for each day in the three months. The similarity
scores show a buildup (0.335) before Election Day, with a declining value
(0.328) on Election Day, and an increase (0.354) after Election Day. Our method
shows that we can effectively identity top stories and quantify news
similarity.
"
2223,"EM and EM'-index sequence: Construction and application in scientific
  assessment of scholars","  Most of the scientometric indicators use only the total number of citations
of an article and produce a single number for scientific assessment of
scholars. Although this concept is very simple to compute, it fails to show the
scientific productivity and impact of scholars during a time-span or in a year.
To overcome this, several time series indicators have been proposed that
consider the citations from the entire research career of a scholar. However,
these indicators fail to give a comparative assessment of two scholars having
same or very similar index value. To overcome this shortcoming, h-index
sequence was proposed to assess the impact of scholars during a particular
time-span and to compare multiple scholars at a similar stage in their careers.
The h-index sequence is based on the h-index formulation. One of the main
issues related to the h-index is that it completely ignores the excess citation
in scientific assessment; h-index sequence also exhibits a similar behaviour.
To overcome these limitations, in this article, we have discussed the EM and
$EM^{'}$-index sequence, and performed an empirical study based on yearly
citation count earned from all publications of 89 scholars' publication data.
The element of the EM and $EM^{'}$-index sequence for a given year shows the
impact of a scholar for that year. We conclude that the EM and $EM^{'}$-index
sequence could be used as an alternative metrics to asses the impact of
scholars.
"
2224,"An empirical investigation of the Tribes and their Territories: are
  research specialisms rural and urban?","  We propose an operationalization of the rural and urban analogy introduced in
Becher and Trowler [2001]. According to them, a specialism is rural if it is
organized into many, smaller topics of research, with higher author mobility
among them, lower rate of collaboration and productivity, lower competition for
resources and citation recognitions compared to an urban specialism. It is
assumed that most humanities specialisms are rural while science specialisms
are in general urban: we set to test this hypothesis empirically. We first
propose an operationalization of the theory in most of its quantifiable
aspects. We then consider specialisms from history, literature, computer
science, biology, astronomy. Our results show that specialisms in the
humanities present a sensibly lower citation and textual connectivity, in
agreement with their organization into more, smaller topics per specialism, as
suggested by the analogy. We argue that the intellectual organization of rural
specialisms might indeed be qualitative different from urban ones, discouraging
the straightforward application of citation-based indicators commonly applied
to urban specialisms without a dedicated re-design in acknowledgement of these
differences.
"
2225,Scientometric analysis of Condensed Matter Physics journal,"  The paper is dedicated to 25th anniversary of Condensed Matter Physics
journal (CMP). It contains the results of comprehensive analysis of different
journal-related data. CMP co-authorship relationships are studied analysing the
collaboration network. Its cumulative statical and dynamical properties as well
as the structure are discussed. The international contribution to the journal
is assessed using the authors' affiliation data. The network of the countries
collaborating within CMP is considered. Another kind of network is used to
investigate the topical spectrum: two PACS indices assigned to one paper are
connected by link here. The structure of the most significant interdisciplinary
connections is analysed. Finally, the download statistics and the corresponding
records of the papers' citations are used to discuss the journal's impact.
"
2226,"Evaluating author name disambiguation for digital libraries: A case of
  DBLP","  Author name ambiguity in a digital library may affect the findings of
research that mines authorship data of the library. This study evaluates author
name disambiguation in DBLP, a widely used but insufficiently evaluated digital
library for its disambiguation performance. In doing so, this study takes a
triangulation approach that author name disambiguation for a digital library
can be better evaluated when its performance is assessed on multiple labeled
datasets with comparison to baselines. Tested on three types of labeled data
containing 5,000 ~ 700K disambiguated names and 6M pairs of disambiguated
names, DBLP is shown to assign author names quite accurately to distinct
authors, resulting in pairwise precision, recall, and F1 measures around 0.90
or above overall. DBLP's author name disambiguation performs well even on large
ambiguous name blocks but deficiently on distinguishing authors with the same
names. When compared to other disambiguation algorithms, DBLP's disambiguation
performance is quite competitive, possibly due to its hybrid disambiguation
approach combining algorithmic disambiguation and manual error correction. A
discussion follows on strengths and weaknesses of labeled datasets used in this
study for future efforts to evaluate author name disambiguation on a digital
library scale.
"
2227,Social media metrics for new research evaluation,"  This chapter approaches, both from a theoretical and practical perspective,
the most important principles and conceptual frameworks that can be considered
in the application of social media metrics for scientific evaluation. We
propose conceptually valid uses for social media metrics in research
evaluation. The chapter discusses frameworks and uses of these metrics as well
as principles and recommendations for the consideration and application of
current (and potentially new) metrics in research evaluation.
"
2228,"Author-Based Analysis of Conference versus Journal Publication in
  Computer Science","  Conference publications in computer science (CS) have attracted scholarly
attention due to their unique status as a main research outlet unlike other
science fields where journals are dominantly used for communicating research
findings. One frequent research question has been how different conference and
journal publications are, considering a paper as a unit of analysis. This study
takes an author-based approach to analyze publishing patterns of 517,763
scholars who have ever published both in CS conferences and journals for the
last 57 years, as recorded in DBLP. The analysis shows that the majority of CS
scholars tend to make their scholarly debut, publish more papers, and
collaborate with more coauthors in conferences than in journals. Importantly,
conference papers seem to serve as a distinct channel of scholarly
communication, not a mere preceding step to journal publications: coauthors and
title words of authors across conferences and journals tend not to overlap
much. This study corroborates findings of previous studies on this topic from a
distinctive perspective and suggests that conference authorship in CS calls for
more special attention from scholars and administrators outside CS who have
focused on journal publications to mine authorship data and evaluate scholarly
performance.
"
2229,"Team assembly mechanisms and the knowledge produced in the Mexico's
  National Institute of Geriatrics: a network analysis and agent-based
  modelling approach","  Mexico's National Institute of Geriatrics (INGER) is the national research
center of reference for matters related to human aging. INGER scientists
perform basic, clinical and demographic research which may imply different
scientific cultures working together in the same specialized institution. In
this paper, by a combination of text mining, co-authorship network analysis and
agent-based modeling we analyzed and modeled the team assembly practices and
the structure of the knowledge produced by scientists from INGER. Our results
showed a weak connection between basic and clinical research, and the emergence
of a highly connected academic leadership. Importantly, basic and
clinical-demographic researchers exhibited different team assembly strategies:
Basic researchers tended to form larger teams mainly with external
collaborators while clinical and demographic researchers formed smaller teams
that very often incorporated internal (INGER) collaborators. We showed how
these two different ways to form research teams impacted the organization of
knowledge produced at INGER. Following these observations, we modeled, via
agent-based modeling, the coexistence of different scientific cultures (basic
and clinical research) exhibiting different team assembly strategies in the
same institution. Our agent model successfully reproduced the current situation
of INGER. Moreover, by modifying the values of homophily we obtain alternative
scenarios in which multidisciplinary and interdisciplinary research could be
done.
"
2230,GOTO Rankings Considered Helpful,"  Rankings are a fact of life. Whether or not one likes them, they exist and
are influential. Within academia, and in computer science in particular,
rankings not only capture our attention but also widely influence people who
have a limited understanding of computing science research, including
prospective students, university administrators, and policy-makers. In short,
rankings matter. This position paper advocates for the adoption of ""GOTO
rankings"": rankings that use Good data, are Open, Transparent, and Objective,
and the rejection of rankings that do not meet these criteria.
"
2231,The Historical Significance of Textual Distances,"  Measuring similarity is a basic task in information retrieval, and now often
a building-block for more complex arguments about cultural change. But do
measures of textual similarity and distance really correspond to evidence about
cultural proximity and differentiation? To explore that question empirically,
this paper compares textual and social measures of the similarities between
genres of English-language fiction. Existing measures of textual similarity
(cosine similarity on tf-idf vectors or topic vectors) are also compared to new
strategies that use supervised learning to anchor textual measurement in a
social context.
"
2232,"Automation of the Export Data from Open Journal Systems to the Russian
  Science Citation Index","  It is shown that the calculation of scientometric indicators of the scientist
and also the scientific journal continues to be an actual problem nowadays. It
is revealed that the leading scientometric databases have the capabilities of
automated metadata collection from the scientific journal website by the use of
specialized electronic document management systems, in particular Open Journal
Systems. It is established that Open Journal Systems successfully exports
metadata about an article from scientific journals to scientometric databases
Scopus, Web of Science and Google Scholar. However, there is no standard method
of export from Open Journal Systems to such scientometric databases as the
Russian Science Citation Index and Index Copernicus, which determined the need
for research. The aim of the study is to develop the plug-in to the Open
Journal Systems for the export of data from this system to scientometric
database Russian Science Citation Index. As a result of the study, an
infological model for exporting metadata from Open Journal Systems to the
Russian Science Citation Index was proposed. The SirenExpo plug-in was
developed to export data from Open Journal Systems to the Russian Science
Citation Index by the use of the Articulus release preparation system.
"
2233,"An approach based on the geometric mean of basic quantitative and
  qualitative bibliometric indicators to evaluate and analyse the research
  performance of countries and institutions","  We present a straightforward procedure to evaluate the scientific
contribution of territories and institutions that combines the size-dependent
geometric mean, Q, of the number of research documents (N) and citations (C),
and a scale-free measure of quality, q=C/N. We introduce a Global Research
Output (GRO-index) as the geometric mean of Q and q. We show that the GRO-index
correlates with the h-index, but appears to be more strongly correlated with
other well known, widely used bibliometric indicators. We also compute relative
GRO-indexes (GROr) associated with the scientific production within research
fields. We note that although total sums of GROr values are larger than the
GRO-index, due to the non-linearity in the computation of the geometric means,
both counts are nevertheless highly correlated. That enables us to make useful
comparative analyses among territories and institutions. Furthermore, to
identify strengths and weaknesses of a given country or institution, we compute
a Relative Research Output count (RROr-index) to tackle variations of the C/N
ratio across research fields. Moreover, by using a wealth-index also based on
quantitative and qualitative variables, we show that the GRO and RRO indexes
are highly correlated with the wealth of the countries and the states of the
USA. Given the simplicity of the procedures introduced in this paper and the
fact that their results are easily understandable by non-specialists, we
believe they could become as useful for the assessment of the research output
of countries and institutions as the impact factor is for journals or the
h-index for individuals.
"
2234,"The USA is an indisputable world leader in medical and biotechnological
  research","  A country's research success can be assessed from the power law function that
links country and world rank numbers when publications are ordered by their
number of citations; a similar function describes the distribution of country
papers in world percentiles. These functions allow calculating the ep index and
the probability of publishing highly cited papers, which measure the efficiency
of the research system and the ability of achieving important discoveries or
scientific breakthroughs, respectively. The aim of this paper was to use these
metrics and other parameters derived from the percentile-based power law
function to investigate research success in the USA, the EU, and other
countries in hot medical, biochemical, and biotechnological topics. The results
show that, in the investigated fields, the USA is scientifically ahead of all
countries and that its research is likely to produce approximately 80% of the
important global breakthroughs in the research topics investigated in this
study. EU research has maintained a constant weak position with reference to
USA research over the last 30 years.
"
2235,Case for the double-blind peer review,"  Peer review is a process designed to produce a fair assessment of research
quality before the publication of scholarly work in a journal. Demographics,
nepotism, and seniority have been all shown to affect reviewer behavior
suggesting the most common, single-blind review method (or the less common open
review method) might be biased. A survey of current research indicates that
double-blind review offers a solution to many biases stemming from author's
gender, seniority, or location without imposing any significant downsides.
"
2236,A diachronic study of historiography,"  The humanities are often characterized by sociologists as having a low mutual
dependence among scholars and high task uncertainty. According to Fuchs' theory
of scientific change, this leads over time to intellectual and social
fragmentation, as new scholarship accumulates in the absence of shared unifying
theories. We consider here a set of specialisms in the discipline of history
and measure the connectivity properties of their bibliographic coupling
networks over time, in order to assess whether fragmentation is indeed
occurring. We construct networks using both reference overlap and textual
similarity. It is shown that the connectivity of reference overlap networks is
gradually and steadily declining over time, whilst that of textual similarity
networks is stable. Author bibliographic coupling networks also show signs of a
decline in connectivity, in the absence of an increasing propensity for
collaborations. We speculate that, despite the gradual weakening of ties among
historians as mapped by references, new scholarship might be continually
integrated through shared vocabularies and narratives. This would support our
belief that citations are but one kind of bibliometric data to consider ---
perhaps even of secondary importance --- when studying the humanities, while
text should play a more prominent role.
"
2237,"Social network aided plagiarism detection: Social network aided
  plagiarism detection","  The prevalence of different kinds of electronic devices and the volume of
content on the Web have increased the amount of plagiarism, which is considered
an unethical act. If we want to be efficient in the detection and prevention of
these acts, we have to improve today's methods of discovering plagiarism. The
paper presents a research study where a framework for the improved detection of
plagiarism is proposed. The framework focuses on the integration of social
network information, information from the Web, and an advanced semantically
enriched visualization of information about authors and documents that enables
the exploration of obtained data by seeking of advanced patterns of plagiarism.
To support the proposed framework, a special software tool was also developed.
The statistical evaluation confirmed that the employment of social network
analysis and advanced visualization techniques led to improvements in the
confirmation and investigation stages of the plagiarism detection process,
thereby enhancing the overall efficiency of the plagiarism detection process.
"
2238,Data Mining in Scientometrics: usage analysis for academic publications,"  We perform a statistical analysis of scientific-publication data with a goal
to provide quantitative analysis of scientific process. Such an investigation
belongs to the newly established field of scientometrics: a branch of the
general science of science that covers all quantitative methods to analyze
science and research process. As a case study we consider download and citation
statistics of the journal `Europhysics Letters' (EPL), as Europe's flagship
letters journal of broad interest to the physics community. While citations are
usually considered as an indicator of academic impact, downloads reflect rather
the level of attractiveness or popularity of a publication. We discuss
peculiarities of both processes and correlations between them.
"
2239,"Do altmetrics assess societal impact in a comparable way to case
  studies? An empirical test of the convergent validity of altmetrics based on
  data from the UK Research Excellence Framework (REF)","  Altmetrics have been proposed as a way to assess the societal impact of
research. Although altmetrics are already in use as impact or attention metrics
in different contexts, it is still not clear whether they really capture or
reflect societal impact. This study is based on altmetrics, citation counts,
research output and case study data from the UK Research Excellence Framework
(REF), and peers' REF assessments of research output and societal impact. We
investigated the convergent validity of altmetrics by using two REF datasets:
publications submitted as research output (PRO) to the REF and publications
referenced in case studies (PCS). Case studies, which are intended to
demonstrate societal impact, should cite the most relevant research papers. We
used the MHq' indicator for assessing impact - an indicator which has been
introduced for count data with many zeros. The results of the first part of the
analysis show that news media as well as mentions on Facebook, in blogs, in
Wikipedia, and in policy-related documents have higher MHq' values for PCS than
for PRO. Thus, the altmetric indicators seem to have convergent validity for
these data. In the second part of the analysis, altmetrics have been correlated
with REF reviewers' average scores on PCS. The negative or close to zero
correlations question the convergent validity of altmetrics in that context. We
suggest that they may capture a different aspect of societal impact (which can
be called unknown attention) to that seen by reviewers (who are interested in
the causal link between research and action in society).
"
2240,"Interdisciplinarity as Diversity in Citation Patterns among Journals:
  Rao-Stirling Diversity, Relative Variety, and the Gini coefficient","  Questions of definition and measurement continue to constrain a consensus on
the measurement of interdisciplinarity. Using Rao-Stirling (RS) Diversity
produces sometimes anomalous results. We argue that these unexpected outcomes
can be related to the use of ""dual-concept diversity"" which combines ""variety""
and ""balance"" in the definitions (ex ante). We propose to modify RS Diversity
into a new indicator (DIV) which operationalizes variety, balance, and
disparity independently and then combines them ex post. ""Balance"" can be
measured using the Gini coefficient. We apply DIV to the aggregated citation
patterns of 11,487 journals covered by the Journal Citation Reports 2016 of the
Science Citation Index and the Social Sciences Citation Index as an empirical
domain and, in more detail, to the citation patterns of 85 journals assigned to
the Web-of-Science category ""information science & library science"" in both the
cited and citing directions. We compare the results of the indicators and show
that DIV provides improved results in terms of distinguishing between
interdisciplinary knowledge integration (citing) versus knowledge diffusion
(cited). The new diversity indicator and RS diversity measure different
features. A routine for the measurement of the various operationalizations of
diversity (in any data matrix) is made available online.
"
2241,Disentangling Gold Open Access,"  This chapter focuses on the analysis of current publication trends in gold
Open Access (OA). The purpose of the chapter is to develop a full understanding
on country patterns, OA journals characteristics and citation differences
between gold OA and non-gold OA publications. For this, we will first review
current literature regarding Open Access and its relation with its so-called
citation advantage. Starting with a chronological perspective we will describe
its development, how different countries are promoting OA publishing, and its
effects on the journal publishing industry. We will deepen the analysis by
investigating the research output produced by different units of analysis.
First, we will focus on the production of countries with a special emphasis on
citation and disciplinary differences. A point of interest will be
identification of national idiosyncrasies and the relation between OA
publication and research of local interest. This will lead to our second unit
of analysis, OA journals indexed in Web of Science. Here we will deepen on
journals characteristics and publisher types to clearly identify factors which
may affect citation differences between OA and traditional journals which may
not necessarily be derived from the OA factor. Gold OA publishing is being
encouraged in many countries as opposed to Green OA. This chapter aims at fully
understanding how it affects researchers' publication patterns and whether it
ensures an alleged citation advantage as opposed to non-gold OA publications.
"
2242,"How to identify the roots of broad research topics and fields? The
  introduction of RPYS sampling using the example of climate change research","  Since the introduction of the reference publication year spectroscopy (RPYS)
method and the corresponding program CRExplorer, many studies have been
published revealing the historical roots of topics, fields, and researchers.
The application of the method was restricted up to now by the available memory
of the computer used for running the CRExplorer. Thus, many users could not
perform RPYS for broader research fields or topics. In this study, we present
various sampling methods to solve this problem: random, systematic, and cluster
sampling. We introduce the script language of the CRExplorer which can be used
to draw many samples from the population dataset. Based on a large dataset of
publications from climate change research, we compare RPYS results using
population data with RPYS results using different sampling techniques. From our
comparison with the full RPYS (population spectrogram), we conclude that the
cluster sampling performs worst and the systematic sampling performs best. The
random sampling also performs very well but not as well as the systematic
sampling. The study therefore demonstrates the fruitfulness of the sampling
approach for applying RPYS.
"
2243,A network-based citation indicator of scientific performance,"  Scientists are embedded in social and information networks that influence and
are influenced by the quality of their scientific work, its impact, and the
recognition they receive. Here we quantify the systematic relationship between
a scientist's position in the network of scientific collaborations and the
citations they receive. As expected, we find that authors closer to others in
this network are, on average, more highly cited than those further away from
others. We construct a novel indicator, the s-index, that explicitly captures
performance linked to network position along two complimentary dimensions:
performance expected due to network position and performance relative to this
position. The basis of our approach is to represent an author's network
position through their distribution of distances to other authors. The s-index
then ranks (1) the citation potential of an individual's network position
relative to all other authors, and (2) the citations they accrue relative to
authors that have a comparable network position. Characterizing scientists
through these two complimentary dimensions can be used to make more informed
evaluations in a networked environment. For example, it can identify
individuals that play an important role in diffusing scientific ideas. It also
sheds a new light on central debates in the Science of Science, namely the
impact of author teams and comparisons of impact across scientific fields.
"
2244,"Heuristics as conceptual lens for understanding and studying the usage
  of bibliometrics in research evaluation","  While bibliometrics are widely used for research evaluation purposes, a
common theoretical framework for conceptually understanding, empirically
studying, and effectively teaching its usage is lacking. In this paper, we
outline such a framework: the fast-and-frugal heuristics research program,
proposed originally in the context of the cognitive and decision sciences,
lends itself particularly well for understanding and investigating the usage of
bibliometrics in research evaluations. Such evaluations represent judgments
under uncertainty in which typically not all possible options, their
consequences, and those consequences' probabilities of occurring may be known.
In these situations of incomplete information, candidate descriptive and
prescriptive models of human behavior are heuristics. Heuristics are simple
strategies that, by exploiting the structure of environments, can aid people to
make smart decisions. Relying on heuristics does not mean trading off accuracy
against effort: while reducing complexity, heuristics can yield better
decisions than more information-greedy procedures in many decision
environments. The prescriptive power of heuristics is documented in a
cross-disciplinary literature, cutting across medicine, crime, business,
sports, and other domains. We outline the fast-and-frugal heuristics research
program, provide examples of past empirical work on heuristics outside the
field of bibliometrics, explain why heuristics may be especially suitable for
studying the usage of bibliometrics, and propose a corresponding conceptual
framework.
"
2245,"Understanding the Twitter Usage of Humanities and Social Sciences
  Academic Journals","  Scholarly communication has the scope to transcend the limitations of the
physical world through social media extended coverage and shortened information
paths. Accordingly, publishers have created profiles for their journals in
Twitter to promote their publications and to initiate discussions with public.
This paper investigates the Twitter presence of humanities and social sciences
(HSS) journal titles obtained from mainstream citation indices, by analysing
the interaction and communication patterns. This study utilizes webometric data
collection, descriptive analysis, and social network analysis. Findings
indicate that the presence of HSS journals in Twitter across disciplines is not
yet substantial. Sharing of general websites appears to be the key activity
performed by HSS journals in Twitter. Among them, web content from news portals
and magazines are highly disseminated. Sharing of research articles and
retweeting was not majorly observed. Inter-journal communication is apparent
within the same citation index, but it is very minimal with journals from the
other index. However, there seems to be an effort to broaden communication
beyond the research community, reaching out to connect with the public.
"
2246,"MIDV-500: A Dataset for Identity Documents Analysis and Recognition on
  Mobile Devices in Video Stream","  A lot of research has been devoted to identity documents analysis and
recognition on mobile devices. However, no publicly available datasets designed
for this particular problem currently exist. There are a few datasets which are
useful for associated subtasks but in order to facilitate a more comprehensive
scientific and technical approach to identity document recognition more
specialized datasets are required. In this paper we present a Mobile Identity
Document Video dataset (MIDV-500) consisting of 500 video clips for 50
different identity document types with ground truth which allows to perform
research in a wide scope of document analysis problems. The paper presents
characteristics of the dataset and evaluation results for existing methods of
face detection, text line recognition, and document fields data extraction.
Since an important feature of identity documents is their sensitiveness as they
contain personal data, all source document images used in MIDV-500 are either
in public domain or distributed under public copyright licenses.
  The main goal of this paper is to present a dataset. However, in addition and
as a baseline, we present evaluation results for existing methods for face
detection, text line recognition, and document data extraction, using the
presented dataset.
  (The dataset is available for download at ftp://smartengines.com/midv-500/.)
"
2247,"Reviewing, indicating, and counting books for modern research evaluation
  systems","  In this chapter, we focus on the specialists who have helped to improve the
conditions for book assessments in research evaluation exercises, with
empirically based data and insights supporting their greater integration. Our
review highlights the research carried out by four types of expert communities,
referred to as the monitors, the subject classifiers, the indexers and the
indicator constructionists. Many challenges lie ahead for scholars affiliated
with these communities, particularly the latter three. By acknowledging their
unique, yet interrelated roles, we show where the greatest potential is for
both quantitative and qualitative indicator advancements in book-inclusive
evaluation systems.
"
2248,"The altmetric performance of publications authored by Brazilian
  researchers: analysis of CNPq productivity scholarship holders","  The present work seeks to analyse the altmetric performance of Brazilian
publications authored by researchers who are productivity scholarship holders
(PQ) of the National Council of Scientific and Technological Development
(CNPq). It was considered, within the scope of this research, the PQs in
activity in October, 2017 (n = 14.609). The scientific production registered on
Lattes was collected via GetLattesData and filtered by articles from academic
journals published between 2016 and October 2017 that hold the Digital Object
Identifier (n = 99064). The online attention data are analysed according to
their distribution by density and variation; language of the publication and
field of knowledge; and by average performance of the type of source that has
provided its altmetric values. The density evidences the long tail behavior of
the variable, with most part of the articles with altmetrics score = 0, while
few articles have a high index. The average of the online attention indicates a
better performance of articles written in English and belonging to the Health
and Biological Sciences field of knowledge. As for the sources, there was a
good performance from Mendeley, followed by Twitter and a low coverage from
Facebook
"
2249,$h_{PI}$: The Citation Index for Principal Investigators,"  A new citation index $h_{PI}$ for principal investigators (PIs) is defined in
analogy to Hirsch's index $h$, but based on renormalized citations of a PI's
papers. To this end, the authors of a paper are divided into two groups: PIs
and non-PIs. A PI is defined as an assistant, associate or full professor at a
university who supervises an individual research program. The citations for
each paper of a certain PI are then divided by the number of PIs among the
authors of that paper. Data are presented for a sample of 48 PIs who are senior
faculty members of physics and physics-related engineering departments at a
private research-oriented U.S. university, using the ISI Web of Science
citations database. The main result is that individual rankings based on $h$
and $h_{PI}$ differ substantially. Also, to a good approximation across the
sample of 48 PIs, one finds that $h_{PI} = h \,/ \sqrt{<N_{PI}>}$ where
<$N_{PI}$> is the average number of principal investigators on the papers of a
particular PI. In addition, $h_{PI} = \frac{1}{2} \sqrt{C_{tot}\,/<N_{PI}>}$,
where $C_{tot}$ is the total number of citations. Approaches to broadening the
scope of $h$ or $h_{PI}$ are discussed briefly, and a new metric for highly
cited papers called $h_x$ is introduced which represents the average number of
citations exceeding the minimum of $h^2$ in the $h$-core.
"
2250,A data-supported history of bioinformatics tools,"  Since the advent of next-generation sequencing in the early 2000s, the volume
of bioinformatics software tools and databases has exploded and continues to
grow rapidly. Documenting this evolution on a global and time-dependent scale
is a challenging task, limited by the scarcity of comprehensive tool
repositories. We collected data from over ~23,000 references classified in the
OMICtools database, spanning the last 26 years of bioinformatics to present a
data-supported snapshot of bioinformatics software tool evolution and the
current status, to shed light on future directions and opportunities in this
field. The present review explores new aspects of computational biology,
including country partnerships, trends in technologies and area of development,
research and development (R&D) investments and coding languages. This is the
most comprehensive systematic overview of the field to date and provides the
community with insights and knowledge on the direction of the development and
evolution of bioinformatics software tools, highlighting the increasing
complexity of analysis.
"
2251,Unveiling Scholarly Communities over Knowledge Graphs,"  Knowledge graphs represent the meaning of properties of real-world entities
and relationships among them in a natural way. Exploiting semantics encoded in
knowledge graphs enables the implementation of knowledge-driven tasks such as
semantic retrieval, query processing, and question answering, as well as
solutions to knowledge discovery tasks including pattern discovery and link
prediction. In this paper, we tackle the problem of knowledge discovery in
scholarly knowledge graphs, i.e., graphs that integrate scholarly data, and
present Korona, a knowledge-driven framework able to unveil scholarly
communities for the prediction of scholarly networks. Korona implements a graph
partition approach and relies on semantic similarity measures to determine
relatedness between scholarly entities. As a proof of concept, we built a
scholarly knowledge graph with data from researchers, conferences, and papers
of the Semantic Web area, and apply Korona to uncover co-authorship networks.
Results observed from our empirical evaluation suggest that exploiting
semantics in scholarly knowledge graphs enables the identification of
previously unknown relations between researchers. By extending the ontology,
these observations can be generalized to other scholarly entities, e.g.,
articles or institutions, for the prediction of other scholarly patterns, e.g.,
co-citations or academic collaboration.
"
2252,RARD II: The 94 Million Related-Article Recommendation Dataset,"  The main contribution of this paper is to introduce and describe a new
recommender-systems dataset (RARD II). It is based on data from Mr. DLib, a
recommender-system as-a-service in the digital library and
reference-management-software domain. As such, RARD II complements datasets
from other domains such as books, movies, and music. The dataset encompasses
94m recommendations, delivered in the two years from September 2016 to
September 2018. The dataset covers an item-space of 24m unique items. RARD II
provides a range of rich recommendation data, beyond conventional ratings. For
example, in addition to the usual (implicit) ratings matrices, RARD II includes
the original recommendation logs, which provide a unique insight into many
aspects of the algorithms that generated the recommendations. The logs enable
researchers to conduct various analyses about a real-world recommender system.
This includes the evaluation of meta-learning approaches for predicting
algorithm performance. In this paper, we summarise the key features of this
dataset release, describe how it was generated and discuss some of its unique
features. Compared to its predecessor RARD, RARD II contains 64% more
recommendations, 187% more features (algorithms, parameters, and statistics),
50% more clicks, 140% more documents, and one additional service partner
(JabRef).
"
2253,"Probability and expected frequency of breakthroughs - a robust method of
  research assessment based on the double rank property of citation
  distributions","  In research policy, effective measures that lead to improvements in the
generation of knowledge must be based on reliable methods of research
assessment, but for many countries and institutions this is not the case.
Publication and citation analyses can be used to estimate the part played by
countries and institutions in the global progress of knowledge, but a concrete
method of estimation is far from evident. The challenge arises because
publications that report real progress of knowledge form an extremely low
proportion of all publications; in most countries and institutions such
contributions appear less than once per year. One way to overcome this
difficulty is to calculate probabilities instead of counting the rare events on
which scientific progress is based. This study reviews and summarizes several
recent publications, and adds new results that demonstrate that the citation
distribution of normal publications allows the probability of the infrequent
events that support the progress of knowledge to be calculated.
"
2254,"Online Evaluations for Everyone: Mr. DLib's Living Lab for Scholarly
  Recommendations","  We introduce the first 'living lab' for scholarly recommender systems. This
lab allows recommender-system researchers to conduct online evaluations of
their novel algorithms for scholarly recommendations, i.e., recommendations for
research papers, citations, conferences, research grants, etc. Recommendations
are delivered through the living lab's API to platforms such as reference
management software and digital libraries. The living lab is built on top of
the recommender-system as-a-service Mr. DLib. Current partners are the
reference management software JabRef and the CORE research team. We present the
architecture of Mr. DLib's living lab as well as usage statistics on the first
sixteen months of operating it. During this time, 1,826,643 recommendations
were delivered with an average click-through rate of 0.21%.
"
2255,Universalizing science: alternative indices to direct research,"  Measurement is a complicated but very necessary task. Many indices have been
created in an effort to define the quality of knowledge produced but they have
attracted strong criticism, having become synonymous with individualism,
competition and mere productivity and, furthermore, they fail to head science
towards addressing local demands or towards producing international knowledge
by means of collaboration. Institutions, countries, publishers, governments and
authors have a latent need to create quality and productivity indices because
they can serve as filters that influence far-reaching decision making and even
decisions on the professional promotion of university teachers. Even so, in the
present-day context, the very creators of those indices admit that they were
not designed for that purpose, given that different research areas, the age of
the researcher, the country and the language spoken all have an influence on
the index calculations. Accordingly, this research sets out three indices
designed to head science towards its universal objective by valuing
collaboration and the dissemination of knowledge. It is hoped that the proposed
indices may provoke new discussions and the proposal of new, more assertive
indicators for the analysis of scientific research quality.
"
2256,"Identification of multidisciplinary research based upon dissimilarity
  analysis of journals included in reference lists of Wageningen University &
  Research articles","  This paper discusses the identification of journal articles that probably
report on multidisciplinary research. Identification of these articles may be
important for strategic purposes for the institution where the research is
performed or for the evaluation of researchers or groups. In order to identify
possibly multidisciplinary research, this paper describes an analysis on the
journals from which articles have been cited in the journal articles published
by Wageningen University & Research (WUR) staff in 2006-2015. The journals with
cited articles are inventoried from the reference lists of the WUR journal
articles. For each WUR article a mean dissimilarity is calculated between the
journal in which it has been published and the journals that contain the cited
articles. Dissimilarities are derived from a large matrix with similarity
values between journals, calculated from co-citations to these journals from
the same WUR articles published in 2006-2015. For 21,191 WUR articles published
in 2,535 journals mean dissimilarities have been calculated. For WUR articles
with high mean dissimilarities this paper shows that they often are published
in multidisciplinary journals. WUR articles with high mean dissimilarities also
are found in non-multidisciplinary (research field specific) journals. For
these articles (with high mean dissimilarities) this paper shows that citations
are often made to more various research fields than for articles with lower
mean dissimilarities. The mean dissimilarities, calculated per WUR article,
also can be aggregated for the journals in which they have been published. This
results in a listing of journals than can be ordered on the mean dissimilarity
of the WUR articles that have been published in them. The analysis described in
this paper shows that journals for which a high mean dissimilarity is
calculated tend to have a multidisciplinary scope.
"
2257,"Which US and European Higher Education Institutions are visible in
  ResearchGate and what affects their RG Score?","  While ResearchGate has become the most popular academic social networking
site in terms of regular users, not all institutions have joined and the scores
it assigns to academics and institutions are controversial. This paper assesses
the presence in ResearchGate of higher education institutions in Europe and the
US in 2017, and the extent to which institutional ResearchGate Scores reflect
institutional academic impact. Most of the 2258 European and 4355 US higher
educational institutions included in the sample had an institutional
ResearchGate profile, with near universal coverage for PhD-awarding
institutions found in the Web of Science (WoS). For non-PhD awarding
institutions that did not publish, size (number of staff members) was most
associated with presence in ResearchGate. For PhD-awarding institutions in WoS,
presence in RG was strongly related to the number of WoS publications. In
conclusion, a) institutional RG scores reflect research volume more than
visibility and b) this indicator is highly correlated to the number of WoS
publications. Hence, the value of RG Scores for institutional comparisons is
limited.
"
2258,"CSIndexbr: Exploring the Brazilian Scientific Production in Computer
  Science","  CSIndexbr is a web-based system that provides meaningful,open,and transparent
data about Brazilian scientific production in Computer Science. Currently, the
system collects full research papers published in the main track of selected
conferences. The papers are retrieved from DBLP. In this article, we describe
the main features and resources provided by CSIndexbr. We also comment on how
other researchers can use the data provided by the system to analyze the
Brazilian production in Computer Science.
"
2259,"Missing author address information in Web of Science-An explorative
  study","  Bibliometric analysis is increasingly used to evaluate and compare research
performance across geographical regions. However, the problem of missing
information from author addresses has not attracted sufficient attention from
scholars and practitioners. This study probes the missing data problem in the
three core journal citation databases of Web of Science (WoS). Our findings
reveal that from 1900 to 2015 over one-fifth of the publications indexed in WoS
have completely missing information from the address field. The magnitude of
the problem varies greatly among time periods, citation databases, document
types, and publishing languages. The problem is especially serious for research
in the sciences and social sciences published before the early 1970s and
remains significant for recent publications in the arts and humanities. Further
examinations suggest that many records with completely missing address
information do not represent scholarly research. Full-text scanning of a random
sample reveals that about 40% of the articles have some address information
that is not indexed in WoS. This study also finds that the problem of partially
missing address information for U.S. research has diminished dramatically since
1998. The paper ends by providing some discussion and tentative remedies.
"
2260,"On Good and Bad Intentions behind Anomalous Citation Patterns among
  Journals in Computer Sciences","  Scientific journals are an important choice of publication venue for most
authors. Publishing in prestigious journal plays a decisive role for authors in
hiring and promotions. In last decade, citation pressure has become intact for
all scientific entities more than ever before. Unethical publication practices
has started to manipulate widely used performance metric such as ""impact
factor"" for journals and citation based indices for authors. This threatens the
integrity of scientific quality and takes away deserved credit of legitimate
authors and their authentic publications.
  In this paper we extract all possible anomalous citation patterns between
journals from a Computer Science bibliographic dataset which contains more than
2,500 journals. Apart from excessive self-citations, we mostly focus on finding
several patterns between two or more journals such as bi-directional mutual
citations, chains, triangles, mesh, cartel relationships. On a macroscopic
scale, the motivation is to understand the nature of these patterns by modeling
how journals mutually interact through citations. On microscopic level, we
differentiate between possible intentions (good or bad) behind such patterns.
We see whether such patterns prevail for long period or during any specific
time duration. For abnormal citation behavior, we study the nature of sudden
inflation in impact factor of journals on a time basis which may occur due to
addition of irrelevant and superfluous citations in such closed pattern
interaction. We also study possible influences such as abrupt increase in paper
count due to the presence of self-referential papers or duplicate manuscripts,
author self-citation, author co-authorship network, author-editor network,
publication houses etc. The entire study is done to question the reliability of
existing bibliometrics, and hence, it is an urgent need to curtail their usage
or redefine them.
"
2261,"Reassembling the English novel, 1789-1919","  The absence of an exhaustive bibliography of novels published in the British
Isles and Ireland during the 19th century blocks several lines of research in
sociologically-inclined literary history and book history. Without a detailed
account of novelistic production, it is difficult to characterize, for example,
the population of individuals who pursued careers as novelists. This paper
contributes to efforts to develop such an account by estimating yearly rates of
new novel publication in the British Isles and Ireland between 1789 and 1919.
This period witnessed, in aggregate, the publication of between 40,000 and
63,000 previously unpublished novels. The number of new novels published each
year counts as essential information for researchers interested in
understanding the development of the text industry between 1789 and 1919.
"
2262,"The impact of imbalanced training data on machine learning for author
  name disambiguation","  In supervised machine learning for author name disambiguation, negative
training data are often dominantly larger than positive training data. This
paper examines how the ratios of negative to positive training data can affect
the performance of machine learning algorithms to disambiguate author names in
bibliographic records. On multiple labeled datasets, three classifiers -
Logistic Regression, Na\""ive Bayes, and Random Forest - are trained through
representative features such as coauthor names, and title words extracted from
the same training data but with various positive-negative training data ratios.
Results show that increasing negative training data can improve disambiguation
performance but with a few percent of performance gains and sometimes degrade
it. Logistic Regression and Na\""ive Bayes learn optimal disambiguation models
even with a base ratio (1:1) of positive and negative training data. Also, the
performance improvement by Random Forest tends to quickly saturate roughly
after 1:10 ~ 1:15. These findings imply that contrary to the common practice
using all training data, name disambiguation algorithms can be trained using
part of negative training data without degrading much disambiguation
performance while increasing computational efficiency. This study calls for
more attention from author name disambiguation scholars to methods for machine
learning from imbalanced data.
"
2263,ReproServer: Making Reproducibility Easier and Less Intensive,"  Reproducibility in the computational sciences has been stymied because of the
complex and rapidly changing computational environments in which modern
research takes place. While many will espouse reproducibility as a value, the
challenge of making it happen (both for themselves and testing the
reproducibility of others' work) often outweigh the benefits. There have been a
few reproducibility solutions designed and implemented by the community. In
particular, the authors are contributors to ReproZip, a tool to enable
computational reproducibility by tracing and bundling together research in the
environment in which it takes place (e.g. one's computer or server). In this
white paper, we introduce a tool for unpacking ReproZip bundles in the cloud,
ReproServer. ReproServer takes an uploaded ReproZip bundle (.rpz file) or a
link to a ReproZip bundle, and users can then unpack them in the cloud via
their browser, allowing them to reproduce colleagues' work without having to
install anything locally. This will help lower the barrier to reproducing
others' work, which will aid reviewers in verifying the claims made in papers
and reusing previously published research.
"
2264,"Use of ""Web Map Image"" and copyright act","  In this paper, we reviewed the notes on using Web map image provided by Web
map service, from the viewpoint of copyright act. The copyright act aims to
contribute to creation of culture by protecting the rights of authors and
others, and promoting fair exploitation of cultural products. Therefore,
everyone can use copyrighted materials to the extent of the copyright
limitation based on copyright act. The Web map image, including maps, aerial
photo and satellite image, are one of copyrighted materials, so it can be used
within the limits of copyright. However, the available range of Web map image
under the copyright act is not wide. In addition, it is pointed out that the
copyright act has not been able to follow the progress of digitalization of
copyrighted materials. It is expected to revise the copyright act corresponding
to digitalization of copyrighted work.
"
2265,"Can Microsoft Academic help to assess the citation impact of academic
  books?","  Despite recent evidence that Microsoft Academic is an extensive source of
citation counts for journal articles, it is not known if the same is true for
academic books. This paper fills this gap by comparing citations to 16,463
books from 2013-2016 in the Book Citation Index (BKCI) against automatically
extracted citations from Microsoft Academic and Google Books in 17 fields.
About 60% of the BKCI books had records in Microsoft Academic, varying by year
and field. Citation counts from Microsoft Academic were 1.5 to 3.6 times higher
than from BKCI in nine subject areas across all years for books indexed by
both. Microsoft Academic found more citations than BKCI because it indexes more
scholarly publications and combines citations to different editions and
chapters. In contrast, BKCI only found more citations than Microsoft Academic
for books in three fields from 2013-2014. Microsoft Academic also found more
citations than Google Books in six fields for all years. Thus, Microsoft
Academic may be a useful source for the impact assessment of books when
comprehensive coverage is not essential.
"
2266,"Statistics on Open Access Books Available through the Directory of Open
  Access Books","  Open Access (OA) books available through the Directory of Open Access Books
(DOAB) are investigated and the number of titles, the distribution of subjects,
languages, publishers, publication years, licensing patterns, etc., are
clarified. Their chronological changes are also shown. The sample comprised
10,866 OA books, which were available through the DOAB as of February 24, 2018.
The results show that OA books are increasing in number at an accelerating
rate. As for distribution of subjects, Social Sciences (""H"" in the Library of
Congress Classification [LCC] codes), Science (""Q"" in LCC) and World History
and History of Europe, Asia, Africa, Australia, New Zealand, etc. (""D"" in LCC)
are the most popular. As for languages, English, French, and German are the
most popular. As for publishers, Frontiers Media SA, Presses universitaires de
Rennes, and ANU Press are the most popular. Many books are newly published
ones, but older books, published in or before 1999, also began to be available
recently. As for the licensing patterns, ""CC by-nc-nd"" and ""CC by"" are the most
popular. Considering these tendencies, libraries should begin to utilize OA
books.
"
2267,"Ethnographie de la structuration d'un corpus collectif de messages de
  soutien social en ligne","  In this paper, we propose a study of progressive development of the structure
of corpus collected starting from the discussion forums. These are the first
results from an ethnographic description of constitution and analysis practices
of a corpus located within an interdisciplinary project. At first, we describe
the personal digital documents folder of a researcher, relative to his corpus
analysis of interactions of social support online. Next, we examine the
constitution of a shared corpus, result of the pooling of individual corpora,
small size, produced by researchers participating in the project, each
individual corpus reflecting his producer's perspectives of research.
"
2268,"Do females create higher impact research? Scopus citations and Mendeley
  readers for articles from five countries","  There are known gender imbalances in participation in scientific fields, from
female dominance of nursing to male dominance of mathematics. It is not clear
whether there is also a citation imbalance, with some claiming that
male-authored research tends to be more cited. No previous study has assessed
gender differences in the readers of academic research on a large scale,
however. In response, this article assesses whether there are gender
differences in the average citations and/or Mendeley readers of academic
publications. Field normalised logged Scopus citations and Mendeley readers
from mid-2018 for articles published in 2014 were investigated for articles
with first authors from India, Spain, Turkey, the UK and the USA in up to 251
fields with at least 50 male and female authors. Although female-authored
research is less cited in Turkey (-4.0%) and India (-3.6%), it is marginally
more cited in Spain (0.4%), the UK (0.4%), and the USA (0.2%). Female-authored
research has fewer Mendeley readers in India (-1.1%) but more in Spain (1.4%),
Turkey (1.1%), the UK (2.7%) and the USA (3.0%). Thus, whilst there may be
little practical gender difference in citation impact in countries with mature
science systems, the higher female readership impact suggests a wider audience
for female-authored research. The results show that the conclusions from a
gender analysis depend on the field normalisation method. A theoretically
informed decision must therefore be made about which normalisation to use. The
results also suggest that arithmetic mean-based field normalisation is
favourable to males.
"
2269,"Monitoring compliance with governmental and institutional open access
  policies across Spanish universities","  Universities and research centers in Spain are subject to a national open
access (OA) mandate and to their own OA institutional policies, if any, but
compliance with these requirements has not been fully monitored yet. We studied
the degree of OA archiving of publications of 28 universities within the period
2012-2014. Of these, 12 have an institutional OA mandate, 9 do not require but
request or encourage OA of scholarly outputs, and 7 do not have a formal OA
statement but are well known for their support of the OA movement. The
potential OA rate was calculated according to the publisher open access
policies indicated in Sherpa/Romeo directory. The universities showed an
asymmetric distribution of 1% to 63% of articles archived in repositories that
matched those indexed by the Web of Science in the same period, of which 1% to
35% were OA and the rest were closed access. For articles on work carried out
with public funding and subject to the Spanish Science law, the percentage was
similar or slightly higher. However, the analysis of potential OA showed that
the figure could have reached 80% in some cases. This means that the real
proportion of articles in OA is far below what it could potentially be.
"
2270,"Systematic analysis of agreement between metrics and peer review in the
  UK REF","  When performing a national research assessment, some countries rely on
citation metrics whereas others, such as the UK, primarily use peer review. In
the influential Metric Tide report, a low agreement between metrics and peer
review in the UK Research Excellence Framework (REF) was found. However,
earlier studies observed much higher agreement between metrics and peer review
in the REF and argued in favour of using metrics. This shows that there is
considerable ambiguity in the discussion on agreement between metrics and peer
review. We provide clarity in this discussion by considering four important
points: (1) the level of aggregation of the analysis; (2) the use of either a
size-dependent or a size-independent perspective; (3) the suitability of
different measures of agreement; and (4) the uncertainty in peer review. In the
context of the REF, we argue that agreement between metrics and peer review
should be assessed at the institutional level rather than at the publication
level. Both a size-dependent and a size-independent perspective are relevant in
the REF. The interpretation of correlations may be problematic and as an
alternative we therefore use measures of agreement that are based on the
absolute or relative differences between metrics and peer review. To get an
idea of the uncertainty in peer review, we rely on a model to bootstrap peer
review outcomes. We conclude that particularly in Physics, Clinical Medicine,
and Public Health, metrics agree quite well with peer review and may offer an
alternative to peer review.
"
2271,"Mapping the efficiency of international scientific collaboration between
  cities worldwide","  International scientific collaboration, a fundamental phenomenon of science,
has been studied from several perspectives for decades. In the spatial aspect
of science of science, cities have been considered by their publication output
or by their citation impact. In this visualization, we go beyond these
well-known approaches and map international scientific collaboration patterns
of the most prominent science hubs considering both the quantity and the impact
of papers produced in the collaboration. The analysis involves 245 cities and
the collaboration matrix contains a total number of 7,718 international
collaboration links. Results show that US-Europe co-publication links are more
efficient in terms of producing highly cited papers than those international
links that Asian cities have built in scientific collaboration.
"
2272,Methodology for identifying study sites in scientific corpus,"  The TERRE-ISTEX project aims at identifying the evolution of research working
relation to study areas, disciplinary crossings and concrete research methods
based on the heterogeneous digital content available in scientific corpora. The
project is divided into three main actions: (1) to identify the periods and
places which have been the subject of empirical studies, and which reflect the
publications resulting from the corpus analyzed, (2) to identify the thematics
addressed in these works and (3) to develop a web-based geographical
information retrieval tool (GIR). The first two actions involve approaches
combining Natural languages processing patterns with text mining methods. By
crossing the three dimensions (spatial, thematic and temporal) in a GIR engine,
it will be possible to understand what research has been carried out on which
territories and at what time. In the project, the experiments are carried out
on a heterogeneous corpus including electronic thesis and scientific articles
from the ISTEX digital libraries and the CIRAD research center.
"
2273,"Google Scholar, Web of Science, and Scopus: a systematic comparison of
  citations in 252 subject categories","  Despite citation counts from Google Scholar (GS), Web of Science (WoS), and
Scopus being widely consulted by researchers and sometimes used in research
evaluations, there is no recent or systematic evidence about the differences
between them. In response, this paper investigates 2,448,055 citations to 2,299
English-language highly-cited documents from 252 GS subject categories
published in 2006, comparing GS, the WoS Core Collection, and Scopus. GS
consistently found the largest percentage of citations across all areas
(93%-96%), far ahead of Scopus (35%-77%) and WoS (27%-73%). GS found nearly all
the WoS (95%) and Scopus (92%) citations. Most citations found only by GS were
from non-journal sources (48%-65%), including theses, books, conference papers,
and unpublished materials. Many were non-English (19%-38%), and they tended to
be much less cited than citing sources that were also in Scopus or WoS. Despite
the many unique GS citing sources, Spearman correlations between citation
counts in GS and WoS or Scopus are high (0.78-0.99). They are lower in the
Humanities, and lower between GS and WoS than between GS and Scopus. The
results suggest that in all areas GS citation data is essentially a superset of
WoS and Scopus, with substantial extra coverage.
"
2274,"Confidence and RISC: How Russian papers indexed in the national citation
  database Russian Index of Science Citation (RISC) characterize universities
  and research institutes","  The paper analyses Russian Index of Science Citation (RISC), a national
citation database. We continue our previous study (Moskaleva et al., 2018) and
focus on difference between bibliometric indicators calculated on, so to say,
""the best"" journals, so called RISC Core, and those which take into account all
Russian journals available. Such a difference may show focuses of insitutional
actors on different document types, publication strategies etc.
"
2275,The rent's too high: Self-archive for fair online publication costs,"  The main contributors of scientific knowledge, researchers, generally aim to
disseminate their findings far and wide. And yet, publishing companies have
largely kept these findings behind a paywall. With digital publication
technology markedly reducing cost, this enduring wall seems disproportionate
and unjustified; moreover, it has sparked a topical exchange concerning how to
modernize academic publishing. This discussion, however, seems to focus on how
to compensate major publishers for providing open access through a ""pay to
publish"" model, in turn transferring financial burdens from libraries to
authors and their funders. Large publishing companies, including Elsevier,
Springer Nature, Wiley, PLoS, and Frontiers, continue to earn exorbitant
revenues each year, hundreds of millions of dollars of which now come from
processing charges for open-access articles. A less expensive and equally
accessible alternative exists: widespread self-archiving of peer-reviewed
articles. All we need is awareness of this alternative and the will to employ
it
"
2276,Go-HEP: writing concurrent software with ease and Go,"  High Energy and Nuclear Physics (HENP) libraries are now required to be more
and more multi-thread-safe, if not multi-thread-friendly and multi-threaded.
This is usually done using the new constructs and library components offered by
the C++11 and C++14 standards. These components are however quite low-level and
hard to use and compose.
  However, Go provides a set of better building blocks for tackling
concurrency: goroutines and channels. This language is now used by the whole
cloud industry: docker/moby, rkt, Kubernetes are obvious flagships for Go. But
to be able to perform any meaningful physics analysis, one needs a set of basic
libraries (matrix operations, linear algebra, plotting, I/O, ...) We present
Go-HEP, a set of packages to easily write concurrent software to interface with
legacy HENP C++ physics libraries.
"
2277,"User Interests in German Social Science Literature Search - A Large
  Scale Log Analysis","  The social sciences are a broad research field with a lot of sub- and related
disciplines. Accordingly, user interests in a digital library for the social
sciences are manifold. In this study we analyzed nine years log data of a
social science digital library to get an overview of the fields, categories,
topics and detailed information needs users are interested in. Based on the log
data we have built interactive visualizations which give an overview and
concurrently let us look at the detailed interests of users. The underlying log
data and the created visualizations are then used to analyze user interests at
different hierarchical levels and on a temporal view. The results show that
there are topical interests of the users in specific fields and topics of the
social sciences but at the same time there exists a diversity of different
information needs. Based on these findings we analyze in detail the gap between
the indexing language of the system used to annotate documents and the language
users apply to articulate their information needs.
"
2278,"Library Information System Audit Senayan Library Management System
  (SLiMS) Using ISO 9126","  The library serves as a vehicle for education, research, conservation,
information, and recreation to improve the nation's intelligence and
empowerment [1]. The function of the library as a place of education, research,
and information provides an opportunity to use the information system of
Senayan Library Management System (SLiMS) in the library in order to improve
the service to the user, increase the reading interest, and expand the insight
and knowledge to educate the nation. The use of ISO 9126 standard is able to
know the quality of SLiMS information system which is said to be free of charge
of usage and license (because it belongs to Open Source Software category [2])
to assist library management in Indonesia. The implementation of the SLiMS
information system audit in several university libraries refers to the ISO 9126
standard by using the Functionality, Reliability, Usability, Efficiency,
Maintainability and Portability aspects through distributing questionnaires to
university librarians in charge. With the help of the use of Google Forms it
turns out that only ten universities librarians in charge who are willing to
fill out the questionnaires are IPMI IBS, Bakrie University, Perbanas Institute
Jakarta, STMIK & Bina Insani Academy, Prasetya Mulya University, Agung Podomoro
University, Indonesian Higher Law School, Matana University, STIKS Tarakanita
Jakarta, and STAI-PIQ West Sumatra. From the results of data processing it is
known that SLiMS included in the category VERY GOOD for use in the management
of libraries in college. This means that the ten universities librarians in
charge admitted and have proven that SLiMS is very helpful in library
management.
"
2279,Empowering open science with reflexive and spatialised indicators,"  Bibliometrics have become commonplace and widely used by authors and journals
to monitor, to evaluate and to identify their readership in an
ever-increasingly publishing scientific world. With this contribution, we aim
to investigate the semantic proximities and evolution of the papers published
in the online journal Cybergeo since its creation in 1996. We propose a
dedicated interactive application that compares three strategies for building
semantic networks, using keywords (self-declared themes), citations (areas of
research using the papers published in Cybergeo) and full-texts (themes derived
from the words used in writing). We interpret these networks and semantic
proximities with respect to their temporal evolution as well as to their
spatial expressions, by considering the countries studied in the papers under
inquiry (Cybergeo being a journal of geography, most articles refer to a
well-defined spatial envelope). Finally, we compare the three methods and
conclude that their complementarity can help go beyond simple statistics to
better understand the epistemological evolution of a scientific community and
the readership target of the journal.
"
2280,Reproducible data citations for computational research,"  The general purpose of a scientific publication is the exchange and spread of
knowledge. A publication usually reports a scientific result and tries to
convince the reader that it is valid. With an ever-growing number of papers
relying on computational methods that make use of large quantities of data and
sophisticated statistical modeling techniques, a textual description of the
result is often not enough for a publication to be transparent and
reproducible. While there are efforts to encourage sharing of code and data, we
currently lack conventions for linking data sources to a computational result
that is stated in the main publication text or used to generate a figure or
table. Thus, here I propose a data citation format that allows for an automatic
reproduction of all computations. A data citation consists of a descriptor that
refers to the functional program code and the input that generated the result.
The input itself may be a set of other data citations, such that all data
transformations, from the original data sources to the final result, are
transparently expressed by a directed graph. Functions can be implemented in a
variety of programming languages since data sources are expected to be stored
in open and standardized text-based file formats. A publication is then an
online file repository consisting of a Hypertext Markup Language (HTML)
document and additional data and code source files, together with a
summarization of all data sources, similar to a list of references in a
bibliography.
"
2281,"Stakes are higher, risk is lower: Citation distributions are more equal
  in high quality journals","  Psychology is a discipline standing at the crossroads of hard and social
sciences. Therefore it is especially interesting to study bibliometric
characteristics of psychology journals. We also take two adjacent disciplines,
neurosciences and sociology. One is closer to hard sciences, another is a
social science. We study not the journal citedness itself (impact factor etc.)
but the citation distribution across papers within journals. This is, so to
say, ""indicators of the second order"" which measure the digression from the
journal's average of the citations received by individual papers. As is shown,
such information about journals may also help authors to correct their
publication strategies.
"
2282,"Long live the scientists: Tracking the scientific fame of great minds in
  physics","  This study utilizes global digitalized books and articles to examine the
scientific fame of the most influential physicists. Our research reveals that
the greatest minds are gone but not forgotten. Their scientific impacts on
human history have persisted for centuries. We also find evidence in support of
own-group fame preference, i.e., that the scientists have greater reputations
in their home countries or among scholars sharing the same languages. We argue
that, when applied appropriately, Google Books and Ngram Viewer can serve as
promising tools for altmetrics, providing a more comprehensive picture of the
impacts scholars and their achievements have made beyond academia.
"
2283,"Harnessing Historical Corrections to build Test Collections for Named
  Entity Disambiguation","  Matching mentions of persons to the actual persons (the name disambiguation
problem) is central for several digital library applications. Scientists have
been working on algorithms to create this matching for decades without finding
a universal solution. One problem is that test collections for this problem are
often small and specific to a certain collection. In this work, we present an
approach that can create large test collections from historical metadata with
minimal extra cost. We apply this approach to the DBLP collection to generate
two freely available test collections. One collection focuses on the properties
of defects and one on the evaluation of disambiguation algorithms.
"
2284,"The Scientific Prize Network Predicts Who Pushes the Boundaries of
  Science","  Scientific prizes are among the greatest recognition a scientist receives
from their peers and arguably shape the direction of a field by conferring
credibility to persons, ideas, and disciplines, providing financial rewards,
and promoting rituals that reinforce scientific communities. The proliferation
of prizes and links among prizes suggest that the prize network embodies
information about scientists and ideas poised to grow in acclaim. Using
comprehensive new data on prizes and prizewinners worldwide and across
disciplines, we examine the growth dynamics and interlocking relationships
found in the worldwide scientific prize network. We focus on understanding how
the knowledge linkages among prizes and scientists' propensities for
prizewinning are related to knowledge pathways across disciplines and
stratification within disciplines. We find several key links between prizes and
scientific advances. First, despite a proliferation of diverse prizes over time
and across the globe, prizes are more concentrated within a relatively small
group of scientific elites, and ties within the elites are more clustered,
suggesting that a relatively constrained number of ideas and scholars lead
science. Second, we find that certain prizes are strongly interlocked within
and between disciplines by scientists who win multiple prizes, revealing the
key pathways by which knowledge systematically gains credit and spreads through
the network. Third, we find that genealogical and co authorship networks
strongly predict who wins one or more prizes and explains the high level of
interconnections among acclaimed scientists and their path breaking ideas.
"
2285,Revisiting Relative Indicators and Provisional Truths,"  Following discussions in 2010 and 2011, scientometric evaluators have
increasingly abandoned relative indicators in favor of comparing observed with
expected citation ratios. The latter method provides parameters with error
values allowing for the statistical testing of differences in citation scores.
A further step would be to proceed to non-parametric statistics (e.g., the
top-10%) given the extreme skewness (non-normality) of the citation
distributions. In response to a plea for returning to relative indicators in
the previous issue of this newsletter, we argue in favor of further progress in
the development of citation impact indicators.
"
2286,"Pillar Universities in Russia: The Rise of ""the Second Wave""","  The problem of identifying the leading universities in a country is rather
easy to solve, one may focus, for example, on highly cited papers or other
indicators of excellence. Sometimes it is more challenging to find the
universities of ""the second wave"" which deserve to receive additional support
because it may help them to become the most prominent ones, ""the first"". We
study progress of the initial phase of such universities, named ""pillar"" in
Russian education system. The first results, measured by 'short-distance
bibliometrics', seem to be encouraging.
"
2287,Assessment and support of emerging research groups,"  The starting point of this paper is a desktop research assessment model that
does not take properly into account the complexities of research assessment,
but rather bases itself on a series of highly simplifying, questionable
assumptions related to the availability, validity and evaluative significance
of research performance indicators, and to funding policy criteria. The paper
presents a critique of this model, and proposes alternative assessment
approaches, based on an explicit evaluative framework, focusing on
preconditions to performance or communication effectiveness rather than on
performance itself, combining metrics and expert knowledge, and using metrics
primarily to set minimum standards. Giving special attention to early career
scientists in emerging research groups, the paper discusses the limits of
classical bibliometric indicators and altmetrics. It proposes alternative
funding formula of research institutions aimed to support emerging research
groups.
"
2288,Challenges of capturing engagement on Facebook for Altmetrics,"  Previous research shows that, despite its popularity, Facebook is less
frequently used to share academic content. In order to investigate this
discrepancy we set out to explore engagement numbers through their Graph API by
querying the Facebook API with multiple URLs for a random set of 103,539
articles from the Web of Science. We identified two major challenge areas:
mapping articles to URLs and the mapping URLs to objects inside Facebook. We
then explored three problem cases within our dataset: (1) identifying a landing
page for any given URL, (2) instances where equivalent URLs are mapped to
different Facebook objects, and (3) instances of different articles being
mapped onto the same Facebook object. We found that the engagement numbers for
11.8% of all articles that have been shared on Facebook at least once are not
reliable because of these problems. Moreover, we were unable to identify the
URL for 11.6% of the articles in our data. Taken together, the three problem
cases constitute 12.3% of the 103,539 tested articles for which engagement
numbers cannot be relied upon. Given that we only tested a small number of
problem cases and URL variants, our results point to large challenges facing
those wishing to collect Facebook metrics programatically through the available
API.
"
2289,"Gender differences in research areas and topics: An analysis of
  publications in 285 fields","  Although the gender gap in academia has narrowed, females are
underrepresented within some fields in the USA. Prior research suggests that
the imbalances between science, technology, engineering and mathematics fields
may be partly due to greater male interest in things and greater female
interest in people, or to off-putting masculine cultures in some disciplines.
To seek more detailed insights across all subjects, this article compares
practising US male and female researchers between and within 285 narrow Scopus
fields inside 26 broad fields from their first-authored articles published in
2017. The comparison is based on publishing fields and the words used in
article titles, abstracts, and keywords. The results cannot be fully explained
by the people/thing dimensions. Exceptions include greater female interest in
veterinary science and cell biology and greater male interest in abstraction,
patients, and power/control fields, such as politics and law. These may be due
to other factors, such as the ability of a career to provide status or social
impact or the availability of alternative careers. As a possible side effect of
the partial people/thing relationship, females are more likely to use
exploratory and qualitative methods and males are more likely to use
quantitative methods. The results suggest that the necessary steps of
eliminating explicit and implicit gender bias in academia are insufficient and
might be complemented by measures to make fields more attractive to minority
genders.
"
2290,"What indicators matter? The Analysis of Perception toward Research
  Assessment Indicators and Leiden Manifesto- The Case Study of Taiwan","  This study aims to investigate the Taiwanese researchers' awareness toward
bibliometric indicators and the principles from Leiden Manifesto. The online
survey was conducted and obtained a total of 417 valid responses. The results
show that evoking the right concept of use of bibliometric indicators and
research evaluation has a long way to go. The lack of recognition of
bibliometric indicators exists in Taiwanese academia. Generally speaking,
researchers may hear of the certain indicator, but they are not familiar with
its definition and calculation process. Only JIF and h-index are considered as
well-known indicators. The results also suggest that the ten principles from
Leiden Manifesto can be considered as the universal guideline in research
evaluation since most of Taiwanese researchers agree the contents. Especially
for the principle 6 ""Account for variation by field in publication and citation
practices"" has highest degree of agreement. However, it is interesting to
compare the result of recognition of relative citation ratio, only few
researchers have fully understood the definition. This result indicates the
scientometricians should make more effort to disseminate the concept of
field-normalization in bibliometric indicators. The researchers do have
understanding about the importance of comparison on the same basis, at the
meantime, they may use the inappropriate indicators just because of lacking of
enough knowledge on the variety of indicators. Hence, it is important to
initiate the education of informetrics to all of the stakeholders in research
evaluation so that the misuse and abuse of bibliometric indicators may possibly
not happen again, and the bibliometric analysis is able to turn to
contextualization-based analysis in the future.
"
2291,"Is together better? Examining scientific collaborations across multiple
  authors, institutions, and departments","  Collaborations are an integral part of scientific research and publishing. In
the past, access to large-scale corpora has limited the ways in which questions
about collaborations could be investigated. However, with improvements in
data/metadata quality and access, it is possible to explore the idea of
research collaboration in ways beyond the traditional definition of multiple
authorship. In this paper, we examine scientific works through three different
lenses of collaboration: across multiple authors, multiple institutions, and
multiple departments. We believe this to be a first look at multiple
departmental collaborations as we employ extensive data curation to
disambiguate authors' departmental affiliations for nearly 70,000 scientific
papers. We then compare citation metrics across the different definitions of
collaboration and find that papers defined as being collaborative were more
frequently cited than their non-collaborative counterparts, regardless of the
definition of collaboration used. We also share preliminary results from
examining the relationship between co-citation and co-authorship by analyzing
the extent to which similar fields (as determined by co-citation) are
collaborating on works (as determined by co-authorship). These preliminary
results reveal trends of compartmentalization with respect to
intra-institutional collaboration and show promise in being expanded.
"
2292,"Predicting citation counts based on deep neural network learning
  techniques","  With the growing number of published scientific papers world-wide, the need
to evaluation and quality assessment methods for research papers is increasing.
Scientific fields such as scientometrics, informetrics and bibliometrics
establish quantified analysis methods and measurements for scientific papers.
In this area, an important problem is to predict the future influence of a
published paper. Particularly, early discrimination between influential papers
and insignificant papers may find important applications. In this regard, one
of the most important metrics is the number of citations to the paper, since
this metric is widely utilized in the evaluation of scientific publications and
moreover, it serves as the basis for many other metrics such as h-index. In
this paper, we propose a novel method for predicting long-term citations of a
paper based on the number of its citations in the first few years after
publication. In order to train a citations prediction model, we employed
artificial neural networks which is a powerful machine learning tool with
recently growing applications in many domains including image and text
processing. The empirical experiments show that our proposed method
out-performs state-of-the-art methods with respect to the prediction accuracy
in both yearly and total prediction of the number of citations.
"
2293,Annotations for Rule-Based Models,"  The chapter reviews the syntax to store machine-readable annotations and
describes the mapping between rule-based modelling entities (e.g., agents and
rules) and these annotations. In particular, we review an annotation framework
and the associated guidelines for annotating rule-based models of molecular
interactions, encoded in the commonly used Kappa and BioNetGen languages, and
present prototypes that can be used to extract and query the annotations. An
ontology is used to annotate models and facilitate their description.
"
2294,"Are all citations worth the same? Valuing citations by the value of the
  citing items","  Bibliometricians have long recurred to citation counts to measure the impact
of publications on the advancement of science. However, since the earliest days
of the field, some scholars have questioned whether all citations should be
worth the same, and have gone on to weight them by a variety of factors.
However sophisticated the operationalization of the measures, the methodologies
used in weighting citations still present limits in their underlying
assumptions. This work takes an alternative approach to resolving the
underlying problem: the proposal is to value citations by the impact of the
citing articles, regardless of the length of their reference list. As well as
conceptualizing a new indicator of impact, the work illustrates its application
to the 2004-2012 Italian scientific production indexed in the WoS. The proposed
impact indicator is highly correlated to the traditional citation count,
however the shifts observed between the two measures are frequent and the
number of outliers not negligible. Moreover, the new indicator shows greater
""sensitivity"" when used to identify the highly-cited papers.
"
2295,"What increases (social) media attention: Research impact, author
  prominence or title attractiveness?","  Do only major scientific breakthroughs hit the news and social media, or does
a 'catchy' title help to attract public attention? How strong is the connection
between the importance of a scientific paper and the (social) media attention
it receives? In this study we investigate these questions by analysing the
relationship between the observed attention and certain characteristics of
scientific papers from two major multidisciplinary journals: Nature
Communication (NC) and Proceedings of the National Academy of Sciences (PNAS).
We describe papers by features based on the linguistic properties of their
titles and centrality measures of their authors in their co-authorship network.
We identify linguistic features and collaboration patterns that might be
indicators for future attention, and are characteristic to different journals,
research disciplines, and media sources.
"
2296,"Nanopublications: A Growing Resource of Provenance-Centric Scientific
  Linked Data","  Nanopublications are a Linked Data format for scholarly data publishing that
has received considerable uptake in the last few years. In contrast to the
common Linked Data publishing practice, nanopublications work at the granular
level of atomic information snippets and provide a consistent container format
to attach provenance and metadata at this atomic level. While the
nanopublications format is domain-independent, the datasets that have become
available in this format are mostly from Life Science domains, including data
about diseases, genes, proteins, drugs, biological pathways, and biotic
interactions. More than 10 million such nanopublications have been published,
which now form a valuable resource for studies on the domain level of the given
Life Science domains as well as on the more technical levels of provenance
modeling and heterogeneous Linked Data. We provide here an overview of this
combined nanopublication dataset, show the results of some overarching
analyses, and describe how it can be accessed and queried.
"
2297,The Archive and Package (arcp) URI scheme,"  The arcp URI scheme is introduced for location-independent identifiers to
consume or reference hypermedia and linked data resources bundled inside a file
archive, as well as to resolve archived resources within programmatic
frameworks for Research Objects. The Research Object for this article is
available at http://s11.no/2018/arcp.html#ro
"
2298,"Specimens as research objects: reconciliation across distributed
  repositories to enable metadata propagation","  Botanical specimens are shared as long-term consultable research objects in a
global network of specimen repositories. Multiple specimens are generated from
a shared field collection event; generated specimens are then managed
individually in separate repositories and independently augmented with research
and management metadata which could be propagated to their duplicate peers.
Establishing a data-derived network for metadata propagation will enable the
reconciliation of closely related specimens which are currently dispersed,
unconnected and managed independently. Following a data mining exercise applied
to an aggregated dataset of 19,827,998 specimen records from 292 separate
specimen repositories, 36% or 7,102,710 specimens are assessed to participate
in duplication relationships, allowing the propagation of metadata among the
participants in these relationships, totalling: 93,044 type citations,
1,121,865 georeferences, 1,097,168 images and 2,191,179 scientific name
determinations. The results enable the creation of networks to identify which
repositories could work in collaboration. Some classes of annotation
(particularly those regarding scientific name determinations) represent units
of scientific work: appropriate management of this data would allow the
accumulation of scholarly credit to individual researchers: potential further
work in this area is discussed.
"
2299,"Towards Better Understanding Researcher Strategies in Cross-Lingual
  Event Analytics","  With an increasing amount of information on globally important events, there
is a growing demand for efficient analytics of multilingual event-centric
information. Such analytics is particularly challenging due to the large amount
of content, the event dynamics and the language barrier. Although memory
institutions increasingly collect event-centric Web content in different
languages, very little is known about the strategies of researchers who conduct
analytics of such content. In this paper we present researchers' strategies for
the content, method and feature selection in the context of cross-lingual
event-centric analytics observed in two case studies on multilingual Wikipedia.
We discuss the influence factors for these strategies, the findings enabled by
the adopted methods along with the current limitations and provide
recommendations for services supporting researchers in cross-lingual
event-centric analytics.
"
2300,"Coordinating and Integrating Faceted Classification with Rich Semantic
  Modeling","  Faceted classifications define dimensions for the types of entities included.
In effect, the facets provide an ""ontological commitment"". We compare a faceted
thesaurus, the Art and Architecture Thesaurus (AAT), with ontologies derived
from the Basic Formal Ontology (BFO2), which is an upper (or formal) ontology
widely used to describe entities in biomedicine. We consider how the AAT and
BFO2-based ontologies could be coordinated and integrated into a Human Activity
and Infrastructure Foundry (HAIF). To extend the AAT to enable this
coordination and integration, we describe how a wider range of relationships
among its terms could be introduced. Using these extensions, we explore richer
modeling of topics from AAT that deal with Technology. Finally, we consider how
ontology-based frames and semantic role frames can be integrated to make rich
semantic statements about changes in the world.
"
2301,The insoluble problems of books: What does Altmetric.com have to offer?,"  The purpose of this paper is to analyze the capabilities, functionalities and
appropriateness of Altmetric.com as a data source for the bibliometric analysis
of books in comparison to PlumX. We perform an exploratory analysis on the
metrics the Altmetric Explorer for Institutions platform offers for books. We
use two distinct datasets of books: the Book Collection included in
Altmetric.com and the Clarivate's Master Book List, to analyze Altmetric.com's
capabilities to download and merge data with external databases. Finally, we
compare our findings with those obtained in a previous study performed in
PlumX. Altmetric.com combines and orderly tracks a set of data sources combined
by DOI identifiers to retrieve metadata from books, being Google Books its main
provider. It also retrieves information from commercial publishers and from
some Open Access initiatives, including those led by university libraries such
as Harvard Library. We find issues with linkages between records and mentions
or ISBN discrepancies. Furthermore, we find that automatic bots affect greatly
Wikipedia mentions to books. Our comparison with PlumX suggests that none of
these tools provide a complete picture of the social attention generated by
books and are rather complementary than comparable tools.
"
2302,Enabling FAIR Research in Earth Science through Research Objects,"  Data-intensive science communities are progressively adopting FAIR practices
that enhance the visibility of scientific breakthroughs and enable reuse. At
the core of this movement, research objects contain and describe scientific
information and resources in a way compliant with the FAIR principles and
sustain the development of key infrastructure and tools. This paper provides an
account of the challenges, experiences and solutions involved in the adoption
of FAIR around research objects over several Earth Science disciplines. During
this journey, our work has been comprehensive, with outcomes including: an
extended research object model adapted to the needs of earth scientists; the
provisioning of digital object identifiers (DOI) to enable persistent
identification and to give due credit to authors; the generation of
content-based, semantically rich, research object metadata through natural
language processing, enhancing visibility and reuse through recommendation
systems and third-party search engines; and various types of checklists that
provide a compact representation of research object quality as a key enabler of
scientific reuse. All these results have been integrated in ROHub, a platform
that provides research object management functionality to a wealth of
applications and interfaces across different scientific communities. To monitor
and quantify the community uptake of research objects, we have defined
indicators and obtained measures via ROHub that are also discussed herein.
"
2303,"Evaluating research and researchers by the journal impact factor: is it
  better than coin flipping?","  The journal impact factor (JIF) is the average of the number of citations of
the papers published in a journal, calculated according to a specific formula;
it is extensively used for the evaluation of research and researchers. The
method assumes that all papers in a journal have the same scientific merit,
which is measured by the JIF of the publishing journal. This implies that the
number of citations measures scientific merits but the JIF does not evaluate
each individual paper by its own number of citations. Therefore, in the
comparative evaluation of two papers, the use of the JIF implies a risk of
failure, which occurs when a paper in the journal with the lower JIF is
compared to another with fewer citations in the journal with the higher JIF. To
quantify this risk of failure, this study calculates the failure probabilities,
taking advantage of the lognormal distribution of citations. In two journals
whose JIFs are ten-fold different, the failure probability is low. However, in
most cases when two papers are compared, the JIFs of the journals are not so
different. Then, the failure probability can be close to 0.5, which is
equivalent to evaluating by coin flipping.
"
2304,Consistency and validity of interdisciplinarity measures,"  Measuring interdisciplinarity is a pertinent but challenging issue in
quantitative studies of science. There seems to be a consensus in the
literature that the concept of interdisciplinarity is multifaceted and
ambiguous. Unsurprisingly, various different measures of interdisciplinarity
have been proposed. However, few studies have thoroughly examined the validity
and relations between these measures. In this study, we present a systematic
review of these interdisciplinarity measures and explore their inherent
relations. We examine these measures in relation to the Web of Science journal
subject categories. Our results corroborate recent claims that the current
measurements of interdisciplinarity in science studies are both confusing and
unsatisfying. We find surprisingly deviant results when comparing measures that
supposedly should capture similar features or dimensions of the concept of
interdisciplinarity. We therefore argue that the current measurements of
interdisciplinarity should be interpreted with much caution in science and
evaluation studies, or in relation to science policies. We also question the
validity of current measures and argue that we do not need more of the same,
but rather something very different in order to be able to measure the
multidimensional and complex construct of interdisciplinarity.
"
2305,"$\mathbf{h_\alpha}$: An index to quantify an individual's scientific
  leadership","  The $\alpha$ person is the dominant person in a group. We define the
$\alpha$-author of a paper as the author of the paper with the highest
$h$-index among all the coauthors, and an $\alpha$-paper of a scientist as a
paper authored or coauthored by the scientist where he/she is the
$\alpha$-author. For most but not all papers in the literature there is only
one $\alpha$-author. We define the $h_\alpha$ index of a scientist as the
number of papers in the $h$-core of the scientist (i.e. the set of papers that
contribute to the $h$-index of the scientist) where this scientist is the
$\alpha$-author. We also define the $h'_\alpha$ index of a scientist as the
number of $\alpha$-papers of this scientist that have $\geq$ $h'_\alpha$
citations. $h_\alpha$ and $h'_\alpha$ contain similar information, while
$h'_\alpha$ is conceptually more appealing it is harder to obtain from existing
databases, hence of less current practical interest. We propose that the
$h_\alpha$ and/or $h'_\alpha$ indices, or other variants discussed in the
paper, are useful complements to the $h$-index of a scientist to quantify
his/her scientific achievement, that rectify an inherent drawback of the
$h$-index, its inability to distinguish between authors with different
coauthorships patterns. A high $h$ index in conjunction with a high
$h_\alpha/h$ ratio is a hallmark of scientific leadership.
"
2306,"Exploring the role and nature of interactions between Institutes in a
  Local Affiliation Network","  In this work, we have studied the collaboration and citation network between
Indian Institutes from publications in American Physical Society(APS) journals
between 1970-2013. We investigate the role of geographic proximity on the
network structure and find that it is the characteristics of the Institution,
rather than the geographic distance, that plays a dominant role in
collaboration networks. We find that Institutions with better federal funding
dominate the network topology and play a crucial role in overall research
output. We find that the citation flow across different categories of
institutions is strongly linked to the collaborations between them. We have
estimated the knowledge flow in and out of Institutions and identified the top
knowledge source and sinks.
"
2307,A Similarity Measure for Weaving Patterns in Textiles,"  We propose a novel approach for measuring the similarity between weaving
patterns that can provide similarity-based search functionality for textile
archives. We represent textile structures using hypergraphs and extract
multisets of k-neighborhoods from these graphs. The resulting multisets are
then compared using Jaccard coefficients, Hamming distances, and cosine
measures. We evaluate the different variants of our similarity measure
experimentally, showing that it can be implemented efficiently and illustrating
its quality using it to cluster and query a data set containing more than a
thousand textile samples.
"
2308,The IFF Foundation for Ontological Knowledge Organization,"  This paper discusses an axiomatic approach for the integration of ontologies,
an approach that extends to first order logic a previous approach (Kent 2000)
based on information flow. This axiomatic approach is represented in the
Information Flow Framework (IFF), a metalevel framework for organizing the
information that appears in digital libraries, distributed databases and
ontologies (Kent 2001). The paper argues that the integration of ontologies is
the two-step process of alignment and unification. Ontological alignment
consists of the sharing of common terminology and semantics through a mediating
ontology. Ontological unification, concentrated in a virtual ontology of
community connections, is fusion of the alignment diagram of participant
community ontologies - the quotient of the sum of the participant portals
modulo the ontological alignment structure.
"
2309,"Measuring Expert Performance at Manually Classifying Domain Entities
  under Upper Ontology Classes","  Classifying entities in domain ontologies under upper ontology classes is a
recommended task in ontology engineering to facilitate semantic
interoperability and modelling consistency. Integrating upper ontologies this
way is difficult and, despite emerging automated methods, remains a largely
manual task.
  Little is known about how well experts perform at upper ontology integration.
To develop methodological and tool support, we first need to understand how
well experts do this task. We designed a study to measure the performance of
human experts at manually classifying classes in a general knowledge domain
ontology with entities in the Basic Formal Ontology (BFO), an upper ontology
used widely in the biomedical domain.
  We conclude that manually classifying domain entities under upper ontology
classes is indeed very difficult to do correctly. Given the importance of the
task and the high degree of inconsistent classifications we encountered, we
further conclude that it is necessary to improve the methodological framework
surrounding the manual integration of domain and upper ontologies.
"
2310,Rough Concept Analysis,"  The theory introduced, presented and developed in this paper, is concerned
with Rough Concept Analysis. This theory is a synthesis of the theory of Rough
Sets pioneered by Zdzislaw Pawlak with the theory of Formal Concept Analysis
pioneered by Rudolf Wille. The central notion in this paper of a rough formal
concept combines in a natural fashion the notion of a rough set with the notion
of a formal concept: ""rough set + formal concept = rough formal concept"". A
follow-up paper will provide a synthesis of the two important data modeling
techniques: conceptual scaling of Formal Concept Analysis and
Entity-Relationship database modeling.
"
2311,h{\alpha}: The Scientist as Chimpanzee or Bonobo,"  In a recent paper, Hirsch (2018) proposes to attribute the credit for a
co-authored paper to the {\alpha}-author--the author with the highest
h-index--regardless of his or her actual contribution, effectively reducing the
role of the other co-authors to zero. The indicator h{\alpha} inherits most of
the disadvantages of the h-index from which it is derived, but adds the
normative element of reinforcing the Matthew effect in science. Using an
example, we show that h{\alpha} can be extremely unstable. The empirical
attribution of credit among co-authors is not captured by abstract models such
as h, h_bar , or h{\alpha}.
"
2312,Gender Bias in Nobel Prizes,"  Strikingly few Nobel laureates within medicine, natural and social sciences
are women. Although it is obvious that there are fewer women researchers within
these fields, does this gender ratio still fully account for the low number of
female Nobel laureates? We examine whether women are awarded the Nobel Prizes
less often than the gender ratio suggests. Based on historical data across four
scientific fields and a Bayesian hierarchical model, we quantify any possible
bias. The model reveals, with exceedingly large confidence, that indeed women
are strongly under-represented among Nobel laureates across all disciplines
examined.
"
2313,"Does the public discuss other topics on climate change than researchers?
  A comparison of explorative networks based on author keywords and hashtags","  Twitter accounts have already been used in many scientometric studies, but
the meaningfulness of the data for societal impact measurements in research
evaluation has been questioned. Earlier research focused on social media counts
and neglected the interactive nature of the data. We explore a new network
approach based on Twitter data in which we compare author keywords to hashtags
as indicators of topics. We analyze the topics of tweeted publications and
compare them with the topics of all publications (tweeted and not tweeted). Our
exploratory study is based on a comprehensive publication set of climate change
research. We are interested in whether Twitter data are able to reveal topics
of public discussions which can be separated from research-focused topics. We
find that the most tweeted topics regarding climate change research focus on
the consequences of climate change for humans. Twitter users are interested in
climate change publications which forecast effects of a changing climate on the
environment and to adaptation, mitigation and management issues rather than in
the methodology of climate-change research and causes of climate change. Our
results indicate that publications using scientific jargon are less likely to
be tweeted than publications using more general keywords. Twitter networks seem
to be able to visualize public discussions about specific topics.
"
2314,Conceptual Collectives,"  The notions of formal contexts and concept lattices, although introduced by
Wille only ten years ago, already have proven to be of great utility in various
applications such as data analysis and knowledge representation. In this paper
we give arguments that Wille's original notion of formal context, although
quite appealing in its simplicity, now should be replaced by a more semantic
notion. This new notion of formal context entails a modified approach to
concept construction. We base our arguments for these new versions of formal
context and concept construction upon Wille's philosophical attitude with
reference to the intensional aspect of concepts. We give a brief development of
the relational theory of formal contexts and concept construction,
demonstrating the equivalence of ""concept-lattice construction"" of Wille with
the well-known ""completion by cuts"" of MacNeille. Generalization and
abstraction of these formal contexts offers a powerful approach to knowledge
representation.
"
2315,Openness and Impact of Leading Scientific Countries,"  The rapid rise of international collaboration over the past three decades,
demonstrated in coauthorship of scientific articles, raises the question of
whether countries benefit from cooperative science and how this might be
measured. We develop and compare measures to ask this question. For all source
publications in 2013, we obtained from Elsevier national level full and
fractional paper counts as well as accompanying field-weighted citation counts.
Then we collected information from Elsevier on the percent of all
internationally coauthored papers for each country, as well as Organization for
Economic Cooperation and Development measures of the international mobility of
the scientific workforce in 2013, and conducted a principle component analysis
that produced an openness index. We added data from the OECD on government
budget allocation on research and development for 2011 to tie in the public
spending that contributed to the 2013 output. We found that openness among
advanced science systems is strongly correlated with impact: the more
internationally engaged a nation is in terms of coauthorships and researcher
mobility, the higher the impact of scientific work. The results have important
implications for policy making around investment, as well as the flows of
students, researchers, and technical workers.
"
2316,"She's Reddit: A source of statistically significant gendered interest
  information?","  Information about gender differences in interests is necessary to disentangle
the effects of discrimination and choice when gender inequalities occur, such
as in employment. This article assesses gender differences in interests within
the popular social news and entertainment site Reddit. A method to detect terms
that are statistically significantly used more by males or females in 181
million comments in 100 subreddits shows that gender affects both the selection
of subreddits and activities within most of them. The method avoids the hidden
gender biases of topic modelling for this task. Although the method reveals
statistically significant gender differences in interests for topics that are
extensively discussed on Reddit, it cannot give definitive causes, and
imitation and sharing within the site mean that additional checking is needed
to verify the results. Nevertheless, with care, Reddit can serve as a useful
source of insights into gender differences in interests.
"
2317,On the Heterogeneous Distributions in Paper Citations,"  Academic papers have been the protagonists in disseminating expertise.
Naturally, paper citation pattern analysis is an efficient and essential way of
investigating the knowledge structure of science and technology. For decades,
it has been observed that citation of scientific literature follows a
heterogeneous and heavy-tailed distribution, and many of them suggest a
power-law distribution, log-normal distribution, and related distributions.
However, many studies are limited to small-scale approaches; therefore, it is
hard to generalize. To overcome this problem, we investigate 21 years of
citation evolution through a systematic analysis of the entire citation history
of 42,423,644 scientific literatures published from 1996 to 2016 and contained
in SCOPUS. We tested six candidate distributions for the scientific literature
in three distinct levels of Scimago Journal & Country Rank (SJR) classification
scheme. First, we observe that the raw number of annual citation acquisitions
tends to follow the log-normal distribution for all disciplines, except for the
first year of the publication. We also find significant disparity between the
yearly acquired citation number among the journals, which suggests that it is
essential to remove the citation surplus inherited from the prestige of the
journals. Our simple method for separating the citation preference of an
individual article from the inherited citation of the journals reveals an
unexpected regularity in the normalized annual acquisitions of citations across
the entire field of science. Specifically, the normalized annual citation
acquisitions have power-law probability distributions with an exponential
cut-off of the exponents around 2.3, regardless of its publication and citation
year. Our results imply that journal reputation has a substantial long-term
impact on the citation.
"
2318,"What Does a Successful Postdoctoral Fellowship Publication Record Look
  Like?","  Obtaining a prize postdoctoral fellowship in astronomy and astrophysics
involves a number of factors, many of which cannot be quantified. One criterion
that can be measured is the publication record of an applicant. The publication
records of past fellowship recipients may, therefore, provide some quantitative
guidance for future prospective applicants. We investigated the publication
patterns of recipients of the NASA prize postdoctoral fellowships in the
Hubble, Einstein, and Sagan programs from 2014 through 2017, using the NASA ADS
reference system. We tabulated their publications at the point where fellowship
applications were submitted, and we find that the 133 fellowship recipients in
that time frame had a median of 6 +/- 2 first-author publications, and 14 +/- 6
co-authored publications. The full range of first author papers is 1 to 15, and
for all papers ranges from 2 to 76, indicating very diverse publication
patterns. Thus, while fellowship recipients generally have strong publication
records, the distribution of both first-author and co-authored papers is quite
broad; there is no apparent threshold of publications necessary to obtain these
fellowships. We also examined the post-PhD publication rates for each of the
three fellowship programs, between male and female recipients, across the four
years of the analysis and find no consistent trends. We hope that these
findings will prove a useful reference to future junior scientists.
"
2319,"Publish-and-Flourish: decentralized co-creation and curation of
  scholarly content","  Scholarly communication is today immersed in publish or perish culture that
propels noncooperative behaviour in the sense of strategic games played by
researchers. Here we introduce and describe a blockchain based platform for
decentralized scholarly communication. The design of the platform rests on
community driven publishing reviewing processes and implements incentives that
promote cooperative user behaviour. Key to achieve cooperation in blockchain
based scholarly communication is to transform a static research paper into a
modifiable research paper under continuous peer review process. We describe and
discuss the implementation of a modifiable research paper as a smart contract
on the blockchain.
"
2320,"Building and Querying Semantic Layers for Web Archives (Extended
  Version)","  Web archiving is the process of collecting portions of the Web to ensure that
the information is preserved for future exploitation. However, despite the
increasing number of web archives worldwide, the absence of efficient and
meaningful exploration methods still remains a major hurdle in the way of
turning them into a usable and useful information source. In this paper, we
focus on this problem and propose an RDF/S model and a distributed framework
for building semantic profiles (""layers"") that describe semantic information
about the contents of web archives. A semantic layer allows describing metadata
information about the archived documents, annotating them with useful semantic
information (like entities, concepts and events), and publishing all this data
on the Web as Linked Data. Such structured repositories offer advanced query
and integration capabilities, and make web archives directly exploitable by
other systems and tools. To demonstrate their query capabilities, we build and
query semantic layers for three different types of web archives. An
experimental evaluation showed that a semantic layer can answer information
needs that existing keyword-based systems are not able to sufficiently satisfy.
"
2321,"Communities as Well Separated Subgraphs With Cohesive Cores:
  Identification of Core-Periphery Structures in Link Communities","  Communities in networks are commonly considered as highly cohesive subgraphs
which are well separated from the rest of the network. However, cohesion and
separation often cannot be maximized at the same time, which is why a
compromise is sought by some methods. When a compromise is not suitable for the
problem to be solved it might be advantageous to separate the two criteria. In
this paper, we explore such an approach by defining communities as well
separated subgraphs which can have one or more cohesive cores surrounded by
peripheries. We apply this idea to link communities and present an algorithm
for constructing hierarchical core-periphery structures in link communities and
first test results.
"
2322,Evolution of semantic networks in biomedical texts,"  Language is hierarchically organized: words are built into phrases,
sentences, and paragraphs to represent complex ideas. Here we ask whether the
organization of language in written text displays the fractal hierarchical
architecture common in systems optimized for efficient information
transmission. We test the hypothesis that the expositional structure of
scientific research articles displays Rentian scaling, and that the exponent of
the scaling law changes as the article's information transmission capacity
changes. Using 32 scientific manuscripts - each containing between three and 26
iterations of revision - we construct semantic networks in which nodes
represented unique words in each manuscript, and edges connect nodes if two
words appeared within the same 5-word window. We show that these semantic
networks display clear Rentian scaling, and that the Rent exponent varies over
the publication life cycle, from the first draft to the final revision.
Furthermore, we observe that manuscripts fell into three clusters in terms of
how the scaling exponents changed across drafts: exponents rising over time,
falling over time, and remaining relatively stable over time. This change in
exponent reflects the evolution in semantic network structure over the
manuscript revision process, highlighting a balance between network complexity,
which increases the exponent, and network efficiency, which decreases the
exponent. Lastly, the final value of the Rent exponent is negatively correlated
with the number of authors. Taken together, our results suggest that semantic
networks reflecting the structure of exposition in scientific research articles
display striking hierarchical architecture that arbitrates tradeoffs between
competing constraints on network organization, and that this arbitration is
navigated differently depending on the social environment characteristic of the
collaboration.
"
2323,"Tracking the History and Evolution of Entities: Entity-centric Temporal
  Analysis of Large Social Media Archives","  How did the popularity of the Greek Prime Minister evolve in 2015? How did
the predominant sentiment about him vary during that period? Were there any
controversial sub-periods? What other entities were related to him during these
periods? To answer these questions, one needs to analyze archived documents and
data about the query entities, such as old news articles or social media
archives. In particular, user-generated content posted in social networks, like
Twitter and Facebook, can be seen as a comprehensive documentation of our
society, and thus meaningful analysis methods over such archived data are of
immense value for sociologists, historians and other interested parties who
want to study the history and evolution of entities and events. To this end, in
this paper we propose an entity-centric approach to analyze social media
archives and we define measures that allow studying how entities were reflected
in social media in different time periods and under different aspects, like
popularity, attitude, controversiality, and connectedness with other entities.
A case study using a large Twitter archive of four years illustrates the
insights that can be gained by such an entity-centric and multi-aspect
analysis.
"
2324,Ranking Archived Documents for Structured Queries on Semantic Layers,"  Archived collections of documents (like newspaper and web archives) serve as
important information sources in a variety of disciplines, including Digital
Humanities, Historical Science, and Journalism. However, the absence of
efficient and meaningful exploration methods still remains a major hurdle in
the way of turning them into usable sources of information. A semantic layer is
an RDF graph that describes metadata and semantic information about a
collection of archived documents, which in turn can be queried through a
semantic query language (SPARQL). This allows running advanced queries by
combining metadata of the documents (like publication date) and content-based
semantic information (like entities mentioned in the documents). However, the
results returned by such structured queries can be numerous and moreover they
all equally match the query. In this paper, we deal with this problem and
formalize the task of ""ranking archived documents for structured queries on
semantic layers"". Then, we propose two ranking models for the problem at hand
which jointly consider: i) the relativeness of documents to entities, ii) the
timeliness of documents, and iii) the temporal relations among the entities.
The experimental results on a new evaluation dataset show the effectiveness of
the proposed models and allow us to understand their limitations
"
2325,Towards a Ranking Model for Semantic Layers over Digital Archives,"  Archived collections of documents (like newspaper archives) serve as
important information sources for historians, journalists, sociologists and
other interested parties. Semantic Layers over such digital archives allow
describing and publishing metadata and semantic information about the archived
documents in a standard format (RDF), which in turn can be queried through a
structured query language (e.g., SPARQL). This enables to run advanced queries
by combining metadata of the documents (like publication date) and
content-based semantic information (like entities mentioned in the documents).
However, the results returned by structured queries can be numerous and also
they all equally match the query. Thus, there is the need to rank these results
in order to promote the most important ones. In this paper, we focus on this
problem and propose a ranking model that considers and combines: i) the
relativeness of documents to entities, ii) the timeliness of documents, and
iii) the relations among the entities.
"
2326,The Information Flow Foundation for Conceptual Knowledge Organization,"  The sharing of ontologies between diverse communities of discourse allows
them to compare their own information structures with that of other communities
that share a common terminology and semantics - ontology sharing facilitates
interoperability between online knowledge organizations. This paper
demonstrates how ontology sharing is formalizable within the conceptual
knowledge model of Information Flow (IF). Information Flow indirectly
represents sharing through a specifiable, ontology extension hierarchy
augmented with synonymic type equivalencing - two ontologies share terminology
and meaning through a common generic ontology that each extends. Using the
paradigm of participant community ontologies formalized as IF logics, a common
shared extensible ontology formalized as an IF theory, participant community
specification links from the common ontology to the participating community
ontology formalizable as IF theory interpretations, this paper argues that
ontology sharing is concentrated in a virtual ontology of community
connections, and demonstrates how this virtual ontology is computable as the
fusion of the participant ontologies - the quotient of the sum of the
participant ontologies modulo the ontological sharing structure.
"
2327,The determinants of academic career advancement: evidence from Italy,"  In this work we investigate the determinants of professors' career
advancement in Italian universities. From the analyses, it emerges that the
fundamental determinant of an academic candidate's success is not scientific
merit, but rather the number of years that the candidate has belonged to the
same university as the selection committee president. Where applicants have
participated in research work with the president, their probability of success
also increases significantly. The factors of the years of service and
occurrence of joint research for the other commission members also have an
effect, however of lesser weight. The specific phenomenon of nepotism, although
it exists, seems less important. The scientific quality of the commission
members has negligible effect on the expected outcome of the competition, and
even more so the geographic location of the university calling for the
competition.
"
2328,"An investigation on the skewness patterns and fractal nature of research
  productivity distributions at field and discipline level","  The paper provides an empirical examination of how research productivity
distributions differ across scientific fields and disciplines. Productivity is
measured using the FSS indicator, which embeds both quantity and impact of
output. The population studied consists of over 31,000 scientists in 180 fields
(10 aggregate disciplines) of a national research system. The Characteristic
Scores and Scale technique is used to investigate the distribution patterns for
the different fields and disciplines. Research productivity distributions are
found to be asymmetrical at the field level, although the degree of skewness
varies substantially among the fields within the aggregate disciplines. We also
examine whether the field productivity distributions show a fractal nature,
which reveals an exception more than a rule. Differently, for the disciplines,
the partitions of the distributions show skewed patterns that are highly
similar.
"
2329,"The effect of multidisciplinary collaborations on research
  diversification","  This work verifies whether research diversification by a scientist is in some
measure related to their collaboration with multidisciplinary teams. The
analysis considers the publications achieved by 5300 Italian academics in the
sciences over the period 2004-2008. The findings show that a scientist's
outputs resulting from research diversification are more often than not the
result of collaborations with multidisciplinary teams. The effect becomes more
pronounced with larger and particularly with more diversified teams. This
phenomenon is observed both at the overall level and for the disciplinary
macro-areas.
"
2330,Who benefits from a country's scientific research?,"  When a publication is cited it generates a benefit. Through the country
affiliations of the citing authors, it is possible to work upwards, tracing the
countries that benefit from results produced in a national research system. In
this work we take the knowledge flow from Italy as an example. We develop a
methodology for examination of how the knowledge flows vary across fields, in
each beneficiary country. We also measure the field comparative advantage of
countries in benefiting from Italian research. The results from this method can
inform bilateral research collaboration policies.
"
2331,"The effects of gender, age and academic rank on research diversification","  In this work we analyze the combined effects of gender, age and academic rank
on the propensity of individual scholars to diversify their scientific
activity. The aspect of research diversification is measured along three main
dimensions, namely its extent and intensity and the cognitive relatedness of
the fields of diversification. We apply two regression models to the dataset of
scientific output of all Italian professors in the sciences over the period
2004-2008. The aim is to understand how personal and organizational traits can
influence individual behaviors in terms of research diversification. Among
other considerations, our findings urge caution in identifying research
diversification as a co-determinant of the gender productivity gap between
males and females.
"
2332,"The relationship among research productivity, research collaboration,
  and their determinants","  This work provides an in-depth analysis of the relation between the different
types of collaboration and research productivity, showing how both are
influenced by some personal and organizational variables. By applying different
cross-lagged panel models, we are able to analyze the relationship among
research productivity, collaboration and their determinants. In particular, we
show that only collaboration at intramural and domestic level has a positive
effect on research productivity. Differently, all the forms of collaboration
are positively affected by research productivity. The results can favor the
reexamination of the theories related to these issues, and inform policies that
would be more suited to their management.
"
2333,"On tit for tat: Franceschini and Maisano versus ANVUR regarding the
  Italian research assessment exercise VQR 2011-2014","  The response by Benedetto, Checchi, Graziosi & Malgarini (2017) (hereafter
""BCG&M""), past and current members of the Italian Agency for Evaluation of
University and Research Systems (ANVUR), to Franceschini and Maisano's (""F&M"")
article (2017), inevitably draws us into the debate. BCG&M in fact complain
""that almost all criticisms to the evaluation procedures adopted in the two
Italian research assessments VQR 2004-2010 and 2011-2014 limit themselves to
criticize the procedures without proposing anything new and more apt to the
scope"". Since it is us who raised most criticisms in the literature, we welcome
this opportunity to retrace our vainly ""constructive"" recommendations, made
with the hope of contributing to assessments of the Italian research system
more in line with the state of the art in scientometrics. We see it as equally
interesting to confront the problem of the failure of knowledge transfer from
R&D (scholars) to engineering and production (ANVUR's practitioners) in the
Italian VQRs. We will provide a few notes to help the reader understand the
context for this failure. We hope that these, together with our more specific
comments, will also assist in communicating the reasons for the level of
scientometric competence expressed in BCG&M's heated response to F&M's
criticism.
"
2334,Do interdisciplinary research teams deliver higher gains to science?,"  The present paper takes its place in the stream of studies that analyze the
effect of interdisciplinarity on the impact of research output. Unlike previous
studies, in this study the interdisciplinarity of the publications is not
inferred through their citing or cited references, but rather by identifying
the authors' designated fields of research. For this we draw on the scientific
classification of Italian academics, and their publications as indexed in the
WoS over a five-year period (2004-2008). We divide the publications in three
subsets on the basis the nature of co-authorship: those papers coauthored with
academics from different fields, which show high intensity of inter-field
collaboration (""specific"" collaboration, occurring in 110 pairings of fields);
those papers coauthored with academics who are simply from different
""non-specific"" fields; and finally co-authorships within a single field. We
then compare the citations of the papers and the impact factor of the
publishing journals between the three subsets. The results show significant
differences, generally in favor of the interdisciplinary authorships, in only
one third (or slightly more) of the cases. The analysis provides the value of
the median differences for each pair of publication subsets.
"
2335,"How long do top scientists maintain their stardom? An analysis by
  region, gender and discipline: evidence from Italy","  We investigate the question of how long top scientists retain their stardom.
We observe the research performance of all Italian professors in the sciences
over three consecutive four-year periods, between 2001 and 2012. The top
scientists of the first period are identified on the basis of research
productivity, and their performance is then tracked through time. The analyses
demonstrate that more than a third of the nation's top scientists maintain this
status over the three consecutive periods, with higher shares occurring in the
life sciences and lower ones in engineering. Compared to males, females are
less likely to maintain top status. There are also regional differences, among
which top status is less likely to survive in southern Italy than in the north.
Finally we investigate the longevity of unproductive professors, and then check
whether the career progress of the top and unproductive scientists is aligned
with their respective performances. The results appear to have implications for
national policies on academic recruitment and advancement.
"
2336,Does your surname affect the citability of your publications?,"  Prior investigations have offered contrasting results on a troubling
question: whether the alphabetical ordering of bylines confers citation
advantages on those authors whose surnames put them first in the list. The
previous studies analyzed the surname effect at publication level, i.e. whether
papers with the first author early in the alphabet trigger more citations than
papers with a first author late in the alphabet. We adopt instead a different
approach, by analyzing the surname effect on citability at the individual
level, i.e. whether authors with alphabetically earlier surnames result as
being more cited. Examining the question at both the overall and discipline
levels, the analysis finds no evidence whatsoever that alphabetically earlier
surnames gain advantage. The same lack of evidence occurs for the subpopulation
of scientists with very high publication rates, where alphabetical advantage
might gain more ground. The field of observation consists of 14,467 scientists
in the sciences.
"
2337,"Refrain from adopting the combination of citation and journal metrics to
  grade publications, as used in the Italian national research assessment
  exercise (VQR 2011-2014)","  The prediction of the long-term impact of a scientific article is challenging
task, addressed by the bibliometrician through resorting to a proxy whose
reliability increases with the breadth of the citation window. In the national
research assessment exercises using metrics the citation window is necessarily
short, but in some cases is sufficient to advise the use of simple citations.
For the Italian VQR 2011-2014, the choice was instead made to adopt a linear
weighted combination of citations and journal metric percentiles, with weights
differentiated by discipline and year. Given the strategic importance of the
exercise, whose results inform the allocation of a significant share of
resources for the national academic system, we examined whether the predictive
power of the proposed indicator is stronger than the simple citation count. The
results show the opposite, for all discipline in the sciences and a citation
window above two years.
"
2338,"The dispersion of the citation distribution of top scientists'
  publications","  This work explores the distribution of citations for the publications of top
scientists. A first objective is to find out whether the 80-20 Pareto rule
applies, that is if 80% of the citations to a top scientist's work concern 20%
of their publications. Observing that the rule does not apply, we also measure
the dispersion of the citation distribution by means of the Gini coefficient.
Further, we investigate the question of what share of a top scientist'
publications go uncited. Finally, we study the relation between the dispersion
of the citation distribution and the share of uncited publications. As well as
the overall level, the analyses are carried out at the field and discipline
level, to assess differences across them.
"
2339,The north-south divide in the Italian higher education system,"  This work examines whether the macroeconomic divide between northern and
southern Italy is also present at the level of higher education. The analysis
confirms that the research performance in the sciences of the professors in the
south is on average less than that of the professors in the north, and that
this gap does not show noticeable variations at the level of gender or academic
rank. For the universities, the gap is still greater. The study analyzes some
possible determinants of the gap, and provides some policy recommendations for
its reduction.
"
2340,"The effect of a country's name in the title of a publication on its
  visibility and citability","  The objective of this research is to determine if the reference to a country
in the title, keywords or abstract of a publication can influence its
visibility (measured by the impact factor of the publishing journal) and
citability (measured by the citations received). The study is based on Italian
scientific production indexed in the Web of Science over the period 2004-2011.
The analysis is conducted by comparing the values of four impact indicators for
two subsets: i) the indexed publications with a country's name in the title,
keywords or abstract; ii) the remainder of the population, with no country'
name. The results obtained both at the general level and by subject category
show that publications with a country name systematically receive lower impact
values, with the exception of a limited number of subject categories, Also, the
incidence of highly-cited articles is lower for the first subset.
"
2341,A comparison of university performance scores and ranks by MNCS and FSS,"  In a previous article of ours, we explained the reasons why the MNCS and all
similar per-publication citation indicators should not be used to measure
research performance, whereas efficiency indicators (output to input) such as
the FSS are valid indicators of performance. The problem frequently indicated
in measuring efficiency indicators lies in the availability of input data. If
we accept that such data are inaccessible, and instead resort to
per-publication citation indicators, the question arises as to what extent
institution performance rankings by MNCS are different from those by FSS (and
so what effects such results could have on policy-makers, managers and other
users of the rankings). Contrasting the 2008-2012 performance by MNCS and FSS
of Italian universities in the Sciences, we try to answer that question at
field, discipline, and overall university level. We present the descriptive
statistics of the shifts in rank, and the correlations of both scores and
ranks. The analysis reveals strong correlations in many fields but weak
correlations in others. The extent of rank shifts is never negligible: a number
of universities shift from top to non-top quartile ranks.
"
2342,"From rankings to funnel plots: the question of accounting for
  uncertainty when measuring university research performance","  The work applies the funnel plot methodology to measure and visualize
uncertainty in the research performance of Italian universities in the science
disciplines. The performance assessment is carried out at both discipline and
overall university level. The findings reveal that for most universities the
citation-based indicator used gives insufficient statistical evidence to infer
that their research productivity is inferior or superior to the average. This
general observation is one that we could indeed expect in a higher education
system that is essentially non-competitive. The question is whether the
introduction of uncertainty in performance reporting, while technically sound,
could weaken institutional motivation to work towards continuous improvement.
"
2343,A farewell to the MNCS and like size-independent indicators,"  The arguments presented demonstrate that the Mean Normalized Citation Score
(MNCS) and other size-independent indicators based on the ratio to publications
are not indicators of research performance. The article provides examples of
the distortions when rankings by MNCS are compared to those based on indicators
of productivity. The authors propose recommendations for the scientometric
community to switch to ranking by research efficiency, instead of MNCS and
other size-independent indicators.
"
2344,"The combined effects of age and seniority on research performance of
  full professors","  In this work we examine the relationship between research performance, age,
and seniority in academic rank of full professors in the Italian academic
system. Differently from a large part of the previous literature, our results
generally show a negative monotonic relationship between age and research
performance, in all the disciplines under analysis. We also highlight a
positive relationship between seniority in rank and performance, occurring
particularly in certain disciplines. While in Medicine, Biology and Chemistry
this result could be explained by the ""accumulative advantage"" effect, in other
disciplines, like Civil engineering and Pedagogy and Psychology, it could be
due to the existence of a large performance differential between young and
mature researchers, at the moment of the promotion to full professors. These
results, witnessed both generally and at the level of the individual
disciplines, offer useful insights for policy makers and academia
administrators on the role of older professors.
"
2345,"The ratio of top scientists to the academic staff as an indicator of the
  competitive strength of universities","  The ability to attract and retain talented professors is a distinctive
competence of world-class universities and a source of competitive advantage.
The ratio of top scientists to academic staff could therefore be an indicator
of the competitive strength of the universities. This work identifies the
Italian top scientists in over 200 fields, by their research productivity. It
then ranks the relative universities by the ratio of top scientists to overall
faculty. Finally, it contrasts this list with the ranking list by average
productivity of the overall faculty. The analysis is carried out at the field,
discipline, and overall university levels. The paper also explores the
secondary question of whether the ratio of top scientists to faculty is related
to the size of the university.
"
2346,"A methodology to measure the effectiveness of academic recruitment and
  turnover","  We propose a method to measure the effectiveness of the recruitment and
turnover of professors, in terms of their research performance. The method
presented is applied to the case of Italian universities over the period
2008-2012. The work then analyses the correlation between the indicators of
effectiveness used, and between the indicators and the universities' overall
research performance. In countries that conduct regular national assessment
exercises, the evaluation of effectiveness in recruitment and turnover could
complement the overall research assessments. In particular, monitoring such
parameters could assist in deterring favoritism, in countries exposed to such
practices.
"
2347,"Accounting for gender research performance differences in ranking
  universities","  The literature on the theme of gender differences in research performance
indicates a quite evident gap in favor of men over women. Beyond the
understanding of the factors that could be at the basis of this phenomenon, it
is worthwhile understanding if it would be appropriate to conduct the
evaluation per population in a manner distinguished by gender. In fact if there
is some factor that structurally determines a penalization of performance by
women researchers compared to men then the comparative evaluation of
organizations' performance that does not take gender into account will lead to
an advantage for those that employ more men, under parity in the capacities of
their staffs. In this work we measure the differences of the performance and
the rank of research institutions as observed when gender is taken into account
compared to when it is ignored. The study population consists of all Italian
universities and the performance measured in the hard sciences for the period
2006-2010.
"
2348,"Funnel plots for visualizing uncertainty in the research performance of
  institutions","  Research performance values are not certain. Performance indexes should
therefore be accompanied by uncertainty measures, to establish whether the
performance of a unit is truly outstanding and not the result of random
fluctuations. In this work we focus on the evaluation of research institutions
on the basis of average individual performance, where uncertainty is inversely
related to the number of research staff. We utilize the funnel plot, a tool
originally developed in meta-analysis, to measure and visualize the uncertainty
in the performance values of research institutions. As an illustrative example,
we apply the funnel plot to represent the uncertainty in the assessed research
performance for Italian universities active in biochemistry
"
2349,"An assessment of the first ""scientific accreditation"" for university
  appointments in Italy","  Nations with non-competitive higher education systems and with high levels of
corruption, are more exposed to phenomena of discrimination and favoritism in
faculty recruitment. Italy is a case in point, as shown by empirical studies,
judicial reports and media attention. Governments have intervened repeatedly to
reduce the problem, with scarce success. The 2010 reforms to the university
recruitment system provided that access to the ranks of associate and full
professor would now be possible only through an initial ""scientific
habilitation"" to be awarded by sectorial committees of national experts. The
objective of this work is to analyze the relationship of the recent
habilitation procedure outcomes to the actual scientific merit of the various
candidates, as well as to other variables that are explicative of possible
practices of favoritism and discrimination. The analyses identify the presence
of potential cases of discrimination and favoritism.
"
2350,"Ranking research institutions by the number of highly-cited articles per
  scientist","  In the literature and on the Web we can readily find research excellence
rankings for organizations and countries by either total number of highly-cited
articles (HCAs) or by ratio of HCAs to total publications. Neither are
indicators of efficiency. In the current work we propose an indicator of
efficiency, the number of HCAs per scientist, which can complement the
productivity indicators based on impact of total output. We apply this
indicator to measure excellence in the research of Italian universities as a
whole, and in each field and discipline of the hard sciences.
"
2351,"Evaluating university research: same performance indicator, different
  rankings","  Assessing the research performance of multi-disciplinary institutions, where
scientists belong to many fields, requires that the evaluators plan how to
aggregate the performance measures of the various fields. Two methods of
aggregation are possible. These are based on: a) the performance of the
individual scientists or b) the performance of the scientific fields present in
the institution. The appropriate choice depends on the evaluation context and
the objectives for the particular measure. The two methods bring about
differences in both the performance scores and rankings. We quantify these
differences through observation of the 2008-2012 scientific production of the
entire research staff employed in the hard sciences in Italian universities
(over 35,000 professors). Evaluators preparing an exercise must comprehend the
differences illustrated, in order to correctly select the methodologies that
will achieve the evaluation objectives.
"
2352,"Should the research performance of scientists be distinguished by
  gender?","  The literature on gender differences in research performance seems to suggest
a gap between men and women, where the former outperform the latter. Whether
one agrees with the different factors proposed to explain the phenomenon, it is
worthwhile to verify if comparing the performance within each gender, rather
than without distinction, gives significantly different ranking lists. If there
were some structural factor that determined a penalty in performance of female
researchers compared to their male peers, then under conditions of equal
capacities of men and women, any comparative evaluations of individual
performance that fail to account for gender differences would lead to
distortion of the judgments in favor of men. In this work we measure the extent
of differences in rank between the two methods of comparing performance in each
field of the hard sciences: for professors in the Italian university system, we
compare the distributions of research performance for men and women and
subsequently the ranking lists with and without distinction by gender. The
results are of interest for the optimization of efficient selection in
formulation of recruitment, career advancement and incentive schemes.
"
2353,A multivariate stochastic model to assess research performance,"  There is a worldwide trend towards application of bibliometric research
evaluation, in support of the needs of policy makers and research
administrators. However the assumptions and limitations of bibliometric
measurements suggest a probabilistic rather than the traditional deterministic
approach to the assessment of research performance. The aim of this work is to
propose a multivariate stochastic model for measuring the performance of
individual scientists and to compare the results of its application with those
arising from a deterministic approach. The dataset of the analysis covers the
scientific production indexed in Web of Science for the 2006-2010 period, of
over 900 Italian academic scientists working in two distinct fields of the life
sciences.
"
2354,How do you define and measure research productivity?,"  Productivity is the quintessential indicator of efficiency in any production
system. It seems it has become a norm in bibliometrics to define research
productivity as the number of publications per researcher, distinguishing it
from impact. In this work we operationalize the economic concept of
productivity for the specific context of research activity and show the limits
of the commonly accepted definition. We propose then a measurable form of
research productivity through the indicator ""Fractional Scientific Strength
(FSS)"", in keeping with the microeconomic theory of production. We present the
methodology for measure of FSS at various levels of analysis: individual,
field, discipline, department, institution, region and nation. Finally, we
compare the ranking lists of Italian universities by the two definitions of
research productivity.
"
2355,Assessing national strengths and weaknesses in research fields,"  National policies aimed at fostering the effectiveness of scientific systems
should be based on reliable strategic analysis identifying strengths and
weaknesses at field level. Approaches and indicators thus far proposed in the
literature have not been completely satisfactory, since they fail to
distinguish the effect of the size of production factors from that of their
quality, particularly the quality of labor. The current work proposes an
innovative ""input-oriented"" approach, which permits: i) estimation of national
research performance in a field and comparison to that of other nations,
independent of the size of their respective research staffs; and, for fields of
comparable intensity of publication, ii) identification of the strong and weak
research fields within a national research system on the basis of international
comparison. In reference to the second objective, the proposed approach is
applied to the Italian case, through the analysis of the 2006-2010 scientific
production of the Italian academic system, in the 200 research fields where
bibliometric analysis is meaningful.
"
2356,"Inefficiency in selecting products for submission to national research
  assessment exercises","  One of the critical issues in national research assessment exercises concerns
the choice of whether to evaluate the entire scientific portfolio of the
institutions or a subset composed of the best products. Under the second
option, the capacities of the institutions to select the appropriate
researchers and their best products (the UK case) or simply the best products
of every researcher (the Italian case) becomes critical, both for purposes of
correct assessment of the real quality of research in the institutions
evaluated, and for the selective funding that follows. In this work, through
case studies of three Italian universities, we analyze the efficiency of the
product selection that is intended to maximize the universities' scores in the
current national research assessment exercise, the results of which will be the
basis for assigning an important share of public financing over the coming
years.
"
2357,"Variability of research performance across disciplines within
  universities in non-competitive higher education systems","  Many nations are adopting higher education strategies that emphasize the
development of elite universities able to compete at the international level in
the attraction of skills and resources. Elite universities pursue excellence in
all their disciplines and fields of action. The impression is that this does
not occur in ""non-competitive"" education systems, and that instead, within
single universities excellent disciplines will coexist with mediocre ones. To
test this, the authors measure research productivity in the hard sciences for
all Italian universities over the period 2004-2008 at the levels of the
institution, their individual disciplines and fields within them. The results
show that the distribution of excellent disciplines is not concentrated in a
few universities: top universities show disciplines and fields that are often
mediocre, while generally mediocre universities will often include top
disciplines.
"
2358,Are the authors of highly cited articles also the most productive ones?,"  Ever more frequently, governments have decided to implement policy measures
intended to foster and reward excellence in scientific research. This is in
fact the intended purpose of national research assessment exercises. These are
typically based on the analysis of the quality of the best research products;
however a different approach to analysis and intervention is based on the
measure of productivity of the individual scientists, meaning the overall
impact of their entire scientific production over the period under observation.
This work analyzes the convergence of the two approaches, asking if and to what
measure the most productive scientists achieve highly-cited articles; or vice
versa, what share of highly-cited articles is achieved by scientists that are
""non-top"" for productivity. To do this we use bibliometric indicators, applied
to the 2004-2008 publications authored by academics of Italian universities and
indexed in the Web of Science.
"
2359,"The suitability of h and g indexes for measuring the research
  performance of institutions. Scientometrics, 97(3), 555-570","  It is becoming ever more common to use bibliometric indicators to evaluate
the performance of research institutions, however there is often a failure to
recognize the limits and drawbacks of such indicators. Since performance
measurement is aimed at supporting critical decisions by research
administrators and policy makers, it is essential to carry out empirical
testing of the robustness of the indicators used. In this work we examine the
accuracy of the popular ""h"" and ""g"" indexes for measuring university research
performance by comparing the ranking lists derived from their application to
the ranking list from a third indicator that better meets the requirements for
robust and reliable assessment of institutional productivity. The test
population is all Italian universities in the hard sciences, observed over the
period 2001-2005. The analysis quantifies the correlations between the three
university rankings (by discipline) and the shifts that occur under changing
indicators, to measure the distortion inherent in use of the h and g indexes
and their comparative accuracy for assessing institutions.
"
2360,"Measuring institutional research productivity for the life sciences: the
  importance of accounting for the order of authors in the byline","  Accurate measurement of institutional research productivity should account
for the real contribution of the research staff to the output produced in
collaboration with other organizations. In the framework of bibliometric
measurement, this implies accounting for both the number of co-authors and each
individual's real contribution to scientific publications. Common practice in
the life sciences is to indicate such contribution through the order of author
names in the byline. In this work, we measure the distortion introduced to
university-level bibliometric productivity rankings when the number of
co-authors or their position in the byline is ignored. The field of observation
consists of all Italian universities active in the life sciences (Biology and
Medicine). The analysis is based on the research output of the university staff
over the period 2004-2008. Based on the results, we recommend against the use
of bibliometric indicators that ignore co-authorship and real contribution of
each author to research outputs.
"
2361,"Bibliometrics-based heuristics: What is their definition and how can
  they be studied?","  When scientists study the phenomena they are interested in, they apply sound
methods and base their work on theoretical considerations. In contrast, when
the fruits of their research is being evaluated, basic scientific standards do
not seem to matter. Instead, simplistic bibliometric indicators (i.e.,
publications and citation counts) are, paradoxically, both widely used and
criticized without any methodological and theoretical framework that would
serve to ground both use and critique. Yet, Bornmann and Marewski [1] proposed
such a framework recently. They developed bibliometrics-based heuristics (BBHs)
based on the fast-and-frugal heuristics approach [2] to decision making, in
order to conceptually understand and empirically investigate the quantitative
evaluation of research as well as to effectively train end-users of
bibliometrics (e.g., science managers, scientists). Heuristics are decision
strategies that use part of the available information and ignore the rest. By
exploiting the statistical structure of task environments, they can aid to make
accurate, fast, effortless, and cost-efficient decisions without that
trade-offs are incurred. Because of their simplicity, heuristics are easy to
understand and communicate, enhancing the transparency of decision processes.
In this commentary, we explain several BBHs and discuss how such heuristics can
be employed in practice (using the evaluation of applicants for funding
programs as one example). Furthermore, we outline why heuristics can perform
well, and how they and their fit to task environments can be studied. In
pointing to the potential of research on BBHs and to the risks that come with
an under-researched, mindless usage of bibliometrics, this commentary
contributes to make research evaluation more scientific.
"
2362,A Large-scale Study of Social Media Sources in News Articles,"  In this study, we closely look at the use of social media contents as source
or reference in the U.S. news media. Specifically, we examine about 60 thousand
news articles published within the 5 years period of 2013-2017 by 153 U.S.
media outlets and analyze use of social media content as source compared to
other sources. We designed a social media source extraction algorithm and
investigated the extent and nature of social media source usage across
different news topics. Our results show that uses of social media content in
news almost doubled in five years. Unreliable media outlets rely on social
media more than the mainstream media. Both mainstream and unreliable sites
prefer Twitter to Facebook as a source of information.
"
2363,"Solving multiple-criteria R&D project selection problems with a
  data-driven evidential reasoning rule","  In this paper, a likelihood based evidence acquisition approach is proposed
to acquire evidence from experts'assessments as recorded in historical
datasets. Then a data-driven evidential reasoning rule based model is
introduced to R&D project selection process by combining multiple pieces of
evidence with different weights and reliabilities. As a result, the total
belief degrees and the overall performance can be generated for ranking and
selecting projects. Finally, a case study on the R&D project selection for the
National Science Foundation of China is conducted to show the effectiveness of
the proposed model. The data-driven evidential reasoning rule based model for
project evaluation and selection (1) utilizes experimental data to represent
experts' assessments by using belief distributions over the set of final
funding outcomes, and through this historic statistics it helps experts and
applicants to understand the funding probability to a given assessment grade,
(2) implies the mapping relationships between the evaluation grades and the
final funding outcomes by using historical data, and (3) provides a way to make
fair decisions by taking experts' reliabilities into account. In the
data-driven evidential reasoning rule based model, experts play different roles
in accordance with their reliabilities which are determined by their previous
review track records, and the selection process is made interpretable and
fairer. The newly proposed model reduces the time-consuming panel review work
for both managers and experts, and significantly improves the efficiency and
quality of project selection process. Although the model is demonstrated for
project selection in the NSFC, it can be generalized to other funding agencies
or industries.
"
2364,"When research assessment exercises leave room for opportunistic behavior
  by the subjects under evaluation","  This study inserts in the stream of research on the perverse effects that
PBRF systems can induce in the subjects evaluated. The authors' opinion is that
more often than not, it is the doubtful scientific basis of the evaluation
criteria that leave room for opportunistic behaviors. The work examines the
2004-2010 Italian national research assessment (VQR) to verify possible
opportunistic behavior by universities in order to limit the penalization of
their performance (and funding) due to the presence of scientifically
unproductive professors in faculty. In particular, institutions may have
favored ""gift authorship"" practices. The analysis thus focuses on the output of
professors who were unproductive in the VQR publication window, but became
productive (""new productives"") in the following five years: a number of
universities show a remarkably higher than average share of publications by new
productives that are in co-authorship exclusively with colleagues from the same
university.
"
2365,Career advancement and scientific performance in universities,"  Many governments have placed priority on excellence in higher education as
part of their policy agendas. Processes for recruitment and career advancement
in universities thus have a critical role. The efficiency of faculty selection
processes can be evaluated by comparing the subsequent performance of
competition winners against that of the losers and the pre-existing staff of
equal academic rank. Our study presents an empirical analysis concerning the
recruitment procedures for associate professors in the Italian university
system. The results of a bibliometric analysis of the hard science areas reveal
that new associate professors are on average more productive than the
incumbents. However a number of crucial concerns emerge, in particular
concerning occurrence of non-winner candidates that are more productive than
the winners over the subsequent triennium, and cases of winners that are
completely unproductive. Beyond the implications for the Italian case, the
analysis offers considerations for all decision-makers regarding the ex post
evaluation of the efficiency of the recruitment process and the desirability of
providing selection committees with bibliometric indicators in support of
evaluation (i.e. informed peer review).
"
2366,Relatives in the same university faculty: nepotism or merit?,"  In many countries culture, practice or regulations inhibit the co-presence of
relatives within the university faculty. We test the legitimacy of such
attitudes and provisions, investigating the phenomenon of nepotism in Italy, a
nation with high rates of favoritism. We compare the individual research
performance of ""children"" who have ""parents"" in the same university against
that of the ""non-children"" with the same academic rank and seniority, in the
same field. The results show non-significant differences in performance.
Analyses of career advancement show that children's research performance is on
average superior to that of their colleagues who did not advance. The study's
findings do not rule out the existence of nepotism, which has been actually
recorded in a low percentage of cases, but do not prove either the most serious
presumed consequences of nepotism, namely that relatives who are poor
performers are getting ahead of non-relatives who are better performers. In
light of these results, many attitudes and norms concerning parental ties in
academia should be reconsidered.
"
2367,Selection committees for academic recruitment: does gender matter?,"  Underrepresentation of women in the academic system is a problem common to
many countries, often associated with gender discrimination. In the Italian
academic context in particular, favoritism is recognized as a diffuse
phenomenon affecting hiring and career advancement. One of the questions that
naturally arises is whether women who do assume decisional roles, having
witnessed other phenomena of discrimination, would practice less favoritism
than men in similar positions. Our analysis refers to the particular case of
favoritism in the work of university selection committees responsible for
career advancement. We observe a moderate positive association between
competitions with expected outcomes and the fact the committee president is a
woman. Although committees presided by women give more weight to scientific
merit than those presided by men, favoritism still occurs. In fact, in the case
the committee president is a woman, the single most important factor for the
success of a candidate is joint research with the president; while in the case
of male presidents, it is the years together in the same university.
"
2368,"National peer-review research assessment exercises for the hard sciences
  can be a complete waste of money: the Italian case","  There has been ample demonstration that bibliometrics is superior to
peer-review for national research assessment exercises in the hard sciences. In
this paper we examine the Italian case, taking the 2001-2003 university
performance rankings list based on bibliometrics as benchmark. We compare the
accuracy of the first national evaluation exercise, conducted entirely by
peer-review, to other rankings lists prepared at zero cost, based on indicators
indirectly linked to performance or available on the Internet. The results show
that, for the hard sciences, the costs of conducting the Italian evaluation of
research institutions could have been completely avoided.
"
2369,"The importance of accounting for the number of co-authors and their
  order when assessing research performance at the individual level in the life
  sciences","  Accurate measurement of research productivity should take account of both the
number of co-authors of every scientific work and of the different
contributions of the individuals. For researchers in the life sciences, common
practice is to indicate such contributions through position in the authors
list. In this work, we measure the distortion introduced to bibliometric
ranking lists for scientific productivity when the number of co-authors or
their position in the list is ignored. The field of observation consists of all
Italian university professors working in the life sciences, with scientific
production examined over the period 2004-2008. The outcomes of the study lead
to a recommendation against using indicators or evaluation methods that ignore
the different authors' contributions to the research results.
"
2370,"The impact of unproductive and top researchers on overall university
  research performance","  Unlike competitive higher education systems, non-competitive systems show
relatively uniform distributions of top professors and low performers among
universities. In this study, we examine the impact of unproductive and top
faculty members on overall research performance of the university they belong
to. Furthermore, we analyze the potential relationship between research
productivity of a university and the indexes of concentration of unproductive
and top professors. Research performance is evaluated using a bibliometric
approach, through publications indexed on the Web of Science between 2004 and
2008. The set analyzed consists of all Italian universities active in the hard
sciences.
"
2371,"How important is choice of the scaling factor in standardizing
  citations?","  Because of the variations in citation behavior across research fields,
appropriate standardization must be applied as part of any bibliometric
analysis of the productivity of individual scientists and research
organizations. Such standardization involves scaling by some factor that
characterizes the distribution of the citations of articles from the same year
and subject category. In this work we conduct an analysis of the sensitivity of
researchers' productivity rankings to the scaling factor chosen to standardize
their citations. To do this we first prepare the productivity rankings for all
researchers (more than 30,000) operating in the hard sciences in Italy, over
the period 2004-2008. We then measure the shifts in rankings caused by adopting
scaling factors other than the particular factor that seems more effective for
comparing the impact of publications in different fields: the citation average
of the distribution of cited-only publications.
"
2372,"The VQR, Italy's second national research assessment: Methodological
  failures and ranking distortions","  The 2004-2010 VQR, completed in July 2013, was Italy's second national
research assessment exercise. The VQR performance evaluation followed a pattern
also seen in other nations, in being based on a selected subset of products. In
this work we identify the exercise's methodological weaknesses and measure the
distortions that result from them in the university performance rankings. First
we create a scenario in which we assume the efficient selection of the products
to be submitted by the universities and from this simulate a set of rankings
applying the precise VQR rating criteria. Next we compare these ""VQR rankings""
with those that would derive from application of more appropriate
bibliometrics. Finally we extend the comparison to university rankings based on
the entire scientific production for the period, as indexed in the Web of
Science.
"
2373,"The relationship between the number of authors of a publication, its
  citations and the impact factor of the publishing journal: Evidence from
  Italy","  Empirical evidence shows that co-authored publications achieve higher
visibility and impact. The aim of the current work is to test for the existence
of a similar correlation for Italian publications. We also verify if such
correlation differs: i) by subject category and macro-area; ii) by document
type; iii) over the course of time. The results confirm world-level evidence,
showing a consistent and significant linear growth in the citability of a
publication with number of co-authors, in almost all subject categories. The
effects are more remarkable in the fields of Social Sciences and Art &
Humanities than in the Sciences-a finding not so obvious scrutinizing previous
studies. Moreover, our results partly disavow the positive association between
number of authors and prestige of the journal, as measured by its impact
factor.
"
2374,"A methodology to compute the territorial productivity of scientists: The
  case of Italy","  Policy-makers working at the national and regional levels could find the
territorial mapping of research productivity by field to be useful in informing
both research and industrial policy. Research-based private companies could
also use such mapping for efficient selection in localizing R&D activities and
university research collaborations. In this work we apply a bibliometric
methodology for ranking by research productivity: i) the fields of research for
each territory (region and province); and ii) the territories for each
scientific field. The analysis is based on the 2008-2012 scientific output
indexed in the Web of Science, by all professors on staff at Italian
universities. The population is over 36,000 professors, active in 192 fields
and 9 disciplines.
"
2375,A new approach to measure the scientific strengths of territories,"  The current work applies a methodology for mapping the supply of new
knowledge from public research organizations, in this case from Italian
institutions at the level of regions and provinces (NUTS2 and NUTS3). Through
the analysis of scientific production indexed in the Web of Science for the
period 2006-2010, the new knowledge is classified in subject categories and
mapped according to an algorithm for the reconciliation of authors'
affiliations. Unlike other studies in the literature based on simple counting
of publications, the present study adopts an indicator, Scientific Strength,
which takes account of both the quantity of scientific production and its
impact on the advancement of knowledge. The differences in the results that
arise from the two approaches are examined. The results of works of this kind
can inform public research policies, at national and local levels, as well as
the localization strategies of research-based companies.
"
2376,"The spin-off of elite universities in non-competitive, undifferentiated
  higher education systems: an empirical simulation in Italy","  Higher education systems featuring intense competition have developed
world-class universities, capable of attracting top professors and students and
considerable public-private funding. This does not occur in non-competitive
systems, where highly-talented faculty and students are dispersed across all
institutions. In such systems, the authors propose the budding of spin-off
universities, staffed by migration of top scientists from the entire public
research system. This work illustrate the proposal through an example: the
spin-off of a new university in Rome-Italy staffed with the best professors
from the three current public city universities. Such a faculty would offer top
national research productivity, a magnet to attract the other critical
ingredients of a world-class university: talented students, abundant resources
and visionary governance.
"
2377,Variation in research collaboration patterns across academic ranks,"  The ability to activate and manage effective collaborations is becoming an
increasingly important criteria in policies on academic career advancement. The
rise of such policies leads to development of indicators that permit
measurement of the propensity to collaborate for academics of different ranks,
and to examine the role of several variables in collaboration, first among
these being the researchers' disciplines. In this work we apply an innovative
bibliometric approach based on individual propensity for collaboration to
measure the differences in propensity across academic ranks, by discipline and
for choice of collaboration forms - intramural, extramural domestic and
international. The analysis is based on the scientific production of Italian
academics for the period 2006 to 2010, totaling over 200,000 publications
indexed in Web of Science. It shows that assistant professors register a
propensity for intramural collaboration that is clearly greater than for
professors of higher ranks. Vice versa, the higher ranks, but not quite so
clearly, register greater propensity to collaborate at the international level.
"
2378,Gender differences in research collaboration,"  The debate on the role of women in the academic world has focused on various
phenomena that could be at the root of the gender gap seen in many nations.
However, in spite of the ever more collaborative character of scientific
research, the issue of gender aspects in research collaborations has been
treated in a marginal manner. In this article we apply an innovative
bibliometric approach based on the propensity for collaboration by individual
academics, which permits measurement of gender differences in the propensity to
collaborate by fields, disciplines and forms of collaboration: intramural,
extramural domestic and international. The analysis of the scientific
production of Italian academics shows that women researchers register a greater
capacity to collaborate in all the forms analyzed, with the exception of
international collaboration, where there is still a gap in comparison to male
colleagues.
"
2379,"Selecting competent referees to assess research projects proposals: a
  study of referees' registers","  The selection of referees for evaluation of research projects under
competitive financing appears particularly critical: the greater the competence
of the referee concerning the core topic, the more their judgment is
trustworthy and the more the financing of the best proposals is probable. The
current work analyzes registers of experts used to select referees for the
evaluation of research proposals in public programs with competitive funding.
The work has the objective to present a methodology to verify the degree of
""coverage"" of the register compared to the spectrum of competencies necessary
for the evaluation of such wide-ranging national programs; and to evaluate the
level of scientific performance of the register's referees in the hard
sciences, compared to the their national colleagues from the same fields.
"
2380,"What is the appropriate length of the publication period over which to
  assess research performance?","  National research assessment exercises are conducted in different nations
over varying periods. The choice of the publication period to be observed has
to address often contrasting needs: it has to ensure the reliability of the
results issuing from the evaluation, but also reach the achievement of frequent
assessments. In this work we attempt to identify which is the most appropriate
or optimal publication period to be observed. For this, we analyze the
variation of individual researchers' productivity rankings with the length of
the publication period within the period 2003-2008, by the over 30,000 Italian
university scientists in the hard sciences. First we analyze the variation in
rankings referring to pairs of contiguous and overlapping publication periods,
and show that the variations reduce markedly with periods above three years.
Then we will show the strong randomness of performance rankings over
publication periods under three years. We conclude that the choice of a three
year publication period would seem reliable, particularly for physics,
chemistry, biology and medicine.
"
2381,Revisiting the scaling of citations for research assessment,"  Over the past decade, national research evaluation exercises, traditionally
conducted using the peer review method, have begun opening to bibliometric
indicators. The citations received by a publication are assumed as proxy for
its quality, but they require standardization prior to use in comparative
evaluation of organizations or individual scientists: the citation data must be
standardized, due to the varying citation behavior across research fields. The
objective of this paper is to compare the effectiveness of the different
methods of normalizing citations, in order to provide useful indications to
research assessment practitioners. Simulating a typical national research
assessment exercise, he analysis is conducted for all subject categories in the
hard sciences and is based on the Thomson Reuters Science Citation
Index-Expanded. Comparisons show that the citations average is the most
effective scaling parameter, when the average is based only on the publications
actually cited.
"
2382,"An individual-level assessment of the relationship between spin-off
  activities and research performance in universities","  One of the most problematic aspects in the creation of spin-offs by
university personnel concerns the relationship between entrepreneurial activity
and research activity by researcher-entrepreneurs. The literature has expressed
varying and opposing views as to the nature of the relationship but very little
has been produced to empirically legitimate one position or another. The
present work proposes to address this shortcoming by exploring the relationship
existing between academic spin-off generation and the research performance of
enterprise founders. The study investigates whether, and to what extent,
scientific performance by academic entrepreneurs is different than that of
their colleagues, and if the involvement in entrepreneurial activity has an
influence on the individual's research activity. The research questions are
answered by considering all spin-offs generated by Italian universities over
the period 2001-2008 and evaluating, through a bibliometric approach, the
scientific performance of founders relative to that of their colleagues who
carry out research in the same field. The data show better scientific
performance by the researcher-entrepreneurs than that of their colleagues and
in addition, although there are some variations across fields, the creation of
a spin-off does not seem, on average, to have negative effects on the
scientific performance of the founders.
"
2383,"A bibliometric tool to assess the regional dimension of
  university-industry research collaborations","  The present work proposes a bibliometric methodology for measuring the grade
of correspondence between regional industry's demand for research collaboration
and supply from public laboratories. The methodology also permits measurement
of the intensity and direction of the regional flows of knowledge in
public-private collaborations. The aim is to provide a diagnostic instrument
for regional and national policy makers, which could add to existing ones to
plan interventions for re-balancing sectorial public supply of knowledge with
industrial absorptive capacity, and maximizing appropriability of knowledge
spillovers. The methodology is applied to university-industry collaborations in
the hard sciences in all Italian administrative regions.
"
2384,Interdisciplinarity: A Nobel Opportunity,"  Interdisciplinary collaborations now sweep most fields of the natural and
life sciences, necessary to tackle the world's most challenging problems. Yet,
the scientific enterprise continues to be dominated by old stereotypes:
Interdisciplinary science is less likely to receive funding and is
discriminated at institutional levels. Ample solutions for funders,
institutions and publishers have been suggested, but the most visible form of
scientific credit has so far been ignored: How interdisciplinary is our award
system? To address this question, we explore interdisciplinarity in the most
prestigious award in science, the Nobel Prize. We document a tendency of Nobel
Prizes to neglect interdisciplinary discoveries, especially between physics and
the life sciences. Given the increased growth of interdisciplinary high-impact
research over the last three decades, we have reached the critical point in
time where the issue of recognizing outstanding interdisciplinary research has
become truly pressing.
"
2385,Exploring Direct Citations between Citing Publications,"  This paper defines and explores the direct citations between citing
publications (DCCPs) of a publication. We construct an ego-centered citation
network for each paper that contains all of its citing papers and itself, as
well as the citation relationships among them. By utilizing a large-scale
scholarly dataset from the computer science field in the Microsoft Academic
Graph (MAG-CS) dataset, we find that DCCPs exist universally in medium and
highly cited papers. For those papers that have DCCPs, DCCPs do occur
frequently; highly cited papers tend to contain more DCCPs than others.
Meanwhile, the number of DCCPs of papers published in different years does not
vary dramatically. The current paper also discusses the relationship between
DCCPs and some indirect citation relationships (e.g., co-citation and
bibliographic coupling).
"
2386,"Unsupervised Identification of Study Descriptors in Toxicology Research:
  An Experimental Study","  Identifying and extracting data elements such as study descriptors in
publication full texts is a critical yet manual and labor-intensive step
required in a number of tasks. In this paper we address the question of
identifying data elements in an unsupervised manner. Specifically, provided a
set of criteria describing specific study parameters, such as species, route of
administration, and dosing regimen, we develop an unsupervised approach to
identify text segments (sentences) relevant to the criteria. A binary
classifier trained to identify publications that met the criteria performs
better when trained on the candidate sentences than when trained on sentences
randomly picked from the text, supporting the intuition that our method is able
to accurately identify study descriptors.
"
2387,"Testing Reviewer Suggestions Derived from Bibliometric Specialty
  Approximations in Real Research Evaluations","  Many contemporary research funding instruments and research policies aim for
excellence at the level of individual scientists, teams or research programmes.
Good bibliometric approximations of related specialties could be useful for
instance to help assign reviewers to applications. This paper reports findings
on the usability of reviewer suggestions derived from a recently developed
specialty approximation method combining key sources, title words, authors and
references (Rons, 2018). Reviewer suggestions for applications for Senior
Research Fellowships were made available to the evaluation coordinators. Those
who were invited to review an application showed a normal acceptance rate, and
responses from experts and coordinators contained no indications of mismatched
scientific focus. The results confirm earlier indications that this specialty
approximation method can successfully support tasks in research management.
"
2388,Bibliometric evaluation of research performance: where do we stand?,"  This work provides a critical examination of the most popular bibliometric
indicators and methodologies to assess the research performance of individuals
and institutions. The aim is to raise the fog and make practitioners more aware
of the inherent risks in do-it-myself practices, or cozy out-of-the-shelf
solutions to the difficult question of how to evaluate research. The manuscript
also proposes what we believe is the correct approach to bibliometric
evaluation of research performance.
"
2389,"Revisiting the scientometric conceptualization of impact and its
  measurement","  The development of scientometric indicators and methods for evaluative
purposes, requires a multitude of assumptions, conventions, limitations, and
caveats. Given this, we cannot permit ambiguities in the key concepts forming
the basis of scientometric science itself, or research assessment exercises
would rest on quicksand. This conceptual work attempts to spell out some
principles leading to a clear definition of ""impact"" of research, and above
all, of the appropriate scientometric indicator to measure it. The aim is to
stimulate a discussion aimed at a definitive convergence on the meaning and
measurement of a fundamental concept of the scientometric science.
"
2390,"Identifying influential patents in citation networks using enhanced
  VoteRank centrality","  This study proposes the usage of a method called VoteRank, created by Zhang
et al. (2016), to identify influential nodes on patent citation networks. In
addition, it proposes enhanced VoteRank algorithms, extending the Zhang et al.
work. These novel algorithms comprise a reduction on the voting ability of the
nodes affected by a chosen spreader if the nodes are distant from the spreader.
One method uses a reduction factor that is linear regarding the distance from
the spreader, which we called VoteRank-LRed. The other method uses a reduction
factor that is exponential concerning the distance from the spreader, which we
called VoteRank-XRed. By applying the methods to a citation network, we were
able to demonstrate that VoteRank-LRed improved performance in choosing
influence spreaders more efficiently than the original VoteRank on the tested
citation network.
"
2391,Revisiting size effects in higher education research productivity,"  The potential occurrence of variable returns to size in research activity is
a factor to be considered in choices about the size of research organizations
and also in the planning of national research assessment exercises, so as to
avoid favoring those organizations that would benefit from such occurrence. The
aim of the current work is to improve on weaknesses in past inquiries
concerning returns to size through application of a research productivity
measurement methodology that is more accurate and robust. The method involves
field-standardized measurements that are free of the typical distortions of
aggregate measurement by discipline or organization. The analysis is conducted
for 183 hard science fields in all 77 Italian universities (time period
2004-2008) and allows detection of potential differences by field.
"
2392,"A sensitivity analysis of researchers' productivity rankings to the time
  of citation observation","  In this work we investigate the sensitivity of individual researchers'
productivity rankings to the time of citation observation. The analysis is
based on observation of research products for the 2001-2003 triennium for all
research staff of Italian universities in the hard sciences, with the year of
citation observation varying from 2004 to 2008. The 2008 rankings list is
assumed the most accurate, as citations have had the longest time to accumulate
and thus represent the best possible proxy of impact. By comparing the rankings
lists from each year against the 2008 benchmark we provide policy-makers and
research organization managers a measure of trade-off between timeliness of
evaluation execution and accuracy of performance rankings. The results show
that with variation in the evaluation citation window there are variable rates
of inaccuracy across the disciplines of researchers. The inaccuracy results
negligible for Physics, Biology and Medicine.
"
2393,"The dispersion of research performance within and between universities
  as a potential indicator of the competitive intensity in higher education
  systems","  Higher education systems in competitive environments generally present top
universities, that are able to attract top scientists, top students and public
and private financing, with notable socio-economic benefits in their region.
The same does not hold true for non-competitive systems. In this study we will
measure the dispersion of research performance within and between universities
in the Italian university system, typically non-competitive. We will also
investigate the level of correlation that occurs between performance in
research and its dispersion in universities. The findings may represent a first
benchmark for similar studies in other nations. Furthermore, they lead to
policy indications, questioning the effectiveness of selective funding of
universities based on national research assessment exercises. The field of
observation is composed of all Italian universities active in the hard
sciences. Research performance will be evaluated using a bibliometric approach,
through publications indexed in the Web of Science between 2004 and 2008.
"
2394,"A sensitivity analysis of research institutions' productivity rankings
  to the time of citation observation","  One of the critical issues in bibliometric research assessments is the time
required to achieve maturity in citations. Citation counts can be considered a
reliable proxy of the real impact of a work only if they are observed after
sufficient time has passed from publication date. In the present work the
authors investigate the effect of varying the time of citation observation on
accuracy of productivity rankings for research institutions. Research
productivity measures are calculated for all Italian universities active in the
hard sciences in the 2001-2003 period, by individual field and discipline, with
the time of the citation observation varying from 2004 to 2008. The objective
is to support policy-makers in choosing a citation window that optimizes the
tradeoff between accuracy of rankings and timeliness of the exercise.
"
2395,"National research assessment exercises: a comparison of peer review and
  bibliometrics rankings","  Development of bibliometric techniques has reached such a level as to suggest
their integration or total substitution for classic peer review in the national
research assessment exercises, as far as the hard sciences are concerned. In
this work we compare rankings lists of universities captured by the first
Italian evaluation exercise, through peer review, with the results of
bibliometric simulations. The comparison shows the great differences between
peer review and bibliometric rankings for excellence and productivity.
"
2396,"Assessing the varying level of impact measurement accuracy as a function
  of the citation window length","  With the passage of more time from the original date of publication, the
measure of the impact of scientific works using subsequent citation counts
becomes more accurate. However the measurement of individual and organizational
research productivity should ideally refer to a period with closing date just
prior to the evaluation exercise. Therefore it is necessary to compromise
between accuracy and timeliness. This work attempts to provide an order of
magnitude for the error in measurement that occurs with decreasing the time
lapse between date of publication and citation count. The analysis is conducted
by scientific discipline on the basis of publications indexed in the Thomson
Reuters Italian National Citation Report.
"
2397,"A field-standardized application of DEA to national-scale research
  assessment of universities","  The current work proposes an application of DEA methodology for measurement
of technical and allocative efficiency of university research activity. The
analysis is based on bibliometric data from the Italian university system for
the five year period 2004-2008. Technical and allocative efficiency is measured
with input being considered as a university's research staff, classified
according to academic rank, and with output considered as the
field-standardized impact of the research product realized by these staff. The
analysis is applied to all scientific disciplines of the so-called hard
sciences, and conducted at subfield level, thus at a greater level of detail
than ever before achieved in national-scale research assessments.
"
2398,"Research productivity: are higher academic ranks more productive than
  lower ones?","  This work analyses the links between individual research performance and
academic rank. A typical bibliometric methodology is used to study the
performance of all Italian university researchers active in the hard sciences,
for the period 2004-2008. The objective is to characterize the performance of
the ranks of full, associate and assistant professors, along various
dimensions, in order to verify the existence of performance differences among
the ranks in general and for single disciplines.
"
2399,"The field-standardized average impact of national research systems
  compared to world average: the case of Italy","  The study presents a time-series analysis of field-standardized average
impact of Italian research compared to the world average. The approach is
purely bibliometric, based on census of the full scientific production from all
Italian public research organizations active in 2001-2006 (hard sciences only).
The analysis is conducted both at sectorial level (aggregated, by scientific
discipline and for single fields within disciplines) and at organizational
level (by type of organization and for single organizations). The essence of
the methodology should be replicable in all other national contexts. Its offers
support to policy-makers and administrators for strategic analysis aimed at
identifying strengths and weaknesses of national research systems and
institutions.
"
2400,"National research assessment exercises: the effects of changing the
  rules of the game during the game","  National research evaluation exercises provide a comparative measure of
research performance of the nation's institutions, and as such represent a tool
for stimulating research productivity, particularly if the results are used to
inform selective funding by government. While a school of thought welcomes
frequent changes in evaluation criteria in order to prevent the subjects
evaluated from adopting opportunistic behaviors, it is evident that the ""rules
of the game"" should above all be functional towards policy objectives, and
therefore be known with adequate forewarning prior to the evaluation period.
Otherwise, the risk is that policy-makers will find themselves faced by a
dilemma: should they reward universities that responded best to the criteria in
effect at the outset of the observation period or those that result as best
according to rules that emerged during or after the observation period? This
study verifies if and to what extent some universities are penalized instead of
rewarded for good behavior, in pursuit of the objectives of the ""known"" rules
of the game, by comparing the research performances of Italian universities for
the period of the nation's next evaluation exercise (2004-2008): first as
measured according to criteria available at the outset of the period and next
according to those announced at the end of the period.
"
2401,"University-industry research collaboration: a model to assess university
  capability","  Scholars and policy makers recognize that collaboration between industry and
the public research institutions is a necessity for innovation and national
economic development. This work presents an econometric model which expresses
the university capability for collaboration with industry as a function of
size, location and research quality. The field of observation is made of the
census of 2001-2003 scientific articles in the hard sciences, co-authored by
universities and private enterprises located in Italy. The analysis shows that
research quality of universities has an impact higher than geographic distance
on the capability for collaborating with industry. The model proposed and the
measures that descend from it are suited for use at various levels of
administration, to assist in realizing the ""third role"" of universities: the
contribution to socio-economic development through public to private technology
transfer.
"
2402,Evaluating research: from informed peer review to bibliometrics,"  National research assessment exercises are becoming regular events in ever
more countries. The present work contrasts the peer-review and bibliometrics
approaches in the conduct of these exercises. The comparison is conducted in
terms of the essential parameters of any measurement system: accuracy,
robustness, validity, functionality, time and costs. Empirical evidence shows
that for the natural and formal sciences, the bibliometric methodology is by
far preferable to peer-review. Setting up national databases of publications by
individual authors, derived from Web of Science or Scopus databases, would
allow much better, cheaper and more frequent national research assessments.
"
2403,"The dangers of performance-based research funding in non-competitive
  higher education systems","  An increasing number of nations allocate public funds to research
institutions on the basis of rankings obtained from national evaluation
exercises. Therefore, in non-competitive higher education systems where top
scientists are dispersed among all the universities, rather than concentrated
among a few, there is a high risk of penalizing those top scientists who work
in lower-performance universities. Using a five-year bibliometric analysis
conducted on all Italian universities active in the hard sciences from
2004-2008, this work analyzes the distribution of publications and relevant
citations by scientists within the universities, measures the research
performance of individual scientists, quantifies the intensity of concentration
of top scientists at each university, provides performance rankings for the
universities, and indicates the effects of selective funding on the top
scientists of low-ranked universities.
"
2404,A national-scale cross-time analysis of university research performance,"  Research policies in the more developed nations are ever more oriented
towards the introduction of productivity incentives and competition mechanisms
intended to increase efficiency in research institutions. Assessments of the
effects of these policy interventions on public research activity often neglect
the normal, inherent variation in the performance of research institutions over
time. In this work, we propose a cross-time bibliometric analysis of research
performance by all Italian universities in two consecutive periods (2001-2003
and 2004-2008) not affected by national policy interventions. Findings show
that productivity and impact increased at the level of individual scientists.
At the level of university, significant variation in the rank was observed.
"
2405,"The relationship between scientists' research performance and the degree
  of internationalization of their research","  Policy makers, at various levels of governance, generally encourage the
development of research collaboration. However the underlying determinants of
collaboration are not completely clear. In particular, the literature lacks
studies that, taking the individual researcher as the unit of analysis, attempt
to understand if and to what extent the researcher's scientific performance
might impact on his/her degree of collaboration with foreign colleagues. The
current work examines the international collaborations of Italian university
researchers for the period 2001-2005, and puts them in relation to each
individual's research performance. The results of the investigation, which
assumes co-authorship as proxy of research collaboration, show that both
research productivity and average quality of output have positive effects on
the degree of international collaboration achieved by a scientist.
"
2406,"The role of information asymmetry in the market for university-industry
  research collaboration","  This study concerns the market for research collaboration between industry
and universities. It presents an analysis of the population of all Italian
university-industry collaborations that resulted in at least one international
scientific publication between 2001 and 2003. Using spatial and bibliometric
analysis relating to scientific output of university researchers, the study
shows the importance of geographic proximity in companies' choices of
university partner. The analysis also reveals inefficiency in the market: in a
large proportion of cases private companies could have chosen more qualified
research partners in universities located closer to the place of business.
"
2407,National-scale research performance assessment at the individual level,"  There is an evident and rapid trend towards the adoption of evaluation
exercises for national research systems for purposes, among others, of
improving allocative efficiency in public funding of individual institutions.
However the desired macroeconomic aims could be compromised if internal
redistribution of government resources within each research institution does
not follow a consistent logic: the intended effects of national evaluation
systems can result only if a ""funds for quality"" rule is followed at all levels
of decision-making. The objective of this study is to propose a bibliometric
methodology for: i) large-scale comparative evaluation of research performance
by individual scientists, research groups and departments within research
institution, to inform selective funding allocations, and ii) assessment of
strengths and weaknesses by field of research, to inform strategic planning and
control. The proposed methodology has been applied to the hard science
disciplines of the Italian university research system for the period 2004-2006.
"
2408,"Are researchers that collaborate more at the international level top
  performers? An investigation on the Italian university system","  The practice of collaboration, and particularly international collaboration,
is becoming ever more widespread in scientific research, and is likewise
receiving greater interest and stimulus from policy-makers. However, the
relation between research performance and degree of internationalization at the
level of single researchers still presents unresolved questions. The present
work, through a bibliometric analysis of the entire Italian university
population working in the hard sciences over the period 2001-2005, seeks to
answer some of these questions. The results show that the researchers with top
performance with respect to their national colleagues are also those who
collaborate more abroad, but that the reverse is not always true. Also,
interesting differences emerge at the sectorial level. Finally, the effect of
the nation involved in the international partnership plays a role that should
not be ignored.
"
2409,"Peer review research assessment: a sensitivity analysis of performance
  rankings to the share of research product evaluated","  In national research assessment exercises that take the peer review approach,
research organizations are evaluated on the basis of a subset of their
scientific production. The dimension of the subset varies from nation to nation
but is typically set as a proportional function of the number of researchers
employed at each research organization. However, scientific fertility varies
from discipline to discipline, meaning that the representativeness of such a
subset also varies according to discipline. The rankings resulting from the
assessments could be quite sensitive to the size of the share of articles
selected for evaluation. The current work examines this issue, developing
empirical evidence of variations in ranking due changes in the dimension of the
subset of products evaluated. The field of observation is represented by the
scientific production from the hard sciences of the entire Italian university
system, from 2001 to 2003.
"
2410,"Citations versus journal impact factor as proxy of quality: Could the
  latter ever be preferable?","  In recent years bibliometricians have paid increasing attention to research
evaluation methodological problems, among these being the choice of the most
appropriate indicators for evaluating quality of scientific publications, and
thus for evaluating the work of single scientists, research groups and entire
organizations. Much literature has been devoted to analyzing the robustness of
various indicators, and many works warn against the risks of using easily
available and relatively simple proxies, such as journal impact factor. The
present work continues this line of research, examining whether it is valid
that the use of the impact factor should always be avoided in favour of
citations, or whether the use of impact factor could be acceptable, even
preferable, in certain circumstances. The evaluation was conducted by observing
all scientific publications in the hard sciences by Italian universities, for
the period 2004-2007. Performance sensitivity analyses were conducted with
changing indicators of quality and years of observation.
"
2411,"National research assessment exercises: a measure of the distortion of
  performance rankings when labor input is treated as uniform","  Measuring the efficiency of scientific research activity presents critical
methodological aspects, many of which have not been sufficiently studied.
Although many studies have assessed the relation between quality and research
productivity and academic rank, not much is known about the extent of
distortion in national university performance rankings when academic rank and
the other labor factors are not considered as a factor of normalization. This
work presents a comparative analysis that aims to quantify the sensitivity of
bibliometric rankings to the choice of input, with input considered as only the
number of researchers on staff, or alternatively where their cost is also
considered. The field of observation consists of all 69 Italian universities
active in the hard sciences. Performance measures are based on the 81,000
publications produced during the 2004-2006 triennium by all 34,000 research
staff, with analysis carried out at the level of individual disciplines, 187 in
total. The effect of the switch from labor to cost seems to be minimal except
for a few outliers.
"
2412,A robust benchmark for the h- and g-indexes,"  The use Hirsch's h-index as a joint proxy of the impact and productivity of a
scientist's research work continues to gain ground, accompanied by the efforts
of bibliometrists to resolve some of its critical issues, through the
application of a number of more or less sophisticated variants. However, the
literature does not reveal any appreciable attempt to overcome the objective
problems of measuring h-indexes on a large scale, for purposes of comparative
evaluation. Scientists may succeed in calculating their own h-indexes but,
being unable to compare them to those of their peers, they are unable to obtain
truly useful indications of their individual research performance. This study
proposes to overcome this gap, measuring the h- and Egghe's g-indexes of all
Italian university researchers in the hard sciences, over a 5-year window.
Descriptive statistics are provided concerning all of the 165 subject fields
examined, offering robust benchmarks for those who wish to compare their
individual performance to those of their colleagues in the same subject field.
"
2413,"Assessing public-private research collaboration: is it possible to
  compare university performance?","  It is widely recognized that collaboration between the public and private
research sectors should be stimulated and supported, as a means of favoring
innovation and regional development. This work takes a bibliometric approach,
based on co-authorship of scientific publications, to propose a model for
comparative measurement of the performance of public research institutions in
collaboration with the domestic industry collaboration with the private sector.
The model relies on an identification and disambiguation algorithm developed by
the authors to link each publication to its real authors. An example of
application of the model is given, for the case of the academic system and
private enterprises in Italy. The study demonstrates that for each scientific
discipline and each national administrative region, it is possible to measure
the performance of individual universities in both intra-regional and
extra-regional collaboration, normalized with respect to advantages of
location. Such results may be useful in informing regional policies and
merit-based public funding of research organizations.
"
2414,"Testing the trade-off between productivity and quality in research
  activities","  In recent years there has been an increasingly pressing need for the
evaluation of results from public sector research activity, particularly to
permit the efficient allocation of ever scarcer resources. Many of the studies
and evaluation exercises that have been conducted at the national and
international level emphasize the quality dimension of research output, while
neglecting that of productivity. This work is intended to test for the possible
existence of correlation between quantity and quality of scientific production
and determine whether the most productive researchers are also those that
achieve results that are qualitatively better than those of their colleagues.
The analysis proposed refers to the entire Italian university system and is
based on the observation of production in the hard sciences by above 26,000
researchers in the period 2001 to 2005. The results show that the output of
more productive researchers is superior in quality than that of less productive
researchers. The relation between productivity and quality results as largely
insensitive to the types of indicators or the test methods applied and also
seems to differ little among the various disciplines examined.
"
2415,"Modeling and Predicting Citation Count via Recurrent Neural Network with
  Long Short-Term Memory","  The rapid evolution of scientific research has been creating a huge volume of
publications every year. Among the many quantification measures of scientific
impact, citation count stands out for its frequent use in the research
community. Although peer review process is the mainly reliable way of
predicting a paper's future impact, the ability to foresee lasting impact on
the basis of citation records is increasingly important in the scientific
impact analysis in the era of big data. This paper focuses on the long-term
citation count prediction for individual publications, which has become an
emerging and challenging applied research topic. Based on the four key
phenomena confirmed independently in previous studies of long-term scientific
impact quantification, including the intrinsic quality of publications, the
aging effect and the Matthew effect and the recency effect, we unify the
formulations of all these observations in this paper. Building on a foundation
of the above formulations, we propose a long-term citation count prediction
model for individual papers via recurrent neural network with long short-term
memory units. Extensive experiments on a real-large citation data set
demonstrate that the proposed model consistently outperforms existing methods,
and achieves a significant performance improvement.
"
2416,NIPS4Bplus: a richly annotated birdsong audio dataset,"  Recent advances in birdsong detection and classification have approached a
limit due to the lack of fully annotated recordings. In this paper, we present
NIPS4Bplus, the first richly annotated birdsong audio dataset, that is
comprised of recordings containing bird vocalisations along with their active
species tags plus the temporal annotations acquired for them. Statistical
information about the recordings, their species specific tags and their
temporal annotations are presented along with example uses. NIPS4Bplus could be
used in various ecoacoustic tasks, such as training models for bird population
monitoring, species classification, birdsong vocalisation detection and
classification.
"
2417,"Scale-free collaboration networks: An author name disambiguation
  perspective","  Several studies have found that collaboration networks are scale-free,
proposing that such networks can be modeled by specific network evolution
mechanisms like preferential attachment. This study argues that collaboration
networks can look more or less scale-free depending on the methods for
resolving author name ambiguity in bibliographic data. Analyzing networks
constructed from multiple datasets containing 3.4M ~ 9.6M publication records,
this study shows that collaboration networks in which author names are
disambiguated by the commonly used heuristic, i.e., forename-initial-based name
matching, tend to produce degree distributions better fitted to power-law
slopes with the typical scaling parameter (2 < {\alpha} < 3) than networks
disambiguated by more accurate algorithm-based methods. Such tendency is
observed across collaboration networks generated under various conditions such
as cumulative years, 5- & 1-year sliding windows, and random sampling, and
through simulation, found to arise due mainly to artefactual entities created
by inaccurate disambiguation. This cautionary study calls for special attention
from scholars analyzing network data in which entities such as people,
organization, and gene can be merged or split by improper disambiguation.
"
2418,h-index and its alternative: A Review,"  In recent years, several Scientometrics and Bibliometrics indicators were
proposed to evaluate the scientific impact of individuals, institutions,
colleges, universities and research teams. The h-index gives a major
breakthrough in the research community to evaluate the scientific impact of an
individual. It got a lot of attention due to its simplicity and several other
indicators were proposed to extend the properties of h-index as well as to
overcome shortcomings of h-index. In this literature review, we have discussed
the advantages and limitations of almost all Scientometrics as well as
Bibliometrics indicators which have been categorized into seven categories :(i)
Complement of h-index, (ii) Based on total number of authors, (iii) Based on
publication age, (iv) Combination of two indices, (v) Based on excess citation
count, (vi) Based on total publication count, (vii) Based on other variants.
The main objective of this article is to study all those indicators which have
been proposed to evaluate the scientific impact of an individual researcher or
a group of researchers.
"
2419,System of Bibliometric Monitoring Sciences in Ukraine,"  The origins of scientometrics (research metrics) were analysed and the lack
of attention to elaboration of its methodology was emphasized. The approaches
to evaluation of scientific activity outcome were considered and the tendency
of transition from formal quantitative indicators to receiving expert
conclusion on the basis of bibliometric indicators was noted. The principles of
the Leiden manifesto of scientometrics were set out, keeping to which provides
clear monitoring and support of science development, and favours establishing
of the constructive dialog between scientific environment and society as well.
The conceptual statements and peculiarities of practical realization of the
informative and analytical system ""Bibliometryka Ukraynskoyi Nauky""
(""Bibliometrics of the Ukrainian Science"") elaborated by the Vernadsky National
Library of Ukraine, are being represented. The proposals on the formation of
advisory councils, which are aimed to adopt conclusions on the effectiveness of
research activity of institutions, were considered. The feasibility of building
a common platform for expert evaluation of scientific studies for countries of
the Eastern Partnership by initiating similar bibliometric projects in these
countries and their further convergence is proved.
"
2420,Fake Comment Detection Based on Sentiment Analysis,"  With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.
"
2421,"A Decision Support System for Public Research Organizations
  Participating in National Research Assessment Exercises","  We are witnessing a rapid trend towards the adoption of exercises for
evaluation of national research systems, generally based on the peer review
approach. They respond to two main needs: stimulating higher efficiency in
research activities by public laboratories, and realizing better allocative
efficiency in government funding of such institutions. However the peer review
approach is typified by several limitations that raise doubts for the
achievement of the ultimate objectives. In particular, subjectivity of
judgment, which occurs during the step of selecting research outputs to be
submitted for the evaluations, risks heavily distorting both the final ratings
of the organizations evaluated and the ultimate funding they receive. These
distortions become ever more relevant if the evaluation is limited to small
samples of the scientific production of the research institutions. The
objective of the current study is to propose a quantitative methodology based
on bibliometric data that would provide a reliable support for the process of
selecting the best products of a laboratory, and thus limit distortions.
Benefits are twofold: single research institutions can maximize the probability
of receiving a fair evaluation coherent with the real quality of their
research. At the same time, broader adoptions of this approach could also
provide strong advantages at the macroeconomic level, since it guarantees
financial allocations based on the real value of the institutions under
evaluation. In this study, the proposed methodology has been applied to the
hard science sectors of the Italian university research system for the period
2004-2006.
"
2422,"The contribution of star scientists to overall sex differences in
  research productivity","  The state of the art on the issue of sex differences in research efficiency
agrees in recognizing higher performances for males, however there are
divergences in explaining the possible causes. One of the causes advanced is
that there are sex differences in the availability of aptitude at the ""high
end"". By comparing sex differences in concentration and performance of Italian
academic star scientists to the case in the population complement, this work
aims to verify if star, or ""high-end"", scientists play a preponderant role in
determining higher performance among males. The study reveals the existence of
a greater relative concentration of males among star scientists, as well as a
performance gap between male and female star scientists that is greater than
for the rest of the population. In the latter subpopulation the performance gap
between the two sexs is seen as truly marginal.
"
2423,University-industry collaboration in Italy: a bibliometric examination,"  This work investigates public-private research collaboration between Italian
universities and domestic industry, applying a bibliometric type of approach.
The study is based on an exhaustive listing of all co-authored publications in
international journals that are jointly realized by Italian university
scientists and researchers in the private sector: this listing permits the
development of a national mapping system for public-private collaboration,
which results unique for its extensive and representative character. It is
shown that, in absolute terms, most collaborations occur in medicine and
chemistry, while it is industrial and information engineering which shows the
highest percentage of co-authored articles out of all articles in the field. In
addition, the investigation empirically examines and tests several hypotheses
concerning the qualitative-quantitative impact of collaboration on the
scientific production of individual university researchers. The analyses
demonstrate that university researchers who collaborate with those in the
private sector show research performance that is superior to that of colleagues
who are not involved in such collaboration. But the impact factor of journals
publishing academic articles co-authored by industry is generally lower than
that concerning co-authorships with other entities. Finally, a further specific
elaboration also reveals that publications with public-private co-authorship do
not show a level of multidisciplinarity that is significantly different than
that of other publications.
"
2424,Resource Mention Extraction for MOOC Discussion Forums,"  In discussions hosted on discussion forums for MOOCs, references to online
learning resources are often of central importance. They contextualize the
discussion, anchoring the discussion participants' presentation of the issues
and their understanding. However they are usually mentioned in free text,
without appropriate hyperlinking to their associated resource. Automated
learning resource mention hyperlinking and categorization will facilitate
discussion and searching within MOOC forums, and also benefit the
contextualization of such resources across disparate views. We propose the
novel problem of learning resource mention identification in MOOC forums. As
this is a novel task with no publicly available data, we first contribute a
large-scale labeled dataset, dubbed the Forum Resource Mention (FoRM) dataset,
to facilitate our current research and future research on this task. We then
formulate this task as a sequence tagging problem and investigate solution
architectures to address the problem. Importantly, we identify two major
challenges that hinder the application of sequence tagging models to the task:
(1) the diversity of resource mention expression, and (2) long-range contextual
dependencies. We address these challenges by incorporating character-level and
thread context information into a LSTM-CRF model. First, we incorporate a
character encoder to address the out-of-vocabulary problem caused by the
diversity of mention expressions. Second, to address the context dependency
challenge, we encode thread contexts using an RNN-based context encoder, and
apply the attention mechanism to selectively leverage useful context
information during sequence tagging. Experiments on FoRM show that the proposed
method improves the baseline deep sequence tagging models notably,
significantly bettering performance on instances that exemplify the two
challenges.
"
2425,The Architecture of Mr. DLib's Scientific Recommender-System API,"  Recommender systems in academia are not widely available. This may be in part
due to the difficulty and cost of developing and maintaining recommender
systems. Many operators of academic products such as digital libraries and
reference managers avoid this effort, although a recommender system could
provide significant benefits to their users. In this paper, we introduce Mr.
DLib's ""Recommendations as-a-Service"" (RaaS) API that allows operators of
academic products to easily integrate a scientific recommender system into
their products. Mr. DLib generates recommendations for research articles but in
the future, recommendations may include call for papers, grants, etc. Operators
of academic products can request recommendations from Mr. DLib and display
these recommendations to their users. Mr. DLib can be integrated in just a few
hours or days; creating an equivalent recommender system from scratch would
require several months for an academic operator. Mr. DLib has been used by
GESIS Sowiport and by the reference manager JabRef. Mr. DLib is open source and
its goal is to facilitate the application of, and research on, scientific
recommender systems. In this paper, we present the motivation for Mr. DLib, the
architecture and details about the effectiveness. Mr. DLib has delivered 94m
recommendations over a span of two years with an average click-through rate of
0.12%.
"
2426,"ParsRec: A Novel Meta-Learning Approach to Recommending Bibliographic
  Reference Parsers","  Bibliographic reference parsers extract machine-readable metadata such as
author names, title, journal, and year from bibliographic reference strings. To
extract the metadata, the parsers apply heuristics or machine learning.
However, no reference parser, and no algorithm, consistently gives the best
results in every scenario. For instance, one tool may be best in extracting
titles in ACM citation style, but only third best when APA is used. Another
tool may be best in extracting English author names, while another one is best
for noisy data (i.e. inconsistent citation styles). In this paper, which is an
extended version of our recent RecSys poster, we address the problem of
reference parsing from a recommender-systems and meta-learning perspective. We
propose ParsRec, a meta-learning based recommender-system that recommends the
potentially most effective parser for a given reference string. ParsRec
recommends one out of 10 open-source parsers: Anystyle-Parser, Biblio, CERMINE,
Citation, Citation-Parser, GROBID, ParsCit, PDFSSA4MET, Reference Tagger, and
Science Parse. We evaluate ParsRec on 105k references from chemistry. We
propose two approaches to meta-learning recommendations. The first approach
learns the best parser for an entire reference string. The second approach
learns the best parser for each metadata type in a reference string. The second
approach achieved a 2.6% increase in F1 (0.909 vs. 0.886) over the best single
parser (GROBID), reducing the false positive rate by 20.2% (0.075 vs. 0.094),
and the false negative rate by 18.9% (0.107 vs. 0.132).
"
2427,Convexity in scientific collaboration networks,"  Convexity in a network (graph) has been recently defined as a property of
each of its subgraphs to include all shortest paths between the nodes of that
subgraph. It can be measured on the scale [0, 1] with 1 being assigned to fully
convex networks. The largest convex component of a graph that emerges after the
removal of the least number of edges is called a convex skeleton. It is
basically a tree of cliques, which has been shown to have many interesting
features. In this article the notions of convexity and convex skeletons in the
context of scientific collaboration networks are discussed. More specifically,
we analyze the co-authorship networks of Slovenian researchers in computer
science, physics, sociology, mathematics, and economics and extract convex
skeletons from them. We then compare these convex skeletons with the residual
graphs (remainders) in terms of collaboration frequency distributions by
various parameters such as the publication year and type, co-authors' birth
year, status, gender, discipline, etc. We also show the top-ranked scientists
by four basic centrality measures as calculated on the original networks and
their skeletons and conclude that convex skeletons may help detect influential
scholars that are hardly identifiable in the original collaboration network. As
their inherent feature, convex skeletons retain the properties of collaboration
networks. These include high-level structural properties but also the fact that
the same authors are highlighted by centrality measures. Moreover, the most
important ties and thus the most important collaborations are retained in the
skeletons.
"
2428,"Challenges of measuring the impact of software: an examination of the
  lme4 R package","  The rise of software as a research object is mirrored in the increasing
interests towards quantitative studies of scientific software. However, due to
the inconsistent practice of citing software, most of the existing studies
analyzing the impact of scientific software are based on identification of
software name mentions in full-text publications. Despite its limitations,
citation data have a much larger quantity and broader coverage of scientific
fields than full-text data and thus could support findings in much larger
scopes. This paper presents an analysis aiming to evaluate the extent to which
citations data can be used to reconstruct the impact of software. Specifically,
we identified the variety of citable objects related to the lme4 R package and
examined how the package's impact is scattered across these objects. Our
results reveal a little-discussed challenge of using citation data to measure
the impact of software, that even within the category of formal citation, there
might be different forms in which the same software object is cited. This
challenge can be mitigated by more carefully selecting objects as the proxy of
software. However, it cannot be fully solved until we have
one-software-one-proxy policy for software citation.
"
2429,"Do the technical universities exhibit distinct behaviour in global
  university rankings? A Times Higher Education (THE) case study","  Technical Universities (TUs) exhibit a distinct ranking performance in
comparison with other universities. In this paper we identify 137 TUs included
in the THE Ranking (2017 edition) and analyse their scores statistically. The
results highlight the existence of clusters of TUs showing a general high
performance in the Industry Income category and, in many cases, a low
performance on Research and Teaching. Finally, the global score weights were
simulated, creating several scenarios that confirmed that the majority of TUs
(except those with a world-class status) would increase their final scores if
industrial income was accounted for at the levels parametrised.
"
2430,"Report on the 3rd Joint Workshop on Bibliometric-enhanced Information
  Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)","  The $3^{rd}$ joint BIRNDL workshop was held at the 41st ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR 2018) in Ann Arbor,
USA. BIRNDL 2018 intended to stimulate IR researchers and digital library
professionals to elaborate on new approaches in natural language processing,
information retrieval, scientometrics, and recommendation techniques that can
advance the state-of-the-art in scholarly document understanding, analysis, and
retrieval at scale. The workshop incorporated three paper sessions and the
$4^{th}$ edition of the CL-SciSumm Shared Task.
"
2431,"Automatically Annotating Articles Towards Opening and Reusing
  Transparent Peer Reviews","  An increasing number of scientific publications are created in open and
transparent peer review models: a submission is published first, and then
reviewers are invited, or a submission is reviewed in a closed environment but
then these reviews are published with the final article, or combinations of
these. Reasons for open peer review include giving better credit to reviewers
and enabling readers to better appraise the quality of a publication. In most
cases, the full, unstructured text of an open review is published next to the
full, unstructured text of the article reviewed. This approach prevents human
readers from getting a quick impression of the quality of parts of an article,
and it does not easily support secondary exploitation, e.g., for scientometrics
on reviews. While document formats have been proposed for publishing structured
articles including reviews, integrated tool support for entire open peer review
workflows resulting in such documents is still scarce. We present AR-Annotator,
the Automatic Article and Review Annotator which employs a semantic information
model of an article and its reviews, using semantic markup and unique
identifiers for all entities of interest. The fine-grained article structure is
not only exposed to authors and reviewers but also preserved in the published
version. We publish articles and their reviews in a Linked Data representation
and thus maximize their reusability by third-party applications. We demonstrate
this reusability by running quality-related queries against the structured
representation of articles and their reviews.
"
2432,"Distilling Information from a Flood: A Possibility for the Use of
  Meta-Analysis and Systematic Review in Machine Learning Research","  The current flood of information in all areas of machine learning research,
from computer vision to reinforcement learning, has made it difficult to make
aggregate scientific inferences. It can be challenging to distill a myriad of
similar papers into a set of useful principles, to determine which new
methodologies to use for a particular application, and to be confident that one
has compared against all relevant related work when developing new ideas.
However, such a rapidly growing body of research literature is a problem that
other fields have already faced - in particular, medicine and epidemiology. In
those fields, systematic reviews and meta-analyses have been used exactly for
dealing with these issues and it is not uncommon for entire journals to be
dedicated to such analyses. Here, we suggest the field of machine learning
might similarly benefit from meta-analysis and systematic review, and we
encourage further discussion and development along this direction.
"
2433,"Toward Exploratory Search in Biomedicine: Evaluating Document Clusters
  by MeSH as a Semantic Anchor","  The current mode of biomedical literature search is severely limited in
effectively finding information relevant to specialists. A potential approach
to solving this problem is exploratory search, which allows users to
interactively navigate through a vast document collection. As the first step
toward exploratory search for specialists in biomedicine, this paper develops a
methodology to evaluate quality of document clusters. For this purpose, we
incorporate human expertise into data set creation and evaluation framework by
leveraging MeSH terms as semantic anchors. In addition, we investigate the
benefit of full-text data for improving cluster quality.
"
2434,"The MeSH-gram Neural Network Model: Extending Word Embedding Vectors
  with MeSH Concepts for UMLS Semantic Similarity and Relatedness in the
  Biomedical Domain","  Eliciting semantic similarity between concepts in the biomedical domain
remains a challenging task. Recent approaches founded on embedding vectors have
gained in popularity as they risen to efficiently capture semantic
relationships The underlying idea is that two words that have close meaning
gather similar contexts. In this study, we propose a new neural network model
named MeSH-gram which relies on a straighforward approach that extends the
skip-gram neural network model by considering MeSH (Medical Subject Headings)
descriptors instead words. Trained on publicly available corpus PubMed MEDLINE,
MeSH-gram is evaluated on reference standards manually annotated for semantic
similarity. MeSH-gram is first compared to skip-gram with vectors of size 300
and at several windows contexts. A deeper comparison is performed with tewenty
existing models. All the obtained results of Spearman's rank correlations
between human scores and computed similarities show that MeSH-gram outperforms
the skip-gram model, and is comparable to the best methods but that need more
computation and external resources.
"
2435,"The Integrated Impact Indicator (I3) Revisited: A Non-Parametric
  Alternative to the Journal Impact Factor","  We propose the I3* indicator as a non-parametric alternative to the Journal
Impact Factor (JIF) and h-index. We apply I3* to more than 10,000 journals. The
results can be compared with other journal metrics. I3* is a promising variant
within the general scheme of non-parametric indicators I3 introduced
previously: it provides a single metric which correlates with both impact in
terms of citations (c) and output in terms of publications (p). We argue for
weighting using four percentile classes: the top-1% and top-10% as excellence
indicators; the top-50% and bottom-50% as output indicators. Like the h-index,
which also incorporates both c and p, I3*-values are size-dependent; however,
division of I3* by the number of publications (I3*/N) provides a
size-independent indicator which correlates strongly with the two- and
five-year Journal Impact Factors (JIF2 and JIF5). Unlike the h-index, I3*
correlates significantly with both the total number of citations and
publications. The values of I3* and I3*/N can be statistically tested against
the expectation or against one another using chi-square tests or effect sizes.
A template (in Excel) is provided online for relevant tests.
"
2436,"Does the Web of Science Accurately Represent Chinese Scientific
  Performance?","  The purpose of this study is to compare Web of Science (WoS) with a Chinese
bibliometric database in terms of authors and their performance, demonstrate
the extent of the overlap between the two groups of Chinese most productive
authors in both international and Chinese bibliometric databases, and determine
how different disciplines may affect this overlap. The results of this study
indicate that Chinese bibliometric databases, or a combination of WoS and
Chinese bibliometric databases, should be used to evaluate Chinese research
performance except in few disciplines in which Chinese research performance
could be assessed using WoS only.
"
2437,Gender and Research Publishing in India: Uniformly high inequality?,"  Women's access to academic careers has been historically limited by
discrimination and cultural constraints. Comprehensive information about gender
inequality within disciplines is needed to understand the problem and target
remedial action. India is the fifth largest research producer but has a low
international index of gender inequality and so is an important case. This
study assesses gender inequalities in Indian journal article publishing in 2017
for 186 research fields. It also seeks overall gender differences in interests
across academia by comparing the terms used in 27,710 articles with an Indian
male or female first author. The data show that there are at least 1.5 male
first authors per female first author in each of 26 broad fields and 2.8 male
first authors per female first author overall. Compared to the USA, India has a
much lower share of female first authors but smaller variations in gender
differences between broad fields. Dentistry, Economics and Maths are all more
female in India, but Veterinary is much less female than in the USA. There is a
tendency for males to research thing-oriented topics and for females to
research helping people and some life science topics. More initiatives to
promote gender equality in science are needed to address the overall imbalance,
but care should be taken to avoid creating the larger between-field gender
differences found in the USA.
"
2438,Graph Embedding for Citation Recommendation,"  As science advances, the academic community has published millions of
research papers. Researchers devote time and effort to search relevant
manuscripts when writing a paper or simply to keep up with current research. In
this paper, we consider the problem of citation recommendation on graph and
propose a task-specific neighborhood construction strategy to learn the
distributed representations of papers. In addition, given the learned
representations, we investigate various schemes to rank the candidate papers
for citation recommendation. The experimental results show our proposed
neighborhood construction strategy outperforms the widely-used random walks
based sampling strategy on all ranking schemes, and the model based ranking
scheme outperforms embedding based rankings for both neighborhood construction
strategies. We also demonstrated that graph embedding is a robust approach for
citation recommendation when hidden ratio changes, while the performance of
classic methods drop significantly when the set of seed papers is becoming
small.
"
2439,"Text data mining and data quality management for research information
  systems in the context of open data and open science","  In the implementation and use of research information systems (RIS) in
scientific institutions, text data mining and semantic technologies are a key
technology for the meaningful use of large amounts of data. It is not the
collection of data that is difficult, but the further processing and
integration of the data in RIS. Data is usually not uniformly formatted and
structured, such as texts and tables that cannot be linked. These include
various source systems with their different data formats such as project and
publication databases, CERIF and RCD data model, etc. Internal and external
data sources continue to develop. On the one hand, they must be constantly
synchronized and the results of the data links checked. On the other hand, the
texts must be processed in natural language and certain information extracted.
Using text data mining, the quality of the metadata is analyzed and this
identifies the entities and general keywords. So that the user is supported in
the search for interesting research information. The information age makes it
easier to store huge amounts of data and increase the number of documents on
the internet, in institutions' intranets, in newswires and blogs is
overwhelming. Search engines should help to specifically open up these sources
of information and make them usable for administrative and research purposes.
Against this backdrop, the aim of this paper is to provide an overview of text
data mining techniques and the management of successful data quality for RIS in
the context of open data and open science in scientific institutions and
libraries, as well as to provide ideas for their application. In particular,
solutions for the RIS will be presented.
"
2440,TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph,"  Knowledge graph is a kind of valuable knowledge base which would benefit lots
of AI-related applications. Up to now, lots of large-scale knowledge graphs
have been built. However, most of them are non-Chinese and designed for general
purpose. In this work, we introduce TechKG, a large scale Chinese knowledge
graph that is technology-oriented. It is built automatically from massive
technical papers that are published in Chinese academic journals of different
research domains. Some carefully designed heuristic rules are used to extract
high quality entities and relations. Totally, it comprises of over 260 million
triplets that are built upon more than 52 million entities which come from 38
research domains. Our preliminary ex-periments indicate that TechKG has high
adaptability and can be used as a dataset for many diverse AI-related
applications. We released TechKG at: http://www.techkg.cn.
"
2441,"CiteScore metrics: Creating journal metrics from the Scopus citation
  index","  In December 2016, after several years of development, Elsevier launched a set
of transparent, comprehensive, current, and freely-available journal citation
metrics called CiteScore metrics. Most of the CiteScore metrics are static,
annual indicators calculated from the dynamic Scopus citation index. In the
spirit of recent public statements on the responsible use of metrics, we
outline the desirable characteristics of journal citation metrics, discuss how
we decided on the cited and citing publications years and document types to be
used for CiteScore metrics, and detail the precise method of calculation of
each metric. We further discuss CiteScore metrics eligibility criteria and
online display choices, as well as our approach to calculating static
indicators from the dynamic citation index. Finally, we look at the feedback
the metrics have so far received and how CiteScore is already developing in
response.
"
2442,Mapping excellence in national research systems: the case of Italy,"  The study of the concept of ""scientific excellence"" and the methods for its
measurement and evaluation is taking on increasing importance in the
development of research policies in many nations. However, scientific
excellence results as difficult to define in both conceptual and operational
terms, because of its multi-dimensional and highly complex character. The
literature on the theme is limited to few studies of an almost pioneering
character. This work intends to contribute to the state of the art by exploring
a bibliometric methodology which is effective, simple and inexpensive, and
which further identifies ""excellent"" centers of research by beginning from
excellence of the individual researchers affiliated with such centers. The
study concentrates on the specific case of public research organizations in
Italy, analyzing 109 scientific categories of research in the so called ""hard""
sciences and identifying 157 centers of excellence operating in 60 of these
categories. The findings from this first application of the methodology should
be considered exploratory and indicative. With a longer period of observation
and the addition of further measurements, making the methodology more robust,
it can be extended and adapted to a variety of national and supranational
contexts, aiding with policy decisions at various levels.
"
2443,Allocative efficiency in public research funding:can bibliometrics help?,"  The use of outcome control modes of research evaluation exercises is ever
more frequent. They are conceived as tools to stimulate increased levels of
research productivity, and to guide choices in allocating components of
government research budgets for publicly funded institutions. There are several
contributions in the literature that compare the different methodological
approaches that policy makers could adopt for these exercises, however the
comparisons are limited to only a few disciplines. This work, examining the
case of the whole of the ""hard sciences"" of the Italian academic system, makes
a comparison between results obtained from peer review type of evaluations (as
adopted by the Ministry of Universities and Research) and those possible from a
bibliometric approach (as developed by the authors). The aim is to understand
to what extent bibliometric methodology, which is noted as relatively
inexpensive, time-saving and exhaustive, can complement and integrate peer
review methodology in research evaluation.
"
2444,"Assessing technical and cost efficiency of research activities: A case
  study of the Italian university system","  This paper employs data envelopment analysis (DEA) to assess both technical
and cost efficiency of research activities of the Italian university system.
Differently from both peer review and the top-down discipline-invariant
bibliographic approaches used elsewhere, a bottom-up bibliometric methodology
is applied. Publications are assigned first to authors and then to one of nine
scientific and technical university disciplinary areas. Inputs are specified in
terms of the numbers of full, associate and assistant professors and outputs as
the number of publications, contributions to publications and their scientific
impact as variously measured across the disciplines included. DEA is undertaken
cross-sectionally using the averages of these inputs and outputs over the
period 2001-2003. The results typically show much variation in the rankings of
the disciplinary areas within and across universities, depending on the
efficiency indicator employed.
"
2445,Research collaboration and productivity: is there correlation?,"  The incidence of extramural collaboration in academic research activities is
increasing as a result of various factors. These factors include policy
measures aimed at fostering partnership and networking among the various
components of the research system, policies which are in turn justified by the
idea that knowledge sharing could increase the effectiveness of the system.
Over the last two decades, the scientific community has also stepped up
activities to assess the actual impact of collaboration intensity on the
performance of research systems. This study draws on a number of empirical
analyses, with the intention of measuring the effects of extramural
collaboration on research performance and, indirectly, verifying the legitimacy
of policies that support this type of collaboration. The analysis focuses on
the Italian academic research system. The aim of the work is to assess the
level of correlation, at institutional level, between scientific productivity
and collaboration intensity as a whole, both internationally and with private
organizations. This will be carried out using a bibliometric type of approach,
which equates collaboration with the co-authorship of scientific publications.
"
2446,"Gender differences in research productivity: a bibliometric analysis of
  the Italian academic system","  The literature dedicated to analysis of the difference in research
productivity between the sexes tends to agree in indicating better performance
for men. This study enters in the vein of work on the subject. Through
bibliometric examination of the entire population of research personnel working
in the scientific-technological disciplines of Italian university system, it
confirms the presence of significant differences in productivity between men
and women. But such differences result as being smaller than reported in a
large part of the literature, confirming an ongoing tendency towards decline,
and are also seen as more noticeable for quantitative performance indicators
than other indicators. The gap between the sexes presents important sectorial
specificities. In spite of the generally better performance of men, it can not
be ignored that there are a significant number of scientific sectors in which
the performance of women does not result as inferior.
"
2447,"A standardized Project Gutenberg corpus for statistical analysis of
  natural language and quantitative linguistics","  The use of Project Gutenberg (PG) as a text corpus has been extremely popular
in statistical analysis of language for more than 25 years. However, in
contrast to other major linguistic datasets of similar importance, no
consensual full version of PG exists to date. In fact, most PG studies so far
either consider only a small number of manually selected books, leading to
potential biased subsets, or employ vastly different pre-processing strategies
(often specified in insufficient details), raising concerns regarding the
reproducibility of published results. In order to address these shortcomings,
here we present the Standardized Project Gutenberg Corpus (SPGC), an open
science approach to a curated version of the complete PG data containing more
than 50,000 books and more than $3 \times 10^9$ word-tokens. Using different
sources of annotated metadata, we not only provide a broad characterization of
the content of PG, but also show different examples highlighting the potential
of SPGC for investigating language variability across time, subjects, and
authors. We publish our methodology in detail, the code to download and process
the data, as well as the obtained corpus itself on 3 different levels of
granularity (raw text, timeseries of word tokens, and counts of words). In this
way, we provide a reproducible, pre-processed, full-size version of Project
Gutenberg as a new scientific resource for corpus linguistics, natural language
processing, and information retrieval.
"
2448,Intermediacy of publications,"  Citation networks of scientific publications offer fundamental insights into
the structure and development of scientific knowledge. We propose a new
measure, called intermediacy, for tracing the historical development of
scientific knowledge. Given two publications, an older and a more recent one,
intermediacy identifies publications that seem to play a major role in the
historical development from the older to the more recent publication. The
identified publications are important in connecting the older and the more
recent publication in the citation network. After providing a formal definition
of intermediacy, we study its mathematical properties. We then present two
empirical case studies, one tracing historical developments at the interface
between the community detection literature and the scientometric literature and
one examining the development of the literature on peer review. We show both
conceptually and empirically how intermediacy differs from main path analysis,
which is the most popular approach for tracing historical developments in
citation networks. Main path analysis tends to favor longer paths over shorter
ones, whereas intermediacy has the opposite tendency. Compared to main path
analysis, we conclude that intermediacy offers a more principled approach for
tracing the historical development of scientific knowledge.
"
2449,"The measurement of italian universities' research productivity by a non
  parametric-bibliometric methodology","  This paper presents a methodology for measuring the technical efficiency of
research activities. It is based on the application of data envelopment
analysis to bibliometric data on the Italian university system. For that
purpose, different input values (research personnel by level and extra funding)
and output values (quantity, quality and level of contribution to actual
scientific publications) are considered. Our study aims at overcoming some of
the limitations connected to the methodologies that have so far been proposed
in the literature, in particular by surveying the scientific production of
universities by authors' name.
"
2450,"A new bibliometric approach to assess the scientific specialization of
  regions","  The objective of the current work is to identify the territorial scientific
specializations present in Italy, at the levels of regions and provinces (NUTS2
and NUTS3). To do this, we take a bibliometric approach based on the scientific
production of the entire public research system in the hard sciences sphere,
for the five years 2006-2010. In particular, we apply a new index of scientific
specialization (Scientific Specialization Index, SSI) that takes account of
both the quantity and quality of scientific production achieved by the research
institutions of a given territory.
"
2451,"Individual research performance: a proposal for comparing apples to
  oranges","  The evaluation of performance at the individual level is of fundamental
importance in informing management decisions. The literature provides various
indicators and types of measures, however a problem that is still unresolved
and little addressed is how to compare the performance of researchers working
in different fields (apples to oranges). In this work we propose a solution,
testing various scaling factors for the distributions of research productivity
in 174 scientific fields. The analysis is based on the observation of
scientific production by all Italian university researchers active in the hard
sciences over the period 2004-2008, as indexed by the Web of Science. The most
effective scaling factor is the average of the productivity distribution of
researchers with productivity above zero.
"
2452,"The collaboration behaviors of scientists in Italy: a field level
  analysis","  The analysis of research collaboration by field is traditionally conducted
beginning with the classification of the publications from the context of
interest. In this work we propose an alternative approach based on the
classification of the authors by field. The proposed method is more precise if
the intended use is to provide a benchmark for the evaluation of individual
propensity to collaborate. In the current study we apply the new methodology to
all Italian university researchers in the hard sciences, measuring the
propensity to collaborate for the various fields: in general, and specifically
with intramural colleagues, extramural domestic and extramural foreign
organizations. Using a simulation, we show that the results present substantial
differences from those obtained through application of traditional approaches.
"
2453,"A heuristic approach to author name disambiguation in bibliometrics
  databases for large-scale research assessments","  National exercises for the evaluation of research activity by universities
are becoming regular practice in ever more countries. These exercises have
mainly been conducted through the application of peer-review methods.
Bibliometrics has not been able to offer a valid large-scale alternative
because of almost overwhelming difficulties in identifying the true author of
each publication. We will address this problem by presenting a heuristic
approach to author name disambiguation in bibliometric datasets for large-scale
research assessments. The application proposed concerns the Italian university
system, consisting of 80 universities and a research staff of over 60,000
scientists. The key advantage of the proposed approach is the ease of
implementation. The algorithms are of practical application and have
considerably better scalability and expandability properties than
state-of-the-art unsupervised approaches. Moreover, the performance in terms of
precision and recall, which can be further improved, seems thoroughly adequate
for the typical needs of large-scale bibliometric research assessments.
"
2454,"Measuring science: irresistible temptations, easy shortcuts and
  dangerous consequences","  In benchmarking international research, although publication and citation
analyses should not be used to compare different disciplines, scientometrists
frequently fail to resist the temptation to present rankings based on total
publications and citations. Such measures are affected by significant
distortions, due to the uneven fertility across scientific disciplines and the
dishomogeneity of scientific specialisation among nations and universities. In
this paper, we provide an indication of the extent of the distortions when
comparative bibliometric analyses fail to recognise the range of levels of
scientific fertility, not only within a given major disciplinary area but also
within the different scientific disciplines encompassed by the same area.
"
2455,"The alignment of public research supply and industry demand for
  effective technology transfer: the case of Italy","  Italy lags quite behind vis-a'-vis other industrialized countries, in public
to private technology transfer. One of the possible causes might be the
mismatch between new knowledge supplied by public research and industry demand.
We test this hypothesis through a survey of leading public research scientists
in four high-tech sectors. The findings show that most research project results
seem to be of immediate industrial interest, which contrasts with the low
patent and licensing performances of Italian public research institutions. For
one third of all the results of the research, there are no Italian companies
able to exploit them. The same, however, is not true for the remaining results,
which shows that the misalignment between public supply and industry demand
alone cannot account for poor technology transfer. What emerges from our
investigation is that a closer coordination of research policy and industrial
policy is required, as well as closer attention to initiatives which may
support the transfer of public research results to domestic industry.
"
2456,"The Technology Transfer of the Italian Public Research System: the Case
  of the National Research Council of Italy","  This paper deals with the technology transfer activities of the main public
research institution in Italy, the Italian Research Council, CNR. A comparative
analysis on patenting and licensing performances between CNR and the
Massachusetts Institute of Technology, MIT has been carried out. Findings show
that: research expenses being equal, CNR patents are 26% of MIT's; and, patents
being equal, CNR licenses are also 26% of MIT's. This means that CNR impact on
domestic competitiveness, in terms of patent licenses, is less than 7% of
MIT's. Moreover, while 83% of CNR patents are never licensed to domestic
industry, the Italian technology balance of payments shows a perennial deficit.
The paper concludes with the identification of the possible causes that may
explain such a gap.
"
2457,"Identifying interdisciplinarity through the disciplinary classification
  of co-authors of scientific publications","  The growing complexity of challenges involved in scientific progress demands
ever more frequent application of competencies and knowledge from different
scientific fields. The present work analyzes the degree of collaboration among
scientists from different disciplines in order to identify the most frequent
""combinations of knowledge"" in research activity. The methodology adopts an
innovative bibliometric approach based on the disciplinary affiliation of
publication co-authors. The field of observation includes all publications
(173,134) indexed in the Science Citation Index Expanded (SCI-E) for the five
years 2004-2008, authored by all scientists in the hard sciences (43,223) at
Italian universities (68). The analysis examines 205 research fields grouped in
nine disciplines. Identifying the fields with the highest potential of
interdisciplinary collaboration is useful to inform research polices at
national and regional levels, as well as management strategies at the
institutional level.
"
2458,"Assessing the accuracy of the h and g indexes for measuring researchers'
  productivity","  Bibliometric indicators are increasingly used in support of decisions for
recruitment, career advancement, rewarding and selective funding for
scientists. Given the importance of the applications, bibliometricians are
obligated to carry out empirical testing of the robustness of the indicators,
in simulations of real contexts. In this work we compare the results of
national-scale research assessments at the individual level, based on three
different indexes: the h-index, g-index and ""fractional scientific strength"",
or FSS, an indicator previously proposed by the authors. For each index, we
construct and compare rankings lists of all Italian academic researchers
working in the hard sciences over the period 2001-2005. The analysis quantifies
the shifts in ranks that occur when researchers' productivity rankings by
simple indicators such as h- or g-index are compared with that by more accurate
FSS.
"
2459,The Chaperone Effect in Scientific Publishing,"  Experience plays a critical role in crafting high impact scientific work.
This is particularly evident in top multidisciplinary journals, where a
scientist is unlikely to appear as senior author if they have not previously
published within the same journal. Here, we develop a quantitative
understanding of author order by quantifying this 'Chaperone Effect', capturing
how scientists transition into senior status within a particular publication
venue. We illustrate that the chaperone effect has different magnitude for
journals in different branches of science, being more pronounced in medical and
biological sciences and weaker in natural sciences. Finally, we show that in
the case of high-impact venues, the chaperone effect has significant
implications, specifically resulting in a higher average impact relative to
papers authored by new PIs. Our findings shed light on the role played by
experience in publishing within specific scientific journals, on the paths
towards acquiring the necessary experience and expertise, and on the skills
required to publish in prestigious venues.
"
2460,"Specialization vs diversification in research activities: the extent,
  intensity and relatedness of field diversification by individual scientists","  We investigate whether and in what measure scientists tend to diversify their
research activity, and if this tendency varies according to their belonging to
different disciplinary areas. We analyze the nature of research diversification
along three dimensions: extent of diversification, intensity of
diversification, and degree of relatedness of topics in which researchers
diversifies. For this purpose we propose three bibliometric indicators, based
on the disciplinary placement of scientific output of individual scientists.
The empirical investigation shows that the extent of diversification is lowest
for scientists in Mathematics and highest in Chemistry; intensity of
diversification is lowest in Earth sciences and highest in Industrial and
information engineering; and degree of relatedness is lowest in Earth sciences
and highest in Chemistry.
"
2461,"Identifying translational science through embeddings of controlled
  vocabularies","  Objective: Translational science aims at ""translating"" basic scientific
discoveries into clinical applications. The identification of translational
science has practicality such as evaluating the effectiveness of investments
made into large programs like the Clinical and Translational Science Awards.
Despite several proposed methods that group publications---the primary unit of
research output---into some categories, we still lack a quantitative way to
place papers onto the full, continuous spectrum from basic research to clinical
medicine. Methods: Here we learn vector-representations of controlled
vocabularies assigned to MEDLINE papers to obtain a Translational Axis (TA)
that points from basic science to clinical medicine. The projected position of
a term on the TA, expressed by a continuous quantity, indicates the term's
""appliedness."" The position of a paper, determined by the average location over
its terms, quantifies the degree of its ""appliedness,"" which we term as ""level
score."" Results: We validate our method by comparing with previous techniques,
showing excellent agreement yet uncovering significant variations of scores of
papers in previously defined categories. The measure allows us to characterize
the standing of journals, disciplines, and the entire biomedical literature
along the basic-applied spectrum. Analysis on large-scale citation network
reveals two main findings. First, direct citations mainly occurred between
papers with similar scores. Second, shortest paths are more likely ended up
with a paper closer to the basic end of the spectrum, regardless of where the
starting paper is on the spectrum. Conclusions: The proposed method provides a
quantitative way to identify translational science.
"
2462,Wikibook-Bot - Automatic Generation of a Wikipedia Book,"  A Wikipedia book (known as Wikibook) is a collection of Wikipedia articles on
a particular theme that is organized as a book. We propose Wikibook-Bot, a
machine-learning based technique for automatically generating high quality
Wikibooks based on a concept provided by the user. In order to create the
Wikibook we apply machine learning algorithms to the different steps of the
proposed technique. Firs, we need to decide whether an article belongs to a
specific Wikibook - a classification task. Then, we need to divide the chosen
articles into chapters - a clustering task - and finally, we deal with the
ordering task which includes two subtasks: order articles within each chapter
and order the chapters themselves. We propose a set of structural, text-based
and unique Wikipedia features, and we show that by using these features, a
machine learning classifier can successfully address the above challenges. The
predictive performance of the proposed method is evaluated by comparing the
auto-generated books to existing 407 Wikibooks which were manually generated by
humans. For all the tasks we were able to obtain high and statistically
significant results when comparing the Wikibook-bot books to books that were
manually generated by Wikipedia contributors
"
2463,"Towards Finding Non-obvious Papers: An Analysis of Citation Recommender
  Systems","  As science advances, the academic community has published millions of
research papers. Researchers devote time and effort to search relevant
manuscripts when writing a paper or simply to keep up with current research. In
this paper, we consider the problem of citation recommendation by extending a
set of known-to-be-relevant references. Our analysis shows the degrees of cited
papers in the subgraph induced by the citations of a paper, called projection
graph, follow a power law distribution. Existing popular methods are only good
at finding the long tail papers, the ones that are highly connected to others.
In other words, the majority of cited papers are loosely connected in the
projection graph but they are not going to be found by existing methods. To
address this problem, we propose to combine author, venue and keyword
information to interpret the citation behavior behind those loosely connected
papers. Results show that different methods are finding cited papers with
widely different properties. We suggest multiple recommended lists by different
algorithms could satisfy various users for a real citation recommendation
system. Moreover, we also explore the fast local approximation for combined
methods in order to improve the efficiency.
"
2464,Issues for Using Semantic Modeling to Represent Mechanisms,"  Mechanisms are a fundamental concept in many areas of science. Nonetheless,
there has been little effort to develop structures to represent mechanisms. We
explore the issues in developing a basic semantic modeling framework for
describing some types of mechanisms. We draw together threads from a number of
different approaches and then consider two examples. From this survey, we
propose a rich Semantic Modeling Framework (SMF) based on Transitionals and
hierarchies of Aggregates and Mechanisms, which could be implemented with the
XFO programming environment. Potentially, the framework will be useful for
developing direct-representation scientific research reports and community
models.
"
2465,Do altmetrics work for assessing research quality?,"  Alternative metrics (aka altmetrics) are gaining increasing interest in the
scientometrics community as they can capture both the volume and quality of
attention that a research work receives online. Nevertheless, there is limited
knowledge about their effectiveness as a mean for measuring the impact of
research if compared to traditional citation-based indicators. This work aims
at rigorously investigating if any correlation exists among indicators, either
traditional (i.e. citation count and h-index) or alternative (i.e. altmetrics)
and which of them may be effective for evaluating scholars. The study is based
on the analysis of real data coming from the National Scientific Qualification
procedure held in Italy by committees of peers on behalf of the Italian
Ministry of Education, Universities and Research.
"
2466,Gender bias in academic recruitment,"  It is well known that women are underrepresented in the academic systems of
many countries. Gender discrimination is one of the factors that could
contribute to this phenomenon. This study considers a recent national academic
recruitment campaign in Italy, examining whether women are subject to more or
less bias than men. The findings show that no gender-related differences occur
among the candidates who benefit from positive bias, while among those
candidates affected by negative bias, the incidence of women is lower than that
of men. Among the factors that determine success in a competition for an
academic position, the number of the applicant's career years in the same
university as the committee members assumes greater weight for male candidates
than for females. Being of the same gender as the committee president is also a
factor that assumes greater weight for male applicants. On the other hand, for
female applicants, the presence of a full professor in the same university with
the same family name as the candidate assumes greater weight than for male
candidates.
"
2467,A Joint Model for Multimodal Document Quality Assessment,"  The quality of a document is affected by various factors, including
grammaticality, readability, stylistics, and expertise depth, making the task
of document quality assessment a complex one. In this paper, we explore this
task in the context of assessing the quality of Wikipedia articles and academic
papers. Observing that the visual rendering of a document can capture implicit
quality indicators that are not present in the document text --- such as
images, font choices, and visual layout --- we propose a joint model that
combines the text content with a visual rendering of the document for document
quality assessment. Experimental results over two datasets reveal that textual
and visual features are complementary, achieving state-of-the-art results.
"
2468,Taking census of physics,"  Over the past decades, the diversity of areas explored by physicists has
exploded, encompassing new topics from biophysics and chemical physics to
network science. However, it is unclear how these new subfields emerged from
the traditional subject areas and how physicists explore them. To map out the
evolution of physics subfields, here, we take an intellectual census of physics
by studying physicists' careers. We use a large-scale publication data set,
identify the subfields of 135,877 physicists and quantify their heterogeneous
birth, growth and migration patterns among research areas. We find that the
majority of physicists began their careers in only three subfields, branching
out to other areas at later career stages, with different rates and transition
times. Furthermore, we analyse the productivity, impact and team sizes across
different subfields, finding drastic changes attributable to the recent rise in
large-scale collaborations. This detailed, longitudinal census of physics can
inform resource allocation policies and provide students, editors and
scientists with a broader view of the field's internal dynamics.
"
2469,"The $h$-index and multi-author $h_m$-index for individual researchers in
  condensed matter physics","  The characteristics of the $h$-index in the field of condensed matter physics
are studied using high-quality data from ResearcherID. The results are examined
in terms of theoretical descriptions of the $h$-index' overall dependence on a
researcher's total number of published papers, and total number of citations.
In particular, the models by Hirsch, Egghe and Rousseau, as well as by
Gl\""anzel and Schubert are examined. Special emphasis is placed on the
deviations from such statistical descriptions, and it is argued that the
deviation of a particular researcher's $h$ value from the Egghe-Rouseau model's
prediction can be used as a supplementary measure of impact. A corresponding
analysis with similar results is performed using the multi-author $h_m$-index.
"
2470,"Exploring the Role of Interdisciplinarity in Physics: Success, Talent
  and Luck","  Although interdisciplinarity is often touted as a necessity for modern
research, the evidence on the relative impact of sectorial versus to
interdisciplinary science is qualitative at best. In this paper we leverage the
bibliographic data set of the American Physical Society to quantify the role of
interdisciplinarity in physics, and that of talent and luck in achieving
success in scientific careers. We analyze a period of 30 years (1980-2009)
tagging papers and their authors by means of the Physics and Astronomy
Classification Scheme (PACS), to show that some degree of interdisciplinarity
is quite helpful to reach success, measured as a proxy of either the number of
articles or the citations score. We also propose an agent-based model of the
publication-reputation-citation dynamics reproduces the trends observed in the
APS data set. On the one hand, the results highlight the crucial role of
randomness and serendipity in real scientific research; on the other, they shed
light on a counter-intuitive effect indicating that the most talented authors
are not necessarily the most successful ones.
"
2471,The Superior Knowledge Proximity Measure for Patent Mapping,"  Network maps of patent classes have been widely used to analyze the coherence
and diversification of technology or knowledge positions of inventors, firms,
industries, regions, and so on. To create such networks, a measure is required
to associate different classes of patents in the patent database and often
indicates knowledge proximity (or distance). Prior studies have used a variety
of knowledge proximity measures based on different perspectives and association
rules. It is unclear how to consistently assess and compare them, and which
ones are superior for constructing a generally useful total patent class
network. Such uncertainty has limited the generality and applications of the
previously reported maps. Herein, we use a statistical method to identify the
superior proximity measure from a comprehensive set of typical measures, by
evaluating and comparing their explanatory powers on the historical expansions
of the patent portfolios of individual inventors and organizations across
different patent classes. Based on the complete United States granted patent
database from 1976 to 2017, our analysis identifies a reference-based Jaccard
index as the statistically superior measure, for explaining the historical
diversifications and predicting future movement directions of both individual
inventors and organizations across technology domains.
"
2472,"Granularity of algorithmically constructed publication-level
  classifications of research publications: Identification of specialties","  In this work, in which we build on, and use the outcome of, an earlier study
on topic identification in an algorithmically constructed publication-level
classification (ACPLC), we address the issue how to algorithmically obtain a
classification of topics (containing articles), where the classes of the
classification correspond to specialties. The methodology we propose, which is
similar to the one used in the earlier study, uses journals and their articles
to construct a baseline classification. The underlying assumption of our
approach is that journals of a particular size and foci have a scope that
correspond to specialties. By measuring the similarity between (1) the baseline
classification and (2) multiple classifications obtained by topic clustering
and using different values of a resolution parameter, we have identified a
best-performing ACPLC. In two case studies, we could identify the subject foci
of involved specialties, and the subject foci of specialties were relatively
easy to distinguish. Further, the class size variation regarding the best
performing ACPLC is moderate, and only a small proportion of the articles
belong to very small classes. For these reasons, we conclude that the proposed
methodology is suitable to determine the specialty granularity level of an
ACPLC.
"
2473,"A System Dynamics Analysis of National R&D Performance Measurement
  System in Korea","  Peer review is one of useful and powerful performance measurement process. In
Korea, it needs to increase quality of R&D performance, but bibliometric
evaluation and lack of peers have opposite effect. We used system dynamics to
describe Korean R&D performance measurement system and ways to increase
performance quality. To meet a desired R&D performance quality, increasing
fairness and quality of evaluation is needed. Size of peer pool decreased
because of the specialization of R&D projects and the Sangpi process both, and
it is critical to acquire both fairness and quality. Also, shortening
evaluation period affect to R&D performance quality, by causing workloads
increase, limiting long-term and innovative R&D projects, and decreasing
evaluation quality. Previous evaluation policies do a role like
micro-controlling the R&D's activities, but increasing the size of peer pool
and changing evaluation period would make a change to quality and fairness of
evaluation.
"
2474,"Fundamentals of effective cloud management for the new NASA Astrophysics
  Data System","  The new NASA Astrophysics Data System (ADS) is designed with a
serviceoriented architecture (SOA) that consists of multiple customized Apache
Solr search engine instances plus a collection of microservices, containerized
using Docker, and deployed in Amazon Web Services (AWS). For complex systems,
like the ADS, this loosely coupled architecture can lead to a more scalable,
reliable and resilient system if some fundamental questions are addressed.
After having experimented with different AWS environments and deployment
methods, we decided in December 2017 to go with Kubernetes as our container
orchestration. Defining the best strategy to properly setup Kubernetes has
shown to be challenging: automatic scaling services and load balancing traffic
can lead to errors whose origin is difficult to identify, monitoring and
logging the activity that happens across multiple layers for a single request
needs to be carefully addressed, and the best workflow for a Continuous
Integration and Delivery (CI/CD) system is not self-evident. We present here
how we tackle these challenges and our plans for the future.
"
2475,Avoiding a Tragedy of the Commons in the Peer Review Process,"  Peer review is the foundation of scientific publication, and the task of
reviewing has long been seen as a cornerstone of professional service. However,
the massive growth in the field of machine learning has put this community
benefit under stress, threatening both the sustainability of an effective
review process and the overall progress of the field. In this position paper,
we argue that a tragedy of the commons outcome may be avoided by emphasizing
the professional aspects of this service. In particular, we propose a rubric to
hold reviewers to an objective standard for review quality. In turn, we also
propose that reviewers be given appropriate incentive. As one possible such
incentive, we explore the idea of financial compensation on a per-review basis.
We suggest reasonable funding models and thoughts on long term effects.
"
2476,"A principled methodology for comparing relatedness measures for
  clustering publications","  There are many different relatedness measures, based for instance on citation
relations or textual similarity, that can be used to cluster scientific
publications. We propose a principled methodology for evaluating the accuracy
of clustering solutions obtained using these relatedness measures. We formally
show that the proposed methodology has an important consistency property. The
empirical analyses that we present are based on publications in the fields of
cell biology, condensed matter physics, and economics. Using the BM25
text-based relatedness measure as evaluation criterion, we find that
bibliographic coupling relations yield more accurate clustering solutions than
direct citation relations and co-citation relations. The so-called extended
direct citation approach performs similarly to or slightly better than
bibliographic coupling in terms of the accuracy of the resulting clustering
solutions. The other way around, using a citation-based relatedness measure as
evaluation criterion, BM25 turns out to yield more accurate clustering
solutions than other text-based relatedness measures.
"
2477,"How well does I3 perform for impact measurement compared to other
  bibliometric indicators? The convergent validity of several
  (field-normalized) indicators","  Recently, the integrated impact indicator (I3) indicator was introduced where
citations are weighted in accordance with the percentile rank class of each
publication in a set of publications. I3 can also be used as a field-normalized
indicator. Field-normalization is common practice in bibliometrics, especially
when institutions and countries are compared. Publication and citation
practices are so different among fields that citation impact is normalized for
cross-field comparisons. In this study, we test the ability of the indicator to
discriminate between quality levels of papers as defined by Faculty members at
F1000Prime. F1000Prime is a post-publication peer review system for assessing
papers in the biomedical area. Thus, we test the convergent validity of I3 (in
this study, we test I3/N - the size-independent variant of I3 where I3 is
divided by the number of papers) using assessments by peers as baseline and
compare its validity with several other (field-normalized) indicators: the
mean-normalized citation score (MNCS), relative-citation ratio (RCR), citation
score normalized by cited references (CSNCR), characteristic scores and scales
(CSS), source-normalized citation score (SNCS), citation percentile, and
proportion of papers which belong to the x% most frequently cited papers (PPtop
x%). The results show that the PPtop 1% indicator discriminates best among
different quality levels. I3 performs similar as (slightly better than) most of
the other field-normalized indicators. Thus, the results point out that the
indicator could be a valuable alternative to other indicators in bibliometrics.
"
2478,Discovering seminal works with marker papers,"  Bibliometric information retrieval in databases can employ different
strategies. Com-monly, queries are performed by searching in title, abstract
and/or author keywords (author vocabulary). More advanced queries employ
database keywords to search in a controlled vo-cabulary. Queries based on
search terms can be augmented with their citing papers if a re-search field
cannot be curtailed by the search query alone. Here, we present another
strategy to discover the most important papers of a research field. A marker
paper is used to reveal the most important works for the relevant community.
All papers co-cited with the marker paper are analyzed using reference
publication year spectroscopy (RPYS). For demonstration of the marker paper
approach, density functional theory (DFT) is used as a research field.
Compari-sons between a prior RPYS on a publication set compiled using a
keyword-based search in a controlled vocabulary and three different co-citation
RPYS (RPYS-CO) analyses show very similar results. Similarities and differences
are discussed.
"
2479,Readership Data and Research Impact,"  Reading academic publications is a key scholarly activity. Scholars accessing
and recording academic publications online are producing new types of
readership data. These include publisher, repository, and academic social
network download statistics as well as online reference manager records. This
chapter discusses the use of download and reference manager data for research
evaluation and library collection development. The focus is on the validity and
application of readership data as an impact indicator for academic publications
across different disciplines. Mendeley is particularly promising in this
regard, although all data sources are not subjected to rigorous quality control
and can be manipulated.
"
2480,"Which are the influential publications in the Web of Science subject
  categories over a long period of time? CRExplorer software used for big-data
  analyses in bibliometrics","  What are the landmark papers in scientific disciplines? On whose shoulders
does research in these fields stand? Which papers are indispensable for
scientific progress? These are typical questions which are not only of interest
for researchers (who frequently know the answers - or guess to know them), but
also for the interested general public. Citation counts can be used to identify
very useful papers, since they reflect the wisdom of the crowd; in this case,
the scientists using the published results for their own research. In this
study, we identified with recently developed methods for the program CRExplorer
landmark publications in nearly all Web of Science subject categories (WoSSCs).
These are publications which belong more frequently than other publications
across the citing years to the top-per mill in their subject category. The
results for three subject categories ""Information Science and Library Science"",
""Computer Science, Information Systems"", and ""Computer Science, Software
Engineering"" are exemplarily discussed in more detail. The results for the
other WoSSCs can be found online at http://crexplorer.net.
"
2481,"Topological and Semantic Graph-based Author Disambiguation on DBLP Data
  in Neo4j","  In this work, we introduce a novel method for entity resolution author
disambiguation in bibliographic networks. Such a method is based on a 2-steps
network traversal using topological similarity measures for rating candidate
nodes. Topological similarity is widely used in the Link Prediction application
domain to assess the likelihood of an unknown link. A similarity function can
be a good approximation for equality, therefore can be used to disambiguate,
basing on the hypothesis that authors with many common co-authors are similar.
Our method has experimented on a graph-based representation of the public DBLP
Computer Science database. The results obtained are extremely encouraging
regarding Precision, Accuracy, and Specificity. Further good aspects are the
locality of the method for disambiguation assessment which avoids the need to
know the global network, and the exploitation of only a few data, e.g. author
name and paper title (i.e., co-authorship data).
"
2482,"Measuring national capability over big sciences multidisciplinarity: A
  case study of nuclear fusion research","  In the era of big science, countries allocate big research and development
budgets to large scientific facilities that boost collaboration and research
capability. A nuclear fusion device called the ""tokamak"" is a source of great
interest for many countries because it ideally generates sustainable energy
expected to solve the energy crisis in the future. Here, to explore the
scientific effects of tokamaks, we map a country's research capability in
nuclear fusion research with normalized revealed comparative advantage on five
topical clusters -- material, plasma, device, diagnostics, and simulation --
detected through a dynamic topic model. Our approach captures not only the
growth of China, India, and the Republic of Korea but also the decline of
Canada, Japan, Sweden, and the Netherlands. Time points of their rise and fall
are related to tokamak operation, highlighting the importance of large
facilities in big science. The gravity model points out that two countries
collaborate less in device, diagnostics, and plasma research if they have
comparative advantages in different topics. This relation is a unique feature
of nuclear fusion compared to other science fields. Our results can be used and
extended when building national policies for big science.
"
2483,"A multi-dimensional framework for characterizing the citation impact of
  scientific publications","  The citation impact of a scientific publication is usually seen as a
one-dimensional concept. We introduce a multi-dimensional framework for
characterizing the citation impact of a publication. In addition to the level
of citation impact, quantified by the number of citations received by a
publication, we also conceptualize and operationalize the depth and breadth and
the dependence and independence of the citation impact of a publication. The
proposed framework enables us to distinguish between publications that have a
deep citation impact in a relatively narrow research area and publications that
have a broad citation impact in a wider research area. It also allows us to
make a distinction between publications that are strongly dependent on earlier
work and publications that make a more independent scientific contribution. We
use our multi-dimensional citation impact framework to report basic descriptive
statistics on the citation impact of highly cited publications in all
scientific disciplines. In addition, we present a detailed case study focusing
on the field of scientometrics. The proposed citation impact framework provides
a more in-depth understanding of the citation impact of a publication than a
traditional one-dimensional perspective.
"
2484,"Open Research Knowledge Graph: Next Generation Infrastructure for
  Semantic Scholarly Knowledge","  Despite improved digital access to scholarly knowledge in recent decades,
scholarly communication remains exclusively document-based. In this form,
scholarly knowledge is hard to process automatically. In this paper, we present
the first steps towards a knowledge graph based infrastructure that acquires
scholarly knowledge in machine actionable form thus enabling new possibilities
for scholarly knowledge curation, publication and processing. The primary
contribution is to present, evaluate and discuss multi-modal scholarly
knowledge acquisition, combining crowdsourced and automated techniques. We
present the results of the first user evaluation of the infrastructure with the
participants of a recent international conference. Results suggest that users
were intrigued by the novelty of the proposed infrastructure and by the
possibilities for innovative scholarly knowledge processing it could enable.
"
2485,Methods to Evaluate Lifecycle Models for Research Data Management,"  Lifecycle models for research data are often abstract and simple. This comes
at the danger of oversimplifying the complex concepts of research data
management. The analysis of 90 different lifecycle models lead to two
approaches to assess the quality of these models. While terminological issues
make direct comparisons of models hard, an empirical evaluation seems possible.
"
2486,Climate Change and Social Sciences: a bibliometric analysis,"  The complexity of emergent wicked problems, such as climate change,
culminates in a reformulation of how we think about society and mobilize
scientists from various disciplines to seek solutions and perspectives on the
problem. From an epistemological point of view, it is essential to evaluate how
such topics can be developed inside the academic arena but, to do that, it is
necessary to perform complex analysis of the great number of recent academic
publications. In this work, we discuss how climate change has been addressed by
social sciences in practice. Can we observe the development of a new
epistemology by the emergence of the climate change debate? Are there
contributions in academic journals within the field of social sciences
addressing climate change? Which journals are these? Who are the authors? To
answer these questions, we developed an innovative method combining different
tools to search, filter, and analyze the impact of the academic production
related to climate change in social sciences in the most relevant journals.
"
2487,Automating Software Citation using GitCite,"  The ability to cite software and give credit to its authors and contributors
is increasingly important. While the number of online open-source software
repositories has grown rapidly over the past few years, few are being properly
cited when used due to the difficulty of creating appropriate citations and the
lack of automated techniques. This paper presents GitCite, a model for software
citation with version control which enables citations to be inferred for any
project component based on a small number of explicit citations attached to
subdirectories/files, and an implementation that integrates with Git and
GitHub. The implementation includes a browser extension and a local executable
tool, which enable citations to be added/modified/deleted to software project
repositories and managed through functions such as fork/merge/copy.
"
2488,"The relationship between usage and citations in an open access mega
  journal","  How do the level of usage of an article, the timeframe of its usage and its
subject area relate to the number of citations it accrues? This paper aims to
answer this question through an observational study of usage and citation data
collected about the multidisciplinary, open access mega-journal Scientific
Reports. This observational study answers these questions using the following
methods: an overlap analysis of most read and top-cited articles; Spearman
correlation tests between total citation counts over two years and usage over
various timeframes; a comparison of first months of citation for most read and
all articles; a Wilcoxon test on the distribution of total citations of early
cited articles and the distribution of total citations of all other articles.
All analyses were performed using the programming language R. As Scientific
Reports is a multidisciplinary journal covering all natural and clinical
sciences, we also looked at the differences across subjects. We found a
moderate correlation between usage in the first year and citations in the first
two years since publication, and that articles with high usage in the first 6
months are more likely to have their first citation earlier (Wilcoxon=1811500,
p < 0.0001), which is also related to higher citations in the first two years
(Wilcoxon=8071200, p < 0.0001). As this final assertion is inferred based on
the results of the other elements of this paper, it requires further analysis.
Moreover, our choice of a 2 year window for our analysis did not consider the
articles' citation half-life, and our use of Scientific Reports (a journal that
is atypical compared to most academic journals) as the source of the articles
analysed has likely played a role in our findings, and so analysing a longer
timeframe and carrying out similar analysis on a different journal (or group of
journals) may lead to different conclusions.
"
2489,Bibliometrics for collaboration works,"  An important issue in bibliometrics is the weighing of co-authorship in the
production of scientific collaborations, which are becoming the standard
modality of research activity in many disciplines. The problem is especially
relevant in the field of high-energy physics, where collaborations reach 3000
authors, but it can no longer be ignored also in other domains, like medicine
or biology. We present theoretical and numerical arguments in favour of
weighing the individual contributions as $1/N_{\rm aut}^\alpha$ where $N_{\rm
aut}$ is the number of co-authors. When counting citations we suggest the
exponent $\alpha\approx 1$, that corresponds to fractional counting. When
counting the number of papers we suggest $\alpha \approx 1/3 - 1/2$, with the
former (latter) value more appropriate for larger (smaller) collaborations. We
expect and verify that the $h$ index scales as the square root of the average
number of co-authors, and define a fractionalized $h$ index that does not scale
with collaboration size.
"
2490,Mobile Information Retrieval,"  Mobile Information Retrieval (Mobile IR) is a relatively recent branch of
Information Retrieval (IR) that is concerned with enabling users to carry out,
using a mobile device, all the classical IR operations that they were used to
carry out on a desktop. This includes finding content available on local
repositories or on the web in response to a user query, interacting with the
system in an explicit or implicit way, reformulate the query and/or visualise
the content of the retrieved documents, as well as providing relevance
judgments to improve the retrieval process.
  This book is structured as follows. Chapter 2 provides a very brief overview
of IR and of Mobile IR, briefly outlining what in Mobile IR is different from
IR. Chapter 3 provides the foundations of Mobile IR, looking at the
characteristics of mobile devices and what they bring to IR, but also looking
at how the concept of relevance changed from standard IR to Mobile IR. Chapter
4 presents an overview of the document collections that are searchable by a
Mobile IR system, and that are somehow different from classical IR ones;
available for experimentation, including collections of data that have become
complementary to Mobile IR. Similarly, Chapter 5 reviews mobile information
needs studies and users log analysis. Chapter 6 reviews studies aimed at
adapting and improving the users interface to the needs of Mobile IR. Chapter
7, instead, reviews work on context awareness, which studies the many aspects
of the user context that Mobile IR employs. Chapter 8 reviews some of
evaluation work done in Mobile IR, highlighting the distinctions with classical
IR from the perspectives of two main IR evaluation methodologies: users studies
and test collections. Finally, Chapter 9 reports the conclusions of this
review, highlighting briefly some trends in Mobile IR that we believe will
drive research in the next few years.
"
2491,"Crowdsourcing open citations with CROCI -- An analysis of the current
  status of open citations, and a proposal","  In this paper, we analyse the current availability of open citations data in
one particular dataset, namely COCI (the OpenCitations Index of Crossref open
DOI-to-DOI citations; http://opencitations.net/index/coci) provided by
OpenCitations. The results of these analyses show a persistent gap in the
coverage of the currently available open citation data. In order to address
this specific issue, we propose a strategy whereby the community (e.g. scholars
and publishers) can directly involve themselves in crowdsourcing open
citations, by uploading their citation data via the OpenCitations
infrastructure into our new index, CROCI, the Crowdsourced Open Citations
Index.
"
2492,"Open data to evaluate academic researchers: an experiment with the
  Italian Scientific Habilitation","  The need for scholarly open data is ever increasing. While there are large
repositories of open access articles and free publication indexes, there are
still a few examples of free citation networks and their coverage is partial.
One of the results is that most of the evaluation processes based on citation
counts rely on commercial citation databases. Things are changing under the
pressure of the Initiative for Open Citations (I4OC), whose goal is to campaign
for scholarly publishers to make their citations as totally open. This paper
investigates the growth of open citations with an experiment on the Italian
Scientific Habilitation, the National process for University Professor
qualification which instead uses data from commercial indexes. We simulated the
procedure by only using open data and explored similarities and differences
with the official results. The outcomes of the experiment show that the amount
of open citation data currently available is not yet enough for obtaining
similar results.
"
2493,"Merits and Limits: Applying open data to monitor open access
  publications in bibliometric databases","  Identifying and monitoring Open Access (OA) publications might seem a trivial
task while practical efforts prove otherwise. Contradictory information arise
often depending on metadata employed. We strive to assign OA status to
publications in Web of Science (WOS) and Scopus while complementing it with
different sources of OA information to resolve contradicting cases. We linked
publications from WOS and Scopus via DOIs and ISSNs to Unpaywall, Crossref,
DOAJ and ROAD. Only about 50% of articles and reviews from WOS and Scopus could
be matched via a DOI to Unpaywall. Matching with Crossref brought 56 distinct
licences, which define in many cases the legally binding access status of
publications. But only 44% of publications hold only a single licence on
Crossref, while more than 50% have no licence information submitted to
Crossref. Contrasting OA information from Crossref licences with Unpaywall we
found contradictory cases overall amounting to more than 25%, which might be
partially explained by (ex-)including green OA. A further manual check found
about 17% of OA publications that are not accessible and 15% non-OA
publications that are accessible through publishers' websites. These
preliminary results suggest that identification of OA state of publications
denotes a difficult and currently unfulfilled task.
"
2494,"WikiLinkGraphs: A Complete, Longitudinal and Multi-Language Dataset of
  the Wikipedia Link Networks","  Wikipedia articles contain multiple links connecting a subject to other pages
of the encyclopedia. In Wikipedia parlance, these links are called internal
links or wikilinks. We present a complete dataset of the network of internal
Wikipedia links for the $9$ largest language editions. The dataset contains
yearly snapshots of the network and spans $17$ years, from the creation of
Wikipedia in 2001 to March 1st, 2018. While previous work has mostly focused on
the complete hyperlink graph which includes also links automatically generated
by templates, we parsed each revision of each article to track links appearing
in the main text. In this way we obtained a cleaner network, discarding more
than half of the links and representing all and only the links intentionally
added by editors. We describe in detail how the Wikipedia dumps have been
processed and the challenges we have encountered, including the need to handle
special pages such as redirects, i.e., alternative article titles. We present
descriptive statistics of several snapshots of this network. Finally, we
propose several research opportunities that can be explored using this new
dataset.
"
2495,GrapAL: Connecting the Dots in Scientific Literature,"  We introduce GrapAL (Graph database of Academic Literature), a versatile tool
for exploring and investigating a knowledge base of scientific literature, that
was semi-automatically constructed using NLP methods. GrapAL satisfies a
variety of use cases and information needs requested by researchers. At the
core of GrapAL is a Neo4j graph database with an intuitive schema and a simple
query language. In this paper, we describe the basic elements of GrapAL, how to
use it, and several use cases such as finding experts on a given topic for peer
reviewing, discovering indirect connections between biomedical entities and
computing citation-based metrics. We open source the demo code to help other
researchers develop applications that build on GrapAL.
"
2496,"Ex-ante measure of patent quality reveals intrinsic fitness for
  citation-network growth","  We have constructed a fitness parameter, characterizing the intrinsic
attractiveness for patents to be cited, from attributes of the associated
inventions known at the time a patent is granted. This exogenously obtained
fitness is shown to determine the temporal growth of the citation network in
conjunction with mechanisms of preferential attachment and obsolescence-induced
ageing that operate without reference to characteristics of individual patents.
Our study opens a window on understanding quantitatively the interplay of the
rich-gets-richer and fit-gets-richer paradigms that have been suggested to
govern the growth dynamics of real-world complex networks.
"
2497,The artist libraries project,"  The creation of the Artist Libraries Project was sparked by the observation
that artist libraries are still not well known, yet many art historians are
interested in this archive for the value it adds to understanding the person
behind the artist and his or her creative process. The problem is that these
libraries are rarely physically preserved. To remedy this dispersion, we built
an online database and a website www.lesbibliothequesdartistes.org that house
this valuable source in the form of lists of books and their electronic
versions. First data on Monet's library was made available, and several
additional artist libraries from the 19 th and 20 th centuries are on the way
for 2019. By gathering all these bibliographical data in a central database,
it's possible to explore one library and to compare several. This article
explains how we built the database and the website and how the implementation
of those IT tools has raised questions about the use of this resource as an
archive on the one hand, as well as its value for art history on the other.
"
2498,"Analysis of Coauthorship Network in Political Science using Centrality
  Measures","  In recent era, networks of data are growing massively and forming a shape of
complex structure. Data scientists try to analyze different complex networks
and utilize these networks to understand the complex structure of a network in
a meaningful way. There is a need to detect and identify such a complex network
in order to know how these networks provide communication means while using the
complex structure. Social network analysis provides methods to explore and
analyze such complex networks using graph theories, network properties and
community detection algorithms. In this paper, an analysis of coauthorship
network of Public Relation and Public Administration subjects of Microsoft
Academic Graph (MAG) is presented, using common centrality measures. The
authors belong to different research and academic institutes present all over
the world. Cohesive groups of authors have been identified and ranked on the
basis of centrality measures, such as betweenness, degree, page rank and
closeness. Experimental results show the discovery of authors who are good in
specific domain, have a strong field knowledge and maintain collaboration among
their peers in the field of Public Relations and Public Administration.
"
2499,Analysis of the Wikipedia Network of Mathematicians,"  We look at the network of mathematicians defined by the hyperlinks between
their biographies on Wikipedia. We show how to extract this information using
three snapshots of the Wikipedia data, taken in 2013, 2017 and 2018. We
illustrate how such Wikipedia data can be used by performing a centrality
analysis. These measures show that Hilbert and Newton are the most important
mathematicians. We use our example to illustrate the strengths and weakness of
centrality measures and to show how to provide estimates of the robustness of
centrality measurements. In part, we do this by comparison to results from two
other sources: an earlier study of biographies on the MacTutor website and a
small informal survey of the opinion of mathematics and physics students at
Imperial College London.
"
2500,"Can Google Scholar and Mendeley help to assess the scholarly impacts of
  dissertations?","  Dissertations can be the single most important scholarly outputs of junior
researchers. Whilst sets of journal articles are often evaluated with the help
of citation counts from the Web of Science or Scopus, these do not index
dissertations and so their impact is hard to assess. In response, this article
introduces a new multistage method to extract Google Scholar citation counts
for large collections of dissertations from repositories indexed by Google. The
method was used to extract Google Scholar citation counts for 77,884 American
doctoral dissertations from 2013-2017 via ProQuest, with a precision of over
95%. Some ProQuest dissertations that were dual indexed with other repositories
could not be retrieved with ProQuest-specific searches but could be found with
Google Scholar searches of the other repositories. The Google Scholar citation
counts were then compared with Mendeley reader counts, a known source of
scholarly-like impact data. A fifth of the dissertations had at least one
citation recorded in Google Scholar and slightly fewer had at least one
Mendeley reader. Based on numerical comparisons, the Mendeley reader counts
seem to be more useful for impact assessment purposes for dissertations that
are less than two years old, whilst Google Scholar citations are more useful
for older dissertations, especially in social sciences, arts and humanities.
Google Scholar citation counts may reflect a more scholarly type of impact than
that of Mendeley reader counts because dissertations attract a substantial
minority of their citations from other dissertations. In summary, the new
method now makes it possible for research funders, institutions and others to
systematically evaluate the impact of dissertations, although additional Google
Scholar queries for other online repositories are needed to ensure
comprehensive coverage.
"
2501,"Telling the Early Story of Solar Energy Meteorology by Applying
  (Co-Citation) Reference Publication Year Spectroscopy","  Studying the history of research fields by analyzing publication records and
topical and/or keyword searches with a full Reference Publication Year
Spectroscopy (RPYS) has been introduced as a powerful tool to identify the
corresponding root publications. However, for a rather new and
interdisciplinary research field like Solar Energy Meteorology (SEM), this
method is not feasible to get a reasonably exhaustive publication set.
Therefore we apply its variant RPYS-CO to all publications co-cited with one
highly important marker paper, using the CRExplorer for plotting and inspecting
the spectrogram of the number of cited references. Examining its peaks and
their main contributing publications, we get a list of seminal papers, which
are able to adequately tell us the story of SEM up to the 1990s. Generally, we
recommend this method to gain valuable insights in (new) research fields.
"
2502,Data-centric online ecosystem for digital materials science,"  Materials science is becoming increasingly more reliant on digital data to
facilitate progress in the field. Due to a large diversity in its scope,
breadth, and depth, organizing the data in a standard way to optimize the speed
and creative breadth of the resulting research represents a significant
challenge. We outline a modular and extensible ecosystem aimed at facilitating
research work performed in an accessible, collaborative, and agile manner,
without compromising on fidelity, security, and defensibility of the findings.
We discuss the critical components of the ecosystem and explain the
implementation of data standards and associated tools. We focus initial
attention on modeling and simulations from nanoscale and explain how to add
support for other domains. Finally, we discuss example applications or the data
convention and future outlook.
"
2503,"Representation Learning for Recommender Systems with Application to the
  Scientific Literature","  The scientific literature is a large information network linking various
actors (laboratories, companies, institutions, etc.). The vast amount of data
generated by this network constitutes a dynamic heterogeneous attributed
network (HAN), in which new information is constantly produced and from which
it is increasingly difficult to extract content of interest. In this article, I
present my first thesis works in partnership with an industrial company,
Digital Scientific Research Technology. This later offers a scientific watch
tool, Peerus, addressing various issues, such as the real time recommendation
of newly published papers or the search for active experts to start new
collaborations. To tackle this diversity of applications, a common approach
consists in learning representations of the nodes and attributes of this HAN
and use them as features for a variety of recommendation tasks. However, most
works on attributed network embedding pay too little attention to textual
attributes and do not fully take advantage of recent natural language
processing techniques. Moreover, proposed methods that jointly learn node and
document representations do not provide a way to effectively infer
representations for new documents for which network information is missing,
which happens to be crucial in real time recommender systems. Finally, the
interplay between textual and graph data in text-attributed heterogeneous
networks remains an open research direction.
"
2504,"Citation Needed: A Taxonomy and Algorithmic Assessment of Wikipedia's
  Verifiability","  Wikipedia is playing an increasingly central role on the web,and the policies
its contributors follow when sourcing and fact-checking content affect million
of readers. Among these core guiding principles, verifiability policies have a
particularly important role. Verifiability requires that information included
in a Wikipedia article be corroborated against reliable secondary sources.
Because of the manual labor needed to curate and fact-check Wikipedia at scale,
however, its contents do not always evenly comply with these policies.
Citations (i.e. reference to external sources) may not conform to verifiability
requirements or may be missing altogether, potentially weakening the
reliability of specific topic areas of the free encyclopedia. In this paper, we
aim to provide an empirical characterization of the reasons why and how
Wikipedia cites external sources to comply with its own verifiability
guidelines. First, we construct a taxonomy of reasons why inline citations are
required by collecting labeled data from editors of multiple Wikipedia language
editions. We then collect a large-scale crowdsourced dataset of Wikipedia
sentences annotated with categories derived from this taxonomy. Finally, we
design and evaluate algorithmic models to determine if a statement requires a
citation, and to predict the citation reason based on our taxonomy. We evaluate
the robustness of such models across different classes of Wikipedia articles of
varying quality, as well as on an additional dataset of claims annotated for
fact-checking purposes.
"
2505,"The FAIR Funder pilot programme to make it easy for funders to require
  and for grantees to produce FAIR Data","  There is a growing acknowledgement in the scientific community of the
importance of making experimental data machine findable, accessible,
interoperable, and reusable (FAIR). Recognizing that high quality metadata are
essential to make datasets FAIR, members of the GO FAIR Initiative and the
Research Data Alliance (RDA) have initiated a series of workshops to encourage
the creation of Metadata for Machines (M4M), enabling any self-identified
stakeholder to define and promote the reuse of standardized, comprehensive
machine-actionable metadata. The funders of scientific research recognize that
they have an important role to play in ensuring that experimental results are
FAIR, and that high quality metadata and careful planning for FAIR data
stewardship are central to these goals. We describe the outcome of a recent M4M
workshop that has led to a pilot programme involving two national science
funders, the Health Research Board of Ireland (HRB) and the Netherlands
Organisation for Health Research and Development (ZonMW). These funding
organizations will explore new technologies to define at the time that a
request for proposals is issued the minimal set of machine-actionable metadata
that they would like investigators to use to annotate their datasets, to enable
investigators to create such metadata to help make their data FAIR, and to
develop data-stewardship plans that ensure that experimental data will be
managed appropriately abiding by the FAIR principles. The FAIR Funders design
envisions a data-management workflow having seven essential stages, where
solution providers are openly invited to participate. The initial pilot
programme will launch using existing computer-based tools of those who attended
the M4M Workshop.
"
2506,"Como Mensurar a Import\^ancia, Influ\^encia e a Relev\^ancia de
  Usu\'arios do Twitter? Uma an\'alise da intera\c{c}\~ao dos candidatos \`a
  presid\^encia do Brasil nas elei\c{c}\~oes de 2018","  In the contemporary world, a significant number of people use social
networking services for a variety of purposes, including, but not limited to,
communicating, exchanging messages and searching for information. A popular
social network in the political arena is Twitter, a microblogging service for
posting messages of up to 280 characters, called ""tweets,"" where influential
politicians from various countries often use this medium to spread ideas and
make public statements. In this work, an analysis was made of the connections
of candidates for the presidency of the Republic of Brazil in the year 2018.
Using the analysis of complex networks to measure influence and relevance, a
metric was established able to quantify the importance of users in the network.
As part of the analysis, a Memory Algorithm was used to detect communities,
groups of strongly connected vertices (tweets) evidencing groupings of users.
"
2507,"Perception, Prestige and PageRank","  Academic esteem is difficult to quantify in objective terms. Network theory
offers the opportunity to use a mathematical formalism to model both the esteem
associated with an academic and the relationships between academic colleagues.
Early attempts using this line of reasoning have focused on intellectual
genealogy as constituted by supervisor student networks. The process of
examination is critical in many areas of study but has not played a part in
existing models. A network theoretical ""social"" model is proposed as a tool to
explore and understand the dynamics of esteem in the academic hierarchy. It is
observed that such a model naturally gives rise to the idea that the esteem
associated with a node in the graph (the esteem of an individual academic) can
be viewed as a dynamic quantity that evolves with time based on both local and
non-local changes in the properties in the network. The toy model studied here
includes both supervisor-student and examiner-student relationships. This gives
an insight into some of the key features of academic genealogies and naturally
leads to a proposed model for ""esteem propagation"" on academic networks. This
propagation is not solely directed forward in time (from teacher to progeny)
but sometimes also flows in the other direction. As collaborators do well, this
reflects well on those with whom they choose to collaborate and those that
taught them. Furthermore, esteem as a quantity continues to be dynamic even
after the end of a relationship or career. In other words, esteem can be
thought of as flowing both forward and backward in time.
"
2508,"Keeping out the Masses: Understanding the Popularity and Implications of
  Internet Paywalls","  Funding the production of quality online content is a pressing problem for
content producers. The most common funding method, online advertising, is rife
with well-known performance and privacy harms, and an intractable subject-agent
conflict: many users do not want to see advertisements, depriving the site of
needed funding.
  Because of these negative aspects of advertisement-based funding, paywalls
are an increasingly popular alternative for websites. This shift to a
""pay-for-access"" web is one that has potentially huge implications for the web
and society. Instead of a system where information (nominally) flows freely,
paywalls create a web where high quality information is available to fewer and
fewer people, leaving the rest of the web users with less information, that
might be also less accurate and of lower quality. Despite the potential
significance of a move from an ""advertising-but-open"" web to a ""paywalled"" web,
we find this issue understudied.
  This work addresses this gap in our understanding by measuring how widely
paywalls have been adopted, what kinds of sites use paywalls, and the
distribution of policies enforced by paywalls. A partial list of our findings
include that (i) paywall use is accelerating (2x more paywalls every 6 months),
(ii) paywall adoption differs by country (e.g. 18.75% in US, 12.69% in
Australia), (iii) paywalls change how users interact with sites (e.g. higher
bounce rates, less incoming links), (iv) the median cost of an annual paywall
access is $108 per site, and (v) paywalls are in general trivial to circumvent.
  Finally, we present the design of a novel, automated system for detecting
whether a site uses a paywall, through the combination of runtime browser
instrumentation and repeated programmatic interactions with the site. We intend
this classifier to augment future, longitudinal measurements of paywall use and
behavior.
"
2509,A Bibliometric Analysis of Publications in Computer Networking Research,"  This study uses the article content and metadata of four important computer
networking periodicals-IEEE Communications Surveys and Tutorials (COMST),
IEEE/ACM Transactions on Networking (TON), ACM Special Interest Group on Data
Communications (SIGCOMM), and IEEE International Conference on Computer
Communications (INFOCOM)-obtained using ACM, IEEE Xplore, Scopus and CrossRef,
for an 18-year period (2000-2017) to address important bibliometrics questions.
All of the venues are prestigious, yet they publish quite different research.
The first two of these periodicals (COMST and TON) are highly reputed journals
of the fields while SIGCOMM and INFOCOM are considered top conferences of the
field. SIGCOMM and INFOCOM publish new original research. TON has a similar
genre and publishes new original research as well as the extended versions of
different research published in the conferences such as SIGCOMM and INFOCOM,
while COMST publishes surveys and reviews (which not only summarize previous
works but highlight future research opportunities). In this study, we aim to
track the co-evolution of trends in the COMST and TON journals and compare them
to the publication trends in INFOCOM and SIGCOMM. Our analyses of the computer
networking literature include: (a) metadata analysis; (b) content-based
analysis; and (c) citation analysis. In addition, we identify the significant
trends and the most influential authors, institutes and countries, based on the
publication count as well as article citations. Through this study, we are
proposing a methodology and framework for performing a comprehensive
bibliometric analysis on computer networking research. To the best of our
knowledge, no such study has been undertaken in computer networking until now.
"
2510,An Explorative Study of GitHub Repositories of AI Papers,"  With the rapid development of AI technologies, thousands of AI papers are
being published each year. Many of these papers have released sample code to
facilitate follow-up researchers. This paper presents an explorative study of
over 1700 code repositories of AI papers hosted on GitHub. We find that these
repositories are often poorly written, lack of documents, lack of maintenance,
and hard to configure the underlying runtime environment. Thus, many code
repositories become inactive and abandoned. Such a situation makes follow-up
researchers hard to reproduce the results or do further research. In addition,
these hard-to-reuse code makes a gap between academia and industry. Based on
the findings, we give some recommendations on how to improve the quality of
code repositories of AI papers.
"
2511,A data analysis of women's trails among ICM speakers,"  The International Congress of Mathematicians (ICM), inaugurated in 1897, is
the greatest effort of the mathematical community to strengthen international
communication and connections across all mathematical fields. Meetings of the
ICM have historically hosted some of the most prominent mathematicians of their
time. Receiving an invitation to present a talk at an ICM signals the high
international reputation of the recipient, and is akin to entering a `hall of
fame for mathematics'. Women mathematicians attended the ICMs from the start.
With the invitation of Laura Pisati to present a lecture in 1908 in Rome and
the plenary talk of Emmy Noether in 1932 in Zurich, they entered the grand
international stage of their field. At the congress in 2014 in Seoul, Maryam
Mirzakhani became the first woman to be awarded the Fields Medal, the most
prestigious award in mathematics. In this article, we dive into assorted data
sources to follow the footprints of women among the ICM invited speakers,
analyzing their demographics and topic distributions, and providing glimpses
into their diverse biographies.
"
2512,Predicting Research Trends From Arxiv,"  We perform trend detection on two datasets of Arxiv papers, derived from its
machine learning (cs.LG) and natural language processing (cs.CL) categories.
Our approach is bottom-up: we first rank papers by their normalized citation
counts, then group top-ranked papers into different categories based on the
tasks that they pursue and the methods they use. We then analyze these
resulting topics. We find that the dominating paradigm in cs.CL revolves around
natural language generation problems and those in cs.LG revolve around
reinforcement learning and adversarial principles. By extrapolation, we predict
that these topics will remain lead problems/approaches in their fields in the
short- and mid-term.
"
2513,"DIALOG: A framework for modeling, analysis and reuse of digital forensic
  knowledge","  This paper presents DIALOG (Digital Investigation Ontology); a framework for
the management, reuse, and analysis of Digital Investigation knowledge. DIALOG
provides a general, application independent vocabulary that can be used to
describe an investigation at different levels of detail. DIALOG is defined to
encapsulate all concepts of the digital forensics field and the relationships
between them. In particular, we concentrate on the Windows Registry, where
registry keys are modeled in terms of both their structure and function.
Registry analysis software tools are modeled in a similar manner and we
illustrate how the interpretation of their results can be done using the
reasoning capabilities of ontology
"
2514,"Caution, DOI! Bibliographic detective story in the era of digitalization","  An example of inconsistencies in information provided by popular
bibliographic services is described and the reasons for these inconsistencies
are discussed.
"
2515,"The rhetorical structure of science? A multidisciplinary analysis of
  article headings","  An effective structure helps an article to convey its core message. The
optimal structure depends on the information to be conveyed and the
expectations of the audience. In the current increasingly interdisciplinary
era, structural norms can be confusing to the authors, reviewers and audiences
of scientific articles. Despite this, no prior study has attempted to assess
variations in the structure of academic papers across all disciplines. This
article reports on the headings commonly used by over 1 million research
articles from the PubMed Central Open Access collection, spanning 22 broad
categories covering all academia and 172 out of 176 narrow categories. The
results suggest that no headings are close to ubiquitous in any broad field and
that there are substantial differences in the extent to which most headings are
used. In the humanities, headings may be avoided altogether. Researchers should
therefore be aware of unfamiliar structures that are nevertheless legitimate
when reading, writing and reviewing articles.
"
2516,Science Quality and the Value of Inventions,"  Despite decades of research, the relationship between the quality of science
and the value of inventions has remained unclear. We present the result of a
large-scale matching exercise between 4.8 million patent families and 43
million publication records. We find a strong positive relationship between
quality of scientific contributions referenced in patents and the value of the
respective inventions. We rank patents by the quality of the science they are
linked to. Strikingly, high-rank patents are twice as valuable as low-rank
patents, which in turn are about as valuable as patents without direct science
link. We show this core result for various science quality and patent value
measures. The effect of science quality on patent value remains relevant even
when science is linked indirectly through other patents. Our findings imply
that what is considered ""excellent"" within the science sector also leads to
outstanding outcomes in the technological or commercial realm.
"
2517,Deep Patent Landscaping Model Using Transformer and Graph Embedding,"  Patent landscaping is a method used for searching related patents during a
research and development (R&D) project. To avoid the risk of patent
infringement and to follow current trends in technology, patent landscaping is
a crucial task required during the early stages of an R&D project. As the
process of patent landscaping requires advanced resources and can be tedious,
the demand for automated patent landscaping has been gradually increasing.
However, a shortage of well-defined benchmark datasets and comparable models
makes it difficult to find related research studies. In this paper, we propose
an automated patent landscaping model based on deep learning. To analyze the
text of patents, the proposed model uses a modified transformer structure. To
analyze the metadata of patents, we propose a graph embedding method that uses
a diffusion graph called Diff2Vec. Furthermore, we introduce four benchmark
datasets for comparing related research studies in patent landscaping. The
datasets are produced by querying Google BigQuery, based on a search formula
from a Korean patent attorney. The obtained results indicate that the proposed
model and datasets can attain state-of-the-art performance, as compared with
current patent landscaping models.
"
2518,The practice of self-citations: a longitudinal study,"  In this article, we discuss the outcomes of an experiment where we analysed
whether and to what extent the introduction, in 2012, of the new research
assessment exercise in Italy (a.k.a. Italian Scientific Habilitation) affected
self-citation behaviours in the Italian research community. The Italian
Scientific Habilitation attests to the scientific maturity of researchers and
in Italy, as in many other countries, is a requirement for accessing to a
professorship. To this end, we obtained from ScienceDirect 35,673 articles
published from 1957 and 2016 by the participants to the 2012 Italian Scientific
Habilitation, that resulted in the extraction of 1,379,050 citations retrieved
through Semantic Publishing technologies. Our analysis showed an overall
increment in author self-citations (i.e. where the citing article and the cited
article share at least one author) in several of the 24 academic disciplines
considered. However, we depicted a stronger causal relation between such
increment and the rules introduced by the 2012 Italian Scientific Habilitation
in 10 out of 24 disciplines analysed.
"
2519,"Data objects and documenting scientific processes: An analysis of data
  events in biodiversity data papers","  The data paper, an emerging scholarly genre, describes research datasets and
is intended to bridge the gap between the publication of research data and
scientific articles. Research examining how data papers report data events,
such as data transactions and manipulations, is limited. The research reported
on in this paper addresses this limitation and investigated how data events are
inscribed in data papers. A content analysis was conducted examining the full
texts of 82 data papers, drawn from the curated list of data papers connected
to the Global Biodiversity Information Facility (GBIF). Data events recorded
for each paper were organized into a set of 17 categories. Many of these
categories are described together in the same sentence, which indicates the
messiness of data events in the laboratory space. The findings challenge the
degrees to which data papers are a distinct genre compared to research papers
and they describe data-centric research processes in a through way. This paper
also discusses how our results could inform a better data publication ecosystem
in the future.
"
2520,"The $CI$-index: a new index to characterize the scientific output of
  researchers","  We propose a simple new index, named the $CI$-index, based on the Choquet
integral to characterize the scientific output of researchers. This index is an
improvement of the $A$-index and $R$-index and has a notable feature that
highly cited papers have highly weights and lowly cited papers have lowly
weights. In applications many researchers may have the same $h$-index,
$g$-index or $R$-index. The $CI$-index can be provided an effective method of
distinguish among such researchers.
"
2521,Availability of Hyperlinked Resources in Astrophysics Papers,"  Astrophysics papers often rely on software which may or may not be available,
and URLs are often used as proxy citations for software and data. We extracted
all URLs from two journals' 2015 research articles, removed those from certain
long-term reliable domains, and tested the remainder to determine what
percentage of these URLs were accessible in October 2018.
"
2522,Early-career setback and future career impact,"  Setbacks are an integral part of a scientific career, yet little is known
about whether an early-career setback may augment or hamper an individual's
future career impact. Here we examine junior scientists applying for U.S.
National Institutes of Health (NIH) R01 grants. By focusing on grant proposals
that fell just below and just above the funding threshold, we compare
""near-miss"" with ""near-win"" individuals to examine longer-term career outcomes.
Our analyses reveal that an early-career near miss has powerful, opposing
effects. On one hand, it significantly increases attrition, with one near miss
predicting more than a 10% chance of disappearing permanently from the NIH
system. Yet, despite an early setback, individuals with near misses
systematically outperformed those with near wins in the longer run, as their
publications in the next ten years garnered substantially higher impact. We
further find that this performance advantage seems to go beyond a screening
mechanism, whereby a more selected fraction of near-miss applicants remained
than the near winners, suggesting that early-career setback appears to cause a
performance improvement among those who persevere. Overall, the findings are
consistent with the concept that ""what doesn't kill me makes me stronger.""
Whereas science is often viewed as a setting where early success begets future
success, our findings unveil an intimate yet previously unknown relationship
where early-career setback can become a marker for future achievement, which
may have broad implications for identifying, training and nurturing junior
scientists whose career will have lasting impact.
"
2523,Should Citations be Counted Separately from Each Originating Section,"  Articles are cited for different purposes and differentiating between reasons
when counting citations may therefore give finer-grained citation count
information. Although identifying and aggregating the individual reasons for
each citation may be impractical, recording the number of citations that
originate from different article sections might illuminate the general reasons
behind a citation count (e.g., 110 citations = 10 Introduction citations + 100
Methods citations). To help investigate whether this could be a practical and
universal solution, this article compares 19 million citations with DOIs from
six different standard sections in 799,055 PubMed Central open access articles
across 21 out of 22 fields. There are apparently non-systematic differences
between fields in the most citing sections and the extent to which citations
from one section overlap with citations from another, with some degree of
overlap in most cases. Thus, at a science-wide level, section headings are
partly unreliable indicators of citation context, even if they are more
standard within individual fields. They may still be used within fields to help
identify individual highly cited articles that have had one type of impact,
especially methodological (Methods) or context setting (Introduction), but
expert judgement is needed to validate the results.
"
2524,"Quantifying dynamics of failure across science, startups, and security","  Human achievements are often preceded by repeated attempts that initially
fail, yet little is known about the mechanisms governing the dynamics of
failure. Here, building on the rich literature on innovation, human dynamics
and learning, we develop a simple one-parameter model that mimics how
successful future attempts build on those past. Analytically solving this model
reveals a phase transition that separates dynamics of failure into regions of
stagnation or progression, predicting that near the critical threshold, agents
who share similar characteristics and learning strategies may experience
fundamentally different outcomes following failures. Below the critical point,
we see those who explore disjoint opportunities without a pattern of
improvement, and above it, those who exploit incremental refinements to
systematically advance toward success. The model makes several empirically
testable predictions, demonstrating that those who eventually succeed and those
who do not may be initially similar, yet are characterized by fundamentally
distinct failure dynamics in terms of the efficiency and quality of each
subsequent attempt. We collected large-scale data from three disparate domains,
tracing repeated attempts by (i) NIH investigators to fund their research, (ii)
innovators to successfully exit their startup ventures, and (iii) terrorist
organizations to post casualties in violent attacks, finding broadly consistent
empirical support across all three domains. Together, our findings unveil
identifiable yet previously unknown early signals that allow us to identify
failure dynamics that will lead to ultimate victory or defeat. Given the
ubiquitous nature of failures and the paucity of quantitative approaches to
understand them, these results represent a crucial step toward deeper
understanding of the complex dynamics beneath failures, the essential
prerequisites for success.
"
2525,"ReviewerNet: Visualizing Citation and Authorship Relations for Finding
  Reviewers","  We propose ReviewerNet, an online, interactive visualization system aimed to
improve the reviewer selection process in the academic domain. Given a paper
submitted for publication, we assume that good candidate reviewers can be
chosen among the authors of a small set of relevant and pertinent papers;
ReviewerNet supports the construction of such set of papers, by visualizing and
exploring a literature citation network. Then, the system helps to select
reviewers that are both well distributed in the scientific community and that
do not have any conflict-of-interest, by visualising the careers and
co-authorship relations of candidate reviewers. The system is publicly
available, and it has been evaluated by a set of experienced researchers in the
field of Computer Graphics.
"
2526,"An analysis of the evolution of science-technology linkage in
  biomedicine","  Demonstrating the practical value of public research has been an important
subject in science policy. Here we present a detailed study on the evolution of
the citation linkage between life science related patents and biomedical
research over a 37-year period. Our analysis relies on a newly-created dataset
that systematically links millions of non-patent references to biomedical
papers. We find a large disparity in the volume of science linkage among
technology sectors, with biotechnology and drug patents dominating it. The
linkage has been growing exponentially over a long period of time, doubling
every 2.9 years. The U.S. has been the largest producer of cited science for
years, receiving nearly half of the citations. More than half of citations goes
to universities. We use a new paper-level indicator to quantify to what extent
a paper is basic research or clinical medicine. We find that the cited papers
are likely to be basic research, yet a significant portion of papers cited in
patents that are related to FDA-approved drugs are clinical research. The U.S.
National Institute of Health continues to be an important funder of cited
science. For the majority of companies, more than half of citations in their
patents are authored by public research. Taken together, these results indicate
a continuous linkage of public science to private sector inventions.
"
2527,"Like-for-like bibliometric substitutes for peer review: advantages and
  limits of indicators calculated from the ep index","  The use of bibliometric indicators would simplify research assessments. The
2014 Research Excellence Framework (REF) is a peer review assessment of UK
universities, whose results can be taken as benchmarks for bibliometric
indicators. In this study we use the REF results to investigate whether the ep
index and a top percentile of most cited papers could substitute for peer
review. The probability that a random university's paper reaches a certain top
percentile in the global distribution of papers is a power of the ep index,
which can be calculated from the citation-based distribution of university's
papers in global top percentiles. Making use of the ep index in each university
and research area, we calculated the ratios between the percentage of
4-star-rated outputs in REF and the percentages of papers in global top
percentiles. Then, we fixed the assessment percentile so that the mean ratio
between these two indicators across universities is 1.0. This method was
applied to four units of assessment in REF: Chemistry, Economics & Econometrics
joined to Business & Management Studies, and Physics. Some relevant deviations
from the 1.0 ratio could be explained by the evaluation procedure in REF or by
the characteristics of the research field; other deviations need specific
studies by experts in the research area. The present results indicate that in
many research areas the substitution of a top percentile indicator for peer
review is possible. However, this substitution cannot be made
straightforwardly; more research is needed to establish the conditions of the
bibliometric assessment.
"
2528,From closed to open access: A case study of flipped journals,"  In recent years, increased stakeholder pressure to transition research to
Open Access has led to many journals ""flipping"" from a toll access to an open
access publishing model. Changing the publishing model can influence the
decision of authors to submit their papers to a journal, and increased article
accessibility may influence citation behaviour. The aim of this paper is to
show changes in the number of published articles and citations after the
flipping of a journal. We analysed a set of 171 journals in the Web of Science
(WoS) which flipped to open access. In addition to comparing the number of
articles, average relative citation (ARC) and normalized impact factor (IF) are
applied, respectively, as bibliometric indicators at the article and journal
level, to trace the transformation of flipped journals covered. Our results
show that flipping mostly has had positive effects on journal's IF. But it has
had no obvious citation advantage for the articles. We also see a decline in
the number of published articles after flipping. We can conclude that flipping
to open access can improve the performance of journals, despite decreasing the
tendency of authors to submit their articles and no better citation advantages
for articles.
"
2529,Highly cited references in PLOS ONE and their in-text usage over time,"  In this article, we describe highly cited publications in a PLOS ONE
full-text corpus. For these publications, we analyse the citation contexts
concerning their position in the text and their age at the time of citing. By
selecting the perspective of highly cited papers, we can distinguish them based
on the context during citation even if we do not have any other information
source or metrics. We describe the top cited references based on how, when and
in which context they are cited. The focus of this study is on a time
perspective to explain the nature of the reception of highly cited papers. We
have found that these references are distinguishable by the IMRaD sections of
their citation. And further, we can show that the section usage of highly cited
papers is time-dependent: the longer the citation interval, the higher the
probability that a reference is cited in a method section.
"
2530,"Assessing Simulations of Imperial Dynamics and Conflict in the Ancient
  World","  The development of models to capture large-scale dynamics in human history is
one of the core contributions of cliodynamics. Most often, these models are
assessed by their predictive capability on some macro-scale and aggregated
measure and compared to manually curated historical data. In this report, we
consider the model from Turchin et al. (2013), where the evaluation is done on
the prediction of ""imperial density"": the relative frequency with which a
geographical area belonged to large-scale polities over a certain time window.
We implement the model and release both code and data for reproducibility. We
then assess its behaviour against three historical data sets: the relative size
of simulated polities vs historical ones; the spatial correlation of simulated
imperial density with historical population density; the spatial correlation of
simulated conflict vs historical conflict. At the global level, we show good
agreement with population density ($R^2 < 0.75$), and some agreement with
historical conflict in Europe ($R^2 < 0.42$). The model instead fails to
reproduce the historical shape of individual polities. Finally, we tweak the
model to behave greedily by having polities preferentially attacking weaker
neighbours. Results significantly degrade, suggesting that random attacks are a
key trait of the original model. We conclude by proposing a way forward by
matching the probabilistic imperial strength from simulations to inferred
networked communities from real settlement data.
"
2531,"Penobscot Dataset: Fostering Machine Learning Development for Seismic
  Interpretation","  We have seen in the past years the flourishing of machine and deep learning
algorithms in several applications such as image classification and
segmentation, object detection and recognition, among many others. This was
only possible, in part, because datasets like ImageNet -- with +14 million
labeled images -- were created and made publicly available, providing
researches with a common ground to compare their advances and extend the
state-of-the-art. Although we have seen an increasing interest in machine
learning in geosciences as well, we will only be able to achieve a significant
impact in our community if we collaborate to build such a common basis. This is
even more difficult when it comes to the Oil&Gas industry, in which
confidentiality and commercial interests often hinder the sharing of datasets
with others. In this letter, we present the Penobscot interpretation dataset,
our contribution to the development of machine learning in geosciences, more
specifically in seismic interpretation. The Penobscot 3D seismic dataset was
acquired in the Scotian shelf, offshore Nova Scotia, Canada. The data is
publicly available and comprises pre- and pos-stack data, 5 horizons and well
logs of 2 wells. However, for the dataset to be of practical use for our tasks,
we had to reinterpret the seismic, generating 7 horizons separating different
seismic facies intervals. The interpreted horizons were used to generated
+100,000 labeled images for inlines and crosslines. To demonstrate the utility
of our dataset, results of two experiments are presented.
"
2532,"Mapping social media attention in Microbiology: Identifying main topics
  and actors","  This paper aims to map and identify topics of interest within the field of
Microbiology and identify the main sources driving such attention. We combine
data from Web of Science and Altmetric.com, a platform which retrieves mentions
to scientific literature from social media and other non-academic communication
outlets. We focus on the dissemination of microbial publications in Twitter,
news media and policy briefs. A two-mode network of social accounts shows
distinctive areas of activity. We identify a cluster of papers mentioned solely
by regional news media. A central area of the network is formed by papers
discussed by the three outlets. A large portion of the network is driven by
Twitter activity. When analyzing top actors contributing to such network, we
observe that more than half of the Twitter accounts are bots, mentioning 32% of
the documents in our dataset. Within news media outlets, there is a
predominance of popular science outlets. With regard to policy briefs, both
international and national bodies are represented. Finally, our topic analysis
shows that the thematic focus of papers mentioned varies by outlet. While news
media cover the wider range of topics, policy briefs are focused on
translational medicine, and bacterial outbreaks.
"
2533,arXiv and the Symbiosis of Physics Preprints and Journal Review Articles,"  New thinking needs to emerge about how to reform publishing along lines that
best meet two perennial needs of scientific communication. This paper discusses
a model that addresses these two needs with respect to physics. Given the
considerable barriers that its realization in pristine form faces, the model
aspires merely to be a heuristic or guidepost. It provides an analytical
framework for criticizing aspects of the current publishing ecosystem, helps
diagnose problems in current efforts to reform it, including those emanating
from the open access movement, and raises consciousness about certain emphases
that could gradually enrich scholarly publishing. [VERSION 2 , 5/19/2019;
SUCCESSIVE REVISIONS WILL OCCUR THROUGHOUT 2019.]
"
2534,"Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence
  Labeling","  Contextualized word embeddings such as ELMo and BERT provide a foundation for
strong performance across a wide range of natural language processing tasks by
pretraining on large corpora of unlabeled text. However, the applicability of
this approach is unknown when the target domain varies substantially from the
pretraining corpus. We are specifically interested in the scenario in which
labeled data is available in only a canonical source domain such as newstext,
and the target domain is distinct from both the labeled and pretraining texts.
To address this scenario, we propose domain-adaptive fine-tuning, in which the
contextualized embeddings are adapted by masked language modeling on text from
the target domain. We test this approach on sequence labeling in two
challenging domains: Early Modern English and Twitter. Both domains differ
substantially from existing pretraining corpora, and domain-adaptive
fine-tuning yields substantial improvements over strong BERT baselines, with
particularly impressive results on out-of-vocabulary words. We conclude that
domain-adaptive fine-tuning offers a simple and effective approach for the
unsupervised adaptation of sequence labeling to difficult new domains.
"
2535,"Citation gaming induced by bibliometric evaluation: a country-level
  comparative analysis","  It is several years since national research evaluation systems around the
globe started making use of quantitative indicators to measure the performance
of researchers. Nevertheless, the effects on these systems on the behavior of
the evaluated researchers are still largely unknown. We attempt to shed light
on this topic by investigating how Italian researchers reacted to the
introduction in 2011 of national regulations in which key passages of
professional careers are governed by bibliometric indicators. A new inwardness
measure, able to gauge the degree of scientific self-referentiality of a
country, is defined as the proportion of citations coming from the country
itself compared to the total number of citations gathered by the country.
Compared to the trends of the other G10 countries in the period 2000-2016,
Italy's inwardness shows a net increase after the introduction of the new
evaluation rules. Indeed, globally and also for a large majority of the
research fields, Italy became the European country with the highest inwardness.
Possible explanations are proposed and discussed, concluding that the observed
trends are strongly suggestive of a generalized strategic use of citations,
both in the form of author self-citations and of citation clubs. We argue that
the Italian case offers crucial insights on the constitutive effects of
evaluation systems. As such, it could become a paradigmatic case in the debate
about the use of indicators in science-policy contexts.
"
2536,AMRec: An Intelligent System for Academic Method Recommendation,"  Finding new academic Methods for research problems is the key task in a
researcher's research career. It is usually very difficult for new researchers
to find good Methods for their research problems since they lack of research
experiences. In order to help researchers carry out their researches in a more
convenient way, we describe a novel recommendation system called AMRec to
recommend new academic Methods for research problems in this paper. Our
proposed system first extracts academic concepts (Tasks and Methods) and their
relations from academic literatures, and then leverages the regularized matrix
factorization Method for academic Method recommendation. Preliminary evaluation
results verify the effectiveness of our proposed system.
"
2537,Distribution of scientific journals impact factor,"  We consider distributions of scientific journals impact factor. Analysing
9028 scientific journals with the largest impact factors, we found that the
distribution of them is year-to-year stable (at least for analysed 2011-2013
years), and it has the character of the exponential Boltzmann distribution with
the power law asymptotic (tail).
"
2538,Event-based Access to Historical Italian War Memoirs,"  The progressive digitization of historical archives provides new, often
domain specific, textual resources that report on facts and events which have
happened in the past; among these, memoirs are a very common type of primary
source. In this paper, we present an approach for extracting information from
Italian historical war memoirs and turning it into structured knowledge. This
is based on the semantic notions of events, participants and roles. We evaluate
quantitatively each of the key-steps of our approach and provide a graph-based
representation of the extracted knowledge, which allows to move between a Close
and a Distant Reading of the collection.
"
2539,"Ontologies-based Architecture for Sociocultural Knowledge
  Co-Construction Systems","  Considering the evolution of the semantic wiki engine based platforms, two
main approaches could be distinguished: Ontologies for Wikis (OfW) and Wikis
for Ontologies (WfO). OfW vision requires existing ontologies to be imported.
Most of them use the RDF-based (Resource Description Framework) systems in
conjunction with the standard SQL (Structured Query Language) database to
manage and query semantic data. But, relational database is not an ideal type
of storage for semantic data. A more natural data model for SMW (Semantic
MediaWiki) is RDF, a data format that organizes information in graphs rather
than in fixed database tables. This paper presents an ontology based
architecture, which aims to implement this idea. The architecture mainly
includes three layered functional architectures: Web User Interface Layer,
Semantic Layer and Persistence Layer.
"
2540,"COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations","  In this paper, we present COCI, the OpenCitations Index of Crossref open
DOI-to-DOI citations (http://opencitations.net/index/coci). COCI is the first
open citation index created by OpenCitations, in which we have applied the
concept of citations as first-class data entities, and it contains more than
445 million DOI-to-DOI citation links derived from the data available in
Crossref. These citations are described in RDF by means of the newly extended
version of the OpenCitations Data Model (OCDM). We introduce the workflow we
have developed for creating these data, and also show the additional services
that facilitate the access to and querying of these data via different access
points: a SPARQL endpoint, a REST API, bulk downloads, Web interfaces, and
direct access to the citations via HTTP content negotiation. Finally, we
present statistics regarding the use of COCI citation data, and we introduce
several projects that have already started to use COCI data for different
purposes.
"
2541,Journal ranking should depend on the level of aggregation,"  Journal ranking is becoming more important in assessing the quality of
academic research. Several indices have been suggested for this purpose,
typically on the basis of a citation graph between the journals. We follow an
axiomatic approach and find an impossibility theorem: any self-consistent
ranking method, which satisfies a natural monotonicity property, should depend
on the level of aggregation. Our result presents a trade-off between two
axiomatic properties and reveals a dilemma of aggregation.
"
2542,"Explore with caution: mapping the evolution of scientific interest in
  Physics","  In the book The Essential Tension Thomas Kuhn described the conflict between
tradition and innovation in scientific research --i.e., the desire to explore
new promising areas, counterposed to the need to capitalize on the work done in
the past. While it is true that along their careers many scientists probably
felt this tension, only few works have tried to quantify it. Here, we address
this question by analyzing a large-scale dataset, containing all the papers
published by the American Physical Society (APS) in more than $25$ years, which
allows for a better understanding of scientists' careers evolution in Physics.
We employ the Physics and Astronomy Classification Scheme (PACS) present in
each paper to map the scientific interests of $181,397$ authors and their
evolution along the years. Our results indeed confirm the existence of the
`essential tension' with scientists balancing between exploring the boundaries
of their area and exploiting previous work. In particular, we found that
although the majority of physicists change the topics of their research, they
stay within the same broader area thus exploring with caution new scientific
endeavors. Furthermore, we quantify the flows of authors moving between
different subfields and pinpoint which areas are more likely to attract or
donate researchers to the other ones. Overall, our results depict a very
distinctive portrait of the evolution of research interests in Physics and can
help in designing specific policies for the future.
"
2543,Female scholars need to achieve more for equal public recognition,"  Different kinds of ""gender gap"" have been reported in different walks of the
scientific life, almost always favouring male scientists over females. In this
work, for the first time, we present a large-scale empirical analysis to ask
whether female scientists with the same level of scientific accomplishment are
as likely as males to be recognised. We particularly focus on Wikipedia, the
open online encyclopedia that its open nature allows us to have a proxy of
community recognition. We calculate the probability of appearing on Wikipedia
as a scientist for both male and female scholars in three different fields. We
find that women in Physics, Economics and Philosophy are considerable less
likely than men to be recognised on Wikipedia across all levels of achievement.
"
2544,"Go Wide, Go Deep: Quantifying the Impact of Scientific Papers through
  Influence Dispersion Trees","  Despite a long history of use of citation count as a measure to assess the
impact or influence of a scientific paper, the evolution of follow-up work
inspired by the paper and their interactions through citation links have rarely
been explored to quantify how the paper enriches the depth and breadth of a
research field. We propose a novel data structure, called Influence Dispersion
Tree (IDT) to model the organization of follow-up papers and their dependencies
through citations. We also propose the notion of an ideal IDT for every paper
and show that an ideal (highly influential) paper should increase the knowledge
of a field vertically and horizontally. Upon suitably exploring the structural
properties of IDT, we derive a suite of metrics, namely Influence Dispersion
Index (IDI), Normalized Influence Divergence (NID) to quantify the influence of
a paper. Our theoretical analysis shows that an ideal IDT configuration should
have equal depth and breadth (and thus minimize the NID value). We establish
the superiority of NID as a better influence measure in two experimental
settings. First, on a large real-world bibliographic dataset, we show that NID
outperforms raw citation count as an early predictor of the number of new
citations a paper will receive within a certain period after publication.
Second, we show that NID is superior to the raw citation count at identifying
the papers recognized as highly influential through Test of Time Award among
all their contemporary papers (published in the same venue). We conclude that
in order to quantify the influence of a paper, along with the total citation
count, one should also consider how the citing papers are organized among
themselves to better understand the influence of a paper on the research field.
For reproducibility, the code and datasets used in this study are being made
available to the community.
"
2545,"Sleeping Beauties in Medical Research: Technological Relevance, High
  Scientific Impact","  We investigate Sleeping Beauties (SBs) in medical research with a special
focus on SBs cited in patents. We find that the increasing trend of the
relative number of SBs comes to an end around 1998. However, still a constant
fraction of publications becomes an SB. Many SBs become highly cited
publications, they even belong to the top-10 to 20% most cited publications in
their field. We measured the scaling of the number of SBs with sleeping period
length, during-sleep citation-intensity, and with awake citation-intensity. We
determined the Grand Sleeping Beauty Equation which shows that the probability
of awakening after a deep sleep is becoming rapidly smaller for longer sleeping
periods and that the probability for higher awakening intensities decreases
extremely rapidly. Scaling exponents show a time-dependent behavior which
suggests a decreasing occurrence of SBs with longer sleeping periods. We
demonstrate that the fraction of SBs cited by patents before awakening is
exponentially increasing. This finding shows that the technological time lag is
becoming shorter than the sleeping time. Inventor-author self-citations may
result in shorter technological time lags, but this effect is small. Finally,
we discuss characteristics of an SBs that became one of the highest cited
medical papers ever.
"
2546,Software Tools for Big Data Resources in Family Names Dictionaries,"  This paper describes the design and development of specific software tools
used during the creation of Family Names in Britain and Ireland (FaNBI)
research project, started by the University of the West of England in 2010 and
finished successfully in 2016. First, the overview of the project and
methodology is provided. Next section contains the description of dictionary
management tools and software tools to combine input data resources.
"
2547,Venue Analytics: A Simple Alternative to Citation-Based Metrics,"  We present a method for automatically organizing and evaluating the quality
of different publishing venues in Computer Science. Since this method only
requires paper publication data as its input, we can demonstrate our method on
a large portion of the DBLP dataset, spanning 50 years, with millions of
authors and thousands of publishing venues. By formulating venue authorship as
a regression problem and targeting metrics of interest, we obtain venue scores
for every conference and journal in our dataset. The obtained scores can also
provide a per-year model of conference quality, showing how fields develop and
change over time. Additionally, these venue scores can be used to evaluate
individual academic authors and academic institutions. We show that using venue
scores to evaluate both authors and institutions produces quantitative measures
that are comparable to approaches using citations or peer assessment. In
contrast to many other existing evaluation metrics, our use of large-scale,
openly available data enables this approach to be repeatable and transparent.
  To help others build upon this work, all of our code and data is available at
https://github.com/leonidk/venue_scores
"
2548,"Author name disambiguation of bibliometric data: A comparison of several
  unsupervised approaches","  Adequately disambiguating author names in bibliometric databases is a
precondition for conducting reliable analyses at the author level. In the case
of bibliometric studies that include many researchers, it is not possible to
disambiguate each single researcher manually. Several approaches have been
proposed for author name disambiguation but there has not yet been a comparison
of them under controlled conditions. In this study, we compare a set of
unsupervised disambiguation approaches. Unsupervised approaches specify a model
to assess the similarity of author mentions a priori instead of training a
model with labelled data. In order to evaluate the approaches, we applied them
to a set of author mentions annotated with a ResearcherID, this being an author
identifier maintained by the researchers themselves. Apart from comparing the
overall performance, we take a more detailed look at the role of the
parametrization of the approaches and analyse the dependence of the results on
the complexity of the disambiguation task. It could be shown that all of the
evaluated approaches produce better results than those that can be obtained by
using only author names. In the context of this study, the approach proposed by
Caron and van Eck (2014) produced the best results.
"
2549,"Female citation impact superiority 1996-2018 in six out of seven
  English-speaking nations","  Efforts to combat continuing gender inequalities in academia need to be
informed by evidence about where differences occur. Citations are relevant as
potential evidence in appointment and promotion decisions, but it is unclear
whether there have been historical gender differences in average citation
impact that might explain the current shortfall of senior female academics.
This study investigates the evolution of gender differences in citation impact
1996-2018 for six million articles from seven large English-speaking nations:
Australia, Canada, Ireland, Jamaica, New Zealand, UK, and the USA. The results
show that a small female citation advantage has been the norm over time for all
these countries except the USA, where there has been no practical difference.
The female citation advantage is largest, and statistically significant in most
years, for Australia and the UK. This suggests that any academic bias against
citing female authored research cannot explain current employment inequalities.
Nevertheless, comparisons using recent citation data, or avoiding it
altogether, during appointments or promotion may disadvantage females in some
countries by underestimating the likely impact of their work, especially in the
long term.
"
2550,"Community Detection and Growth Potential Prediction Using the Stochastic
  Block Model and the Long Short-term Memory from Patent Citation Networks","  Scoring patent documents is very useful for technology management. However,
conventional methods are based on static models and, thus, do not reflect the
growth potential of the technology cluster of the patent. Because even if the
cluster of a patent has no hope of growing, we recognize the patent is
important if PageRank or other ranking score is high. Therefore, there arises a
necessity of developing citation network clustering and prediction of future
citations. In our research, clustering of patent citation networks by
Stochastic Block Model was done with the aim of enabling corporate managers and
investors to evaluate the scale and life cycle of technology. As a result, we
confirmed nested SBM is appropriate for graph clustering of patent citation
networks. Also, a high MAPE value was obtained and the direction accuracy
achieved a value greater than 50% when predicting growth potential for each
cluster by using LSTM.
"
2551,"The State of Open Access in Germany: An Analysis of the Publication
  Output of German Universities","  Starting with the Berlin declaration in 2003, Open Access (OA) publishing has
established a new era of scholarly communication due to the unrestricted
electronic access to peer reviewed publications. OA offers a number of benefits
like e.g. increased citation counts (Gargouri et al., 2010) and enhanced
visibility and accessibility of research output (Tennant et al., 2016). The OA
movement with its powerful mandating and policymaking has been very successful
in recent years. Relatively little is known about the real effects of these
activities in terms of OA publication output of institutions on a larger scale
(Piwowar et al., 2018). The aim of this article is to investigate to what
extent the OA fraction of the publication output of German universities has
increased in the last years. To answer this question, we analysed and compared
total number of publications which have been published by researchers of the
largest German universities. We compared the numbers of OA versus closed
publications for 66 large German universities in the time span of 2000-2017.
"
2552,"Reproducible Research is more than Publishing Research Artefacts: A
  Systematic Analysis of Jupyter Notebooks from Research Articles","  With the advent of Open Science, researchers have started to publish their
research artefacts (i. e., data, software, and other products of the
investigations) in order to allow others to reproduce their investigations.
While this publication is beneficial for science in general, it often lacks a
comprehensive documentation and completeness with respect to the artefacts.
This, in turn, prevents the successful reproduction of the analyses. Typical
examples are missing scripts, incomplete datasets or specification of used
software. Moreover, issues about licences often create legal concerns. This is
true for the use of commercial software but also for the publication of
research artefacts without proper sharing licence. As a result, the sole
publication of research artefacts does not automatically result in reproducible
research.
  To empirically confirm this, we have been systematically analysing research
publications that also published their investigations as Jupyter notebooks. In
this paper, we present preliminary results of this analysis for five
publications. The results show, that the quality of the published research
artefacts must be improved in order to assure reproducibility.
"
2553,"Time-series Insights into the Process of Passing or Failing Online
  University Courses using Neural-Induced Interpretable Student States","  This paper addresses a key challenge in Educational Data Mining, namely to
model student behavioral trajectories in order to provide a means for
identifying students most at-risk, with the goal of providing supportive
interventions. While many forms of data including clickstream data or data from
sensors have been used extensively in time series models for such purposes, in
this paper we explore the use of textual data, which is sometimes available in
the records of students at large, online universities. We propose a time series
model that constructs an evolving student state representation using both
clickstream data and a signal extracted from the textual notes recorded by
human mentors assigned to each student. We explore how the addition of this
textual data improves both the predictive power of student states for the
purpose of identifying students at risk for course failure as well as for
providing interpretable insights about student course engagement processes.
"
2554,"The Literary Theme Ontology for Media Annotation and Information
  Retrieval","  Literary theme identification and interpretation is a focal point of literary
studies scholarship. Classical forms of literary scholarship, such as close
reading, have flourished with scarcely any need for commonly defined literary
themes. However, the rise in popularity of collaborative and algorithmic
analyses of literary themes in works of fiction, together with a requirement
for computational searching and indexing facilities for large corpora, creates
the need for a collection of shared literary themes to ensure common
terminology and definitions. To address this need, we here introduce a first
draft of the Literary Theme Ontology. Inspired by a traditional framing from
literary theory, the ontology comprises literary themes drawn from the authors
own analyses, reference books, and online sources. The ontology is available at
https://github.com/theme-ontology/lto under a Creative Commons Attribution 4.0
International license (CC BY 4.0).
"
2555,The Price of Gold: Curiosity?,"  Gold open access as characterised by the payment of an article processing
charge (APC) has become one of the dominant models in open access publication.
This paper examines an extreme hypothetical case in which the APC model is the
only model and the systematic issues that could develop in such a scenario.
"
2556,Within-Journal Self-citations and the Pinski-Narin Influence Weights,"  The Journal Impact Factor (JIF) is linearly sensitive to self-citations
because each self-citation adds to the numerator, whereas the denominator is
not affected. Pinski & Narin (1976) derived the Influence Weight (IW) as an
alternative to Garfield's JIF. Whereas the JIF is based on raw citation counts
normalized by the number of publications, IWs are based on the eigenvectors in
the matrix of aggregated journal-journal citations without a reference to size:
the cited and citing sides are combined by a matrix approach. IWs emerge as a
vector after recursive iteration of the normalized matrix. Before recursion, IW
is a (vector-based) non-network indicator of impact, but after recursion (i.e.
repeated improvement by iteration), IWs can be considered a network measure of
prestige among the journals in the (sub)graph as a representation of a field of
science. As a consequence (not intended by Pinski & Narin in 1976), the
self-citations are integrated at the field level and no longer disturb the
analysis as outliers. In our opinion, this is a very desirable property of a
measure of quality or impact. As illustrations, we use data of journal citation
matrices already studied in the literature, and also the complete set of data
in the Journal Citation Reports 2017 (n = 11,579 journals). The values of IWs
are sometimes counter-intuitive and difficult to interpret. Furthermore,
iterations do not always converge. Routines for the computation of IWs are made
available at http://www.leydesdorff.net/iw.
"
2557,Crediting multi-authored papers to single authors,"  A fair assignment of credit for multi-authored publications is a
long-standing issue in scientometrics. In the calculation of the $h$-index, for
instance, all co-authors receive equal credit for a given publication,
independent of a given author's contribution to the work or of the total number
of co-authors. Several attempts have been made to distribute the credit in a
more appropriate manner. In a recent paper, Hirsch has suggested a new way of
credit assignment that is fundamentally different from the previous ones: All
credit for a multi-author paper goes to a single author, the called
``$\alpha$-author'', defined as the person with the highest current $h$-index
not the highest $h$-index at the time of the paper's publication) (J. E.
Hirsch, Scientometrics 118, 673 (2019)). The collection of papers this author
has received credit for as $\alpha$-author is then used to calculate a new
index, $h_{\alpha}$, following the same recipe as for the usual $h$ index. The
objective of this new assignment is not a fairer distribution of credit, but
rather the determination of an altogether different property, the degree of a
person's scientific leadership. We show that given the complex time dependence
of $h$ for individual scientists, the approach of using the current $h$ value
instead of the historic one is problematic, and we argue that it would be
feasible to determine the $\alpha$-author at the time of the paper's
publication instead. On the other hand, there are other practical
considerations that make the calculation of the proposed $h_{\alpha}$ very
difficult. As an alternative, we explore other ways of crediting papers to a
single author in order to test early career achievement or scientific
leadership.
"
2558,Authorship analysis of specialized vs diversified research output,"  The present work investigates the relations between amplitude and type of
collaboration (intramural, extramural domestic or international) and output of
specialized versus diversified research. By specialized or diversified
research, we mean within or beyond the author's dominant research topic. The
field of observation is the scientific production over five years from about
23,500 academics. The analyses are conducted at the aggregate and disciplinary
level. The results lead to the conclusion that in general, the output of
diversified research is no more frequently the fruit of collaboration than is
specialized research. At the level of the particular collaboration types,
international collaborations weakly underlie the specialized kind of research
output; on the contrary, extramural domestic and intramural collaborations are
weakly associated with diversified research. While the weakness of association
remains, exceptions are observed at the level of the individual disciplines.
"
2559,Does Environmental Economics lead to patentable research?,"  In this feasibility study, the impact of academic research from social
sciences and humanities on technological innovation is explored through a study
of citations patterns of journal articles in patents. Specifically we focus on
citations of journals from the field of environmental economics in patents
included in an American patent database (USPTO). Three decades of patents have
led to a small set of journal articles (85) that are being cited from the field
of environmental economics. While this route of measuring how academic research
is validated through its role in stimulating technological progress may be
rather limited (based on this first exploration), it may still point to a
valuable and interesting topic for further research.
"
2560,Pilot evaluation of Collection API with PID Kernel Information,"  As digital data become increasingly available for research, there is a
growing awareness of the value of domain agnostic Persistent Identifiers (PIDs)
for data. A PID is a globally unique reference to a digital object, which in
our case is data. In an ecosystem of connected digital objects, a PID will
reference a digital object, and the digital object will be a simple entity, a
collection of homogeneous objects, or a set of heterogeneous objects.
  In this paper, we study two recent recommendations from the Research Data
Alliance (RDA) that both address pieces of an ecosystem of connected digital
objects. The recommendations address Persistent ID records and representations
of collections of data. We evaluate different approaches in where to locate key
information about a data collection between these two component solutions.
"
2561,Interdisciplinary Relationships Between Biological and Physical Sciences,"  Several interdisciplinary areas have appeared at the interface between
biological and physical sciences. In this work, we suggest a complex
network-based methodology for analyzing the interrelationships between some of
these interdisciplinary areas, including Bioinformatics, Computational Biology,
Biochemistry, among others. This approach has been applied over respective data
derived from Wikipedia. Related reviews from the scientific literature are also
considered as a reference, yielding a respective bipartite hypergraph which can
be used to gain insights about the interrelationships underlying the considered
interdisciplinary areas. Several interesting results are obtained, including
greater interconnection between the considered interdisciplinary areas with
biological than with physical sciences. A good agreement was also found between
the network obtained from Wikipedia and the interrelationships revealed by the
literature reviews. At the same time, the former network was found to exhibit
more intricate relationships than in the hypergraph derived from the literature
review.
"
2562,Forms of Plagiarism in Digital Mathematical Libraries,"  We report on an exploratory analysis of the forms of plagiarism observable in
mathematical publications, which we identified by investigating editorial notes
from zbMATH. While most cases we encountered were simple copies of earlier
work, we also identified several forms of disguised plagiarism. We investigated
11 cases in detail and evaluate how current plagiarism detection systems
perform in identifying these cases. Moreover, we describe the steps required to
discover these and potentially undiscovered cases in the future.
"
2563,"Solo citations, duet citations, and prelude citations: New measures of
  the disruption of academic papers","  It is important to measure the disruption of academic papers. According to
the characteristics of three different kinds of citations, this paper borrows
musical vocabulary and names them solo citations (SC), duet citations (DC), and
prelude citations (PC) respectively. Studying how to measure the disruption of
a published work effectively, this study analyzes nine indicators and suggests
a general evaluation formula. Seven of the nine indicators are innovations
introduced by this paper: SC, SC-DC, SC-PC, SC-DC-PC, (SC-DC)/(SC+DC),
(SC-PC)/(SC+DC), and (SC-DC-PC)/(SC+DC), as is the general formula. These
indices are discussed considering two cases: One case concerns the Citation
Indexes for Science and the other concerns Co-citations. The results show that,
compared with other indicators, four indicators (SC, SC-DC, SC/(SC+DC), and
(SC-DC)/(SC+DC)) are logically and empirically reasonable. Future research may
consider combining these indices, for example, using SC multiplied by
SC/(SC+DC) or SC-DC multiplied by (SC-DC)/(SC+DC), to get final evaluation
results that contain desirable characteristics of two types of indicators.
Confirming which of the evaluation results from these indicators can best
reflect the innovation of research papers requires much empirical analysis.
"
2564,"How to interpret algorithmically constructed topical structures of
  research specialties? A case study comparing an internal and an external
  mapping of the topical structure of invasion biology","  In our paper we seek to address a shortcoming in the scientometric
literature, namely that, given the proliferation of algorithmic approaches to
topic detection from bibliometric data, there is a relative lack of studies
that validate and create a deeper understanding of the topical structures these
algorithmic approaches generate. To take a closer look at this issue, we
investigate the results of the new Leiden algorithm when applied to the direct
citation network of a field-level data set. We compare this internal
perspective which is constructed from the citation links within a data set of
30,000 publications in invasion biology, with an external perspective onto the
topic structures in this research specialty, which is based on a global science
map in form of the CWTS microfield classification underlying the Leiden
Ranking. We present an initial comparative analysis of the results and lay out
our next steps that will involve engaging with domain experts to examine how
the algorithmically identified topics relate to understandings of topics and
topical perspectives that operate within this research specialty.
"
2565,Collecting 16K archived web pages from 17 public web archives,"  We document the creation of a data set of 16,627 archived web pages, or
mementos, of 3,698 unique live web URIs (Uniform Resource Identifiers) from 17
public web archives. We used four different methods to collect the dataset.
First, we used the Los Alamos National Laboratory (LANL) Memento Aggregator to
collect mementos of an initial set of URIs obtained from four sources: (a) the
Moz Top 500, (b) the dataset used in our previous study, (c) the HTTP Archive,
and (d) the Web Archives for Historical Research group. Second, we extracted
URIs from the HTML of already collected mementos. These URIs were then used to
look up mementos in LANL's aggregator. Third, we downloaded web archives'
published lists of URIs of both original pages and their associated mementos.
Fourth, we collected more mementos from archives that support the Memento
protocol by requesting TimeMaps directly from archives, not through the Memento
aggregator. Finally, we downsampled the collected mementos to 16,627 due to our
constraints of a maximum of 1,600 mementos per archive and being able to
download all mementos from each archive in less than 40 hours.
"
2566,"A human-inspired recognition system for premodern Japanese historical
  documents","  Recognition of historical documents is a challenging problem due to the
noised, damaged characters and background. However, in Japanese historical
documents, not only contains the mentioned problems, pre-modern Japanese
characters were written in cursive and are connected. Therefore, character
segmentation based methods do not work well. This leads to the idea of creating
a new recognition system. In this paper, we propose a human-inspired document
reading system to recognize multiple lines of premodern Japanese historical
documents. During the reading, people employ eyes movement to determine the
start of a text line. Then, they move the eyes from the current character/word
to the next character/word. They can also determine the end of a line or skip a
figure to move to the next line. The eyes movement integrates with visual
processing to operate the reading process in the brain. We employ
attention-based encoder-decoder to implement this recognition system. First,
the recognition system detects where to start a text line. Second, the system
scans and recognize character by character until the text line is completed.
Then, the system continues to detect the start of the next text line. This
process is repeated until reading the whole document. We tested our
human-inspired recognition system on the pre-modern Japanese historical
document provide by the PRMU Kuzushiji competition. The results of the
experiments demonstrate the superiority and effectiveness of our proposed
system by achieving Sequence Error Rate of 9.87% and 53.81% on level 2 and
level 3 of the dataset, respectively. These results outperform to any other
systems participated in the PRMU Kuzushiji competition.
"
2567,"Transfer Learning for Scientific Data Chain Extraction in Small Chemical
  Corpus with BERT-CRF Model","  Computational chemistry develops fast in recent years due to the rapid growth
and breakthroughs in AI. Thanks for the progress in natural language
processing, researchers can extract more fine-grained knowledge in publications
to stimulate the development in computational chemistry. While the works and
corpora in chemical entity extraction have been restricted in the biomedicine
or life science field instead of the chemistry field, we build a new corpus in
chemical bond field annotated for 7 types of entities: compound, solvent,
method, bond, reaction, pKa and pKa value. This paper presents a novel BERT-CRF
model to build scientific chemical data chains by extracting 7 chemical
entities and relations from publications. And we propose a joint model to
extract the entities and relations simultaneously. Experimental results on our
Chemical Special Corpus demonstrate that we achieve state-of-art and
competitive NER performance.
"
2568,On the share of mathematics published by Elsevier and Springer,"  For-profit editors such as Elsevier and Springer have been subject to
sustained criticism from academics and university libraries, including calls to
boycott, and discontinued subscriptions. Mathematicians have played a
particularly active role in this critique, and have endeavored to imagine new
publication practices and create new journals. This motivates the monitoring of
the share of articles published by different editors. I used data from
MathSciNet over the period 2000-2017, and focused on the 100 journals with
highest citations per article. Within this category, the share of articles
published by Elsevier and Springer has steadily increased over this period,
from about a third to almost half of the total.
"
2569,"Missing Movie Synergistic Completion across Multiple Isomeric Online
  Movie Knowledge Libraries","  Online knowledge libraries refer to the online data warehouses that
systematically organize and categorize the knowledge-based information about
different kinds of concepts and entities. In the era of big data, the setup of
online knowledge libraries is an extremely challenging and laborious task, in
terms of efforts, time and expense required in the completion of knowledge
entities. Especially nowadays, a large number of new knowledge entities, like
movies, are keeping on being produced and coming out at a continuously
accelerating speed, which renders the knowledge library setup and completion
problem more difficult to resolve manually. In this paper, we will take the
online movie knowledge libraries as an example, and study the ""Multiple aligned
ISomeric Online Knowledge LIbraries Completion problem"" (Miso-Klic) problem
across multiple online knowledge libraries. Miso-Klic aims at identifying the
missing entities for multiple knowledge libraries synergistically and ranking
them for editing based on certain ranking criteria. To solve the problem, a
thorough investigation of two isomeric online knowledge libraries, Douban and
IMDB, have been carried out in this paper. Based on analyses results, a novel
deep online knowledge library completion framework ""Integrated Deep alignEd
Auto-encoder"" (IDEA) is introduced to solve the problem. By projecting the
entities from multiple isomeric knowledge libraries to a shared feature space,
IDEA solves the Miso-Klic problem via three steps: (1) entity feature space
unification via embedding, (2) knowledge library fusion based missing entity
identification, and (3) missing entity ranking. Extensive experiments done on
the real-world online knowledge library dataset have demonstrated the
effectiveness of IDEA in addressing the problem.
"
2570,"The CEDAR Workbench: An Ontology-Assisted Environment for Authoring
  Metadata that Describe Scientific Experiments","  The Center for Expanded Data Annotation and Retrieval (CEDAR) aims to
revolutionize the way that metadata describing scientific experiments are
authored. The software we have developed--the CEDAR Workbench--is a suite of
Web-based tools and REST APIs that allows users to construct metadata
templates, to fill in templates to generate high-quality metadata, and to share
and manage these resources. The CEDAR Workbench provides a versatile,
REST-based environment for authoring metadata that are enriched with terms from
ontologies. The metadata are available as JSON, JSON-LD, or RDF for easy
integration in scientific applications and reusability on the Web. Users can
leverage our APIs for validating and submitting metadata to external
repositories. The CEDAR Workbench is freely available and open-source.
"
2571,"Mining university rankings: Publication output and citation impact as
  their basis","  World University rankings have become well-established tools that students,
university managers and policy makers read and use. Each ranking claims to have
a unique methodology capable of measuring the 'quality' of universities. The
purpose of this paper is to analyze to which extent these different rankings
measure the same phenomenon and what it is that they are measuring. For this,
we selected a total of seven world-university rankings and performed a
principal component analysis. After ensuring that despite their methodological
differences, they all come together to a single component, we hypothesized that
bibliometric indicators could explain what is being measured. Our analyses show
that ranking scores from whichever of the seven league tables under study can
be explained by the number of publications and citations received by the
institution. We conclude by discussing policy implications and opportunities on
how a nuanced and responsible use of rankings can help decision making at the
institutional level
"
2572,"Why Machines Cannot Learn Mathematics, Yet","  Nowadays, Machine Learning (ML) is seen as the universal solution to improve
the effectiveness of information retrieval (IR) methods. However, while
mathematics is a precise and accurate science, it is usually expressed by less
accurate and imprecise descriptions, contributing to the relative dearth of
machine learning applications for IR in this domain. Generally, mathematical
documents communicate their knowledge with an ambiguous, context-dependent, and
non-formal language. Given recent advances in ML, it seems canonical to apply
ML techniques to represent and retrieve mathematics semantically. In this work,
we apply popular text embedding techniques to the arXiv collection of STEM
documents and explore how these are unable to properly understand mathematics
from that corpus. In addition, we also investigate the missing aspects that
would allow mathematics to be learned by computers.
"
2573,Software Citation Implementation Challenges,"  The main output of the FORCE11 Software Citation working group
(https://www.force11.org/group/software-citation-working-group) was a paper on
software citation principles (https://doi.org/10.7717/peerj-cs.86) published in
September 2016. This paper laid out a set of six high-level principles for
software citation (importance, credit and attribution, unique identification,
persistence, accessibility, and specificity) and discussed how they could be
used to implement software citation in the scholarly community. In a series of
talks and other activities, we have promoted software citation using these
increasingly accepted principles. At the time the initial paper was published,
we also provided guidance and examples on how to make software citable, though
we now realize there are unresolved problems with that guidance. The purpose of
this document is to provide an explanation of current issues impacting
scholarly attribution of research software, organize updated implementation
guidance, and identify where best practices and solutions are still needed.
"
2574,"A Scalable Hybrid Research Paper Recommender System for Microsoft
  Academic","  We present the design and methodology for the large scale hybrid paper
recommender system used by Microsoft Academic. The system provides
recommendations for approximately 160 million English research papers and
patents. Our approach handles incomplete citation information while also
alleviating the cold-start problem that often affects other recommender
systems. We use the Microsoft Academic Graph (MAG), titles, and available
abstracts of research papers to build a recommendation list for all documents,
thereby combining co-citation and content based approaches. Tuning system
parameters also allows for blending and prioritization of each approach which,
in turn, allows us to balance paper novelty versus authority in recommendation
results. We evaluate the generated recommendations via a user study of 40
participants, with over 2400 recommendation pairs graded and discuss the
quality of the results using P@10 and nDCG scores. We see that there is a
strong correlation between participant scores and the similarity rankings
produced by our system but that additional focus needs to be put towards
improving recommender precision, particularly for content based
recommendations. The results of the user survey and associated analysis scripts
are made available via GitHub and the recommendations produced by our system
are available as part of the MAG on Azure to facilitate further research and
light up novel research paper recommendation applications.
"
2575,"From heterogeneous data to heterogeneous public: thoughts on transmedia
  applications for digital heritage research and dissemination","  In recent years, we have seen a tenfold increase in volume and complexity of
digital data acquired for cultural heritage documentation. Meanwhile, open data
and open science have become leading trends in digital humanities. The
convergence of those two parameters compels us to deliver, in an interoperable
fashion, datasets that are vastly heterogeneous both in content and format and,
moreover, in such a way that they fit the expectation of a broad array of
researchers and an even broader public audience. Tackling those issues is one
of the main goal of the ""HeritageS"" digital platform project supported by the
""Intelligence des Patrimoines"" research program. This platform is designed to
allow research projects from many interdisciplinary fields to share, integrate
and valorize cultural and natural heritage datasets related to the Loire
Valley. In this regard, one of our main project is the creation of the
""Renaissance Transmedia Lab"". Its core element is a website which acts as a hub
to access various interactive experiences linked to project about the
Renaissance period: augmented web-documentary, serious game, virtual reality,
3D application. We expect to leverage those transmedia experiences to foster
better communication between researchers and the public while keeping the
quality of scientific discourse. By presenting the current and upcoming
productions, we intend to share our experience with other participants:
preparatory work and how we cope with researchers to produce, in concertation,
tailor-made experiences that convey the desired scientific discourse while
remaining appealing to the general public.
"
2576,"R package for producing beamplots as a preferred alternative to the h
  index when assessing single researchers (based on downloads from Web of
  Science)","  We propose the use of beamplots - which can be produced by using the R
package BibPlots and WoS downloads - as a preferred alternative to h index
values for assessing single researchers.
"
2577,Patent Citation Dynamics Modeling via Multi-Attention Recurrent Networks,"  Modeling and forecasting forward citations to a patent is a central task for
the discovery of emerging technologies and for measuring the pulse of inventive
progress. Conventional methods for forecasting these forward citations cast the
problem as analysis of temporal point processes which rely on the conditional
intensity of previously received citations. Recent approaches model the
conditional intensity as a chain of recurrent neural networks to capture memory
dependency in hopes of reducing the restrictions of the parametric form of the
intensity function. For the problem of patent citations, we observe that
forecasting a patent's chain of citations benefits from not only the patent's
history itself but also from the historical citations of assignees and
inventors associated with that patent. In this paper, we propose a
sequence-to-sequence model which employs an attention-of-attention mechanism to
capture the dependencies of these multiple time sequences. Furthermore, the
proposed model is able to forecast both the timestamp and the category of a
patent's next citation. Extensive experiments on a large patent citation
dataset collected from USPTO demonstrate that the proposed model outperforms
state-of-the-art models at forward citation forecasting.
"
2578,"TrendNets: Mapping Emerging Research Trends From Dynamic Co-Word
  Networks via Sparse Representation","  Mapping the knowledge structure from word co-occurrences in a collection of
academic papers has been widely used to provide insight into the topic
evolution in an arbitrary research field. In a traditional approach, the paper
collection is first divided into temporal subsets, and then a co-word network
is independently depicted in a 2D map to characterize each period's trend. To
effectively map emerging research trends from such a time-series of co-word
networks, this paper presents TrendNets, a novel visualization methodology that
highlights the rapid changes in edge weights over time. Specifically, we
formulated a new convex optimization framework that decomposes the matrix
constructed from dynamic co-word networks into a smooth part and a sparse part:
the former represents stationary research topics, while the latter corresponds
to bursty research topics. Simulation results on synthetic data demonstrated
that our matrix decomposition approach achieved the best burst detection
performance over four baseline methods. In experiments conducted using papers
published in the past 16 years at three conferences in different fields, we
showed the effectiveness of TrendNets compared to the traditional co-word
representation. We have made our codes available on the Web to encourage
scientific mapping in all research fields.
"
2579,Shared Feelings: Understanding Facebook Reactions to Scholarly Articles,"  Research on social-media platforms has tended to rely on textual analysis to
perform research tasks. While text-based approaches have significantly
increased our understanding of online behavior and social dynamics, they
overlook features on these platforms that have grown in prominence in the past
few years: click-based responses to content. In this paper, we present a new
dataset of Facebook Reactions to scholarly content. We give an overview of its
structure, analyze some of the statistical trends in the data, and use it to
train and test two supervised learning algorithms. Our preliminary tests
suggest the presence of stratification in the number of users following pages,
divisions that seem to fall in line with distinctions in the subject matter of
those pages.
"
2580,"Does the $h_\alpha$ index reinforce the Matthew effect in science?
  Agent-based simulations using Stata and R","  Recently, Hirsch (2019a) proposed a new variant of the h index called the
$h_\alpha$ index. He formulated as follows: ""we define the $h_\alpha$ index of
a scientist as the number of papers in the h-core of the scientist (i.e. the
set of papers that contribute to the h-index of the scientist) where this
scientist is the $\alpha$-author"" (p. 673). The $h_\alpha$ index was criticized
by Leydesdorff, Bornmann, and Opthof (2019). One of their most important points
is that the index reinforces the Matthew effect in science. We address this
point in the current study using a recently developed Stata command (h_index)
and R package (hindex), which can be used to simulate h index and
$h_\alpha$index applications in research evaluation. The user can investigate
under which conditions $h_\alpha$ reinforces the Matthew effect. The results of
our study confirm what Leydesdorff et al. (2019) expected: the $h_\alpha$ index
reinforces the Matthew effect. This effect can be intensified if strategic
behavior of the publishing scientists and cumulative advantage effects are
additionally considered in the simulation.
"
2581,"Statistical Significance Testing in Information Retrieval: An Empirical
  Analysis of Type I, Type II and Type III Errors","  Statistical significance testing is widely accepted as a means to assess how
well a difference in effectiveness reflects an actual difference between
systems, as opposed to random noise because of the selection of topics.
According to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is
the most popular choice among IR researchers. However, previous work has
suggested computer intensive tests like the bootstrap or the permutation test,
based mainly on theoretical arguments. On empirical grounds, others have
suggested non-parametric alternatives such as the Wilcoxon test. Indeed, the
question of which tests we should use has accompanied IR and related fields for
decades now. Previous theoretical studies on this matter were limited in that
we know that test assumptions are not met in IR experiments, and empirical
studies were limited in that we do not have the necessary control over the null
hypotheses to compute actual Type I and Type II error rates under realistic
conditions. Therefore, not only is it unclear which test to use, but also how
much trust we should put in them. In contrast to past studies, in this paper we
employ a recent simulation methodology from TREC data to go around these
limitations. Our study comprises over 500 million p-values computed for a range
of tests, systems, effectiveness measures, topic set sizes and effect sizes,
and for both the 2-tail and 1-tail cases. Having such a large supply of IR
evaluation data with full knowledge of the null hypotheses, we are finally in a
position to evaluate how well statistical significance tests really behave with
IR data, and make sound recommendations for practitioners.
"
2582,"Attributing and Referencing (Research) Software: Best Practices and
  Outlook from Inria","  Software is a fundamental pillar of modern scientiic research, not only in
computer science, but actually across all elds and disciplines. However, there
is a lack of adequate means to cite and reference software, for many reasons.
An obvious rst reason is software authorship, which can range from a single
developer to a whole team, and can even vary in time. The panorama is even more
complex than that, because many roles can be involved in software development:
software architect, coder, debugger, tester, team manager, and so on. Arguably,
the researchers who have invented the key algorithms underlying the software
can also claim a part of the authorship. And there are many other reasons that
make this issue complex. We provide in this paper a contribution to the ongoing
eeorts to develop proper guidelines and recommendations for software citation,
building upon the internal experience of Inria, the French research institute
for digital sciences. As a central contribution, we make three key
recommendations. (1) We propose a richer taxonomy for software contributions
with a qualitative scale. (2) We claim that it is essential to put the human at
the heart of the evaluation. And (3) we propose to distinguish citation from
reference.
"
2583,"Document Embeddings vs. Keyphrases vs. Terms: An Online Evaluation in
  Digital Library Recommender Systems","  Many recommendation algorithms are available to digital library recommender
system operators. The effectiveness of algorithms is largely unreported by way
of online evaluation. We compare a standard term-based recommendation approach
to two promising approaches for related-article recommendation in digital
libraries: document embeddings, and keyphrases. We evaluate the consistency of
their performance across multiple scenarios. Through our
recommender-as-a-service Mr. DLib, we delivered 33.5M recommendations to users
of Sowiport and Jabref over the course of 19 months, from March 2017 to October
2018. The effectiveness of the algorithms differs significantly between
Sowiport and Jabref (Wilcoxon rank-sum test; p < 0.05). There is a ~400%
difference in effectiveness between the best and worst algorithm in both
scenarios separately. The best performing algorithm in Sowiport (terms) is the
worst performing in Jabref. The best performing algorithm in Jabref
(keyphrases) is 70% worse in Sowiport, than Sowiport`s best algorithm
(click-through rate; 0.1% terms, 0.03% keyphrases).
"
2584,"Social Cards Probably Provide For Better Understanding Of Web Archive
  Collections","  Used by a variety of researchers, web archive collections have become
invaluable sources of evidence. If a researcher is presented with a web archive
collection that they did not create, how do they know what is inside so that
they can use it for their own research? Search engine results and social media
links are represented as surrogates, small easily digestible summaries of the
underlying page. Search engines and social media have a different focus, and
hence produce different surrogates than web archives. Search engine surrogates
help a user answer the question ""Will this link meet my information need?""
Social media surrogates help a user decide ""Should I click on this?"" Our use
case is subtly different. We hypothesize that groups of surrogates together are
useful for summarizing a collection. We want to help users answer the question
of ""What does the underlying collection contain?"" But which surrogate should we
use? With Mechanical Turk participants, we evaluate six different surrogate
types against each other. We find that the type of surrogate does not influence
the time to complete the task we presented the participants. Of particular
interest are social cards, surrogates typically found on social media, and
browser thumbnails, screen captures of web pages rendered in a browser. At
$p=0.0569$, and $p=0.0770$, respectively, we find that social cards and social
cards paired side-by-side with browser thumbnails probably provide better
collection understanding than the surrogates currently used by the popular
Archive-It web archiving platform. We measure user interactions with each
surrogate and find that users interact with social cards less than other types.
The results of this study have implications for our web archive summarization
work, live web curation platforms, social media, and more.
"
2585,Paper Matching with Local Fairness Constraints,"  Automatically matching reviewers to papers is a crucial step of the peer
review process for venues receiving thousands of submissions. Unfortunately,
common paper matching algorithms often construct matchings suffering from two
critical problems: (1) the group of reviewers assigned to a paper do not
collectively possess sufficient expertise, and (2) reviewer workloads are
highly skewed. In this paper, we propose a novel local fairness formulation of
paper matching that directly addresses both of these issues. Since optimizing
our formulation is not always tractable, we introduce two new algorithms,
FairIR and FairFlow, for computing fair matchings that approximately optimize
the new formulation. FairIR solves a relaxation of the local fairness
formulation and then employs a rounding technique to construct a valid matching
that provably maximizes the objective and only compromises on fairness with
respect to reviewer loads and papers by a small constant. In contrast, FairFlow
is not provably guaranteed to produce fair matchings, however it can be 2x as
efficient as FairIR and an order of magnitude faster than matching algorithms
that directly optimize for fairness. Empirically, we demonstrate that both
FairIR and FairFlow improve fairness over standard matching algorithms on real
conference data. Moreover, in comparison to state-of-the-art matching
algorithms that optimize for fairness only, FairIR achieves higher objective
scores, FairFlow achieves competitive fairness, and both are capable of more
evenly allocating reviewers.
"
2586,"Using Micro-collections in Social Media to Generate Seeds for Web
  Archive Collections","  In a Web plagued by disappearing resources, Web archive collections provide a
valuable means of preserving Web resources important to the study of past
events ranging from elections to disease outbreaks. These archived collections
start with seed URIs (Uniform Resource Identifiers) hand-selected by curators.
Curators produce high quality seeds by removing non-relevant URIs and adding
URIs from credible and authoritative sources, but it is time consuming to
collect these seeds. Two main strategies adopted by curators for discovering
seeds include scraping Web (e.g., Google) Search Engine Result Pages (SERPs)
and social media (e.g., Twitter) SERPs. In this work, we studied three social
media platforms in order to provide insight on the characteristics of seeds
generated from different sources. First, we developed a simple vocabulary for
describing social media posts across different platforms. Second, we introduced
a novel source for generating seeds from URIs in the threaded conversations of
social media posts created by single or multiple users. Users on social media
sites routinely create and share posts about news events consisting of
hand-selected URIs of news stories, tweets, videos, etc. In this work, we call
these posts micro-collections, and we consider them as an important source for
seeds because the effort taken to create micro-collections is an indication of
editorial activity, and a demonstration of domain expertise. Third, we
generated 23,112 seed collections with text and hashtag queries from 449,347
social media posts from Reddit, Twitter, and Scoop.it. We collected in total
120,444 URIs from the conventional scraped SERP posts and micro-collections. We
characterized the resultant seed collections across multiple dimensions
including the distribution of URIs, precision, ages, diversity of webpages,
etc...
"
2587,"Citation concept analysis (CCA) - A new form of citation analysis
  revealing the usefulness of concepts for other researchers illustrated by two
  exemplary case studies including classic books by Thomas S. Kuhn and Karl R.
  Popper","  In recent years, the full text of papers are increasingly available
electronically which opens up the possibility of quantitatively investigating
citation contexts in more detail. In this study, we introduce a new form of
citation analysis, which we call citation concept analysis (CCA). CCA is
intended to reveal the cognitive impact certain concepts -- published in a
document -- have on the citing authors. It counts the number of times the
concepts are mentioned (cited) in the citation context of citing publications.
We demonstrate the method using three classical examples: (1) The structure of
scientific revolutions by Thomas S. Kuhn, (2) The logic of scientific discovery
- Logik der Forschung: Zur Erkenntnistheorie der modernen Naturwissenschaft in
German -, and (3) Conjectures and refutations: the growth of scientific
knowledge by Karl R. Popper. It is not surprising -- as our results show --
that Kuhn's ""paradigm"" concept has had a significant impact. What is surprising
is that it has had such a disproportionately larger impact than Kuhn's other
concepts, e.g., ""scientific revolution"". The paradigm concept accounts for over
80% of the concept-related citations to Kuhn's work, and its impact is
resilient across all disciplines and over time. With respect to Popper,
""falsification"" is the most used concept derived from his books. Falsification,
after all, is the cornerstone of Popper's critical rationalism.
"
2588,Archive Assisted Archival Fixity Verification Framework,"  The number of public and private web archives has increased, and we
implicitly trust content delivered by these archives. Fixity is checked to
ensure an archived resource has remained unaltered since the time it was
captured. Some web archives do not allow users to access fixity information
and, more importantly, even if fixity information is available, it is provided
by the same archive from which the archived resources are requested. In this
research, we propose two approaches, namely Atomic and Block, to establish and
check fixity of archived resources. In the Atomic approach, the fixity
information of each archived web page is stored in a JSON file (or a manifest),
and published in a well-known web location (an Archival Fixity server) before
it is disseminated to several on-demand web archives. In the Block approach, we
first batch together fixity information of multiple archived pages in a single
binary-searchable file (or a block) before it is published and disseminated to
archives. In both approaches, the fixity information is not obtained directly
from archives. Instead, we compute the fixity information (e.g., hash values)
based on the playback of archived resources. One advantage of the Atomic
approach is the ability to verify fixity of archived pages even with the
absence of the Archival Fixity server. The Block approach requires pushing
fewer resources into archives, and it performs fixity verification faster than
the Atomic approach. On average, it takes about 1.25X, 4X, and 36X longer to
disseminate a manifest to perma.cc, archive.org, and webcitation.org,
respectively, than archive.is, while it takes 3.5X longer to disseminate a
block to archive.org than perma.cc. The Block approach performs 4.46X faster
than the Atomic approach on verifying the fixity of archived pages.
"
2589,MementoMap Framework for Flexible and Adaptive Web Archive Profiling,"  In this work we propose MementoMap, a flexible and adaptive framework to
efficiently summarize holdings of a web archive. We described a simple, yet
extensible, file format suitable for MementoMap. We used the complete index of
the Arquivo.pt comprising 5B mementos (archived web pages/files) to understand
the nature and shape of its holdings. We generated MementoMaps with varying
amount of detail from its HTML pages that have an HTTP status code of 200 OK.
Additionally, we designed a single-pass, memory-efficient, and
parallelization-friendly algorithm to compact a large MementoMap into a small
one and an in-file binary search method for efficient lookup. We analyzed more
than three years of MemGator (a Memento aggregator) logs to understand the
response behavior of 14 public web archives. We evaluated MementoMaps by
measuring their Accuracy using 3.3M unique URIs from MemGator logs. We found
that a MementoMap of less than 1.5% Relative Cost (as compared to the
comprehensive listing of all the unique original URIs) can correctly identify
the presence or absence of 60% of the lookup URIs in the corresponding archive
while maintaining 100% Recall (i.e., zero false negatives).
"
2590,"Exploring the Effects of Data Set Choice on Measuring International
  Research Collaboration: an Example Using the ACM Digital Library and
  Microsoft Academic Graph","  International research collaboration (IRC) measurement is important because
countries can and want to benefit from international collaboration but
performing the same measurement procedure on different data sets can lead to
different results. This study aims to explore the effects of data set choice on
IRC measurement.
"
2591,"Enriching Bibliographic Data by Combining String Matching and the
  Wikidata Knowledge Graph to Improve the Measurement of International Research
  Collaboration","  Measuring international research collaboration is necessary when evaluating,
for example, the efficacy of policy meant to increase cooperation between
countries, but is currently very difficult as bibliographic records contain
only affiliation data from which there is no standard method to identify the
relevant countries. In this paper we describe a method to address this
difficulty, and evaluate it using both general and domain-specific data sets.
"
2592,DFS: A Dataset File System for Data Discovering Users,"  Many research questions can be answered quickly and efficiently using data
already collected for previous research. This practice is called secondary data
analysis (SDA), and has gained popularity due to lower costs and improved
research efficiency. In this paper we propose DFS, a file system to standardize
the metadata representation of datasets, and DDU, a scalable architecture based
on DFS for semi-automated metadata generation and data recommendation on the
cloud. We discuss how DFS and DDU lays groundwork for automatic dataset
aggregation, how it integrates with existing data wrangling and machine
learning tools, and explores their implications on datasets stored in digital
libraries.
"
2593,"A gender analysis of top scientists' collaboration behavior: evidence
  from Italy","  This work analyzes the differences in collaboration behavior between males
and females among a particular type of scholars: top scientists, and as
compared to non top scientists. The field of observation consists of the
Italian academic system and the co-authorships of scientific publications by
11,145 professors. The results obtained from a cross-sectional analysis
covering the five-year period 2006-2010 show that there are no significant
differences in the overall propensity to collaborate in the top scientists of
the two genders. At the level of single disciplines there are no differences in
collaboration behavior, except in the case of: i) international collaborations,
for Mathematics and Chemistry - where the propensity for collaboration is
greater for males; and ii) extramural domestic collaborations in Physics, in
which it is the females that show greater propensity for collaboration. Because
international collaboration is positively correlated to research performance,
findings can inform science policy aimed at increasing the representation of
female top performers.
"
2594,"Tracing Antisemitic Language Through Diachronic Embedding Projections:
  France 1789-1914","  We investigate some aspects of the history of antisemitism in France, one of
the cradles of modern antisemitism, using diachronic word embeddings. We
constructed a large corpus of French books and periodicals issues that contain
a keyword related to Jews and performed a diachronic word embedding over the
1789-1914 period. We studied the changes over time in the semantic spaces of 4
target words and performed embedding projections over 6 streams of antisemitic
discourse. This allowed us to track the evolution of antisemitic bias in the
religious, economic, socio-politic, racial, ethic and conspiratorial domains.
Projections show a trend of growing antisemitism, especially in the years
starting in the mid-80s and culminating in the Dreyfus affair. Our analysis
also allows us to highlight the peculiar adverse bias towards Judaism in the
broader context of other religions.
"
2595,"Large publishing consortia produce higher citation impact research but
  co-author contributions are hard to evaluate","  This paper introduces a simple agglomerative clustering method to identify
large publishing consortia with at least 20 authors and 80% shared authorship
between articles. Based on Scopus journal articles 1996-2018, under these
criteria, nearly all (88%) of the large consortia published research with
citation impact above the world average, with the exceptions being mainly the
newer consortia for which average citation counts are unreliable. On average,
consortium research had almost double (1.95) the world average citation impact
on the log scale used (Mean Normalised Log Citation Score). At least partial
alphabetical author ordering was the norm in most consortia. The 250 largest
consortia were for nuclear physics and astronomy around expensive equipment,
and for predominantly health-related issues in genomics, medicine, public
health, microbiology and neuropsychology. For the health-related issues, except
for the first and last few authors, authorship seem to primary indicate
contributions to the shared project infrastructure necessary to gather the raw
data. It is impossible for research evaluators to identify the contributions of
individual authors in the huge alphabetical consortia of physics and astronomy,
and problematic for the middle and end authors of health-related consortia. For
small scale evaluations, authorship contribution statements could be used, when
available.
"
2596,"How a Single Paper Affects the Impact Factor: Implications for Scholarly
  Publishing","  Because the Impact Factor (IF) is an average quantity and most journals are
small, IFs are volatile. We study how a single paper affects the IF using data
from 11639 journals in the 2017 Journal Citation Reports. We define as
volatility the IF gain (or loss) caused by a single paper, and this is
inversely proportional to journal size. We find high volatilities for hundreds
of journals annually due to their top-cited paper: whether it is a highly-cited
paper in a small journal, or a moderately (or even low) cited paper in a small
and low-cited journal. For example, 1218 journals had their most cited paper
boost their IF by more than 20%, while for 231 journals the boost exceeded 50%.
We find that small journals are rewarded much more than large journals for
publishing a highly-cited paper, and are also penalized more for publishing a
low-cited paper, especially if they have a high IF. This produces a strong
incentive for prestigious, high-IF journals to stay small, to remain
competitive in IF rankings. We discuss the implications for breakthrough papers
to appear in prestigious journals. We also question the practice of ranking
journals by IF given this uneven reward mechanism.
"
2597,"Please, no more scientific journals! The strategy of the scientific
  publication system","  In the same way ecosystems tend to increase maturity by decreasing the flow
of energy per unit biomass, we should move towards a more mature science by
publishing less but high-quality papers and getting away from joining large
teams in small roles. That is, we should decrease our scientific productivity
for good.
"
2598,Token-Curated Registry with Citation Graph,"  In this study, we aim to incorporate the expertise of anonymous curators into
a token-curated registry (TCR), a decentralized recommender system for
collecting a list of high-quality content. This registry is important, because
previous studies on TCRs have not specifically focused on technical content,
such as academic papers and patents, whose effective curation requires
expertise in relevant fields. To measure expertise, curation in our model
focuses on both the content and its citation relationships, for which curator
assignment uses the Personalized PageRank (PPR) algorithm while reward
computation uses a multi-task peer-prediction mechanism. Our proposed CitedTCR
bridges the literature on network-based and token-based recommender systems and
contributes to the autonomous development of an evolving citation graph for
high-quality content. Moreover, we experimentally confirm the incentive for
registration and curation in CitedTCR using the simplification of a one-to-one
correspondence between users and content (nodes).
"
2599,Do Authors Deposit on Time? Tracking Open Access Policy Compliance,"  Recent years have seen fast growth in the number of policies mandating Open
Access (OA) to research outputs. We conduct a large-scale analysis of over 800
thousand papers from repositories around the world published over a period of 5
years to investigate: a) if the time lag between the date of publication and
date of deposit in a repository can be effectively tracked across thousands of
repositories globally, and b) if introducing deposit deadlines is associated
with a reduction of time from acceptance to public availability of research
outputs. We show that after the introduction of the UK REF 2021 OA policy, this
time lag has decreased significantly in the UK and that the policy introduction
might have accelerated the UK's move towards immediate OA compared to other
countries. This supports the argument for the inclusion of a time-limited
deposit requirement in OA policies.
"
2600,Indicators of Open Access for universities,"  This paper presents a first attempt to analyse Open Access integration at the
institutional level. For this, we combine information from Unpaywall and the
Leiden Ranking to offer basic OA indicators for universities. We calculate the
overall number of Open Access publications for 930 universities worldwide. OA
indicators are also disaggregated by green, gold and hybrid Open Access. We
then explore differences between and within countries and offer a general
ranking of universities based on the proportion of their output which is openly
accessible.
"
2601,"Qualifying threshold of take off stage for successfully disseminated
  creative ideas","  The creative process is essentially Darwinian and only a small proportion of
creative ideas are selected for further development. However, the threshold
that identifies this small fraction of successfully disseminated creative ideas
at their early stage has not been thoroughly analyzed through the lens of
Rogers innovation diffusion theory. Here, we take highly cited (top 1%)
research papers as an example of the most successfully disseminated creative
ideas and explore the time it takes and citations it receives at their take off
stage, which play a crucial role in the dissemination of creativity. Results
show the majority of highly cited papers will reach 10% and 25% of their total
citations within two years and four years, respectively. Interestingly, our
results also present a minimal number of articles that attract their first
citation before publication. As for the discipline, number of references, and
Price index, we find a significant difference exists: Clinical, Pre-Clinical &
Health and Life Sciences are the first two disciplines to reach the C10% and
C25% in a shorter amount of time. Highly cited papers with limited references
usually take more time to reach 10% and 25% of their total citations. In
addition, highly cited papers will attract citations rapidly when they cite
more recent references. These results provide insights into the timespan and
citations for a research paper to become highly cited at the take off stage in
its diffusion process, as well as the factors that may influence it.
"
2602,"EXmatcher: Combining Features Based on Reference Strings and Segments to
  Enhance Citation Matching","  Citation matching is a challenging task due to different problems such as the
variety of citation styles, mistakes in reference strings and the quality of
identified reference segments. The classic citation matching configuration used
in this paper is the combination of blocking technique and a binary classifier.
Three different possible inputs (reference strings, reference segments and a
combination of reference strings and segments) were tested to find the most
efficient strategy for citation matching. In the classification step, we
describe the effect which the probabilities of reference segments can have in
citation matching. Our evaluation on a manually curated gold standard showed
that the input data consisting of the combination of reference segments and
reference strings lead to the best result. In addition, the usage of the
probabilities of the segmentation slightly improves the result.
"
2603,"What Do Citation Counts Measure? An Updated Review of Studies on
  Citations in Scientific Documents Published between 2006 and 2018","  The purpose of this paper is to update the review of Bornmann and Daniel
(2008) presenting a narrative review of studies on citations in scientific
documents. The current review covers 41 studies published between 2006 and
2018. Bornmann and Daniel (2008) focused on earlier years. The current review
describes the (new) studies on citation content and context analyses as well as
the studies that explore the citation motivation of scholars through surveys or
interviews. One focus in this paper is on the technical developments in the
last decade, such as the richer meta-data available and machine-readable
formats of scientific papers. These developments have resulted in citation
context analyses of large datasets in comprehensive studies (which was not
possible previously). Many studies in recent years have used computational and
machine learning techniques to determine citation functions and polarities,
some of which have attempted to overcome the methodological weaknesses of
previous studies. The automated recognition of citation functions seems to have
the potential to greatly enhance citation indices and information retrieval
capabilities. Our review of the empirical studies demonstrates that a paper may
be cited for very different scientific and non-scientific reasons. This result
accords with the finding by Bornmann and Daniel (2008). The current review also
shows that to better understand the relationship between citing and cited
documents, a variety of features should be analyzed, primarily the citation
context, the semantics and linguistic patterns in citations, citation locations
within the citing document, and citation polarity (negative, neutral,
positive).
"
2604,"Achieving competitive advantage in academia through early career
  coauthorship with top scientists","  We quantify the long term impact that the coauthorship with established
top-cited scientists has on the career of junior researchers in four different
scientific disciplines. Through matched pair analysis, we find that junior
researchers who coauthor work with top scientists enjoy a persistent
competitive advantage throughout the rest of their careers with respect to
peers with similar early career profiles. Such a competitive advantage
materialises as a higher probability of repeatedly coauthoring work with
top-cited scientists, and, ultimately, as a higher probability of becoming one.
Notably, we find that the coauthorship with a top scientist has the strongest
impact on the careers of junior researchers affiliated with less prestigious
institutions. As a consequence, we argue that such institutions may hold vast
amounts of untapped potential, which may be realised by improving access to top
scientists.
"
2605,"Visualizing a Field of Research: A Methodology of Systematic
  Scientometric Reviews","  Systematic scientometric reviews, empowered by scientometric and visual
analytic techniques, offer opportunities to improve the timeliness,
accessibility, and reproducibility of conventional systematic reviews. While
increasingly accessible science mapping tools enable end users to visualize the
structure and dynamics of a research field, a common bottleneck in the current
practice is the construction of a collection of scholarly publications as the
input of the subsequent scientometric analysis and visualization. End users
often have to face a dilemma in the preparation process: the more they know
about a knowledge domain, the easier it is for them to find the relevant data
to meet their needs adequately; the little they know, the harder the problem
is. What can we do to avoid missing something valuable but beyond our initial
description? In this article, we introduce a flexible and generic methodology,
cascading citation expansion, to increase the quality of constructing a
bibliographic dataset for systematic reviews. Furthermore, the methodology
simplifies the conceptualization of globalism and localism in science mapping
and unifies them on a consistent and continuous spectrum. We demonstrate an
application of the methodology to the research of literature-based discovery
and compare five datasets constructed based on three use scenarios, namely a
conventional keyword-based search (one dataset), an expansion process starting
with a groundbreaking article of the knowledge domain (two datasets), and an
expansion process starting with a recently published review article by a
prominent expert in the domain (two datasets). The unique coverage of each of
the datasets is inspected through network visualization overlays with reference
to other datasets in a broad and integrated context.
"
2606,"Nine Million Book Items and Eleven Million Citations: A Study of
  Book-Based Scholarly Communication Using OpenCitations","  Books have been widely used to share information and contribute to human
knowledge. However, the quantitative use of books as a method of scholarly
communication is relatively unexamined compared to journal articles and
conference papers. This study uses the COCI dataset (a comprehensive open
citation dataset provided by OpenCitations) to explore books' roles in
scholarly communication. The COCI data we analyzed includes 445,826,118
citations from 46,534,705 bibliographic entities. By analyzing such a large
amount of data, we provide a thorough, multifaceted understanding of books.
Among the investigated factors are 1) temporal changes to book citations; 2)
book citation distributions; 3) years to citation peak; 4) citation half-life;
and 5) characteristics of the most-cited books. Results show that books have
received less than 4% of total citations, and have been cited mainly by journal
articles. Moreover, 97.96% of books have been cited fewer than ten times. Books
take longer than other bibliographic materials to reach peak citation levels,
yet are cited for the same duration as journal articles. Most-cited books tend
to cover general (yet essential) topics, theories, and technological concepts
in mathematics and statistics.
"
2607,SchenQL -- A Domain-Specific Query Language on Bibliographic Metadata,"  Information access needs to be uncomplicated, users rather use incorrect data
which is easily received than correct information which is harder to obtain.
Querying bibliographic metadata from digital libraries mainly supports simple
textual queries. A user's demand for answering more sophisticated queries could
be fulfilled by the usage of SQL. As such means are highly complex and
challenging even for trained programmers, a domain-specific query language is
needed to provide a straightforward way to access data.
  In this paper we present SchenQL, a simple query language focused on
bibliographic metadata in the area of computer science while using the
vocabulary of domain-experts. By facilitating a plain syntax and fundamental
aggregate functions, we propose an easy-to-learn domain-specific query language
capable of search and exploration. It is suitable for domain-experts as well as
casual users while still providing the possibility to answer complicated
queries. A user study with computer scientists directly compared our query
language to SQL and clearly demonstrated SchenQL's suitability and usefulness
for given queries as well as users' acceptance.
"
2608,Software and Dependencies in Research Citation Graphs,"  Following the widespread digitalization of scholarship, software has become
essential for research, but the current sociotechnical system of citation does
not reflect this sufficiently. Citation provides context for research, but the
current model for the respective research citation graphs does not integrate
software. In this paper, I develop a directed graph model to alleviate this,
describe challenges for its instantiation, and give an outlook of useful
applications of research citation graphs, including transitive credit.
"
2609,"A Strategy for Expert Recommendation From Open Data Available on the
  Lattes Platform","  With the increasing volume of data and users of curriculum systems, the
difficulty of finding specialists is increasing.This work proposes an open data
extraction methodology of the Lattes Platform curricula, a treatment for this
data and investigates a Recommendation Agent approach based on deep neural
networks with autoencoder.
"
2610,"Predicting Research Trends with Semantic and Neural Networks with an
  application in Quantum Physics","  The vast and growing number of publications in all disciplines of science
cannot be comprehended by a single human researcher. As a consequence,
researchers have to specialize in narrow sub-disciplines, which makes it
challenging to uncover scientific connections beyond the own field of research.
Thus access to structured knowledge from a large corpus of publications could
help pushing the frontiers of science. Here we demonstrate a method to build a
semantic network from published scientific literature, which we call SemNet. We
use SemNet to predict future trends in research and to inspire new,
personalized and surprising seeds of ideas in science. We apply it in the
discipline of quantum physics, which has seen an unprecedented growth of
activity in recent years. In SemNet, scientific knowledge is represented as an
evolving network using the content of 750,000 scientific papers published since
1919. The nodes of the network correspond to physical concepts, and links
between two nodes are drawn when two physical concepts are concurrently studied
in research articles. We identify influential and prize-winning research topics
from the past inside SemNet thus confirm that it stores useful semantic
knowledge. We train a deep neural network using states of SemNet of the past,
to predict future developments in quantum physics research, and confirm high
quality predictions using historic data. With the neural network and
theoretical network tools we are able to suggest new, personalized,
out-of-the-box ideas, by identifying pairs of concepts which have unique and
extremal semantic network properties. Finally, we consider possible future
developments and implications of our findings.
"
2611,"The Evolving Ecosystem of Predatory Journals: A Case Study in Indian
  Perspective","  Digital advancement in scholarly repositories has led to the emergence of a
large number of open access predatory publishers that charge high article
processing fees from authors but fail to provide necessary editorial and
publishing services. Identifying and blacklisting such publishers has remained
a research challenge due to the highly volatile scholarly publishing ecosystem.
This paper presents a data-driven approach to study how potential predatory
publishers are evolving and bypassing several regularity constraints. We
empirically show the close resemblance of predatory publishers against reputed
publishing groups. In addition to verifying standard constraints, we also
propose distinctive signals gathered from network-centric properties to
understand this evolving ecosystem better.
"
2612,Accuracy of citation data in Web of Science and Scopus,"  We present a large-scale analysis of the accuracy of citation data in the Web
of Science and Scopus databases. The analysis is based on citations given in
publications in Elsevier journals. We reveal significant data quality problems
for both databases. Missing and incorrect references are important problems in
Web of Science. Duplicate publications are a serious problem in Scopus.
"
2613,Supporting Web Archiving via Web Packaging,"  We describe challenges related to web archiving, replaying archived web
resources, and verifying their authenticity. We show that Web Packaging has
significant potential to help address these challenges and identify areas in
which changes are needed in order to fully realize that potential.
"
2614,Impact of HTTP Cookie Violations in Web Archives,"  Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.
"
2615,Gender trends in computer science authorship,"  A comprehensive and up-to-date analysis of Computer Science literature (2.87
million papers through 2018) reveals that, if current trends continue, parity
between the number of male and female authors will not be reached in this
century. Under our most optimistic projection models, gender parity is forecast
to be reached by 2100, and significantly later under more realistic
assumptions. In contrast, parity is projected to be reached within two to three
decades in the biomedical literature. Finally, our analysis of collaboration
trends in Computer Science reveals decreasing rates of collaboration between
authors of different genders.
"
2616,"Paper-Patent Citation Linkages as Early Signs for Predicting Delayed
  Recognized Knowledge: Macro and Micro Evidence","  In this study, we investigate the extent to which patent citations to papers
can serve as early signs for predicting delayed recognized knowledge in science
using a comparative study with a control group, i.e., instant recognition
papers. We identify the two opposite groups of papers by the Bcp measure, a
parameter-free index for identifying papers which were recognized with delay.
We provide a macro (Science/Nature papers dataset) and micro (a case chosen
from the dataset) evidence on paper-patent citation linkages as early signs for
predicting delayed recognized knowledge in science. It appears that papers with
delayed recognition show a stronger and longer technical impact than instant
recognition papers. We provide indication that in the more recent years papers
with delayed recognition are awakened more often and earlier by a patent rather
than by a scientific paper (also called ""prince""). We also found that patent
citations seem to play an important role to avoid instant recognition papers to
level off or to become a so called ""flash in the pan"", i.e., instant
recognition. It also appears that the sleeping beauties may firstly encounter
negative citations and then patent citations and finally get widely recognized.
In contrast to the two focused fields (biology and chemistry) for instant
recognition papers, delayed recognition papers are rather evenly distributed in
biology, chemistry, psychology, geology, materials science, and physics. We
discovered several pairs of ""science sleeping""-""technology [...]. We propose in
further research to discover the potential ahead of time and transformative
research by using citation delay analysis, patent & NPL analysis, and citation
context analysis.
"
2617,"Predicting Patent Citations to measure Economic Impact of Scholarly
  Research","  A crucial goal of funding research and development has always been to advance
economic development. On this basis, a consider-able body of research
undertaken with the purpose of determining what exactly constitutes economic
impact and how to accurately measure that impact has been published. Numerous
indicators have been used to measure economic impact, although no single
indicator has been widely adapted. Based on patent data collected from
Altmetric we predict patent citations through various social media features
using several classification models. Patents citing a research paper implies
the potential it has for direct application inits field. These predictions can
be utilized by researchers in deter-mining the practical applications for their
work when applying for patents.
"
2618,"Cleaning Noisy and Heterogeneous Metadata for Record Linking Across
  Scholarly Big Datasets","  Automatically extracted metadata from scholarly documents in PDF formats is
usually noisy and heterogeneous, often containing incomplete fields and
erroneous values. One common way of cleaning metadata is to use a bibliographic
reference dataset. The challenge is to match records between corpora with high
precision. The existing solution which is based on information retrieval and
string similarity on titles works well only if the titles are cleaned. We
introduce a system designed to match scholarly document entities with noisy
metadata against a reference dataset. The blocking function uses the classic
BM25 algorithm to find the matching candidates from the reference data that has
been indexed by ElasticSearch. The core components use supervised methods which
combine features extracted from all available metadata fields. The system also
leverages available citation information to match entities. The combination of
metadata and citation achieves high accuracy that significantly outperforms the
baseline method on the same test dataset. We apply this system to match the
database of CiteSeerX against Web of Science, PubMed, and DBLP. This method
will be deployed in the CiteSeerX system to clean metadata and link records to
other scholarly big datasets.
"
2619,A bibliometric analysis of Bitcoin scientific production,"  Blockchain technology, and more specifically Bitcoin (one of its foremost
applications), have been receiving increasing attention in the scientific
community. The first publications with Bitcoin as a topic, can be traced back
to 2012. In spite of this short time span, the production magnitude (1162
papers) makes it necessary to make a bibliometric study in order to observe
research clusters, emerging topics, and leading scholars. Our paper is aimed at
studying the scientific production only around bitcoin, excluding other
blockchain applications. Thus, we restricted our search to papers indexed in
the Web of Science Core Collection, whose topic is ""bitcoin"". This database is
suitable for such diverse disciplines such as economics, engineering,
mathematics, and computer science. This bibliometric study draws the landscape
of the current state and trends of Bitcoin-related research in different
scientific disciplines.
"
2620,"A Retrospective Recount of Computer Architecture Research with a
  Data-Driven Study of Over Four Decades of ISCA Publications","  This study began with a research project, called DISCvR, conducted at the
IBM-ILLINOIS Center for Cognitive Computing Systems Reseach. The goal of DISCvR
was to build a practical NLP based AI pipeline for document understanding which
will help us better understand the computation patterns and requirements of
modern computing systems. While building such a prototype, an early use case
came to us thanks to the 2017 IEEE/ACM International Symposium on
Microarchitecture (MICRO-50) Program Co-chairs, Drs. Hillery Hunter and Jaime
Moreno. They asked us if we can perform some data-driven analysis of the past
50 years of MICRO papers and show some interesting historical perspectives on
MICRO's 50 years of publication. We learned two important lessons from that
experience: (1) building an AI solution to truly understand unstructured data
is hard in spite of the many claimed successes in natural language
understanding; and (2) providing a data-driven perspective on computer
architecture research is a very interesting and fun project. Recently we
decided to conduct a more thorough study based on all past papers of
International Symposium on Computer Architecture (ISCA) from 1973 to 2018,
which resulted this article. We recognize that we have just scratched the
surface of natural language understanding of unstructured data, and there are
many more aspects that we can improve. But even with our current study, we felt
there were enough interesting findings that may be worthwhile to share with the
community. Hence we decided to write this article to summarize our findings so
far based only on ISCA publications. Our hope is to generate further interests
from the community in this topic, and we welcome collaboration from the
community to deepen our understanding both of the computer architecture
research and of the challenges of NLP-based AI solutions.
"
2621,"Systematic improvement of user engagement with academic titles using
  computational linguistics","  This paper describes a novel approach to systematically improve information
interactions based solely on its wording. Following an interdisciplinary
literature review, we recognized three key attributes of words that drive user
engagement: (1) Novelty (2) Familiarity (3) Emotionality. Based on these
attributes, we developed a model to systematically improve a given content
using computational linguistics, natural language processing (NLP) and text
analysis (word frequency, sentiment analysis and lexical substitution). We
conducted a pilot study (n=216) in which the model was used to formalize
evaluation and optimization of academic titles. A between-group design (A/B
testing) was used to compare responses to the original and modified (treatment)
titles. Data was collected for selection and evaluation (User Engagement
Scale). The pilot results suggest that user engagement with digital information
is fostered by, and perhaps dependent upon, the wording being used. They also
provide empirical support that engaging content can be systematically evaluated
and produced. The preliminary results show that the modified (treatment) titles
had significantly higher scores for information use and user engagement
(selection and evaluation). We propose that computational linguistics is a
useful approach for optimizing information interactions. The empirically based
insights can inform the development of digital content strategies, thereby
improving the success of information interactions.elop more sophisticated
interaction measures.
"
2622,Characterisation of the $\chi$-index and the $rec$-index,"  Axiomatic characterisation of a bibliometric index provides insight into the
properties that the index satisfies and facilitates the comparison of different
indices. A geometric generalisation of the $h$-index, called the $\chi$-index,
has recently been proposed to address some of the problems with the $h$-index,
in particular, the fact that it is not scale invariant, i.e., multiplying the
number of citations of each publication by a positive constant may change the
relative ranking of two researchers. While the square of the $h$-index is the
area of the largest square under the citation curve of a researcher, the square
of the $\chi$-index, which we call the $rec$-index (or {\em rectangle}-index),
is the area of the largest rectangle under the citation curve. Our main
contribution here is to provide a characterisation of the $rec$-index via three
properties: {\em monotonicity}, {\em uniform citation} and {\em uniform
equivalence}. Monotonicity is a natural property that we would expect any
bibliometric index to satisfy, while the other two properties constrain the
value of the $rec$-index to be the area of the largest rectangle under the
citation curve. The $rec$-index also allows us to distinguish between {\em
influential} researchers who have relatively few, but highly-cited,
publications and {\em prolific} researchers who have many, but less-cited,
publications.
"
2623,The BIDS Toolbox: A web service to manage brain imaging datasets,"  Data sharing is a key factor for ensuring reproducibility and transparency of
scientific experiments, and neuroimaging is no exception. The vast
heterogeneity of data formats and imaging modalities utilised in the field
makes it a very challenging problem. In this context, the Brain Imaging Data
Structure (BIDS) appears as a solution for organising and describing
neuroimaging datasets. Since its publication in 2015, BIDS has gained
widespread attention in the field, as it provides a common way to arrange and
share multimodal brain images. Although the evident benefits it presents, BIDS
has not been widely adopted in the field of MRI yet and we believe that this is
due to the lack of a go-to tool to create and managed BIDS datasets. Motivated
by this, we present the BIDS Toolbox, a web service to manage brain imaging
datasets in BIDS format. Different from other tools, the BIDS Toolbox allows
the creation and modification of BIDS-compliant datasets based on MRI data. It
provides both a web interface and REST endpoints for its use. In this paper we
describe its design and early prototype, and provide a link to the public
source code repository.
"
2624,The UN Security Council debates 1995-2017,"  This paper presents a new dataset containing 65,393 speeches held in the
public meetings of the UN Security Council (UNSC) between 1995 and 2017. The
dataset is based on publicly available meeting transcripts with the S/PV
document symbol and includes the full substance of individual speeches as well
as automatically extracted and manually corrected metadata on the speaker, the
position of the speech in the sequence of speeches of a meeting, and the date
of the speech. After contextualizing the dataset in recent research on the
UNSC, the paper presents descriptive statistics on UNSC meetings and speeches
that characterize the period covered by the dataset. Data highlight the
extensive presence of the UN bureaucracy in UNSC meetings as well as an
emerging trend towards more lengthy open UNSC debates. These open debates cover
key issues that have emerged only during the period that is covered by the
dataset, for example the debates relating to Women, Peace and Security or
Climate-related Disasters.
"
2625,Taxonomy-as-a-Service: How To Structure Your Related Work,"  Structuring related work is a daunting task encompassing literature review,
classification, comparison (primarily in the form of concepts), and gap
analysis. Building taxonomies is a compelling way to structure concepts in the
literature yielding reusable and extensible models. However, constructing
taxonomies as a product of literature reviews could become, to our experiences,
immensely complex and error-prone. Including new literature or addressing
errors may cause substantial changes (ripple effects) in taxonomies coping with
which requires adequate tools. To this end, we propose a
\emph{Taxonomy-as-a-Service (TaaS)} platform. TaaS combines the systematic
paper review process with taxonomy development, visualization, and analysis
capabilities. We evaluate the effectiveness and efficiency of our platform by
employing it in the development of a real-world taxonomy. Our results indicate
that our TaaS can be used to effectively craft and maintain UML-conforming
taxonomies and thereby structure related work. The screencast of our tool
demonstration is available at \url{https://goo.gl/GsTjsP}.
"
2626,BioGen: Automated Biography Generation,"  A biography of a person is the detailed description of several life events
including his education, work, relationships, and death. Wikipedia, the free
web-based encyclopedia, consists of millions of manually curated biographies of
eminent politicians, film and sports personalities, etc. However, manual
curation efforts, even though efficient, suffers from significant delays. In
this work, we propose an automatic biography generation framework BioGen.
BioGen generates a short collection of biographical sentences clustered into
multiple events of life. Evaluation results show that biographies generated by
BioGen are significantly closer to manually written biographies in Wikipedia. A
working model of this framework is available at nlpbiogen.herokuapp.com/home/.
"
2627,"Semantic Preserving Bijective Mappings for Expressions involving Special
  Functions in Computer Algebra Systems and Document Preparation Systems","  Purpose: Modern mathematicians and scientists of math-related disciplines
often use Document Preparation Systems (DPS) to write and Computer Algebra
Systems (CAS) to calculate mathematical expressions. Usually, they translate
the expressions manually between DPS and CAS. This process is time-consuming
and error-prone. Our goal is to automate this translation. This paper uses
Maple and Mathematica as the CAS, and LaTeX as our DPS.
  Design/methodology/approach: Bruce Miller at the National Institute of
Standards and Technology (NIST) developed a collection of special LaTeX macros
that create links from mathematical symbols to their definitions in the NIST
Digital Library of Mathematical Functions (DLMF). We are using these macros to
perform rule-based translations between the formulae in the DLMF and CAS.
Moreover, we develop software to ease the creation of new rules and to discover
inconsistencies.
  Findings: We created 396 mappings and translated 58.8% of DLMF formulae
(2,405 expressions) successfully between Maple and DLMF. For a significant
percentage, the special function definitions in Maple and the DLMF were
different. Therefore, an atomic symbol in one system maps to a composite
expression in the other system. The translator was also successfully used for
automatic verification of mathematical online compendia and CAS. Our evaluation
techniques discovered two errors in the DLMF and one defect in Maple.
  Originality: This paper introduces the first translation tool for special
functions between LaTeX and CAS. The approach improves error-prone manual
translations and can be used to verify mathematical online compendia and CAS.
"
2628,"Improving Academic Plagiarism Detection for STEM Documents by Analyzing
  Mathematical Content and Citations","  Identifying academic plagiarism is a pressing task for educational and
research institutions, publishers, and funding agencies. Current plagiarism
detection systems reliably find instances of copied and moderately reworded
text. However, reliably detecting concealed plagiarism, such as strong
paraphrases, translations, and the reuse of nontextual content and ideas is an
open research problem. In this paper, we extend our prior research on analyzing
mathematical content and academic citations. Both are promising approaches for
improving the detection of concealed academic plagiarism primarily in Science,
Technology, Engineering and Mathematics (STEM). We make the following
contributions: i) We present a two-stage detection process that combines
similarity assessments of mathematical content, academic citations, and text.
ii) We introduce new similarity measures that consider the order of
mathematical features and outperform the measures in our prior research. iii)
We compare the effectiveness of the math-based, citation-based, and text-based
detection approaches using confirmed cases of academic plagiarism. iv) We
demonstrate that the combined analysis of math-based and citation-based content
features allows identifying potentially suspicious cases in a collection of
102K STEM documents. Overall, we show that analyzing the similarity of
mathematical content and academic citations is a striking supplement for
conventional text-based detection approaches for academic literature in the
STEM disciplines.
"
2629,"OpenCitations, an infrastructure organization for open scholarship","  OpenCitations is an infrastructure organization for open scholarship
dedicated to the publication of open citation data as Linked Open Data using
Semantic Web technologies, thereby providing a disruptive alternative to
traditional proprietary citation indexes. Open citation data are valuable for
bibliometric analysis, increasing the reproducibility of large-scale analyses
by enabling publication of the source data. Following brief introductions to
the development and benefits of open scholarship and to Semantic Web
technologies, this paper describes OpenCitations and its datasets, tools,
services and activities. These include the OpenCitations Data Model; the SPAR
(Semantic Publishing and Referencing) Ontologies; OpenCitations' open software
of generic applicability for searching, browsing and providing REST APIs over
RDF triplestores; Open Citation Identifiers (OCIs) and the OpenCitations OCI
Resolution Service; the OpenCitations Corpus (OCC), a database of open
downloadable bibliographic and citation data made available in RDF under a
Creative Commons public domain dedication; and the OpenCitations Indexes of
open citation data, of which the first and largest is COCI, the OpenCitations
Index of Crossref Open DOI-to-DOI Citations, which currently contains over 445
million bibliographic citations and is receiving considerable usage by the
scholarly community.
"
2630,Infrastructure-Agnostic Hypertext,"  This paper presents a novel and formal interpretation of the original vision
of hypertext: infrastructure-agnostic hypertext is independent from specific
standards such as data formats and network protocols. Its model is illustrated
with examples and references to existing technologies that allow for
implementation and integration in current information infrastructures such as
the Internet.
"
2631,Topic Modeling the Reading and Writing Behavior of Information Foragers,"  The general problem of ""information foraging"" in an environment about which
agents have incomplete information has been explored in many fields, including
cognitive psychology, neuroscience, economics, finance, ecology, and computer
science. In all of these areas, the searcher aims to enhance future performance
by surveying enough of existing knowledge to orient themselves in the
information space. Individuals can be viewed as conducting a cognitive search
in which they must balance exploration of ideas that are novel to them against
exploitation of knowledge in domains in which they are already expert.
  In this dissertation, I present several case studies that demonstrate how
reading and writing behaviors interact to construct personal knowledge bases.
These studies use LDA topic modeling to represent the information environment
of the texts each author read and wrote. Three studies revolve around Charles
Darwin. Darwin left detailed records of every book he read for 23 years, from
disembarking from the H.M.S. Beagle to just after publication of The Origin of
Species. Additionally, he left copies of his drafts before publication. I
characterize his reading behavior, then show how that reading behavior
interacted with the drafts and subsequent revisions of The Origin of Species,
and expand the dataset to include later readings and writings. Then, through a
study of Thomas Jefferson's correspondence, I expand the study to non-book
data. Finally, through an examination of neuroscience citation data, I move
from individual behavior to collective behavior in constructing an information
environment. Together, these studies reveal ""the interplay between individual
and collective phenomena where innovation takes place"" (Tria et al. 2014).
"
2632,Introducing MathQA -- A Math-Aware Question Answering System,"  We present an open source math-aware Question Answering System based on Ask
Platypus. Our system returns as a single mathematical formula for a natural
language question in English or Hindi. This formulae originate from the
knowledge-base Wikidata. We translate these formulae to computable data by
integrating the calculation engine sympy into our system. This way, users can
enter numeric values for the variables occurring in the formula. Moreover, the
system loads numeric values for constants occurring in the formula from
Wikidata. In a user study, our system outperformed a commercial computational
mathematical knowledge engine by 13%. However, the performance of our system
heavily depends on the size and quality of the formula data available in
Wikidata. Since only a few items in Wikidata contained formulae when we started
the project, we facilitated the import process by suggesting formula edits to
Wikidata editors. With the simple heuristic that the first formula is
significant for the article, 80% of the suggestions were correct.
"
2633,"Comparison of research productivity of Italian and Norwegian professors
  and universities","  This is the first ever attempt of application in a country other than Italy
of a research efficiency indicator (FSS), to assess and compare the performance
of professors and universities, within and between countries. A special
attention has been devoted to the presentation of the methodology developed to
set up a common field classification scheme of professors, and to overcome the
limited availability of comparable input data. Results of the comparison
between countries, carried out in the 2011-2015 period, show similar average
performances of professors, but noticeable differences in the distributions,
whereby Norwegian professors are more concentrated in the tails. Norway shows
notable higher performance in Mathematics and Earth and Space Sciences, while
Italy in Biomedical Research and Engineering.
"
2634,The citation advantage of linking publications to research data,"  Efforts to make research results open and reproducible are increasingly
reflected by journal policies encouraging or mandating authors to provide data
availability statements. As a consequence of this, there has been a strong
uptake of data availability statements in recent literature. Nevertheless, it
is still unclear what proportion of these statements actually contain
well-formed links to data, for example via a URL or permanent identifier, and
if there is an added value in providing such links. We consider 531,889 journal
articles published by PLOS and BMC, develop an automatic system for labelling
their data availability statements according to four categories based on their
content and the type of data availability they display, and finally analyze the
citation advantage of different statement categories via regression. We find
that, following mandated publisher policies, data availability statements
become very common. In 2018 93.7% of 21,793 PLOS articles and 88.2% of 31,956
BMC articles had data availability statements. Data availability statements
containing a link to data in a repository -- rather than being available on
request or included as supporting information files -- are a fraction of the
total. In 2017 and 2018, 20.8% of PLOS publications and 12.2% of BMC
publications provided DAS containing a link to data in a repository. We also
find an association between articles that include statements that link to data
in a repository and up to 25.36% ($\pm$~1.07%) higher citation impact on
average, using a citation prediction model. We discuss the potential
implications of these results for authors (researchers) and journal publishers
who make the effort of sharing their data in repositories. All our data and
code are made available in order to reproduce and extend our results.
"
2635,"Materials databases: the need for open, interoperable databases with
  standardized data and rich metadata","  Driven by the recent rapid increase in the number of materials databases
published (open and commercial), I discuss here some perspectives on the
growing need for standardized, interoperable, open databases. The field of
computational materials discovery is quickly expanding, and recent advances in
data mining, high throughput screening, and machine learning highlight the
potential of open databases.
"
2636,Un Modelo Ontol\'ogico para el Gobierno Electr\'onico,"  Decision making often requires information that must be Provided with the
rich data format. Addressing these new requirements appropriately makes it
necessary for government agencies to orchestrate large amounts of information
from different sources and formats, to be efficiently delivered through the
devices commonly used by people, such as computers, netbooks, tablets and
smartphones. To overcome these problems, a model is proposed for the conceptual
representation of the State's organizational units, seen as georeferenced
entities of Electronic Government, based on ontologies designed under the
principles of Linked Open Data, which allows the automatic extraction of
information through the machines, which supports the process of governmental
decision making and gives citizens full access to find and process through
mobile technologies.
"
2637,"Publication modalities 'article in press' and 'open access' in relation
  to journal average citation","  There has been a generalization in the use of two publication practices by
scientific journals during the past decade: 1. 'article in press' or early
view, which allows access to the accepted paper before its formal publication
in an issue; 2. 'open access', which allows readers to obtain it freely and
free of charge. This paper studies the influence of both publication modalities
on the average impact of the journal and its evolution over time. It tries to
identify the separate effect of access on citation into two major parts: early
view and selection effect, managing to provide some evidence of the positive
effect of both. Scopus is used as the database and CiteScore as the measure of
journal impact. The prevalence of both publication modalities is quantified.
Differences in the average impact factor of group of journals, according to
their publication modalities, are tested. The evolution over time of the
citation influence, from 2011 to 2016, is also analysed. Finally, a linear
regression to explain the correlation of these publication practices with the
CiteScore in 2016, in a ceteris paribus context, is estimated. Our main
findings show evidence of a positive correlation between average journal impact
and advancing the publication of accepted articles, moreover this correlation
increases over time. The open access modality, in a ceteris paribus context,
also correlates positively with average journal impact.
"
2638,"Historical comparison of gender inequality in scientific careers across
  countries and disciplines","  There is extensive, yet fragmented, evidence of gender differences in
academia suggesting that women are under-represented in most scientific
disciplines, publish fewer articles throughout a career, and their work
acquires fewer citations. Here, we offer a comprehensive picture of
longitudinal gender discrepancies in performance through a bibliometric
analysis of academic careers by reconstructing the complete publication history
of over 1.5 million gender-identified authors whose publishing career ended
between 1955 and 2010, covering 83 countries and 13 disciplines. We find that,
paradoxically, the increase of participation of women in science over the past
60 years was accompanied by an increase of gender differences in both
productivity and impact. Most surprisingly though, we uncover two gender
invariants, finding that men and women publish at a comparable annual rate and
have equivalent career-wise impact for the same size body of work. Finally, we
demonstrate that differences in dropout rates and career length explain a large
portion of the reported career-wise differences in productivity and impact.
This comprehensive picture of gender inequality in academia can help rephrase
the conversation around the sustainability of women's careers in academia, with
important consequences for institutions and policy makers.
"
2639,"Cui Prodest? Reciprocity of collaboration measured by Russian Index of
  Science Citation","  Scientific collaboration is often not perfectly reciprocal. Scientifically
strong countries/institutions/laboratories may help their less prominent
partners with leading scholars, or finance, or other resources. What is
interesting in such type of collaboration is that (1) it may be measured by
bibliometrics and (2) it may shed more light on the scholarly level of both
collaborating organizations themselves. In this sense measuring institutions in
collaboration sometimes may tell more than attempts to assess them as
stand-alone organizations. Evaluation of collaborative patterns was explained
in detail, for example, by Glanzel (2001; 2003). Here we combine these methods
with a new one, made available by separating 'the best' journals from 'others'
on the same platform of Russian Index of Science Citation (RISC). Such
sub-universes of journals from 'different leagues' provide additional methods
to study how collaboration influences the quality of papers published by
organizations.
"
2640,Geographical Distribution of Biomedical Research in the USA and China,"  We analyze nearly 20 million geocoded PubMed articles with author
affiliations. Using K-means clustering for the lower 48 US states and mainland
China, we find that the average published paper is within a relatively short
distance of a few centroids. These centroids have shifted very little over the
past 30 years, and the distribution of distances to these centroids has not
changed much either. The overall country centroids have gradually shifted south
(about 0.2{\deg} for the USA and 1.7{\deg} for China), while the longitude has
not moved significantly. These findings indicate that there are few large
scientific hubs in the USA and China and the typical investigator is within
geographical reach of one such hub. This sets the stage to study centralization
of biomedical research at national and regional levels across the globe, and
over time.
"
2641,Robust Archives Maximize Scientific Accessibility,"  We present a bibliographic analysis of Chandra, Hubble, and Spitzer
publications. We find (a) archival data are used in >60% of the publication
output and (b) archives for these missions enable a much broader set of
institutions and countries to scientifically use data from these missions.
Specifically, we find that authors from institutions that have published few
papers from a given mission publish 2/3 archival publications, while those with
many publications typically have 1/3 archival publications. We also show that
countries with lower GDP per capita overwhelmingly produce archival
publications, while countries with higher GDP per capital produce guest
observer and archival publications in equal amounts. We argue that robust
archives are thus not only critical for the scientific productivity of mission
data, but also the scientific accessibility of mission data. We argue that the
astronomical community should support archives to maximize the overall
scientific societal impact of astronomy, and represent an excellent investment
in astronomy's future.
"
2642,"Non-English language publications in Citation Indexes -- quantity and
  quality","  We analyzed publications data in WoS and Scopus to compare publications in
native languages vs publications in English and find any distinctive patterns.
We analyzed their distribution by research areas, languages, type of access and
citation patterns. The following trends were found: share of English
publications increases over time; native-language publications are read and
cited less than English-language outside the origin country; open access impact
on views and citation is higher for native languages; journal ranking
correlates with the share of English publications for multi-language journals.
We conclude also that the role of non-English publications in research
evaluation in non-English speaking countries is underestimated when research in
social science and humanities is assessed only by publications in Web of
Science and Scopus.
"
2643,CupQ: A New Clinical Literature Search Engine,"  A new clinical literature search engine, called CupQ, is presented. It aims
to help clinicians stay updated with medical knowledge. Although PubMed is
currently one of the most widely used digital libraries for biomedical
information, it frequently does not return clinically relevant results. CupQ
utilizes a ranking algorithm that filters non-medical journals, compares
semantic similarity between queries, and incorporates journal impact factor and
publication date. It organizes search results into useful categories for
medical practitioners: reviews, guidelines, and studies. Qualitative
comparisons suggest that CupQ may return more clinically relevant information
than PubMed. CupQ is available at https://cupq.io/.
"
2644,Evaluating the Reproducibility of Research in Obstetrics and Gynecology,"  Objective: Reproducibility is a core tenet of scientific research. A
reproducible study is one where the results can be recreated by different
investigators in different circumstances using the same methodology and
materials. Unfortunately, reproducibility is not a standard to which the
majority of research is currently adherent. Methods: We objectively evaluated
300 trials in the field of Obstetrics and Gynecology for fourteen indicators of
reproducibility. These indicators include availability of data, analysis
scripts, pre-registration information, study protocols and whether or not the
study was available via Open Access. We also assessed the trials for financial
conflict of interest statements and source of funding. Results: Of the 300
trials in our sample, 208 contained empirical data that could be assessed for
reproducibility. None of the trials in our sample provided a link to their
protocols or provided a statement on availability of materials. None were
replication studies. Just 10.58% provided a statement regarding their data
availability, while only 5.82% provided a statement on preregistration. 25.85%
failed to report the presence or absence of conflicts of interest and 54.08%
did not state the origin of their funding. Conclusion: Research in the field of
Obstetrics and Gynecology is not consistently reproducible and frequently lacks
conflict of interest disclosure. Consequences of this could be far-reaching and
include increased research waste, widespread acceptance of misleading results
and erroneous conclusions guiding clinical decision-making.
"
2645,"Might Europe one day again be a global scientific powerhouse? Analysis
  of ERC publications suggests it will not be possible without changes in
  research policy","  Numerous EU documents praise the excellence of EU research without empirical
evidence and against academic studies. We investigated research performance in
two fields of high socioeconomic importance, advanced technology and basic
medical research, in two sets of European countries, Germany, France, Italy,
and Spain (GFIS), and the UK, the Netherlands, and Switzerland (UKNCH). Despite
historical and geographical proximity, research performance in GFIS is much
lower than in UKNCH, and well below the world average. Funding from the
European Research Council (ERC) greatly improves performance both in GFIS and
UKNCH, but ERC-GFIS publications are less cited than ERC-UKNCH publications. We
conclude that research performance in GFIS and in other EU countries is
intrinsically low even when it is generously funded. The technological and
economic future of the EU depends on improving research, which requires
structural changes in research policy within the EU, and in most EU countries.
"
2646,Overview and Results: CL-SciSumm Shared Task 2019,"  The CL-SciSumm Shared Task is the first medium-scale shared task on
scientific document summarization in the computational linguistics~(CL) domain.
In 2019, it comprised three tasks: (1A) identifying relationships between
citing documents and the referred document, (1B) classifying the discourse
facets, and (2) generating the abstractive summary. The dataset comprised 40
annotated sets of citing and reference papers of the CL-SciSumm 2018 corpus and
1000 more from the SciSummNet dataset. All papers are from the open access
research papers in the CL domain. This overview describes the participation and
the official results of the CL-SciSumm 2019 Shared Task, organized as a part of
the 42nd Annual Conference of the Special Interest Group in Information
Retrieval (SIGIR), held in Paris, France in July 2019. We compare the
participating systems in terms of two evaluation metrics and discuss the use of
ROUGE as an evaluation metric. The annotated dataset used for this shared task
and the scripts used for evaluation can be accessed and used by the community
at: https://github.com/WING-NUS/scisumm-corpus.
"
2647,"Investigating Correlations of Inter-coder Agreement and Machine
  Annotation Performance for Historical Video Data","  Video indexing approaches such as visual concept classification and person
recognition are essential to enable fine-grained semantic search in large-scale
video archives such as the historical video collection of former German
Democratic Republic (GDR) maintained by the German Broadcasting Archive (DRA).
Typically, a lexicon of visual concepts has to be defined for semantic search.
However, the definition of visual concepts can be more or less subjective due
to individually differing judgments of annotators, which may have an impact on
annotation quality and subsequently training of supervised machine learning
methods. In this paper, we analyze the inter-coder agreement for historical TV
data of the former GDR for visual concept classification and person
recognition. The inter-coder agreement is evaluated for a group of expert as
well as non-expert annotators in order to determine differences in annotation
homogeneity. Furthermore, correlations between visual recognition performance
and inter-annotator agreement are measured. In this context, information about
image quantity and agreement are used to predict average precision for concept
classification. Finally, the influence of expert vs. non-expert annotations
acquired in the study are used to evaluate person recognition.
"
2648,Analyzing Linguistic Complexity and Scientific Impact,"  The number of publications and the number of citations received have become
the most common indicators of scholarly success. In this context, scientific
writing increasingly plays an important role in scholars' scientific careers.
To understand the relationship between scientific writing and scientific
impact, this paper selected 12 variables of linguistic complexity as a proxy
for depicting scientific writing. We then analyzed these features from 36,400
full-text Biology articles and 1,797 full-text Psychology articles. These
features were compared to the scientific impact of articles, grouped into high,
medium, and low categories. The results suggested no practical significant
relationship between linguistic complexity and citation strata in either
discipline. This suggests that textual complexity plays little role in
scientific impact in our data sets.
"
2649,"Coping with the delineation of emerging fields: Nanoscience and
  Nanotechnology as a case study","  Proper field delineation plays an important role in scientometric studies,
although it is a tough task. Based on an emerging and interdisciplinary field,
nanoscience and nanotechnology, this paper highlights the problem of field
delineation. First, we review the related literature. Then, three different
approaches to delineate a field of knowledge were applied at three different
levels of aggregation: subject category, publication level, and journal level.
Expert opinion interviews served to assess the data, and precision and recall
of each approach were calculated for comparison. Our findings confirm that
field delineation is a complicated issue at both the quantitative and the
qualitative level, even when experts validate results.
"
2650,Mapping the backbone of the Humanities through the eyes of Wikipedia,"  The present study aims to establish a valid method by which to apply the
theory of co-citations to Wikipedia article references and, subsequently, to
map these relationships between scientific papers. This theory, originally
applied to scientific literature, will be transferred to the digital
environment of collective knowledge generation. To this end, a dataset
containing Wikipedia references collected from Altmetric and Scopus' Journal
Metrics journals has been used. The articles have been categorized according to
the disciplines and specialties established in the All Science Journal
Classification (ASJC). They have also been grouped by journal of publication. A
set of articles in the Humanities, comprising 25 555 Wikipedia articles with 41
655 references to 32 245 resources, has been selected. Finally, a descriptive
statistical study has been conducted and co-citations have been mapped using
networks and indicators of degree and betweenness centrality.
"
2651,"The demography of the peripatetic researcher: Evidence on highly mobile
  scholars from the Web of Science","  The policy debate around researchers' geographic mobility has been moving
away from a theorized zero-sum game in which countries can be winners (brain
gain) or losers (brain drain), and toward the concept of brain circulation,
which implies that researchers move in and out of countries and everyone
benefits. Quantifying trends in researchers' movements is key to understanding
the drivers of the mobility of talent, as well as the implications of these
patterns for the global system of science, and for the competitive advantages
of individual countries. Existing studies have investigated bilateral flows of
researchers. However, in order to understand migration systems, determining the
extent to which researchers have worked in more than two countries is
essential. This study focuses on the subgroup of highly mobile researchers whom
we refer to as peripatetic researchers or super-movers.
  More specifically, our aim is to track the international movements of
researchers who have published in more than two countries through changes in
the main affiliation addresses of researchers in over 62 million publications
indexed in the Web of Science database over the 1956-2016 period. Using this
approach, we have established a longitudinal dataset on the international
movements of highly mobile researchers across all subject categories, and in
all disciplines of scholarship. This article contributes to the literature by
offering for the first time a snapshot of the key features of highly mobile
researchers, including their patterns of migration and return migration by
academic age, the relative frequency of their disciplines, and the relative
frequency of their countries of origin and destination. Among other findings,
the results point to the emergence of a global system that includes the USA and
China as two large hubs, and England and Germany as two smaller hubs for highly
mobile researchers.
"
2652,A computational EXFOR database,"  The EXFOR library is a useful resource for many people in the field of
nuclear physics. In particular, the experimental data in the EXFOR library
serves as a starting point for nuclear data evaluations. There is an ongoing
discussion about how to make evaluations more transparent and reproducible. One
important ingredient may be convenient programmatic access to the data in the
EXFOR library from high-level languages. To this end, the complete EXFOR
library can be converted to a MongoDB database. This database can be
conveniently searched and accessed from a wide variety of programming
languages, such as C++, Python, Java, Matlab, and R. This contribution provides
some details about the successful conversion of the EXFOR library to a MongoDB
database and shows simple usage examples to underline its merits. All codes
required for the conversion have been made available online and are
open-source. In addition, a Dockerfile has been created to facilitate the
installation process.
"
2653,Plan S. Pardon impossible to execute,"  The Plan S initiative is expected to radically change the market of scholarly
periodicals, resulting in the abandoning of the subscription model in favour of
the open access model. This transition poses new challenges, as well as sets
new tasks for researchers, managers, librarians and scientific publishers,
particularly in countries with transforming economies. In this study, we set
out to estimate approximate costs and future prospects associated with the
transition of Belarus and Ukraine towards open access to scientific information
on the example of publications by researchers from these two countries in
Elsevier journals. To this end, we accessed the Scopus database and selected
all papers affiliated with Belarus and Ukraine published by Elsevier journals
in 2018. Subsequently, we established which of these articles indicated a
corresponding author from Ukraine or Belarus, as well as collected journal
titles, correspondence addresses, access types and the APC fee for each
article. In addition, funding sources were established for each open access
article. This allowed us to determine the total amount of money that Ukraine
and Belarus would have needed to pay so that all the articles of their
researchers, which were published in Elsevier journals in 2018, were available
in an open-access form. The obtained results show that Belarus and Ukraine are
currently under-investing in the support of scientific publications and
subscription to scientific resources, which is necessary for making a
transition to the model of open access to scientific literature. The proposed
method for calculating the expenses of publishing papers written by researchers
from a given country in open access can be used in the preparation of
transformative agreements with developing countries.
"
2654,"Economic Power, Population, and the Size of Astronomical Community","  The number of astronomers for a country registered to the IAU is known to
have a correlation with the GDP. However, the robustness of this relationship
can be doubted, because the fraction of astronomers joining the IAU differs
from country to country. Here we revisit this correlation by using the recent
data updated as of 2017, and then we find a similar correlation by using the
total enumeration of astronomers and astrophysicists with PhD degrees and
working in each country, instead of adopting the number of IAU members. We
confirm the existence of two subgroup in the correlation. One group consists of
European advanced countries having long history of modern astronomy, while the
other group consists of countries having experienced recent rapid economic
development. In order to find causation in the correlation, we obtain the
long-term variations of the number of astronomers, population, and the GDP for
a number of countries to find that the number of astronomers per citizen for
recently developing countries has increased more rapidly as GDP per capita
increased, than that for fully developed countries. We collect a demographic
data of the Korean astronomical community. From these findings we estimate the
proper size of the Korean astronomical community by considering the society's
economic power and population. The current number of PhD astronomers working in
Korea is approximately 310, but it should be 550 that is large enough to be
comparable and competitive to the sizes of Spainish, Canadian, and Japanese
astronomical communities. We discuss on the way how to overcome the
vulnerability of the Korean astronomical community, based on the statistics of
national R&D expenditure structure comparing with that of other major advanced
countries.
"
2655,Towards a Comprehensive Bibliography for SETI,"  In this work, we motivate, describe, and announce a living bibliography for
academic papers and other works published in the Search for Extraterrestrial
Intelligence (SETI). The bibliography makes use of bibliographic groups
(bibgroups) in the NASA Astrophysics Data System (ADS), allowing it to be
accessed and searched by any interested party, and is composed only of works
which have a presence on the ADS. We establish criteria that describe the scope
of our bibliography, which we define as any academic work which broadly: 1)
advances knowledge within SETI, 2) deals with topics that are fundamentally
related to or about SETI, or 3) is useful for the better understanding of SETI,
and which has a presence on ADS. We discuss the future work needed to continue
the development of the bibliography. The bibliography can be found by using the
bibgroup field (bibgroup: SETI) in the ADS search engine.
"
2656,Criteria for assessing grant applications: A systematic review,"  Criteria are an essential component of any procedure for assessing merit.
Yet, little is known about the criteria peers use in assessing grant
applications. In this systematic review we therefore identify and synthesize
studies that examine grant peer review criteria in an empirical and inductive
manner. To facilitate the synthesis, we introduce a framework that classifies
what is generally referred to as 'criterion' into an evaluated entity (i.e. the
object of evaluation) and an evaluation criterion (i.e. the dimension along
which an entity is evaluated). In total, this synthesis includes 12 studies.
Two-thirds of these studies examine criteria in the medical and health
sciences, while studies in other fields are scarce. Few studies compare
criteria across different fields, and none focus on criteria for
interdisciplinary research. We conducted a qualitative content analysis of the
12 studies and thereby identified 15 evaluation criteria and 30 evaluated
entities as well as the relations between them. Based on a network analysis, we
propose a conceptualization that groups the identified evaluation criteria and
evaluated entities into aims, means, and outcomes. We compare our results to
criteria found in studies on research quality and guidelines of funding
agencies. Since peer review is often approached from a normative perspective,
we discuss our findings in relation to two normative positions, the fairness
doctrine and the ideal of impartiality. Our findings suggest that future
studies on criteria in grant peer review should focus on the applicant, include
data from non-Western countries, and examine fields other than the medical and
health sciences.
"
2657,"Making Recommendations from Web Archives for ""Lost"" Web Pages","  When a user requests a web page from a web archive, the user will typically
either get an HTTP 200 if the page is available, or an HTTP 404 if the web page
has not been archived. This is because web archives are typically accessed by
URI lookup, and the response is binary: the archive either has the page or it
does not, and the user will not know of other archived web pages that exist and
are potentially similar to the requested web page. In this paper, we propose
augmenting these binary responses with a model for selecting and ranking
recommended web pages in a Web archive. This is to enhance both HTTP 404
responses and HTTP 200 responses by surfacing web pages in the archive that the
user may not know existed. First, we check if the URI is already classified in
DMOZ or Wikipedia. If the requested URI is not found, we use ML to classify the
URI using DMOZ as our ontology and collect candidate URIs to recommended to the
user. Next, we filter the candidates based on if they are present in the
archive. Finally, we rank candidates based on several features, such as
archival quality, web page popularity, temporal similarity, and URI similarity.
We calculated the F1 score for different methods of classifying the requested
web page at the first level. We found that using all-grams from the URI after
removing numerals and the TLD produced the best result with F1=0.59. For
second-level classification, the micro-average F1=0.30. We found that 44.89% of
the correctly classified URIs contained at least one word that exists in a
dictionary and 50.07% of the correctly classified URIs contained long strings
in the domain. In comparison with the URIs from our Wayback access logs, only
5.39% of those URIs contained only words from a dictionary, and 26.74%
contained at least one word from a dictionary. These percentages are low and
may affect the ability for the requested URI to be correctly classified.
"
2658,"Research Leadership Flow Determinants and the Role of Proximity in
  Research Collaborations Networks","  Characterizing the leadership in research is important to revealing the
interaction pattern and organizational structure through research
collaboration. This research defines the leadership role based on the
corresponding author's affiliation, and presents, the first quantitative
research on the factors and evolution of five proximity dimensions
(geographical, cognitive, institutional, social and economic) of research
leadership. The data to capture research leadership consists of a set of
multi-institution articles in the life sciences and biomedical during 2013-2017
from Web of Science Core Citation Database. Our sample consists of 484,903
articles from 244 Chinese institutions, which have been the primary affiliation
of the corresponding author for at least one paper (with multiple institutions)
in each year. A Tobit regression-based gravity model indicates that research
leadership mass of both the leading and participating institutions and the
geographical, cognitive, institutional, social and economic proximity are
important factors of the flow of research leadership among Chinese
institutions. In general, the effect of these proximity for research leadership
flow has been declining recently. The outcome of this research sheds light on
the leadership evolution and flow among Chinese institutions, and thus can
provide evidence and support for grant allocation policy to facilitate
scientific research and collaborations.
"
2659,"The rise and rise of interdisciplinary research: Understanding the
  interaction dynamics of three major fields -- Physics, Mathematics & Computer
  Science","  The distinction between sciences is becoming increasingly more artificial --
an approach from one area can be easily applied to the other. More exciting
research nowadays is happening perhaps at the interfaces of disciplines like
Physics, Mathematics and Computer Science. How do these interfaces emerge and
interact? For instance, is there a specific pattern in which these fields cite
each other? In this article, we investigate a collection of more than 1.2
million papers from three different scientific disciplines -- Physics,
Mathematics, and Computer Science. We show how over a timescale the citation
patterns from the core science fields (Physics, Mathematics) to the applied and
fast-growing field of Computer Science have drastically increased. Further, we
observe how certain subfields in these disciplines are shrinking while others
are becoming tremendously popular. For instance, an intriguing observation is
that citations from Mathematics to the subfield of machine learning in Computer
Science in recent times are exponentially increasing.
"
2660,The Evolution of IJHCS and CHI: A Quantitative Analysis,"  In this paper we focus on the International Journal of Human-Computer Studies
(IJHCS) as a domain of analysis, to gain insights about its evolution in the
past 50 years and what this evolution tells us about the research landscape
associated with the journal. To this purpose we use techniques from the field
of Science of Science and analyse the relevant scholarly data to identify a
variety of phenomena, including significant geopolitical patterns, the key
trends that emerge from a topic-centric analysis, and the insights that can be
drawn from an analysis of citation data. Because the area of Human-Computer
Interaction (HCI) has always been a central focus for IJHCS, we also include in
the analysis the CHI conference, which is the premiere scientific venue in HCI.
Analysing both venues provides more data points to our study and allows us to
consider two alternative viewpoints on the evolution of HCI research.
"
2661,"Citations in Software Engineering -- Paper-related, Journal-related, and
  Author-related Factors","  Many factors could affect the number of citations to a paper. Citations have
an important role in research policy and in measuring the excellence of
research and researchers. This work is the first study in software engineering
(SE) to assess multiple factors affecting the number of citations to SE papers.
We use (a) negative binomial regression and (b) quantile regression to study
arithmetic mean and median expected citations of a paper. Our dataset includes
all the 25,113 papers which have been published in a set of 16 main SE
journals, between 1970 and 2018. Our results indicate that publication venue,
author team's past citations, paper length, the number of references, and the
recency of references are the most influential factors on the number of
citations to SE papers. From our empirical findings, we present several
implications and advice to researchers for getting higher citations on their
papers, which are in addition to the obvious case of conducting high-quality
technical research, e.g. (1) Aim for high-profile venues, (2) Build a
high-quality author team with highly cited past papers, and (3) Aim for
high-quality work that has comprehensive content (thus longer paper length and
reference list).
"
2662,Assessing the Quality of Scientific Papers,"  A multitude of factors are responsible for the overall quality of scientific
papers, including readability, linguistic quality, fluency,semantic complexity,
and of course domain-specific technical factors. These factors vary from one
field of study to another. In this paper, we propose a measure and method for
assessing the overall quality of the scientific papers in a particular field of
study. We evaluate our method in the computer science domain, but it can be
applied to other technical and scientific fields.Our method is based on the
corpus linguistics technique. This technique enables the extraction of required
information and knowledge associated with a specific domain. For this purpose,
we have created a large corpus, consisting of papers from very high impact
conferences. First, we analyze this corpus in order to extract rich
domain-specific terminology and knowledge. Then we use the acquired knowledge
to estimate the quality of scientific papers by applying our proposed measure.
We examine our measure on high and low scientific impact test corpora. Our
results show a significant difference in the measure scores of the high and low
impact test corpora. Second, we develop a classifier based on our proposed
measure and compare it to the baseline classifier. Our results show that the
classifier based on our measure over-performed the baseline classifier. Based
on the presented results the proposed measure and the technique can be used for
automated assessment of scientific papers.
"
2663,Reducing the Effort for Systematic Reviews in Software Engineering,"  Context. Systematic Reviews (SRs) are means for collecting and synthesizing
evidence from the identification and analysis of relevant studies from multiple
sources. To this aim, they use a well-defined methodology meant to mitigate the
risks of biases and ensure repeatability for later updates. SRs, however,
involve significant effort. Goal. The goal of this paper is to introduce a
novel methodology that reduces the amount of manual tedious tasks involved in
SRs while taking advantage of the value provided by human expertise. Method.
Starting from current methodologies for SRs, we replaced the steps of
keywording and data extraction with an automatic methodology for generating a
domain ontology and classifying the primary studies. This methodology has been
applied in the Software Engineering sub-area of Software Architecture and
evaluated by human annotators. Results. The result is a novel Expert-Driven
Automatic Methodology, EDAM, for assisting researchers in performing SRs. EDAM
combines ontology-learning techniques and semantic technologies with the
human-in-the-loop. The first (thanks to automation) fosters scalability,
objectivity, reproducibility and granularity of the studies; the second allows
tailoring to the specific focus of the study at hand and knowledge reuse from
domain experts. We evaluated EDAM on the field of Software Architecture against
six senior researchers. As a result, we found that the performance of the
senior researchers in classifying papers was not statistically significantly
different from EDAM. Conclusions. Thanks to automation of the less-creative
steps in SRs, our methodology allows researchers to skip the tedious tasks of
keywording and manually classifying primary studies, thus freeing effort for
the analysis and the discussion.
"
2664,"Delineating Knowledge Domains in the Scientific Literature Using Visual
  Information","  Figures are an important channel for scientific communication, used to
express complex ideas, models and data in ways that words cannot. However, this
visual information is mostly ignored in analyses of the scientific literature.
In this paper, we demonstrate the utility of using scientific figures as
markers of knowledge domains in science, which can be used for classification,
recommender systems, and studies of scientific information exchange. We encode
sets of images into a visual signature, then use distances between these
signatures to understand how patterns of visual communication compare with
patterns of jargon and citation structures. We find that figures can be as
effective for differentiating communities of practice as text or citation
patterns. We then consider where these metrics disagree to understand how
different disciplines use visualization to express ideas. Finally, we further
consider how specific figure types propagate through the literature, suggesting
a new mechanism for understanding the flow of ideas apart from conventional
channels of text and citations. Our ultimate aim is to better leverage these
information-dense objects to improve scientific communication across
disciplinary boundaries.
"
2665,"Predicting publication productivity for researchers: a piecewise Poisson
  model","  Predicting the scientific productivity of researchers is a basic task for
academic administrators and funding agencies. This study provided a model for
the publication dynamics of researchers, inspired by the distribution feature
of researchers' publications in quantity. It is a piecewise Poisson model,
analyzing and predicting the publication productivity of researchers by
regression. The principle of the model is built on the explanation for the
distribution feature as a result of an inhomogeneous Poisson process that can
be approximated as a piecewise Poisson process. The model's principle was
validated by the high quality dblp dataset, and its effectiveness was testified
in predicting the publication productivity for majority of researchers and the
evolutionary trend of their publication productivity. Tests to confirm or
disconfirm the model are also proposed. The model has the advantage of
providing results in an unbiased way; thus is useful for funding agencies that
evaluate a vast number of applications with a quantitative index on
publications.
"
2666,"The citation advantage of foreign language references for Chinese social
  science papers","  Contemporary scientific exchanges are international, yet language continues
to be a persistent barrier to scientific communication, particularly for
non-native English-speaking scholars. Since the ability to absorb knowledge has
a strong impact on how researchers create new scientific knowledge, a
comprehensive access to and understanding of both domestic and international
scientific publications is essential for scientific performance. This study
explores the effect of absorbed knowledge on research impact by analyzing the
relationship between the language diversity of cited references and the number
of citations received by the citing paper. Chinese social sciences are taken as
the research object, and the data, 950,302 papers published between 1998 and
2013 with 8,151,327 cited references, were collected from the Chinese Social
Sciences Citation Index. Results show that there is a stark increase in the
consumption of foreign language material within the Chinese social science
community, and English material accounts for the vast majority of this
consumption. Papers with foreign language references receive significantly more
citations than those without, and the citation advantage of these
internationalized work holds when we control for characteristics of the citing
papers. However, the citation advantage has decreased from 1998 to 2008,
largely as an artifact of the increased number of papers citing foreign
language material. After 2008, the decline of the citation advantage subsided
and became relatively stable, which suggests that incorporating foreign
language literature continues to increase scientific impact, even as the
scientific community itself becomes increasingly international. However,
internationalization is not without concerns: the work closes with a discussion
of the benefits and potential problems of the lack of linguistic diversity in
scientific communication.
"
2667,"Replication of the Keyword Extraction part of the paper ""'Without the
  Clutter of Unimportant Words': Descriptive Keyphrases for Text Visualization""","  ""Keyword Extraction"" refers to the task of automatically identifying the most
relevant and informative phrases in natural language text. As we are deluged
with large amounts of text data in many different forms and content - emails,
blogs, tweets, Facebook posts, academic papers, news articles - the task of
""making sense"" of all this text by somehow summarizing them into a coherent
structure assumes paramount importance. Keyword extraction - a well-established
problem in Natural Language Processing - can help us here. In this report, we
construct and test three different hypotheses (all related to the task of
keyword extraction) that take us one step closer to understanding how to
meaningfully identify and extract ""descriptive"" keyphrases. The work reported
here was done as part of replicating the study by Chuang et al. [3].
"
2668,"Two Decades of Network Science as seen through the co-authorship network
  of network scientists","  Complex networks have attracted a great deal of research interest in the last
two decades since Watts & Strogatz, Barab\'asi & Albert and Girvan & Newman
published their highly-cited seminal papers on small-world networks, on
scale-free networks and on the community structure of complex networks,
respectively. These fundamental papers initiated a new era of research
establishing an interdisciplinary field called network science. Due to the
multidisciplinary nature of the field, a diverse but not divided network
science community has emerged in the past 20 years. This paper honors the
contributions of network science by exploring the evolution of this community
as seen through the growing co-authorship network of network scientists (here
the notion refers to a scholar with at least one paper citing at least one of
the three aforementioned milestone papers). After investigating various
characteristics of 29,528 network science papers, we construct the
co-authorship network of 52,406 network scientists and we analyze its topology
and dynamics. We shed light on the collaboration patterns of the last 20 years
of network science by investigating numerous structural properties of the
co-authorship network and by using enhanced data visualization techniques. We
also identify the most central authors, the largest communities, investigate
the spatiotemporal changes, and compare the properties of the network to
scientometric indicators.
"
2669,"Delivering Scientific Influence Analysis as a Service on Research Grants
  Repository","  Research grants have played an important role in seeding and promoting
fundamental research projects worldwide. There is a growing demand for
developing and delivering scientific influence analysis as a service on
research grant repositories. Such analysis can provide insight on how research
grants help foster new research collaborations, encourage cross-organizational
collaborations, influence new research trends, and identify technical
leadership. This paper presents the design and development of a grants-based
scientific influence analysis service, coined as GImpact. It takes a
graph-theoretic approach to design and develop large scale scientific influence
analysis over a large research-grant repository with three original
contributions. First, we mine the grant database to identify and extract
important features for grants influence analysis and represent such features
using graph theoretic models. For example, we extract an institution graph and
multiple associated aspect-based collaboration graphs, including a discipline
graph and a keyword graph. Second, we introduce self-influence and co-influence
algorithms to compute two types of collaboration relationship scores based on
the number of grants and the types of grants for institutions. We compute the
self-influence scores to reflect the grant based research collaborations among
institutions and compute multiple co-influence scores to model the various
types of cross-institution collaboration relationships in terms of disciplines
and subject areas. Third, we compute the overall scientific influence score for
every pair of institutions by introducing a weighted sum of the self-influence
score and the multiple co-influence scores and conduct an influence-based
clustering analysis. We evaluate GImpact using a real grant database,
consisting of 2512 institutions and their grants received over a period of 14
years...
"
2670,"Ten years of research on ResearchGate, a scoping review using Google
  Scholar 2008_2017","  Objective. To analyse quantitatively the articles published during 2008_2017
about the academic social networking site ResearchGate. Methods. A scoping
bibliometric review of documents retrieved using Google Scholar was conducted,
limited to publications that contained the word ""ResearchGate"" in their title
and were published from 2008 to 2017. Results. The search yielded 159
documents, once a preliminary list of 386 documents retrieved from Google
Scholar was filtered, which eliminated about 60% of the results that were
bibliographic citations and not documents. Papers in journals were the most
numerous type of documents (n73; 46%), followed by conference papers (n_31;
19.5 %). Contributing eight publications, two Spanish scholars (Delgado
Lopez-Cozar and Orduna Malea, who were coauthors in each case) were the most
prolific authors writing on this topic during the ten-year period. The keywords
most used in the documents were ""ResearchGate"" and ""Altmetrics"". The
publications were cited frequently since 2014 (more than 90% of the total cites
fell in that period), and those with more than one author were the most cited
ones. The authors of the documents were mainly librarians and information
science professionals, who wrote primarily as co-authors with colleagues from
their own institutions, mostly published in English. Conclusions. Interest in
ResearchGate has grown since 2015, as evident from the number of articles
published and the citations they received. Keywords. Academic social networks,
bibliometrics, scholarly communication
"
2671,"Intellectual and social similarity among scholarly journals: an
  exploratory comparison of the networks of editors, authors and co-citations","  This paper explores, by using suitable quantitative techniques, to what
extent the intellectual proximity among scholarly journals is also a proximity
in terms of social communities gathered around the journals. Three fields are
considered: statistics, economics and information and library sciences.
Co-citation networks (CC) represent the intellectual proximity among journals.
The academic communities around the journals are represented by considering the
networks of journals generated by authors writing in more than one journal
(interlocking authorship: IA), and the networks generated by scholars sitting
in the editorial board of more than one journal (interlocking editorship: IE).
For comparing the whole structure of the networks, the dissimilarity matrices
are considered. The CC, IE and IA networks appear to be correlated for the
three fields. The strongest correlations is between CC and IA for the three
fields. Lower and similar correlations are obtained for CC and IE, and for IE
and IA. The CC, IE and IA networks are then partitioned in communities.
Information and library sciences is the field where communities are more easily
detectable, while the most difficult field is economics. The degrees of
association among the detected communities show that they are not independent.
For all the fields, the strongest association is between CC and IA networks;
the minimum level of association is between IE and CC. Overall, these results
indicate that the intellectual proximity is also a proximity among authors and
among editors of the journals. Thus, the three maps of editorial power,
intellectual proximity and authors communities tell similar stories.
"
2672,"Annals of Library and Information Studies. A bibliometric analysis of
  the journal and a comparison with the top library and information studies
  journals in Asia and worldwide (2011_2017)","  This paper presents a thorough bibliometric analysis of research published in
Annals of Library and Information Studies (ALIS), an India-based journal, for
the period 2011_2017. Specifically, it compares this journal's trends with
those of other library and information science (LIS) journals from the same
geographical area (India, and Asia as a whole) and with the 10 highest-rated
LIS journals worldwide. The source of the data used was the multidisciplinary
database Scopus. To perform this comparison, ALIS' production was analyzed in
order to identify authorship patterns; for example, authors' countries of
residence, co-authorship trends, and collaboration networks. Research topics
were identified through keyword analysis, while performance was measured by
examining the number of citations articles received. This study provides
substantial information. The research lines detected through examining the
keywords in ALIS articles were determined to be similar to those for the top
LIS journals in both Asia and worldwide. Specifically, ALIS authors are
focusing on metrics, bibliometrics, and social networking, which follows global
trends. Notably, however, collaboration among Asia-based journals was found to
be lower than that in the top-indexed journals in the LIS field. The results
obtained present a roadmap for expanding the research in this field.
"
2673,Time evolution of the hierarchical networks between PubMed MeSH terms,"  Hierarchical organisation is a prevalent feature of many complex networks
appearing in nature and society. A relating interesting, yet less studied
question is how does a hierarchical network evolve over time? Here we take a
data driven approach and examine the time evolution of the network between the
Medical Subject Headings (MeSH) provided by the National Center for
Biotechnology Information (NCBI, part of the U. S. National Library of
Medicine). The network between the MeSH terms is organised into 16 different,
yearly updated hierarchies such as ""Anatomy"", ""Diseases"", ""Chemicals and
Drugs"", etc. The natural representation of these hierarchies is given by
directed acyclic graphs, composed of links pointing from nodes higher in the
hierarchy towards nodes in lower levels. Due to the yearly updates, the
structure of these networks is subject to constant evolution: new MeSH terms
can appear, terms becoming obsolete can be deleted or be merged with other
terms, and also already existing parts of the network may be rewired. We
examine various statistical properties of the time evolution, with a special
focus on the attachment and detachment mechanisms of the links, and find a few
general features that are characteristic for all MeSH hierarchies. According to
the results, the hierarchies investigated display an interesting interplay
between non-uniform preference with respect to multiple different topological
and hierarchical properties.
"
2674,Analysis on MathSciNet database: some preliminary results,"  In this paper we initiate some investigations on MathSciNet database. For
many mathematicians this website is used on a regular basis, but surprisingly
except for the information provided by MathSciNet itself, there exist almost no
independent investigations or independent statistics on this database. This
current research has been triggered by a rumor: do international collaborations
increase the number of citations of an academic work in mathematics? We use
MathSciNet for providing some information about this rumor, and more generally
pave the way for further investigations on or with MathSciNet. Keywords:
MathSciNet, tree-based methods, international collaborations
"
2675,A Longitudinal Analysis of University Rankings,"  Pressured by globalization and the increasing demand for public organisations
to be accountable, efficient and transparent, university rankings have become
an important tool for assessing the quality of higher education institutions.
It is therefore important to carefully assess exactly what these rankings
measure. In this paper, the three major global university rankings, The
Academic Ranking of World Universities, The Times Higher Education and the
Quacquarelli Symonds World University Rankings, are studied. After a
description of the ranking methodologies, it is shown that university rankings
are stable over time but that there is variation between the three rankings.
Furthermore, using Principal Component Analysis and Exploratory Factor
Analysis, we show that the variables used to construct the rankings primarily
measure two underlying factors: a universities reputation and its research
performance. By correlating these factors and plotting regional aggregates of
universities on the two factors, differences between the rankings are made
visible. Last, we elaborate how the results from these analysis can be viewed
in light of often voiced critiques of the ranking process. This indicates that
the variables used by the rankings might not capture the concepts they claim to
measure. Doing so the study provides evidence of the ambiguous nature of
university ranking's quantification of university performance.
"
2676,"The Rise and Fall of the Note: Changing Paper Lengths in ACM CSCW,
  2000-2018","  In this note, I quantitatively examine various trends in the lengths of
published papers in ACM CSCW from 2000-2018, focusing on several major
transitions in editorial and reviewing policy. The focus is on the rise and
fall of the 4-page note, which was introduced in 2004 as a separate submission
type to the 10-page double-column ""full paper"" format. From 2004-2012, 4-page
notes of 2,500 to 4,500 words consistently represented about 20-35\% of all
publications. In 2013, minimum and maximum page lengths were officially
removed, with no formal distinction made between full papers and notes. The
note soon completely disappeared as a distinct genre, which co-occurred with a
trend in steadily rising paper lengths. I discuss such findings both as they
directly relate to local concerns in CSCW and in the context of longstanding
theoretical discussions around genre theory and how socio-technical structures
and affordances impact participation in distributed, computer-mediated
organizations and user-generated content platforms. There are many possible
explanations for the decline of the note and the emergence of longer and longer
papers, which I identify for future work. I conclude by addressing the
implications of such findings for the CSCW community, particularly given how
genre norms impact what kinds of scholarship and scholars thrive in CSCW, as
well as whether new top-down rules or bottom-up guidelines ought to be
developed around paper lengths and different kinds of contributions.
"
2677,Scientific Statement Classification over arXiv.org,"  We introduce a new classification task for scientific statements and release
a large-scale dataset for supervised learning. Our resource is derived from a
machine-readable representation of the arXiv.org collection of preprint
articles. We explore fifty author-annotated categories and empirically motivate
a task design of grouping 10.5 million annotated paragraphs into thirteen
classes. We demonstrate that the task setup aligns with known success rates
from the state of the art, peaking at a 0.91 F1-score via a BiLSTM
encoder-decoder model. Additionally, we introduce a lexeme serialization for
mathematical formulas, and observe that context-aware models could improve when
also trained on the symbolic modality. Finally, we discuss the limitations of
both data and task design, and outline potential directions towards
increasingly complex models of scientific discourse, beyond isolated
statements.
"
2678,"Going beneath the shoulders of giants: tracking the cumulative knowledge
  spreading in a comprehensive citation network","  In all of science, the authors of publications depend on the knowledge
presented by the previous publications. Thus they ""stand on the shoulders of
giants"" and there is a flow of knowledge from previous publications to more
recent ones. The dominating paradigm for tracking this flow of knowledge is to
count the number of direct citations, but this neglects the fact that beneath
the first layer of citations there is a full body of literature. In this study,
we go underneath the ""shoulders"" by investigating the cumulative knowledge
creation process in a citation network of around 35 million publications. In
particular, we study stylized models of persistent influence and diffusion that
take into account all the possible chains of citations. When we study the
persistent influence values of publications and their citation counts, we find
that the publications related to Nobel Prizes i.e. Nobel papers have higher
ranks in terms of persistent influence than that due to citations, and that the
most outperforming publications are typically early works leading to hot
research topics of their time. The diffusion model reveals a significant
variation in the rates at which different fields of research share knowledge.
We find that these rates have been increasing systematically for several
decades, which can be explained by the increase in the publication volumes.
Overall, our results suggest that analyzing cumulative knowledge creation on a
global scale can be useful in estimating the type and scale of scientific
influence of individual publications and entire research areas as well as
yielding insights which could not be discovered by using only the direct
citation counts.
"
2679,"Wikidata from a Research Perspective -- A Systematic Mapping Study of
  Wikidata","  Wikidata is one of the most edited knowledge bases which contains structured
data. It serves as the data source for many projects in the Wikimedia sphere
and beyond. Since its inception in October 2012, it has been increasingly
growing in term of both its community and its content. This growth is reflected
by an expanding number of research focusing on Wikidata. Our study aims to
provide a general overview of the research performed on Wikidata through a
systematic mapping study in order to identify the current topical coverage of
existing research as well as the white spots which need further investigation.
In this study, 67 peer-reviewed research from journals and conference
proceedings were selected, and classified into meaningful categories. We
describe this data set descriptively by showing the publication frequency, the
publication venue and the origin of the authors and reveal current research
focuses. These especially include aspects concerning data quality, including
questions related to language coverage and data integrity. These results
indicate a number of future research directions, such as, multilingualism and
overcoming language gaps, the impact of plurality on the quality of Wikidata's
data, Wikidata's potential in various disciplines, and usability of user
interface.
"
2680,Evolution of interdependent co-authorship and citation networks,"  Studies of bibliographic data suggest a strong correlation between the growth
of citation networks and their corresponding co-authorship networks. We explore
the interdependence between evolving citation and co-authorship networks
focused on the publications, by Indian authors, in American Physical Society
journals between 1970 and 2013. We record interactions between each possible
pair of authors in two ways: first, by tracing the change in citations they
exchanged and, second, by tracing the shortest path between authors in the
co-authorship network. We create these data for every year of the period of our
analysis. We use probability methods to quantify the correlation between
citations and shortest paths, and the effect on the dynamics of the
citation-co-authorship system. We find that author pairs who have a
co-authorship distance $d \leq 3$ significantly affect each others citations,
but that this effect falls off rapidly for longer distances in the
co-authorship network. The exchange of citation between pairs with $d=1$
exhibits a sudden increase at the time of first co-authorship events and decays
thereafter, indicating an aging effect in collaboration. This suggests that the
dynamics of the co-authorship network appear to be driving those of the
citation network rather than vice versa. Moreover, the majority of citations
received by most authors are due to reciprocal citations from current, or past,
co-authors. We conclude that, in order to answer questions on nature and
dynamics of scientific collaboration, it is necessary to study both
co-authorship and citation network simultaneously.
"
2681,Lost or found? Discovering data needed for research,"  Finding data is a necessary precursor to being able to reuse data, although
relatively little large-scale empirical evidence exists about how researchers
discover, make sense of and (re)use data for research. This study presents
evidence from the largest known survey investigating how researchers discover
and use data that they do not create themselves. We examine the data needs and
discovery strategies of respondents, propose a typology for data reuse and
probe the role of social interactions and literature search in data discovery.
We consider how data communities can be conceptualized according to data uses
and propose practical applications of our findings for designers of data
discovery systems and repositories. Specifically, we consider how to design for
a diversity of practices, how communities of use can serve as an entry point
for design and the role of metadata in supporting both sensemaking and social
interactions.
"
2682,"How much research shared on Facebook happens outside of public pages and
  groups? A comparison of public and private online activity around PLOS ONE
  papers","  Despite its undisputed position as the biggest social media platform,
Facebook has never entered the main stage of altmetrics research. In this
study, we argue that the lack of attention by altmetrics researchers is due, in
part, to the challenges in collecting Facebook data regarding activity that
takes place outside of public pages and groups. We present a new method of
collecting aggregate counts of shares, reactions, and comments across the
platform-including users' personal timelines-and use it to gather data for all
articles published between 2015 to 2017 in the journal PLOS ONE. We compare the
gathered data with altmetrics collected and aggregated by Altmetric. The
results show that 58.7% of papers shared on Facebook happen outside of public
spaces and that, when collecting all shares, the volume of activity
approximates patterns of engagement previously only observed for Twitter. Both
results suggest that the role and impact of Facebook as a medium for science
and scholarly communication has been underestimated. Furthermore, they
emphasise the importance of openness and transparency around the collection and
aggregation of altmetrics.
"
2683,"Author Growth Outstrips Publication Growth in Computer Science and
  Publication Quality Correlates with Collaboration","  Although the computer science community successfully harnessed exponential
increases in computer performance to drive societal and economic change, the
exponential growth in publications is proving harder to accommodate. To gain a
deeper understanding of publication growth and inform how the computer science
community should handle this growth, we analyzed publication practices from
several perspectives: ACM sponsored publications in the ACM Digital Library as
a whole: subdisciplines captured by ACM's Special Interest Groups (SIGs); ten
top conferences; institutions; four top U.S. departments; authors; faculty; and
PhDs between 1990 and 2012. ACM publishes a large fraction of all computer
science research. We first summarize how we believe our main findings inform
(1) expectations on publication growth, (2) how to distinguish research quality
from output quantity; and (3) the evaluation of individual researchers. We then
further motivate the study of computer science publication practices and
describe our methodology and results in detail.
"
2684,"Common Library 1.0: A Corpus of Victorian Novels Reflecting the
  Population in Terms of Publication Year and Author Gender","  Research in 19th-century book history, sociology of literature, and
quantitative literary history is blocked by the absence of a collection of
novels which captures the diversity of literary production. We introduce a
corpus of 75 Victorian novels sampled from a 15,322-record bibliography of
novels published between 1837 and 1901 in the British Isles. This corpus, the
Common Library, is distinctive in the following way: the shares of novels in
the corpus associated with sociologically important subgroups match the shares
in the broader population. For example, the proportion of novels written by
women in 1880s in the corpus is approximately the same as in the population.
Although we do not, in this particular paper, claim that the corpus is a
representative sample in the familiar sense--a sample is representative if
""characteristics of interest in the population can be estimated from the sample
with a known degree of accuracy"" (Lohr 2010, p. 3)--we are confident that the
corpus will be useful to researchers. This is because existing
corpora--frequently convenience samples--are conspicuously misaligned with the
population of published novels. They tend to over-represent novels published in
specific periods and novels by men. The Common Library may be used alongside or
in place of these non-representative convenience corpora.
"
2685,"Mendeley Reader Counts for US Computer Science Conference Papers and
  Journal articles","  Although bibliometrics are normally applied to journal articles when used to
support research evaluations, conference papers are at least as important in
fast-moving computing-related fields. It is therefore important to assess the
relative advantages of citations and altmetrics for computing conference papers
to make an informed decision about which, if any, to use. This paper compares
Scopus citations with Mendeley reader counts for conference papers and journal
articles that were published between 1996 and 2018 in 11 computing fields and
had at least one US author. The data showed high correlations between Scopus
citation counts and Mendeley reader counts in all fields and most years, but
with few Mendeley readers for older conference papers and few Scopus citations
for new conference papers and journal articles. The results therefore suggest
that Mendeley reader counts have a substantial advantage over citation counts
for recently-published conference papers due to their greater speed, but are
unsuitable for older conference papers.
"
2686,How much research output from India gets social media attention?,"  Scholarly articles are now increasingly being mentioned and discussed in
social media platforms, sometimes even as pre- or post-print version uploads.
Measures of social media mentions and coverage are now emerging as an
alternative indicator of impact of scholarly articles. This article aims to
explore how much scholarly research output from India is covered in different
social media platforms, and how similar or different it is from the world
average. It also analyses the discipline-wise variations in coverage and
altmetric attention for Indian research output, including a comparison with the
world average. Results obtained show interesting patterns. Only 28.5% of the
total research output from India is covered in social media platforms, which is
about 18% less than the world average. ResearchGate and Mendeley are the most
popular social media platforms in India for scholarly article coverage. In
terms of discipline-wise variation, medical sciences and biological sciences
have relatively higher coverage across different platforms compared to
disciplines like information science and engineering.
"
2687,"The Memento Tracer Framework: Balancing Quality and Scalability for Web
  Archiving","  Web archiving frameworks are commonly assessed by the quality of their
archival records and by their ability to operate at scale. The ubiquity of
dynamic web content poses a significant challenge for crawler-based solutions
such as the Internet Archive that are optimized for scale. Human driven
services such as the Webrecorder tool provide high-quality archival captures
but are not optimized to operate at scale. We introduce the Memento Tracer
framework that aims to balance archival quality and scalability. We outline its
concept and architecture and evaluate its archival quality and operation at
scale. Our findings indicate quality is on par or better compared against
established archiving frameworks and operation at scale comes with a manageable
overhead.
"
2688,"Making sense of global collaboration dynamics: Developing a
  methodological framework to study (dis)similarities between country
  disciplinary profiles and choice of collaboration partners","  This paper presents a novel methodological framework by which the effects of
globalization on international collaboration can be studied and understood.
Using the cosine similarity of the disciplinary and partner profiles of
countries by collaboration types it is possible to analyse the effects of
globalization and the costs and benefits of an increasing global networked
research system.
"
2689,"Democracy, Complexity, and Science: Exploring Structural Sources of
  National Scientific Performance","  Scholars have long hypothesized that democratic forms of government are more
compatible with scientific advancement. However, empirical analysis testing the
democracy-science compatibility hypothesis remains underdeveloped. This article
explores the effect of democratic governance on scientific performance using
panel data on 124 countries between 2007 and 2018. We find evidence supporting
the democracy-science hypothesis, with strongest effects of egalitarian
democracy on national scientific performance. Further, using both internal and
external measures of complexity, we estimate the effects of complexity as a
moderating factor between the democracy-science connection. The results show
differential main effects of economic complexity, globalization, and
international collaboration on scientific performance, as well as significant
interaction effects that moderate the effect of democracy on scientific
performance. The findings show the significance of democratic governance and
complex systems in national scientific performance.
"
2690,Is culture related to strong science? An empirical investigation,"  National culture is among those societal factors which could influence
research and innovation activities. In this study, we investigated the
associations of two national culture models with citation impact of nations
(measured by the proportion of papers belonging to the 10% and 1% most cited
papers in the corresponding fields, PPtop 10% and PPtop 1%). Bivariate
statistical analyses showed that of six Hofstede's national culture dimensions
(HNCD), three dimensions of power distance, individualism, and uncertainty
avoidance had statistically significant associations with citation impact of
nations. The study also revealed that of two Inglehart-Welzel cultural values
(IWCV), the value survival versus self-expression is statistically
significantly related to citation impact indicators. We additionally calculated
multiple regression analyses controlling for the possible effects of
confounding factors including international migrant stock, investments in
research and development, number of researchers, international co-authorships,
and national self-citations. The results revealed that the statistically
significant associations of HNCD with citation impact indicators disappeared.
But the statistically significant relationship between survivals versus
self-expression values and citation impact indicators remained stable even
after controlling for the confounding variables. Thus, the freedom of
expression and trust in society might contribute to better scholarly
communication systems, higher level of international collaborations, and
further quality research.
"
2691,"Report on the 8th International Workshop on Bibliometric-enhanced
  Information Retrieval (BIR 2019)","  The Bibliometric-enhanced Information Retrieval workshop series (BIR) at ECIR
tackled issues related to academic search, at the crossroads between
Information Retrieval and Bibliometrics. BIR is a hot topic investigated by
both academia (e.g., ArnetMiner, CiteSeerx, DocEar) and the industry (e.g.,
Google Scholar, Microsoft Academic Search, Semantic Scholar). This report
presents the 8th iteration of the one-day BIR workshop held at ECIR 2019 in
Cologne, Germany.
"
2692,"Diversification versus specialization in scientific research: which
  strategy pays off?","  The current work addresses a theme previously unexplored in the literature:
that of whether the results arising from research activity in fields other than
the scientist's pri-mary field have greater value than the others.
Operationally, the authors proceed by identifying: the scientific production of
each researcher under observation; field classifi-cation of the publications;
the field containing the greatest number of the researcher's publications;
attribution of value of each publication. The results show that
diversifica-tion at the aggregate level does not pay off, although there are
some exceptions at the level of individual disciplines. The implications at
policy level are notable. Since the in-centive systems of research
organizations are based on the impact of scientific output, the scientists
concerned could resist engaging in multidisciplinary projects.
"
2693,"Peer review vs bibliometrics: which method better predicts the scholarly
  impact of publications?","  In this work, we try to answer the question of which method, peer review vs
bibliometrics, better predicts the future overall scholarly impact of
scientific publications. We measure the agreement between peer review
evaluations of Web of Science indexed publications submitted to the first
Italian research assessment exercise and long-term citations of the same
publications. We do the same for an early citation-based indicator. We find
that the latter shows stronger predictive power, i.e., it more reliably
predicts late citations in all the disciplinary areas examined, and for any
citation time window starting one year after publication.
"
2694,"A nation's foreign and domestic professors: which have better research
  performance? (The Italian case)","  This work investigates the research performance of foreign faculty in the
Italian academic system. Incoming professors compose l'1% of total faculty
across the sciences, although with variations by discipline. Their scientific
performance measured over 2010-2014 is on average better than that of their
Italian colleagues: the greatest difference is for associate professors.
Psychology is the discipline with the greatest concentration of top foreign
scientists. However there are notable shares of unproductive foreign professors
or of those with mediocre performance. The findings stimulate reflection on
issues of national policy concerning attractiveness of the higher education
system to skilled people from abroad, given the ongoing heavy Italian brain
drain.
"
2695,"A Step Toward Quantifying Independently Reproducible Machine Learning
  Research","  What makes a paper independently reproducible? Debates on reproducibility
center around intuition or assumptions but lack empirical results. Our field
focuses on releasing code, which is important, but is not sufficient for
determining reproducibility. We take the first step toward a quantifiable
answer by manually attempting to implement 255 papers published from 1984 until
2017, recording features of each paper, and performing statistical analysis of
the results. For each paper, we did not look at the authors code, if released,
in order to prevent bias toward discrepancies between code and paper.
"
2696,Fast Search with Poor OCR,"  The indexing and searching of historical documents have garnered attention in
recent years due to massive digitization efforts of important collections
worldwide. Pure textual search in these corpora is a problem since optical
character recognition (OCR) is infamous for performing poorly on such
historical material, which often suffer from poor preservation. We propose a
novel text-based method for searching through noisy text. Our system represents
words as vectors, projects queries and candidates obtained from the OCR into a
common space, and ranks the candidates using a metric suited to
nearest-neighbor search. We demonstrate the practicality of our method on
typewritten German documents from the WWII era.
"
2697,"Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding
  Space","  The trends of open science have enabled several open scholarly datasets which
include millions of papers and authors. Managing, exploring, and utilizing such
large and complicated datasets effectively are challenging. In recent years,
the knowledge graph has emerged as a universal data format for representing
knowledge about heterogeneous entities and their relationships. The knowledge
graph can be modeled by knowledge graph embedding methods, which represent
entities and relations as embedding vectors in semantic space, then model the
interactions between these embedding vectors. However, the semantic structures
in the knowledge graph embedding space are not well-studied, thus knowledge
graph embedding methods are usually only used for knowledge graph completion
but not data representation and analysis. In this paper, we propose to analyze
these semantic structures based on the well-studied word embedding space and
use them to support data exploration. We also define the semantic queries,
which are algebraic operations between the embedding vectors in the knowledge
graph embedding space, to solve queries such as similarity and analogy between
the entities on the original datasets. We then design a general framework for
data exploration by semantic queries and discuss the solution to some
traditional scholarly data exploration tasks. We also propose some new
interesting tasks that can be solved based on the uncanny semantic structures
of the embedding space.
"
2698,Testing for universality of Mendeley readership distributions,"  Altmetrics promise useful support for assessing the impact of scientific
works, including beyond the scholarly community and with very limited citation
windows. Unfortunately, altmetrics scores are currently available only for
recent articles and cannot be used as covariates in predicting long term impact
of publications. However, the study of their statistical properties is a
subject of evident interest to scientometricians. Applying the same approaches
used in the literature to assess the universality of citation distributions,
the intention here is to test whether the universal distribution also holds for
Mendeley readerships. Results of the analysis carried out on a sample of
publications randomly extracted from the Web of Science confirm that
readerships seem to share similar shapes across fields and can be rescaled to a
common and universal form. Such rescaling results as not particularly effective
on the right tails. In other regions, rescaling causes a good collapse of field
specific distributions, even for very recent publications.
"
2699,The collaboration behavior of top scientists,"  The intention of this work is to analyze top scientists' collaboration
behavior at the ""international"", ""domestic extramural"" and ""intramural"" levels,
and compare it to that of their lesser performing colleagues. The field of
observation consists of the entire faculty of the Italian academic system, and
so the coauthorship of scientific publications by over 12,000 professors. The
broader aim is to improve understanding of the causal nexus between research
collaboration and performance. The analysis is thus longitudinal, over two
successive five-year periods. Results show a strong increase in the propensity
to collaborate at domestic level (both extramural and intramural), however this
is less for scientists who remain or become top, than it is for their
lower-performing colleagues. In contrast, the increase in international
collaboration behavior is greater for scientists who become or remain top than
it is for their peers. The increase in productivity by those who acquire top
scientist status is due precisely to the greater average impact of the
publications achieved in collaboration with foreign colleagues.
"
2700,Co-citations in context: disciplinary heterogeneity is relevant,"  Citation analysis of the scientific literature has been used to study and
define disciplinary boundaries, to trace the dissemination of knowledge, and to
estimate impact. Co-citation, the frequency with which pairs of publications
are cited, provides insight into how documents relate to each other and across
fields. Co-citation analysis has been used to characterize combinations of
prior work as conventional or innovative and to derive features of highly cited
publications. Given the organization of science into disciplines, a key
question is the sensitivity of such analyses to frame of reference. Our study
examines this question using semantically-themed citation networks. We observe
that trends reported to be true across the scientific literature do not hold
for focused citation networks, and we conclude that inferring novelty using
co-citation analysis and random graph models benefits from disciplinary
context.
"
2701,"Biblioth\`eque de la communaut\'e assomptionniste : saisie informatique
  et classement Dewey","  The Library of Saint Peter in Gallicantu has had an eventful history and
different phases of classification. It was constituted by the contribution of
various private libraries of Religious of the Holy Land. Its history is
intimately linked to the Assumptionist presence in Jerusalem. The
computerization work carried out from 2018 onwards made it possible to clarify
the classification framework based on Dewey's decimal classification and to use
the databases - Wikidata, VIAF - to improve the BNF catalogue.
"
2702,Understanding the Information needs of Social Scientists in Germany,"  The information needs of social science researchers are manifold and almost
studied in every decade since the 1950s. With this paper, we contribute to this
series and present the results of three studies. We asked 367 social science
researchers in Germany for their information needs and identified needs in
different categories: literature, research data, measurement instruments,
support for data analysis, support for data collection, variables in research
data, software support, networking/cooperation, and illustrative material.
Thereby, the search for literature and research data is still the main
information need with more than three-quarter of our participants expressing
needs in these categories. With comprehensive lists of altogether 154 concrete
information needs, even those that are only expressed by one participant, we
contribute to the holistic understanding of the information needs of social
science researchers of today.
"
2703,"Predicting long-term publication impact through a combination of early
  citations and journal impact factor","  The ability to predict the long-term impact of a scientific article soon
after its publication is of great value towards accurate assessment of research
performance. In this work we test the hypothesis that good predictions of
long-term citation counts can be obtained through a combination of a
publication's early citations and the impact factor of the hosting journal. The
test is performed on a corpus of 123,128 WoS publications authored by Italian
scientists, using linear regression models. The average accuracy of the
prediction is good for citation time windows above two years, decreases for
lowly-cited publications, and varies across disciplines. As expected, the role
of the impact factor in the combination becomes negligible after only two years
from publication.
"
2704,The balance of knowledge flows,"  In analogy to the technology balance of payments, in this paper we propose a
possible way to set up a ""balance of knowledge flows"" (BKF), recording world
flows of knowledge within the scientific community. Adopting a pure
bibliometric approach, the ""knowledge"" traced in the BKF is that produced and
exchanged by the scientific community by means of publications and relevant
citations. A description of the theoretical foundation of such a tool is
presented together with its empirical testing over the scientific production of
four different countries. The BKF can be part of yearly reports of science and
technology indicators, aimed at informing research policy.
"
2705,Does Monetary Support Increase Citation Impact of Scholarly Papers?,"  One of the main indicators of scientific development of a given country is
the number of papers published in high impact scholarly journals. Many
countries introduced performance-based research funding systems (PRFSs) to
create a more competitive environment where prolific researchers get rewarded
with subsidies to increase both the quantity and quality of papers. Yet,
subsidies do not always function as a leverage to improve the citation impact
of scholarly papers. This paper investigates the effect of the publication
support system of Turkey (TR) on the citation impact of papers authored by
Turkish researchers. Based on a stratified probabilistic sample of 4,521
TR-addressed papers, it compares the number of citations to determine if
supported papers were cited more often than those of not supported ones, and if
they were published in journals with relatively higher citation impact in terms
of journal impact factors, article influence scores and quartiles. Both
supported and not supported papers received comparable number of citations per
paper, and were published in journals with similar citation impact values.
Findings suggest that subsidies do not seem to be an effective incentive to
improve the quality of scholarly papers. Such support programs should therefore
be reconsidered.
"
2706,"The Golden Eras of Graphene Science and Technology: Bibliographic
  Evidences From Journal and Patent Publications","  Today's scientific research is an expensive enterprise funded largely by
taxpayers' and corporate groups' monies. It is a critical part in the
competition between nations, and all nations want to discover fields of
research that promise to create future industries, and dominate these by
building up scientific and technological expertise early. However, our
understanding of the value chain going from science to technology is still in a
relatively infant stage, and the conversion of scientific leadership into
market dominance remains very much an alchemy rather than a science. In this
paper, we analyze bibliometric records of scientific journal publications and
patents related to graphene, at the aggregate level as well as on the temporal
and spatial dimensions. We find the present leaders of graphene science and
technology emerged rather late in the race, after the initial scientific
leaders lost their footings. More importantly, notwithstanding the amount of
funding already committed, we find evidences that suggest the 'Golden Eras' of
graphene science and technology were in 2010 and 2012 respectively, in spite of
the continued growth of journal and patent publications in this area.
"
2707,"The differing meanings of indicators under different policy contexts.
  The case of internationalisation","  In this chapter we build upon Moed's conceptual contributions on the
importance of the policy context when using and interpreting scientometric
indicators. We focus on the use of indicators in research evaluation regarding
internationalisation policies. The globalization of higher education presents
important challenges to institutions worldwide, which are confronted with
tensions derived from the need to respond both, to their local necessities and
demands while participating in global networks. In this context, indicators
have served as measures for monitoring and enforcing internationalisation
policies, in many cases interpreting them regardless of the policy context in
which they are enforced. We will analyse three examples of indicators related
to internationalisation. The first one is about international collaborations,
under the assumption that a greater number of internationally co-authored
publications will benefit a national science system as it will result in higher
citation impact. The second one relates to the promotion of English language as
the dominant language of science. The third case analyses how different policy
contexts shape the selection and construction of indicators, sometimes in a
partial way which does not properly reflect the phenomenon under study. The
examples illustrate that the interpretation and policy implications of the
'same' S&T indicators differ depending on specific contexts.
"
2708,"A Digital Library for Research Data and Related Information in the
  Social Sciences","  In the social sciences, researchers search for information on the Web, but
this is most often distributed on different websites, search portals, digital
libraries, data archives, and databases. In this work, we present an integrated
search system for social science information that allows finding information
around research data in a single digital library. Users can search for research
data sets, publications, survey variables, questions from questionnaires,
survey instruments, and tools. Information items are linked to each other so
that users can see, for example, which publications contain data citations to
research data. The integration and linking of different kinds of information
increase their visibility so that it is easier for researchers to find
information for re-use. In a log-based usage study, we found that users search
across different information types, that search sessions contain a high rate of
positive signals and that link information is often explored.
"
2709,"Recognizing Topic Change in Search Sessions of Digital Libraries based
  on Thesaurus and Classification System","  Log analysis in Web search showed that user sessions often contain several
different topics. This means sessions need to be segmented into parts which
handle the same topic in order to give appropriate user support based on the
topic, and not on a mixture of topics. Different methods have been proposed to
segment a user session to different topics based on timeouts, lexical analysis,
query similarity or external knowledge sources. In this paper, we study the
problem in a digital library for the social sciences. We present a method based
on a thesaurus and a classification system which are typical knowledge
organization systems in digital libraries. Five experts evaluated our approach
and rated it as good for the segmentation of search sessions into parts that
treat the same topic.
"
2710,"How to use Software Heritage for archiving and referencing your source
  code: guidelines and walkthrough","  Software source code is an essential research output, and many research
communities strongly encourage making the source code of the artefact available
by archiving it in publicly-accessible long-term archives.Software Heritage is
a non profit, long term universal archive specifically designed for software
source code, and able to store not only a software artifact, but also its full
development history. It provides the ideal place to preserve research software
artifacts, and offers powerful mechanisms to enhance research articles with
precise references to relevant fragments of your source code.Using Software
Heritage for your research software artifacts is straightforward and involves
three simple steps. This document details each of these three steps, providing
guidelines for making the most out of Software Heritage for your research.
"
2711,Understanding the Twitter Usage of Science Citation Index (SCI) Journals,"  This paper investigates the Twitter interaction patterns of journals from the
Science Citation Index (SCI) of Master Journal List (MJL). A total of 953,253
tweets extracted from 857 journal accounts, were analyzed in this study.
Findings indicate that SCI journals interacted more with each other but much
less with journals from other citation indices. The network structure of the
communication graph resembled a tight crowd network, with Nature journals
playing a major part. Information sources such as news portals and scientific
organizations were mentioned more in tweets, than academic journal Twitter
accounts. Journals with high journal impact factors (JIFs) were found to be
prominent hubs in the communication graph. Differences were found between the
Twitter usage of SCI journals with Humanities and Social Sciences (HSS)
journals.
"
2712,Towards Key Performance Indicators of Research Infrastructures,"  In 2018, the European Strategic Forum for research infrastructures (ESFRI)
was tasked by the Competitiveness Council, a configuration of the Council of
the EU, to develop a common approach for monitoring of Research
Infrastructures' performance. To this end, ESFRI established a working group,
which has proposed 21 Key Performance Indicators (KPIs) to monitor the progress
of the Research Infrastructures (RIs) addressed towards their objectives. The
RIs were then asked to assess their relevance for their institution. The paper
aims to identify the relevance of certain indicators for particular groups of
RIs by using cluster and discriminant analysis. This could contribute to
development of a monitoring system, tailored to particular RIs.
  To obtain a typology of the RIs, we first performed cluster analysis of the
RIs according to their properties, which revealed clusters of RIs with similar
characteristics, based on to the domain of operation, such as food, environment
or engineering. Then, discriminant analysis was used to study how the relevance
of the KPIs differs among the obtained clusters. This analysis revealed that
the percentage of RIs correctly classified into five clusters, using the KPIs,
is 80%. Such a high percentage indicates that there are significant differences
in the relevance of certain indicators, depending on the ESFRI domain of the
RI. The indicators therefore need to be adapted to the type of infrastructure.
It is therefore proposed that the Strategic Working Groups of ESFRI addressing
specific domains should be involved in the tailored development of the
monitoring of pan-European RIs.
"
2713,"Academic collaboration rates and citation associations vary
  substantially between countries and fields","  Research collaboration is promoted by governments and research funders but if
the relative prevalence and merits of collaboration vary internationally
different national and disciplinary strategies may be needed to promote it.
This study compares the team size and field normalised citation impact of
research across all 27 Scopus broad fields in the ten countries with the most
journal articles indexed in Scopus 2008-2012. The results show that team size
varies substantially by discipline and country, with Japan (4.2) having two
thirds more authors per article than the UK (2.5). Solo authorship is rare in
China (4%) but common in the UK (27%). Whilst increasing team size associates
with higher citation impact in almost all countries and fields, this
association is much weaker in China than elsewhere. There are also field
differences in the association between citation impact and collaboration. For
example, larger team sizes in the Business, Management & Accounting category do
not seem to associate with greater research impact, and for China and India,
solo authorship associates with higher citation impact. Overall, there are
substantial international and field differences in the extent to which
researchers collaborate and the extent to which collaboration associates with
higher citation impact.
"
2714,Prediction of citation dynamics of individual papers,"  We apply stochastic model of citation dynamics of individual papers developed
in our previous work (M. Golosovsky and S. Solomon, Phys. Rev. E\textbf{ 95},
012324 (2017)) to forecast citation career of individual papers. We focus not
only on the estimate of the future citations of a paper but on the
probabilistic margins of such estimate as well.
"
2715,Citation network centrality: a scientific awards predictor?,"  The $K$-index is an easily computable centrality index in complex networks,
such as a scientific citations network. A researcher has a $K$-index equal to
$K$ if he or she is cited by $K$ articles that have at least $K$ citations. The
$K$-index has several advantages over Hirsh's $h$-index and, in previous
studies, has shown better correlation with Nobel prizes than any other index
given by the {\em Web of Science}, including the $h$-index. It is plausible
that researchers who are the most connected to other scientifically
well-connected researchers are the most likely to be doing important work and
more likely to be awarded major prizes in a given area. However, the
correlation found does not imply causation. Here we perform an experiment using
the $K$-index, producing a shortlist of twelve candidates for major scientific
prizes, including the Physics Nobel award, in the near future. For example, our
top-12 $K$-index list naturally selects the 2019 Nobel laureate, James Peebles.
The list can be updated annually and should be compared to laureates of the
following years
"
2716,"Predicting publication productivity for researchers: A latent variable
  model","  This study provided a model for the publication dynamics of researchers,
which is based on the relationship between the publication productivity of
researchers and two covariates: time and historical publication quantity. The
relationship allows to estimate the latent variable the publication creativity
of researchers. The variable is applied to the prediction of publication
productivity for researchers. The statistical significance of the relationship
is validated by the high quality dblp dataset. The effectiveness of the model
is testified on the dataset by the fine fittings on the quantitative
distribution of researchers' publications, the evolutionary trend of their
publication productivity, and the occurrence of publication events. Due to its
nature of regression, the model has the potential to be extended for assessing
the confidence level of prediction results, and thus has clear applicability to
empirical research.
"
2717,"The false myth of the rise in self-citations, and the impressively
  positive effect of bibliometric evaluations on the increase of the impact of
  Italian research","  It has recently been claimed by Baccini and coauthors that due to ANVUR's
bibliometric evaluations of individuals, departments, and universities, in
Italy there has been a surge in self-citations in the last ten years, thus
increasing the ""inwardness"" of Italian research more than has happened abroad.
We have studied the database of Ioannidis et al. published on 12 August 2019 of
the one hundred thousand most ""highly cited"" scientists, including about two
thousand Italians, and we found that the problem of self-citations in relation
to this scientific elite is not significant in Italy, while perhaps observing a
small deviation in the low scores in the rankings. The effect indicated by
Baccini et al. consequently, does not seem worrying for the scientific elite
(we quantified it in 2 percent of the total of scientists of the ""best"" one
hundred thousand), and is probably largely concentrated in the further less
cited scientists. Evaluation agencies like ANVUR should probably exclude
self-citations in future evaluations, for the noise introduced by the young
researchers. The overall state of health of the Italian research system and the
positive effect of the ANVUR assessments are demonstrated by the number of
Italian researchers in the top one hundred thousand, which has increased by
comparing the ""career"" databased of 22 years, with that of the ""young""
researchers in the ""2017"" database. Italy, looking at the elite researchers,
not only is not the most indulgent in self-citations, but has shown the best
improvements, proving that the introduction of ANVUR had a positive effect.
Indeed, all countries apart from Italy have suffered a decline, even
substantial (-20 percent on a national Japan scale), of the number of
researchers present in the 2017 data sets compared to career data. Italy
instead shows a +0.2 percent on a global basis and an impressive +11.53 percent
on a national basis.
"
2718,"Peer Reviewing Revisited: Assessing Research with Interlinked Semantic
  Comments","  Scientific publishing seems to be at a turning point. Its paradigm has stayed
basically the same for 300 years but is now challenged by the increasing volume
of articles that makes it very hard for scientists to stay up to date in their
respective fields. In fact, many have pointed out serious flaws of current
scientific publishing practices, including the lack of accuracy and efficiency
of the reviewing process. To address some of these problems, we apply here the
general principles of the Web and the Semantic Web to scientific publishing,
focusing on the reviewing process. We want to determine if a fine-grained model
of the scientific publishing workflow can help us make the reviewing processes
better organized and more accurate, by ensuring that review comments are
created with formal links and semantics from the start. Our contributions
include a novel model called Linkflows that allows for such detailed and
semantically rich representations of reviews and the reviewing processes. We
evaluate our approach on a manually curated dataset from several recent
Computer Science journals and conferences that come with open peer reviews. We
gathered ground-truth data by contacting the original reviewers and asking them
to categorize their own review comments according to our model. Comparing this
ground truth to answers provided by model experts, peers, and automated
techniques confirms that our approach of formally capturing the reviewers'
intentions from the start prevents substantial discrepancies compared to when
this information is later extracted from the plain-text comments. In general,
our analysis shows that our model is well understood and easy to apply, and it
revealed the semantic properties of such review comments.
"
2719,"Do we measure novelty when we analyze unusual combinations of cited
  references? A validation study of bibliometric novelty indicators based on
  F1000Prime data","  Lee, Walsh, and Wang (2015) - based on Uzzi, Mukherjee, Stringer, and Jones
(2013) - and Wang, Veugelers, and Stephan (2017) proposed scores based on cited
references (cited journals) data which can be used to measure the novelty of
papers (named as novelty scores U and W in this study). Although previous
research has used novelty scores in various empirical analyses, no study has
been published up to now - to the best of our knowledge - which quantitatively
tested the convergent validity of novelty scores: do these scores measure what
they propose to measure? Using novelty assessments by faculty members (FMs) at
F1000Prime for comparison, we tested the convergent validity of the two novelty
scores (U and W). FMs' assessments not only refer to the quality of biomedical
papers, but also to their characteristics (by assigning certain tags to the
papers): for example, are the presented findings or formulated hypotheses novel
(tags ""new findings"" and ""hypothesis"")? We used these and other tags to
investigate the convergent validity of both novelty scores. Our study reveals
different results for the novelty scores: the results for novelty score U are
mostly in agreement with previously formulated expectations. We found, for
instance, that for a standard deviation (one unit) increase in novelty score U,
the expected number of assignments of the ""new finding"" tag increase by 7.47%.
The results for novelty score W, however, do not reflect convergent validity
with the FMs' assessments: only the results for some tags are in agreement with
the expectations. Thus, we propose - based on our results - the use of novelty
score U for measuring novelty quantitatively, but question the use of novelty
score W.
"
2720,"The role of mainstreamness and interdisciplinarity for the relevance of
  scientific papers","  There is demand from science funders, industry, and the public that science
should become more risk-taking, more out-of-the-box, and more
interdisciplinary. Is it possible to tell how interdisciplinary and
out-of-the-box scientific papers are, or which papers are mainstream? Here we
use the bibliographic coupling network, derived from all physics papers that
were published in the Physical Review journals in the past century, to try to
identify them as mainstream, out-of-the-box, or interdisciplinary. We show that
the network clusters into scientific fields. The position of individual papers
with respect to these clusters allows us to estimate their degree of
mainstreamness or interdisciplinary. We show that over the past decades the
fraction of mainstream papers increases, the fraction of out-of-the-box
decreases, and the fraction of interdisciplinary papers remains constant.
Studying the rewards of papers, we find that in terms of absolute citations,
both, mainstream and interdisciplinary papers are rewarded. In the long run,
mainstream papers perform less than interdisciplinary ones in terms of citation
rates. We conclude that to avoid a trend towards mainstreamness a new incentive
scheme is necessary.
"
2721,"Library Catalog Analysis and Library Holdings Counts: origins,
  methodological issues and application to the field of Informetrics","  In 2009, Torres-Salinas & Moed proposed the use of library catalogs to
analyze the impact and dissemination of academic books in different ways.
Library Catalog Analysis (LCA) can be defined as the application of
bibliometric techniques to a set of online library catalogs in order to
describe quantitatively a scientific-scholarly field on the basis of published
book titles. The aim of the present chapter is to conduct an in-depth analysis
of major scientific contributions since the birth of LCA in order to determine
the state of the art of this research topic. Hence, our specific objectives
are: 1) to discuss the original purposes of library holdings 2) to present
correlations between library holdings and altmetrics indicators and interpret
their feasible meanings 3) to analyze the principal sources of information 4)
to use WorldCat Identities to identify the principal authors and works in the
field of Informetrics.
"
2722,Disciplinary Variations in Altmetric Coverage of Scholarly Articles,"  The popular social media platforms are now making it possible for scholarly
articles to be shared rapidly in different forms, which in turn can
significantly improve the visibility and reach of articles. Many authors are
now utilizing the social media platforms to disseminate their scholarly
articles (often as pre- or post- prints) beyond the paywalls of journals. It is
however not very well established if the level of social media coverage and
attention of scholarly articles is same across all research disciplines or
there exist discipline-wise variations. This paper aims to explore the
disciplinary variations in coverage and altmetric attention by analyzing a
significantly large amount of data from Web of Science and Altmetric.com.
Results obtained show interesting patterns. Medical Sciences and Biology are
found to account for more than 50% of all instances in Altmetrics. In terms of
coverage, disciplines like Biology, Medical Science and Multidisciplinary
Sciences have more than 60% of their articles covered in Altmetrics, whereas
disciplines like Engineering, Mathematics and Material Science have less than
25% of their articles covered in Altmetrics. The coverage percentages further
vary across different altmetric platforms, with Twitter and Mendeley having
much higher overall coverage than Facebook and News. Disciplinary variations in
coverage are also found in different altmetric platforms, with variations as
large as 7.5% for Engineering discipline to 55.7% for Multidisciplinary in
Twitter. The paper also looks into the possible role of source of publication
in altmetric coverage level of articles. Interestingly, some journals are found
to have a higher altmetric coverage in comparison to the average altmetric
coverage level of that discipline.
"
2723,The Natural Selection of Conservative Science,"  Social epistemologists have argued that high risk, high reward science has an
important role to play in scientific communities. Recently, though, it has also
been argued that various scientific fields seem to be trending towards
conservatism -- the increasing production of what Kuhn (1970) would have called
`normal science'. This paper will explore a possible explanation for this sort
of trend: that the process by which scientific research groups form, grow, and
dissolve might be inherently hostile to high risk science. In particular, I
employ a paradigm developed by Smaldino and McElreath (2016) that treats a
scientific community as a population undergoing selection. As will become
clear, perhaps counter-intuitively this sort of process in some ways promotes
high risk, high reward science. But, as I will point out, high risk high reward
science is, in general, the sort of thing that is hard to repeat. While more
conservative scientists will be able to train students capable of continuing
their successful projects, and so create thriving lineages, successful risky
science may not be the sort of thing one can easily pass on. In such cases, the
structure of scientific communities selects against high risk, high rewards
projects. More generally, this paper makes clear that there are at least two
processes to consider in thinking about how incentives shape scientific
communities -- the process by which individual scientists make choices about
their careers and research, and the selective process governing the formation
of new research groups.
"
2724,"Efficiency in managing peer-review of scientific manuscripts -- editors'
  perspective","  The purpose of this paper is to introduce a model for measuring the
efficiency in managing peer-review of scientific manuscripts by editors. The
approach employed is based on the assumption that the editorial aim is to
manage publication with high efficiency, employing the least amount of
editorial resources. Efficiency is defined in this research as a measure based
on 7 variables. An on-line survey was constructed and editors of journals
originating from Serbia regularly publishing articles in the field of chemistry
were invited to participate. An evaluation of the model is given based on
responses from 24 journals and 50 editors. With this investigation we aimed to
contribute to our understanding of the peer-review process and, possibly, offer
a tool to improve the ""efficiency"" in journal editing. The proposed protocol
may be adapted by other journals in order to assess the managing potential of
editors.
"
2725,"Correlations between submission and acceptance of papers in peer review
  journals","  This paper provides a comparative study about seasonal influence on editorial
decisions for papers submitted to two peer review journals. We distinguish a
specialized one, the Journal of the Serbian Chemical Society (JSCS) and an
interdisciplinary one, Entropy. Dates of electronic submission for about 600
papers to JSCS and 2500 to Entropy have been recorded over 3 recent years. Time
series of either accepted or rejected papers are subsequently analyzed. We take
either editors or authors view points into account, thereby considering
magnitudes and probabilities. In this sample, it is found that there are
distinguishable peaks and dips in the time series, demonstrating preferred
months for the submission of papers. It is also found that papers are more
likely accepted if they are submitted during a few specific months, - these
depending on the journal. The probability of having a rejected paper also
appears to be seasonally biased. In view of clarifying reports with
contradictory findings, we discuss previously proposed conjectures for such
effects, like holiday effects and the desk rejection by editors. We conclude
that, in this sample, the type of journal, specialized or multidisciplinary,
seems to be the drastic criterion for distinguishing the outcomes rates.
"
2726,"Seasonal Entropy, Diversity and Inequality Measures of Submitted and
  Accepted Papers Distributions In Peer-Reviewed Journals","  This paper presents a novel method for finding features in the analysis of
variable distributions stemming from time series. We apply the methodology to
the case of submitted and accepted papers in peer-reviewed journals. We provide
a comparative study of editorial decisions for papers submitted to two
peer-reviewed journals: the Journal of the Serbian Chemical Society (JSCS) and
this MDPI Entropy journal. We cover three recent years for which the fate of
submitted papers, about 600 papers to JSCS and 2500 to Entropy, is completely
determined. Instead of comparing the number distributions of these papers as a
function of time with respect to a uniform distribution, we analyze the
relevant probabilities, from which we derive the information entropy. It is
argued that such probabilities are indeed more relevant for authors than the
actual number of submissions. We tie this entropy analysis to the so called
diversity of the variable distributions. Furthermore, we emphasize the
correspondence between the entropy and the diversity with inequality measures,
like the Herfindahl-Hirschman index and the Theil index, itself being in the
class of entropy measures; the Gini coefficient which also measures the
diversity in ranking is calculated for further discussion. In this sample, the
seasonal aspects of the peer review process are outlined. It is found that the
use of such indices, non linear transformations of the data distributions,
allow to distinguish features and evolutions of peer review process as a
function of time as well as comparing non-uniformity of distributions.
Furthermore, t- and z- statistical tests are applied in order to measure the
significance (p-level) of the findings, i.e. whether papers are more likely to
be accepted if they are submitted during a few specific months or ""season""; the
predictability strength depends on the journal.
"
2727,"From Academia to Software Development: Publication Citations in Source
  Code Comments","  Academic publications have been evaluated in terms of their impact on
research communities based on many metrics, such as the number of citations. On
the other hand, the impact of academic publications on industry has been rarely
studied. This paper investigates how academic publications contribute to
software development by analyzing publication citations in source code comments
in open source software repositories. We propose an automated approach for
detecting academic publications based on Named Entity Recognition, and achieve
0.90 in $F_1$ as detection accuracy. We conduct a large-scale study of
publication citations with 319,438,977 comments collected from 25,925 active
repositories written in seven programming languages. Our findings indicate that
academic publications can be knowledge sources for software development. These
referenced publications are particularly from journals. In terms of knowledge
transfer, algorithm is the most prevalent type of knowledge transferred from
the publications, with proposed formulas or equations typically implemented in
methods or functions in source code files. In a closer look at GitHub
repositories referencing academic publications, we find that science-related
repositories are the most frequent among GitHub repositories with publication
citations, and that the vast majority of these publications are referenced by
repository owners who are different from the publication authors. We also find
that referencing older publications can lead to potential issues related to
obsolete knowledge.
"
2728,The Landscape of Academic Literature in Quantum Technologies,"  In this study, we investigated the academic literature on quantum
technologies (QT) using bibliometric tools. We used a set of 49,823 articles
obtained from the Web of Science (WoS) database using a search query
constructed through expert opinion. Analysis of this revealed that QT is deeply
rooted in physics, and the majority of the articles are published in physics
journals. Keyword analysis revealed that the literature could be clustered into
three distinct sets, which are (i) quantum communication/cryptography, (ii)
quantum computation, and (iii) physical realizations of quantum systems. We
performed a burst analysis that showed the emergence and fading away of certain
key concepts in the literature. This is followed by co-citation analysis on the
highly cited articles provided by the WoS, using these we devised a set of core
corpus of 34 publications. Comparing the most highly cited articles in this set
with respect to the initial set we found that there is a clear difference in
most cited subjects. Finally, we performed co-citation analyses on country and
organization levels to find the central nodes in the literature. Overall, the
analyses of the datasets allowed us to cluster the literature into three
distinct sets, construct the core corpus of the academic literature in QT, and
to identify the key players on country and organization levels, thus offering
insight into the current state of the field. Search queries and access to
figures are provided in the appendix.
"
2729,NLPExplorer: Exploring the Universe of NLP Papers,"  Understanding the current research trends, problems, and their innovative
solutions remains a bottleneck due to the ever-increasing volume of scientific
articles. In this paper, we propose NLPExplorer, a completely automatic portal
for indexing, searching, and visualizing Natural Language Processing (NLP)
research volume. NLPExplorer presents interesting insights from papers,
authors, venues, and topics. In contrast to previous topic modelling based
approaches, we manually curate five course-grained non-exclusive topical
categories namely Linguistic Target (Syntax, Discourse, etc.), Tasks (Tagging,
Summarization, etc.), Approaches (unsupervised, supervised, etc.), Languages
(English, Chinese,etc.) and Dataset types (news, clinical notes, etc.). Some of
the novel features include a list of young popular authors, popular URLs, and
datasets, a list of topically diverse papers and recent popular papers. Also,
it provides temporal statistics such as yearwise popularity of topics,
datasets, and seminal papers. To facilitate future research and system
development, we make all the processed datasets accessible through API calls.
The current system is available at http://nlpexplorer.org.
"
2730,"A Study of Annotation and Alignment Accuracy for Performance Comparison
  in Complex Orchestral Music","  Quantitative analysis of commonalities and differences between recorded music
performances is an increasingly common task in computational musicology. A
typical scenario involves manual annotation of different recordings of the same
piece along the time dimension, for comparative analysis of, e.g., the musical
tempo, or for mapping other performance-related information between
performances. This can be done by manually annotating one reference
performance, and then automatically synchronizing other performances, using
audio-to-audio alignment algorithms. In this paper we address several questions
related to those tasks. First, we analyze different annotations of the same
musical piece, quantifying timing deviations between the respective human
annotators. A statistical evaluation of the marker time stamps will provide (a)
an estimate of the expected timing precision of human annotations and (b) a
ground truth for subsequent automatic alignment experiments. We then carry out
a systematic evaluation of different audio features for audio-to-audio
alignment, quantifying the degree of alignment accuracy that can be achieved,
and relate this to the results from the annotation study.
"
2731,"Using Supervised Learning to Classify Metadata of Research Data by
  Discipline of Research","  Automated classification of metadata of research data by their discipline(s)
of research can be used in scientometric research, by repository service
providers, and in the context of research data aggregation services. Openly
available metadata of the DataCite index for research data were used to compile
a large training and evaluation set comprised of 609,524 records, which is
published alongside this paper. These data allow to reproducibly assess
classification approaches, such as tree-based models and neural networks.
According to our experiments with 20 base classes (multi-label classification),
multi-layer perceptron models perform best with a f1-macro score of 0.760
closely followed by Long Short-Term Memory models (f1-macro score of 0.755). A
possible application of the trained classification models is the quantitative
analysis of trends towards interdisciplinarity of digital scholarly output or
the characterization of growth patterns of research data, stratified by
discipline of research. Both applications perform at scale with the proposed
models which are available for re-use.
"
2732,Science and Technology Advance through Surprise,"  Breakthrough discoveries and inventions involve unexpected combinations of
contents including problems, methods, and natural entities, and also diverse
contexts such as journals, subfields, and conferences. Drawing on data from
tens of millions of research papers, patents, and researchers, we construct
models that predict next year's content and context combinations with an AUC of
95% based on embeddings constructed from high-dimensional stochastic block
models, where the improbability of new combinations itself predicts up to 50%
of the likelihood that they will gain outsized citations and major awards. Most
of these breakthroughs occur when problems in one field are unexpectedly solved
by researchers from a distant other. These findings demonstrate the critical
role of surprise in advance, and enable evaluation of scientific institutions
ranging from education and peer review to awards in supporting it.
"
2733,"Towards Automated Management and Analysis of Heterogeneous Data Within
  Cannabinoids Domain","  Cannabinoid research requires the cooperation of experts from various field
biochemistry and chemistry to psychological and social sciences. The data that
have to be managed and analysed are highly heterogeneous, especially because
they are provided by a very diverse range of sources. A number of approaches
focused on data collection and the corresponding analysis, restricting the
scope to a sub-domain. Our goal is to elaborate a solution that would allow for
automated management and analysis of heterogeneous data within the complete
cannabinoids domain. The corresponding integration of diverse data sources
would increase the quality and preciseness of the analysis. In this paper, we
introduce the core ideas of the proposed framework as well as present the
implemented prototype of a cannabinoids data platform.
"
2734,Legends of Nature and Editors-in-chief,"  This year marks the 150th celebration of Nature. However, the understanding
of the way the army of unsung editors-in-chief has strengthened and enriched
the integrity and quality of the journal under the umbrella of its original
mission remains nominal rather than substantial. This paper scrutinizes the
chief vehicle guided by Nature's doctrine with regard to the ways it has
conflicted with the advancement of both science and social progress. We first
recast quantitative spatiotemporal analysis on the diachronic discourse of
Nature since its debut, which promises to articulate the unfolding
chronological picture of Nature on a historical time scale, and pinpoint
overdue corrective to the strongly-held but flawed notions on editors-in-chief
of Nature. Our findings strongly indicate that the army of editors-in-chief
have never met with their fair share of identification, and they took on the
challenge guided by Nature's doctrine with extraordinary polymath, unparalleled
enthusiasm and diverse characters.
"
2735,Open Access -- Towards a non-normative and systematic understanding,"  The term Open Access not only describes a certain model of scholarly
publishing -- namely in digital format freely accessible to readers -- but
often also implies that free availability of research results is desirable, and
hence has a normative character. Together with the large variety of presently
used definitions of different Open Access types, this normativity hinders a
systematic investigation of the development of open availability of scholarly
literature. In this paper, we propose a non-normative definition of Open Access
and its usage as a neutral, descriptive term in bibliometric studies and
research on science. To this end, we first specify what normative figures are
commonly associated with the term Open Access and then develop a neutral
definition. We further identify distinguishing characteristics of openly
accessible literature, called dimensions, and derive a classification scheme
into Open Access categories based on these dimensions. Additionally, we present
an operationalisation method to assign scientific publications to the
respective categories in practice. Here, we describe useful data sources, which
can be employed to gather the information needed for the classification of
scholarly works according to the presented classification scheme.
"
2736,"Should citations be field-normalized in evaluative bibliometrics? An
  empirical analysis based on propensity score matching","  Field-normalization of citations is bibliometric standard. Despite the
observed differences in citation counts between fields, the question remains
how strong fields influence citation rates beyond the effect of attributes or
factors possibly influencing citations (FICs). We considered several FICs such
as number of pages and number of co-authors in this study. We wondered whether
there is a separate field-effect besides other effects (e.g., from numbers of
pages and co-authors). To find an answer on the question in this study, we
applied inverse-probability of treatment weighting (IPW). Using Web of Science
data (a sample of 308,231 articles), we investigated whether mean differences
among subject categories in citation rates still remain, even if the subject
categories are made comparable in the field-related attributes (e.g.,
comparable of co-authors, comparable number of pages) by IPW. In a diagnostic
step of our statistical analyses, we considered propensity scores as covariates
in regression analyses to examine whether the differences between the fields in
FICs vanish. The results revealed that the differences did not completely
vanish but were strongly reduced. We received similar results when we
calculated mean value differences of the fields after IPW representing the
causal or unconfounded field effects on citations. However, field differences
in citation rates remain. The results point out that field-normalization seems
to be a prerequisite for citation analysis and cannot be replaced by the
consideration of any set of FICs in citation analyses.
"
2737,"The effect of the Indonesian higher education evaluation system on
  conference proceedings publications","  Indonesia has exhibited a unique pattern of conference proceedings publishing
that was distinct from global and regional norms. Conference proceedings are
now an integral part of the major citation databases, and this study examined
their progressive coverage in the context of academic career advancement and
Indonesian government policy. The results of analyses in Web of Science (WoS),
Scopus and to a lesser extent, Dimensions showed an increase in the proportion
of conference proceedings far in excess of global norms and not seen in any
other South East Asian country. In the conference series containing most
Indonesian proceedings papers, there was a recent increase in representation
from Indonesia and an increase in the proportion of those conferences hosted in
Indonesia. Local policy documents and guidelines from 2012 and 2014 appeared to
encourage academics to increase research outputs in high impact internationally
indexed sources but did not make any distinction between document types. The
conclusion suggests that scholars might have found advantage in advancing
through the hierarchy of academia through publishing conference papers that
were quicker and easier to publish than journal articles. The study is
important to policy makers in the area of research evaluation because it
highlights potential changes in academic publishing behaviour by those being
assessed.
"
2738,Online Disinformation and the Role of Wikipedia,"  The aim of this study is to find key areas of research that can be useful to
fight against disinformation on Wikipedia. To address this problem we perform a
literature review trying to answer three main questions: (i) What is
disinformation? (ii) What are the most popular mechanisms to spread online
disinformation? and (iii) Which are the mechanisms that are currently being
used to fight against disinformation?. In all these three questions we take
first a general approach, considering studies from different areas such as
journalism and communications, sociology, philosophy, information and political
sciences. And comparing those studies with the current situation on the
Wikipedia ecosystem. We conclude that in order to keep Wikipedia as free as
possible from disinformation, it is necessary to help patrollers to early
detect disinformation and assess the credibility of external sources. More
research is needed to develop tools that use state-of-the-art machine learning
techniques to detect potentially dangerous content, empowering patrollers to
deal with attacks that are becoming more complex and sophisticated.
"
2739,"Targeted sampling from massive block model graphs with personalized
  PageRank","  The paper provides statistical theory and intuition for personalized PageRank
(called ""PPR""): a popular technique that samples a small community from a
massive network. We study a setting where the entire network is expensive to
obtain thoroughly or to maintain, but we can start from a seed node of interest
and ""crawl"" the network to find other nodes through their connections. By
crawling the graph in a designed way, the PPR vector can be approximated
without querying the entire massive graph, making it an alternative to snowball
sampling. Using the degree-corrected stochastic block model, we study whether
the PPR vector can select nodes that belong to the same block as the seed node.
We provide a simple and interpretable form for the PPR vector, highlighting its
biases towards high degree nodes outside the target block. We examine a simple
adjustment based on node degrees and establish consistency results for PPR
clustering that allows for directed graphs. These results are enabled by recent
technical advances showing the elementwise convergence of eigenvectors. We
illustrate the method with the massive Twitter friendship graph, which we crawl
by using the Twitter application programming interface. We find that the
adjusted and unadjusted PPR techniques are complementary approaches, where the
adjustment makes the results particularly localized around the seed node, and
that the bias adjustment greatly benefits from degree regularization.
"
2740,How Coupled are Mass Spectrometry and Capillary Electrophoresis?,"  The understanding of how science works can contribute to making scientific
development more effective. In this paper, we report an analysis of the
organization and interconnection between two important issues in chemistry,
namely mass spectrometry (MS) and capillary electrophoresis (CE). For that
purpose, we employed science of science techniques based on complex networks.
More specifically, we considered a citation network in which the nodes and
connections represent papers and citations, respectively. Interesting results
were found, including a good separation between some clusters of articles
devoted to instrumentation techniques and applications. However, the papers
that describe CE-MS did not lead to a well-defined cluster. In order to better
understand the organization of the citation network, we considered a
multi-scale analysis, in which we used the information regarding sub-clusters.
Firstly, we analyzed the sub-cluster of the first article devoted to the
coupling between CE and MS, which was found to be a good representation of its
sub-cluster. The second analysis was about the sub-cluster of a seminal paper
known to be the first that dealt with proteins by using CE-MS. By considering
the proposed methodologies, our paper paves the way for researchers working
with both techniques, since it elucidates the knowledge organization and can
therefore lead to better literature reviews.
"
2741,Towards a Predictive Patent Analytics and Evaluation Platform,"  The importance of patents is well recognised across many regions of the
world. Many patent mining systems have been proposed, but with limited
predictive capabilities. In this demo, we showcase how predictive algorithms
leveraging the state-of-the-art machine learning and deep learning techniques
can be used to improve understanding of patents for inventors, patent
evaluators, and business analysts alike. Our demo video is available at
http://ibm.biz/ecml2019-demo-patent-analytics
"
2742,"Practice meets Principle: Tracking Software and Data Citations to Zenodo
  DOIs","  Data and software citations are crucial for the transparency of research
results and for the transmission of credit. But they are hard to track, because
of the absence of a common citation standard. As a consequence, the FORCE11
recently proposed data and software citation principles as guidance for
authors. Zenodo is recognized for the implementation of DOIs for software on a
large scale. The minting of complementary DOIs for the version and concept
allows measuring the impact of dynamic software. This article investigates
characteristics of 5,456 citations to Zenodo data and software that were
captured by the Asclepias Broker in January 2019. We analyzed the current state
of data and software citation practices and the quality of software citation
recommendations with regard to the impact of recent standardization efforts.
Our findings prove that current citation practices and recommendations do not
match proposed citation standards. We consequently suggest practical first
steps towards the implementation of the software citation principles.
"
2743,"GRAPHENE: A Precise Biomedical Literature Retrieval Engine with Graph
  Augmented Deep Learning and External Knowledge Empowerment","  Effective biomedical literature retrieval (BLR) plays a central role in
precision medicine informatics. In this paper, we propose GRAPHENE, which is a
deep learning based framework for precise BLR. GRAPHENE consists of three main
different modules 1) graph-augmented document representation learning; 2) query
expansion and representation learning and 3) learning to rank biomedical
articles. The graph-augmented document representation learning module
constructs a document-concept graph containing biomedical concept nodes and
document nodes so that global biomedical related concept from external
knowledge source can be captured, which is further connected to a BiLSTM so
both local and global topics can be explored. Query expansion and
representation learning module expands the query with abbreviations and
different names, and then builds a CNN-based model to convolve the expanded
query and obtain a vector representation for each query. Learning to rank
minimizes a ranking loss between biomedical articles with the query to learn
the retrieval function. Experimental results on applying our system to TREC
Precision Medicine track data are provided to demonstrate its effectiveness.
"
2744,Impact Factor volatility to a single paper: A comprehensive analysis,"  We study how a single paper affects the Impact Factor (IF) by analyzing data
from 3,088,511 papers published in 11639 journals in the 2017 Journal Citation
Reports of Clarivate Analytics. We find that IFs are highly volatile. For
example, the top-cited paper of 381 journals caused their IF to increase by
more than 0.5 points, while for 818 journals the relative increase exceeded
25%. And one in 10 journals had their IF boosted by more than 50% by their top
three cited papers. Because the single-paper effect on the IF is inversely
proportional to journal size, small journals are rewarded much more strongly
than large journals for a highly-cited paper, while they are penalized more for
a low-cited paper, especially if their IF is high. This skewed reward mechanism
incentivizes high-IF journals to stay small, to remain competitive in rankings.
We discuss the implications for breakthrough papers appearing in prestigious
journals. We question the reliability of IF rankings given the high IF
sensitivity to a few papers for thousands of journals.
"
2745,Gextext: Disease Network Extraction from Biomedical Literature,"  PURPOSE: We propose a fully unsupervised method to learn latent disease
networks directly from unstructured biomedical text corpora. This method
addresses current challenges in unsupervised knowledge extraction, such as the
detection of long-range dependencies and requirements for large training
corpora. METHODS: Let C be a corpus of n text chunks. Let V be a set of p
disease terms occurring in the corpus. Let X indicate the occurrence of V in C.
Gextext identifies disease similarities by positively correlated occurrence
patterns. This information is combined to generate a graph on which geodesic
distance describes dissimilarity. Diseasomes were learned by Gextext and GloVE
on corpora of 100-1000 PubMed abstracts. Similarity matrix estimates were
validated against biomedical semantic similarity metrics and gene profile
similarity. RESULTS: Geodesic distance on Gextext-inferred diseasomes
correlated inversely with external measures of semantic similarity. Gene
profile similarity also correlated significant with proximity on the inferred
graph. Gextext outperformed GloVE in our experiments. The information contained
on the Gextext graph exceeded the explicit information content within the text.
CONCLUSIONS: Gextext extracts latent relationships from unstructured text,
enabling fully unsupervised modelling of diseasome graphs from PubMed
abstracts.
"
2746,"Textual analysis of artificial intelligence manuscripts reveals features
  associated with peer review outcome","  We analysed a dataset of scientific manuscripts that were submitted to
various conferences in artificial intelligence. We performed a combination of
semantic, lexical and psycholinguistic analyses of the full text of the
manuscripts and compared them with the outcome of the peer review process. We
found that accepted manuscripts scored lower than rejected manuscripts on two
indicators of readability, and that they also used more scientific and
artificial intelligence jargon. We also found that accepted manuscripts were
written with words that are less frequent, that are acquired at an older age,
and that are more abstract than rejected manuscripts. The analysis of
references included in the manuscripts revealed that the subset of accepted
submissions were more likely to cite the same publications. This finding was
echoed by pairwise comparisons of the word content of the manuscripts (i.e. an
indicator or semantic similarity), which were more similar in the subset of
accepted manuscripts. Finally, we predicted the peer review outcome of
manuscripts with their word content, with words related to machine learning and
neural networks positively related with acceptance, whereas words related to
logic, symbolic processing and knowledge-based systems negatively related with
acceptance.
"
2747,The effect of novelty on the future impact of scientific grants,"  Government funding agencies and foundations tend to perceive novelty as
necessary for scientific impact and hence prefer to fund novel instead of
incremental projects. Evidence linking novelty and the eventual impact of a
grant is surprisingly scarce, however. Here, we examine this link by analyzing
920,000 publications funded by 170,000 grants from the National Science
Foundation (NSF) and the National Institutes of Health (NIH) between 2008 and
2016. We use machine learning to quantify grant novelty at the time of funding
and relate that measure to the citation dynamics of these publications. Our
results show that grant novelty leads to robust increases in citations while
controlling for the principal investigator's grant experience, award amount,
year of publication, prestige of the journal, and team size. All else held
constant, an article resulting from a fully-novel grant would on average double
the citations of a fully-incremental grant. We also find that novel grants
produce as many articles as incremental grants while publishing in higher
prestige journals. Taken together, our results provide compelling evidence
supporting NSF, NIH, and many other funding agencies' emphases on novelty.
"
2748,S2ORC: The Semantic Scholar Open Research Corpus,"  We introduce S2ORC, a large corpus of 81.1M English-language academic papers
spanning many academic disciplines. The corpus consists of rich metadata, paper
abstracts, resolved bibliographic references, as well as structured full text
for 8.1M open access papers. Full text is annotated with automatically-detected
inline mentions of citations, figures, and tables, each linked to their
corresponding paper objects. In S2ORC, we aggregate papers from hundreds of
academic publishers and digital archives into a unified source, and create the
largest publicly-available collection of machine-readable academic text to
date. We hope this resource will facilitate research and development of tools
and tasks for text mining over academic text.
"
2749,Perspectives on urban theories,"  At the end of the five years of work in our GeoDiverCity program, we brought
together a diversity of authors from different disciplines. Each person was
invited to present an important question about the theories and models of
urbanization. They are representative of a variety of currents in urban
research. Rather than repeat here the contents of all chapters, we propose two
ways to synthesize the scientific contributions of this book. In a first part
we replace them in relation to a few principles that were experimented in our
program, and in a second part we situate them with respect to a broader view of
international literature on these topics.
"
2750,"Does the use of open, non-anonymous peer review in scholarly publishing
  introduce bias? Evidence from the F1000 post-publication open peer review
  publishing model","  This study examines whether there is any evidence of bias in two areas of
common critique of open, non-anonymous peer review - and used in the
post-publication, peer review system operated by the open-access scholarly
publishing platform F1000Research. First, is there evidence of bias where a
reviewer based in a specific country assesses the work of an author also based
in the same country? Second, are reviewers influenced by being able to see the
comments and know the origins of previous reviewer? Methods: Scrutinising the
open peer review comments published on F1000Research, we assess the extent of
two frequently cited potential influences on reviewers that may be the result
of the transparency offered by a fully attributable, open peer review
publishing model: the national affiliations of authors and reviewers, and the
ability of reviewers to view previously-published reviewer reports before
submitting their own. The effects of these potential influences were
investigated for all first versions of articles published by 8 July 2019 to
F1000Research. In 16 out of the 20 countries with the most articles, there was
a tendency for reviewers based in the same country to give a more positive
review. The difference was statistically significant in one. Only 3 countries
had the reverse tendency. Second, there is no evidence of a conformity bias.
When reviewers mentioned a previous review in their peer review report, they
were not more likely to give the same overall judgement. Although reviewers who
had longer to potentially read a previously published reviewer reports were
slightly less likely to agree with previous reviewer judgements, this could be
due to these articles being difficult to judge rather than deliberate
non-conformity.
"
2751,The State of NLP Literature: A Diachronic Analysis of the ACL Anthology,"  The ACL Anthology (AA) is a digital repository of tens of thousands of
articles on Natural Language Processing (NLP). This paper examines the
literature as a whole to identify broad trends in productivity, focus, and
impact. It presents the analyses in a sequence of questions and answers. The
goal is to record the state of the AA literature: who and how many of us are
publishing? what are we publishing on? where and in what form are we
publishing? and what is the impact of our publications? The answers are usually
in the form of numbers, graphs, and inter-connected visualizations. Special
emphasis is laid on the demographics and inclusiveness of NLP publishing.
Notably, we find that only about 30% of first authors are female, and that this
percentage has not improved since the year 2000. We also show that, on average,
female first authors are cited less than male first authors, even when
controlling for experience. We hope that recording citation and participation
gaps across demographic groups will encourage more inclusiveness and fairness
in research.
"
2752,"Shorter Distances between Papers over Time are Due to More Cross-Field
  References and Increased Citation Rate to Higher Impact Papers","  The exponential increase in the number of scientific publications raises the
question of whether the sciences are expanding into a fractured structure,
making cross-field communication difficult. On the other hand, scientists may
be motivated to learn extensively across fields to enhance their innovative
capacity, and this may offset the negative effects of fragmentation. Through an
investigation of the distances within and clustering of cross-sectional
citation networks, this study presents evidence that fields of science become
more integrated over time. The average citation distance between papers
published in the same year decreased from approximately 5.33 to 3.18 steps
between 1950 and 2018. This observation is attributed to the growth of
cross-field communication throughout the entire period as well as the growing
importance of high impact papers to bridge networks in the same year. Three
empirical findings support this conclusion. First, distances decreased between
almost all disciplines throughout the time period. Second, inequality in the
number of citations received by papers increased, and as a consequence the
shortest paths in the network depend more on high impact papers later in the
period. Third, the dispersion of connections between fields increased
continually. Moreover, these changes did not entail a lower level of clustering
of citations. Both within- and cross-field citations show a similar rate of
slowly growing clustering values in all years. The latter findings suggest that
domain spanning scholarly communication is partly enabled by new fields that
connect disciplines.
"
2753,"Building the National Radio Recordings Database: A Big Data Approach to
  Documenting Audio Heritage","  This paper traces strategies used by the Radio Preservation Task Force of the
Library of Congress's National Recording Preservation Board to develop a
publicly searchable database documenting extant radio materials held by
collecting institutions throughout the country. Having aggregated metadata on
2,500 unique collections to date, the project has encountered a series of
logistical challenges that are not only technical in nature but also
institutional and social, raising critical issues involving organizational
structure, political representation, and the ethics of data access. As the
project continues to expand and evolve, lessons from its early development
offer valuable reminders of the human judgment, hidden labor, and interpersonal
relations required for successful big data work.
"
2754,Arbitrage opportunities in publication and ghost authors,"  In some research evaluation systems, credit awarded to an article depends on
the number of co-authors on the article with total credit to the article
increasing with the number of co-authors. There are many examples of such
evaluation systems (e.g., the United States National Research Council
evaluation of graduate programs gave full credit to each co-author). Such
credit systems run the risk of encouraging ghost or honorary authorships. In a
recent article, Antonio Osorio and Lutz Bornmann (2019) propose a scheme to
discourage ghost authorships but increase the total credit to a paper when
co-authorships increase. It is shown that if articles are valued more highly as
the number of co-authorships increases, then there are opportunities to
increase credit by mutually agreeing to add each other as authors. Unrelated
authors of unrelated papers may all benefit by expanding their co-author list.
I call this phenomena arbitrage--a term borrowed from economics and
finance--since the content of the articles do not change, but the value
increases by moving to a ""market"" of more co-authors where articles are valued
differently.
"
2755,"Do disruption index indicators measure what they propose to measure? The
  comparison of several indicator variants with assessments by peers","  Recently, Wu, Wang, and Evans (2019) and Bu, Waltman, and Huang (2019)
proposed a new family of indicators, which measure whether a scientific
publication is disruptive to a field or tradition of research. Such disruptive
influences are characterized by citations to a focal paper, but not its cited
references. In this study, we are interested in the question of convergent
validity, i.e., whether these indicators of disruption are able to measure what
they propose to measure ('disruptiveness'). We used external criteria of
newness to examine convergent validity: in the post-publication peer review
system of F1000Prime, experts assess papers whether the reported research
fulfills these criteria (e.g., reports new findings). This study is based on
120,179 papers from F1000Prime published between 2000 and 2016. In the first
part of the study we discuss the indicators. Based on the insights from the
discussion, we propose alternate variants of disruption indicators. In the
second part, we investigate the convergent validity of the indicators and the
(possibly) improved variants. Although the results of a factor analysis show
that the different variants measure similar dimensions, the results of
regression analyses reveal that one variant (DI5) performs slightly better than
the others.
"
2756,Talking datasets: Understanding data sensemaking behaviours,"  The sharing and reuse of data are seen as critical to solving the most
complex problems of today. Despite this potential, relatively little is known
about a key step in data reuse: people's behaviours involved in data-centric
sensemaking. We aim to address this gap by presenting a mixed-methods study
combining in-depth interviews, a think-aloud task and a screen recording
analysis with 31 researchers as they summarised and interacted with both
familiar and unfamiliar data. We use our findings to identify and detail common
activity patterns and necessary data attributes across three clusters of
sensemaking activities: inspecting data, engaging with content, and placing
data within broader contexts. We conclude by proposing design recommendations
for tools and documentation practices which can be used to facilitate
sensemaking and subsequent data reuse.
"
2757,Do top conferences contain well cited papers or junk?,"  In order to answer questions about top conference publication patterns,
citation data is collected and analyzed for several computer science
conferences, with focus on computer vision and graphics. Both top and second
tier conferences are included, and sampling occurred for two different 5 year
periods. Example questions include: Do top conferences contain well cited
papers or junk? (Yes) Are top conferences similarly cited? (No) Are second tier
conferences as good as first tier conferences? (Sometimes) Has something been
changing at CVPR? (Yes)
"
2758,"Co-contributorship Network and Division of Labor in Individual
  Scientific Collaborations","  Collaborations are pervasive in current science. Collaborations have been
studied and encouraged in many disciplines. However, little is known how a team
really functions from the detailed division of labor within. In this research,
we investigate the patterns of scientific collaboration and division of labor
within individual scholarly articles by analyzing their co-contributorship
networks. Co-contributorship networks are constructed by performing the
one-mode projection of the author-task bipartite networks obtained from 138,787
papers published in PLoS journals. Given a paper, we define three types of
contributors: Specialists, Team-players, and Versatiles. Specialists are those
who contribute to all their tasks alone; team-players are those who contribute
to every task with other collaborators; and versatiles are those who do both.
We find that team-players are the majority and they tend to contribute to the
five most common tasks as expected, such as ""data analysis"" and ""performing
experiments"". The specialists and versatiles are more prevalent than expected
by a random-graph null model. Versatiles tend to be senior authors associated
with funding and supervisions. Specialists are associated with two contrasting
roles: the supervising role as team leaders or marginal and specialized
contributions.
"
2759,Exposing SED Models And Snapshots Via VO Simulation Artefacts,"  The Virtual Observatory (VO) simulation standards, Simulation Data Model
(SimDM) and Simulation Data Access Layer (SimDAL), establish a framework for
the discoverability and dissemination of data created in simulation projects.
These standards address the complexity of having a standard access and facade
for data which is expected to be multifaceted and, of a diverse range. In this
paper, we detail the realisation of an application exposing the theoretical
products of one such scientific project via the simulation facades proposed by
the VO. The scientific project in question, is a study of the evolution of
young clusters in dense molecular clumps. The theoretical products arising from
this study include a grid of 20 million SED (Spectral Energy Distribution)
models for synthetic young clusters and related data products. Details on the
implementation of SimDAL components in the application as well as the ways in
which the data structures of SimDM are incorporated onto the existing data
products are provided.
"
2760,"My Approach = Your Apparatus? Entropy-Based Topic Modeling on Multiple
  Domain-Specific Text Collections","  Comparative text mining extends from genre analysis and political bias
detection to the revelation of cultural and geographic differences, through to
the search for prior art across patents and scientific papers. These
applications use cross-collection topic modeling for the exploration,
clustering, and comparison of large sets of documents, such as digital
libraries. However, topic modeling on documents from different collections is
challenging because of domain-specific vocabulary. We present a
cross-collection topic model combined with automatic domain term extraction and
phrase segmentation. This model distinguishes collection-specific and
collection-independent words based on information entropy and reveals
commonalities and differences of multiple text collections. We evaluate our
model on patents, scientific papers, newspaper articles, forum posts, and
Wikipedia articles. In comparison to state-of-the-art cross-collection topic
modeling, our model achieves up to 13% higher topic coherence, up to 4% lower
perplexity, and up to 31% higher document classification accuracy. More
importantly, our approach is the first topic model that ensures disjunct
general and specific word distributions, resulting in clear-cut topic
representations.
"
2761,Recency predicts bursts in the evolution of author citations,"  The citations process for scientific papers has been studied extensively. But
while the citations accrued by authors are the sum of the citations of their
papers, translating the dynamics of citation accumulation from the paper to the
author level is not trivial. Here we conduct a systematic study of the
evolution of author citations, and in particular their bursty dynamics. We find
empirical evidence of a correlation between the number of citations most
recently accrued by an author and the number of citations they receive in the
future. Using a simple model where the probability for an author to receive new
citations depends only on the number of citations collected in the previous
12-24 months, we are able to reproduce both the citation and burst size
distributions of authors across multiple decades.
"
2762,"Legal document retrieval across languages: topic hierarchies based on
  synsets","  Cross-lingual annotations of legislative texts enable us to explore major
themes covered in multilingual legal data and are a key facilitator of semantic
similarity when searching for similar documents. Multilingual probabilistic
topic models have recently emerged as a group of semi-supervised machine
learning models that can be used to perform thematic explorations on
collections of texts in multiple languages. However, these approaches require
theme-aligned training data to create a language-independent space, which
limits the amount of scenarios where this technique can be used. In this work,
we provide an unsupervised document similarity algorithm based on hierarchies
of multi-lingual concepts to describe topics across languages. The algorithm
does not require parallel or comparable corpora, or any other type of
translation resource. Experiments performed on the English, Spanish, French and
Portuguese editions of JCR-Acquis corpora reveal promising results on
classifying and sorting documents by similar content.
"
2763,"The future of urban models in the Big Data and AI era: a bibliometric
  analysis (2000-2019)","  This article questions the effects on urban research dynamics of the Big Data
and AI turn in urban management. To identify these effects, we use two
complementary materials: bibliometric data and interviews. We consider two
areas in urban research: one, covering the academic research dealing with
transportation systems and the other, with water systems. First, we measure the
evolution of AI and Big Data keywords in these two areas. Second, we measure
the evolution of the share of publications published in computer science
journals about urban traffic and water quality. To guide these bibliometric
analyses, we rely on the content of interviews conducted with academics and
higher education officials in Paris and Edinburgh at the beginning of 2018.
"
2764,The citation disadvantage of clinical research,"  Biomedical research encompasses diverse types of activities, from basic
science (""bench"") to clinical medicine (""bedside"") to bench-to-bedside
translational research. It, however, remains unclear whether different types of
research receive citations at varying rates. Here we aim to answer this
question by using a newly proposed paper-level indicator that quantifies the
extent to which a paper is basic science or clinical medicine. Applying this
measure to 5 million biomedical papers, we find a systematic citation
disadvantage of clinical oriented papers; they tend to garner far fewer
citations and are less likely to be hit works than papers oriented towards
basic science. At the same time, clinical research has a higher variance in its
citation. We also find that the citation difference between basic and clinical
research decreases, yet still persists, if longer citation-window is used.
Given the increasing adoption of short-term, citation-based bibliometric
indicators in funding decisions, the under-cited effect of clinical research
may provide disincentives for bio-researchers to venture into the translation
of basic scientific discoveries into clinical applications, thus providing
explanations of reasons behind the existence of the gap between basic and
clinical research that is commented as ""valley of death"" and the commentary of
""extinction"" risk of translational researchers. Our work may provide insights
to policy-makers on how to evaluate different types of biomedical research.
"
2765,Blockchain Applications in Power Systems: A Bibliometric Analysis,"  Power systems are growing rapidly, due to the ever-increasing demand for
electrical power. These systems require novel methodologies and modern tools
and technologies, to better perform, particularly for communication among
different parts. Therefore, power systems are facing new challenges such as
energy trading and marketing and cyber threats. Using blockchain in power
systems, as a solution, is one of the newest methods. Most studies aim to
investigate innovative approach-es of blockchain application in power systems.
Even though, many articles published to support the research activities, there
has not been any bibliometric analysis which specifies the research trends.
This paper aims to present a bibliographic analysis of the blockchain
application in power systems related literature, in the Web of Science (WoS)
database between January 2009 and July 2019. This paper discusses the research
activities and performed a detailed analysis by looking at the number of
articles published, citations, institutions, research areas, and authors. From
the analysis, it was concluded that there are several significant impacts of
research activities in China and the USA, in comparison to other countries.
"
2766,A Collaborative Ecosystem for Digital Coptic Studies,"  Scholarship on underresourced languages bring with them a variety of
challenges which make access to the full spectrum of source materials and their
evaluation difficult. For Coptic in particular, large scale analyses and any
kind of quantitative work become difficult due to the fragmentation of
manuscripts, the highly fusional nature of an incorporational morphology, and
the complications of dealing with influences from Hellenistic era Greek, among
other concerns. Many of these challenges, however, can be addressed using
Digital Humanities tools and standards. In this paper, we outline some of the
latest developments in Coptic Scriptorium, a DH project dedicated to bringing
Coptic resources online in uniform, machine readable, and openly available
formats. Collaborative web-based tools create online 'virtual departments' in
which scholars dispersed sparsely across the globe can collaborate, and natural
language processing tools counterbalance the scarcity of trained editors by
enabling machine processing of Coptic text to produce searchable, annotated
corpora.
"
2767,The Rise of Multiple Institutional Affiliations in Academia,"  This study provides the first systematic, international, large-scale evidence
on the extent and nature of multiple institutional affiliations on journal
publications. Studying more than 13.6M authors and 20.5M articles from 40
countries we document that: In 2019, almost one in three articles was
(co-)authored by authors with multiple affiliations and the share of authors
with multiple affiliations increased from 6.7% to 11.8% since 1996. The growth
of multiple affiliations is prevalent in all disciplines and it is stronger in
high impact journals. About 60% of multiple affiliations involve institutions
from within the academic sector and international co-affiliations often involve
institutions from the United States, Germany and the United Kingdom as well as
institutions in neighboring countries. We discuss potential causes and show
that the timing of the rise in multiple affiliations can be linked to the
introduction of more competitive funding structures such as 'excellence
initiatives' in a number of countries. We discuss implications for science and
science policy.
"
2768,The role of Web of Science publications in China's tenure system,"  Tenure provides a permanent position to faculty in higher education
institutions. In North America, it is granted to those who have established a
record of excellence in research, teaching and services in a limited period.
However, in China, research excellence represented by the number of Web of
Science publications is highly weighted in the tenure assessment compared to
excellence in teaching and services, but this has never been systematically
investigated. By analyzing the tenure assessment documents from Chinese
universities, this study reveals the role of Web of Science publications in
China tenure system and presents the landscape of the tenure assessment process
in Chinese higher education institutions.
"
2769,LScDC-new large scientific dictionary,"  In this paper, we present a scientific corpus of abstracts of academic papers
in English -- Leicester Scientific Corpus (LSC). The LSC contains 1,673,824
abstracts of research articles and proceeding papers indexed by Web of Science
(WoS) in which publication year is 2014. Each abstract is assigned to at least
one of 252 subject categories. Paper metadata include these categories and the
number of citations. We then develop scientific dictionaries named Leicester
Scientific Dictionary (LScD) and Leicester Scientific Dictionary-Core (LScDC),
where words are extracted from the LSC. The LScD is a list of 974,238 unique
words (lemmas). The LScDC is a core list (sub-list) of the LScD with 104,223
lemmas. It was created by removing LScD words appearing in not greater than 10
texts in the LSC. LScD and LScDC are available online. Both the corpus and
dictionaries are developed to be later used for quantification of meaning in
academic texts.
  Finally, the core list LScDC was analysed by comparing its words and word
frequencies with a classic academic word list 'New Academic Word List (NAWL)'
containing 963 word families, which is also sampled from an academic corpus.
The major sources of the corpus where NAWL is extracted are Cambridge English
Corpus (CEC), oral sources and textbooks. We investigate whether two
dictionaries are similar in terms of common words and ranking of words. Our
comparison leads us to main conclusion: most of words of NAWL (99.6%) are
present in the LScDC but two lists differ in word ranking. This difference is
measured.
"
2770,Hunting for supernovae articles in the universe of scientometrics,"  This short note records an unusual situation with some Google Scholar's
profiles that imply the existence of ""supernovae"" articles, i.e., articles
whose impact -- in terms of number of citations -- in a single year gets
(almost) an order of magnitude higher than the previous year and immediate
drops (and remains steady) to a very low level after the next year. We analyse
the issue and resolve the situation providing an answer whether there exist
supernovae articles.
"
2771,"A Hybrid Approach and Unified Framework for Bibliographic Reference
  Extraction","  Publications are an integral part in a scientific community. Bibliographic
reference extraction from scientific publication is a challenging task due to
diversity in referencing styles and document layout. Existing methods perform
sufficiently on one dataset however, applying these solutions to a different
dataset proves to be challenging. Therefore, a generic solution was anticipated
which could overcome the limitations of the previous approaches. The
contribution of this paper is three-fold. First, it presents a novel approach
called DeepBiRD which is inspired by human visual perception and exploits
layout features to identify individual references in a scientific publication.
Second, we release a large dataset for image-based reference detection with
2401 scans containing 38863 references, all manually annotated for individual
reference. Third, we present a unified and highly configurable end-to-end
automatic bibliographic reference extraction framework called BRExSys which
employs DeepBiRD along with state-of-the-art text-based models to detect and
visualize references from a bibliographic document. Our proposed approach
pre-processes the images in which a hybrid representation is obtained by
processing the given image using different computer vision techniques. Then, it
performs layout driven reference detection using Mask R-CNN on a given
scientific publication. DeepBiRD was evaluated on two different datasets to
demonstrate the generalization of this approach. The proposed system achieved
an AP50 of 98.56% on our dataset. DeepBiRD significantly outperformed the
current state-of-the-art approach on their dataset. Therefore, suggesting that
DeepBiRD is significantly superior in performance, generalized, and independent
of any domain or referencing style.
"
2772,"Selecting efficient and reliable preservation strategies: modeling
  long-term information integrity using large-scale hierarchical discrete event
  simulation","  This article addresses the problem of formulating efficient and reliable
operational preservation policies that ensure bit-level information integrity
over long periods, and in the presence of a diverse range of real-world
technical, legal, organizational, and economic threats. We develop a
systematic, quantitative prediction framework that combines formal modeling,
discrete-event-based simulation, hierarchical modeling, and then use
empirically calibrated sensitivity analysis to identify effective strategies.
The framework offers flexibility for the modeling of a wide range of
preservation policies and threats. Since this framework is open source and
easily deployed in a cloud computing environment, it can be used to produce
analysis based on independent estimates of scenario-specific costs,
reliability, and risks.
"
2773,"Garbage In, Garbage Out? Do Machine Learning Application Papers in
  Social Computing Report Where Human-Labeled Training Data Comes From?","  Many machine learning projects for new application areas involve teams of
humans who label data for a particular purpose, from hiring crowdworkers to the
paper's authors labeling the data themselves. Such a task is quite similar to
(or a form of) structured content analysis, which is a longstanding methodology
in the social sciences and humanities, with many established best practices. In
this paper, we investigate to what extent a sample of machine learning
application papers in social computing --- specifically papers from ArXiv and
traditional publications performing an ML classification task on Twitter data
--- give specific details about whether such best practices were followed. Our
team conducted multiple rounds of structured content analysis of each paper,
making determinations such as: Does the paper report who the labelers were,
what their qualifications were, whether they independently labeled the same
items, whether inter-rater reliability metrics were disclosed, what level of
training and/or instructions were given to labelers, whether compensation for
crowdworkers is disclosed, and if the training data is publicly available. We
find a wide divergence in whether such practices were followed and documented.
Much of machine learning research and education focuses on what is done once a
""gold standard"" of training data is available, but we discuss issues around the
equally-important aspect of whether such data is reliable in the first place.
"
2774,"Towards an automatic recognition of mixed languages: The
  Ukrainian-Russian hybrid language Surzhyk","  Language interference is common in today's multilingual societies where more
languages are being in contact and as a global final result leads to the
creation of hybrid languages. These, together with doubts on their right to be
officially recognised made emerge in the area of computational linguistics the
problem of their automatic identification and further elaboration. In this
paper, we propose a first attempt to identify the elements of a
Ukrainian-Russian hybrid language, Surzhyk, through the adoption of the
example-based rules created with the instruments of programming language R. Our
example-based study consists of: 1) analysis of spoken samples of Surzhyk
registered by Del Gaudio (2010) in Kyiv area and creation of the written
corpus; 2) production of specific rules on the identification of Surzhyk
patterns and their implementation; 3) testing the code and analysing the
effectiveness.
"
2775,Inferring the causal effect of journals on citations,"  Articles in high-impact journals are by definition more highly cited on
average. But are they cited more often because the articles are somehow
""better""? Or are they cited more often simply because they appeared in a
high-impact journal? Although some evidence suggests the latter the causal
relationship is not clear. We here compare citations of published journal
articles to citations of their preprint versions to uncover the causal
mechanism. We build on an earlier model to infer the causal effect of journals
on citations. We find evidence for both effects. We show that high-impact
journals seem to select articles that tend to attract more citations. At the
same time, we find that high-impact journals augment the citation rate of
published articles. Our results yield a deeper understanding of the role of
journals in the research system. The use of journal metrics in research
evaluation has been increasingly criticised in recent years and article-level
citations are sometimes suggested as an alternative. Our results show that
removing impact factors from evaluation does not negate the influence of
journals. This insight has important implications for changing practices of
research evaluation.
"
2776,"Meta-Learned Per-Instance Algorithm Selection in Scholarly Recommender
  Systems","  The effectiveness of recommender system algorithms varies in different
real-world scenarios. It is difficult to choose a best algorithm for a scenario
due to the quantity of algorithms available, and because of their varying
performances. Furthermore, it is not possible to choose one single algorithm
that will work optimally for all recommendation requests. We apply
meta-learning to this problem of algorithm selection for scholarly article
recommendation. We train a random forest, gradient boosting machine, and
generalized linear model, to predict a best-algorithm from a pool of content
similarity-based algorithms. We evaluate our approach on an offline dataset for
scholarly article recommendation and attempt to predict the best algorithm
per-instance. The best meta-learning model achieved an average increase in F1
of 88% when compared to the average F1 of all base-algorithms (F1; 0.0708 vs
0.0376) and was significantly able to correctly select each base-algorithm
(Paired t-test; p < 0.1). The meta-learner had a 3% higher F1 when compared to
the single-best base-algorithm (F1; 0.0739 vs 0.0717). We further perform an
online evaluation of our approach, conducting an A/B test through our
recommender-as-a-service platform Mr. DLib. We deliver 148K recommendations to
users between January and March 2019. User engagement was significantly
increased for recommendations generated using our meta-learning approach when
compared to a random selection of algorithm (Click-through rate (CTR); 0.51%
vs. 0.44%, Chi-Squared test; p < 0.1), however our approach did not produce a
higher CTR than the best algorithm alone (CTR; MoreLikeThis (Title): 0.58%).
"
2777,"Research Frontiers in Transfer Learning -- a systematic and bibliometric
  review","  Humans can learn from very few samples, demonstrating an outstanding
generalization ability that learning algorithms are still far from reaching.
Currently, the most successful models demand enormous amounts of well-labeled
data, which are expensive and difficult to obtain, becoming one of the biggest
obstacles to the use of machine learning in practice. This scenario shows the
massive potential for Transfer Learning, which aims to harness previously
acquired knowledge to the learning of new tasks more effectively and
efficiently. In this systematic review, we apply a quantitative method to
select the main contributions to the field and make use of bibliographic
coupling metrics to identify research frontiers. We further analyze the
linguistic variation between the classics of the field and the frontier and map
promising research directions.
"
2778,"Establishing a Search String to Detect Secondary Studies in Software
  Engineering","  Search for secondary studies is essential to establish whether the review on
the intended topic has already been done, avoiding waste time. In addition,
secondary studies are the inputs of a tertiary study. However, one critical
step in searching for secondary studies is to elaborate a search string. The
main goal of this work is to analyze search strings to establish directions to
better detect secondary studies in Software Engineering (SE). We analyzed seven
tertiary studies under two perspectives: (1) structure - strings' terms to
detect secondary studies; and (2) field: where searching - titles alone or
abstracts alone or titles and abstracts together, among others. We also
performed a validation of the results found. The suitable search string for
finding secondary studies in SE contain the terms ""systematic review"",
""literature review"", ""systematic mapping"", ""mapping study"", ""systematic map"",
""meta-analysis"", ""survey"" and ""literature analysis"". Furthermore, we recommend
(1) researchers use the title, abstract and keywords search fields in their
searches to increase studies recall; (2) researchers choose carefully their
paper title, abstract and keyword terms to increase the chance of having such
studies found on digital libraries.
"
2779,Early Detection of Research Trends,"  Being able to rapidly recognise new research trends is strategic for many
stakeholders, including universities, institutional funding bodies, academic
publishers and companies. The literature presents several approaches to
identifying the emergence of new research topics, which rely on the assumption
that the topic is already exhibiting a certain degree of popularity and
consistently referred to by a community of researchers. However, detecting the
emergence of a new research area at an embryonic stage, i.e., before the topic
has been consistently labelled by a community of researchers and associated
with a number of publications, is still an open challenge. In this
dissertation, we begin to address this challenge by performing a study of the
dynamics preceding the creation of new topics. This study indicates that the
emergence of a new topic is anticipated by a significant increase in the pace
of collaboration between relevant research areas, which can be seen as the
'ancestors' of the new topic. Based on this understanding, we developed Augur,
a novel approach to effectively detecting the emergence of new research topics.
Augur analyses the diachronic relationships between research areas and is able
to detect clusters of topics that exhibit dynamics correlated with the
emergence of new research topics. Here we also present the Advanced Clique
Percolation Method (ACPM), a new community detection algorithm developed
specifically for supporting this task. Augur was evaluated on a gold standard
of 1,408 debutant topics in the 2000-2011 timeframe and outperformed four
alternative approaches in terms of both precision and recall.
"
2780,Mislabel Detection of Finnish Publication Ranks,"  The paper proposes to analyze a data set of Finnish ranks of academic
publication channels with Extreme Learning Machine (ELM). The purpose is to
introduce and test recently proposed ELM-based mislabel detection approach with
a rich set of features characterizing a publication channel. We will compare
the architecture, accuracy, and, especially, the set of detected mislabels of
the ELM-based approach to the corresponding reference results on the reference
paper.
"
2781,"Na\""iveRole: Author-Contribution Extraction and Parsing from Biomedical
  Manuscripts","  Information about the contributions of individual authors to scientific
publications is important for assessing authors' achievements. Some biomedical
publications have a short section that describes authors' roles and
contributions. It is usually written in natural language and hence author
contributions cannot be trivially extracted in a machine-readable format. In
this paper, we present 1) A statistical analysis of roles in author
contributions sections, and 2) Na\""iveRole, a novel approach to extract
structured authors' roles from author contribution sections. For the first
part, we used co-clustering techniques, as well as Open Information Extraction,
to semi-automatically discover the popular roles within a corpus of 2,000
contributions sections from PubMed Central. The discovered roles were used to
automatically build a training set for Na\""iveRole, our role extractor
approach, based on Na\""ive Bayes. Na\""iveRole extracts roles with a
micro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the
best of our knowledge, the first attempt to automatically extract author roles
from research papers. This paper is an extended version of a previous poster
published at JCDL 2018.
"
2782,How Frequently are Articles in Predatory Open Access Journals Cited,"  Predatory journals are Open Access journals of highly questionable scientific
quality. Such journals pretend to use peer review for quality assurance, and
spam academics with requests for submissions, in order to collect author
payments. In recent years predatory journals have received a lot of negative
media. While much has been said about the harm that such journals cause to
academic publishing in general, an overlooked aspect is how much articles in
such journals are actually read and in particular cited, that is if they have
any significant impact on the research in their fields. Other studies have
already demonstrated that only some of the articles in predatory journals
contain faulty and directly harmful results, while a lot of the articles
present mediocre and poorly reported studies. We studied citation statistics
over a five-year period in Google Scholar for 250 random articles published in
such journals in 2014, and found an average of 2,6 citations per article and
that 60 % of the articles had no citations at all. For comparison a random
sample of articles published in the approximately 25,000 peer reviewed journals
included in the Scopus index had an average of 18,1 citations in the same
period with only 9 % receiving no citations. We conclude that articles
published in predatory journals have little scientific impact.
"
2783,"Viewing Computer Science through Citation Analysis; Salton and Bergmark
  Redux","  Computer science has experienced dramatic growth and diversification over the
last twenty years. Towards a current understanding of the structure of this
discipline, we analyze a cohort of the computer science literature using the
DBLP database. For insight on the features of this cohort and the relationship
within its components, we constructed article level clusters based on either
direct citations or co-citations, and reconciled them to major and minor
subject categories in the Scopus All Science Journal Classification (ASJC). We
described complementary insights from clustering by direct citation and
co-citation, and both point to the increase in computer science publications
and their scope. Our analysis shows cross-category clusters, some that interact
with external fields, such as the biological sciences, while others remain
inward looking.
"
2784,"Visual Summarization of Scholarly Videos using Word Embeddings and
  Keyphrase Extraction","  Effective learning with audiovisual content depends on many factors. Besides
the quality of the learning resource's content, it is essential to discover the
most relevant and suitable video in order to support the learning process most
effectively. Video summarization techniques facilitate this goal by providing a
quick overview over the content. It is especially useful for longer recordings
such as conference presentations or lectures. In this paper, we present an
approach that generates a visual summary of video content based on semantic
word embeddings and keyphrase extraction. For this purpose, we exploit video
annotations that are automatically generated by speech recognition and video
OCR (optical character recognition).
"
2785,"Where Are We? Using Scopus to Map the Literature at the Intersection
  Between Artificial Intelligence and Research on Crime","  Research on Artificial Intelligence (AI) applications has spread over many
scientific disciplines. Scientists have tested the power of intelligent
algorithms developed to predict (or learn from) natural, physical and social
phenomena. This also applies to crime-related research problems. Nonetheless,
studies that map the current state of the art at the intersection between AI
and crime are lacking. What are the current research trends in terms of topics
in this area? What is the structure of scientific collaboration when
considering works investigating criminal issues using machine learning, deep
learning, and AI in general? What are the most active countries in this
specific scientific sphere? Using data retrieved from the Scopus database, this
work quantitatively analyzes 692 published works at the intersection between AI
and crime employing network science to respond to these questions. Results show
that researchers are mainly focusing on cyber-related criminal topics and that
relevant themes such as algorithmic discrimination, fairness, and ethics are
considerably overlooked. Furthermore, data highlight the extremely disconnected
structure of co-authorship networks. Such disconnectedness may represent a
substantial obstacle to a more solid community of scientists interested in
these topics. Additionally, the graph of scientific collaboration indicates
that countries that are more prone to engage in international partnerships are
generally less central in the network. This means that scholars working in
highly productive countries (e.g. the United States, China) tend to mostly
collaborate domestically. Finally, current issues and future developments
within this scientific area are also discussed.
"
2786,"Analysis of Reference and Citation Copying in Evolving Bibliographic
  Networks","  Extensive literature demonstrates how the copying of references (links) can
lead to the emergence of various structural properties (e.g., power-law degree
distribution and bipartite cores) in bibliographic and other similar directed
networks. However, it is also well known that the copying process is incapable
of mimicking the number of directed triangles in such networks; neither does it
have the power to explain the obsolescence of older papers. In this paper, we
propose RefOrCite, a new model that allows for copying of both the references
from (i.e., out-neighbors of) as well as the citations to (i.e., in-neighbors
of) an existing node. In contrast, the standard copying model (CP) only copies
references. While retaining its spirit, RefOrCite differs from the Forest Fire
(FF) model in ways that makes RefOrCite amenable to mean-field analysis for
degree distribution, triangle count, and densification. Empirically, RefOrCite
gives the best overall agreement with observed degree distribution, triangle
count, diameter, h-index, and the growth of citations to newer papers.
"
2787,"Scholarly journal publishing in transition: from restricted to open
  access","  While the business models used in most segments of the media industry have
been profoundly changed by the Internet surprisingly little has been changed in
the publishing of scholarly peer reviewed journals. Electronic delivery has
become the norm, but the same publishers as before are dominating the market,
selling content to subscribers. This article asks the question why Open Access
(OA) to the output of mainly publicly funded research hasn't yet become the
mainstream business model. OA implies a reversal of business logic from readers
paying for content to authors paying fro dissemination via universa free
access. The current situation is analyzed using Porter's five forces model. The
analysis demonstrates a lack of competitive pressure in this industry, leading
to so high profit levels of the leading publishers that they have yet to feel a
strong need to change the way they operate.
"
2788,"Domain-topic models with chained dimensions: charting the evolution of a
  major oncology conference (1995-2017)","  This paper presents three main contributions to the computational study of
science from bibliographic corpora. First, by combining hypergraphs and
stochastic block models, it introduces a new approach to model corpora based on
their substantive contents and integrating both temporal and other metadata
dimensions. We call this simultaneous modeling of documents and words
""domain-topic models"", and their integration with metadata their ""chained
dimensions"". Second, the paper introduces a new form of interactive map for the
exploration of hypergraph data that enables the seamless navigation of the
different dimensions, scales, and their relations, as expressed in the models,
and describes the steps to accurately read these new science maps. Third, it
introduces a new corpus that is both of great interest to current STS research
and an exemplary case for the new methodology presented here: the 1995-2017
collection of abstracts presented at ASCO, the largest annual oncology research
conference. It is shown that the new approach, named SASHIMI, is able to infer
thematic clusters in the corpus, describe them as assemblages of topics, and
detect the presence of significant temporal patterns, identifying the major
thematic transformations of oncology during the period.
"
2789,"The Demise of Single-Authored Publications in Computer Science: A
  Citation Network Analysis","  In this study, I analyze the DBLP bibliographic database to study role of
single author publications in the computer science literature between 1940 and
2019. I examine the demographics and reception by computing the population
fraction, citation statistics, and PageRank scores of single author
publications over the years. Both the population fraction and reception have
been continuously declining since the 1940s. The overall decaying trend of
single author publications is qualitatively consistent with those observed in
other scientific disciplines, though the diminution is taking place several
decades later than those in the natural sciences. Additionally, I analyze the
scope and volume of single author publications, using page length and reference
count as first-order approximations of the scope of publications. Although both
metrics on average show positive correlations with citation count, single
author papers show no significant difference in page or reference counts
compared to the rest of the publications, suggesting that there exist other
factors that impact the citations of single author publications.
"
2790,"Publishing computational research -- A review of infrastructures for
  reproducible and transparent scholarly communication","  The trend toward open science increases the pressure on authors to provide
access to the source code and data they used to compute the results reported in
their scientific papers. Since sharing materials reproducibly is challenging,
several projects have developed solutions to support the release of executable
analyses alongside articles. We reviewed 11 applications that can assist
researchers in adhering to reproducibility principles. The applications were
found through a literature search and interactions with the reproducible
research community. An application was included in our analysis if it was
actively maintained at the time the data for this paper was collected, supports
the publication of executable code and data, is connected to the scholarly
publication process. By investigating the software documentation and published
articles, we compared the applications across 19 criteria, e.g. features that
support authors in creating and readers in studying executable papers. From the
11 applications, eight allow publishers to self-host the system for free,
whereas three provide paid services. Authors can submit an executable analysis
using Jupyter Notebooks or R Markdown documents (10 applications support these
formats). All approaches provide features to assist readers in studying the
materials, e.g., one-click reproducible results or tools for manipulating the
analysis parameters. Six applications allow for modifying materials after
publication. The applications support authors to publish reproducible research
predominantly with literate programming. Concerning readers, most applications
provide user interfaces to inspect and manipulate the computational analysis.
"
2791,"The extent and drivers of gender imbalance in neuroscience reference
  lists","  Like many scientific disciplines, neuroscience has increasingly attempted to
confront pervasive gender imbalances within the field. While much of the
conversation has centered around publishing and conference participation,
recent research in other fields has called attention to the prevalence of
gender bias in citation practices. Because of the downstream effects that
citations can have on visibility and career advancement, understanding and
eliminating gender bias in citation practices is vital for addressing inequity
in a scientific community. In this study, we sought to determine whether there
is evidence of gender bias in the citation practices of neuroscientists. Using
data from five top neuroscience journals, we find that reference lists tend to
include more papers with men as first and last author than would be expected if
gender were not a factor in referencing. Importantly, we show that this
overcitation of men and undercitation of women is driven largely by the
citation practices of men, and is increasing over time as the field becomes
more diverse. We develop a co-authorship network to assess homophily in
researchers' social networks, and we find that men tend to overcite men even
when their social networks are representative. We discuss possible mechanisms
and consider how individual researchers might address these findings in their
own practices.
"
2792,Causal Impact of Web Browsing and Other Factors on Research Publications,"  In this paper, we study the causal impact of the web-search activity on the
research publication. We considered observational prospective study design,
where research activity of 267 scientists is being studied. We considered the
Poisson and negative binomial regression model for our analysis. Based on the
Akaike's Model selection criterion, we found the negative binomial regression
performs better than the Poisson regression. Detailed analysis indicates that
the higher web-search activity of 2016 related to the sci-indexed website has a
positive significant impact on the research publication of 2017. We observed
that unique collaborations of 2016 and web-search activity of 2016 have a
non-linear but significant positive impact on the research publication of 2017.
What-if analysis indicates the high web browsing activity leads to more number
of the publication. However, interestingly we see a scientist with low web
activity can be as productive as others if her/his maximum hits are the
sci-indexed journal. That is if the scientist uses web browsing only for
research-related activity, then she/he can be equally productive even if
her/his web activity is lower than fellow scientists.
"
2793,"Identifying Historical Travelogues in Large Text Corpora Using Machine
  Learning","  Travelogues represent an important and intensively studied source for
scholars in the humanities, as they provide insights into people, cultures, and
places of the past. However, existing studies rarely utilize more than a dozen
primary sources, since the human capacities of working with a large number of
historical sources are naturally limited. In this paper, we define the notion
of travelogue and report upon an interdisciplinary method that, using machine
learning as well as domain knowledge, can effectively identify German
travelogues in the digitized inventory of the Austrian National Library with F1
scores between 0.94 and 1.00. We applied our method on a corpus of 161,522
German volumes and identified 345 travelogues that could not be identified
using traditional search methods, resulting in the most extensive collection of
early modern German travelogues ever created. To our knowledge, this is the
first time such a method was implemented for the bibliographic indexing of a
text corpus on this scale, improving and extending the traditional methods in
the humanities. Overall, we consider our technique to be an important first
step in a broader effort of developing a novel mixed-method approach for the
large-scale serial analysis of travelogues.
"
2794,With Registered Reports Towards Large Scale Data Curation,"  The scale of manually validated data is currently limited by the effort that
small groups of researchers can invest for the curation of such data. Within
this paper, we propose the use of registered reports to scale the curation of
manually validated data. The idea is inspired by the mechanical turk and
replaces monetary payment with authorship of data set publication.
"
2795,A Network-Level View of Author Influence,"  I compare several network-level measures of centrality to common measures of
author reputation and influence (e.g. hindex, i10index), all taken over the
data set of papers published in 2017 at major computer systems conferences and
some controls. I hypothesize that centrality measures will correlate strongly
with the reputation and influence measures. My results confirm several expected
correlations and exhibit a few surprising absences of correlation. In
particular, there was an absence of statistically significant correlation
between degree centrality and hindex,
"
2796,"Practical method to reclassify Web of Science articles into unique
  subject categories and broad disciplines","  Classification of bibliographic items into subjects and disciplines in large
databases is essential for many quantitative science studies. The Web of
Science classification of journals into ~250 subject categories, which has
served as a basis for many studies, is known to have some fundamental problems
and several practical limitations that may affect the results from such
studies. Here we present an easily reproducible method to perform
reclassification of the Web of Science into existing subject categories and
into 14 broad areas. Our reclassification is at a level of articles, so it
preserves disciplinary differences that may exist among individual articles
published in the same journal. Reclassification also eliminates ambiguous
(multiple) categories that are found for 50% of items, and assigns a
discipline/field category to all articles that come from broad-coverage
journals such as Nature and Science. The correctness of the assigned subject
categories is evaluated manually and is found to be ~95%.
"
2797,"Domain-independent Extraction of Scientific Concepts from Research
  Articles","  We examine the novel task of domain-independent scientific concept extraction
from abstracts of scholarly articles and present two contributions. First, we
suggest a set of generic scientific concepts that have been identified in a
systematic annotation process. This set of concepts is utilised to annotate a
corpus of scientific abstracts from 10 domains of Science, Technology and
Medicine at the phrasal level in a joint effort with domain experts. The
resulting dataset is used in a set of benchmark experiments to (a) provide
baseline performance for this task, (b) examine the transferability of concepts
between domains. Second, we present two deep learning systems as baselines. In
particular, we propose active learning to deal with different domains in our
task. The experimental results show that (1) a substantial agreement is
achievable by non-experts after consultation with domain experts, (2) the
baseline system achieves a fairly high F1 score, (3) active learning enables us
to nearly halve the amount of required training data.
"
2798,"On the challenges ahead of spatial scientometrics focusing on the city
  level","  Since the mid-1970s, it has become highly acknowledged to measure and
evaluate changes in international research collaborations and the scientific
performance of institutions and countries through the prism of bibliometric and
scientometric data. Spatial bibliometrics and scientometrics (henceforward
spatial scientometrics) have traditionally focused on examining both country
and regional levels; however, in recent years, numerous spatial analyses on the
city level have been carried out. While city-level scientometric analyses have
gained popularity among policymakers and statistical/economic research
organizations, researchers in the field of bibliometrics are divided regarding
whether it is possible to observe the spatial unit 'city' through bibliometric
and scientometric tools. After systematically scrutinizing relevant studies in
the field, three major problems have been identified: 1) there is no
standardized method of how cities should be defined and how metropolitan areas
should be delineated, 2) there is no standardized method of how bibliometric
and scientometric data on the city level should be collected and processed and
3) it is not clearly defined how cities can profit from the results of
bibliometric and scientometric analysis focusing on them. This paper
investigates major challenges ahead of spatial scientometrics, focusing on the
city level and presents some possible solutions.
"
2799,"Possibility and prevention of inappropriate data manipulation in Polar
  Data Journal","  Stakeholders in the scientific field must always maintain transparency in the
process of publishing research results in journals. Unfortunately, although
research misconduct has stopped, certain forms of manipulation continue to
appear in other forms. As new techniques of scientific publishing develop,
science stakeholders need to examine the possibility of inappropriate activity
in these new platforms. The National Institute of Polar Research in Japan
launched a new data journal Polar Data Journal (PDJ) in 2017 to review the
quality of data obtained in the polar region. To maintain transparency in this
new data journal, we investigated the possibility of inappropriate data
manipulation in peer reviews before the inception of this journal. We clarified
inappropriate activity for the data in the peer review and considered
preventive measures. We designed a specific workflow for PDJ. This included two
measures: (i) the comparison of hash values in the review process and (ii) open
peer review report publishing. Using the hash value comparison, we detected two
instances of inappropriate data manipulation after the start of the journal.
This research will help improve workflow in data journals and data
repositories.
"
2800,Comparing the impact of subfields in scientific journals,"  The impact factor has been extensively used in the last years to assess
journals visibility and prestige. While the impact factor is useful to compare
journals, the specificities of subfields visibility in journals are overlooked
whenever visibility is measured only at the journal level. In this paper, we
analyze the subfields visibility in a subset of over 450,000 Physics papers. We
show that the visibility of subfields is not regular in the considered dataset.
In particular years, the variability in subfields impact factor in a journal
reached 75% of the average subfields impact factor. We also found that the
difference of subfields visibility in the same journal can be even higher than
the difference of visibility between different journals. Our results show that
subfields impact is an important factor accounting for journals visibility.
"
2801,"An Evaluation of Percentile Measures of Citation Impact, and a Proposal
  for Making Them Better","  Percentiles are statistics pointing to the standing of a paper's citation
impact relative to other papers in a given citation distribution. Percentile
Ranks (PRs) often play an important role in evaluating the impact of scholars,
institutions, and lines of study. Because PRs are so important for the
assessment of scholarly impact, and because citation practices differ greatly
across time and fields, various percentile approaches have been proposed to
time- and field-normalize citations. Unfortunately, current popular methods
often face significant problems in time- and field-normalization, including
when papers are assigned to multiple fields or have been published by more than
one unit (e.g., researchers or countries). They also face problems for
estimating citation counts (CCs) for pre-defined PRs (e.g., the 90th PR). We
offer a series of guidelines and procedures that, we argue, address these
problems and others and provide a superior means to make the use of percentile
methods more accurate and informative. In particular, we introduce two
approaches, CP-IN and CP-EX, that should be preferred in bibliometric studies
because they consider the complete citation distribution. Both approaches are
based on cumulative frequencies in percentages (CPs). The paper further shows
how bar graphs and beamplots can present PRs in a more meaningful and accurate
manner.
"
2802,Funding information in Web of Science: An updated overview,"  Despite the limitations of funding acknowledgment (FA) data in Web of Science
(WoS), studies using FA information have increased rapidly over the last
several years. Considering this WoS'recent practice of updating funding data,
this paper further investigates the characteristics and distribution of FA data
in four WoS journal citation indexes. The research reveals that FA information
coverage variances persist cross all four citation indexes by time coverage,
language and document type. Our evidence suggests an improvement in FA
information collection in humanity and social science research. Departing from
previous studies, we argue that FA text (FT) alone no longer seems an
appropriate field to retrieve and analyze funding information, since a
substantial number of documents only report funding agency or grant number
information in respective fields. Articles written in Chinese have a higher FA
presence rate than other non-English WoS publications. This updated study
concludes with a discussion of new findings and practical guidance for the
future retrieval and analysis of funded research.
"
2803,"The Archives Unleashed Project: Technology, Process, and Community to
  Improve Scholarly Access to Web Archives","  The Archives Unleashed project aims to improve scholarly access to web
archives through a multi-pronged strategy involving tool creation, process
modeling, and community building - all proceeding concurrently in
mutually-reinforcing efforts. As we near the end of our initially-conceived
three-year project, we report on our progress and share lessons learned along
the way. The main contribution articulated in this paper is a process model
that decomposes scholarly inquiries into four main activities: filter, extract,
aggregate, and visualize. Based on the insight that these activities can be
disaggregated across time, space, and tools, it is possible to generate
""derivative products"", using our Archives Unleashed Toolkit, that serve as
useful starting points for scholarly inquiry. Scholars can download these
products from the Archives Unleashed Cloud and manipulate them just like any
other dataset, thus providing access to web archives without requiring any
specialized knowledge. Over the past few years, our platform has processed over
a thousand different collections from about two hundred users, totaling over
280 terabytes of web archives.
"
2804,"Unbiased evaluation of ranking metrics reveals consistent performance in
  science and technology citation data","  Despite the increasing use of citation-based metrics for research evaluation
purposes, we do not know yet which metrics best deliver on their promise to
gauge the significance of a scientific paper or a patent. We assess 17
network-based metrics by their ability to identify milestone papers and patents
in three large citation datasets. We find that traditional
information-retrieval evaluation metrics are strongly affected by the interplay
between the age distribution of the milestone items and age biases of the
evaluated metrics. Outcomes of these metrics are therefore not representative
of the metrics' ranking ability. We argue in favor of a modified evaluation
procedure that explicitly penalizes biased metrics and allows us to reveal
metrics' performance patterns that are consistent across the datasets. PageRank
and LeaderRank turn out to be the best-performing ranking metrics when their
age bias is suppressed by a simple transformation of the scores that they
produce, whereas other popular metrics, including citation count, HITS and
Collective Influence, produce significantly worse ranking results.
"
2805,"The effect of national and international multiple affiliations on
  citation impact","  Researchers affiliated with multiple institutions are increasingly seen in
current scientific environment. In this paper we systematically analyze the
multi-affiliated authorship and its effect on citation impact, with focus on
the scientific output of research collaboration. By considering the nationality
of each institutions, we further differentiate the national multi-affiliated
authorship and international multi-affiliated authorship and reveal their
different patterns across disciplines and countries. We observe a large share
of publications with multi-affiliated authorship (45.6%) in research
collaboration, with a larger share of publications containing national
multi-affiliated authorship in medicine related and biology related
disciplines, and a larger share of publications containing international type
in Space Science, Physics and Geosciences. To a country-based view, we
distinguish between domestic and foreign multi-affiliated authorship to a
specific country. Taking G7 and BRICS countries as samples from different S&T
level, we find that the domestic national multi-affiliated authorship relate to
more on citation impact for most disciplines of G7 countries, while domestic
international multi-affiliated authorships are more positively influential for
most BRICS countries.
"
2806,Measuring Diversity of Artificial Intelligence Conferences,"  The lack of diversity of the Artificial Intelligence (AI) field is nowadays a
concern, and several initiatives such as funding schemes and mentoring programs
have been designed to fight against it. However, there is no indication on how
these initiatives actually impact AI diversity in the short and long term. This
work studies the concept of diversity in this particular context and proposes a
small set of diversity indicators (i.e. indexes) of AI scientific events. These
indicators are designed to quantify the lack of diversity of the AI field and
monitor its evolution. We consider diversity in terms of gender, geographical
location and business (understood as the presence of academia versus industry).
We compute these indicators for the different communities of a conference:
authors, keynote speakers and organizing committee. From these components we
compute a summarized diversity indicator for each AI event. We evaluate the
proposed indexes for a set of recent major AI conferences and we discuss their
values and limitations.
"
2807,The evolution of knowledge within and across fields in modern physics,"  The exchange of knowledge across different areas and disciplines plays a key
role in the process of knowledge creation, and can stimulate innovation and the
emergence of new fields. We develop here a quantitative framework to extract
significant dependencies among scientific disciplines and turn them into a
time-varying network whose nodes are the different fields, while the weighted
links represent the flow of knowledge from one field to another at a given
period of time. Drawing on a comprehensive data set on scientific production in
modern physics and on the patterns of citations between articles published in
the various fields in the last thirty years, we are then able to map, over
time, how the ideas developed in a given field in a certain time period have
influenced later discoveries in the same field or in other fields. The analysis
of knowledge flows internal to each field displays a remarkable variety of
temporal behaviours, with some fields of physics showing to be more
self-referential than others. The temporal networks of knowledge exchanges
across fields reveal cases of one field continuously absorbing knowledge from
another field in the entire observed period, pairs of fields mutually
influencing each other, but also cases of evolution from absorbing to mutual or
even to back-nurture behaviors.
"
2808,"The stability of Twitter metrics: A study on unavailable Twitter
  mentions of scientific publications","  This paper investigates the stability of Twitter counts of scientific
publications over time. For this, we conducted an analysis of the availability
statuses of over 2.6 million Twitter mentions received by the 1,154 most
tweeted scientific publications recorded by Altmetric.com up to October 2017.
Results show that of the Twitter mentions for these highly tweeted
publications, about 14.3% have become unavailable by April 2019. Deletion of
tweets by users is the main reason for unavailability, followed by suspension
and protection of Twitter user accounts. This study proposes two measures for
describing the Twitter dissemination structures of publications: Degree of
Originality (i.e., the proportion of original tweets received by a paper) and
Degree of Concentration (i.e., the degree to which retweets concentrate on a
single original tweet). Twitter metrics of publications with relatively low
Degree of Originality and relatively high Degree of Concentration are observed
to be at greater risk of becoming unstable due to the potential disappearance
of their Twitter mentions. In light of these results, we emphasize the
importance of paying attention to the potential risk of unstable Twitter
counts, and the significance of identifying the different Twitter dissemination
structures when studying the Twitter metrics of scientific publications.
"
2809,"Neural Embeddings of Scholarly Periodicals Reveal Complex Disciplinary
  Organizations","  Understanding the structure of knowledge domains has been one of the
foundational challenges in the science of science. Although there have been a
series of studies on the self-organized structure of knowledge domains and
their relationships, creating rich, coherent, and quantitative representation
frameworks is still an open challenge. Meanwhile, neural embedding methods,
which learn continuous vector representations of entities by using neural
networks and contextual information, are emerging as a powerful representation
framework that can encode nuanced semantic relationships into geometric ones.
Here, we propose a neural embedding technique that leverages the information
contained in the paper citation network to obtain continuous representations of
scientific periodicals. We demonstrate that our embeddings encode nuanced
relationships between periodicals as well as the complex disciplinary structure
of science, even allowing us to make cross-disciplinary analogies between
periodicals. Furthermore, we show that the embeddings capture meaningful ""axes""
that encompass all disciplines in the knowledge domains, such as an axis from
""soft"" to ""hard"" sciences or from ""social"" to ""biological"" sciences, which
allow us to quantitatively ground a periodical on a given spectrum. Using this
new capacity, we test the hypothesis of the hierarchy of the sciences, showing
that, in most disciplines such as Social Sciences and Life Sciences, most
widely cited papers tend to appear in ""harder"" periodicals. Our framework may
offer novel quantification methods in science of science, which may in turn
facilitate the study of how knowledge is created and organized.
"
2810,"Referencing Source Code Artifacts: a Separate Concern in Software
  Citation","  Among the entities involved in software citation, software source code
requires special attention, due to the role it plays in ensuring scientific
reproducibility. To reference source code we need identifiers that are not only
unique and persistent, but also support \emph{integrity} checking
intrinsically. Suitable identifiers must guarantee that denotedobjects will
always stay the same, without relying on external third parties and
administrative processes. We analyze the role of identifiers for digital
objects (IDOs), whose properties are different from, and complementary to,
those of the various digital identifiers of objects (DIOs) that are today
popular building blocks of software and data citation toolchains.We argue that
both kinds of identifiers are needed and detail the syntax, semantics, and
practical implementation of the persistent identifiers (PIDs) adopted by the
Software Heritage project to reference billions of softwaresource code
artifacts such as source code files, directories, and commits.
"
2811,"Navigation-Based Candidate Expansion and Pretrained Language Models for
  Citation Recommendation","  Citation recommendation systems for the scientific literature, to help
authors find papers that should be cited, have the potential to speed up
discoveries and uncover new routes for scientific exploration. We treat this
task as a ranking problem, which we tackle with a two-stage approach: candidate
generation followed by re-ranking. Within this framework, we adapt to the
scientific domain a proven combination based on ""bag of words"" retrieval
followed by re-scoring with a BERT model. We experimentally show the effects of
domain adaptation, both in terms of pretraining on in-domain data and
exploiting in-domain vocabulary. In addition, we introduce a novel
navigation-based document expansion strategy to enrich the candidate documents
processed by our neural models. On three different collections from different
scientific disciplines, we achieve the best-reported results in the citation
recommendation task.
"
2812,"Towards a Framework for the Design, Implementation and Reporting of
  Methodology Scoping Reviews","  Background: In view of the growth of published papers, there is an increasing
need for studies that summarise scientific research. An increasingly common
review is a 'Methodology scoping review', which provides a summary of existing
analytical methods, techniques and software, proposed or applied in research
articles, which address an analytical problem or further an analytical
approach. However, guidelines for their design, implementation and reporting
are limited.
  Methods: Drawing on the experiences of the authors, which were consolidated
through a series of face-to-face workshops, we summarise the challenges
inherent in conducting a methodology scoping review and offer suggestions of
best practice to promote future guideline development.
  Results: We identified three challenges of conducting a methodology scoping
review. First, identification of search terms; one cannot usually define the
search terms a priori and the language used for a particular method can vary
across the literature. Second, the scope of the review requires careful
consideration since new methodology is often not described (in full) within
abstracts. Third, many new methods are motivated by a specific clinical
question, where the methodology may only be documented in supplementary
materials. We formulated several recommendations that build upon existing
review guidelines. These recommendations ranged from an iterative approach to
defining search terms through to screening and data extraction processes.
  Conclusion: Although methodology scoping reviews are an important aspect of
research, there is currently a lack of guidelines to standardise their design,
implementation and reporting. We recommend a wider discussion on this topic.
"
2813,"Twenty Years of Network Science: A Bibliographic and Co-Authorship
  Network Analysis","  Two decades ago three pioneering papers turned the attention to complex
networks and initiated a new era of research, establishing an interdisciplinary
field called network science. Namely, these highly-cited seminal papers were
written by Watts&Strogatz, Barab\'asi&Albert, and Girvan&Newman on small-world
networks, on scale-free networks and on the community structure of complex
networks, respectively. In the past 20 years - due to the multidisciplinary
nature of the field - a diverse but not divided network science community has
emerged. In this paper, we investigate how this community has evolved over time
with respect to speed, diversity and interdisciplinary nature as seen through
the growing co-authorship network of network scientists (here the notion refers
to a scholar with at least one paper citing at least one of the three
aforementioned milestone papers). After providing a bibliographic analysis of
31,763 network science papers, we construct the co-authorship network of 56,646
network scientists and we analyze its topology and dynamics. We shed light on
the collaboration patterns of the last 20 years of network science by
investigating numerous structural properties of the co-authorship network and
by using enhanced data visualization techniques. We also identify the most
central authors, the largest communities, investigate the spatiotemporal
changes, and compare the properties of the network to scientometric indicators.
"
2814,Determining crucial factors for the popularity of scientific articles,"  Using a set of over 70.000 records from PLOS One journal consisting of 37
lexical, sentiment and bibliographic variables we perform analysis backed with
machine learning methods to predict the class of popularity of scientific
papers defined by the number of times they have been viewed. Our study shows
correlations among the features and recovers a threshold for the number of
views that results in the best prediction results in terms of Matthew's
correlation coefficient. Moreover, by creating a variable importance plot for
random forest classifier, we are able to reduce the number of features while
keeping similar predictability and determine crucial factors responsible for
the popularity.
"
2815,"Bibliometric-enhanced Information Retrieval 10th Anniversary Workshop
  Edition","  The Bibliometric-enhanced Information Retrieval workshop series (BIR) was
launched at ECIR in 2014 \cite{MayrEtAl2014} and it was held at ECIR each year
since then. This year we organize the 10th iteration of BIR. The workshop
series at ECIR and JCDL/SIGIR tackles issues related to academic search, at the
crossroads between Information Retrieval, Natural Language Processing and
Bibliometrics. In this overview paper, we summarize the past workshops, present
the workshop topics for 2020 and reflect on some future steps for this workshop
series.
"
2816,"The role of geographic proximity in knowledge diffusion, measured by
  citations to scientific literature","  This paper analyses the influence of geographic distance on knowledge flows,
measured through citations to scientific publications. Previous works using the
same approach are limited to single disciplines. In this study, we analyse the
Italian scientific production in all disciplines matured in the period
2010-2012. To calculate the geographic distances between citing and cited
publications, each one is associated with a ""prevalent"" territory on the basis
of the authors' affiliations. The results of the application of a gravity
model, estimated using ordinary least squares regression, show that despite the
spread of IT, geographic distance continues to be an influential factor in the
process of knowledge flows between territories. In particular, the analysis
reveals that the effect of geographic distance on knowledge flows is
significant at the national level, not negligible at the continental level, but
completely irrelevant at the intercontinental level.
"
2817,"Experiments with Different Indexing Techniques for Text Retrieval tasks
  on Gujarati Language using Bag of Words Approach","  This paper presents results of various experiments carried out to improve
text retrieval of gujarati text documents. Text retrieval involves searching
and ranking of text documents for a given set of query terms. We have tested
various retrieval models that uses bag-of-words approach. Bag-of-words approach
is a traditional approach that is being used till date where the text document
is represented as collection of words. Measures like frequency count, inverse
document frequency etc. are used to signify and rank relevant documents for
user queries. Different ranking models have been used to quantify ranking
performance using the metric of mean average precision. Gujarati is a
morphologically rich language, we have compared techniques like stop word
removal, stemming and frequent case generation against baseline to measure the
improvements in information retrieval tasks. Most of the techniques are
language dependent and requires development of language specific tools. We used
plain unprocessed word index as the baseline, we have seen significant
improvements in comparison of MAP values after applying different indexing
techniques when compared to the baseline.
"
2818,"A tale of two databases: The use of Web of Science and Scopus in
  academic papers","  Web of Science and Scopus are two world-leading and competing citation
databases. By using the Science Citation Index Expanded and Social Sciences
Citation Index, this paper conducts a comparative, dynamic, and empirical study
focusing on the use of Web of Science (WoS) and Scopus in academic papers
published during 2004 and 2018. This brief communication reveals that although
both Web of Science and Scopus are increasingly used in academic papers, Scopus
as a new-comer is really challenging the dominating role of WoS. Researchers
from more and more countries/regions and knowledge domains are involved in the
use of these two databases. Even though the main producers of related papers
are developed economies, some developing economies such as China, Brazil and
Iran also act important roles but with different patterns in the use of these
two databases. Both two databases are widely used in meta-analysis related
studies especially for researchers in China. Health/medical science related
domains and the traditional Information Science & Library Science field stand
out in the use of citation databases.
"
2819,"Discovering Mathematical Objects of Interest -- A Study of Mathematical
  Notations","  Mathematical notation, i.e., the writing system used to communicate concepts
in mathematics, encodes valuable information for a variety of information
search and retrieval systems. Yet, mathematical notations remain mostly
unutilized by today's systems. In this paper, we present the first in-depth
study on the distributions of mathematical notation in two large scientific
corpora: the open access arXiv (2.5B mathematical objects) and the mathematical
reviewing service for pure and applied mathematics zbMATH (61M mathematical
objects). Our study lays a foundation for future research projects on
mathematical information retrieval for large scientific corpora. Further, we
demonstrate the relevance of our results to a variety of use-cases. For
example, to assist semantic extraction systems, to improve scientific search
engines, and to facilitate specialized math recommendation systems. The
contributions of our presented research are as follows: (1) we present the
first distributional analysis of mathematical formulae on arXiv and zbMATH; (2)
we retrieve relevant mathematical objects for given textual search queries
(e.g., linking $P_{n}^{(\alpha, \beta)}\!\left(x\right)$ with `Jacobi
polynomial'); (3) we extend zbMATH's search engine by providing relevant
mathematical formulae; and (4) we exemplify the applicability of the results by
presenting auto-completion for math inputs as the first contribution to math
recommendation systems. To expedite future research projects, we have made
available our source code and data.
"
2820,"Chemistry research in India in a global perspective- A scientometrics
  profile","  Papers from India are cited 14.68 times on average compared to cites per
paper of 45.34 for Singapore, 30.47 for USA, 23.12 for China, 26.51 for the UK,
21.77 for South Korea and 24.77 for Germany. Less than 39% of papers from India
are found in quartile 1 (high impact factor) journals, compared to 53.6% for
China and 53.8% for South Korea. Percent share of papers in quartile 1 journals
from India is lower than that for the world for all of chemistry and for each
one of the eight categories, viz. analytical, applied, inorganic & nuclear,
medicinal, multidisciplinary, organic, physical and electrochemistry whether
one considers data for the entire five-year period or for 2015 alone. About 20%
of Indian chemistry papers are in collaboration with international coauthors.
Researchers from only 160 Indian institutions have published at least 100
papers (compared to 362 in USA and 399 in China) and these include 67 state, 14
central and 11 private universities, 27 institutions under the Ministry of
Human Resource Development, 20 CSIR laboratories, seven Department of Atomic
Energy institutions, and seven Department of Science & Technology institutions.
About 40% of all Indian chemistry papers have come from public universities.
Only three Indian institutions, viz Bhabha Atomic Research Centre, Indian
Institute of Science and Indian Institute of Chemical Technology, have
published more than 2,000 papers. None of the Indian universities has performed
as well as leading Asian universities. Amrita Vishwa Vidyapeetham, a small
institution with less than 200 papers, has performed reasonably well.
"
2821,Testing of Support Tools for Plagiarism Detection,"  There is a general belief that software must be able to easily do things that
humans find difficult. Since finding sources for plagiarism in a text is not an
easy task, there is a wide-spread expectation that it must be simple for
software to determine if a text is plagiarized or not. Software cannot
determine plagiarism, but it can work as a support tool for identifying some
text similarity that may constitute plagiarism. But how well do the various
systems work? This paper reports on a collaborative test of 15 web-based
text-matching systems that can be used when plagiarism is suspected. It was
conducted by researchers from seven countries using test material in eight
different languages, evaluating the effectiveness of the systems on
single-source and multi-source documents. A usability examination was also
performed. The sobering results show that although some systems can indeed help
identify some plagiarized content, they clearly do not find all plagiarism and
at times also identify non-plagiarized material as problematic.
"
2822,"Science through Wikipedia: A novel representation of open knowledge
  through co-citation networks","  This study provides an overview of science from the Wikipedia perspective. A
methodology has been established for the analysis of how Wikipedia editors
regard science through their references to scientific papers. The method of
co-citation has been adapted to this context in order to generate Pathfinder
networks (PFNET) that highlight the most relevant scientific journals and
categories, and their interactions in order to find out how scientific
literature is consumed through this open encyclopaedia. In addition to this,
their obsolescence has been studied through Price index. A total of 1 433 457
references available at Altmetric.com have been initially taken into account.
After pre-processing and linking them to the data from Elsevier's CiteScore
Metrics the sample was reduced to 847 512 references made by 193 802 Wikipedia
articles to 598 746 scientific articles belonging to 14 149 journals indexed in
Scopus. As highlighted results we found a significative presence of ""Medicine""
and ""Biochemistry, Genetics and Molecular Biology"" papers and that the most
important journals are multidisciplinary in nature, suggesting also that
high-impact factor journals were more likely to be cited. Furthermore, only
13.44% of Wikipedia citations are to Open Access journals.
"
2823,"Unveiling the research landscape of Sustainable Development Goals and
  their inclusion in Higher Education Institutions and Research Centers: major
  trends in 2000-2017","  Sustainable Development Goals are the blueprint to achieve a better and more
sustainable future for society. Its legacy is linked with the Millennium
Development Goals, set up in 2000. A bibliometric analysis was conducted to 1)
measure ""core"" research output from 2000-2017, with the aim to map the global
research of sustainability goals, 2) describe thematic specialization based on
keywords co-occurrence analysis and strongest citation burst, 3) present a
methodology to classify scientific output (based on an ad-hoc glossary) and
assess SDGs interconnections.
  Sustainability goals publications (core+expand based on direct citations)
were identified in-house CWTS Web of Science by using search terms in titles,
abstracts, and keywords. 25,299 bibliographic records were analyzed, from which
21,653 (85.59%) are from HEIs and research centres (RC). The purpose of this
paper is to analyze the role of these organizations in sustainability research.
The findings reveal the increasing participation of these organizations in this
research (660 institutions in 2000-2005 to 1744 institutions involved in
2012-2017). In terms of specialization, some institutions present a higher
production and specialization on the topic (e.g., London School of Hygiene &
Tropical Medicine and World Health Organization); however, others present less
production but higher specialization (e.g., Stockholm Environment Institute).
Regarding the topics, health (especially in developing countries), women and
socio-economic aspects are the most prominent ones. Moreover, it is observed
the interlinked nature of SDGs between some SDGs in research output (e.g.,
SDG11 and SDG3). This study provides important orientation for HEIs and RCs in
terms of Research, Development and Innovation (R&D+i) to respond to major
societal challenges and could be useful for the policymakers in order to
promote the research agenda on this topic.
"
2824,"menoci: Lightweight Extensible Web Portal enabling FAIR Data Management
  for Biomedical Research Projects","  Background: Biomedical research projects deal with data management
requirements from multiple sources like funding agencies' guidelines, publisher
policies, discipline best practices, and their own users' needs. We describe
functional and quality requirements based on many years of experience
implementing data management for the CRC 1002 and CRC 1190. A fully equipped
data management software should improve documentation of experiments and
materials, enable data storage and sharing according to the FAIR Guiding
Principles while maximizing usability, information security, as well as
software sustainability and reusability. Results: We introduce the modular web
portal software menoci for data collection, experiment documentation, data
publication, sharing, and preservation in biomedical research projects. Menoci
modules are based on the Drupal content management system which enables
lightweight deployment and setup, and creates the possibility to combine
research data management with a customisable project home page or collaboration
platform. Conclusions: Management of research data and digital research
artefacts is transforming from individual researcher or groups best practices
towards project- or organisation-wide service infrastructures. To enable and
support this structural transformation process, a vital ecosystem of open
source software tools is needed. Menoci is a contribution to this ecosystem of
research data management tools that is specifically designed to support
biomedical research projects.
"
2825,HybridCite: A Hybrid Model for Context-Aware Citation Recommendation,"  Citation recommendation systems aim to recommend citations for either a
complete paper or a small portion of text called a citation context. The
process of recommending citations for citation contexts is called local
citation recommendation and is the focus of this paper. Firstly, we develop
citation recommendation approaches based on embeddings, topic modeling, and
information retrieval techniques. We combine, for the first time to the best of
our knowledge, the best-performing algorithms into a semi-genetic hybrid
recommender system for citation recommendation. We evaluate the single
approaches and the hybrid approach offline based on several data sets, such as
the Microsoft Academic Graph (MAG) and the MAG in combination with arXiv and
ACL. We further conduct a user study for evaluating our approaches online. Our
evaluation results show that a hybrid model containing embedding and
information retrieval-based components outperforms its individual components
and further algorithms by a large margin.
"
2826,"Knowledge and Social Relatedness Shape Research Portfolio
  Diversification","  Scientific discovery is shaped by scientists' choices and thus by their
career patterns. The increasing knowledge required to work at the frontier of
science makes it harder for an individual to embark on unexplored paths. Yet
collaborations can reduce learning costs -- albeit at the expense of increased
coordination costs. In this article, we use data on the publication histories
of a very large sample of physicists to measure the effects of knowledge and
social relatedness on their diversification strategies. Using bipartite
networks, we compute a measure of topics similarity and a measure of social
proximity. We find that scientists' strategies are not random, and that they
are significantly affected by both. Knowledge relatedness across topics
explains $\approx 10\%$ of logistic regression deviances and social relatedness
as much as $\approx 30\%$, suggesting that science is an eminently social
enterprise: when scientists move out of their core specialization, they do so
through collaborations. Interestingly, we also find a significant negative
interaction between knowledge and social relatedness, suggesting that the
farther scientists move from their specialization, the more they rely on
collaborations. Our results provide a starting point for broader quantitative
analyses of scientific diversification strategies, which could also be extended
to the domain of technological innovation -- offering insights from a
comparative and policy perspective.
"
2827,Citation Recommendation: Approaches and Datasets,"  Citation recommendation describes the task of recommending citations for a
given text. Due to the overload of published scientific works in recent years
on the one hand, and the need to cite the most appropriate publications when
writing scientific texts on the other hand, citation recommendation has emerged
as an important research topic. In recent years, several approaches and
evaluation data sets have been presented. However, to the best of our
knowledge, no literature survey has been conducted explicitly on citation
recommendation. In this article, we give a thorough introduction into automatic
citation recommendation research. We then present an overview of the approaches
and data sets for citation recommendation and identify differences and
commonalities using various dimensions. Last but not least, we shed light on
the evaluation methods, and outline general challenges in the evaluation and
how to meet them. We restrict ourselves to citation recommendation for
scientific publications, as this document type has been studied the most in
this area. However, many of the observations and discussions included in this
survey are also applicable to other types of text, such as news articles and
encyclopedic articles.
"
2828,"The h index for research assessment: Simple and popular, but shown by
  mathematical analysis to be inconsistent and misleading","  Citation distributions are lognormal. We use 30 lognormally distributed
synthetic series of numbers that simulate real series of citations to
investigate the consistency of the h index. Using the lognormal cumulative
distribution function, the equation that defines the h index can be formulated;
this equation shows that h has a complex dependence on the number of papers
(N). Specifically, after a certain limit, an increase of N has a very small
effect on h, which contradicts a rational expectation. We also investigate the
correlation between h and the number of papers exceeding various citation
thresholds, from 5 to 500 citations. The best correlation, for 100 citations,
is not linear, and numerous data points deviate from the general trend. The
size-independent indicator h/N shows no correlation with the probability of
publishing a paper exceeding any of the citation thresholds. In contrast with
the h index, the total number of citations in the series shows a high linear
correlation with the number of papers exceeding the thresholds of 10 and 50
citations. Dividing by N, the mean number of citations correlates with the
probability of publishing a paper that exceeds any level of citations. The
dependence is never linear, but for the thresholds of 20 and 30 citations, the
deviation from linearity is low. Thus, in synthetic series, the number of
citations and the mean number of citations are much better indicators of
research performance than h and h/N. We discuss that this conclusion can be
extended to real citation series.
"
2829,"Identifying the Development and Application of Artificial Intelligence
  in Scientific Text","  We describe a strategy for identifying the universe of research publications
relevant to the application and development of artificial intelligence. The
approach leverages the arXiv corpus of scientific preprints, in which authors
choose subject tags for their papers from a set defined by editors. We compose
a functional definition of AI relevance by learning these subjects from paper
metadata, and then inferring the arXiv-subject labels of papers in larger
corpora: Clarivate Web of Science, Digital Science Dimensions, and Microsoft
Academic Graph. This yields predictive classification $F_1$ scores between .75
and .86 for Natural Language Processing (cs.CL), Computer Vision (cs.CV), and
Robotics (cs.RO). For a single model that learns these and four other
AI-relevant subjects (cs.AI, cs.LG, stat.ML, and cs.MA), we see precision of
.83 and recall of .85. We evaluate the out-of-domain performance of our
classifiers against other sources of topic information and predictions from
alternative methods. We find that a supervised solution can generalize to
identify publications that belong to the high-level fields of study represented
on arXiv. This offers a method for identifying AI-relevant publications that
updates at the pace of research output, without reliance on subject-matter
experts for query development or labeling.
"
2830,"Are nationally oriented journals indexed in Scopus becoming more
  international? The effect of publication language and access modality","  An exploratory, descriptive analysis is presented of the national orientation
of scientific, scholarly journals as reflected in the affiliations of
publishing or citing authors. It calculates for journals covered in Scopus an
Index of National Orientation (INO), and analyses the distribution of INO
values across disciplines and countries, and the correlation between INO values
and journal impact factors. The study did not find solid evidence that journal
impact factors are good measures of journal internationality in terms of the
geographical distribution of publishing or citing authors, as the relationship
between a journal's national orientation and its citation impact is found to be
inverse U-shaped. In addition, journals publishing in English are not
necessarily internationally oriented in terms of the affiliations of publishing
or citing authors; in social sciences and humanities also USA has their
nationally oriented literatures. The paper examines the extent to which
nationally oriented journals entering Scopus in earlier years, have become in
recent years more international. It is found that in the study set about 40 per
cent of such journals does reveal traces of internationalization, while the use
of English as publication language and an Open Access (OA) status are important
determinants.
"
2831,MODMA dataset: a Multi-modal Open Dataset for Mental-disorder Analysis,"  According to the World Health Organization, the number of mental disorder
patients, especially depression patients, has grown rapidly and become a
leading contributor to the global burden of disease. However, the present
common practice of depression diagnosis is based on interviews and clinical
scales carried out by doctors, which is not only labor-consuming but also
time-consuming. One important reason is due to the lack of physiological
indicators for mental disorders. With the rising of tools such as data mining
and artificial intelligence, using physiological data to explore new possible
physiological indicators of mental disorder and creating new applications for
mental disorder diagnosis has become a new research hot topic. However, good
quality physiological data for mental disorder patients are hard to acquire. We
present a multi-modal open dataset for mental-disorder analysis. The dataset
includes EEG and audio data from clinically depressed patients and matching
normal controls. All our patients were carefully diagnosed and selected by
professional psychiatrists in hospitals. The EEG dataset includes not only data
collected using traditional 128-electrodes mounted elastic cap, but also a
novel wearable 3-electrode EEG collector for pervasive applications. The
128-electrodes EEG signals of 53 subjects were recorded as both in resting
state and under stimulation; the 3-electrode EEG signals of 55 subjects were
recorded in resting state; the audio data of 52 subjects were recorded during
interviewing, reading, and picture description. We encourage other researchers
in the field to use it for testing their methods of mental-disorder analysis.
"
2832,"Dr. Strangelove or: how I learned to stop worrying and love the
  citations","  Citations are getting more and more important in the career of a researcher.
But how to use them in the best possible way? This is a satirical paper,
showing a bad trend currently happening in citation trends, due to intensive
use of citation metrics. I am putting this on the arXiv and on Researchgate.
Should you be interested to publish this paper on a journal of which you are
editor, let me know.
"
2833,"Author Name Disambiguation on Heterogeneous Information Network with
  Adversarial Representation Learning","  Author name ambiguity causes inadequacy and inconvenience in academic
information retrieval, which raises the necessity of author name disambiguation
(AND). Existing AND methods can be divided into two categories: the models
focusing on content information to distinguish whether two papers are written
by the same author, the models focusing on relation information to represent
information as edges on the network and to quantify the similarity among
papers. However, the former requires adequate labeled samples and informative
negative samples, and are also ineffective in measuring the high-order
connections among papers, while the latter needs complicated feature
engineering or supervision to construct the network. We propose a novel
generative adversarial framework to grow the two categories of models together:
(i) the discriminative module distinguishes whether two papers are from the
same author, and (ii) the generative module selects possibly homogeneous papers
directly from the heterogeneous information network, which eliminates the
complicated feature engineering. In such a way, the discriminative module
guides the generative module to select homogeneous papers, and the generative
module generates high-quality negative samples to train the discriminative
module to make it aware of high-order connections among papers. Furthermore, a
self-training strategy for the discriminative module and a random walk based
generating algorithm are designed to make the training stable and efficient.
Extensive experiments on two real-world AND benchmarks demonstrate that our
model provides significant performance improvement over the state-of-the-art
methods.
"
2834,Universality of citation distributions and its explanation,"  Universality or near-universality of citation distributions was found
empirically a decade ago but its theoretical justification has been lacking so
far. Here, we systematically study citation distributions for different
disciplines in order to characterize this putative universality and to
understand it theoretically. Using our calibrated model of citation dynamics,
we find microscopic explanation of the universality of citation distributions
and explain deviations therefrom. We demonstrate that citation count of the
paper is determined, on the one hand, by its fitness -- the attribute which,
for most papers, is set at the moment of publication. The fitness distributions
for different disciplines are very similar and can be approximated by the
log-normal distribution. On another hand, citation dynamics of a paper is
related to the mechanism by which the knowledge about it spreads in the
scientific community. This viral propagation is non-universal and
discipline-specific. Thus, universality of citation distributions traces its
origin to the fitness distribution, while deviations from universality are
associated with the discipline-specific citation dynamics of papers.
"
2835,Uncited papers are not unread,"  We study citation dynamics of the Physics, Economics, and Mathematics papers
published in 1984 and focus on the fraction of uncited papers in these three
collections. Our model of citation dynamics, which considers citation process
as an inhomogeneous Poisson process, captures this uncitedness ratio fairly
well. It should be noted that all parameters and variables in our model are
related to citations and their dynamics, while uncited papers appear as a
byproduct of the citation process and this is the Poisson statistics which
makes the cited and uncited papers inseparable. This indicates that the most
part of uncited papers constitute the inherent part of the scientific
enterprise, namely, uncited papers are not unread.
"
2836,"A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility","  Data makes science possible. Sharing data improves visibility, and makes the
research process transparent. This increases trust in the work, and allows for
independent reproduction of results. However, a large proportion of data from
published research is often only available to the original authors. Despite the
obvious benefits of sharing data, and scientists' advocating for the importance
of sharing data, most advice on sharing data discusses its broader benefits,
rather than the practical considerations of sharing. This paper provides
practical, actionable advice on how to actually share data alongside research.
The key message is sharing data falls on a continuum, and entering it should
come with minimal barriers.
"
2837,"Who pays? Comparing cost sharing models for a Gold Open Access
  publication environment","  The article focuses on possible financial effects of the transformation
towards Gold Open Access publishing based on article processing charges and
studies an aspect that has so far been overlooked: Do possible cost sharing
models lead to the same overall expenses or do they result in different
financial burdens for the research institutions involved? It takes the current
state of Gold OA publishing as a starting point, develops five possible models
of attributing costs based on different author roles, number of authors and
author-address-combinations. The analysis of the distributional effects of the
application of the different models shows that all models result in similar
expenditures for the overwhelming majority of institutions. Still, there are
some research institutions where the difference between most and least
expensive model results in a considerable amount of money. Given that the model
calculation only considers publications that are Open Access and where all
authors come from Germany, it is likely that different cost sharing models will
become an issue in the debate on how to shoulder a possible large scale
transformation towards Open Access based on publication fees.
"
2838,"Gender Disparities in International Research Collaboration: A
  Large-scale Bibliometric Study of 25,000 University Professors","  In this research, we examine the hypothesis that gender disparities in
international research collaboration differ by collaboration intensity,
academic position, age, and academic discipline. The following are the major
findings: (1) while female scientists exhibit a higher rate of general,
national, and institutional collaboration, male scientists exhibit a higher
rate of international collaboration, a finding critically important in
explaining gender disparities in impact, productivity, and access to large
grants. (2) An aggregated picture of gender disparities hides a more nuanced
cross-disciplinary picture of them. (3) An analysis of international research
collaboration at three separate intensity levels (low, medium, and high)
reveals that male scientists dominate in international collaboration at each
level. However, at each level, there are specific disciplines in which females
collaborate more than males. Further (4), gender disparities are clearly linked
with age. Until about the age of 40, they are marginal and then they begin to
grow. Finally, we estimate the odds of being involved in international research
collaboration using an analytical linear logistic model. The examined sample
includes 25,463 internationally productive Polish university professors from 85
universities, grouped into 27 disciplines, who authored 159,943 Scopus-indexed
articles.
"
2839,"The STEM-ECR Dataset: Grounding Scientific Entity References in STEM
  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources","  We introduce the STEM (Science, Technology, Engineering, and Medicine)
Dataset for Scientific Entity Extraction, Classification, and Resolution,
version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to
provide a benchmark for the evaluation of scientific entity extraction,
classification, and resolution tasks in a domain-independent fashion. It
comprises abstracts in 10 STEM disciplines that were found to be the most
prolific ones on a major publishing platform. We describe the creation of such
a multidisciplinary corpus and highlight the obtained findings in terms of the
following features: 1) a generic conceptual formalism for scientific entities
in a multidisciplinary scientific context; 2) the feasibility of the
domain-independent human annotation of scientific entities under such a generic
formalism; 3) a performance benchmark obtainable for automatic extraction of
multidisciplinary scientific entities using BERT-based neural models; 4) a
delineated 3-step entity resolution procedure for human annotation of the
scientific entities via encyclopedic entity linking and lexicographic word
sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic
links and lexicographic senses for our entities. Our findings cumulatively
indicate that human annotation and automatic learning of multidisciplinary
scientific concepts as well as their semantic disambiguation in a wide-ranging
setting as STEM is reasonable.
"
2840,The dominance of big teams in China's scientific output,"  Modern science is dominated by scientific productions from teams. A recent
finding shows that teams with both large and small sizes are essential in
research, prompting us to analyze the extent to which a country's scientific
work is carried out by big/small teams. Here, using over 26 million
publications from Web of Science, we find that China's research output is more
dominated by big teams than the rest of the world, which is particularly the
case in fields of natural science. Despite the global trend that more papers
are done by big teams, China's drop in small team output is much steeper. As
teams in China shift from small to large size, the team diversity that is
essential for innovative works does not increase as much as that in other
countries. Using the national average as the baseline, we find that the
National Natural Science Foundation of China (NSFC) supports fewer small team
works than the National Science Foundation of U.S. (NSF) does, implying that
big teams are more preferred by grant agencies in China. Our finding provides
new insights into the concern of originality and innovation in China, which
urges a need to balance small and big teams.
"
2841,"ASMD: an automatic framework for compiling multimodal datasets with
  audio and scores","  This paper describes an open-source Python framework for handling datasets
for music processing tasks, built with the aim of improving the reproducibility
of research projects in music computing and assessing the generalization
abilities of machine learning models. The framework enables the automatic
download and installation of several commonly used datasets for multimodal
music processing. Specifically, we provide a Python API to access the datasets
through Boolean set operations based on particular attributes, such as
intersections and unions of composers, instruments, and so on. The framework is
designed to ease the inclusion of new datasets and the respective ground-truth
annotations so that one can build, convert, and extend one's own collection as
well as distribute it by means of a compliant format to take advantage of the
API. All code and ground-truth are released under suitable open licenses.
"
2842,"Impact of JD Bernal Thoughts in the Science of Science upon China:
  Implications for Quantitative Studies of Science Today","  John Desmond Bernal (1901-1970) was one of the most eminent scientists in
molecular biology, and also regarded as the founding father of the Science of
Science. His book The Social Function of Science laid the theoretical
foundations for the discipline. In this article, we summarize four chief
characteristics of his ideas in the Science of Science: the socio-historical
perspective, theoretical models, qualitative and quantitative approaches, and
studies of science planning and policy. China has constantly reformed its
scientific and technological system based on research evidence of the Science
of Science. Therefore, we analyze the impact of Bernal Science-of-Science
thoughts on the development of Science of Science in China, and discuss how
they might be usefully taken still further in quantitative studies of science.
"
2843,A Quantitative History of A.I. Research in the United States and China,"  Motivated by recent interest in the status and consequences of competition
between the U.S. and China in A.I. research, we analyze 60 years of abstract
data scraped from Scopus to explore and quantify trends in publications on A.I.
topics from institutions affiliated with each country. We find the total volume
of publications produced in both countries grows with a remarkable regularity
over tens of years. While China initially experienced faster growth in
publication volume than the U.S., growth slowed in China when it reached parity
with the U.S. and the growth rates of both countries are now similar. We also
see both countries undergo a seismic shift in topic choice around 1990, and
connect this to an explosion of interest in neural network methods. Finally, we
see evidence that between 2000 and 2010, China's topic choice tended to lag
that of the U.S. but that in recent decades the topic portfolios have come into
closer alignment.
"
2844,Cross-tier web programming for curated databases: A case study,"  Curated databases have become important sources of information across
scientific disciplines, and due to the manual work of experts, often become
important reference works. Features such as provenance tracking, archiving, and
data citation are widely regarded as important features for curated databases,
but implementing such features is challenging, and small database projects
often lack the resources to do so. A scientific database application is not
just the database itself, but also an ecosystem of web applications to display
the data, and applications supporting data curation. Supporting advanced
curation features requires changing all of these components, and there is
currently no way to provide such capabilities in a reusable way. Cross-tier
programming languages have been proposed to simplify the creation of web
applications, where developers write an application in a single, uniform
language. Consequently, database queries and updates can be written in the same
language as the rest of the program, and at least in principle, it should be
possible to provide curation features reusably via program transformations. As
a first step, it is important to establish that realistic curated databases can
be implemented in a cross-tier programming language. In this paper, we describe
such a case study: reimplementing the web frontend of a real-world scientific
database, the IUPHAR/BPS Guide to Pharmacology (GtoPdb), in the Links
programming language. We show how features such as language-integrated query
simplify the development process, and rule out common errors. We show that the
Links implementation performs fewer database queries, while the time needed to
handle the queries is comparable to the Java version. While there is some
overhead to using Links because of its comparative immaturity compared to Java,
the Links version is viable as a proof-of-concept case study.
"
2845,"A Catalogue of Locus Algorithm Pointings for Optimal Differential
  Photometry for 23,779 Quasars","  This paper presents a catalogue of optimised pointings for differential
photometry of 23,779 quasars extracted from the Sloan Digital Sky Survey (SDSS)
Catalogue and a score for each indicating the quality of the Field of View
(FoV) associated with that pointing. Observation of millimagnitude variability
on a timescale of minutes typically requires differential observations with
reference to an ensemble of reference stars. For optimal performance, these
reference stars should have similar colour and magnitude to the target quasar.
In addition, the greatest quantity and quality of suitable reference stars may
be found by using a telescope pointing which offsets the target object from the
centre of the field of view. By comparing each quasar with the stars which
appear close to it on the sky in the SDSS Catalogue, an optimum pointing can be
calculated, and a figure of merit, referred to as the ""score"" calculated for
that pointing. Highly flexible software has been developed to enable this
process to be automated and implemented in a distributed computing paradigm,
which enables the creation of catalogues of pointings given a set of input
targets. Applying this technique to a sample of 40,000 targets from the 4th
SDSS quasar catalogue resulted in the production of pointings and scores for
23,779 quasars. This catalogue is a useful resource for observers planning
differential photometry studies and surveys of quasars to select those which
have many suitable celestial neighbours for differential photometry
"
2846,"Expressiveness and machine processability of Knowledge Organization
  Systems (KOS): An analysis of concepts and relations","  This study considers the expressiveness (that is the expressive power or
expressivity) of different types of Knowledge Organization Systems (KOS) and
discusses its potential to be machine-processable in the context of the
Semantic Web. For this purpose, the theoretical foundations of KOS are reviewed
based on conceptualizations introduced by the Functional Requirements for
Subject Authority Data (FRSAD) and the Simple Knowledge Organization System
(SKOS); natural language processing techniques are also implemented. Applying a
comparative analysis, the dataset comprises a thesaurus (Eurovoc), a subject
headings system (LCSH) and a classification scheme (DDC). These are compared
with an ontology (CIDOC-CRM) by focusing on how they define and handle concepts
and relations. It was observed that LCSH and DDC focus on the formalism of
character strings (nomens) rather than on the modelling of semantics; their
definition of what constitutes a concept is quite fuzzy, and they comprise a
large number of complex concepts. By contrast, thesauri have a coherent
definition of what constitutes a concept, and apply a systematic approach to
the modelling of relations. Ontologies explicitly define diverse types of
relations, and are by their nature machine-processable. The paper concludes
that the potential of both the expressiveness and machine processability of
each KOS is extensively regulated by its structural rules. It is harder to
represent subject headings and classification schemes as semantic networks with
nodes and arcs, while thesauri are more suitable for such a representation. In
addition, a paradigm shift is revealed which focuses on the modelling of
relations between concepts, rather than the concepts themselves.
"
2847,"New Research Trends in Unconventional Oil and Gas Environmental Issue: A
  Bibliometric Analysis","  With the booming of unconventional gas production in the world, how to
balance environment pollution risk and economy of unconventional gas have
become a common dilemma around the world. The aim of this study is to elucidate
the research about environmental issue brought with development of
unconventional oil and gas industry. To achieve this goal, we present a
bibliometrics overview of this field from 1990 to 2018. Firstly, this study
outlines a basic statistical analysis over journals, publications, authors,
institutions and documents. Secondly, VOSviewer is employed to visualize the
collaborative relationship to show the link between different author,
institutions, regions and journals. Finally, document bibliographic coupling,
cooccurrence and keyword burst detection are analyzed to reveal the emerging
trend and hot topic. The results indicate that among all countries, America was
the most productive country as well as cooperated the most with other
countries, followed by China, while the China University of Petroleum is the
most productive institution in the world, with 105 publications. Additionally,
most articles were classified as energy fuels, environmental sciences and
geosciences multidisciplinary. Furthermore, based on emerging trends analysis,
it was concluded that hydraulic fracturing technology has become a hot topic,
other popular research topics include: energy policy and regulation of
unconventional gas development, greenhouse gas emissions, energy and water
consumption of unconventional gas life cycle assessment.
"
2848,Correlation between Content and Traffic of the Universities Website,"  The purpose of this study is to analyse the correlation between content and
traffic of 21,485 academic websites (universities and research institutes). The
achieved result is used as an indicator which shows the performance of the
websites for attracting more visitors. This inspires a best practice for
developing new websites or promoting the traffic of the existing websites. At
the first step, content of the site is divided into three major items which
are: Size, Papers and Rich Files. Then, the Spearman correlation between
traffic of the websites and these items are calculated for each country and for
the world, respectively. At the next step, countries are ranked based on their
correlations, also a new indicator is proposed from combining these three
correlations of the countries. Results show that in most countries, correlation
between traffic of the websites and Papers is less than correlations between
traffic of the websites and Rich Files and Size.
"
2849,"Prevalence of Potentially Predatory Publishing in Scopus on the Country
  Level","  We present the results of a large-scale study of potentially predatory
journals (PPJ) represented in the Scopus database, which is widely used for
research evaluation. Both journal metrics and country, disciplinary data have
been evaluated for different groups of PPJ: those listed by Jeffrey Beall and
those delisted by Scopus because of ""publication concerns"". Our results show
that even after years of delisting, PPJ are still highly visible in the Scopus
database with hundreds of active potentially predatory journals. PPJ papers are
continuously produced by all major countries, but with different shares. All
major subject areas are affected. The largest number of PPJ papers are in
engineering and medicine. On average, PPJ have much lower citation metrics than
other Scopus-indexed journals. We conclude with a brief survey of the case of
Kazakhstan where the share of PPJ papers at one time amounted to almost a half
of all Kazakhstan papers in Scopus, and propose a link between PPJ share and
national research evaluation policies (in particular, rules of awarding
academic degrees). The progress of potentially predatory journal research will
be increasingly important because such evaluation methods are becoming more
widespread in times of the Metric Tide.
"
2850,A Corpus of Adpositional Supersenses for Mandarin Chinese,"  Adpositions are frequent markers of semantic relations, but they are highly
ambiguous and vary significantly from language to language. Moreover, there is
a dearth of annotated corpora for investigating the cross-linguistic variation
of adposition semantics, or for building multilingual disambiguation systems.
This paper presents a corpus in which all adpositions have been semantically
annotated in Mandarin Chinese; to the best of our knowledge, this is the first
Chinese corpus to be broadly annotated with adposition semantics. Our approach
adapts a framework that defined a general set of supersenses according to
ostensibly language-independent semantic criteria, though its development
focused primarily on English prepositions (Schneider et al., 2018). We find
that the supersense categories are well-suited to Chinese adpositions despite
syntactic differences from English. On a Mandarin translation of The Little
Prince, we achieve high inter-annotator agreement and analyze semantic
correspondences of adposition tokens in bitext.
"
2851,Predicting the number of coauthors for researchers: A learning model,"  Predicting the number of coauthors for researchers contributes to
understanding the development of team science. However, it is an elusive task
due to diversity in the collaboration patterns of researchers. This study
provides a learning model for the dynamics of this variable; the parameters are
learned from empirical data that consist of the number of publications and the
number of coauthors at given time intervals. The model is based on relationship
between the annual number of new coauthors and time given an annual number of
publications, the relationship between the annual number of publications and
time given a historical number of publications, and Lotka's law. The
assumptions of the model are validated by applying it on the high-quality dblp
dataset. The effectiveness of the model is tested on the dataset by
satisfactory fittings on the evolutionary trend of the number of coauthors for
researchers, the distribution of this variable, and the occurrence probability
of collaboration events. Due to its regression nature, the model has the
potential to be extended to assess the confidence level of the prediction
results and thus has applicability to other empirical research.
"
2852,Mathematical Formulae in Wikimedia Projects 2020,"  This poster summarizes our contributions to Wikimedia's processing pipeline
for mathematical formulae. We describe how we have supported the transition
from rendering formulae as course-grained PNG images in 2001 to providing
modern semantically enriched language-independent MathML formulae in 2020.
Additionally, we describe our plans to improve the accessibility and
discoverability of mathematical knowledge in Wikimedia projects further.
"
2853,"Pairwise Multi-Class Document Classification for Semantic Relations
  between Wikipedia Articles","  Many digital libraries recommend literature to their users considering the
similarity between a query document and their repository. However, they often
fail to distinguish what is the relationship that makes two documents alike. In
this paper, we model the problem of finding the relationship between two
documents as a pairwise document classification task. To find the semantic
relation between documents, we apply a series of techniques, such as GloVe,
Paragraph-Vectors, BERT, and XLNet under different configurations (e.g.,
sequence length, vector concatenation scheme), including a Siamese architecture
for the Transformer-based systems. We perform our experiments on a newly
proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that
define the semantic document relations. Our results show vanilla BERT as the
best performing system with an F1-score of 0.93, which we manually examine to
better understand its applicability to other domains. Our findings suggest that
classifying semantic relations between documents is a solvable task and
motivates the development of recommender systems based on the evaluated
techniques. The discussions in this paper serve as first steps in the
exploration of documents through SPARQL-like queries such that one could find
documents that are similar in one aspect but dissimilar in another.
"
2854,"Embedding technique and network analysis of scientific innovations
  emergence in an arXiv-based concept network","  Novelty is an inherent part of innovations and discoveries. Such processes
may be considered as an appearance of new ideas or as an emergence of atypical
connections between the existing ones. The importance of such connections hints
for investigation of innovations through network or graph representation in the
space of ideas. In such representation, a graph node corresponds to the
relevant concept (idea), whereas an edge between two nodes means that the
corresponding concepts have been used in a common context. In this study we
address the question about a possibility to identify the edges between existing
concepts where the innovations may emerge. To this end, we use a
well-documented scientific knowledge landscape of 1.2M arXiv.org manuscripts
dated starting from April 2007 and until September 2019. We extract relevant
concepts for them using the ScienceWISE.info platform. Combining approaches
developed in complex networks science and graph embedding, we discuss the
predictability of edges (links) on the scientific knowledge landscape where the
innovations may appear.
"
2855,Interdisciplinarity metric based on the co-citation network,"  Quantifying the interdisciplinarity of a research is a relevant problem in
the evaluative bibliometrics. The concept of interdisciplinarity is ambiguous
and multidimensional. Thus, different measures of interdisciplinarity have been
propose in the literature. However, few studies have proposed interdisciplinary
metrics without previously defining classification sets, and no one use the
co-citation network for this purpose. In this study we propose an
interdisciplinary metric based on the co-citation network. This is a way to
define the publication's field without resorting to pre-defined classification
sets. We present a characterization of a publication's field and then we use
this definition to propose a new metric of the interdisciplinarity degree for
publications (papers) and journals as units of analysis. The proposed measure
has an aggregative property that makes it scalable from a paper individually to
a set of them (journal) without more than adding the numerators and
denominators in the proportions that define this new indicator. Moreover, the
aggregated value of two or more units is strictly among all the individual
values.
"
2856,Translational Knowledge Map of COVID-19,"  A translational knowledge map of COVID-19, based on the analysis of
scientific papers and networks citation concurrence of terms and keywords of
the terms: covid- 19, 2019-ncov and sars-cov-2 in leading databases (MEDLINE,
web of Science and Scopus), was constructed. Some fields of the research on
covid-19 are connected together, differing in structure, content and evolution.
"
2857,"How do academic topics shift across altmetric sources? A case study of
  the research area of Big Data","  Taking the research area of Big Data as a case study, we propose an approach
for exploring how academic topics shift through the interactions among
audiences across different altmetric sources. Data used is obtained from Web of
Science (WoS) and Altmetric.com, with a focus on Blog, News, Policy, Wikipedia,
and Twitter. Author keywords from publications and terms from online events are
extracted as the main topics of the publications and the online discussion of
their audiences at Altmetric. Different measures are applied to determine the
(dis)similarities between the topics put forward by the publication authors and
those by the online audiences. Results show that overall there are substantial
differences between the two sets of topics around Big Data scientific research.
The main exception is Twitter, where high-frequency hashtags in tweets have a
stronger concordance with the author keywords in publications. Among the online
communities, Blogs and News show a strong similarity in the terms commonly
used, while Policy documents and Wikipedia articles exhibit the strongest
dissimilarity in considering and interpreting Big Data related research.
Specifically, the audiences not only focus on more easy-to-understand academic
topics related to social or general issues, but also extend them to a broader
range of topics in their online discussions. This study lays the foundations
for further investigations about the role of online audiences in the
transformation of academic topics across altmetric sources, and the degree of
concern and reception of scholarly contents by online communities.
"
2858,Covid-19 Tweeting in English: Gender Differences,"  At the start of 2020, COVID-19 became the most urgent threat to global public
health. Uniquely in recent times, governments have imposed partly voluntary,
partly compulsory restrictions on the population to slow the spread of the
virus. In this context, public attitudes and behaviors are vitally important
for reducing the death rate. Analyzing tweets about the disease may therefore
give insights into public reactions that may help guide public information
campaigns. This article analyses 3,038,026 English tweets about COVID-19 from
March 10 to 23, 2020. It focuses on one relevant aspect of public reaction:
gender differences. The results show that females are more likely to tweet
about the virus in the context of family, social distancing and healthcare
whereas males are more likely to tweet about sports cancellations, the global
spread of the virus and political reactions. Thus, women seem to be taking a
disproportionate share of the responsibility for directly keeping the
population safe. The detailed results may be useful to inform public
information announcements and to help understand the spread of the virus. For
example, failure to impose a sporting bans whilst encouraging social distancing
may send mixed messages to males.
"
2859,"Which papers cited which tweets? An empirical analysis based on Scopus
  data","  Many altmetric studies analyze which papers were mentioned how often in
specific altmetrics sources. In order to study the potential policy relevance
of tweets from another perspective, we investigate which tweets were cited in
papers. If many tweets were cited in publications, this might demonstrate that
tweets have substantial and useful content. Overall, a rather low number of
tweets (n=5506) were cited by less than 3000 papers. Most tweets do not seem to
be cited because of any cognitive influence they might have had on studies;
they rather were study objects. Most of the papers citing tweets are from the
subject areas Social Sciences, Arts and Humanities, and Computer Sciences. Most
of the papers cited only one tweet. Up to 55 tweets cited in a single paper
were found. This research-in-progress does not support a high policy-relevance
of tweets. However, a content analysis of the tweets and/or papers might lead
to a more detailed conclusion.
"
2860,Overview of the TREC 2019 Fair Ranking Track,"  The goal of the TREC Fair Ranking track was to develop a benchmark for
evaluating retrieval systems in terms of fairness to different content
providers in addition to classic notions of relevance. As part of the
benchmark, we defined standardized fairness metrics with evaluation protocols
and released a dataset for the fair ranking problem. The 2019 task focused on
reranking academic paper abstracts given a query. The objective was to fairly
represent relevant authors from several groups that were unknown at the system
submission time. Thus, the track emphasized the development of systems which
have robust performance across a variety of group definitions. Participants
were provided with querylog data (queries, documents, and relevance) from
Semantic Scholar. This paper presents an overview of the track, including the
task definition, descriptions of the data and the annotation process, as well
as a comparison of the performance of submitted systems.
"
2861,"A Heterogeneous Dynamical Graph Neural Networks Approach to Quantify
  Scientific Impact","  Quantifying and predicting the long-term impact of scientific writings or
individual scholars has important implications for many policy decisions, such
as funding proposal evaluation and identifying emerging research fields. In
this work, we propose an approach based on Heterogeneous Dynamical Graph Neural
Network (HDGNN) to explicitly model and predict the cumulative impact of papers
and authors. HDGNN extends heterogeneous GNNs by incorporating temporally
evolving characteristics and capturing both structural properties of attributed
graph and the growing sequence of citation behavior. HDGNN is significantly
different from previous models in its capability of modeling the node impact in
a dynamic manner while taking into account the complex relations among nodes.
Experiments conducted on a real citation dataset demonstrate its superior
performance of predicting the impact of both papers and authors.
"
2862,Open Access uptake by universities worldwide,"  The implementation of policies promoting the adoption of an Open Science
culture must be accompanied by indicators that allow monitoring the penetration
of such policies and their potential effects on research publishing and sharing
practices. This study presents indicators of Open Access (OA) penetration at
the institutional level for universities worldwide. By combining data from Web
of Science, Unpaywall and the Leiden Ranking disambiguation of institutions, we
track OA coverage of universities' output for 963 institutions. This paper
presents the methodological challenges, conceptual discrepancies and
limitations and discusses further steps needed to move forward the discussion
on fostering Open Access and Open Science practices and policies.
"
2863,"Text-based Technological Signatures and Similarities: How to create them
  and what to do with them","  This paper describes a new approach to measure technological similarity
between patents by leveraging their textual description. Using embedding
techniques from natural language processing, we represent their description as
a high dimensional numerical vector, thus capturing their technological
signature. Deploying an almost near linear-scaling approximate nearest neighbor
matching techniques, we are able to compute technological similarity scores for
all existing patents. This enables us to represent the whole patent universe as
a technological network. We validate both technological signature and
similarity in various ways, and demonstrate their usefulness to create patent
quality indicators, measure knowledge flows, and map technological change.
"
2864,"Scientific elite revisited: Patterns of productivity, collaboration,
  authorship and impact","  Throughout history, a relatively small number of individuals have made a
profound and lasting impact on science and society. Despite long-standing,
multi-disciplinary interests in understanding careers of elite scientists,
there have been limited attempts for a quantitative, career-level analysis.
Here, we leverage a comprehensive dataset we assembled, allowing us to trace
the entire career histories of nearly all Nobel laureates in physics,
chemistry, and physiology or medicine over the past century. We find that,
although Nobel laureates were energetic producers from the outset, producing
works that garner unusually high impact, their careers before winning the prize
follow relatively similar patterns as ordinary scientists, being characterized
by hot streaks and increasing reliance on collaborations. We also uncovered
notable variations along their careers, often associated with the Nobel prize,
including shifting coauthorship structure in the prize-winning work, and a
significant but temporary dip in the impact of work they produce after winning
the Nobel. Together, these results document quantitative patterns governing the
careers of scientific elites, offering an empirical basis for a deeper
understanding of the hallmarks of exceptional careers in science.
"
2865,Ontology Extraction and Usage in the Scholarly Knowledge Domain,"  Ontologies of research areas have been proven to be useful in many
application for analysing and making sense of scholarly data. In this chapter,
we present the Computer Science Ontology (CSO), which is the largest ontology
of research areas in the field of Computer Science, and discuss a number of
applications that build on CSO, to support high-level tasks, such as topic
classification, metadata extraction, and recommendation of books.
"
2866,Opioids for pain treatment of cancer: a knowledge maturity mapping,"  The conceptual structure of opioids, based on the bibliometric analysis of
4,935 articles of the Web of Science was constructed. The results were
processed identifying the most cited articles to extract the main connections
and frequencies of key words, authors, journals, countries, institutions, and
their tendencies and their connection and degree of collaboration. The temporal
tendencies, the word cloud, the keyword network, the evolution of words, author
production and the scientific production by country are analyzed in terms of
the increasing frequency in which opioids are employed to treat both cancerous
and non-cancerous pain.
"
2867,Persistent Identification Of Instruments,"  Instruments play an essential role in creating research data. Given the
importance of instruments and associated metadata to the assessment of data
quality and data reuse, globally unique, persistent and resolvable
identification of instruments is crucial. The Research Data Alliance Working
Group Persistent Identification of Instruments (PIDINST) developed a
community-driven solution for persistent identification of instruments which we
present and discuss in this paper. Based on an analysis of 10 use cases,
PIDINST developed a metadata schema and prototyped schema implementation with
DataCite and ePIC as representative persistent identifier infrastructures and
with HZB (Helmholtz-Zentrum Berlin f\""ur Materialien und Energie) and BODC
(British Oceanographic Data Centre) as representative institutional instrument
providers. These implementations demonstrate the viability of the proposed
solution in practice. Moving forward, PIDINST will further catalyse adoption
and consolidate the schema by addressing new stakeholder requirements.
"
2868,"A bibliometric analysis of research based on the Roy Adaptation Model: a
  contribution to Nursing","  Objective. To perform a modern bibliometric analysis of the research based on
the Roy Adaptation Model, a founding nursing model proposed by Sor Callista Roy
in the1970s. Method. A descriptive and longitudinal study. We used information
from the two dominant scientific databases, Web Of Science and SCOPUS. We
obtained 137 publications from the Core Collection of WoS, and 338 publications
from SCOPUS. We conducted our analysis using the software Bibliometrix, an
R-package specialized in creating bibliometric analyses from a perspective of
descriptive statistics and network analysis, including co-citation, co-keyword
occurrence and collaboration networks. Results. Our quantitative results show
the main actors around the research based on the model and the founding
literature or references on which this research was based. We analyze the main
keywords and how they are linked. Furthermore, we present the most prolific
authors both in number of publications and in centrality in the network of
coauthors. We present the most central institutions in the global network of
collaboration. Conclusions. We highlight the relevance of this theoretical
model in nursing and detail its evolution. The United States is the dominant
country in production of documents on the topic, and the University of
Massachusetts Boston and Boston College are the most influential institutions.
The network of collaboration also describes clusters in Mexico, Turkey and
Spain. Our findings are useful to acquire a general vision of the field.
"
2869,"Best Practices for Implementing FAIR Vocabularies and Ontologies on the
  Web","  With the adoption of Semantic Web technologies, an increasing number of
vocabularies and ontologies have been developed in different domains, ranging
from Biology to Agronomy or Geosciences. However, many of these ontologies are
still difficult to find, access and understand by researchers due to a lack of
documentation, URI resolving issues, versioning problems, etc. In this chapter
we describe guidelines and best practices for creating accessible,
understandable and reusable ontologies on the Web, using standard practices and
pointing to existing tools and frameworks developed by the Semantic Web
community. We illustrate our guidelines with concrete examples, in order to
help researchers implement these practices in their future vocabularies.
"
2870,"Making Metadata Fit for Next Generation Language Technology Platforms:
  The Metadata Schema of the European Language Grid","  The current scientific and technological landscape is characterised by the
increasing availability of data resources and processing tools and services. In
this setting, metadata have emerged as a key factor facilitating management,
sharing and usage of such digital assets. In this paper we present ELG-SHARE, a
rich metadata schema catering for the description of Language Resources and
Technologies (processing and generation services and tools, models, corpora,
term lists, etc.), as well as related entities (e.g., organizations, projects,
supporting documents, etc.). The schema powers the European Language Grid
platform that aims to be the primary hub and marketplace for industry-relevant
Language Technology in Europe. ELG-SHARE has been based on various metadata
schemas, vocabularies, and ontologies, as well as related recommendations and
guidelines.
"
2871,"The European Language Technology Landscape in 2020: Language-Centric and
  Human-Centric AI for Cross-Cultural Communication in Multilingual Europe","  Multilingualism is a cultural cornerstone of Europe and firmly anchored in
the European treaties including full language equality. However, language
barriers impacting business, cross-lingual and cross-cultural communication are
still omnipresent. Language Technologies (LTs) are a powerful means to break
down these barriers. While the last decade has seen various initiatives that
created a multitude of approaches and technologies tailored to Europe's
specific needs, there is still an immense level of fragmentation. At the same
time, AI has become an increasingly important concept in the European
Information and Communication Technology area. For a few years now, AI,
including many opportunities, synergies but also misconceptions, has been
overshadowing every other topic. We present an overview of the European LT
landscape, describing funding programmes, activities, actions and challenges in
the different countries with regard to LT, including the current state of play
in industry and the LT market. We present a brief overview of the main
LT-related activities on the EU level in the last ten years and develop
strategic guidance with regard to four key dimensions.
"
2872,"The Case For Alternative Web Archival Formats To Expedite The
  Data-To-Insight Cycle","  The WARC file format is widely used by web archives to preserve collected web
content for future use. With the rapid growth of web archives and the
increasing interest to reuse these archives as big data sources for statistical
and analytical research, the speed to turn these data into insights becomes
critical. In this paper we show that the WARC format carries significant
performance penalties for batch processing workload. We trace the root cause of
these penalties to its data structure, encoding, and addressing method. We then
run controlled experiments to illustrate how severe these problems can be.
Indeed, performance gain of one to two orders of magnitude can be achieved
simply by reformatting WARC files into Parquet or Avro formats. While these
results do not necessarily constitute an endorsement for Avro or Parquet, the
time has come for the web archiving community to consider replacing WARC with
more efficient web archival formats.
"
2873,"GitHub Repositories with Links to Academic Papers: Open Access,
  Traceability, and Evolution","  Traceability between published scientific breakthroughs and their
implementation is essential, especially in the case of Open Source Software
implements bleeding edge science into its code. However, aligning the link
between GitHub repositories and academic papers can prove difficult, and the
link impact remains unknown. This paper investigates the role of academic paper
references contained in these repositories. We conducted a large-scale study of
20 thousand GitHub repositories to establish prevalence of references to
academic papers. We use a mixed-methods approach to identify Open Access (OA),
traceability and evolutionary aspects of the links. Although referencing a
paper is not typical, we find that a vast majority of referenced academic
papers are OA. In terms of traceability, our analysis revealed that machine
learning is the most prevalent topic of repositories. These repositories tend
to be affiliated with academic communities. More than half of the papers do not
link back to any repository. A case study of referenced arXiv paper shows that
most of these papers are high-impact and influential and do align with
academia, referenced by repositories written in different programming
languages. From the evolutionary aspect, we find very few changes of papers
being referenced and links to them.
"
2874,Archiving and referencing source code with Software Heritage,"  Software, and software source code in particular, is widely used in modern
research. It must be properly archived, referenced, described and cited in
order to build a stable and long lasting corpus of scientic knowledge. In this
article we show how the Software Heritage universal source code archive
provides a means to fully address the first two concerns, by archiving
seamlessly all publicly available software source code, and by providing
intrinsic persistent identifiers that allow to reference it at various
granularities in a way that is at the same time convenient and effective. We
call upon the research community to adopt widely this approach.
"
2875,"Evolution and Transformation of Scientific Knowledge over the Sphaera
  Corpus: A Network Study","  We investigated the evolution and transformation of scientific knowledge in
the early modern period, analyzing more than 350 different editions of
textbooks used for teaching astronomy in European universities from the late
fifteenth century to mid-seventeenth century. These historical sources
constitute the Sphaera Corpus. By examining different semantic relations among
individual parts of each edition on record, we built a multiplex network
consisting of six layers, as well as the aggregated network built from the
superposition of all the layers. The network analysis reveals the emergence of
five different communities. The contribution of each layer in shaping the
communities and the properties of each community are studied. The most
influential books in the corpus are found by calculating the average age of all
the out-going and in-coming links for each book. A small group of editions is
identified as a transmitter of knowledge as they bridge past knowledge to the
future through a long temporal interval. Our analysis, moreover, identifies the
most disruptive books. These books introduce new knowledge that is then adopted
by almost all the books published afterwards until the end of the whole period
of study. The historical research on the content of the identified books, as an
empirical test, finally corroborates the results of all our analyses.
"
2876,Mapping Three Decades of Intellectual Change in Academia,"  Research on the development of science has focused on the creation of
multidisciplinary teams. However, while this coming together of people is
symmetrical, the ideas, methods, and vocabulary of science have a directional
flow. We present a statistical model of the text of dissertation abstracts from
1980 to 2010, revealing for the first time the large-scale flow of language
across fields. Results of the analysis include identifying methodological
fields that export broadly, emerging topical fields that borrow heavily and
expand, and old topical fields that grow insular and retract. Particular
findings show a growing split between molecular and ecological forms of biology
and a sea change in the humanities and social sciences driven by the rise of
gender and ethnic studies.
"
2877,"An alternative analysis on the scientific output of Spanish Sociology
  What can altmetrics tell us?","  In recent years, new indicators known as altmetrics have been introduced to
measure the impact of scientific activity. These indicators are obtained
through the mentions realised from different social media, existing several
aggregators of these data that collect several of them in the same database,
being Altmetric.com the most popular. However, in spite of the popularization
of these metrics, several limitations in their use have been manifested. For
this reason, rhe objective of this work is twofold: (1) to show the
possibilities of altimetric techniques applied to the Spanish social sciences
in general and sociology in particular; (2) to critically analyse the results
to observe the limitations of these indicators; (3) to check whether they can
really add useful information that can be used to describe a scientific field
and (4) to see the reasons why altmetrics cannot be applied in these fields.
"
2878,"Open access institutional and news media tweet dataset for COVID-19
  social science research","  As COVID-19 quickly became one of the most concerned global crisis, the
demand for data in academic research is also increasing. Currently, there are
several open access Twitter datasets, but none of them is dedicated to the
institutional and news media Twitter data collection, to fill this blank, we
retrieved data from 69 institutional/news media Twitter accounts, 17 of them
were related to government and international organizations, 52 of them were
news media across North America, Europe and Asia. We believe our open access
data can provide researchers more availability to conduct social science
research.
"
2879,"Information Mining for COVID-19 Research From a Large Volume of
  Scientific Literature","  The year 2020 has seen an unprecedented COVID-19 pandemic due to the outbreak
of a novel strain of coronavirus in 180 countries. In a desperate effort to
discover new drugs and vaccines for COVID-19, many scientists are working
around the clock. Their valuable time and effort may benefit from
computer-based mining of a large volume of health science literature that is a
treasure trove of information. In this paper, we have developed a graph-based
model using abstracts of 10,683 scientific articles to find key information on
three topics: transmission, drug types, and genome research related to
coronavirus. A subgraph is built for each of the three topics to extract more
topic-focused information. Within each subgraph, we use a betweenness
centrality measurement to rank order the importance of keywords related to
drugs, diseases, pathogens, hosts of pathogens, and biomolecules. The results
reveal intriguing information about antiviral drugs (Chloroquine, Amantadine,
Dexamethasone), pathogen-hosts (pigs, bats, macaque, cynomolgus), viral
pathogens (zika, dengue, malaria, and several viruses in the coronaviridae
virus family), and proteins and therapeutic mechanisms (oligonucleotide,
interferon, glycoprotein) in connection with the core topic of coronavirus. The
categorical summary of these keywords and topics may be a useful reference to
expedite and recommend new and alternative directions for COVID-19 research.
"
2880,"A thematic analysis of highly retweeted early COVID -19 tweets:
  Consensus, information, dissent, and lockdown life","  Purpose: Public attitudes towards COVID-19 and social distancing are critical
in reducing its spread. It is therefore important to understand public
reactions and information dissemination in all major forms, including on social
media. This article investigates important issues reflected on Twitter in the
early stages of the public reaction to COVID-19. Design/methodology/approach: A
thematic analysis of the most retweeted English-language tweets mentioning
COVID-19 during March 10-29, 2020. Findings: The main themes identified for the
87 qualifying tweets accounting for 14 million retweets were: lockdown life;
attitude towards social restrictions; politics; safety messages; people with
COVID-19; support for key workers; work; and COVID-19 facts/news. Research
limitations/implications: Twitter played many positive roles, mainly through
unofficial tweets. Users shared social distancing information, helped build
support for social distancing, criticised government responses, expressed
support for key workers, and helped each other cope with social isolation. A
few popular tweets not supporting social distancing show that government
messages sometimes failed. Practical implications: Public health campaigns in
future may consider encouraging grass roots social web activity to support
campaign goals. At a methodological level, analysing retweet counts emphasised
politics and ignored practical implementation issues. Originality/value: This
is the first qualitative analysis of general COVID-19-related retweeting.
"
2881,"Ontologies in CLARIAH: Towards Interoperability in History, Language and
  Media","  One of the most important goals of digital humanities is to provide
researchers with data and tools for new research questions, either by
increasing the scale of scholarly studies, linking existing databases, or
improving the accessibility of data. Here, the FAIR principles provide a useful
framework as these state that data needs to be: Findable, as they are often
scattered among various sources; Accessible, since some might be offline or
behind paywalls; Interoperable, thus using standard knowledge representation
formats and shared vocabularies; and Reusable, through adequate licensing and
permissions. Integrating data from diverse humanities domains is not trivial,
research questions such as ""was economic wealth equally distributed in the 18th
century?"", or ""what are narratives constructed around disruptive media
events?"") and preparation phases (e.g. data collection, knowledge organisation,
cleaning) of scholars need to be taken into account. In this chapter, we
describe the ontologies and tools developed and integrated in the Dutch
national project CLARIAH to address these issues across datasets from three
fundamental domains or ""pillars"" of the humanities (linguistics, social and
economic history, and media studies) that have paradigmatic data
representations (textual corpora, structured data, and multimedia). We
summarise the lessons learnt from using such ontologies and tools in these
domains from a generalisation and reusability perspective.
"
2882,On the Persistence of Persistent Identifiers of the Scholarly Web,"  Scholarly resources, just like any other resources on the web, are subject to
reference rot as they frequently disappear or significantly change over time.
Digital Object Identifiers (DOIs) are commonplace to persistently identify
scholarly resources and have become the de facto standard for citing them. We
investigate the notion of persistence of DOIs by analyzing their resolution on
the web. We derive confidence in the persistence of these identifiers in part
from the assumption that dereferencing a DOI will consistently return the same
response, regardless of which HTTP request method we use or from which network
environment we send the requests. Our experiments show, however, that
persistence, according to our interpretation, is not warranted. We find that
scholarly content providers respond differently to varying request methods and
network environments and even change their response to requests against the
same DOI. In this paper we present the results of our quantitative analysis
that is aimed at informing the scholarly communication community about this
disconcerting lack of consistency.
"
2883,SciWING -- A Software Toolkit for Scientific Document Processing,"  We introduce SciWING, an open-source software toolkit which provides access
to pre-trained models for scientific document processing tasks, inclusive of
citation string parsing and logical structure recovery. SciWING enables
researchers to rapidly experiment with different models by swapping and
stacking different modules. It also enables them declare and run models from a
configuration file. It enables researchers to perform production-ready transfer
learning from general, pre-trained transformers (i.e., BERT, SciBERT etc), and
aids development of end-user applications. It includes ready-to-use web and
terminal-based applications and demonstrations (Available from
http://sciwing.io).
"
2884,"Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research
  Dataset: Preliminary Thoughts and Lessons Learned","  We present the Neural Covidex, a search engine that exploits the latest
neural ranking architectures to provide information access to the COVID-19 Open
Research Dataset curated by the Allen Institute for AI. This web application
exists as part of a suite of tools that we have developed over the past few
weeks to help domain experts tackle the ongoing global pandemic. We hope that
improved information access capabilities to the scientific literature can
inform evidence-based decision making and insight generation. This paper
describes our initial efforts and offers a few thoughts about lessons we have
learned along the way.
"
2885,"Using Photo Modeling Based 3DGRSL to Promote the Sustainability of
  Geo-Education, a case study from China","  In earth science education, observation of field geological phenomena is very
important. Due to China's huge student population, it is difficult to guarantee
education fairness and teaching quality in field teaching. Specimens are
indispensable geo-education resources. However, the specimen cabinet or picture
specimen library has many limitations and it is difficult to meet the
internet-spirit or geo-teaching needs. Based on photo modeling, this research
builds a 3D Geo-Resource Sharing Library (3DGRSL) for Geo-Education. It uses
the Cesium engine and data-oriented distributed architecture to provide the
educational resources to many universities. With Browser/Server (B/S)
architecture, the system can realize multi-terminal and multi-scenario access
of mobile phones, tablets, VR, PC, indoor, outdoor, field, providing a flexible
and convenient way for preserving and sharing scientific information about
geo-resources. This makes sense to students who cannot accept field teaching in
under-funded colleges, and the ones with mobility problems. Tests and scoring
results show that 3DGRSL is a suitable solution for displaying and sharing
geological specimens. Which is of great significance for the sustainable use
and protection of geoscience teaching resources, the maintenance of the right
to fair education, and the construction of virtual simulation solutions in the
future.
"
2886,"Approximating percentage of academic traffic in the World Wide Web and
  rankings of countries based on academic traffic","  The paper introduces a novel mechanism for approximating traffic of the
academic sites (universities and research institutes) in the World Wide Web
based on Alexa rankings. Firstly we introduce and discuss new method for
calculating score (weight) of each site based on its Alexa rank. Secondly we
calculate percentage of academic traffic in the World Wide Web. Thirdly we
introduce and discuss two new rankings of countries based on academic traffic.
Finally we discuss about three indicators and effects of them in traffic of the
academic sites. Results indicate that the methodology can be useful for
approximating traffic of the academic sites and producing rankings of countries
in practice
"
2887,Bibliometric Analysis of Agile Software Development,"  Agile methodologies are currently considered one of the main paradigms of
software development. Its study, from a scientific point of view, has deserved
prominence in recent years by the scientific community related to the area of
software engineering. This study intends to perform a bibliometric analysis of
the quantity, characteristics and scope of the most relevant studies published
in this area of knowledge. The findings indicate that the number of studies
published from 2010 to 2016 significantly increased, having reached a peak in
2015. The study identifies the main journals and conferences in the field and
we also concluded that the majority of published studies are literature reviews
of agile software development, and qualitative and quantitative research
methods have identical number of publications.
"
2888,"Return to basics: Clustering of scientific literature using structural
  information","  Scholars frequently employ relatedness measures to estimate the similarity
between two different items (e.g., documents, authors, and institutes). Such
relatedness measures are commonly based on overlapping references
($\textit{i.e.}$, bibliographic coupling) or citations ($\textit{i.e.}$,
co-citation) and can then be used with cluster analysis to find boundaries
between research fields. Unfortunately, calculating a relatedness measure is
challenging, especially for a large number of items, because the computational
complexity is greater than linear. We propose an alternative method for
identifying the research front that uses direct citation inspired by
relatedness measures. Our novel approach simply replicates a node into two
distinct nodes: a citing node and cited node. We then apply typical clustering
methods to the modified network. Clusters of citing nodes should emulate those
from the bibliographic coupling relatedness network, while clusters of cited
nodes should act like those from the co-citation relatedness network. In
validation tests, our proposed method demonstrated high levels of similarity
with conventional relatedness-based methods. We also found that the clustering
results of proposed method outperformed those of conventional relatedness-based
measures regarding similarity with natural language processing--based
classification.
"
2889,"A Digital Ecosystem for Animal Movement Science: Making animal movement
  datasets, data-linkage techniques, methods, and environmental layers easier
  to find, interpret, and analyze","  Movement is a fundamental aspect of animal life and plays a crucial role in
determining the structure of population dynamics, communities, ecosystems, and
diversity. In recent years, the recording of animal movements via GPS collars,
camera traps, acoustic sensors, and citizen science, along with the abundance
of environmental and other ancillary data used by researchers to contextualize
those movements, has reached a level of volume, velocity, and variety that puts
movement ecology research in the realm of big data science. That data growth
has spawned increasingly complex methods for movement analysis. Consequently,
animal ecologists need a greater understanding of technical skills such as
statistics, geographic information systems (GIS), remote sensing, and coding.
Therefore, collaboration has become increasingly crucial, as research requires
both domain knowledge and technical expertise. Datasets of animal movement and
environmental data are typically available in repositories run by government
agencies, universities, and non-governmental organizations (NGOs) with methods
described in scientific journals. However, there is little connectivity between
these entities. The construction of a digital ecosystem for animal movement
science is critically important right now. The digital ecosystem represents a
setting where movement data, environmental layers, and analysis methods are
discoverable and available for efficient storage, manipulation, and analysis.
We argue that such a system which will help mature the field of movement
ecology by engendering collaboration, facilitating replication, expanding the
spatiotemporal range of potential analyses, and limiting redundancy in method
development. We describe the key components of the digital ecosystem, the
critical challenges that would need addressing, as well as potential solutions
to those challenges.
"
2890,"Improving Scholarly Knowledge Representation: Evaluating BERT-based
  Models for Scientific Relation Classification","  With the rapid growth of research publications, there is a vast amount of
scholarly knowledge that needs to be organized in digital libraries. To deal
with this challenge, techniques relying on knowledge-graph structures are being
advocated. Within such graph-based pipelines, inferring relation types between
related scientific concepts is a crucial step. Recently, advanced techniques
relying on language models pre-trained on the large corpus have been popularly
explored for automatic relation classification. Despite remarkable
contributions that have been made, many of these methods were evaluated under
different scenarios, which limits their comparability. To this end, we present
a thorough empirical evaluation on eight Bert-based classification models by
focusing on two key factors: 1) Bert model variants, and 2) classification
strategies. Experiments on three corpora show that domain-specific pre-training
corpus benefits the Bert-based classification model to identify the type of
scientific relations. Although the strategy of predicting a single relation
each time achieves a higher classification accuracy than the strategy of
identifying multiple relation types simultaneously in general, the latter
strategy demonstrates a more consistent performance in the corpus with either a
large or small size of annotations. Our study aims to offer recommendations to
the stakeholders of digital libraries for selecting the appropriate technique
to build knowledge-graph-based systems for enhanced scholarly information
organization.
"
2891,"Using altmetrics for detecting impactful research in quasi-zero-day
  time-windows: the case of COVID-19","  On December 31st 2019, the World Health Organization (WHO) China Country
Office was informed of cases of pneumonia of unknown etiology detected in Wuhan
City. The cause of the syndrome was a new type of coronavirus isolated on
January 7th 2020 and named Severe Acute Respiratory Syndrome CoronaVirus 2
(SARS-CoV-2). SARS-CoV-2 is the cause of the coronavirus disease 2019
(COVID-19). Since January 2020 an ever increasing number of scientific works
have appeared in literature. Identifying relevant research outcomes at very
early stages is challenging. In this work we use COVID-19 as a use-case for
investigating: (i) which tools and frameworks are mostly used for early
scholarly communication; (ii) to what extent altmetrics can be used to identify
potential impactful research in tight (i.e. quasi-zero-day) time-windows. A
literature review with rigorous eligibility criteria is performed for gathering
a sample composed of scientific papers about SARS-CoV-2/COVID-19 appeared in
literature in the tight time-window ranging from January 15th 2020 to February
24th 2020. This sample is used for building a knowledge graph that represents
the knowledge about papers and indicators formally. This knowledge graph feeds
a data analysis process which is applied for experimenting with altmetrics as
impact indicators. We find moderate correlation among traditional citation
count, citations on social media, and mentions on news and blogs. This suggests
there is a common intended meaning of the citational acts associated with
aforementioned indicators. Additionally, we define a method that harmonises
different indicators for providing a multi-dimensional impact indicator.
"
2892,"Scholarly migration within Mexico: Analyzing internal migration among
  researchers using Scopus longitudinal bibliometric data","  The migration of scholars is a major driver of innovation and of diffusion of
knowledge. Although large-scale bibliometric data have been used to measure
international migration of scholars, our understanding of internal migration
among researchers is very limited. This is partly due to a lack of data
aggregated at a suitable sub-national level. In this study, we analyze internal
migration in Mexico based on over 1.1 million authorship records from the
Scopus database. We trace the movements of scholars between Mexican states, and
provide key demographic measures of internal migration for the 1996-2018
period. From a methodological perspective, we develop a new framework for
enhancing data quality, inferring states from affiliations, and detecting moves
from modal states for the purposes of studying internal migration among
researchers. Substantively, we combine demographic and network science
techniques to improve our understanding of internal migration patterns within
country boundaries. The migration patterns between states in Mexico appear to
be heterogeneous in size and direction across regions. However, while many
scholars remain in their regions, there seems to be a preference for Mexico
City and the surrounding states as migration destinations. We observed that
over the past two decades, there has been a general decreasing trend in the
crude migration intensity. However, the migration network has become more dense
and more diverse, and has included greater exchanges between states along the
Gulf and the Pacific Coast. Our analysis, which is mostly empirical in nature,
lays the foundations for testing and developing theories that can rely on the
analytical framework developed by migration scholars, and the richness of
appropriately processed bibliometric data.
"
2893,"Daily growth rate of scientific production on Covid-19. Analysis in
  databases and open access repositories","  The scientific community is facing one of its greatest challenges in solving
a global health problem: COVID-19 pandemic. This situation has generated an
unprecedented volume of publications. What is the volume, in terms of
publications, of research on COVID-19? The general objective of this research
work is to obtain a global vision of the daily growth of scientific production
on COVID-19 in different databases (Dimensions, Web of Science Core Collection,
Scopus-Elsevier, Pubmed and eight repositories). In relation to the results
obtained, Dimensions indexes a total of 9435 publications (69% with peer review
and 2677 preprints) well above Scopus (1568) and WoS (718). This is a classic
biliometric phenomenon of exponential growth (R2 = 0.92). The global growth
rate is 500 publications and the production doubles every 15 days. In the case
of Pubmed the weekly growth is around 1000 publications. Of the eight
repositories analysed, Pubmed Central, Medrxiv and SSRN are the leaders.
Despite their enormous contribution, the journals continue to be the core of
scientific communication. Finally, it has been established that three out of
every four publications on the COVID-19 are available in open access. The
information explosion demands a serious and coordinated response from
information professionals, which places us at the centre of the information
pandemic.
"
2894,"Characterizing Research Leadership on Geographical Weighted
  Collaboration Network","  Research collaborations, especially long-distance and cross-border
collaborations, have become increasingly prevalent worldwide. Recent studies
highlighted the significant role of research leadership in collaborations.
However, existing measures of the research leadership do not take into account
the intensity of leadership in the co-authorship network. More importantly, the
spatial features, which influence the collaboration patterns and research
outcomes, have not been incorporated in measuring the research leadership. To
fill the gap, we construct an institution-level weighted co-authorship network
that has two types of weight on the edges: the intensity of collaborations and
the spatial score (the geographic distance adjusted by the cross-border
nature). Based on this network, we propose a novel metric, namely the spatial
research leadership rank (SpatialLeaderRank), to identify the leading
institutions while considering both the collaboration intensity and the spatial
features. Harnessing a dataset of 323,146 journal publications in
pharmaceutical sciences during 2010-2018, we perform a comprehensive analysis
of the geographical distribution and dynamic patterns of research leadership
flows at the institution level. The results demonstrate that the
SpatialLeaderRank outperforms baseline metrics in predicting the scholarly
impact of institutions. And the result remains robust in the field of
Information Science & Library Science.
"
2895,Nouvelles repr\'esentations concises exactes des motifs rares,"  Until a present, the majority of work in data mining were interested in the
extraction of the frequent itemsets and the generation of the frequent
association rules from these itemsets. Sometimes, the frequent of associations
rules can revealed not-interesting in the direction where a frequent behavior
is in general a normal behavior in the database. These last years, some work
was focused on the exploitation and the extraction of rare itemset and shows
them interest. However, the very important size of those itemset was the
handicap of algorithms that exploit the rare pattern. In order to relieve this
problem, the present report proposes two exact concise representations of the
rare itemset, one based on the minimal generators and the other based on the
closed itemset. In this context, we introduce two new algorithms called GMRare
and MFRare which extract these two exact concise representations.
"
2896,"Unveiling the distinctive traits of a nation's research performance: the
  case of Italy and Norway","  In this study we are analysing the research performance of Italian and
Norwegian professors using constituent components of the Fractional Scientific
Strength (FSS) indicator. The main focus is on differences across fields in
publication output and citation impact. The overall performance (FSS) of the
two countries, which differ considerably in research size and profile, is
remarkedly similar. However, an in-depth analysis shows that there are large
underlying performance differences. An average Italian professor publishes more
papers than a Norwegian, while the citation impact of the research output is
higher for the Norwegians. In addition, at field level the pattern varies along
both dimensions, and we analyse in which fields each country have their
relative strengths. Overall, this study contributes to further insights on how
the research performance of different countries may be analysed and compared,
to inform research policy.
"
2897,"Algorithmic labeling in hierarchical classifications of publications:
  Evaluation of bibliographic fields and term weighting approaches","  Algorithmic classifications of research publications can be used to study
many different aspects of the science system, such as the organization of
science into fields, the growth of fields, interdisciplinarity, and emerging
topics. How to label the classes in these classifications is a problem that has
not been thoroughly addressed in the literature. In this study we evaluate
different approaches to label the classes in algorithmically constructed
classifications of research publications. We focus on two important choices:
the choice of (1) different bibliographic fields and (2) different approaches
to weight the relevance of terms. To evaluate the different choices, we created
two baselines: one based on the Medical Subject Headings in MEDLINE and another
based on the Science-Metrix journal classification. We tested to what extent
different approaches yield the desired labels for the classes in the two
baselines. Based on our results we recommend extracting terms from titles and
keywords to label classes at high levels of granularity (e.g. topics). At low
levels of granularity (e.g. disciplines) we recommend extracting terms from
journal names and author addresses. We recommend the use of a new approach,
term frequency to specificity ratio, to calculate the relevance of terms.
"
2898,On the rules of science game,"  Credit allocation in the mainstream bibliometrics is fundamentally flawed and
the popular indicators have been misleading science for decades. Originally a
simple technical mistake has become an integral part of our culture and is very
difficult to correct. Although the problem has been raised in scientific
articles, it seems mostly unknown to wider audience.
"
2899,How Reliable are University Rankings?,"  University or college rankings have almost become an industry of their own,
published by US News \& World Report (USNWR) and similar organizations. Most of
the rankings use a similar scheme: Rank universities in decreasing score order,
where each score is computed using a set of attributes and their weights; the
attributes can be objective or subjective while the weights are always
subjective. This scheme is general enough to be applied to ranking objects
other than universities. As shown in the related work, these rankings have
important implications and also many issues. In this paper, we take a fresh
look at this ranking scheme using the public College dataset; we both formally
and experimentally show in multiple ways that this ranking scheme is not
reliable and cannot be trusted as authoritative because it is too sensitive to
weight changes and can easily be gamed. For example, we show how to derive
reasonable weights programmatically to move multiple universities in our
dataset to the top rank; moreover, this task takes a few seconds for over 600
universities on a personal laptop. Our mathematical formulation, methods, and
results are applicable to ranking objects other than universities too. We
conclude by making the case that all the data and methods used for rankings
should be made open for validation and repeatability.
"
2900,"Innovation and Revenue: Deep Diving into the Temporal Rank-shifts of
  Fortune 500 Companies","  Research and innovation is important agenda for any company to remain
competitive in the market. The relationship between innovation and revenue is a
key metric for companies to decide on the amount to be invested for future
research. Two important parameters to evaluate innovation are the quantity and
quality of scientific papers and patents. Our work studies the relationship
between innovation and patenting activities for several Fortune 500 companies
over a period of time. We perform a comprehensive study of the patent citation
dataset available in the Reed Technology Index collected from the US Patent
Office. We observe several interesting relations between parameters like the
number of (i) patent applications, (ii) patent grants, (iii) patent citations
and Fortune 500 ranks of companies. We also study the trends of these
parameters varying over the years and derive causal explanations for these with
qualitative and intuitive reasoning. To facilitate reproducible research, we
make all the processed patent dataset publicly available at
https://github.com/mayank4490/Innovation-and-revenue.
"
2901,"On the Performance of Hybrid Search Strategies for Systematic Literature
  Reviews in Software Engineering","  Context: When conducting a Systematic Literature Review (SLR), researchers
usually face the challenge of designing a search strategy that appropriately
balances result quality and review effort. Using digital library (or database)
searches or snowballing alone may not be enough to achieve high-quality
results. On the other hand, using both digital library searches and snowballing
together may increase the overall review effort.
  Objective: The goal of this research is to propose and evaluate hybrid search
strategies that selectively combine database searches with snowballing.
  Method: We propose four hybrid search strategies combining database searches
in digital libraries with iterative, parallel, or sequential backward and
forward snowballing. We simulated the strategies over three existing SLRs in SE
that adopted both database searches and snowballing. We compared the outcome of
digital library searches, snowballing, and hybrid strategies using precision,
recall, and F-measure to investigate the performance of each strategy.
  Results: Our results show that, for the analyzed SLRs, combining database
searches from the Scopus digital library with parallel or sequential
snowballing achieved the most appropriate balance of precision and recall.
  Conclusion: We put forward that, depending on the goals of the SLR and the
available resources, using a hybrid search strategy involving a representative
digital library and parallel or sequential snowballing tends to represent an
appropriate alternative to be used when searching for evidence in SLRs.
"
2902,Rankings of countries based on rankings of universities,"  Although many methods have been designed for ranking universities, there is
no suitable system that focuses on the ranking of countries based on the
performance of their universities. The overall ranking of the universities in a
region can indicate the growth of interests in science among the people of that
land. This paper introduces a novel ranking mechanism based on the rankings of
universities. Firstly, we introduce and discuss two new rankings of countries,
based on the rank of their universities. Secondly, we create rankings of
countries according to the selected method, based on the top 12000 universities
in webometrics.info (January 2012) and compare rankings of countries in 4
editions (January 2012 to July 2013). Firstly, we introduce two new methods of
ranking countries based on their university rankings, Weighted Ranking (WR) and
Average Ranking (AR). Secondly, we discuss how the introduced ranking systems,
perform in ranking countries based on the two years of data. Thirdly, we choose
QS (http://www.topuniversities.com) and webometrics.info as two different
classification systems for comparing rankings of countries, based on the top
500 universities in these rankings. Results indicate that the methodology can
be used to show the quality of the whole universities of each country used to
compare rankings of countries in practice compare to other countries in the
world.
"
2903,"Use of Wikipedia categories on information retrieval research: a brief
  review","  Wikipedia categories, a classification scheme built for organizing and
describing Wikpedia articles, are being applied in computer science research.
This paper adopts a systematic literature review approach, in order to identify
different approaches and uses of Wikipedia categories in information retrieval
research. Several types of work are identified, depending on the intrinsic
study of the categories structure, or its use as a tool for the processing and
analysis of other documentary corpus different to Wikipedia. Information
retrieval is identified as one of the major areas of use, in particular its
application in the refinement and improvement of search expressions, and the
construction of textual corpus. However, the set of available works shows that
in many cases research approaches applied and results obtained can be
integrated into a comprehensive and inclusive concept of information retrieval.
"
2904,The rise of science in low-carbon energy technologies,"  Successfully combating climate change will require substantial technological
improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient
allocation of R\&D budgets requires a better understanding of how LCETs rely on
scientific knowledge. Using data covering almost all US patents and scientific
articles that are cited by them over the past two centuries, we describe the
evolution of knowledge bases of ten key LCETs and show how technological
interdependencies have changed over time. The composition of low-carbon energy
innovations shifted over time, from Hydro and Wind energy in the 19th and early
20th century, to Nuclear fission after World War II, and more recently to Solar
PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels
(including energy from waste) have 35-65\% of their citations directed toward
scientific papers, while this ratio is less than 10\% for Wind, Solar thermal,
Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing
science and the share of citations that are to scientific papers has been
increasing for all technology types. The analysis of the scientific knowledge
base of each LCET reveals three fairly separate clusters, with nuclear energy
technologies, Biofuels and Waste, and all the other LCETs. Our detailed
description of knowledge requirements for each LCET helps to design of targeted
innovation policies.
"
2905,"COVID-19 publications: Database coverage, citations, readers, tweets,
  news, Facebook walls, Reddit posts","  The COVID-19 pandemic requires a fast response from researchers to help
address biological, medical and public health issues to minimize its impact. In
this rapidly evolving context, scholars, professionals and the public may need
to quickly identify important new studies. In response, this paper assesses the
coverage of scholarly databases and impact indicators during 21 March to 18
April 2020. The results confirm a rapid increase in the volume of research,
which particularly accessible through Google Scholar and Dimensions, and less
through Scopus, the Web of Science, PubMed. A few COVID-19 papers from the
21,395 in Dimensions were already highly cited, with substantial news and
social media attention. For this topic, in contrast to previous studies, there
seems to be a high degree of convergence between articles shared in the social
web and citation counts, at least in the short term. In particular, articles
that are extensively tweeted on the day first indexed are likely to be highly
read and relatively highly cited three weeks later. Researchers needing wide
scope literature searches (rather than health focused PubMed or medRxiv
searches) should start with Google Scholar or Dimensions and can use tweet and
Mendeley reader counts as indicators of likely importance.
"
2906,CORD-19: The COVID-19 Open Research Dataset,"  The COVID-19 Open Research Dataset (CORD-19) is a growing resource of
scientific papers on COVID-19 and related historical coronavirus research.
CORD-19 is designed to facilitate the development of text mining and
information retrieval systems over its rich collection of metadata and
structured full text papers. Since its release, CORD-19 has been downloaded
over 200K times and has served as the basis of many COVID-19 text mining and
discovery systems. In this article, we describe the mechanics of dataset
construction, highlighting challenges and key design decisions, provide an
overview of how CORD-19 has been used, and describe several shared tasks built
around the dataset. We hope this resource will continue to bring together the
computing community, biomedical experts, and policy makers in the search for
effective treatments and management policies for COVID-19.
"
2907,Validation of counting methods in bibliometrics,"  The discussion about counting methods in bibliometrics is often reduced to
the choice between full and fractional counting. However, several studies
document that this distinction is too simple. The aim of the present study is
to give an overview of counting methods in the bibliometric literature and to
provide insight into their properties and use. A mix of methods is used. In the
preliminary results, a literature review covering 1970-2018 identified 29
original counting methods. Seventeen were introduced in the period 2010-2018.
Twenty-one of the 29 counting methods are rank-dependent and fractionalized
meaning that the authors of a publications share 1 credit but do not receive
equal shares, for example harmonic counting. The internal and external
validation of the counting methods are assessed. Three criteria for
well-constructed bibliometric indicators - adequacy, sensitivity, and
homogeneity - are used to assess the internal validity. Regarding the external
validation of the counting methods, it is investigated whether the intentions
in the studies that introduced the 29 counting methods comply with the
subsequent use of the counting methods. This study has the potential to give a
solid foundation for the use of and discussion about counting methods.
"
2908,"Visible Insights of the Invisible Pandemic: A Scientometric, Altmetric
  and Topic Trend Analysis","  The recent SARS-COV-2 virus outbreak has created an unprecedented global
health crisis! The disease is showing alarming trends with the number of people
getting infected with this disease, new cases and death rate are all
highlighting the need to control this disease at the earliest. The strategy now
for the governments around the globe is how to limit the spread of the virus
until the research community develops treatment/drug or vaccination against the
virus. The outbreak of this disease has unsurprisingly led to huge volume of
research within a short period of time surrounding this disease. It has also
led to aggressive social media activity on twitter, Facebook, dedicated blogs,
news reports and other online sites actively involved in discussing about the
various aspects of and related to this disease. It becomes a useful and
challenging exercise to draw from this huge volume of research, the key papers
that form the research front, its influence in the research community, and
other important research insights. Similarly, it becomes important to discern
the key issues that influence the society concerning this disease. The paper is
motivated by this. It attempts to distinguish which are the most influential
papers, the key knowledge base and major topics surrounding the research
covered by COVID-19. Further it attempts to capture the society's perception by
discerning key topics that are trending online. The study concludes by
highlighting the implications of this study.
"
2909,"Jupyter notebooks as discovery mechanisms for open science: Citation
  practices in the astronomy community","  Citing data and software is a means to give scholarly credit and to
facilitate access to research objects. Citation principles encourage authors to
provide full descriptions of objects, with stable links, in their papers. As
Jupyter notebooks aggregate data, software, and other objects, they may
facilitate or hinder citation, credit, and access to data and software. We
report on a study of references to Jupyter notebooks in astronomy over a 5-year
period (2014-2018). References increased rapidly, but fewer than half of the
references led to Jupyter notebooks that could be located and opened. Jupyter
notebooks appear better suited to supporting the research process than to
providing access to research objects. We recommend that authors cite individual
data and software objects, and that they stabilize any notebooks cited in
publications. Publishers should increase the number of citations allowed in
papers and employ descriptive metadata-rich citation styles that facilitate
credit and discovery.
"
2910,"A gender equality paradox in academic publishing: Countries with a
  higher proportion of female first-authored journal articles have larger first
  author gender disparities between fields","  Current attempts to address the shortfall of female researchers in Science,
Technology, Engineering and Mathematics (STEM) have not yet succeeded despite
other academic subjects having female majorities. This article investigates the
extent to which gender disparities are subject-wide or nation-specific by a
first author gender comparison of 30 million articles from all 27 Scopus broad
fields within the 31 countries with the most Scopus-indexed articles 2014-18.
The results show overall and geocultural patterns as well as individual
national differences. Almost half of the subjects were always more male (7;
e.g., Mathematics) or always more female (6; e.g., Immunology & Microbiology)
than the national average. A strong overall trend (Spearman correlation 0.546)
is for countries with a higher proportion of female first-authored research to
also have larger differences in gender disparities between fields (correlation
0.314 for gender ratios). This confirms the international gender equality
paradox previously found for degree subject choices: increased gender equality
overall associates with moderately greater gender differentiation between
subjects. This is consistent with previous USA-based claims that gender
differences in academic careers are partly due to (socially constrained) gender
differences in personal preferences. Radical solutions may therefore be needed
for some STEM subjects to overcome gender disparities.
"
2911,QURATOR: Innovative Technologies for Content and Data Curation,"  In all domains and sectors, the demand for intelligent systems to support the
processing and generation of digital content is rapidly increasing. The
availability of vast amounts of content and the pressure to publish new content
quickly and in rapid succession requires faster, more efficient and smarter
processing and generation methods. With a consortium of ten partners from
research and industry and a broad range of expertise in AI, Machine Learning
and Language Technologies, the QURATOR project, funded by the German Federal
Ministry of Education and Research, develops a sustainable and innovative
technology platform that provides services to support knowledge workers in
various industries to address the challenges they face when curating digital
content. The project's vision and ambition is to establish an ecosystem for
content curation technologies that significantly pushes the current state of
the art and transforms its region, the metropolitan area Berlin-Brandenburg,
into a global centre of excellence for curation technologies.
"
2912,Citation Cascade and the Evolution of Topic Relevance,"  Citation analysis, as a tool for quantitative studies of science, has long
emphasized direct citation relations, leaving indirect or high order citations
overlooked. However, a series of early and recent studies demonstrate the
existence of indirect and continuous citation impact across generations. Adding
to the literature on high order citations, we introduce the concept of a
citation cascade: the constitution of a series of subsequent citing events
initiated by a certain publication. We investigate this citation structure by
analyzing more than 450,000 articles and over 6 million citation relations. We
show that citation impact exists not only within the three generations
documented in prior research, but also in much further generations. Still, our
experimental results indicate that two to four generations are generally
adequate to trace a work's scientific impact. We also explore specific
structural properties such as depth, width, structural virality, and size,
which account for differences among individual citation cascades. Finally, we
find evidence that it is more important for a scientific work to inspire trans
domain (or indirectly related domain) works than to receive only intra domain
recognition in order to achieve high impact. Our methods and findings can serve
as a new tool for scientific evaluation and the modeling of scientific history.
"
2913,A Novel Approach to Predicting Exceptional Growth in Research,"  The prediction of exceptional or surprising growth in research is an issue
with deep roots and few practical solutions. In this study we develop and
validate a novel approach to forecasting growth in highly specific research
communities. Each research community is represented by a cluster of papers.
Multiple indicators were tested, and a composite indicator was created that
predicts which research communities will experience exceptional growth over the
next three years. The accuracy of this predictor was tested using hundreds of
thousands of community-level forecasts and was found to exceed the performance
benchmarks established in Intelligence Advanced Research Projects Activity's
(IARPA) Foresight Using Scientific Exposition (FUSE) program in six of nine
major fields in science. Furthermore, ten of eleven disciplines within the
Computing Technologies field met the benchmarks. Specific detailed forecast
examples are given and evaluated, and a critical evaluation of the forecasting
approach is also provided.
"
2914,Informational Space of Meaning for Scientific Texts,"  In Natural Language Processing, automatic extracting the meaning of texts
constitutes an important problem. Our focus is the computational analysis of
meaning of short scientific texts (abstracts or brief reports). In this paper,
a vector space model is developed for quantifying the meaning of words and
texts. We introduce the Meaning Space, in which the meaning of a word is
represented by a vector of Relative Information Gain (RIG) about the subject
categories that the text belongs to, which can be obtained from observing the
word in the text. This new approach is applied to construct the Meaning Space
based on Leicester Scientific Corpus (LSC) and Leicester Scientific
Dictionary-Core (LScDC). The LSC is a scientific corpus of 1,673,350 abstracts
and the LScDC is a scientific dictionary which words are extracted from the
LSC. Each text in the LSC belongs to at least one of 252 subject categories of
Web of Science (WoS). These categories are used in construction of vectors of
information gains. The Meaning Space is described and statistically analysed
for the LSC with the LScDC. The usefulness of the proposed representation model
is evaluated through top-ranked words in each category. The most informative n
words are ordered. We demonstrated that RIG-based word ranking is much more
useful than ranking based on raw word frequency in determining the
science-specific meaning and importance of a word. The proposed model based on
RIG is shown to have ability to stand out topic-specific words in categories.
The most informative words are presented for 252 categories. The new scientific
dictionary and the 103,998 x 252 Word-Category RIG Matrix are available online.
Analysis of the Meaning Space provides us with a tool to further explore
quantifying the meaning of a text using more complex and context-dependent
meaning models that use co-occurrence of words and their combinations.
"
2915,"Are papers addressing certain diseases perceived where these diseases
  are prevalent? The proposal to use Twitter data as social-spatial sensors","  We propose to use Twitter data as social-spatial sensors. This study deals
with the question whether research papers on certain diseases are perceived by
people in regions (worldwide) that are especially concerned by the diseases.
Since (some) Twitter data contain location information, it is possible to
spatially map the activity of Twitter users referring to certain papers (e.g.,
dealing with tuberculosis). The resulting maps reveal whether heavy activity on
Twitter is correlated with large numbers of people having certain diseases. In
this study, we focus on tuberculosis, human immunodeficiency virus (HIV), and
malaria, since the World Health Organization ranks these diseases as the top
three causes of death worldwide by a single infectious agent. The results of
the social-spatial Twitter maps (and additionally performed regression models)
reveal the usefulness of the proposed sensor approach. One receives an
impression of how research papers on the diseases have been perceived by people
in regions that are especially concerned by the diseases. Our study
demonstrates a promising approach for using Twitter data for research
evaluation purposes beyond simple counting of tweets.
"
2916,"Google Scholar, Microsoft Academic, Scopus, Dimensions, Web of Science,
  and OpenCitations' COCI: a multidisciplinary comparison of coverage via
  citations","  New sources of citation data have recently become available, such as
Microsoft Academic, Dimensions, and the OpenCitations Index of CrossRef open
DOI-to-DOI citations (COCI). Although these have been compared to the Web of
Science (WoS), Scopus, or Google Scholar, there is no systematic evidence of
their differences across subject categories. In response, this paper
investigates 3,073,351 citations found by these six data sources to 2,515
English-language highly-cited documents published in 2006 from 252 subject
categories, expanding and updating the largest previous study. Google Scholar
found 88% of all citations, many of which were not found by the other sources,
and nearly all citations found by the remaining sources (89%-94%). A similar
pattern held within most subject categories. Microsoft Academic is the second
largest overall (60% of all citations), including 82% of Scopus citations and
86% of Web of Science citations. In most categories, Microsoft Academic found
more citations than Scopus and WoS (182 and 223 subject categories,
respectively), but had coverage gaps in some areas, such as Physics and some
Humanities categories. After Scopus, Dimensions is fourth largest (54% of all
citations), including 84% of Scopus citations and 88% of WoS citations. It
found more citations than Scopus in 36 categories, more than WoS in 185, and
displays some coverage gaps, especially in the Humanities. Following WoS, COCI
is the smallest, with 28% of all citations. Google Scholar is still the most
comprehensive source. In many subject categories Microsoft Academic and
Dimensions are good alternatives to Scopus and WoS in terms of coverage.
"
2917,"Getting Insights from a Large Corpus of Scientific Papers on
  Specialisted Comprehensive Topics -- the Case of COVID-19","  COVID-19 is one of the most important topic these days, specifically on
search engines and news. While fake news are easily shared, scientific papers
are reliable sources where information can be extracted. With about 24,000
scientific publications on COVID-19 and related research on PUBMED, automatic
computer-assisted analysis is required. In this paper, we develop two
methodologies to get insights on specific sub-topics of interest and latest
research sub-topics. They rely on natural language processing and graph-based
visualizations. We run these methodologies on two cases: the virus origin and
the uses of existing drugs.
"
2918,Code Replicability in Computer Graphics,"  Being able to duplicate published research results is an important process of
conducting research whether to build upon these findings or to compare with
them. This process is called ""replicability"" when using the original authors'
artifacts (e.g., code), or ""reproducibility"" otherwise (e.g., re-implementing
algorithms). Reproducibility and replicability of research results have gained
a lot of interest recently with assessment studies being led in various fields,
and they are often seen as a trigger for better result diffusion and
transparency. In this work, we assess replicability in Computer Graphics, by
evaluating whether the code is available and whether it works properly. As a
proxy for this field we compiled, ran and analyzed 151 codes out of 374 papers
from 2014, 2016 and 2018 SIGGRAPH conferences. This analysis shows a clear
increase in the number of papers with available and operational research codes
with a dependency on the subfields, and indicates a correlation between code
replicability and citation count. We further provide an interactive tool to
explore our results and evaluation data.
"
2919,Examining Citations of Natural Language Processing Literature,"  We extracted information from the ACL Anthology (AA) and Google Scholar (GS)
to examine trends in citations of NLP papers. We explore questions such as: how
well cited are papers of different types (journal articles, conference papers,
demo papers, etc.)? how well cited are papers from different areas of within
NLP? etc. Notably, we show that only about 56\% of the papers in AA are cited
ten or more times. CL Journal has the most cited papers, but its citation
dominance has lessened in recent years. On average, long papers get almost
three times as many citations as short papers; and papers on sentiment
classification, anaphora resolution, and entity recognition have the highest
median citations. The analyses presented here, and the associated dataset of
NLP papers mapped to citations, have a number of uses including: understanding
how the field is growing and quantifying the impact of different types of
papers.
"
2920,"Gender Gap in Natural Language Processing Research: Disparities in
  Authorship and Citations","  Disparities in authorship and citations across gender can have substantial
adverse consequences not just on the disadvantaged genders, but also on the
field of study as a whole. Measuring gender gaps is a crucial step towards
addressing them. In this work, we examine female first author percentages and
the citations to their papers in Natural Language Processing (1965 to 2019). We
determine aggregate-level statistics using an existing manually curated
author--gender list as well as first names strongly associated with a gender.
We find that only about 29% of first authors are female and only about 25% of
last authors are female. Notably, this percentage has not improved since the
mid 2000s. We also show that, on average, female first authors are cited less
than male first authors, even when controlling for experience and area of
research. Finally, we discuss the ethical considerations involved in automatic
demographic analysis.
"
2921,"Adoption of the open access business model in scientific journal
  publishing: A cross-disciplinary study","  Scientific journal publishers have over the past twenty-five years rapidly
converted to predominantly electronic dissemination, but the reader-pays
business model continues to dominate the market. Open Access (OA) publishing,
where the articles are freely readable on the net, has slowly increased its
market share to near 20%, but has failed to fulfill the visions of rapid
proliferation predicted by many early proponents. The growth of OA has also
been very uneven across fields of science. We report market shares of open
access in eighteen Scopus-indexed disciplines ranging from 27% (agriculture) to
7% (business). The differences become far more pronounced for journals
published in the four countries, which dominate commercial scholarly publishing
(US, UK, Germany and the Netherlands). We present contrasting developments
within six academic disciplines. Availability of funding to pay publication
charges, pressure from research funding agencies, and the diversity of
discipline-specific research communication cultures arise as potential
explanations for the observed differences.
"
2922,"Piveau: A Large-scale Open Data Management Platform based on Semantic
  Web Technologies","  The publication and (re)utilization of Open Data is still facing multiple
barriers on technical, organizational and legal levels. This includes
limitations in interfaces, search capabilities, provision of quality
information and the lack of definite standards and implementation guidelines.
Many Semantic Web specifications and technologies are specifically designed to
address the publication of data on the web. In addition, many official
publication bodies encourage and foster the development of Open Data standards
based on Semantic Web principles. However, no existing solution for managing
Open Data takes full advantage of these possibilities and benfits. In this
paper, we present our solution ""Piveau"", a fully-fledged Open Data management
solution, based on Semantic Web technologies. It harnesses a variety of
standards, like RDF, DCAT, DQV, and SKOS, to overcome the barriers in Open Data
publication. The solution puts a strong focus on assuring data quality and
scalability. We give a detailed description of the underlying, highly scalable,
service-oriented architecture, how we integrated the aforementioned standards,
and used a triplestore as our primary database. We have evaluated our work in a
comprehensive feature comparison to established solutions and through a
practical application in a production environment, the European Data Portal.
Our solution is available as Open Source.
"
2923,"Building Journal Impact Factor Quartile into the Assessment of Academic
  Performance: A Case Study","  This study aims to provide information about the Q Concept defined as the
division of journal impact factors into quartiles based on given field
categories so that the disadvantages resulting from the direct use of journal
impact factors can be eliminated. While the number of ""Original articles
published in the Web of Science (WoS) database-indexed journals like SCI, SSCI
and A&HCI"" is an important indicator for research assessment in Turkey, neither
the journal impact factors nor the Q Concept of these papers have been taken
into account. Present study analyzes the scientific production of the Amasya
University researchers in journals indexed in WoS database in the period
2014-2018 using the Q concept. The share of publications by Q category journals
as well as the average citations received by the works from Amasya University
were compared to the average situation in Turkey and other different countries
in the world. Results indicate that the articles published by Amasya University
researchers were mostly published in low impact factor journals (Q4 journals)
(36.49%), in fact, only a small share of papers were published in high impact
journals (14.32% in Q1 journals). The share of papers published in low impact
journals by researchers from Amasya University is higher than the Turkish
average and much higher than the scientific leading countries. The average
citations received by papers published in Q1 journals was around six times
higher than papers published in Q4 journals (8.92 vs. 1.56), thus papers
published in Q1 journals received 30.02% citations despite only 14.32% of the
papers was published in these journals. The share of papers published which
were never cited in WoS was 27.48%, increasing from 9.68% in Q1 to almost half
(48.10%) in Q4. The study concludes with some suggestions on how and where the
Q Concept can be used.
"
2924,"Advancing computational reproducibility in the Dataverse data repository
  platform","  Recent reproducibility case studies have raised concerns showing that much of
the deposited research has not been reproducible. One of their conclusions was
that the way data repositories store research data and code cannot fully
facilitate reproducibility due to the absence of a runtime environment needed
for the code execution. New specialized reproducibility tools provide
cloud-based computational environments for code encapsulation, thus enabling
research portability and reproducibility. However, they do not often enable
research discoverability, standardized data citation, or long-term archival
like data repositories do. This paper addresses the shortcomings of data
repositories and reproducibility tools and how they could be overcome to
improve the current lack of computational reproducibility in published and
archived research outputs.
"
2925,"Global Distribution of Google Scholar Citations: A Size-independent
  Institution-based Analysis","  Most currently available schemes for performance based ranking of
Universities or Research organizations, such as, Quacarelli Symonds (QS), Times
Higher Education (THE), Shanghai University based All Research of World
Universities (ARWU) use a variety of criteria that include productivity,
citations, awards, reputation, etc., while Leiden and Scimago use only
bibliometric indicators. The research performance evaluation in the aforesaid
cases is based on bibliometric data from Web of Science or Scopus, which are
commercially available priced databases. The coverage includes peer reviewed
journals and conference proceedings. Google Scholar (GS) on the other hand,
provides a free and open alternative to obtaining citations of papers available
on the net, (though it is not clear exactly which journals are covered.)
Citations are collected automatically from the net and also added to self
created individual author profiles under Google Scholar Citations (GSC). This
data was used by Webometrics Lab, Spain to create a ranked list of 4000+
institutions in 2016, based on citations from only the top 10 individual GSC
profiles in each organization. (GSC excludes the top paper for reasons
explained in the text; the simple selection procedure makes the ranked list
size-independent as claimed by the Cybermetrics Lab). Using this data
(Transparent Ranking TR, 2016), we find the regional and country wise
distribution of GS-TR Citations. The size independent ranked list is subdivided
into deciles of 400 institutions each and the number of institutions and
citations of each country obtained for each decile. We test for correlation
between institutional ranks between GS TR and the other ranking schemes for the
top 20 institutions.
"
2926,Building a PubMed knowledge graph,"  PubMed is an essential resource for the medical domain, but useful concepts
are either difficult to extract or are ambiguated, which has significantly
hindered knowledge discovery. To address this issue, we constructed a PubMed
knowledge graph (PKG) by extracting bio-entities from 29 million PubMed
abstracts, disambiguating author names, integrating funding data through the
National Institutes of Health (NIH) ExPORTER, collecting affiliation history
and educational background of authors from ORCID, and identifying fine-grained
affiliation data from MapAffil. Through the integration of the credible
multi-source data, we could create connections among the bio-entities, authors,
articles, affiliations, and funding. Data validation revealed that the BioBERT
deep learning method of bio-entity extraction significantly outperformed the
state-of-the-art models based on the F1 score (by 0.51%), with the author name
disambiguation (AND) achieving a F1 score of 98.09%. PKG can trigger broader
innovations, not only enabling us to measure scholarly impact, knowledge usage,
and knowledge transfer, but also assisting us in profiling authors and
organizations based on their connections with bio-entities. The PKG is freely
available on Figshare (https://figshare.com/s/6327a55355fc2c99f3a2, simplified
version that exclude PubMed raw data) and TACC website
(http://er.tacc.utexas.edu/datasets/ped, full version).
"
2927,"Day of the week submission effect for accepted papers in Physica A, PLOS
  ONE, Nature and Cell","  The particular day of the week when an event occurs seems to have unexpected
consequences. For example, the day of the week when a paper is submitted to a
peer reviewed journal correlates with whether that paper is accepted. Using an
econometric analysis (a mix of log-log and semi-log based on undated and panel
structured data) we find that more papers are submitted to certain peer review
journals on particular weekdays than others, with fewer papers being submitted
on weekends. Seasonal effects, geographical information as well as potential
changes over time are examined. This finding rests on a large (178 000) and
reliable sample; the journals polled are broadly recognized (Nature, Cell, PLOS
ONE and Physica A). Day of the week effect in the submission of accepted papers
should be of interest to many researchers, editors and publishers, and perhaps
also to managers and psychologists.
"
2928,"Prediction of scientific collaborations through multiplex interaction
  networks","  Link prediction algorithms can help to understand the structure and dynamics
of scientific collaborations and the evolution of Science. However, available
algorithms based on similarity between nodes of collaboration networks are
bounded by the limited amount of links present in these networks. In this work,
we reduce the latter intrinsic limitation by generalizing the Adamic-Adar
method to multiplex networks composed by an arbitrary number of layers, that
encode diverse forms of scientific interactions. We show that the new metric
outperforms other single-layered, similarity-based scores and that scientific
credit, represented by citations, and common interests, measured by the usage
of common keywords, can be predictive of new collaborations. Our work paves the
way for a deeper understanding of the dynamics driving scientific
collaborations, and provides a new algorithm for link prediction in multiplex
networks that can be applied to a plethora of systems.
"
2929,"Classification of abrupt changes along viewing profiles of scientific
  articles","  With the expansion of electronic publishing, a new dynamics of scientific
articles dissemination was initiated. Nowadays, many works are widely
disseminated even before publication, in the form of preprints. Another
important new element concerns the views of published articles. Thanks to the
availability of respective data by some journals, such as PLoS ONE, it became
possible to develop investigations on how scientific works are viewed along
time, often before the first citations appear. This provides the main theme of
the present work. More specifically, our research was motivated by preliminary
observations that the view profiles along time tend to present a piecewise
linear nature. A methodology was then delineated in order to identify the main
segments in the view profiles, which allowed several related measurements to be
derived. In particular, we focused on the inclination and length of each
subsequent segment. Basic statistics indicated that the inclination can vary
substantially along subsequent segments, while the segment lengths resulted
more stable. Complementary joint statistics analysis, considering pairwise
correlations, provided further information about the properties of the views.
In order to better understand the view profiles, we performed respective
multivariate statistical analysis, including principal component analysis and
hierarchical clustering. The results suggest that a portion of the polygonal
views are organized into clusters or groups. These groups were characterized in
terms of prototypes indicating the relative increase or decrease along
subsequent segments. Four respective distinct models were then developed for
representing the observed segments. It was found that models incorporating
joint dependencies between the properties of the segments provided the most
accurate results among the considered alternatives.
"
2930,Article citation study: Context enhanced citation sentiment detection,"  Citation sentimet analysis is one of the little studied tasks for
scientometric analysis. For citation analysis, we developed eight datasets
comprising citation sentences, which are manually annotated by us into three
sentiment polarities viz. positive, negative, and neutral. Among eight
datasets, three were developed by considering the whole context of citations.
Furthermore, we proposed an ensembled feature engineering method comprising
word embeddings obtained for texts, parts-of-speech tags, and dependency
relationships together. Ensembled features were considered as input to deep
learning based approaches for citation sentiment classification, which is in
turn compared with Bag-of-Words approach. Experimental results demonstrate that
deep learning is useful for higher number of samples, whereas support vector
machine is the winner for smaller number of samples. Moreover, context-based
samples are proved to be more effective than context-less samples for citation
sentiment analysis.
"
2931,"Peer Review: Objectivity, Anonymity, Trust","  This dissertation is focused on the role of objectivity in peer review.
Through an examination of aspects of peer review including anonymity, trust,
expertise, and the question of who has standing to evaluate research, we find
that objectivity in peer review differs significantly from other uses of the
term objectivity in science. In peer review it is not required for this
objectivity to have correspondence to an outside world, instead it is enough
for it to operate inside the ""rules"" of the community. Neither is the
objectivity here empirical in the sense of using data about the scientific
problem in question. Rather, the objectivity is one of judgment, cleaving to
the epistemological standards of a community that are formed by background
assumptions and beliefs. As a consequence, we highlight the role of
subjectivity in what is usually taken as a practice of objectivity, and arrive
at the insight that objectivity is not defined by one core value, but a balance
of transparency, confidentiality, trust, representation, and living up to
community standards. As such, objectivity in peer review is a highly specific
sense of the term that is not reducible to that used in other aspects of
scientific practice.
"
2932,Frequently Co-cited Publications: Features and Kinetics,"  Co-citation measurements can reveal the extent to which a concept
representing a novel combination of existing ideas evolves towards a specialty.
The strength of co-citation is represented by its frequency, which accumulates
over time. Of interest is whether underlying features associated with the
strength of co-citation can be identified. We use the proximal citation network
for a given pair of articles (x, y) to compute theta, an a priori estimate of
the probability of co-citation between x and y, prior to their first
co-citation.Thus, low values for theta reflect pairs of articles for which
co-citation is presumed less likely. We observe that co-citation frequencies
are a composite of power-law and lognormal distributions, and that very high
co-citation frequencies are more likely to be composed of pairs with low values
of theta, reflecting the impact of a novel combination of ideas. Furthermore,
we note that the occurrence of a direct citation between two members of a
co-cited pair increases with co-citation frequency. Finally, we identify cases
of frequently co-cited publications that accumulate co-citations after an
extended period of dormancy.
"
2933,"Citations versus expert opinions: Citation analysis of Featured Reviews
  of the American Mathematical Society","  Peer review and citation metrics are two means of gauging the value of
scientific research, but the lack of publicly available peer review data makes
the comparison of these methods difficult. Mathematics can serve as a useful
laboratory for considering these questions because as an exact science, there
is a narrow range of reasons for citations. In mathematics, virtually all
published articles are post-publication reviewed by mathematicians in
Mathematical Reviews (MathSciNet). For a decade, especially important articles
were singled out in Mathematical Reviews for featured reviews. In this study,
we analyze the bibliometrics of elite articles selected by peer review and by
citation count. We conclude that the two notions of significance described by
being a featured review article and being highly cited are substantially
distinct. This indicates that peer review and citation counts give largely
independent determinations of highly distinguished papers. In another
direction, we consider how hiring patterns of subfields and mathematicians'
interest in subfields may be assessed in terms of the subfields of featured
review and highly cited articles.
"
2934,"Co-author weighting in bibliometric methodology and subfields of a
  scientific discipline","  Collaborative work and co-authorship are fundamental to the advancement of
modern science. However, it is not clear how collaboration should be measured
in achievement-based metrics. Co-author weighted credit introduces distortions
into the bibliometric description of a discipline. It puts great weight on
collaboration - not based on the results of collaboration - but purely because
of the existence of collaborations. In terms of publication and citation
impact, it artificially favors some subdisciplines. In order to understand how
credit is given in a co-author weighted system (like the NRC's method), we
introduced credit spaces. We include a study of the discipline of physics to
illustrate the method. Indicators are introduced to measure the proportion of a
credit space awarded to a subfield or a set of authors.
"
2935,2020 NDSA Agenda for Digital Stewardship,"  The NDSA Agenda is a comprehensive overview of the state of global digital
preservation. It casts its eye over current research trends, grants, projects,
and various efforts spanning the preservation ecosystem. The agenda identifies
successes and ongoing challenges in addition to providing some tangible
recommendations to both researcher and practitioner alike. As both an overview
and comprehensive dive into digital preservation issues, the audience ranges
from high level to hands on experts. Funders can use this report as a signpost
for the overall state of the profession.
"
2936,"COVID-19Base: A knowledgebase to explore biomedical entities related to
  COVID-19","  We are presenting COVID-19Base, a knowledgebase highlighting the biomedical
entities related to COVID-19 disease based on literature mining. To develop
COVID-19Base, we mine the information from publicly available scientific
literature and related public resources. We considered seven topic-specific
dictionaries, including human genes, human miRNAs, human lncRNAs, diseases,
Protein Databank, drugs, and drug side effects, are integrated to mine all
scientific evidence related to COVID-19. We have employed an automated
literature mining and labeling system through a novel approach to measure the
effectiveness of drugs against diseases based on natural language processing,
sentiment analysis, and deep learning. To the best of our knowledge, this is
the first knowledgebase dedicated to COVID-19, which integrates such large
variety of related biomedical entities through literature mining. Proper
investigation of the mined biomedical entities along with the identified
interactions among those, reported in COVID-19Base, would help the research
community to discover possible ways for the therapeutic treatment of COVID-19.
"
2937,"MeSH descriptors indicate the knowledge growth in the
  SARS-CoV-2/COVID-19 pandemic","  The scientific papers dealing with the novel betacoronavirus SARS-CoV-2 and
the coronavirus disease 2019 (COVID-19) caused by this virus, published in 2020
and recorded in the database PUBMED, were retrieved on April 27, 2020. About
20\% of the records contain Medical Subject Headings (MeSH), keywords assigned
to records in the course of the indexing process in order to summarise the
articles' contents. The temporal sequence of the first occurrences of the
keywords was determined, thus giving insight into the growth of the knowledge
base of the pandemic.
"
2938,"Meta-Research: COVID-19 medical papers have fewer women first authors
  than expected","  The COVID-19 pandemic has resulted in school closures and distancing
requirements that have disrupted both work and family life for many. Concerns
exist that these disruptions caused by the pandemic may not have influenced men
and women researchers equally. Many medical journals have published papers on
the pandemic, which were generated by researchers facing the challenges of
these disruptions. Here we report the results of an analysis that compared the
gender distribution of authors on 1,893 medical papers related to the pandemic
with that on papers published in the same journals in 2019, for papers with
first authors and last authors from the United States. Using mixed-effects
regression models, we estimated that the proportion of COVID-19 papers with a
woman first author was 19% lower than that for papers published in the same
journals in 2019, while our comparisons for last authors and overall proportion
of women authors per paper were inconclusive. A closer examination suggested
that women's representation as first authors of COVID-19 research was
particularly low for papers published in March and April 2020. Our findings are
consistent with the idea that the research productivity of women, especially
early-career women, has been affected more than the research productivity of
men.
"
2939,An Improved Topic Masking Technique for Authorship Analysis,"  Authorship verification (AV) is an important sub-area of digital text
forensics and has been researched for more than two decades. The fundamental
question addressed by AV is whether two documents were written by the same
person. A serious problem that has received little attention in the literature
so far is the question if AV methods actually focus on the writing style during
classification, or whether they are unintentionally distorted by the topic of
the documents. To counteract this problem, we propose an effective technique
called POSNoise, which aims to mask topic-related content in documents. In this
way, AV methods are forced to focus on those text units that are more related
to the author's writing style. Based on a comprehensive evaluation with eight
existing AV methods applied to eight corpora, we demonstrate that POSNoise is
able to outperform a well-known topic masking approach in 51 out of 64 cases
with up to 12.5% improvement in terms of accuracy. Furthermore, we show that
for corpora preprocessed with POSNoise, the AV methods examined often achieve
higher accuracies (improvement of up to 20.6%) compared to the original
corpora.
"
2940,ImpactCite: An XLNet-based method for Citation Impact Analysis,"  Citations play a vital role in understanding the impact of scientific
literature. Generally, citations are analyzed quantitatively whereas
qualitative analysis of citations can reveal deeper insights into the impact of
a scientific artifact in the community. Therefore, citation impact analysis
(which includes sentiment and intent classification) enables us to quantify the
quality of the citations which can eventually assist us in the estimation of
ranking and impact. The contribution of this paper is two-fold. First, we
benchmark the well-known language models like BERT and ALBERT along with
several popular networks for both tasks of sentiment and intent classification.
Second, we provide ImpactCite, which is XLNet-based method for citation impact
analysis. All evaluations are performed on a set of publicly available citation
analysis datasets. Evaluation results reveal that ImpactCite achieves a new
state-of-the-art performance for both citation intent and sentiment
classification by outperforming the existing approaches by 3.44% and 1.33% in
F1-score. Therefore, we emphasize ImpactCite (XLNet-based solution) for both
tasks to better understand the impact of a citation. Additional efforts have
been performed to come up with CSC-Clean corpus, which is a clean and reliable
dataset for citation sentiment classification.
"
2941,"The ATLAS Publication Process Supported by Continuous Integration and
  Web Framework","  The ATLAS Collaboration defines methods, establishes procedures, and
organises advisory groups to manage the publication processes of scientific
papers, conference papers, and public notes. All stages are managed through web
systems, computing programs, and tools that are designed and developed by the
Collaboration. The Phase 0 system was implemented using the FENCE framework and
is integrated into the CERN GitLab software repository, to automatically
configure workspaces where the analysis can be documented and used by the
analysis team and managed by the conveners. Continuous integration is used to
guide the writers in applying accurate format and valid statements when
preparing papers to be submitted to scientific journals. Additional software
assures the correctness of other aspects such as lists of collaboration
authors, funding agencies, and foundations. The ATLAS Physics and Committees
Office provides support to the researchers and facilitates each phase of the
publication process, allowing authors to focus on the article's contents that
describe the results of the ATLAS experiment.
"
2942,Success in creative careers depends little on product quality,"  In the recent article Janosov, Battiston, & Sinatra report that they
separated the inputs of talent and luck in creative careers. They build on the
previous work of Sinatra et al which introduced the Q-model. Under the model
the popularity of different elements of culture is a product of two factors: a
random factor and a Qfactor, or talent. The latter is fixed for an individual
but randomly distributed among different people. This way they explain how some
individuals can consistently produce high-impact work. They extract the
Q-factors for different scientists, writers, and movie makers from statistical
data on popularity of their work. However, in their article they reluctantly
state that there is little correlation between popularity and quality ratings
of of books and movies (correlation coefficients 0.022 and 0.15). I analyzed
the data of the original Q-factor article and obtained a correlation between
the citation-based Q-factor and Nobel Prize winning of merely 0.19. I also
briefly review few other experiments that found a meager, sometimes even
negative, correlation between popularity and quality of cultural products. I
conclude that, if there is an ability associated with a high Q-factor it should
be more of a marketing ability than an ability to produce a higher quality
product. Janosov,
"
2943,"The MUIR Framework: Cross-Linking MOOC Resources to Enhance Discussion
  Forums","  New learning resources are created and minted in Massive Open Online Courses
every week -- new videos, quizzes, assessments and discussion threads are
deployed and interacted with -- in the era of on-demand online learning.
However, these resources are often artificially siloed between platforms and
artificial web application models. Facilitating the linking between such
resources facilitates learning and multimodal understanding, bettering
learners' experience.
  We create a framework for MOOC Uniform Identifier for Resources (MUIR). MUIR
enables applications to refer and link to such resources in a cross-platform
way, allowing the easy minting of identifiers to MOOC resources, akin to
#hashtags. We demonstrate the feasibility of this approach to the automatic
identification, linking and resolution -- a task known as Wikification -- of
learning resources mentioned on MOOC discussion forums, from a harvested
collection of 100K+ resources. Our Wikification system achieves a high initial
rate of 54.6% successful resolutions on key resource mentions found in
discussion forums, demonstrating the utility of the MUIR framework. Our
analysis on this new problem shows that context is a key factor in determining
the correct resolution of such mentions.
"
2944,"Referencing Sources of Molecular Spectroscopic Data in the Era of Data
  Science: Application to the HITRAN and AMBDAS Databases","  The application described has been designed to create bibliographic entries
in large databases with diverse sources automatically, which reduces both the
frequency of mistakes and the workload for the administrators. This new system
uniquely identifies each reference from its digital object identifier (DOI) and
retrieves the corresponding bibliographic information from any of several
online services, including the SAO/NASA Astrophysics Data Systems (ADS) and
CrossRef APIs. Once parsed into a relational database, the software is able to
produce bibliographies in any of several formats, including HTML and BibTeX,
for use on websites or printed articles. The application is provided
free-of-charge for general use by any scientific database. The power of this
application is demonstrated when used to populate reference data for the HITRAN
and AMBDAS databases as test cases. HITRAN contains data that is provided by
researchers and collaborators throughout the spectroscopic community. These
contributors are accredited for their contributions through the bibliography
produced alongside the data returned by an online search in HITRAN. Prior to
the work presented here, HITRAN and AMBDAS created these bibliographies
manually, which is a tedious, time-consuming and error-prone process. The
complete code for the new referencing system can be found at
\url{https://github.com/hitranonline/refs}.
"
2945,"Analyzing the relationship between text features and research proposal
  success","  Predicting the success of research proposals is of considerable relevance to
research funding bodies, scientific entities and government agencies. In this
study, we investigate whether text features extracted from proposals title and
abstracts are able to identify successful funded research proposals. Our
analysis was conducted in three distinct areas, namely Medicine, Dentistry and
Veterinary Medicine. Topical and complexity text features were used to identify
predictors of success. The results indicate that both topical and complexity
measurements are relevant for identifying successful proposals in the
considered dataset. A feature relevance analysis revealed that abstract text
length and metrics derived from lexical diversity are among the most
discriminative features. We also found that the prediction accuracy has no
significant dependence on the considered proposal language. Our findings
suggest that textual information, in combination with other features, are
potentially useful to assist the identification of relevant research ideas.
"
2946,"Do journals flipping to Gold Open Access show an OA Citation or
  Publication Advantage?","  The effects of Open Access (OA) upon journal performance are investigated.
The key research question holds: How does the citation impact and publication
output of journals switching ('flipping') from non-OA to Gold-OA develop after
their switch to Gold-OA? A review is given of the literature, with an emphasis
on studies dealing with flipping journals. Two study sets with 119 and 100
flipping journals, derived from two different OA data sources (DOAJ and OAD),
are compared with two control groups, one based on a standard bibliometric
criterion, and a second controlling for a journal's national orientation.
Comparing post-switch indicators with pre-switch ones in paired T-tests,
evidence was obtained of an OA Citation advantage but not of an OA Publication
Advantage. Shifts in the affiliation countries of publishing and citing authors
are characterized in terms of countries' income class and geographical world
region. Suggestions are made for qualitative follow-up studies to obtain more
insight into OA flipping or reverse-flipping
"
2947,"A Semantically Enriched Dataset based on Biomedical NER for the COVID19
  Open Research Dataset Challenge","  Research into COVID-19 is a big challenge and highly relevant at the moment.
New tools are required to assist medical experts in their research with
relevant and valuable information. The COVID-19 Open Research Dataset Challenge
(CORD-19) is a ""call to action"" for computer scientists to develop these
innovative tools. Many of these applications are empowered by entity
information, i. e. knowing which entities are used within a sentence. For this
paper, we have developed a pipeline upon the latest Named Entity Recognition
tools for Chemicals, Diseases, Genes and Species. We apply our pipeline to the
COVID-19 research challenge and share the resulting entity mentions with the
community.
"
2948,"NEJM-enzh: A Parallel Corpus for English-Chinese Translation in the
  Biomedical Domain","  Machine translation requires large amounts of parallel text. While such
datasets are abundant in domains such as newswire, they are less accessible in
the biomedical domain. Chinese and English are two of the most widely spoken
languages, yet to our knowledge a parallel corpus in the biomedical domain does
not exist for this language pair. In this study, we develop an effective
pipeline to acquire and process an English-Chinese parallel corpus, consisting
of about 100,000 sentence pairs and 3,000,000 tokens on each side, from the New
England Journal of Medicine (NEJM). We show that training on out-of-domain data
and fine-tuning with as few as 4,000 NEJM sentence pairs improve translation
quality by 25.3 (13.4) BLEU for en$\to$zh (zh$\to$en) directions. Translation
quality continues to improve at a slower pace on larger in-domain datasets,
with an increase of 33.0 (24.3) BLEU for en$\to$zh (zh$\to$en) directions on
the full dataset.
"
2949,"Open Access effect on uncitedness: A large-scale study controlling by
  discipline, source type and visibility","  There are many factors that affect the probability of being uncited during
the first years after publication. In this study, we analyze three of these
factors for journals, conference proceedings and book series: the field (in 316
subject categories of the Scopus database), the access modality (open access
vs. paywalled), and the visibility of the source (through the percentile of the
average impact in the subject category). We quantify the effect of these
factors on the probability of being uncited. This probability is measured
through the percentage of uncited documents in the serial sources of the Scopus
database at about two years after publication. As a main result, we do not find
any strong correlation between open access and uncitedness. Within the group of
most cited journals (Q1 and top 10%), open access journals generally have
somewhat lower uncited rates. However, in the intermediate quartiles (Q2 and
Q3) almost no differences are observed, while for Q4 the uncited rate is again
somewhat lower in the case of the OA group. This is important because it
provides new evidence in the debate about open access citation advantage.
"
2950,"Comparing like with like: China ranks first in SCI-indexed research
  articles since 2018","  China's rising in scientific research output is impressive. The academic
community is curious about the time when the cross-over in the number of annual
scientific publication production between China and the USA can happen. By
using Web of Science Core Collection's Science Citation Index Expanded
database, this study finds that China still ranks the second in the production
of SCI-indexed publications in 2019 but may leapfrog the USA to be the first in
2020 or 2021, if all document types are considered. Comparatively, China has
already overtaken the USA and been the largest SCI-indexed original research
article producer since 2018. However, China still lags behind the USA regarding
the number of review paper production. In general, quantitative advantage does
not equal quality or impact advantage. We think that the USA will continue to
be the global scientific leader for a long time.
"
2951,"What country, university or research institute, performed the best on
  COVID-19? Bibliometric analysis of scientific literature","  In this article, we conduct data mining to discover the countries,
universities and companies, produced or collaborated the most research on
Covid-19 since the pandemic started. We present some interesting findings, but
despite analysing all available records on COVID-19 from the Web of Science
Core Collection, we failed to reach any significant conclusions on how the
world responded to the COVID-19 pandemic. Therefore, we increased our analysis
to include all available data records on pandemics and epidemics from 1900 to
2020. We discover some interesting results on countries, universities and
companies, that produced collaborated most the most in research on pandemic and
epidemics. Then we compared the results with the analysing on COVID-19 data
records. This has created some interesting findings that are explained and
graphically visualised in the article.
"
2952,"Machine Identification of High Impact Research through Text and Image
  Analysis","  The volume of academic paper submissions and publications is growing at an
ever increasing rate. While this flood of research promises progress in various
fields, the sheer volume of output inherently increases the amount of noise. We
present a system to automatically separate papers with a high from those with a
low likelihood of gaining citations as a means to quickly find high impact,
high quality research. Our system uses both a visual classifier, useful for
surmising a document's overall appearance, and a text classifier, for making
content-informed decisions. Current work in the field focuses on small datasets
composed of papers from individual conferences. Attempts to use similar
techniques on larger datasets generally only considers excerpts of the
documents such as the abstract, potentially throwing away valuable data. We
rectify these issues by providing a dataset composed of PDF documents and
citation counts spanning a decade of output within two separate academic
domains: computer science and medicine. This new dataset allows us to expand on
current work in the field by generalizing across time and academic domain.
Moreover, we explore inter-domain prediction models - evaluating a classifier's
performance on a domain it was not trained on - to shed further insight on this
important problem.
"
2953,Requirements Analysis for an Open Research Knowledge Graph,"  Current science communication has a number of drawbacks and bottlenecks which
have been subject of discussion lately: Among others, the rising number of
published articles makes it nearly impossible to get an overview of the state
of the art in a certain field, or reproducibility is hampered by fixed-length,
document-based publications which normally cannot cover all details of a
research work. Recently, several initiatives have proposed knowledge graphs
(KGs) for organising scientific information as a solution to many of the
current issues. The focus of these proposals is, however, usually restricted to
very specific use cases. In this paper, we aim to transcend this limited
perspective by presenting a comprehensive analysis of requirements for an Open
Research Knowledge Graph (ORKG) by (a) collecting daily core tasks of a
scientist, (b) establishing their consequential requirements for a KG-based
system, (c) identifying overlaps and specificities, and their coverage in
current solutions. As a result, we map necessary and desirable requirements for
successful KG-based science communication, derive implications and outline
possible solutions.
"
2954,"China's SCI-indexed publications: facts, feelings, and future directions","  Purpose:In relation to the boom in China's SCI-indexed publications, this
opinion piece examines this phenomenon and looks at future possible directions
for the reform of China's research evaluation processes.
Design/Approach/Methods:This opinion piece uses bibliographic data for the past
decade (2010-2019) from the Science Citation Index Expanded in the Web of
Science Core Collection to examine the rise in China's SCI-indexed
publications. Findings: China has surpassed the United States and been the
largest contributor of SCI publications since 2018. However, while the impact
of China's SCI publications is rising, the scale of this impact still lags
behind that of other major contributing countries. China's SCI publications are
also overrepresented in some journals. Originality/Value: Reporting the latest
facts about China's SCI-indexed publications, this article will benefit the
reform of China's research evaluation system.
"
2955,"Large-scale comparison of bibliographic data sources: Scopus, Web of
  Science, Dimensions, Crossref, and Microsoft Academic","  We present a large-scale comparison of five multidisciplinary bibliographic
data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft
Academic. The comparison considers all scientific documents from the period
2008-2017 covered by these data sources. Scopus is compared in a pairwise
manner with each of the other data sources. We first analyze differences
between the data sources in the coverage of documents, focusing for instance on
differences over time, differences per document type, and differences per
discipline. We then study differences in the completeness and accuracy of
citation links. Based on our analysis, we discuss strengths and weaknesses of
the different data sources. We emphasize the importance of combining a
comprehensive coverage of the scientific literature with a flexible set of
filters for making selections of the literature.
"
2956,"Classification and Clustering of arXiv Documents, Sections, and
  Abstracts, Comparing Encodings of Natural and Mathematical Language","  In this paper, we show how selecting and combining encodings of natural and
mathematical language affect classification and clustering of documents with
mathematical content. We demonstrate this by using sets of documents, sections,
and abstracts from the arXiv preprint server that are labeled by their subject
class (mathematics, computer science, physics, etc.) to compare different
encodings of text and formulae and evaluate the performance and runtimes of
selected classification and clustering algorithms. Our encodings achieve
classification accuracies up to $82.8\%$ and cluster purities up to $69.4\%$
(number of clusters equals number of classes), and $99.9\%$ (unspecified number
of clusters) respectively. We observe a relatively low correlation between text
and math similarity, which indicates the independence of text and formulae and
motivates treating them as separate features of a document. The classification
and clustering can be employed, e.g., for document search and recommendation.
Furthermore, we show that the computer outperforms a human expert when
classifying documents. Finally, we evaluate and discuss multi-label
classification and formula semantification.
"
2957,A First Step Towards Content Protecting Plagiarism Detection,"  Plagiarism detection systems are essential tools for safeguarding academic
and educational integrity. However, today's systems require disclosing the full
content of the input documents and the document collection to which the input
documents are compared. Moreover, the systems are centralized and under the
control of individual, typically commercial providers. This situation raises
procedural and legal concerns regarding the confidentiality of sensitive data,
which can limit or prohibit the use of plagiarism detection services. To
eliminate these weaknesses of current systems, we seek to devise a plagiarism
detection approach that does not require a centralized provider nor exposing
any content as cleartext. This paper presents the initial results of our
research. Specifically, we employ Private Set Intersection to devise a
content-protecting variant of the citation-based similarity measure
Bibliographic Coupling implemented in our plagiarism detection system HyPlag.
Our evaluation shows that the content-protecting method achieves the same
detection effectiveness as the original method while making common attacks to
disclose the protected content practically infeasible. Our future work will
extend this successful proof-of-concept by devising plagiarism detection
methods that can analyze the entire content of documents without disclosing it
as cleartext.
"
2958,"All downhill from the PhD? The typical impact trajectory of US academic
  careers","  Within academia, mature researchers tend to be more senior, but do they also
tend to write higher impact articles? This article assesses long-term
publishing (16+ years) United States (US) researchers, contrasting them with
shorter-term publishing researchers (1, 6 or 10 years). A long-term US
researcher is operationalised as having a first Scopus-indexed journal article
in exactly 2001 and one in 2016-2019, with US main affiliations in their first
and last articles. Researchers publishing in large teams (11+ authors) were
excluded. The average field and year normalised citation impact of long- and
shorter-term US researchers' journal articles decreases over time relative to
the national average, with especially large falls to the last articles
published that may be at least partly due to a decline in self-citations. In
many cases researchers start by publishing above US average citation impact
research and end by publishing below US average citation impact research. Thus,
research managers should not assume that senior researchers will usually write
the highest impact papers.
"
2959,The OpenCitations Data Model,"  A variety of schemas and ontologies are currently used for the
machine-readable description of bibliographic entities and citations. This
diversity, and the reuse of the same ontology terms with different nuances,
generates inconsistencies in data. Adoption of a single data model would
facilitate data integration tasks regardless of the data supplier or context
application. In this paper we present the OpenCitations Data Model (OCDM), a
generic data model for describing bibliographic entities and citations,
developed using Semantic Web technologies. We also evaluate the effective
reusability of OCDM according to ontology evaluation practices, mention
existing users of OCDM, and discuss the use and impact of OCDM in the wider
open science community.
"
2960,"AutoMSC: Automatic Assignment of Mathematics Subject Classification
  Labels","  Authors of research papers in the fields of mathematics, and other math-heavy
disciplines commonly employ the Mathematics Subject Classification (MSC) scheme
to search for relevant literature. The MSC is a hierarchical alphanumerical
classification scheme that allows librarians to specify one or multiple codes
for publications. Digital Libraries in Mathematics, as well as reviewing
services, such as zbMATH and Mathematical Reviews (MR) rely on these MSC labels
in their workflows to organize the abstracting and reviewing process.
Especially, the coarse-grained classification determines the subject editor who
is responsible for the actual reviewing process.
  In this paper, we investigate the feasibility of automatically assigning a
coarse-grained primary classification using the MSC scheme, by regarding the
problem as a multi-class classification machine learning task. We find that our
method achieves an (F_1)-score of over 77%, which is remarkably close to the
agreement of zbMATH and MR ((F_1)-score of 81%). Moreover, we find that the
method's confidence score allows for reducing the effort by 86% compared to the
manual coarse-grained classification effort while maintaining a precision of
81% for automatically classified articles.
"
2961,"SciSight: Combining faceted navigation and research group detection for
  COVID-19 exploratory scientific search","  The COVID-19 pandemic has sparked unprecedented mobilization of scientists,
generating a deluge of papers that makes it hard for researchers to keep track
and explore new directions. Search engines are designed for targeted queries,
not for discovery of connections across a corpus. In this paper, we present
SciSight, a system for exploratory search of COVID-19 research integrating two
key capabilities: first, exploring associations between biomedical facets
automatically extracted from papers (e.g., genes, drugs, diseases, patient
outcomes); second, combining textual and network information to search and
visualize groups of researchers and their ties. SciSight has so far served over
$15K$ users with over $42K$ page views and $13\%$ returns.
"
2962,"Spatial organisation of French research from the scholarly publication
  standpoint (1999-2017): Long-standing dynamics and policy-induced disorder","  In social processes, long-term trends can be influenced or disrupted by
various factors, including public policy. When public policies depend on a
misrepresentation of trends in the areas they are aimed at, they become random
and disruptive, which can be interpreted as a source of disorder. Here we
consider policies on the spatial organization of the French Higher Education
and Research system, which reflects the authorities' hypothesis that scientific
excellence is the prerogative of a few large urban agglomerations. By
geographically identifying all the French publications listed in the Web of
Science databases between 1999 and 2017, we highlight a spatial deconcentration
trend, which has slowed down in recent years due to a freezed growth of the
teaching force. This deconcentration continues, however, to sustain the growth
of scientific production in small and medium-sized towns. An examination of the
large conurbations shows the relative decline of sites that nevertheless have
been highlighted as examples to be followed by the Excellence policies
(Strasbourg among others). The number of students and faculty has grown less
there, and it is a plaussible explanation for the relative decline in
scientific production. We show that the publication output of a given site
depends directly and strongly on the number of researchers hosted there. Based
on precise data at the French level, our results confirm what is already known
at world scale. In conclusion, we question the amount of disorder resulting
from policies aligned with poorly assessed trends.
"
2963,Towards an Open Platform for Legal Information,"  Recent advances in the area of legal information systems have led to a
variety of applications that promise support in processing and accessing legal
documents. Unfortunately, these applications have various limitations, e.g.,
regarding scope or extensibility. Furthermore, we do not observe a trend
towards open access in digital libraries in the legal domain as we observe in
other domains, e.g., economics of computer science. To improve open access in
the legal domain, we present our approach for an open source platform to
transparently process and access Legal Open Data. This enables the sustainable
development of legal applications by offering a single technology stack.
Moreover, the approach facilitates the development and deployment of new
technologies. As proof of concept, we implemented six technologies and
generated metadata for more than 250,000 German laws and court decisions. Thus,
we can provide users of our platform not only access to legal documents, but
also the contained information.
"
2964,"Knowledge Utilization and Open Science Policies: Noble aims that ensure
  quality research or Ordering discoveries like a pizza?","  Open Science has been a rising theme in the landscape of science policy in
recent years. The goal is to make research that emerges from publicly funded
science to become findable, accessible, interoperable and reusable (FAIR) for
use by other researchers. Knowledge utilization policies aim to efficiently
make scientific knowledge beneficial for society at large. This paper
demonstrates how Astronomy aspires to be open and transparent given their
criteria for high research quality, which aim at pushing knowledge forward and
clear communication of findings. However, the use of quantitative metrics in
research evaluation puts pressure on the researcher, such that taking the extra
time for transparent publishing of data and results is difficult, given that
astronomers are not rewarded for the quality of research papers, but rather
their quantity. This paper explores the current mode of openness in Astronomy
and how incentives due to funding, publication practices and indicators affect
this field. The paper concludes with some recommendations on how policies such
as making science more open have the potential to contribute to scientific
quality in Astronomy.
"
2965,"The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and
  Outlet Popularity","  News articles covering policy issues are an essential source of information
in the social sciences and are also frequently used for other use cases, e.g.,
to train NLP language models. To derive meaningful insights from the analysis
of news, large datasets are required that represent real-world distributions,
e.g., with respect to the contained outlets' popularity, topically, or across
time. Information on the political leanings of media publishers is often
needed, e.g., to study differences in news reporting across the political
spectrum, which is one of the prime use cases in the social sciences when
studying media bias and related societal issues. Concerning these requirements,
existing datasets have major flaws, resulting in redundant and cumbersome
effort in the research community for dataset creation. To fill this gap, we
present POLUSA, a dataset that represents the online media landscape as
perceived by an average US news consumer. The dataset contains 0.9M articles
covering policy topics published between Jan. 2017 and Aug. 2019 by 18 news
outlets representing the political spectrum. Each outlet is labeled by its
political leaning, which we derive using a systematic aggregation of eight data
sources. The news dataset is balanced with respect to publication date and
outlet popularity. POLUSA enables studying a variety of subjects, e.g., media
effects and political partisanship. Due to its size, the dataset allows to
utilize data-intense deep learning methods.
"
2966,Unsupervised Anomaly Detection in Journal-Level Citation Networks,"  Journal Impact Factor is a popular metric for determining the quality of a
journal in academia. The number of citations received by a journal is a crucial
factor in determining the impact factor, which may be misused in multiple ways.
Therefore, it is crucial to detect citation anomalies for further identifying
manipulation and inflation of impact factor. Citation network models the
citation relationship between journals in terms of a directed graph. Detecting
anomalies in the citation network is a challenging task which has several
applications in spotting citation cartels and citation stack and understanding
the intentions behind the citations. In this paper, we present a novel approach
to detect the anomalies in a journal-level scientific citation network, and
compare the results with the existing graph anomaly detection algorithms. Due
to the lack of proper ground-truth, we introduce a journal-level citation
anomaly dataset which consists of synthetically injected citation anomalies and
use it to evaluate our methodology. Our method is able to predict the anomalous
citation pairs with a precision of 100\% and an F1-score of 86%. We further
categorize the detected anomalies into various types and reason out possible
causes. We also analyze our model on the Microsoft Academic Search dataset - a
real-world citation dataset and interpret our results using a case study,
wherein our results resemble the citations and SCImago Journal Rank (SJR)
rating-change charts, thus indicating the usefulness of our method. We further
design `Journal Citation Analysis Tool', an interactive web portal which, given
the citation network as an input, shows the journal-level anomalous citation
patterns and helps users analyze citation patterns of a given journal over the
years.
"
2967,"OSDG -- Open-Source Approach to Classify Text Data by UN Sustainable
  Development Goals (SDGs)","  Sustainable Development Goals (SDGs) bring together the diverse development
community and provide a clear set of development targets for 2030. Given a
large number of actors and initiatives related to these goals, there is a need
to have a way to accurately and reliably assign text to different input:
scientific research, research projects, technological output or documents to
specific SDGs. In this paper we present Open Source SDG (OSDG) project and tool
which does so by integrating existing research and previous classification into
a robust and coherent framework. This integration is based on linking the
features from the variety of previous approaches, like ontology items, keywords
or features from machine-learning models, to the topics in Microsoft Academic
Graph.
"
2968,"Crossing the Academic Ocean? Judit Bar-Ilan's Oeuvre on Search Engines
  studies","  The main objective of this work is to analyse the contributions of Judit
Bar-Ilan to the search engines studies. To do this, two complementary
approaches have been carried out. First, a systematic literature review of 47
publications authored and co-authored by Judit and devoted to this topic.
Second, an interdisciplinarity analysis based on the cited references
(publications cited by Judit) and citing documents (publications that cite
Judit's work) through Scopus. The systematic literature review unravels an
immense amount of search engines studied (43) and indicators measured
(especially technical precision, overlap and fluctuation over time). In
addition to this, an evolution over the years is detected from descriptive
statistical studies towards empirical user studies, with a mixture of
quantitative and qualitative methods. Otherwise, the interdisciplinary analysis
evidences that a significant portion of Judit's oeuvre was intellectually
founded on the computer sciences, achieving a significant, but not exclusively,
impact on library and information sciences.
"
2969,"Universities through the Eyes of Bibliographic Databases: A Retroactive
  Growth Comparison of Google Scholar, Scopus and Web of Science","  The purpose of this study is to ascertain the suitability of GS's url-based
method as a valid approximation of universities' academic output measures,
taking into account three aspects (retroactive growth, correlation, and
coverage). To do this, a set of 100 Turkish universities were selected as a
case study. The productivity in Web of Science (WoS), Scopus and GS (2000 to
2013) were captured in two different measurement iterations (2014 and 2018). In
addition, a total of 18,174 documents published by a subset of 14
research-focused universities were retrieved from WoS, verifying their presence
in GS within the official university web domain. Findings suggest that the
retroactive growth in GS is unpredictable and dependent on each university,
making this parameter hard to evaluate at the institutional level. Otherwise,
the correlation of productivity between GS (url-based method) and WoS and
Scopus (selected sources) is moderately positive, even though it varies
depending on the university, the year of publication, and the year of
measurement. Finally, only 16% out of 18,174 articles analyzed were indexed in
the official university website, although up to 84% were indexed in other GS
sources. This work proves that the url-based method to calculate institutional
productivity in GS is not a good proxy for the total number of publications
indexed in WoS and Scopus, at least in the national context analyzed. However,
the main reason is not directly related to the operation of GS, but with a lack
of universities' commitment to open access.
"
2970,"Covid-19 pandemic and the unprecedented mobilisation of scholarly
  efforts prompted by a health crisis: Scientometric comparisons across SARS,
  MERS and 2019-nCov literature","  During the current century, each major coronavirus outbreak has triggered a
quick surge of academic publications on this topic. The spike in research
publications following the 2019 Novel Coronavirus (Covid-19), however, has been
like no other. The global crisis caused by the Covid-19 pandemic has mobilised
scientific efforts in an unprecedented way. In less than five months, more than
12,000 research items have been indexed while the number increasing every day.
With the crisis affecting all aspects of life, research on Covid-19 seems to
have become a focal point of interest across many academic disciplines. Here,
scientometric aspects of the Covid-19 literature are analysed and contrasted
with those of the two previous major Coronavirus diseases, i.e. SARS and MERS.
The focus is on the co-occurrence of key-terms, bibliographic coupling and
citation relations of journals and collaborations between countries. Certain
recurring patterns across all three literatures were discovered. All three
outbreaks have commonly generated three distinct and major cohort of studies:
(i) studies linked to the public health response and epidemic control, (ii)
studies associated with the chemical constitution of the virus and (iii)
studies related to treatment, vaccine and clinical care. While studies
affiliated with the category (i) seem to have been the first to emerge, they
overall received least numbers of citations compared to those of the two other
categories. Covid-19 studies seem to have been distributed across a broader
variety of journals and subject areas. Clear links are observed between the
geographical origins of each outbreak or the local geographical severity of
each outbreak and the magnitude of research originated from regions. Covid-19
studies also display the involvement of authors from a broader variety of
countries compared to SARS and MRS.
"
2971,Ranking Papers by their Short-Term Scientific Impact,"  The constantly increasing rate at which scientific papers are published makes
it difficult for researchers to identify papers that currently impact the
research field of their interest. Hence, approaches to effectively identify
papers of high impact have attracted great attention in the past. In this work,
we present a method that seeks to rank papers based on their estimated
short-term impact, as measured by the number of citations received in the near
future. Similar to previous work, our method models a researcher as she
explores the paper citation network. The key aspect is that we incorporate an
attention-based mechanism, akin to a time-restricted version of preferential
attachment, to explicitly capture a researcher's preference to read papers
which received a lot of attention recently. A detailed experimental evaluation
on four real citation datasets across disciplines, shows that our approach is
more effective than previous work in ranking papers based on their short-term
impact.
"
2972,"NLP Scholar: An Interactive Visual Explorer for Natural Language
  Processing Literature","  As part of the NLP Scholar project, we created a single unified dataset of
NLP papers and their meta-information (including citation numbers), by
extracting and aligning information from the ACL Anthology and Google Scholar.
In this paper, we describe several interconnected interactive visualizations
(dashboards) that present various aspects of the data. Clicking on an item
within a visualization or entering query terms in the search boxes filters the
data in all visualizations in the dashboard. This allows users to search for
papers in the area of their interest, published within specific time periods,
published by specified authors, etc. The interactive visualizations presented
here, and the associated dataset of papers mapped to citations, have additional
uses as well including understanding how the field is growing (both overall and
across sub-areas), as well as quantifying the impact of different types of
papers on subsequent publications.
"
2973,Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019,"  Person identification based on eye movements is getting more and more
attention, as it is anti-spoofing resistant and can be useful for continuous
authentication. Therefore, it is noteworthy for researchers to know who and
what is relevant in the field, including authors, journals, conferences, and
institutions. This paper presents a comprehensive quantitative overview of the
field of eye movement biometrics using a bibliometric approach. All data and
analyses are based on documents written in English published between 2004 and
2019. Scopus was used to perform information retrieval. This research focused
on temporal evolution, leading authors, most cited papers, leading journals,
competitions and collaboration networks.
"
2974,Question Answering on Scholarly Knowledge Graphs,"  Answering questions on scholarly knowledge comprising text and other
artifacts is a vital part of any research life cycle. Querying scholarly
knowledge and retrieving suitable answers is currently hardly possible due to
the following primary reason: machine inactionable, ambiguous and unstructured
content in publications. We present JarvisQA, a BERT based system to answer
questions on tabular views of scholarly knowledge graphs. Such tables can be
found in a variety of shapes in the scholarly literature (e.g., surveys,
comparisons or results). Our system can retrieve direct answers to a variety of
different questions asked on tabular data in articles. Furthermore, we present
a preliminary dataset of related tables and a corresponding set of natural
language questions. This dataset is used as a benchmark for our system and can
be reused by others. Additionally, JarvisQA is evaluated on two datasets
against other baselines and shows an improvement of two to three folds in
performance compared to related methods.
"
2975,Generate FAIR Literature Surveys with Scholarly Knowledge Graphs,"  Reviewing scientific literature is a cumbersome, time consuming but crucial
activity in research. Leveraging a scholarly knowledge graph, we present a
methodology and a system for comparing scholarly literature, in particular
research contributions describing the addressed problem, utilized materials,
employed methods and yielded results. The system can be used by researchers to
quickly get familiar with existing work in a specific research domain (e.g., a
concrete research question or hypothesis). Additionally, it can be used to
publish literature surveys following the FAIR Data Principles. The methodology
to create a research contribution comparison consists of multiple tasks,
specifically: (a) finding similar contributions, (b) aligning contribution
descriptions, (c) visualizing and finally (d) publishing the comparison. The
methodology is implemented within the Open Research Knowledge Graph (ORKG), a
scholarly infrastructure that enables researchers to collaboratively describe,
find and compare research contributions. We evaluate the implementation using
data extracted from published review articles. The evaluation also addresses
the FAIRness of comparisons published with the ORKG.
"
2976,"Being published successfully or getting arXived? The importance of
  social capital and interdisciplinary collaboration for getting printed in a
  high impact journal in Physics","  The structure of collaboration is known to be of great importance for the
success of scientific endeavors. In particular, various types of social capital
employed in co-authored work and projects bridging disciplinary boundaries have
attracted researchers' interest. Almost all previous studies, however, use
samples with an inherent survivor bias, i.e., they focus on papers that have
already been published. In contrast, our article examines the chances for
getting a working paper published by using a unique dataset of 245,000 papers
uploaded to arXiv. ArXiv is a popular preprint platform in Physics which allows
us to construct a co-authorship network from which we can derive different
types of social capital and interdisciplinary teamwork. To emphasize the
'normal case' of community-specific standards of excellence, we assess
publications in Physics' high impact journals as success. Utilizing multilevel
event history models, our results reveal that already a moderate number of
persistent collaborations spanning at least two years is the most important
social antecedent of getting a manuscript published successfully. In contrast,
inter- and subdisciplinary collaborations decrease the probability of
publishing in an eminent journal in Physics, which can only partially be
mitigated by scientists' social capital.
"
2977,"Mapping the co-evolution of artificial intelligence, robotics, and the
  internet of things over 20 years (1998-2017)","  Understanding the emergence, co-evolution, and convergence of science and
technology (S&T) areas offers competitive intelligence for researchers,
managers, policy makers, and others. The resulting data-driven decision support
helps set proper research and development (R&D) priorities; develop future S&T
investment strategies; monitor key authors, organizations, or countries;
perform effective research program assessment; and implement cutting-edge
education/training efforts. This paper presents new funding, publication, and
scholarly network metrics and visualizations that were validated via expert
surveys. The metrics and visualizations exemplify the emergence and convergence
of three areas of strategic interest: artificial intelligence (AI), robotics,
and internet of things (IoT) over the last 20 years (1998-2017). For 32,716
publications and 4,497 NSF awards, we identify their conceptual space (using
the UCSD map of science), geospatial network, and co-evolution landscape. The
findings demonstrate how the transition of knowledge (through cross-discipline
publications and citations) and the emergence of new concepts (through term
bursting) create a tangible potential for interdisciplinary research and new
disciplines.
"
2978,"Technological impact of biomedical research: the role of basicness and
  novelty","  An ongoing interest in innovation studies is to understand how knowledge
generated from scientific research can be used in the development of
technologies. While previous inquiries have devoted to studying the scientific
capacity of technologies and institutional factors facilitating technology
transfer, little is known about the intrinsic characteristics of scientific
publications that gain direct technological impact. Here we focus on two
features, namely basicness and novelty. Using a corpus of 3.8 million papers
published between 1980 and 1999, we find that basic science papers and novel
papers are substantially more likely to achieve direct technological impact.
Further analysis that limits to papers with technological impact reveals that
basic science and novel science have more patent citations, experience shorter
time lag, and have impact in broader technological fields.
"
2979,Visualizing Webpage Changes Over Time,"  We report on the development of TMVis, a web service to provide
visualizations of how individual webpages have changed over time. We leverage
past research on summarizing collections of webpages with thumbnail-sized
screenshots and on choosing a small number of representative past archived
webpages from a large collection. We offer four visualizations: image grid,
image slider, timeline, and animated GIF. Embed codes for the image grid and
image slider can be produced to include these on separate webpages. The
animated GIF can be downloaded as an image file for the same purpose. This tool
can be used to allow scholars from various disciplines, as well as the general
public, to explore the temporal nature of web archives. We hope that these
visualizations will just be the beginning and will provide a starting point for
others to expand these types of offerings for users of web archives.
"
2980,"Digital interfaces of historical newspapers: opportunities, restrictions
  and recommendations","  Many libraries offer free access to digitised historical newspapers via user
interfaces. After an initial period of search and filter options as the only
features, the availability of more advanced tools and the desire for more
options among users has ushered in a period of interface development. However,
this raises a number of open questions and challenges. For example, how can we
provide interfaces for different user groups? What tools should be available on
interfaces and how can we avoid too much complexity? What tools are helpful and
how can we improve usability? This paper will not provide definite answers to
these questions, but it gives an insight into the difficulties, challenges and
risks of using interfaces to investigate historical newspapers. More
importantly, it provides ideas and recommendations for the improvement of user
interfaces and digital tools.
"
2981,"Characteristics of Dataset Retrieval Sessions: Experiences from a
  Real-life Digital Library","  Secondary analysis or the reuse of existing survey data is a common practice
among social scientists. Searching for relevant datasets in Digital Libraries
is a somehow unfamiliar behaviour for this community. Dataset retrieval,
especially in the social sciences, incorporates additional material such as
codebooks, questionnaires, raw data files and more. Our assumption is that due
to the diverse nature of datasets, document retrieval models often do not work
as efficiently for retrieving datasets. One way of enhancing these types of
searches is to incorporate the users' interaction context in order to
personalise dataset retrieval sessions. As a first step towards this long term
goal, we study characteristics of dataset retrieval sessions from a real-life
Digital Library for the social sciences that incorporates both: research data
and publications. Previous studies reported a way of discerning queries between
document search and dataset search by query length. In this paper, we argue the
claim and report our findings of an indistinguishability of queries, whether
aiming for a dataset or a document. Amongst others, we report our findings of
dataset retrieval sessions with respect to query characteristics, interaction
sequences and topical drift within 65,000 unique sessions.
"
2982,Towards Long-term and Archivable Reproducibility,"  Reproducible workflow solutions commonly use high-level technologies that
were popular when they were created, providing an immediate solution which is
unlikely to be sustainable in the long term. We therefore introduce a set of
criteria to address this problem and demonstrate their practicality and
implementation. The criteria have been tested in several research publications
and can be summarized as: completeness (no dependency beyond a POSIX-compatible
operating system, no administrator privileges, no network connection and
storage primarily in plain text); modular design; minimal complexity;
scalability; verifiable inputs and outputs; temporal provenance; linking
analysis with narrative; and free-and-open-source software. As a proof of
concept, we have implemented ""Maneage"", a solution which stores the project in
machine-actionable and human-readable plain-text, enables version-control,
cheap archiving, automatic parsing to extract data provenance, and
peer-reviewable verification. We show that requiring longevity of a
reproducible workflow solution is realistic, without sacrificing immediate or
short-term reproducibility and discuss the benefits of the criteria for
scientific progress. This paper has itself been written in Maneage, with
snapshot 1637cce.
"
2983,Aspect-based Sentiment Analysis of Scientific Reviews,"  Scientific papers are complex and understanding the usefulness of these
papers requires prior knowledge. Peer reviews are comments on a paper provided
by designated experts on that field and hold a substantial amount of
information, not only for the editors and chairs to make the final decision,
but also to judge the potential impact of the paper. In this paper, we propose
to use aspect-based sentiment analysis of scientific reviews to be able to
extract useful information, which correlates well with the accept/reject
decision.
  While working on a dataset of close to 8k reviews from ICLR, one of the top
conferences in the field of machine learning, we use an active learning
framework to build a training dataset for aspect prediction, which is further
used to obtain the aspects and sentiments for the entire dataset. We show that
the distribution of aspect-based sentiments obtained from a review is
significantly different for accepted and rejected papers. We use the aspect
sentiments from these reviews to make an intriguing observation, certain
aspects present in a paper and discussed in the review strongly determine the
final recommendation. As a second objective, we quantify the extent of
disagreement among the reviewers refereeing a paper. We also investigate the
extent of disagreement between the reviewers and the chair and find that the
inter-reviewer disagreement may have a link to the disagreement with the chair.
One of the most interesting observations from this study is that reviews, where
the reviewer score and the aspect sentiments extracted from the review text
written by the reviewer are consistent, are also more likely to be concurrent
with the chair's decision.
"
2984,"The citation impact of social sciences and humanities upon patentable
  technology","  This paper examines the citation impact of papers published in
scientific-scholarly journals upon patentable technology, as reflected in
examiner- or inventor-given references in granted patents. It analyses data
created by SCImago Research Group, linking PATSTAT's scientific non-patent
references (SNPRs) to source documents indexed in Scopus. The frequency of
patent citations to journal papers is calculated per discipline, year,
institutional sector, journal subject category, and for ""top"" journals.
PATSTAT/Scopus-based statistics are compared to those derived from Web of
Science/USPTO linkage. A detailed assessment is presented of the technological
impact of research publications in social sciences and humanities (SSH).
Several subject fields perform well in terms of the number of citations from
patents, especially Library & Information Science, Language & Linguistics,
Education, and Law, but many of the most cited journals find themselves in the
interface between SSH and biomedical or natural sciences. Analyses of the
titles of citing patents and cited papers are presented that shed light upon
the cognitive content of patent citations. It is proposed to develop more
advanced indicators of citation impact of papers upon patents, and ways to
combine citation counts with citation content and context analysis.
"
2985,"Statistical relationships between corresponding authorship,
  international co-authorship and citation impact of national research systems","  This paper presents a statistical analysis of the relationship between three
science indicators applied in earlier bibliometric studies, namely research
leadership based on corresponding authorship, international collaboration using
international co-authorship data, and field-normalized citation impact.
Indicators at the level of countries are extracted from the SIR database
created by SCImago Research Group from publication records indexed for
Elsevier's Scopus. The relationship between authorship and citation-based
indicators is found to be complex, as it reflects a country's phase of
scientific development and the coverage policy of the database. Moreover, one
should distinguish a genuine leadership effect from a purely statistical effect
due to fractional counting. Further analyses at the level of institutions and
qualitative validation studies are recommended.
"
2986,Quantifying the higher-order influence of scientific publications,"  Citation impact is commonly assessed using direct, first-order citation
relations. We consider here instead the indirect influence of publications on
new publications via citations. We present a novel method to quantify the
higher-order citation influence of publications, considering both direct, or
first-order, and indirect, or higher-order citations. In particular, we are
interested in higher-order citation influence at the level of disciplines. We
apply this method to the whole Web of Science data at the level of disciplines.
We find that a significant amount of influence -- 42% -- stems from
higher-order citations. Furthermore, we show that higher-order citation
influence is helpful to quantify and visualize citation flows among
disciplines, and to assess their degree of interdisciplinarity.
"
2987,Citing is earlier than Cited?,"  Generally, it is common that cited papers are earlier than citing papers. But
we found three different cases, with more undiscovered. In this letter, we
attempted to explain the reasons. However, negative time lag between citing and
cited papers may mislead us when we study the characteristics of science.
"
2988,"Coronavirus research before 2020 is more relevant than ever, especially
  when interpreted for COVID-19","  The speed with which biomedical researchers were able to identify and
characterise COVID-19 was clearly due to prior research with other
coronaviruses. Early epidemiological comparisons with two previous
coronaviruses, Severe Acute Respiratory Syndrome (SARS) and Middle East
Respiratory Syndrome (MERS), also made it easier to predict COVID-19's likely
spread and lethality. This article assesses whether academic interest in prior
coronavirus research has translated into interest in the primary source
material, using Mendeley reader counts for early academic impact evidence. The
results confirm that SARS and MERS research 2008-2017 experienced anomalously
high increases in Mendeley readers in April-May 2020. Nevertheless, studies
learning COVID-19 lessons from SARS and MERS or using them as a benchmark for
COVID-19 have generated much more academic interest than primary studies of
SARS or MERS. Thus, research that interprets prior relevant research for new
diseases when they are discovered seems to be particularly important to help
researchers to understand its implications in the new context.
"
2989,"Gender-Based Homophily in Research: A Large-Scale Study of Man-Woman
  Collaboration","  This paper investigates the respective impacts of (1) biological age, (2)
academic position, (3) academic discipline, (4) journal prestige, and (5) type
of institution of employment on the propensity to conduct same-sex
collaboration in research. The gender homophily principle was found to work for
male scientists - but not for females. The majority of male scientists
collaborate solely with males; most female scientists, in contrast, do not
collaborate with females at all. The propensity for same-sex collaboration of
males is three times that of females. Across all age cohorts, scientists of
both genders tend to collaborate more with males. All-female collaboration is
marginal, while all-male collaboration is pervasive. However, a year-by-year
approach confirmed a downward trend in same-sex collaboration among males and
an upward trend among females. Additionally, gender homophily in
research-intensive institutions proved stronger for males than for females.
Finally, we estimated odds ratios of high homophily in publishing and used
linear regression to explain the variability of the same-sex collaboration
ratio in publishing. A comprehensive, fully integrated, biographical,
administrative, publication, and citation database was used, and our sample (N
= 25,463) included all Polish professors employed in 85 research-involved
universities, grouped into 27 disciplines, with all their Scopus-indexed
2009-2018 publications (158,743 articles).
"
2990,"The utilization of paper-level classification system on the evaluation
  of journal impact","  CAS Journal Ranking, a ranking system of journals based on the bibliometric
indicator of citation impact, has been widely used in meso and macro-scale
research evaluation in China since its first release in 2004. The ranking's
coverage is journals which contained in the Clarivate's Journal Citation
Reports (JCR). This paper will mainly introduce the upgraded version of the
2019 CAS journal ranking. Aiming at limitations around the indicator and
classification system utilized in earlier editions, also the problem of
journals' interdisciplinarity or multidisciplinarity, we will discuss the
improvements in the 2019 upgraded version of CAS journal ranking (1) the CWTS
paper-level classification system, a more fine-grained system, has been
utilized, (2) a new indicator, Field Normalized Citation Success Index (FNCSI),
which ia robust against not only extremely highly cited publications, but also
the wrongly assigned document type, has been used, and (3) the calculation of
the indicator is from a paper-level. In addition, this paper will present a
small part of ranking results and an interpretation of the robustness of the
new FNCSI indicator. By exploring more sophisticated methods and indicators,
like the CWTS paper-level classification system and the new FNCSI indicator,
CAS Journal Ranking will continue its original purpose for responsible research
evaluation.
"
2991,"Comparison of Citations Trends between the COVID-19 Pandemic and
  SARS-CoV, MERS-CoV, Ebola, Zika, Avian and Swine Influenza Epidemics","  Objective: The novel coronavirus COVID-19 outbreak rapidly evolved into
pandemic. Global research efforts focus on this topic and with the
collaboration of the scientific journals publication industry produced more
than 16,000 related published articles in PubMed within five months from the
onset of the outbreak. Herein, a comparison of the COVID-19 citations in PubMed
and Web of Science was performed with SARS-CoV, MERS-CoV, Ebola, Zika, avian
and swine influenza epidemics. Methods: The citations were searched and
collected using the disease terms and the date of publication restriction. The
total number of PubMed citations and the HIV associated papers during the same
chronological periods were examined in parallel. The journal category and
country information of the publications were gathered from Web of Science. The
collected data were statistically analyzed and compared. Results: Significant
correlations were found between COVID-19 and MERS (CC=0.988; p=0.003; q=0.006),
Ebola (CC=0.987; p=0.003; q=0.011), and SARS (CC=0.964; p=0.015; q=0.028)
epidemics five-month pick of novel citations in PubMed. However, COVID-19
publications were accumulated earlier and in larger numbers than any other 21st
century major communicable disease outbreak. Conclusion: The acceleration and
the total number of COVID-19 publications represent an unprecedented landmark
event in the medical library history. The immediate adoption of the fast-track
peer-reviewing and publishing as well as the open access publication policies
by the journal publishers are significant contributors to this bibliographic
phenomenon.
"
2992,Tropes in films: an initial analysis,"  TVTropes is a wiki that describes tropes and which ones are used in which
artistic work. We are mostly interested in films, so after releasing the
TropeScraper Python module that extracts data from this site, in this report we
use scraped information to describe statistically how tropes and films are
related to each other and how these relations evolve in time. In order to do
so, we generated a dataset through the tool TropeScraper in April 2020. We have
compared it to the latest snapshot of DB Tropes, a dataset covering the same
site and published in July 2016, providing descriptive analysis, studying the
fundamental differences and addressing the evolution of the wiki in terms of
the number of tropes, the number of films and connections. The results show
that the number of tropes and films doubled their value and quadrupled their
relations, and films are, at large, better described in terms of tropes.
However, while the types of films with the most tropes has not changed
significantly in years, the list of most popular tropes has. This outcome can
help on shedding some light on how popular tropes evolve, which ones become
more popular or fade away, and in general how a set of tropes represents a film
and might be a key to its success. The dataset generated, the information
extracted, and the summaries provided are useful resources for any research
involving films and tropes. They can provide proper context and explanations
about the behaviour of models built on top of the dataset, including the
generation of new content or its use in machine learning.
"
2993,"Tracking the Twitter attention around the research efforts on the
  COVID-19 pandemic","  The outbreak of the COVID-19 pandemic has been accompanied by a bulk of
scientific research and related Twitter discussions. To unravel the public
concerns about the COVID-19 crisis reflected in the science-based Twitter
conversations, this study tracked the Twitter attention around the COVID-19
research efforts during the first three months of 2020. On the basis of nearly
1.4 million Twitter mentions of 6,162 COVID-19-related scientific publications,
we investigated the temporal tweeting dynamic and the Twitter users involved in
the online discussions around COVID-19-related research. The results show that
the quantity of Twitter mentions of COVID-19-related publications was on
rising. Scholarly-oriented Twitter users played an influential role in
disseminating research outputs on COVID-19, with their tweets being frequently
retweeted. Over time, a change in the focus of the Twitter discussions can be
observed, from the initial attention to virological and clinical research to
more practical topics, such as the potential treatments, the countermeasures by
the governments, the healthcare measures, and the influences on the economy and
society, in more recent times.
"
2994,"PeopleMap: Visualization Tool for Mapping Out Researchers using Natural
  Language Processing","  Discovering research expertise at institutions can be a difficult task.
Manually curated university directories easily become out of date and they
often lack the information necessary for understanding a researcher's interests
and past work, making it harder to explore the diversity of research at an
institution and identify research talents. This results in lost opportunities
for both internal and external entities to discover new connections and nurture
research collaboration. To solve this problem, we have developed PeopleMap, the
first interactive, open-source, web-based tool that visually ""maps out""
researchers based on their research interests and publications by leveraging
embeddings generated by natural language processing (NLP) techniques. PeopleMap
provides a new engaging way for institutions to summarize their research
talents and for people to discover new connections. The platform is developed
with ease-of-use and sustainability in mind. Using only researchers' Google
Scholar profiles as input, PeopleMap can be readily adopted by any institution
using its publicly-accessible repository and detailed documentation.
"
2995,COVID-19 amplifies gender disparities in research,"  Early evidence suggests that women, including female researchers, are
disproportionately affected by the \mbox{COVID-19} pandemic, with negative
consequences to their productivity. Here, we test this hypothesis by analyzing
the proportion of male and female researchers that publish scientific papers
during the pandemic. We use data from biomedical preprint servers and
Springer-Nature journals to show that the fraction of women publishing during
the pandemic drops significantly across disciplines and research topics, after
controlling for temporal trends. The impact is particularly pronounced for
biomedical papers related to COVID-19 research. Further, by geocoding author's
affiliations, we show that gender disparities are exacerbated in poorer
countries, even though these countries had less of a gender gap in research
prior to the pandemic. Our results illustrate how exceptional events like a
global pandemic can further amplify gender inequalities in research. Our work
could inform fairer scientific evaluation practices, especially for
early-career female researchers who may be disproportionately affected by the
pandemic.
"
2996,"A Unified Nanopublication Model for Effective and User-Friendly Access
  to the Elements of Scientific Publishing","  Scientific publishing is the means by which we communicate and share
scientific knowledge, but this process currently often lacks transparency and
machine-interpretable representations. Scientific articles are published in
long coarse-grained text with complicated structures, and they are optimized
for human readers and not for automated means of organization and access. Peer
reviewing is the main method of quality assessment, but these peer reviews are
nowadays rarely published and their own complicated structure and linking to
the respective articles is not accessible. In order to address these problems
and to better align scientific publishing with the principles of the Web and
Linked Data, we propose here an approach to use nanopublications as a unifying
model to represent in a semantic way the elements of publications, their
assessments, as well as the involved processes, actors, and provenance in
general. To evaluate our approach, we present a dataset of 627 nanopublications
representing an interlinked network of the elements of articles (such as
individual paragraphs) and their reviews (such as individual review comments).
Focusing on the specific scenario of editors performing a meta-review, we
introduce seven competency questions and show how they can be executed as
SPARQL queries. We then present a prototype of a user interface for that
scenario that shows different views on the set of review comments provided for
a given manuscript, and we show in a user study that editors find the interface
useful to answer their competency questions. In summary, we demonstrate that a
unified and semantic publication model based on nanopublications can make
scientific communication more effective and user-friendly.
"
2997,"Characterising authors on the extent of their paper acceptance: A case
  study of the Journal of High Energy Physics","  New researchers are usually very curious about the recipe that could
accelerate the chances of their paper getting accepted in a reputed forum
(journal/conference). In search of such a recipe, we investigate the profile
and peer review text of authors whose papers almost always get accepted at a
venue (Journal of High Energy Physics in our current work). We find authors
with high acceptance rate are likely to have a high number of citations, high
$h$-index, higher number of collaborators etc. We notice that they receive
relatively lengthy and positive reviews for their papers. In addition, we also
construct three networks -- co-reviewer, co-citation and collaboration network
and study the network-centric features and intra- and inter-category edge
interactions. We find that the authors with high acceptance rate are more
`central' in these networks; the volume of intra- and inter-category
interactions are also drastically different for the authors with high
acceptance rate compared to the other authors. Finally, using the above set of
features, we train standard machine learning models (random forest, XGBoost)
and obtain very high class wise precision and recall. In a followup discussion
we also narrate how apart from the author characteristics, the peer-review
system might itself have a role in propelling the distinction among the
different categories which could lead to potential discrimination and
unfairness and calls for further investigation by the system admins.
"
2998,"Journal article publishing in the social sciences and humanities: a
  comparison of Web of Science coverage for five European countries","  This study compares publication pattern dynamics in the social sciences and
humanities in five European countries. Three are Central and Eastern European
countries that share a similar cultural and political heritage (the Czech
Republic, Slovakia, and Poland). The other two are Flanders (Belgium) and
Norway, representing Western Europe and the Nordics, respectively. We analysed
449,409 publications from 2013-2016 and found that, despite persisting
differences between the two groups of countries across all disciplines,
publication patterns in the Central and Eastern European countries are becoming
more similar to those in their Western and Nordic counterparts. Articles from
the Central and Eastern European countries are increasingly published in
journals indexed in Web of Science and also in journals with the highest
citation impacts. There are, however, clear differences between social science
and humanities disciplines, which need to be considered in research evaluation
and science policy.
"
2999,"Pot, kettle: Nonliteral titles aren't (natural) science","  Researchers may be tempted to attract attention through poetic titles for
their publications, but would this be mistaken in some fields? Whilst poetic
titles are known to be common in medicine, it is not clear whether the practice
is widespread elsewhere. This article investigates the prevalence of poetic
expressions in journal article titles 1996-2019 in 3.3 million articles from
all 27 Scopus broad fields. Expressions were identified by manually checking
all phrases with at least 5 words that occurred at least 25 times, finding 149
stock phrases, idioms, sayings, literary allusions, film names and song titles
or lyrics. The expressions found are most common in the social sciences and the
humanities. They are also relatively common in medicine, but almost absent from
engineering and the natural and formal sciences. The differences may reflect
the less hierarchical and more varied nature of the social sciences and
humanities, where interesting titles may attract an audience. In engineering,
natural science and formal science fields, authors should take extra care with
poetic expressions, in case their choice is judged inappropriate. This includes
interdisciplinary research overlapping these areas. Conversely, reviewers of
interdisciplinary research involving the social sciences should be more
tolerant of poetic license.
"
3000,"Communities of attention networks: introducing qualitative and
  conversational perspectives for altmetrics","  We propose to analyze the level of recommendation and spreading in the
sharing of scientific papers on Twitter to understand the interactions of
communities around papers and to develop the ""Community of Attention Network""
(CAN). In this paper, a pilot case study was conducted for the paper
'Pharmacological Treatment of Obesity' authored by Mancini & Halpern (2002), an
extensive review of the criteria for evaluating the efficacy of anti-obesity
treatments and derived pharmacological agents. The altmetric data was collected
from Altmetric.com and the description information for each tweeter was
extracted from their Twitter profiles. The data were analyzed with
Microanalysis Of Online Data perspective to investigate the formation of a CAN
around this focal paper and the context of its formation. The studied article
received 736 tweets from 134 different users with a combined exposure of more
than 459,018 followers and a high level of spreading (67.26%) and
recommendation (28.53%). The user's bios information analysis of who shares the
article indicate individual profiles focused on personal issues and strong
civic and political engagement. Personal-professional and institutional
tweeters of the national political scene are often mentioned in the tweets. In
analyzing the content of the tweets, we note that the altmetric score of the
paper is a result of its strategic use as an online activism resource and a
digital advocacy tool used to mobilize stakeholders for awareness and support
activities. This study and the contextual and network perspective it introduces
may help to understand the social impact of publications by using altmetrics.
"
3001,"Scientometric analysis and knowledge mapping of literature-based
  discovery (1986-2020)","  Literature-based discovery (LBD) aims to discover valuable latent
relationships between disparate sets of literatures. LBD research has undergone
an evolution from being an emerging area to a mature research field. Hence it
is timely and necessary to summarize the LBD literature and scrutinize general
bibliographic characteristics, current and future publication trends, and its
intellectual structure. This paper presents the first inclusive scientometric
overview of LBD research. We utilize a comprehensive scientometric approach
incorporating CiteSpace to systematically analyze the literature on LBD from
the last four decades (1986-2020). After manual cleaning, we have retrieved a
total of 409 documents from six bibliographic databases (Web of Science,
Scopus, PubMed, IEEE Xplore, ACM Digital Library, and Springer Link) and two
preprint servers (ArXiv and BiorXiv). The results have shown that Thomas C.
Rindflesch published the highest number of LBD papers, followed by Don R.
Swanson. The United States plays a leading role in LBD research with the
University of Chicago as the dominant institution. To go deeper, we also
perform science mapping including cascading citation expansion. The knowledge
base of LBD research has changed significantly since its inception, with
emerging topics including deep learning and explainable artificial
intelligence. The results have indicated that LBD is still growing and
evolving. Drawing on our insights, we now better understand the historical
progress of LBD in the last 35 years and are able to improve publishing
practices to contribute to the field in the future.
"
3002,The role of metadata in reproducible computational research,"  Reproducible computational research (RCR) is the keystone of the scientific
method for in silico analyses, packaging the transformation of raw data to
published results. In addition to its role in research integrity, RCR has the
capacity to significantly accelerate evaluation and reuse. This potential and
wide-support for the FAIR principles have motivated interest in metadata
standards supporting RCR. Metadata provides context and provenance to raw data
and methods and is essential to both discovery and validation. Despite this
shared connection with scientific data, few studies have explicitly described
the relationship between metadata and RCR. This article employs a functional
content analysis to identify metadata standards that support RCR functions
across an analytic stack consisting of input data, tools, notebooks, pipelines,
and publications. Our article provides background context, explores gaps, and
discovers component trends of embeddedness and methodology weight from which we
derive recommendations for future work.
"
3003,Causal Knowledge Extraction from Scholarly Papers in Social Sciences,"  The scale and scope of scholarly articles today are overwhelming human
researchers who seek to timely digest and synthesize knowledge. In this paper,
we seek to develop natural language processing (NLP) models to accelerate the
speed of extraction of relationships from scholarly papers in social sciences,
identify hypotheses from these papers, and extract the cause-and-effect
entities. Specifically, we develop models to 1) classify sentences in scholarly
documents in business and management as hypotheses (hypothesis classification),
2) classify these hypotheses as causal relationships or not (causality
classification), and, if they are causal, 3) extract the cause and effect
entities from these hypotheses (entity extraction). We have achieved high
performance for all the three tasks using different modeling techniques. Our
approach may be generalizable to scholarly documents in a wide range of social
sciences, as well as other types of textual materials.
"
3004,Gender Inequality in Research Productivity During the COVID-19 Pandemic,"  We study the disproportionate impact of the lockdown as a result of the
COVID-19 outbreak on female and male academics' research productivity in social
science. The lockdown has caused substantial disruptions to academic
activities, requiring people to work from home. How this disruption affects
productivity and the related gender equity is an important operations and
societal question. We collect data from the largest open-access preprint
repository for social science on 41,858 research preprints in 18 disciplines
produced by 76,832 authors across 25 countries over a span of two years. We use
a difference-in-differences approach leveraging the exogenous pandemic shock.
Our results indicate that, in the 10 weeks after the lockdown in the United
States, although the total research productivity increased by 35%, female
academics' productivity dropped by 13.9% relative to that of male academics. We
also show that several disciplines drive such gender inequality. Finally, we
find that this intensified productivity gap is more pronounced for academics in
top-ranked universities, and the effect exists in six other countries. Our work
points out the fairness issue in productivity caused by the lockdown, a finding
that universities will find helpful when evaluating faculty productivity. It
also helps organizations realize the potential unintended consequences that can
arise from telecommuting.
"
3005,"Extraction and Evaluation of Formulaic Expressions Used in Scholarly
  Papers","  Formulaic expressions, such as 'in this paper we propose', are helpful for
authors of scholarly papers because they convey communicative functions; in the
above, it is showing the aim of this paper'. Thus, resources of formulaic
expressions, such as a dictionary, that could be looked up easily would be
useful. However, forms of formulaic expressions can often vary to a great
extent. For example, 'in this paper we propose', 'in this study we propose' and
'in this paper we propose a new method to' are all regarded as formulaic
expressions. Such a diversity of spans and forms causes problems in both
extraction and evaluation of formulaic expressions. In this paper, we propose a
new approach that is robust to variation of spans and forms of formulaic
expressions. Our approach regards a sentence as consisting of a formulaic part
and non-formulaic part. Then, instead of trying to extract formulaic
expressions from a whole corpus, by extracting them from each sentence,
different forms can be dealt with at once. Based on this formulation, to avoid
the diversity problem, we propose evaluating extraction methods by how much
they convey specific communicative functions rather than by comparing extracted
expressions to an existing lexicon. We also propose a new extraction method
that utilises named entities and dependency structures to remove the
non-formulaic part from a sentence. Experimental results show that the proposed
extraction method achieved the best performance compared to other existing
methods.
"
3006,"Convergent validity of several indicators measuring disruptiveness with
  milestone assignments to physics papers by experts","  This study focuses on a recently introduced type of indicator measuring
disruptiveness in science. Disruptive research diverges from current lines of
research by opening up new lines. In the current study, we included the
initially proposed indicator of this new type (Wu, Wang, & Evans, 2019) and
several variants with DI1: DI5, DI1n, DI5n, and DEP. Since indicators should
measure what they propose to measure, we investigated the convergent validity
of the indicators. We used a list of milestone papers, selected and published
by editors of Physical Review Letters, and investigated whether this human
(experts - based list is related to values of the several disruption indicators
variants and - if so - which variants show the highest correlation with expert
judgements. We used bivariate statistics, multiple regression models, and
(coarsened) exact matching (CEM) to investigate the convergent validity of the
indicators. The results show that the indicators correlate differently with the
milestone paper assignments by the editors. It is not the initially proposed
disruption index that performed best (DI1), but the variant DI5 which has been
introduced by Bornmann, Devarakonda, Tekles, and Chacko (2019). In the CEM
analysis of this study, the DEP variant - introduced by Bu, Waltman, and Huang
(2019) - also showed favorable results.
"
3007,"Mapping the ""long tail"" of research funding: A topic analysis of NSF
  grant proposals in the Division of Astronomical Sciences","  ""Long tail"" data are considered to be smaller, heterogeneous, researcher-held
data, which present unique data management and scholarly communication
challenges. These data are presumably concentrated within relatively
lower-funded projects due to insufficient resources for curation. To better
understand the nature and distribution of long tail data, we examine National
Science Foundation (NSF) funding patterns using Latent Dirichlet Analysis (LDA)
and bibliographic data. We also introduce the concept of ""Topic Investment"" to
capture differences in topics across funding levels and to illuminate the
distribution of funding across topics. This study uses the discipline of
astronomy as a case study, overall exploring possible associations between
topic, funding level and research output, with implications for research policy
and practice. We find that while different topics demonstrate different funding
levels and publication patterns, dynamics predicted by the ""long tail""
theoretical framework presented here can be observed within NSF-funded topics
in astronomy.
"
3008,Digital personal health libraries: a systematic literature review,"  Objective: This paper gives context on recent literature regarding the
development of digital personal health libraries (PHL) and provides insights
into the potential application of consumer health informatics in diverse
clinical specialties. Materials and Methods: A systematic literature review was
conducted following the Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) statement. Here, 2,850 records were retrieved from
PubMed and EMBASE in March 2020 using search terms: personal, health, and
library. Information related to the health topic, target population, study
purpose, library function, data source, data science method, evaluation
measure, and status were extracted from each eligible study. In addition,
knowledge discovery methods, including co-occurrence analysis and multiple
correspondence analysis, were used to explore research trends of PHL. Results:
After screening, this systematic review focused on a dozen articles related to
PHL. These encompassed health topics such as infectious diseases, congestive
heart failure, electronic prescribing. Data science methods included relational
database, information retrieval technology, ontology construction technology.
Evaluation measures were heterogeneous regarding PHL functions and settings. At
the time of writing, only one of the PHLs described in these articles is
available for the public while the others are either prototypes or in the pilot
stage. Discussion: Although PHL researches have used different methods to
address problems in diverse health domains, there is a lack of an effective PHL
to meet the needs of older adults. Conclusion: The development of PHLs may
create an unprecedented opportunity for promoting the health of older consumers
by providing diverse health information.
"
3009,"An extensive analysis of the presence of altmetric data for Web of
  Science publications across subject fields and research topics","  Sufficient data presence is one of the key preconditions for applying metrics
in practice. Based on both Altmetric.com data and Mendeley data collected up to
2019, this paper presents a state-of-the-art analysis of the presence of 12
kinds of altmetric events for nearly 12.3 million Web of Science publications
published between 2012 and 2018. Results show that even though an upward trend
of data presence can be observed over time, except for Mendeley readers and
Twitter mentions, the overall presence of most altmetric data is still low. The
majority of altmetric events go to publications in the fields of Biomedical and
Health Sciences, Social Sciences and Humanities, and Life and Earth Sciences.
As to research topics, the level of attention received by research topics
varies across altmetric data, and specific altmetric data show different
preferences for research topics, on the basis of which a framework for
identifying hot research topics is proposed and applied to detect research
topics with higher levels of attention garnered on certain altmetric data
source. Twitter mentions and policy document citations were selected as two
examples to identify hot research topics of interest of Twitter users and
policy-makers, respectively, shedding light on the potential of altmetric data
in monitoring research trends of specific social attention.
"
3010,"NLPContributions: An Annotation Scheme for Machine Reading of Scholarly
  Contributions in Natural Language Processing Literature","  We describe an annotation initiative to capture the scholarly contributions
in natural language processing (NLP) articles, particularly, for the articles
that discuss machine learning (ML) approaches for various information
extraction tasks. We develop the annotation task based on a pilot annotation
exercise on 50 NLP-ML scholarly articles presenting contributions to five
information extraction tasks 1. machine translation, 2. named entity
recognition, 3. question answering, 4. relation classification, and 5. text
classification. In this article, we describe the outcomes of this pilot
annotation phase. Through the exercise we have obtained an annotation
methodology; and found ten core information units that reflect the contribution
of the NLP-ML scholarly investigations. The resulting annotation scheme we
developed based on these information units is called NLPContributions.
  The overarching goal of our endeavor is four-fold: 1) to find a systematic
set of patterns of subject-predicate-object statements for the semantic
structuring of scholarly contributions that are more or less generically
applicable for NLP-ML research articles; 2) to apply the discovered patterns in
the creation of a larger annotated dataset for training machine readers of
research contributions; 3) to ingest the dataset into the Open Research
Knowledge Graph (ORKG) infrastructure as a showcase for creating user-friendly
state-of-the-art overviews; 4) to integrate the machine readers into the ORKG
to assist users in the manual curation of their respective article
contributions. We envision that the NLPContributions methodology engenders a
wider discussion on the topic toward its further refinement and development.
Our pilot annotated dataset of 50 NLP-ML scholarly articles according to the
NLPContributions scheme is openly available to the research community at
https://doi.org/10.25835/0019761.
"
3011,"Introducing recalibrated academic performance indicators in the
  evaluation of individuals' research performance: A case study from Eastern
  Europe","  In Hungary, the highest and most prestigious scientific qualification is
considered to be the Doctor of Science (DSc) title being awarded by the
Hungarian Academy of Sciences. The academic performance indicators of the DSc
title are of high importance in the evaluation of individuals' research
performance not only when a researcher applies for obtaining a DSc title, but
also during promotions and appointments at universities, and in the case of the
evaluation of applications for scientific titles and degrees, and the
assessment of applications for funding. In the Section of Earth Sciences
encompassing nine related disciplines, rather than carrying out a
straightforward bibliometric analysis, the performance indicators were designed
as a result of a consensual agreement between leading academicians, each of
whom represented a particular discipline. Therefore, the minimum values of the
indicators, required to be fulfilled if one is applying for a DSc title, do not
adequately reflect the actual discipline-specific performance of researchers.
This problem may generate tension between researchers during the evaluation
process. The main goal of this paper is to recalibrate the minimum values of
four major performance indicators by taking the actual discipline-specific
distance ratios into account. In addition, each minimum value will be defined
by employing integer and fractional counting methods as well. The research
outcome of this study can provide impetus for the Section of Earth Sciences to
optimize the minimum values of the DSc title performance indicators by taking
the specifics of each discipline into account. Because academic performance
indicators are also employed in other Eastern European countries in the
evaluation of individuals' research performance, the methods used in that paper
can be placed into a wider geographical context.
"
3012,DINGO: an ontology for projects and grants linked data,"  We present DINGO (Data INtegration for Grants Ontology), an ontology that
provides a machine readable extensible framework to model data for
semantically-enabled applications relative to projects, funding, actors, and,
notably, funding policies in the research landscape. DINGO is designed to yield
high modeling power and elasticity to cope with the huge variety in funding,
research and policy practices, which makes it applicable also to other areas
besides research where funding is an important aspect. We discuss its main
features, the principles followed for its development, its community uptake,
its maintenance and evolution.
"
3013,Mining Misdiagnosis Patterns from Biomedical Literature,"  Diagnostic errors can pose a serious threat to patient safety, leading to
serious harm and even death. Efforts are being made to develop interventions
that allow physicians to reassess for errors and improve diagnostic accuracy.
Our study presents an exploration of misdiagnosis patterns mined from PubMed
abstracts. Article titles containing certain phrases indicating misdiagnosis
were selected and frequencies of these misdiagnoses calculated. We present the
resulting patterns in the form of a directed graph with frequency-weighted
misdiagnosis edges connecting diagnosis vertices. We find that the most
commonly misdiagnosed diseases were often misdiagnosed as many different
diseases, with each misdiagnosis having a relatively low frequency, rather than
as a single disease with greater probability. Additionally, while a
misdiagnosis relationship may generally exist, the relationship was often found
to be one-sided.
"
3014,"Operational Research Literature as a Use Case for the Open Research
  Knowledge Graph","  The Open Research Knowledge Graph (ORKG) provides machine-actionable access
to scholarly literature that habitually is written in prose. Following the FAIR
principles, the ORKG makes traditional, human-coded knowledge findable,
accessible, interoperable, and reusable in a structured manner in accordance
with the Linked Open Data paradigm. At the moment, in ORKG papers are described
manually, but in the long run the semantic depth of the literature at scale
needs automation. Operational Research is a suitable test case for this vision
because the mathematical field and, hence, its publication habits are highly
structured: A mundane problem is formulated as a mathematical model, solved or
approximated numerically, and evaluated systematically. We study the existing
literature with respect to the Assembly Line Balancing Problem and derive a
semantic description in accordance with the ORKG. Eventually, selected papers
are ingested to test the semantic description and refine it further.
"
3015,"Quantifying Policy Responses to a Global Emergency: Insights from the
  COVID-19 Pandemic","  Public policy must confront emergencies that evolve in real time and in
uncertain directions, yet little is known about the nature of policy response.
Here we take the coronavirus pandemic as a global and extraordinarily
consequential case, and study the global policy response by analyzing a novel
dataset recording policy documents published by government agencies, think
tanks, and intergovernmental organizations (IGOs) across 114 countries (37,725
policy documents from Jan 2nd through May 26th 2020). Our analyses reveal four
primary findings. (1) Global policy attention to COVID-19 follows a remarkably
similar trajectory as the total confirmed cases of COVID-19, yet with evolving
policy focus from public health to broader social issues. (2) The COVID-19
policy frontier disproportionately draws on the latest, peer-reviewed, and
high-impact scientific insights. Moreover, policy documents that cite science
appear especially impactful within the policy domain. (3) The global policy
frontier is primarily interconnected through IGOs, such as the WHO, which
produce policy documents that are central to the COVID19 policy network and
draw especially strongly on scientific literature. Removing IGOs' contributions
fundamentally alters the global policy landscape, with the policy citation
network among government agencies increasingly fragmented into many isolated
clusters. (4) Countries exhibit highly heterogeneous policy attention to
COVID-19. Most strikingly, a country's early policy attention to COVID-19 shows
a surprising degree of predictability for the country's subsequent deaths.
Overall, these results uncover fundamental patterns of policy interactions and,
given the consequential nature of emergent threats and the paucity of
quantitative approaches to understand them, open up novel dimensions for
assessing and effectively coordinating global and local responses to COVID-19
and beyond.
"
3016,"Towards an automated repository for indexing, analysis and
  characterization of municipal e-government websites in Mexico","  This article addresses a problem in the electronic government discipline with
special interest in Mexico: the need for a concentrated and updated information
source about municipal e-government websites. One reason for this is the lack
of a complete and updated database containing the electronic addresses (web
domain names) of the municipal governments having a website. Due to diverse
causes, not all the Mexican municipalities have one, and a number of those
having it do not present information corresponding to the current governments
but, instead, to other previous ones. The scarce official lists of municipal
websites are not updated with the sufficient frequency, and manually
determining which municipalities have an operating and valid website in a given
moment is a time-consuming process. Besides, website contents do not always
comply with legal requirements and are considerably heterogeneous. In turn, the
evolution development level of municipal websites is valuable information that
can be harnessed for diverse theoretical and practical purposes in the public
administration field. Obtaining all these pieces of information requires
website content analysis. Therefore, this article investigates the need for and
the feasibility to automate implementation and updating of a digital repository
to perform diverse analyses of these websites. Its technological feasibility is
addressed by means of a literature review about web scraping and by proposing a
preliminary manual methodology. This takes into account known, proven,
techniques and software tools for web crawling and scraping. No new techniques
for crawling or scraping are proposed because the existing ones satisfy the
current needs. Finally, software requirements are specified in order to
automate the creation, updating, indexing, and analyses of the repository.
"
3017,Metrics and peer review agreement at the institutional level,"  In the past decades, many countries have started to fund academic
institutions based on the evaluation of their scientific performance. In this
context, peer review is often used to assess scientific performance.
Bibliometric indicators have been suggested as an alternative. A recurrent
question in this context is whether peer review and metrics tend to yield
similar outcomes. In this paper, we study the agreement between bibliometric
indicators and peer review at the institutional level. Additionally, we also
quantify the internal agreement of peer review at the institutional level. We
find that the level of agreement is generally higher at the institutional level
than at the publication level. Overall, the agreement between metrics and peer
review is on par with the internal agreement among two reviewers for certain
fields of science. This suggests that for some fields, bibliometric indicators
may possibly be considered as an alternative to peer review for national
research assessment exercises.
"
3018,The Sci-hub Effect: Sci-hub downloads lead to more article citations,"  Citations are often used as a metric of the impact of scientific
publications. Here, we examine how the number of downloads from Sci-hub as well
as various characteristics of publications and their authors predicts future
citations. Using data from 12 leading journals in economics, consumer research,
neuroscience, and multidisciplinary research, we found that articles downloaded
from Sci-hub were cited 1.72 times more than papers not downloaded from Sci-hub
and that the number of downloads from Sci-hub was a robust predictor of future
citations. Among other characteristics of publications, the number of figures
in a manuscript consistently predicts its future citations. The results suggest
that limited access to publications may limit some scientific research from
achieving its full impact.
"
3019,Interdisciplinary research and technological impact,"  Interdisciplinary research has been considered as a solution to today's
complex societal challenges. While its relationship with scientific impact has
been extensively studied, the technological impact of interdisciplinary
research remains unexplored. Here, we examine how interdisciplinarity is
associated with technological impact at the paper level. We measure the degree
of interdisciplinarity of a paper using three popular indicators, namely
variety, balance, and disparity, and track how it gets cited by patented
technologies over time. Drawing on a large sample of biomedical papers
published in 18 years, we find that papers that cites more fields (variety) and
whose distributions over those cited fields are more even (balance) are more
likely to receive patent citations, but both effects can be offset if papers
draw upon more distant fields (disparity). Those associations are consistent
across different citation-window lengths. Additional analysis that focuses on
the subset of papers with at least one patent citation reveals that the
intensity of their technological impact, as measured as the number of patent
citations, increases with balance and disparity. Our work may have policy
implications for interdisciplinary research and scientific and technology
impact.
"
3020,An agent-based model of interdisciplinary interactions in science,"  An increased interdisciplinarity in science projects has been highlighted as
crucial to tackle complex real-world challenges, but also as beneficial for the
development of disciplines themselves. This paper introduces a parcimonious
agent-based model of interdisciplinary relationships in collective entreprises
of knowledge discovery, to investigate the impact of scientist-level decisions
and preferences on global interdisciplinarity patterns. Under the assumption of
simple rules for individual researcher project management, such as trade-offs
between invested time overhead and knowledge benefit, model simulations show
that individual choices influence the distribution of compromise points between
emergent level of disciplinary depth and interdisciplinarity in a non-linear
way. Different structures for collaboration networks may also yield various
outcomes in terms of global interdisciplinarity. We conclude that independently
of the research field, the organization of research, and more particularly the
local balancing between vertical and horizontal research, already influences
the final positioning of research results and the extent of the knowledge
front. This suggests direct applications to research policies with a bottom-up
leverage on the interactions between disciplines.
"
3021,Delayed Recognition; the Co-citation Perspective,"  A Sleeping Beauty is a publication that is apparently unrecognized for some
period of time before experiencing sudden recognition by citation. Various
reasons, including resistance to new ideas, have been attributed to such
delayed recognition. We examine this phenomenon in the special case of
co-citations, which represent new ideas generated through the combination of
existing ones. Using relatively stringent selection criteria derived from the
work of others, we analyze a very large dataset of over 940 million unique
co-cited article pairs, and identified 1,196 cases of delayed co-citations. We
further classify these 1,196 cases with respect to amplitude, rate of citation,
and disciplinary origin and discuss alternative approaches towards identifying
such instances.
"
3022,"Lest We Forget: A Dataset of Coronavirus-Related News Headlines in Swiss
  Media","  We release our COVID-19 news dataset, containing more than 10,000 links to
news articles related to the Coronavirus pandemic published in the Swiss media
since early January 2020. This collection can prove beneficial in mining and
analysis of the reaction of the Swiss media and the COVID-19 pandemic and
extracting insightful information for further research. We hope this dataset
helps researchers and the public deliver results that will help analyse the
pandemic and potentially lead to a better understanding of the events.
"
3023,"How circular economy and industrial ecology concepts are intertwined? A
  bibliometric and text mining analysis","  Combining new insights from both bibliometric and text mining analyses, with
prior relevant research conversations on circular economy (CE) and industrial
ecology (IE), this paper aims to clarify the recent development trends and
relations between these concepts, including their representations and
applications. On this basis, discussions are made and recommendations provided
on how CE and IE approaches, tools, and indicators can complement each other to
enable and catalyze a more circular and sustainable development, by supporting
sustainable policy-making and monitoring sound CE strategies in industrial
practices.
"
3024,TICO-19: the Translation Initiative for Covid-19,"  The COVID-19 pandemic is the worst pandemic to strike the world in over a
century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating
to vulnerable populations the means by which they can protect themselves. To
this end, the collaborators forming the Translation Initiative for COvid-19
(TICO-19) have made test and development data available to AI and MT
researchers in 35 different languages in order to foster the development of
tools and resources for improving access to information about COVID-19 in these
languages. In addition to 9 high-resourced, ""pivot"" languages, the team is
targeting 26 lesser resourced languages, in particular languages of Africa,
South Asia and South-East Asia, whose populations may be the most vulnerable to
the spread of the virus. The same data is translated into all of the languages
represented, meaning that testing or development can be done for any pairing of
languages in the set. Further, the team is converting the test and development
data into translation memories (TMXs) that can be used by localizers from and
to any of the languages.
"
3025,Incorrect Data in the Widely Used Inside Airbnb Dataset,"  The use of datasets that contain incorrect data could have several severe
consequences. In recent years, large datasets that consist of online user
generated contents have been leveraged by scholars to study social phenomena,
test hypotheses, and develop predictive models. The collection of such large
datasets, however, remains a challenge for researchers. Many use existing
datasets that have been collected and verified by other scholars. Others use
open datasets that are available online for direct download. By using an open
dataset, scholars are able to avoid the often tedious data collection phase and
focus instead on their research inquiries. While many of these datasets are
verified and reliable, some are not, and therefore may have data quality
issues. Inside Airbnb is a website that collects data of places and their
reviews as posted by users of Airbnb.com. Visitors can effortlessly download
data collected by Inside Airbnb for several locations around the globe. While
the dataset is widely used in academic research, no thorough investigation of
the dataset and its validity has been conducted. This paper analyzes the
dataset and explains one major data quality issue that was discovered. The
primary contribution of this evidence-based work is its documentation of
incorrect data added to the dataset. Findings suggest that this issue is
attributed to systemic errors in the data collection process. Additionally,
this paper explains why reproducibility is a problem when two different
releases of the dataset are compared.
"
3026,"Open-Pub: A Transparent yet Privacy-Preserving Academic Publication
  System based on Blockchain","  Academic publication of latest research results are crucial to advance
development of all disciplines. However, there are a number of severe
disadvantages in current academic publication systems. The first problem is the
misconduct during the publication process due to the opaque paper review
process. An anonymous reviewer may give biased comments to a paper without
being noticed or punished, because the comments are seldom published for
evaluation. Secondly, the author anonymity during the paper review process is
easily compromised since this information is simply open to the conference
chair or the journal editor. Last but not least, access to research papers is
restricted to only subscribers, and even the authors cannot access their own
papers. In this paper, we propose Open-Pub, a decentralized, transparent yet
privacy-preserving academic publication scheme using the blockchain technology,
aiming to reduce academic misconducts and promote free sharing of research
results. To this end, we design a threshold group signature to achieve
anonymity for reviewers and authors. With this group signature, authors can
choose to submit papers anonymously and validators take turns to distribute
papers anonymously to reviewers on the blockchain according to their research
interests. After the reviewers submit their review comments, the identities of
reviewers and anonymous authors will be disclosed. These processes will be
recorded on the blockchain so that everyone can trace the entire process. To
evaluate its efficiency, we implement Open-Pub based on Ethereum source code
and conduct comprehensive experiments to evaluate its performance, including
computation cost and processing delay. The experiment results show that
Open-Pub is highly efficient in computation and processing anonymous
transactions.
"
3027,Impact of the COVID-19 Pandemic on Fundamental Physics Research,"  The outbreak and pandemic of the COVID-19 has dramatically changed the world
in almost every aspect. Its impact on the community of high energy physics is
also apparent. In this analysis we show that social distancing has greatly
changed the working style of the physics community. In the field of high energy
physics the online communications and video conferences are becoming daily
routine arrangements. However, such a change of working styles may need some
time to get used to. The influence can be seen by scientific outcomes harvested
from arXiv, INSPIRE databases and JHEP, PRD official websites.
"
3028,"Physics Letters B publications from 1967 to 2020. An analysis of the WEB
  page content of PLB","  Having been an editor for Physics Letters B (PLB) for many years I became
interested in the history of publishing, especially in particle physics (HEP).
Since PLB goes back to 1967 and an index of all PLB publications is available
online, this information was used to look at the history of PLB publications
over time. It should be noted that PLB publishes new results in particle
physics or high energy physics, nuclear physics and also in astrophysics and
cosmology. This is the start of an effort to look at all publishers that
publish research from HEP.
"
3029,"India's rank and global share in scientific research -- how data sourced
  from different databases can produce varying outcomes","  India is emerging as a major knowledge producer of the world in terms of
proportionate share of global research output and the overall research
productivity rank. Many recent reports, both of commissioned studies from
Government of India as well as independent international agencies, show India
at different ranks of global research productivity (variations as large as from
3rd to 9th place). The paper examines this contradiction; tries to analyse as
to why different reports places India at different ranks and what may be the
reasons thereof. The research output data for India, along with the ten most
productive countries in the world, is analysed from three major scholarly
databases: Web of Science, Scopus and Dimensions for this purpose. Results show
that both, the endogenous factors (such as database coverage variation and
different subject classification schemes) and the exogenous factors (such as
subject selection and publication counting methodology) cause the variations in
different reports. This paper reports first part of the analysis, focusing
mainly on variations due to use of data from different databases. The policy
implications of the study are also discussed.
"
3030,"India's rank and global share in scientific research -- how publication
  counting method and subject selection can vary the outcomes","  During the last two decades, India has emerged as a major knowledge producer
in the world, however different reports put it at different ranks, varying from
3rd to 9th places. The recent commissioned study reports of Department of
Science and Technology (DST) done by Elsevier and Clarivate Analytics, rank
India at 5thand 9th places, respectively. On the other hand, an independent
report by National Science Foundation (NSF) of United States (US), ranks India
at 3rd place on research output in Science and Engineering area. Interestingly,
both, the Elsevier and the NSF reports use Scopus data, and yet surprisingly
their outcomes are different. This article, therefore, attempts to investigate
as to how the use of same database can still produce different outcomes, due to
differences in methodological approaches. The publication counting method used
and the subject selection approach are the two main exogenous factors
identified to cause these variations. The implications of the analytical
outcomes are discussed with special focus on policy perspectives.
"
3031,"Does presence of social media plugins in a journal website result in
  higher social media attention of its research publications","  Social media platforms have now emerged as an important medium for wider
dissemination of research articles; with authors, readers and publishers
creating different kinds of social media activity about the article. Some
research studies have even shown that articles that get more social media
attention may get higher visibility and citations. These factors are now
persuading journal publishers to integrate social media plugins in their
webpages to facilitate sharing and dissemination of articles in social media
platforms. Many past studies have analyzed several factors (like journal impact
factor, open access, collaboration etc.) that may impact social media attention
of scholarly articles. However, there are no studies to analyze whether the
presence of social media plugin in a journal could result in higher social
media attention of articles published in the journal. This paper aims to bridge
this gap in knowledge by analyzing a sufficiently large-sized sample of 99,749
articles from 100 different journals. Results obtained show that journals that
have social media plugins integrated in their webpages get significantly higher
social media mentions and shares for their articles as compared to journals
that do not provide such plugins. Authors and readers visiting journal webpages
appear to be a major contributor to social media activity around articles
published in such journals. The results suggest that publishing houses should
actively provide social media plugin integration in their journal webpages to
increase social media visibility (altmetric impact) of their articles.
"
3032,"Wikipedia Citations: A comprehensive dataset of citations with
  identifiers extracted from English Wikipedia","  Wikipedia's contents are based on reliable and published sources. To this
date, little is known about what sources Wikipedia relies on, in part because
extracting citations and identifying cited sources is challenging. To close
this gap, we release Wikipedia Citations, a comprehensive dataset of citations
extracted from Wikipedia. A total of 29.3M citations were extracted from 6.1M
English Wikipedia articles as of May 2020, and classified as being to books,
journal articles or Web contents. We were thus able to extract 4.0M citations
to scholarly publications with known identifiers -- including DOI, PMC, PMID,
and ISBN -- and further labeled an extra 261K citations with DOIs from
Crossref. As a result, we find that 6.7% of Wikipedia articles cite at least
one journal article with an associated DOI. Scientific articles cited from
Wikipedia correspond to 3.5% of all articles with a DOI currently indexed in
the Web of Science. We release all our code to allow the community to extend
upon our work and update the dataset in the future.
"
3033,"The Case of Significant Variations in Gold-Green and Black Open Access:
  Evidence from Indian Research Output","  Open Access has emerged as an important movement worldwide during the last
decade. There are several initiatives now that persuade researchers to publish
in open access journals and to archive their pre- or post-print versions of
papers in repositories. Institutions and funding agencies are also promoting
ways to make research outputs available as open access. This paper looks at
open access levels and patterns in research output from India by
computationally analyzing research publication data obtained from Web of
Science for India for the last five years (2014-2018). The corresponding data
from other connected platforms -- Unpaywall and Sci-Hub -- are also obtained
and analyzed. The results obtained show that about 24% of research output from
India, during last five years, is available in legal forms of open access as
compared to world average of about 30%. More articles are available in gold
open access as compared to green and bronze. On the contrary, more than 90% of
the research output from India is available for free download in Sci-Hub. We
also found disciplinary differentiation in open access, but surprisingly these
patterns are different for gold-green and black open access forms. Sci-Hub
appears to be complementing the legal gold-green open access for less covered
disciplines in them. The central institutional repositories in India are found
to have low volume of research papers deposited.
"
3034,"Physics Letters B, Physical Review Letters and arXiv publications.
  Correlating PLB, PRL and arXiv articles for nuclear, particle and
  astro-physics","  When analyzing the content of Physics Letter B (PLB) web pages, one goal was
to separate articles from particle physics(HEP) and nuclear physics(NP). PLB
contains information about the subject area of an article: Astrophysics and
Cosmology, Experiments, Phenomenology or Theory. Those subject areas have been
used since 2004. To extend those areas to earlier dates and try to separate HEP
and NP publications, the idea was to use the information on the arXiv to
accomplish this, by correlating a publication in PLB with a publication in one
of the different arXiv repositories for particle, nuclear or astrophysics. The
arXiv articles go back to at least 1995. When building the correlation between
an arXiv article and each PLB article, it was found that many PLB articles do
not have an existing reference in an arXiv article. Given that, the same
analysis was performed using particle, nuclear and astro physics publications
in Physical Review Letters (PRL). A similar result was obtained, but for a much
shorter period of time. Finally, a title match was performed between the two
journals and the arXiv, to see whether the preprint version of the journal
article existed on the arXiv at all. They do, but without a journal reference.
"
3035,Trends in Cuban research output: publications and patents,"  Cuban science and technology are known for important achievements,
particularly in human healthcare and biotechnology. During the second half of
XX century, the country developed a system of scientific institutions to
address and solve major economical, cultural, social and health problems.
However, the economic crisis faced by the island during the last three decades
has had a major impact in Cuban scientific research. In addition to decreased
investment, the emigration of thousands of young as well as senior scientists
to other countries have had a major impact in Cuban research output. To date,
no systematic analysis regarding scientific publications, citations, or patents
granted to Cuban authors during this period, are available. Here, an analysis
of Cuban scientific production since 1970, with an especial focus on the last
three decades (1990 - 2019), is provided. All national metrics are compared
with other countries, emphasizing those from Latin America. Preliminary results
show that Cuban scientific publications are increased at a lower rate
(two-fold) compare with several Latin American countries (five-fold average).
In addition, since 2014 the annual number of Cuban scientific publications is
decreasing. Finally, the analysis shows that most young Cuban authors with the
higher index of citations (1990-2019) are working abroad. All the data and the
code related to this study are open and can be found in GitHub
(https://github.com/ypriverol/cubascience).
"
3036,"For a ""computational"" study of the intellectuals satellites","  The present paper wishes to extend the theoretical reflections that we're
currently working on: the ""satellite intellectuals"". This concept points a new
""repertoire"" of intellectuals that critics have not studied thoroughly and that
is emerging with the amplification of the archives' digitalization of archives
and their digital edition. Today, it is necessary to rethink and reevaluate the
criteria we used to apply to define, integrate or exclude works and authors
from, or in, a ""canon"". At the same time, it suggests new methods to process
and deal with some concepts of computer tools and science.
"
3037,"Understanding the temporal evolution of COVID-19 research through
  machine learning and natural language processing","  The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the
severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been
continuously affecting human lives and communities around the world in many
ways, from cities under lockdown to new social experiences. Although in most
cases COVID-19 results in mild illness, it has drawn global attention due to
the extremely contagious nature of SARS-CoV-2. Governments and healthcare
professionals, along with people and society as a whole, have taken any
measures to break the chain of transition and flatten the epidemic curve. In
this study, we used multiple data sources, i.e., PubMed and ArXiv, and built
several machine learning models to characterize the landscape of current
COVID-19 research by identifying the latent topics and analyzing the temporal
evolution of the extracted research themes, publications similarity, and
sentiments, within the time-frame of January- May 2020. Our findings confirm
the types of research available in PubMed and ArXiv differ significantly, with
the former exhibiting greater diversity in terms of COVID-19 related issues and
the latter focusing more on intelligent systems/tools to predict/diagnose
COVID-19. The special attention of the research community to the high-risk
groups and people with complications was also confirmed.
"
3038,"Anecdotal Survey of Variations in Path Stroking among Real-world
  Implementations","  Stroking a path is one of the two basic rendering operations in vector
graphics standards (e.g., PostScript, PDF, SVG). We survey path stroking
rendering results from real-world software implementations of path stroking for
anecdotal evidence that such implementations are prone to rendering variances.
While our survey is limited and informal, the rendering results we gathered
indicate widespread rendering variations for simple-but-problematic stroked
paths first identified decades ago. We conclude that creators of vector
graphics content would benefit from a mathematically grounded standardization
for how a stroked path should be rasterized.
"
3039,On the Programmatic Generation of Reproducible Documents,"  Reproducible document standards, like R Markdown, facilitate the programmatic
creation of documents whose content is itself programmatically generated. While
these documents are generally not complete in the sense that they will not
include prose content, generated by an author to provide context, a narrative,
etc., programmatic generation can provide substantial efficiencies for
structuring and constructing documents. This paper explores the programmatic
generation of reproducible by distinguishing components than can be created by
computational means from those requiring human-generated prose, providing
guidelines for the generation of these documents, and identifying a use case in
clinical trial reporting. These concepts and use case are illustrated through
the listdown package for the R programming environment, which is is currently
available on the Comprehensive R Archive Network (CRAN).
"
3040,Open Access all you wanted to know and never dared to ask,"  This editorial presents the various forms of open access, discusses their
pros and cons from the perspective of the Journal of Object Technology and its
editors in chiefs, and illustrates how JOT implements a platinum open access
model. The regular reader will also notice that this editorial features a new
template for the journal that will be used from now on.
"
3041,"Identification, Tracking and Impact: Understanding the trade secret of
  catchphrases","  Understanding the topical evolution in industrial innovation is a challenging
problem. With the advancement in the digital repositories in the form of patent
documents, it is becoming increasingly more feasible to understand the
innovation secrets -- ""catchphrases"" of organizations. However, searching and
understanding this enormous textual information is a natural bottleneck. In
this paper, we propose an unsupervised method for the extraction of
catchphrases from the abstracts of patents granted by the U.S. Patent and
Trademark Office over the years. Our proposed system achieves substantial
improvement, both in terms of precision and recall, against state-of-the-art
techniques. As a second objective, we conduct an extensive empirical study to
understand the temporal evolution of the catchphrases across various
organizations. We also show how the overall innovation evolution in the form of
introduction of newer catchphrases in an organization's patents correlates with
the future citations received by the patents filed by that organization. Our
code and data sets will be placed in the public domain soon.
"
3042,"Large Scale Subject Category Classification of Scholarly Papers with
  Deep Attentive Neural Networks","  Subject categories of scholarly papers generally refer to the knowledge
domain(s) to which the papers belong, examples being computer science or
physics. Subject category information can be used for building faceted search
for digital library search engines. This can significantly assist users in
narrowing down their search space of relevant documents. Unfortunately, many
academic papers do not have such information as part of their metadata.
Existing methods for solving this task usually focus on unsupervised learning
that often relies on citation networks. However, a complete list of papers
citing the current paper may not be readily available. In particular, new
papers that have few or no citations cannot be classified using such methods.
Here, we propose a deep attentive neural network (DANN) that classifies
scholarly papers using only their abstracts. The network is trained using 9
million abstracts from Web of Science (WoS). We also use the WoS schema that
covers 104 subject categories. The proposed network consists of two
bi-directional recurrent neural networks followed by an attention layer. We
compare our model against baselines by varying the architecture and text
representation. Our best model achieves micro-F1 measure of 0.76 with F1 of
individual subject categories ranging from 0.50-0.95. The results showed the
importance of retraining word embedding models to maximize the vocabulary
overlap and the effectiveness of the attention mechanism. The combination of
word vectors with TFIDF outperforms character and sentence level embedding
models. We discuss imbalanced samples and overlapping categories and suggest
possible strategies for mitigation. We also determine the subject category
distribution in CiteSeerX by classifying a random sample of one million
academic papers.
"
3043,"How to Investigate the Historical Roots and Evolution of Research Fields
  in China? A Case Study on iMetrics Using RootCite","  This paper aimed to provide an approach to investigate the historical roots
and evolution of research fields in China by extending the reference
publication year spectroscopy (RPYS). RootCite, an open source software accepts
raw data from both the Web of Science and the China Social Science Citation
Index (CSSCI), was developed using python. We took iMetrics in China as the
research case. 5,141 Chinese iMetrics related publications with 73,376
non-distinct cited references (CR) collected from the CSSCI were analyzed using
RootCite. The results showed that the first CR in the field can be dated back
to 1882 and written in English; but the majority (64.2%) of the CR in the field
were Chinese publications. 17 peaks referring to 18 seminal works (13 in
English and 5 in Chinese) were located during the period from 1900 to 2017. The
field shared the same roots with that in the English world but has its own
characteristics, and it was then shaped by contributions from both the English
world and China. The five Chinese works have played irreplaceable and positive
roles in the historical evolutionary path of the field, which should not be
ignored, especially for the evolution of the field. This research demonstrated
how RootCite aided the task of identifying the origin and evolution of research
fields in China, which could be valuable for extending RPYS for countries with
other languages.
"
3044,Finding Scientific Communities In Citation Graphs: Convergent Clustering,"  Understanding the nature and organization of scientific communities is of
broad interest. The `Invisible College' is a historical metaphor for one such
type of community and the search for such `colleges' can be framed as the
detection and analysis of small groups of scientists working on problems of
common interests. Case studies have previously been conducted on individual
communities with respect to their scientific and social behavior. In this
study, we introduce, a new and scalable community finding approach.
Supplemented by expert assessment, we use the convergence of two different
clustering methods to select article clusters generated from over two million
articles from the field of immunology spanning an eleven year period with
relevant cluster quality indicators for evaluation. Finally, we identify author
communities defined by these clusters. A sample of the article clusters
produced by this pipeline was reviewed by experts, and shows strong thematic
relatedness, suggesting that the inferred author communities may represent
valid communities of practice. These findings suggest that such convergent
approaches may be useful in the future.
"
3045,"Construction and Usage of a Human Body Common Coordinate Framework
  Comprising Clinical, Semantic, and Spatial Ontologies","  The National Institutes of Health's (NIH) Human Biomolecular Atlas Program
(HuBMAP) aims to create a comprehensive high-resolution atlas of all the cells
in the healthy human body. Multiple laboratories across the United States are
collecting tissue specimens from different organs of donors who vary in sex,
age, and body size. Integrating and harmonizing the data derived from these
samples and 'mapping' them into a common three-dimensional (3D) space is a
major challenge. The key to making this possible is a 'Common Coordinate
Framework' (CCF), which provides a semantically annotated, 3D reference system
for the entire body. The CCF enables contributors to HuBMAP to 'register'
specimens and datasets within a common spatial reference system, and it
supports a standardized way to query and 'explore' data in a spatially and
semantically explicit manner. [...] This paper describes the construction and
usage of a CCF for the human body and its reference implementation in HuBMAP.
The CCF consists of (1) a CCF Clinical Ontology, which provides metadata about
the specimen and donor (the 'who'); (2) a CCF Semantic Ontology, which
describes 'what' part of the body a sample came from and details anatomical
structures, cell types, and biomarkers (ASCT+B); and (3) a CCF Spatial
Ontology, which indicates 'where' a tissue sample is located in a 3D coordinate
system. An initial version of all three CCF ontologies has been implemented for
the first HuBMAP Portal release. It was successfully used by Tissue Mapping
Centers to semantically annotate and spatially register 48 kidney and spleen
tissue blocks. The blocks can be queried and explored in their clinical,
semantic, and spatial context via the CCF user interface in the HuBMAP Portal.
"
3046,"A Hybrid Adaptive Educational eLearning Project based on Ontologies
  Matching and Recommendation System","  The implementation of teaching interventions in learning needs has received
considerable attention, as the provision of the same educational conditions to
all students, is pedagogically ineffective. In contrast, more effectively
considered the pedagogical strategies that adapt to the real individual skills
of the students. An important innovation in this direction is the Adaptive
Educational Systems (AES) that support automatic modeling study and adjust the
teaching content on educational needs and students' skills. Effective
utilization of these educational approaches can be enhanced with Artificial
Intelligence (AI) technologies in order to the substantive content of the web
acquires structure and the published information is perceived by the search
engines. This study proposes a novel Adaptive Educational eLearning System
(AEeLS) that has the capacity to gather and analyze data from learning
repositories and to adapt these to the educational curriculum according to the
student skills and experience. It is a novel hybrid machine learning system
that combines a Semi-Supervised Classification method for ontology matching and
a Recommendation Mechanism that uses a hybrid method from neighborhood-based
collaborative and content-based filtering techniques, in order to provide a
personalized educational environment for each student.
"
3047,"Topics as Clusters of Citation Links to Highly Cited Sources: The Case
  of Research on International Relations","  Following Henry Small in his approach to co-citation analysis, highly cited
sources are seen as concept symbols of research fronts. But instead of co-cited
sources I cluster citation links, which are the thematically least heterogenous
elements in bibliometric studies. To obtain clusters representing topics
characterised by concepts I restrict link clustering to citation links to
highly cited sources. Clusters of citation links between papers in a
political-science subfield (International Relations) and 300 of their sources
most cited in the period 2006-2015 are constructed by a local memetic
algorithm. It finds local minima in a cost landscape corresponding to clusters,
which can overlap each other pervasively. The clusters obtained are well
separated from the rest of the network but can have suboptimal cohesion.
Cohesive cores of topics are found by applying an algorithm that constructs
core-periphery structures in link sets. In this methodological paper I only
discuss some first clustering results for the second half of the 10-years
period.
"
3048,COVID-19 therapy target discovery with context-aware literature mining,"  The abundance of literature related to the widespread COVID-19 pandemic is
beyond manual inspection of a single expert. Development of systems, capable of
automatically processing tens of thousands of scientific publications with the
aim to enrich existing empirical evidence with literature-based associations is
challenging and relevant. We propose a system for contextualization of
empirical expression data by approximating relations between entities, for
which representations were learned from one of the largest COVID-19-related
literature corpora. In order to exploit a larger scientific context by transfer
learning, we propose a novel embedding generation technique that leverages
SciBERT language model pretrained on a large multi-domain corpus of scientific
publications and fine-tuned for domain adaptation on the CORD-19 dataset. The
conducted manual evaluation by the medical expert and the quantitative
evaluation based on therapy targets identified in the related work suggest that
the proposed method can be successfully employed for COVID-19 therapy target
discovery and that it outperforms the baseline FastText method by a large
margin.
"
3049,"Western ideological homogeneity in entrepreneurial finance research:
  Evidence from highly cited publications","  Entrepreneurs play crucial roles in global sustainable development, but
limited financial resources constrain their performance and survival rate.
Entrepreneurial finance discipline is, therefore, born to explore the
connection between finance and entrepreneurship. Despite the global presence of
entrepreneurship, the literature of entrepreneurial finance is suspected to be
Western ideologically homogenous. Thus, the objective of this study is to
examine the existence of Western ideological homogeneity in entrepreneurial
finance literature. Employing the mindsponge mechanism and bibliometric
analyses (Y-index and social structure), we analyze 412 highly cited
publications extracted from Web of Science database and find Western
ideological dominance as well as weak tolerance towards heterogeneity in the
set of core ideologies of entrepreneurial finance. These results are consistent
across author-, institution-, and country-levels, which reveals strong evidence
for the existence of Western ideological homogeneity in the field. We recommend
editors, reviewers, and authors to have proactive actions to diversify research
topics and enhancing knowledge exchange to avoid the shortfalls of ideological
homogeneity. Moreover, the synthesis of mindsponge mechanism and bibliometric
analyses are suggested as a possible way to evaluate the state of ideological
diversity in other scientific disciplines.
"
3050,MementoEmbed and Raintale for Web Archive Storytelling,"  For traditional library collections, archivists can select a representative
sample from a collection and display it in a featured physical or digital
library space. Web archive collections may consist of thousands of archived
pages, or mementos. How should an archivist display this sample to drive
visitors to their collection? Search engines and social media platforms often
represent web pages as cards consisting of text snippets, titles, and images.
Web storytelling is a popular method for grouping these cards in order to
summarize a topic. Unfortunately, social media platforms are not archive-aware
and fail to consistently create a good experience for mementos. They also allow
no UI alterations for their cards. Thus, we created MementoEmbed to generate
cards for individual mementos and Raintale for creating entire stories that
archivists can export to a variety of formats.
"
3051,SHARI -- An Integration of Tools to Visualize the Story of the Day,"  Tools such as Google News and Flipboard exist to convey daily news, but what
about the past? In this paper, we describe how to combine several existing
tools with web archive holdings to perform news analysis and visualization of
the ""biggest story"" for a given date. StoryGraph clusters news articles
together to identify a common news story. Hypercane leverages ArchiveNow to
store URLs produced by StoryGraph in web archives. Hypercane analyzes these
URLs to identify the most common terms, entities, and highest quality images
for social media storytelling. Raintale then uses the output of these tools to
produce a visualization of the news story for a given day. We name this process
SHARI (StoryGraph Hypercane ArchiveNow Raintale Integration).
"
3052,"Contextual Document Similarity for Content-based Literature Recommender
  Systems","  To cope with the ever-growing information overload, an increasing number of
digital libraries employ content-based recommender systems. These systems
traditionally recommend related documents with the help of similarity measures.
However, current document similarity measures simply distinguish between
similar and dissimilar documents. This simplification is especially crucial for
extensive documents, which cover various facets of a topic and are often found
in digital libraries. Still, these similarity measures neglect to what facet
the similarity relates. Therefore, the context of the similarity remains
ill-defined. In this doctoral thesis, we explore contextual document similarity
measures, i.e., methods that determine document similarity as a triple of two
documents and the context of their similarity. The context is here a further
specification of the similarity. For example, in the scientific domain,
research papers can be similar with respect to their background, methodology,
or findings. The measurement of similarity in regards to one or more given
contexts will enhance recommender systems. Namely, users will be able to
explore document collections by formulating queries in terms of documents and
their contextual similarities. Thus, our research objective is the development
and evaluation of a recommender system based on contextual similarity. The
underlying techniques will apply established similarity measures and as well as
neural approaches while utilizing semantic features obtained from links between
documents and their text.
"
3053,Elsevier OA CC-By Corpus,"  We introduce the Elsevier OA CC-BY corpus. This is the first open corpus of
Scientific Research papers which has a representative sample from across
scientific disciplines. This corpus not only includes the full text of the
article, but also the metadata of the documents, along with the bibliographic
information for each reference.
"
3054,"Independent publishers and social networks in the 21st century: the
  balance of power in the transatlantic Spanish-language book market","  The present paper uses Twitter to analyze the current state of the worldwide,
Spanish-language, independent publishing market. The main purposes are to
determine whether certain Latin American Spanish-language independent
publishers function as gatekeepers of World Literature and to analyze the
geopolitical structure of this global market, addressing both the
Europe-America dialectic and neocolonial practices. After selecting the sample
of publishers, we conducted a search for their Twitter profiles and located
131; we then downloaded data from the corresponding Twitter APIs. Finally, we
applied social network analysis to study the presence of and interaction
between our sample of independent publishers on this social media. Our results
provide data-based evidence supporting the hypothesis of some literary critics
who suggest that in Latin America, certain publishers act as gatekeepers to the
mainstream book market. Therefore, Twitter could be considered a valid source
of information to address the independent book market in Spanish. By extension,
this approach could be applied to other cultural industries in which small and
medium-sized agents develop a digital presence in social media. This paper
combines social network analysis and literary criticism to provide new evidence
about the Spanish-language book market. It helps validate the aforementioned
hypothesis, proposed by literary critics, and opens up new paths along which to
pursue an interpretative, comparative analysis.
"
3055,"Brain Drain and Brain Gain in Russia: Analyzing International Migration
  of Researchers by Discipline using Scopus Bibliometric Data 1996-2020","  We study international mobility in academia with a focus on migration of
researchers to and from Russia. Using all Scopus publications from 1996 to
2020, we analyze bibliometric data from over half a million researchers who
have published with a Russian affiliation address at some point in their
careers. Migration of researchers is observed through the changes in their
affiliation addresses. For the first time, we analyze origins and destinations
of migrant researchers with respect to their fields and performance and compute
net migration rates based on incoming and outgoing flows. Our results indicate
that while Russia has been a donor country in the late 1990s and early 2000s,
it has experienced a relatively symmetric circulation of researchers in more
recent years. Using subject categories of publications, we quantify the impact
of migration on each field of scholarship. Our analysis shows that Russia has
suffered a net loss in almost all disciplines and more so in neuroscience,
decision sciences, dentistry, biochemistry, and mathematics. For economics and
environmental science, there is a relatively balanced circulation of
researchers to and from Russia. Our substantive results reveal new aspects of
international mobility in academia and its impact on a national science system
which speak directly to policy development. Methodologically, our new approach
of handling big data can be adopted as a framework of analysis for studying
scholarly migration in other countries.
"
3056,Transistors: A Network Science-Based Historical Perspective,"  The development of modern electronics was to a large extent related to the
advent and popularization of bipolar junction technology. The present work
applies science of science concepts and methodologies in order to develop a
relatively systematic, quantitative study of the development of electronics
from a bipolar-junction-centered perspective. First, we searched the adopted
dataset (Microsoft Academic Graph) for entries related to ""bipolar junction
transistor"". Community detection was then applied in order to derive sub-areas,
which were tentatively labeled into 10 overall groups. This modular graph was
then studied from several perspectives, including topological measurements and
time evolution. A number of interesting results are reported, including a good
level of thematic coherence within each identified area, as well as the
identification of distinct periods along the time evolution including the onset
and coming of age of bipolater junction technology and related areas. A
particularly surprising result was the verification of stable interrelationship
between the identified areas along time.
"
3057,"Navigating the landscape of COVID-19 research through literature
  analysis: A bird's eye view","  Timely access to accurate scientific literature in the battle with the
ongoing COVID-19 pandemic is critical. This unprecedented public health risk
has motivated research towards understanding the disease in general,
identifying drugs to treat the disease, developing potential vaccines, etc.
This has given rise to a rapidly growing body of literature that doubles in
number of publications every 20 days as of May 2020. Providing medical
professionals with means to quickly analyze the literature and discover growing
areas of knowledge is necessary for addressing their question and information
needs.
  In this study we analyze the LitCovid collection, 13,369 COVID-19 related
articles found in PubMed as of May 15th, 2020 with the purpose of examining the
landscape of literature and presenting it in a format that facilitates
information navigation and understanding. We do that by applying
state-of-the-art named entity recognition, classification, clustering and other
NLP techniques. By applying NER tools, we capture relevant bioentities (such as
diseases, internal body organs, etc.) and assess the strength of their
relationship with COVID-19 by the extent they are discussed in the corpus. We
also collect a variety of symptoms and co-morbidities discussed in reference to
COVID-19. Our clustering algorithm identifies topics represented by groups of
related terms, and computes clusters corresponding to documents associated with
the topic terms. Among the topics we observe several that persist through the
duration of multiple weeks and have numerous associated documents, as well
several that appear as emerging topics with fewer documents. All the tools and
data are publicly available, and this framework can be applied to any
literature collection. Taken together, these analyses produce a comprehensive,
synthesized view of COVID-19 research to facilitate knowledge discovery from
literature.
"
3058,Prediction Methods and Applications in the Science of Science: A Survey,"  Science of science has become a popular topic that attracts great attentions
from the research community. The development of data analytics technologies and
the readily available scholarly data enable the exploration of data-driven
prediction, which plays a pivotal role in finding the trend of scientific
impact. In this paper, we analyse methods and applications in data-driven
prediction in the science of science, and discuss their significance. First, we
introduce the background and review the current state of the science of
science. Second, we review data-driven prediction based on paper citation
count, and investigate research issues in this area. Then, we discuss methods
to predict scholar impact, and we analyse different approaches to promote the
scholarly collaboration in the collaboration network. This paper also discusses
open issues and existing challenges, and suggests potential research
directions.
"
3059,The Role of Positive and Negative Citations in Scientific Evaluation,"  Quantifying the impact of scientific papers objectively is crucial for
research output assessment, which subsequently affects institution and country
rankings, research funding allocations, academic recruitment and
national/international scientific priorities. While most of the assessment
schemes based on publication citations may potentially be manipulated through
negative citations, in this study, we explore Conflict of Interest (COI)
relationships and discover negative citations and subsequently weaken the
associated citation strength. PANDORA (Positive And Negative COI- Distinguished
Objective Rank Algorithm) has been developed, which captures the positive and
negative COI, together with the positive and negative suspected COI
relationships. In order to alleviate the influence caused by negative COI
relationship, collaboration times, collaboration time span, citation times and
citation time span are employed to determine the citing strength; while for
positive COI relationship, we regard it as normal citation relationship.
Furthermore, we calculate the impact of scholarly papers by PageRank and HITS
algorithms, based on a credit allocation algorithm which is utilized to assess
the impact of institutions fairly and objectively. Experiments are conducted on
the publication dataset from American Physical Society (APS) dataset, and the
results demonstrate that our method significantly outperforms the current
solutions in Recommendation Intensity of list R at top-K and Spearman's rank
correlation coefficient at top-K.
"
3060,"Quantifying the Impact of Scholarly Papers Based on Higher-Order
  Weighted Citations","  Quantifying the impact of a scholarly paper is of great significance, yet the
effect of geographical distance of cited papers has not been explored. In this
paper, we examine 30,596 papers published in Physical Review C, and identify
the relationship between citations and geographical distances between author
affiliations. Subsequently, a relative citation weight is applied to assess the
impact of a scholarly paper. A higher-order weighted quantum PageRank algorithm
is also developed to address the behavior of multiple step citation flow.
Capturing the citation dynamics with higher-order dependencies reveals the
actual impact of papers, including necessary self-citations that are sometimes
excluded in prior studies. Quantum PageRank is utilized in this paper to help
differentiating nodes whose PageRank values are identical.
"
3061,An Overview on Evaluating and Predicting Scholarly Article Impact,"  Scholarly article impact reflects the significance of academic output
recognised by academic peers, and it often plays a crucial role in assessing
the scientific achievements of researchers, teams, institutions and countries.
It is also used for addressing various needs in the academic and scientific
arena, such as recruitment decisions, promotions, and funding allocations. This
article provides a comprehensive review of recent progresses related to article
impact assessment and prediction. The~review starts by sharing some insight
into the article impact research and outlines current research status. Some
core methods and recent progress are presented to outline how article impact
metrics and prediction have evolved to consider integrating multiple networks.
Key techniques, including statistical analysis, machine learning, data mining
and network science, are discussed. In particular, we highlight important
applications of each technique in article impact research. Subsequently, we
discuss the open issues and challenges of article impact research. At the same
time, this review points out some important research directions, including
article impact evaluation by considering Conflict of Interest, time and
location information, various distributions of scholarly entities, and rising
stars.
"
3062,"Waiving Article Processing Charges for Least Developed Countries. A
  Brick Stone of a Large-scale Open Access Transformation","  This article investigates the question, if it is economically feasible for a
large publishing house to waive article processing charges for the group of 47
so called least developed countries (LDC). As an example Springer-Nature is
selected. The analysis is based on the Web of Science, OpenAPC and the Jisc
collections Springer compact journal list. As a result, it estimates an average
yearly publication output of 520 publications (or a share of 0,26% of the
worldwide publication output in Springer-Nature journals) for the LDC country
group. The loss of revenues for Springer-Nature would be 1,1 million $ if a
waiver would be applied for all of these countries. Given that money is
indispensable for development in the case of LDC (e.g. life expectancy, health,
education), it is not only desirable but also possible in economic terms for a
publisher like Springer-Nature to waive APCs for these countries without much
loss in revenues.
"
3063,"Comprehensiveness of Archives: A Modern AI-enabled Approach to Build
  Comprehensive Shared Cultural Heritage","  Archives play a crucial role in the construction and advancement of society.
Humans place a great deal of trust in archives and depend on them to craft
public policies and to preserve languages, cultures, self-identity, views and
values. Yet, there are certain voices and viewpoints that remain elusive in the
current processes deployed in the classification and discoverability of records
and archives.
  In this paper, we explore the ramifications and effects of centralized, due
process archival systems on marginalized communities. There is strong evidence
to prove the need for progressive design and technological innovation while in
the pursuit of comprehensiveness, equity and justice. Intentionality and
comprehensiveness is our greatest opportunity when it comes to improving
archival practices and for the advancement and thrive-ability of societies at
large today. Intentionality and comprehensiveness is achievable with the
support of technology and the Information Age we live in today. Reopening,
questioning and/or purposefully including others voices in archival processes
is the intention we present in our paper.
  We provide examples of marginalized communities who continue to lead
""community archive"" movements in efforts to reclaim and protect their cultural
identity, knowledge, views and futures. In conclusion, we offer design and
AI-dominant technological considerations worth further investigation in efforts
to bridge systemic gaps and build robust archival processes.
"
3064,The decline of astronomical research in Venezuela,"  During the last 15 years the number of astronomy-related papers published by
scientists in Venezuela has been continuously decreasing, mainly due to
emigration. If rapid corrective actions are not implemented, Venezuelan
astronomy could disappear.
"
3065,"Measure the Impact of Institution and Paper via Institution-citation
  Network","  This paper investigates the impact of institutes and papers over time based
on the heterogeneous institution-citation network. A new model, IPRank, is
introduced to measure the impact of institution and paper simultaneously. This
model utilises the heterogeneous structural measure method to unveil the impact
of institution and paper, reflecting the effects of citation, institution, and
structural measure. To evaluate the performance, the model first constructs a
heterogeneous institution-citation network based on the American Physical
Society (APS) dataset. Subsequently, PageRank is used to quantify the impact of
institution and paper. Finally, impacts of same institution are merged, and the
ranking of institutions and papers is calculated. Experimental results show
that the IPRank model better identifies universities that host Nobel Prize
laureates, demonstrating that the proposed technique well reflects impactful
research.
"
3066,"Author Impact: Evaluations, Predictions, and Challenges","  Author impact evaluation and prediction play a key role in determining
rewards, funding, and promotion. In this paper, we first introduce the
background of author impact evaluation and prediction. Then, we review recent
developments of author impact evaluation, including data collection, data
pre-processing, data analysis, feature selection, algorithm design, and
algorithm evaluation. Thirdly, we provide an in-depth literature review on
author impact predictive models and common evaluation metrics. Finally, we look
into the representative research issues, including author impact inflation,
unified evaluation standards, academic success gene, identification of the
origins of hot streaks, and higher-order academic networks analysis. This paper
should help the researchers obtain a broader understanding in author impact
evaluation and prediction, and provides future research directions.
"
3067,Quantifying Success in Science: An Overview,"  Quantifying success in science plays a key role in guiding funding
allocations, recruitment decisions, and rewards. Recently, a significant amount
of progresses have been made towards quantifying success in science. This lack
of detailed analysis and summary continues a practical issue. The literature
reports the factors influencing scholarly impact and evaluation methods and
indices aimed at overcoming this crucial weakness. We focus on categorizing and
reviewing the current development on evaluation indices of scholarly impact,
including paper impact, scholar impact, and journal impact. Besides, we
summarize the issues of existing evaluation methods and indices, investigate
the open issues and challenges, and provide possible solutions, including the
pattern of collaboration impact, unified evaluation standards, implicit success
factor mining, dynamic academic network embedding, and scholarly impact
inflation. This paper should help the researchers obtaining a broader
understanding of quantifying success in science, and identifying some potential
research directions.
"
3068,"Scientific Article Recommendation: Exploiting Common Author Relations
  and Historical Preferences","  Scientific article recommender systems are playing an increasingly important
role for researchers in retrieving scientific articles of interest in the
coming era of big scholarly data. Most existing studies have designed unified
methods for all target researchers and hence the same algorithms are run to
generate recommendations for all researchers no matter which situations they
are in. However, different researchers may have their own features and there
might be corresponding methods for them resulting in better recommendations. In
this paper, we propose a novel recommendation method which incorporates
information on common author relations between articles (i.e., two articles
with the same author(s)). The rationale underlying our method is that
researchers often search articles published by the same author(s). Since not
all researchers have such author-based search patterns, we present two
features, which are defined based on information about pairwise articles with
common author relations and frequently appeared authors, to determine target
researchers for recommendation. Extensive experiments we performed on a
real-world dataset demonstrate that the defined features are effective to
determine relevant target researchers and the proposed method generates more
accurate recommendations for relevant researchers when compared to a Baseline
method.
"
3069,"Towards a more realistic citation model: The key role of research team
  sizes","  We propose a new citation model which builds on the existing models that
explicitly or implicitly include ""direct"" and ""indirect"" (learning about a
cited paper's existence from references in another paper) citation mechanisms.
Our model departs from the usual, unrealistic assumption of uniform probability
of direct citation, in which initial differences in citation arise purely
randomly. Instead, we demonstrate that a two-mechanism model in which the
probability of direct citation is proportional to the number of authors on a
paper (team size) is able to reproduce the empirical citation distributions of
articles published in the field of astronomy remarkably well, and at different
points in time. Interpretation of our model is that the intrinsic citation
capacity, and hence the initial visibility of a paper, will be enhanced when
more people are intimately familiar with some work, favoring papers from larger
teams. While the intrinsic citation capacity cannot depend only on the team
size, our model demonstrates that it must be to some degree correlated with it,
and distributed in a similar way, i.e., having a power-law tail. Consequently,
our team-size model qualitatively explains the existence of a correlation
between the number of citations and the number of authors on a paper.
"
3070,"Nature, Science, and PNAS -- Disciplinary profiles and impact","  Nature, Science, and PNAS are the three most prestigious general-science
journals, and Nature and Science are among the most influential journals
overall, based on the journal Impact Factor (IF). In this paper we perform
automatic classification of ~50,000 articles in these journals (published in
the period 2005-2015) into 14 broad areas, to explore disciplinary profiles and
to determine their field-specific IFs. We find that in all three journals the
articles from Bioscience, Astronomy, and Geosciences are over-represented, with
other areas being under-represented, some of them severely. Discipline-specific
IFs in these journals vary greatly, for example, between 18 and 46 for Nature.
We find that the areas that have the highest disciplinary IFs are not the ones
that contribute the most articles. We also find that publishing articles in
these three journals brings prestige for articles in all areas, but at
different levels, the least being for Astronomy. Comparing field-specific IFs
of Nature, Science and PNAS to other top journals in six largest areas
(Bioscience, Medicine, Geosciences, Physics, Astronomy, and Chemistry) these
three journals are always among the top seven journals, with Nature being at
the very top for all fields except in Medicine.
"
3071,Predicting the Citations of Scholarly Paper,"  Citation prediction of scholarly papers is of great significance in guiding
funding allocations, recruitment decisions, and rewards. However, little is
known about how citation patterns evolve over time. By exploring the inherent
involution property in scholarly paper citation, we introduce the Paper
Potential Index (PPI) model based on four factors: inherent quality of
scholarly paper, scholarly paper impact decaying over time, early citations,
and early citers' impact. In addition, by analyzing factors that drive citation
growth, we propose a multi-feature model for impact prediction. Experimental
results demonstrate that the two models improve the accuracy in predicting
scholarly paper citations. Compared to the multi-feature model, the PPI model
yields superior predictive performance in terms of range-normalized RMSE. The
PPI model better interprets the changes in citation, without the need to adjust
parameters. Compared to the PPI model, the multi-feature model performs better
prediction in terms of Mean Absolute Percentage Error and Accuracy; however,
their predictive performance is more dependent on the parameter adjustment.
"
3072,"Prestige of scholarly book publishers: an investigation into criteria,
  processes, and practices across countries","  Numerous national research assessment policies set the goal of promoting
""excellence"" and incentivise scholars to publish their research in the most
prestigious journals or with the most prestigious book publishers. We
investigate the practicalities of the assessment of book outputs based on the
prestige of book publishers (Denmark, Finland, Flanders, Lithuania, Norway).
Additionally, we test whether such assessments are transparent and yield
consistent results. We show inconsistencies in the assessment of publishers,
such as the same publisher being ranked as prestigious and not so prestigious
in different countries or in different years in the same country. Likewise, we
find that verification of compliance with the mandatory prerequisites is not
always possible because of the lack of transparency. Our findings raise doubts
about whether the assessment of books based on a judgement about their
publisher yields acceptable outcomes. Currently used rankings of publishers
focus on evaluating the gatekeeping role of publishers but do not assess their
dissemination role. Our suggestion for future research is to develop approaches
for assessing books which consider both quality control and the distribution of
books (and their metadata) as measured by the importance of communication
between researchers. That means that publishers should be transparent about the
services they deliver in both areas, preferably at the level of individual
books, so that there is no need to rely on general information about
publishers.
"
3073,"Berlin: A Quantitative View of the Structure of Institutional Scientific
  Collaborations","  This paper examines the structure of scientific collaborations in a large
European metropolitan area. It aims to identify strategic coalitions among
organizations in Berlin as a specific case with high institutional and sectoral
diversity. By adopting a global, regional and organization based approach we
provide a quantitative, exploratory and macro view of this diversity. We use
publications data with at least one organization located in Berlin from
1996-2017. We further investigate four members of the Berlin University
Alliance (BUA) through their self-represented research profiles comparing it
with empirical results of OECD disciplines. Using a bipartite network modeling
framework, we are able to move beyond the uncontested trend towards team
science and increasing internationalization. Our results show that BUA members
shape the structure of scientific collaborations in the region. However, they
are not collaborating cohesively in all disciplines. Larger divides exist in
some disciplines e.g., Agricultural Sciences and Humanities. Only Medical and
Health Sciences have cohesive intraregional collaborations which signals the
success of regional cooperation established in 2003. We explain possible
underlying factors shaping the observed trends and sectoral and intra-regional
groupings. A major methodological contribution of this paper is evaluating
coverage and accuracy of different organization name disambiguation techniques.
"
3074,"Understanding the Advisor-advisee Relationship via Scholarly Data
  Analysis","  Advisor-advisee relationship is important in academic networks due to its
universality and necessity. Despite the increasing desire to analyze the career
of newcomers, however, the outcomes of different collaboration patterns between
advisors and advisees remain unknown. The purpose of this paper is to find out
the correlation between advisors' academic characteristics and advisees'
academic performance in Computer Science. Employing both quantitative and
qualitative analysis, we find that with the increase of advisors' academic age,
advisees' performance experiences an initial growth, follows a sustaining
stage, and finally ends up with a declining trend. We also discover the
phenomenon that accomplished advisors can bring up skilled advisees. We explore
the conclusion from two aspects: (1) Advisees mentored by advisors with high
academic level have better academic performance than the rest; (2) Advisors
with high academic level can raise their advisees' h-index ranking. This work
provides new insights on promoting our understanding of the relationship
between advisors' academic characteristics and advisees' performance, as well
as on advisor choosing.
"
3075,PRINCIPIA: a Decentralized Peer-Review Ecosystem,"  Peer review is a cornerstone of modern scientific endeavor. However, there is
growing consensus that several limitations of the current peer review system,
from lack of incentives to reviewers to lack of transparency, risks to
undermine its benefits. Here, we introduce the PRINCIPIA
(http://www.principia.network/) framework for peer-review of scientific outputs
(e.g., papers, grant proposals or patents). The framework allows key players of
the scientific ecosystem -- including existing publishing groups -- to create
and manage peer-reviewed journals, by building a free market for reviews and
publications. PRINCIPIA's referees are transparently rewarded according to
their efforts and the quality of their reviews. PRINCIPIA also naturally allows
to recognize the prestige of users and journals, with an intrinsic reputation
system that does not depend on third-parties. PRINCIPIA re-balances the power
between researchers and publishers, stimulates valuable assessments from
referees, favors a fair competition between journals, and reduces the costs to
access research output and to publish.
"
3076,"MetaMetaZipf. What do analyses of city size distributions have in
  common?","  In this article, I conduct a textual and contextual analysis of the empirical
literature on Zipf's law for cities. Building on previous meta-analysis
material openly available, I collect full texts and bibliographies of 66
scientific articles published in English and construct similarity networks of
the terms they use as well as of the references and disciplines they cite. I
use these networks as explanatory variables in a model of the similarity
network of the distribution of Zipf estimates reported in the 66 articles. I
find that the proximity in words frequently used by authors correlates
positively with their tendency to report similar values and dispersion of Zipf
estimates. The reference framework of articles also plays a role, as articles
which cite similar references tend to report similar average values of Zipf
estimates. As a complement to previous meta-analyses, the present approach
sheds light on the scientific text and context mobilized to report on city size
distributions. It allows to identified gaps in the corpus and potentially
overlooked articles.
"
3077,A 25 Year Retrospective on D-Lib Magazine,"  In July, 1995 the first issue of D-Lib Magazine was published as an on-line,
HTML-only, open access magazine, serving as the focal point for the then
emerging digital library research community. In 2017 it ceased publication, in
part due to the maturity of the community it served as well as the increasing
availability of and competition from eprints, institutional repositories,
conferences, social media, and online journals -- the very ecosystem that D-Lib
Magazine nurtured and enabled. As long-time members of the digital library
community and authors with the most contributions to D-Lib Magazine, we reflect
on the history of the digital library community and D-Lib Magazine, taking its
very first issue as guidance. It contained three articles, which described: the
Dublin Core Metadata Element Set, a project status report from the
NSF/DARPA/NASA-funded Digital Library Initiative (DLI), and a summary of the
Kahn-Wilensky Framework (KWF) which gave us, among other things, Digital Object
Identifiers (DOIs). These technologies, as well as many more described in D-Lib
Magazine through its 23 years, have had a profound and continuing impact on the
digital library and general web communities.
"
3078,Open is not forever: a study of vanished open access journals,"  The preservation of the scholarly record has been a point of concern since
the beginning of knowledge production. With print publications, the
responsibility rested primarily with librarians, but the shift towards digital
publishing and, in particular, the introduction of open access (OA) have caused
ambiguity and complexity. Consequently, the long-term accessibility of journals
is not always guaranteed, and they can even disappear from the web completely.
The purpose of this exploratory study is to systematically study the phenomenon
of vanished journals, something that has not been done before. For the
analysis, we consulted several major bibliographic indexes, such as Scopus,
Ulrichsweb, and the Directory of Open Access Journals, and traced the journals
through the Internet Archive's Wayback Machine. We found 176 OA journals that,
through lack of comprehensive and open archives, vanished from the web between
2000-2019, spanning all major research disciplines and geographic regions of
the world. Our results raise vital concern for the integrity of the scholarly
record and highlight the urgency to take collaborative action to ensure
continued access and prevent the loss of more scholarly knowledge. We encourage
those interested in the phenomenon of vanished journals to use the public
dataset for their own research.
"
3079,"An empirical review of the different variants of the Probabilistic
  Affinity Index as applied to scientific collaboration","  Responsible indicators are crucial for research assessment and monitoring.
Transparency and accuracy of indicators are required to make research
assessment fair and ensure reproducibility. However, sometimes it is difficult
to conduct or replicate studies based on indicators due to the lack of
transparency in conceptualization and operationalization. In this paper, we
review the different variants of the Probabilistic Affinity Index (PAI),
considering both the conceptual and empirical underpinnings. We begin with a
review of the historical development of the indicator and the different
alternatives proposed. To demonstrate the utility of the indicator, we
demonstrate the application of PAI to identifying preferred partners in
scientific collaboration. A streamlined procedure is provided, to demonstrate
the variations and appropriate calculations. We then compare the results of
implementation for five specific countries involved in international scientific
collaboration. Despite the different proposals on its calculation, we do not
observe large differences between the PAI variants, particularly with respect
to country size. As with any indicator, the selection of a particular variant
is dependent on the research question. To facilitate appropriate use, we
provide recommendations for the use of the indicator given specific contexts.
"
3080,Linked Credibility Reviews for Explainable Misinformation Detection,"  In recent years, misinformation on the Web has become increasingly rampant.
The research community has responded by proposing systems and challenges, which
are beginning to be useful for (various subtasks of) detecting misinformation.
However, most proposed systems are based on deep learning techniques which are
fine-tuned to specific domains, are difficult to interpret and produce results
which are not machine readable. This limits their applicability and adoption as
they can only be used by a select expert audience in very specific settings. In
this paper we propose an architecture based on a core concept of Credibility
Reviews (CRs) that can be used to build networks of distributed bots that
collaborate for misinformation detection. The CRs serve as building blocks to
compose graphs of (i) web content, (ii) existing credibility signals
--fact-checked claims and reputation reviews of websites--, and (iii)
automatically computed reviews. We implement this architecture on top of
lightweight extensions to Schema.org and services providing generic NLP tasks
for semantic similarity and stance detection. Evaluations on existing datasets
of social-media posts, fake news and political speeches demonstrates several
advantages over existing systems: extensibility, domain-independence,
composability, explainability and transparency via provenance. Furthermore, we
obtain competitive results without requiring finetuning and establish a new
state of the art on the Clef'18 CheckThat! Factuality task.
"
3081,"CORAL: COde RepresentAtion Learning with Weakly-Supervised Transformers
  for Analyzing Data Analysis","  Large scale analysis of source code, and in particular scientific source
code, holds the promise of better understanding the data science process,
identifying analytical best practices, and providing insights to the builders
of scientific toolkits. However, large corpora have remained unanalyzed in
depth, as descriptive labels are absent and require expert domain knowledge to
generate. We propose a novel weakly supervised transformer-based architecture
for computing joint representations of code from both abstract syntax trees and
surrounding natural language comments. We then evaluate the model on a new
classification task for labeling computational notebook cells as stages in the
data analysis process from data import to wrangling, exploration, modeling, and
evaluation. We show that our model, leveraging only easily-available weak
supervision, achieves a 38% increase in accuracy over expert-supplied
heuristics and outperforms a suite of baselines. Our model enables us to
examine a set of 118,000 Jupyter Notebooks to uncover common data analysis
patterns. Focusing on notebooks with relationships to academic articles, we
conduct the largest ever study of scientific code and find that notebook
composition correlates with the citation count of corresponding papers.
"
3082,"A Decade of In-text Citation Analysis based on Natural Language
  Processing and Machine Learning Techniques: An overview of empirical studies","  Citation analysis is one of the most frequently used methods in research
evaluation. We are seeing significant growth in citation analysis through
bibliometric metadata, primarily due to the availability of citation databases
such as the Web of Science, Scopus, Google Scholar, Microsoft Academic, and
Dimensions. Due to better access to full-text publication corpora in recent
years, information scientists have gone far beyond traditional bibliometrics by
tapping into advancements in full-text data processing techniques to measure
the impact of scientific publications in contextual terms. This has led to
technical developments in citation context and content analysis, citation
classifications, citation sentiment analysis, citation summarisation, and
citation-based recommendation. This article aims to narratively review the
studies on these developments. Its primary focus is on publications that have
used natural language processing and machine learning techniques to analyse
citations.
"
3083,"Pairwise Learning for Name Disambiguation in Large-Scale Heterogeneous
  Academic Networks","  Name disambiguation aims to identify unique authors with the same name.
Existing name disambiguation methods always exploit author attributes to
enhance disambiguation results. However, some discriminative author attributes
(e.g., email and affiliation) may change because of graduation or job-hopping,
which will result in the separation of the same author's papers in digital
libraries. Although these attributes may change, an author's co-authors and
research topics do not change frequently with time, which means that papers
within a period have similar text and relation information in the academic
network. Inspired by this idea, we introduce Multi-view Attention-based
Pairwise Recurrent Neural Network (MA-PairRNN) to solve the name disambiguation
problem. We divided papers into small blocks based on discriminative author
attributes and blocks of the same author will be merged according to pairwise
classification results of MA-PairRNN. MA-PairRNN combines heterogeneous graph
embedding learning and pairwise similarity learning into a framework. In
addition to attribute and structure information, MA-PairRNN also exploits
semantic information by meta-path and generates node representation in an
inductive way, which is scalable to large graphs. Furthermore, a semantic-level
attention mechanism is adopted to fuse multiple meta-path based
representations. A Pseudo-Siamese network consisting of two RNNs takes two
paper sequences in publication time order as input and outputs their
similarity. Results on two real-world datasets demonstrate that our framework
has a significant and consistent improvement of performance on the name
disambiguation task. It was also demonstrated that MA-PairRNN can perform well
with a small amount of training data and have better generalization ability
across different research areas.
"
3084,The prestige and status of research fields within mathematics,"  While the ``hierarchy of science'' has been widely analysed, there is no
corresponding study of the status of subfields within a given scientific field.
We use bibliometric data to show that subfields of mathematics have a different
``standing'' within the mathematics community. Highly ranked departments tend
to specialize in some subfields more than in others, and the same subfields are
also over-represented in the most selective mathematics journals or among
recipients of top prizes. Moreover this status of subfields evolves markedly
over the period of observation (1984--2016), with some subfields gaining and
others losing in standing. The status of subfields is related to different
publishing habits, but some of those differences are opposite to those observed
when considering the hierarchy of scientific fields.
  We examine possible explanations for the ``status'' of different subfields.
Some natural explanations -- availability of funding, importance of
applications -- do not appear to function, suggesting that factors internal to
the discipline are at work. We propose a different type of explanation, based
on a notion of ``focus'' of a subfield, that might or might not be specific to
mathematics.
"
3085,COVID-19 Kaggle Literature Organization,"  The world has faced the devastating outbreak of Severe Acute Respiratory
Syndrome Coronavirus-2 (SARS-CoV-2), or COVID-19, in 2020. Research in the
subject matter was fast-tracked to such a point that scientists were struggling
to keep up with new findings. With this increase in the scientific literature,
there arose a need for organizing those documents. We describe an approach to
organize and visualize the scientific literature on or related to COVID-19
using machine learning techniques so that papers on similar topics are grouped
together. By doing so, the navigation of topics and related papers is
simplified. We implemented this approach using the widely recognized CORD-19
dataset to present a publicly available proof of concept.
"
3086,Mapping Researchers with PeopleMap,"  Discovering research expertise at universities can be a difficult task.
Directories routinely become outdated, and few help in visually summarizing
researchers' work or supporting the exploration of shared interests among
researchers. This results in lost opportunities for both internal and external
entities to discover new connections, nurture research collaboration, and
explore the diversity of research. To address this problem, at Georgia Tech, we
have been developing PeopleMap, an open-source interactive web-based tool that
uses natural language processing (NLP) to create visual maps for researchers
based on their research interests and publications. Requiring only the
researchers' Google Scholar profiles as input, PeopleMap generates and
visualizes embeddings for the researchers, significantly reducing the need for
manual curation of publication information. To encourage and facilitate easy
adoption and extension of PeopleMap, we have open-sourced it under the
permissive MIT license at https://github.com/poloclub/people-map. PeopleMap has
received positive feedback and enthusiasm for expanding its adoption across
Georgia Tech.
"
3087,"What Library Digitization Leaves Out: Predicting the Availability of
  Digital Surrogates of English Novels","  Library digitization has made more than a hundred thousand 19th-century
English-language books available to the public. Do the books which have been
digitized reflect the population of published books? An affirmative answer
would allow book and literary historians to use holdings of major digital
libraries as proxies for the population of published works, sparing them the
labor of collecting a representative sample. We address this question by taking
advantage of exhaustive bibliographies of novels published for the first time
in the British Isles in 1836 and 1838, identifying which of these novels have
at least one digital surrogate in the Internet Archive, HathiTrust, Google
Books, and the British Library. We find that digital surrogate availability is
not random. Certain kinds of novels, notably novels written by men and novels
published in multivolume format, have digital surrogates available at
distinctly higher rates than other kinds of novels. As the processes leading to
this outcome are unlikely to be isolated to the novel and the late 1830s, these
findings suggest that similar patterns will likely be observed during adjacent
decades and in other genres of publishing (e.g., non-fiction).
"
3088,Identifying Documents In-Scope of a Collection from Web Archives,"  Web archive data usually contains high-quality documents that are very useful
for creating specialized collections of documents, e.g., scientific digital
libraries and repositories of technical reports. In doing so, there is a
substantial need for automatic approaches that can distinguish the documents of
interest for a collection out of the huge number of documents collected by web
archiving institutions. In this paper, we explore different learning models and
feature representations to determine the best performing ones for identifying
the documents of interest from the web archived data. Specifically, we study
both machine learning and deep learning models and ""bag of words"" (BoW)
features extracted from the entire document or from specific portions of the
document, as well as structural features that capture the structure of
documents. We focus our evaluation on three datasets that we created from three
different Web archives. Our experimental results show that the BoW classifiers
that focus only on specific portions of the documents (rather than the full
text) outperform all compared methods on all three datasets.
"
3089,"The Pace of Artificial Intelligence Innovations: Speed, Talent, and
  Trial-and-Error","  Innovations in artificial intelligence (AI) are occurring at speeds faster
than ever witnessed before. However, few studies have managed to measure or
depict this increasing velocity of innovations in the field of AI. In this
paper, we combine data on AI from arXiv and Semantic Scholar to explore the
pace of AI innovations from three perspectives: AI publications, AI players,
and AI updates (trial and error). A research framework and three novel
indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed
(US), are proposed to measure the pace of innovations in the field of AI. The
results show that: (1) in 2019, more than 3 AI preprints were submitted to
arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one
deep learning-related preprint submitted to arXiv every 0.87 hours in 2019,
over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers
entered into the field of AI each hour in 2019, more than 175 times faster than
in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint
was submitted to arXiv every 41 days, with around 33% of AI preprints having
been updated at least twice in 2019. In addition, as reported in 2019, it took,
on average, only around 0.2 year for AI preprints to receive their first
citations, which is 5 times faster than 2000-2007. This swift pace in AI
illustrates the increase in popularity of AI innovation. The systematic and
fine-grained analysis of the AI field enabled to portrait the pace of AI
innovation and demonstrated that the proposed approach can be adopted to
understand other fast-growing fields such as cancer research and nano science.
"
3090,Universal Layout Emulation for Long-Term Database Archival,"  Research on alternate media technologies, like film, synthetic DNA, and
glass, for long-term data archival has received a lot of attention recently due
to the media obsolescence issues faced by contemporary storage media like tape,
Hard Disk Drives (HDD), and Solid State Disks (SSD). While researchers have
developed novel layout and encoding techniques for archiving databases on these
new media types, one key question remains unaddressed: How do we ensure that
the decoders developed today will be available and executable by a user who is
restoring an archived database several decades later in the future, on a
computing platform that potentially does not even exist today?
  In this paper, we make the case for Universal Layout Emulation (ULE), a new
approach for future-proof, long-term database archival that advocates archiving
decoders together with the data to ensure successful recovery. In order to do
so, ULE brings together concepts from Data Management and Digital Preservation
communities by using emulation for archiving decoders. In order to show that
ULE can be implemented in practice, we present the design and evaluation of
Micr'Olonys, an end-to-end long-term database archival system that can be used
to archive databases using visual analog media like film, microform, and
archival paper.
"
3091,A Review of Geospatial Content in IEEE Visualization Publications,"  Geospatial analysis is crucial for addressing many of the world's most
pressing challenges. Given this, there is immense value in improving and
expanding the visualization techniques used to communicate geospatial data. In
this work, we explore this important intersection -- between geospatial
analytics and visualization -- by examining a set of recent IEEE VIS Conference
papers (a selection from 2017-2019) to assess the inclusion of geospatial data
and geospatial analyses within these papers. After removing the papers with no
geospatial data, we organize the remaining literature into geospatial data
domain categories and provide insight into how these categories relate to VIS
Conference paper types. We also contextualize our results by investigating the
use of geospatial terms in IEEE Visualization publications over the last 30
years. Our work provides an understanding of the quantity and role of
geospatial subject matter in recent IEEE VIS publications and supplies a
foundation for future meta-analytical work around geospatial analytics and
geovisualization that may shed light on opportunities for innovation.
"
3092,"How much does an interlibrary loan request cost? A review of the
  literature","  Interlibrary loan (ILL) services are used to fill the gap between academic
libraries' collections and the information needs of their users. Today's trend
toward the cancellation of serials ""Big Deals"" has increased the importance of
clear information around ILL to support decision-making. In order to plan the
cancellation of a journal package, academic libraries need to be able to
forecast their total spendings on ILL, which requires to have an appropriate
estimate of what it costs to fulfill an individual ILL request. This paper aims
to help librarians answer this question by reviewing the most recent academic
literature related to these costs. There are several factors that may affect
the cost of an ILL service, including the cost of labour, the geographic
location of the library, the use of a ILL software, and membership to a library
consortium. We find that there is a wide range of estimates for ILL cost, from
$3.75 (USD) to $100.00 (USD). However, Jackson's (2004) figure of $17.50 (USD)
per transaction remains the guideline for most researchers and librarians.
"
3093,"The aftermath of Big Deal cancellations and their impact on interlibrary
  loans","  A ""Big Deal"" is a bundle of journals that is offered to libraries by
publishers as a ""one-price, one size fits all package"" (Frazier, 2001). There
have been several accounts of Big Deals cancellations by academic libraries in
the scientific literature. This paper presents the finding of a literature
review aimed at documenting the aftermath of Big Deal cancellation in
University Libraries, particularly their impacts on interlibrary loan services.
We find that many academic libraries have successfully cancelled their Big
Deals, realizing budget savings while limiting negative effects on library
users. In particular, existing literature reveals that cancellations have a
surprisingly small effect on interlibrary loan requests. The reviewed studies
further highlight the importance of access to proper usage data and inclusion
of community members of the community (staff, faculty members, students, etc.)
in the decision-making process.
"
3094,RapidLearn: A General Purpose Toolkit for Autonomic Networking,"  Software Defined Networking has unfolded a new area of opportunity in
distributed networking and intelligent networks. There has been a great
interest in performing machine learning in distributed setting, exploiting the
abstraction of SDN which makes it easier to write complex ML queries on
standard control plane. However, most of the research has been made towards
specialized problems (security, performance improvement, middlebox management
etc) and not towards a generic framework. Also, existing tools and software
require specialized knowledge of the algorithm/network to operate or monitor
these systems. We built a generic toolkit which abstracts out the underlying
structure, algorithms and other intricacies and gives an intuitive way for a
common user to create and deploy distributed machine learning network
applications. Decisions are made at local level by the switches and
communicated to other switches to improve upon these decisions. Finally, a
global decision is taken by controller based on another algorithm (in our case
voting). We demonstrate efficacy of the framework through a simple DDoS
detection algorithm.
"
3095,How reliable and useful is Cabell's Blacklist ? A data-driven analysis,"  In scholarly publishing, blacklists aim to register fraudulent or deceptive
journals and publishers, also known as ""predatory"", to minimise the spread of
unreliable research and the growing of fake publishing outlets. However,
blacklisting remains a very controversial activity for several reasons: there
is no consensus regarding the criteria used to determine fraudulent journals,
the criteria used may not always be transparent or relevant, and blacklists are
rarely updated regularly. Cabell's paywalled blacklist service attempts to
overcome some of these issues in reviewing fraudulent journals on the basis of
transparent criteria and in providing allegedly up-to-date information at the
journal entry level. We tested Cabell's blacklist to analyse whether or not it
could be adopted as a reliable tool by stakeholders in scholarly communication,
including our own academic library. To do so, we used a copy of Walt Crawford's
Gray Open Access dataset (2012-2016) to assess the coverage of Cabell's
blacklist and get insights on their methodology. Out of the 10,123 journals
that we tested, 4,681 are included in Cabell's blacklist. Out of this number of
journals included in the blacklist, 3,229 are empty journals, i.e. journals in
which no single article has ever been published. Other collected data points to
questionable weighing and reviewing methods and shows a lack of rigour in how
Cabell applies its own procedures: some journals are blacklisted on the basis
of 1 to 3 criteria, identical criteria are recorded multiple times in
individual journal entries, discrepancies exist between reviewing dates and the
criteria version used and recorded by Cabell, reviewing dates are missing, and
we observed two journals blacklisted twice with a different number of
violations. Based on these observations, we conclude with recommendations and
suggestions that could help improve Cabell's blacklist service.
"
3096,"Citing and referencing habits in Medicine and Social Sciences journals
  in 2019","  Purpose. This article explores citing and referencing systems in Social
Sciences and Medicine articles from different theoretical and practical
perspectives, considering bibliographic references as a facet of descriptive
representation. Methodology. The analysis of citing and referencing elements
(i.e. bibliographic references, mentions, quotations, and respective in-text
reference pointers) identified citing and referencing habits within disciplines
under consideration and errors occurring over the long term as stated by
previous studies now expanded. Future expected trends of information retrieval
from bibliographic metadata was gathered by approaching these referencing
elements from the FRBR Entities concepts. Findings. Reference styles do not
fully accomplish with their role of guiding authors and publishers on providing
concise and well-structured bibliographic metadata within bibliographic
references. Trends on representative description revision suggest a predicted
distancing on the ways information is approached by bibliographic references
and bibliographic catalogs adopting FRBR concepts, including the description
levels adopted by each of them under the perspective of the FRBR Entities
concept. Limitations. This study was based on a subset of Medicine and Social
Sciences articles published in 2019 and, therefore, it may not be taken as a
final and broad coverage. Future studies expanding these approaches to other
disciplines and chronological periods are encouraged. Originality. By
approaching citing and referencing issues as descriptive representation's
facets, findings on this study may encourage further studies that will support
Information Science and Computer Science on providing tools to become
bibliographic metadata description simpler, better structured and more
efficient facing the revision of descriptive representation actually in
progress.
"
3097,"Data mining and analysis of scientific research data records on Covid 19
  mortality, immunity, and vaccine development in the first wave of the Covid
  19 pandemic","  In this study, we investigate the scientific research response from the early
stages of the pandemic, and we review key findings on how the early warning
systems developed in previous epidemics responded to contain the virus. The
data records are analysed with commutable statistical methods, including R
Studio, Bibliometrix package, and the Web of Science data mining tool. We
identified few different clusters, containing references to exercise,
inflammation, smoking, obesity and many additional factors. From the analysis
on Covid-19 and vaccine, we discovered that although the USA is leading in
volume of scientific research on Covid 19 vaccine, the leading 3 research
institutions (Fudan, Melbourne, Oxford) are not based in the USA. Hence, it is
difficult to predict which country would be first to produce a Covid 19
vaccine.
"
3098,"Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound
  Image Analysis","  Controlling the COVID-19 pandemic largely hinges upon the existence of fast,
safe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or
X-Ray, has many practical advantages and can serve as a globally-applicable
first-line examination technique. We provide the largest publicly available
lung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three
classes (COVID-19, bacterial pneumonia, and healthy controls); curated and
approved by medical experts. On this dataset, we perform an in-depth study of
the value of deep learning methods for differential diagnosis of COVID-19. We
propose a frame-based convolutional neural network that correctly classifies
COVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of
0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We
further employ class activation maps for the spatio-temporal localization of
pulmonary biomarkers, which we subsequently validate for human-in-the-loop
scenarios in a blindfolded study with medical experts. Aiming for scalability
and robustness, we perform ablation studies comparing mobile-friendly, frame-
and video-based architectures and show reliability of the best model by
aleatoric and epistemic uncertainty estimates. We hope to pave the road for a
community effort toward an accessible, efficient and interpretable screening
method and we have started to work on a clinical validation of the proposed
method. Data and code are publicly available.
"
3099,A matter of time: publication dates in Web of Science Core Collection,"  Web of Science Core Collection, one of the most authoritative bibliographic
databases, is widely used in academia to track high-quality research. This
database has begun to index online-first articles since December 2017. This new
practice has introduced two different publication dates (online and final
publication dates) into the database for more and more early access
publications. It may confuse many users who want to search or analyze
literature by using the publication-year related tools provided by Web of
Science Core Collection. By developing custom retrieval strategies and checking
manually, this study finds that the ""year published"" field in search page
searches in both online and final publication date fields of indexed records.
Each indexed record is allocated to only one ""publication year"" on the left of
the search results page which will inherit first from online publication date
field even when the online publication date is later than the final publication
date. The ""publication year"" field in the results analysis page and the
timespan ""custom year range"" field in the search page have the same function as
that of the filter ""publication year"" in search results page. The potential
impact of the availability of two different publication dates in calculating
bibliometric indicators is also discussed at the end of the article.
"
3100,Towards an RDF Knowledge Graph of Scholars from Early Modern History,"  The use of Semantic Web Technologies supports research in the field of
digital humanities. In this paper we focus on the creation of semantic
independent online databases such as those of historical prosopography. These
databases contain biographical information of historical persons. We focus on
this information with an interest in German professorial career patterns from
the 16th to the 18th century. In that respect, we describe the process of
building an Early Modern Scholarly Career RDF Knowledge Graph from two existing
prosopography online databases: the Catalogus Professorum Lipsiensium and the
Catalogus Professorum Helmstadiensium. Further, we provide an insight in how to
query the information using KBox to answer research questions.
"
3101,"Same data may bring conflict results: a caution to use the disruptive
  index","  In the last two decades, scholars have designed various types of
bibliographic related indicators to identify breakthrough-class academic
achievements. In this study, we take a further step to look at properties of
the promising disruptive index, thus deepening our understanding of this index
and further facilitating its wise use in bibliometrics. Using publication
records for Nobel laureates between 1900 and 2016, we calculate the DI of Nobel
Prize-winning articles and its benchmark articles in each year and use the
median DI to denote the central tendency in each year, and compare results
between Medicine, Chemistry, and Physics. We find that conclusions based on DI
depend on the length of their citation time window, and different citation time
windows may cause different, even controversial, results. Also, discipline and
time play a role on the length of citation window when using DI to measure the
innovativeness of a scientific work. Finally, not all articles with DI equals
to 1 were the breakthrough-class achievements. In other words, the DI stands up
theoretically, but we should not neglect that the DI was only shaped by the
number of citing articles and times the references have been cited, these data
may vary from database to database.
"
3102,"Comment on ""Open is not forever: a study of vanished open access
  journals""","  We comment on a recent article by Laakso et al. (arXiv:2008.11933 [cs.DL]),
in which the disappearance of 176 open access journals from the Internet is
noted. We argue that one reason these journals may have vanished is that they
were predatory journals. The de-listing of predators from the Directory of Open
Access Journals in 2014 and the abundance of predatory journals and awareness
thereof in North America parsimoniously explain the temporal and geographic
patterns Laakso et al. observed.
"
3103,How to Measure the Performance of a Collaborative Research Center,"  New Public Management helps universities and research institutions to perform
in a highly competitive research environment. Evaluating publicly financed
research improves transparency, helps in reflection and self-assessment, and
provides information for strategic decision making. In this paper we provide
empirical evidence using data from a Collaborative Research Center (CRC) on
financial inputs and research output from 2005 to 2016. After selecting
performance indicators suitable for a CRC, we describe main properties of the
data using visualization techniques. To study the relationship between the
dimensions of research performance, we use a time fixed effects panel data
model and fixed effects Poisson model. With the help of year dummy variables,
we show how the pattern of research productivity changes over time after
controlling for staff and travel costs. The joint depiction of the time fixed
effects and the research project's life cycle allows a better understanding of
the development of the number of discussion papers over time.
"
3104,"Representing Semantified Biological Assays in the Open Research
  Knowledge Graph","  In the biotechnology and biomedical domains, recent text mining efforts
advocate for machine-interpretable, and preferably, semantified, documentation
formats of laboratory processes. This includes wet-lab protocols, (in)organic
materials synthesis reactions, genetic manipulations and procedures for faster
computer-mediated analysis and predictions. Herein, we present our work on the
representation of semantified bioassays in the Open Research Knowledge Graph
(ORKG). In particular, we describe a semantification system work-in-progress to
generate, automatically and quickly, the critical semantified bioassay data
mass needed to foster a consistent user audience to adopt the ORKG for
recording their bioassays and facilitate the organisation of research,
according to FAIR principles.
"
3105,"Beyond the Western Core-Periphery Model: Analysing Scientific Mobility
  and Collaboration in the Middle East and North Africa","  This study investigates the scientific mobility and international
collaboration networks in the Middle East and North Africa (MENA) region
between 2008 and 2017. The main goal is to establish mobility and collaboration
profiles at the region and country levels. By using affiliation metadata
available in scientific publications, we track international scientific
mobility and collaboration networks in the region. Three complementary
approaches allow us to obtain a detailed characterization of scientific
mobility. First, we study the mobility flows for each country to uncover the
main destinations and origins of mobile scholars. Results reveal geographical,
cultural, historical, and linguistic proximities. Cooperation and exchange
programs also contribute to explain some of the observed flows. Second, we
introduce mobile scientists' academic age. The average academic age of migrant
scholars in MENA between 2008 and 2017 was about 12.4 years. For most
countries, immigrants are relatively younger than emigrants, except for Iran,
Palestine, Lebanon, and Turkey. Scholars who migrated to Gulf Cooperation
Council (GCC) countries, Jordan and Morocco were in average younger than
emigrants by 1.5 year from the same countries. The academic age group 6-to-10
years is the most common for both emigrant and immigrant scholars. Third, we
analyse gender differences of scholars. We observe a clear gender gap in terms
of scientific mobility: Male scholars represent the largest group of migrants
in MENA countries. We conclude discussing the policy relevance of the
scientific mobility and collaboration aspects and discuss limitations and
further research.
"
3106,"A Deep Learning Approach to Geographical Candidate Selection through
  Toponym Matching","  Recognizing toponyms and resolving them to their real-world referents is
required for providing advanced semantic access to textual data. This process
is often hindered by the high degree of variation in toponyms. Candidate
selection is the task of identifying the potential entities that can be
referred to by a toponym previously recognized. While it has traditionally
received little attention in the research community, it has been shown that
candidate selection has a significant impact on downstream tasks (i.e. entity
resolution), especially in noisy or non-standard text. In this paper, we
introduce a flexible deep learning method for candidate selection through
toponym matching, using state-of-the-art neural network architectures. We
perform an intrinsic toponym matching evaluation based on several new realistic
datasets, which cover various challenging scenarios (cross-lingual and regional
variations, as well as OCR errors). We report its performance on candidate
selection in the context of the downstream task of toponym resolution, both on
existing datasets and on a new manually-annotated resource of
nineteenth-century English OCR'd text.
"
3107,"A Glimpse of the First Eight Months of the COVID-19 Literature on
  Microsoft Academic Graph: Themes, Citation Contexts, and Uncertainties","  As scientists worldwide search for answers to the overwhelmingly unknown
behind the deadly pandemic, the literature concerning COVID-19 has been growing
exponentially. Keeping abreast of the body of literature at such a rapidly
advancing pace poses significant challenges not only to active researchers but
also to the society as a whole. Although numerous data resources have been made
openly available, the analytic and synthetic process that is essential in
effectively navigating through the vast amount of information with heightened
levels of uncertainty remains a significant bottleneck. We introduce a generic
method that facilitates the data collection and sense-making process when
dealing with a rapidly growing landscape of a research domain such as COVID-19
at multiple levels of granularity. The method integrates the analysis of
structural and temporal patterns in scholarly publications with the delineation
of thematic concentrations and the types of uncertainties that may offer
additional insights into the complexity of the unknown. We demonstrate the
application of the method in a study of the COVID-19 literature.
"
3108,"Examining the Impact of Algorithm Awareness on Wikidata's Recommender
  System Recoin","  The global infrastructure of the Web, designed as an open and transparent
system, has a significant impact on our society. However, algorithmic systems
of corporate entities that neglect those principles increasingly populated the
Web. Typical representatives of these algorithmic systems are recommender
systems that influence our society both on a scale of global politics and
during mundane shopping decisions. Recently, such recommender systems have come
under critique for how they may strengthen existing or even generate new kinds
of biases. To this end, designers and engineers are increasingly urged to make
the functioning and purpose of recommender systems more transparent. Our
research relates to the discourse of algorithm awareness, that reconsiders the
role of algorithm visibility in interface design. We conducted online
experiments with 105 participants using MTurk for the recommender system
Recoin, a gadget for Wikidata. In these experiments, we presented users with
one of a set of three different designs of Recoin's user interface, each of
them exhibiting a varying degree of explainability and interactivity. Our
findings include a positive correlation between comprehension of and trust in
an algorithmic system in our interactive redesign. However, our results are not
conclusive yet, and suggest that the measures of comprehension, fairness,
accuracy and trust are not yet exhaustive for the empirical study of algorithm
awareness. Our qualitative insights provide a first indication for further
measures. Our study participants, for example, were less concerned with the
details of understanding an algorithmic calculation than with who or what is
judging the result of the algorithm.
"
3109,COVID-19 Literature Topic-Based Search via Hierarchical NMF,"  A dataset of COVID-19-related scientific literature is compiled, combining
the articles from several online libraries and selecting those with open access
and full text available. Then, hierarchical nonnegative matrix factorization is
used to organize literature related to the novel coronavirus into a tree
structure that allows researchers to search for relevant literature based on
detected topics. We discover eight major latent topics and 52 granular
subtopics in the body of literature, related to vaccines, genetic structure and
modeling of the disease and patient studies, as well as related diseases and
virology. In order that our tool may help current researchers, an interactive
website is created that organizes available literature using this hierarchical
structure.
"
3110,"Open Access Books in the Humanities and Social Sciences: an Open Access
  Altmetric Advantage","  The last decade has seen two significant phenomena emerge in research
communication: the rise of open access (OA) publishing, and evidence of online
sharing in the form of altmetrics. There has been limited examination of the
effect of OA on online sharing for journal articles, and little for books. This
paper examines the altmetrics of a set of 32,222 books (of which 5% are OA) and
a set of 220,527 chapters (of which 7% are OA) indexed by the scholarly
database Dimensions in the Social Sciences and Humanities. Both OA books and
chapters have significantly higher use on social networks, higher coverage in
the mass media and blogs, and evidence of higher rates of social impact in
policy documents. OA chapters have higher rates of coverage on Wikipedia than
their non-OA equivalents, and are more likely to be shared on Mendeley. Even
within the Humanities and Social Sciences, disciplinary differences in
altmetric activity are evident. The effect is confirmed for chapters, although
sampling issues prevent the strong conclusion that OA facilitates extra
attention at whole book level, the apparent OA altmetrics advantage suggests
that the move towards OA is increasing social sharing and broader impact.
"
3111,Crosslingual Topic Modeling with WikiPDA,"  We present Wikipedia-based Polyglot Dirichlet Allocation (WikiPDA), a
crosslingual topic model that learns to represent Wikipedia articles written in
any language as distributions over a common set of language-independent topics.
It leverages the fact that Wikipedia articles link to each other and are mapped
to concepts in the Wikidata knowledge base, such that, when represented as bags
of links, articles are inherently language-independent. WikiPDA works in two
steps, by first densifying bags of links using matrix completion and then
training a standard monolingual topic model. A human evaluation shows that
WikiPDA produces more coherent topics than monolingual text-based LDA, thus
offering crosslinguality at no cost. We demonstrate WikiPDA's utility in two
applications: a study of topical biases in 28 Wikipedia editions, and
crosslingual supervised classification. Finally, we highlight WikiPDA's
capacity for zero-shot language transfer, where a model is reused for new
languages without any fine-tuning.
"
3112,"Funding CRISPR: Understanding the role of government and private sector
  actors in transformative innovation systems","  CRISPR/Cas has the potential to revolutionize medicine, agriculture, and the
way we understand life itself. Understanding the trajectory of innovation, how
it is influenced and who pays for it, is essential for such a transformative
technology. The University of California and the Broad/Harvard/MIT systems are
the two most prominent academic institutions involved in the research and
development of CRISPR/Cas. Here we present a model of co-funding networks for
CRISPR/Cas research at these institutions, using funding acknowledgments to
build connections. We map papers representing 95% of citations on CRISPR/Cas
from these institutions grouped by the stage that each represents in the
translational research process (as a biological phenomenon, as a research tool,
and development and applications of these technologies), and use a novel
technique to analyse the relationships between the structures of the co-funding
networks, the phase of research, and funding sources. The co-funding
subnetworks were similar in that US government research funding played the
decisive role in early stage research. Research at Broad/Harvard/MIT is also
strongly supported by philanthropic/charitable organizations in later stages of
the translational research process, clustered around certain topics.
Applications for CRISPR technologies were underrepresented, which bolsters
findings on the preponderance of the US private sector in developing
applications, and the disproportionate number of Chinese institutions filing
patents for industrial and food systems applications. These network models
raise fundamental questions about the role of the state in supporting
breakthrough innovations, risk, reward, and the influence of the private sector
and philanthropy over the trajectory of transformative technologies.
"
3113,An Analysis of the Impact of SEO on University Website Ranking,"  Today, ranking systems in universities have been considered by the academic
community, and there is a tight competition between world universities to
achieve higher ranks. In the meantime, the ranking of university websites is
also in the spotlight, and the Webometric research center announces the ranks
of university websites twice a year. Examining university rankings indicators
and the Webometric ranks of the university indicates that some of these
indicators, directly and indirectly, affect each other. On the other hand, a
preliminary study of Webometric indicators shows that some Search Engine
Optimization (SEO) indicators can affect Webometric ranks. The purpose of this
research is to show how far the SEO metrics can affect the website rank of the
university. To do this, after extracting 38 points of the significant SEO
metrics of the extracted universities using various tools, data analysis was
conducted along with applying association rules on the data. The results of the
research show that some of the SEO metrics, such as the number of backlinks,
Alexa Rank, and Page Rank have a direct and significant impact on the website
rank of universities, and in this regard, interesting rules have been
extracted.
"
3114,Can pandemics transform scientific novelty? Evidence from COVID-19,"  Scientific novelty is important during the pandemic due to its critical role
in generating new vaccines. Parachuting collaboration and international
collaboration are two crucial channels to expand teams' search activities for a
broader scope of resources required to address the global challenge. Our
analysis of 58,728 coronavirus papers suggests that scientific novelty measured
by the BioBERT model that is pre-trained on 29 million PubMed articles, and
parachuting collaboration dramatically increased after the outbreak of
COVID-19, while international collaboration witnessed a sudden decrease. During
the COVID-19, papers with more parachuting collaboration and internationally
collaborative papers are predicted to be more novel. The findings suggest the
necessity of reaching out for distant resources, and the importance of
maintaining a collaborative scientific community beyond established networks
and nationalism during a pandemic.
"
3115,Visual Exploration and Knowledge Discovery from Biomedical Dark Data,"  Data visualization techniques proffer efficient means to organize and present
data in graphically appealing formats, which not only speeds up the process of
decision making and pattern recognition but also enables decision-makers to
fully understand data insights and make informed decisions. Over time, with the
rise in technological and computational resources, there has been an
exponential increase in the world's scientific knowledge. However, most of it
lacks structure and cannot be easily categorized and imported into regular
databases. This type of data is often termed as Dark Data. Data visualization
techniques provide a promising solution to explore such data by allowing quick
comprehension of information, the discovery of emerging trends, identification
of relationships and patterns, etc. In this empirical research study, we use
the rich corpus of PubMed comprising of more than 30 million citations from
biomedical literature to visually explore and understand the underlying
key-insights using various information visualization techniques. We employ a
natural language processing based pipeline to discover knowledge out of the
biomedical dark data. The pipeline comprises of different lexical analysis
techniques like Topic Modeling to extract inherent topics and major focus
areas, Network Graphs to study the relationships between various entities like
scientific documents and journals, researchers, and, keywords and terms, etc.
With this analytical research, we aim to proffer a potential solution to
overcome the problem of analyzing overwhelming amounts of information and
diminish the limitation of human cognition and perception in handling and
examining such large volumes of data.
"
3116,"Virtual Proximity Citation (VCP): A Supervised Deep Learning Method to
  Relate Uncited Papers On Grounds of Citation Proximity","  Citation based approaches have seen good progress for recommending research
papers using citations in the paper. Citation proximity analysis which uses the
in-text citation proximity to find relatedness between two research papers is
better than co-citation analysis and bibliographic analysis. However, one
common problem which exists in each approach is that paper should be well
cited. If documents are not cited properly or not cited at all, then using
these approaches will not be helpful. To overcome the problem, this paper
discusses the approach Virtual Citation Proximity (VCP) which uses Siamese
Neural Network along with the notion of citation proximity analysis and
content-based filtering. To train this model, the actual distance between the
two citations in a document is used as ground truth, this distance is the word
count between the two citations. VCP is trained on Wikipedia articles for which
the actual word count is available which is used to calculate the similarity
between the documents. This can be used to calculate relatedness between two
documents in a way they would have been cited in the proximity even if the
documents are uncited. This approach has shown a great improvement in
predicting proximity with basic neural networks over the approach which uses
the Average Citation Proximity index value as the ground truth. This can be
improved by using a complex neural network and proper hyper tuning of
parameters.
"
3117,CAT STREET: Chronicle Archive of Tokyo Street-fashion,"  The analysis of daily life fashion trends can help us understand our
societies and human cultures profoundly. However, no appropriate database
exists that includes images illustrating what people wore in their daily lives
over an extended period. In this study, we propose a new fashion image archive,
Chronicle Archive of Tokyo Street-fashion (CAT STREET), to shed light on daily
life fashion trends. CAT STREET includes images showing what people wore in
their daily lives during the period 1970-2017, and these images contain
timestamps and street location annotations. This novel database enables us to
observe long-term daily life fashion trends using quantitative methods. To
evaluate the potential of our database, we corroborated the rules-of-thumb for
two fashion trend phenomena, namely how economic conditions affect fashion
style share in the long term and how fashion styles emerge in the street and
diffuse from street to street. Our findings show that the Conservative style
trend, a type of luxury fashion style, is affected by economic conditions. We
also introduce four cases of how fashion styles emerge in the street and
diffuse from street to street in fashion-conscious streets in Tokyo. Our study
demonstrates CAT STREET's potential to promote understanding of societies and
human cultures through quantitative analysis of daily life fashion trends.
"
3118,"Enabling Synergy: Improving the Information Infrastructure for Planetary
  Science","  In this whitepaper we advocate that the Planetary Science (PS) community
build a discipline-specific digital library, in collaboration with the existing
astronomy digital library, ADS. We suggest that the PS data archives increase
their level of curation to allow for direct linking between the archival data
and the derived journal articles. And we suggest that a new component of the PS
information infrastructure be created to collate and curate information on
features and objects in our solar system, beginning with the USGS/IAU Gazetteer
of Planetary Nomenclature.
"
3119,The h-index and the Harmonic Mean,"  The Harmonic Mean between the number of papers and the citation number per
paper is proposed as a simple single-value index to quantify an individual's
research output. Two simple comparisons with the Hirsch h-index are performed.
"
3120,Second Order Operators in the NASA Astrophysics Data System,"  Second Order Operators (SOOs) are database functions which form secondary
queries based on attributes of the objects returned in an initial query; they
can provide powerful methods to investigate complex, multipartite information
graphs. The NASA Astrophysics Data System (ADS) has implemented four SOOs,
reviews, useful, trending, and similar which use the citations, references,
downloads, and abstract text.
  This tutorial describes these operators in detail, both alone and in
conjunction with other functions. It is intended for scientists and others who
wish to make fuller use of the ADS database. Basic knowledge of the ADS is
assumed.
"
3121,"The Importance of Conference Proceedings in Research Evaluation: a
  Methodology for Assessing Conference Impact","  Conferences are an essential tool for scientific communication. In
disciplines such as Computer Science, over 50% of original research results are
published in conference proceedings. In this study, we have analysed the role
of conference proceedings in various disciplines and propose an alternative
approach to research evaluation based on conference proceedings and Scimago
Journal Rank (SJR). The result of the study is a list of conference
proceedings, categorised Q1 - Q4 in several disciplines by analogy with SJR
journal quartiles. The comparison of this bibliometric-driven ranking with the
expert-driven CORE ranking in Computer Science showed a 62% overlap, as well as
a significant average rank correlation of the category distribution. Moreover,
38 conference proceedings in Engineering (45% of the list) and 23 in Computer
Science (32% of the list) have an SJR level corresponding to the first quartile
journals in these areas. This again emphasises the exceptional importance of
conferences in these disciplines.
"
3122,"Publication Patterns' Changes due to the COVID-19 Pandemic: A
  longitudinal and short-term scientometric analysis","  In recent months the COVID-19 (also known as SARS-CoV-2 and Coronavirus)
pandemic has spread throughout the world. In parallel, scholarly research
regarding various aspects of the pandemic has been extensively published. In
this work, we analyse the changes in biology and medical publishing patterns in
terms of the volume of publications in both preprint servers and peer reviewed
journals, average time until the acceptance of papers to said journals, and
international (co-)authorship of papers. We study these possible changes using
two approaches: a short-term analysis through which changes during the first
six months of the outbreak are examined for both COVID-19 related papers and
non-COVID-19 related papers; and a longitudinal approach through which changes
are examined in comparison to the previous four years. Our results show that
the pandemic has so far had a tremendous effect on all examined accounts of
scholarly publications: A sharp increase in publication volume has been
witnessed and it can be almost entirely attributed to the pandemic; a
significantly lower mean time to acceptance for COVID-19 papers is apparent,
and it has (partially) come at the expense of non-COVID-19 papers; and a
significant reduction in international collaboration for COVID-19 papers has
been identified. As the pandemic continues to spread, these changes may cause
research in non-COVID-19 biology and medical areas to be slowed down and bring
about a lower rate of international collaboration.
"
3123,"'I Updated the <ref>': The Evolution of References in the English
  Wikipedia and the Implications for Altmetrics","  With this work, we present a publicly available dataset of the history of all
the references (more than 55 million) ever used in the English Wikipedia until
June 2019. We have applied a new method for identifying and monitoring
references in Wikipedia, so that for each reference we can provide data about
associated actions: creation, modifications, deletions, and reinsertions. The
high accuracy of this method and the resulting dataset was confirmed via a
comprehensive crowdworker labelling campaign. We use the dataset to study the
temporal evolution of Wikipedia references as well as users' editing behaviour.
We find evidence of a mostly productive and continuous effort to improve the
quality of references: (1) there is a persistent increase of reference and
document identifiers (DOI, PubMedID, PMC, ISBN, ISSN, ArXiv ID), and (2) most
of the reference curation work is done by registered humans (not bots or
anonymous editors). We conclude that the evolution of Wikipedia references,
including the dynamics of the community processes that tend to them should be
leveraged in the design of relevance indexes for altmetrics, and our dataset
can be pivotal for such effort.
"
3124,"Quantitative analysis of the co-publications of Ukrainian scientists
  with the Nobel laureates 1994-2018 in Science","  The Nobel Prize is awarded annually for outstanding scientific discoveries
and inventions. Most scientific papers today are co-authored by a large number
of researchers. However, very few scientists can receive the Nobel Prize
according to the Statutes of the Nobel Foundation. An analysis of the
co-authorship of the Nobel laureates will make it possible to identify
employees of Ukrainian institutions who have collaborated with leading
scientists of the world, whose scientific works were noted by Nobel. For the
development of science in Ukraine it is important to study the successful
experience of cooperation of domestic research institutions with leading world
scientists and research centers, because international scientific collaboration
facilitates the process of acquiring new knowledge, promotes mutual enrichment
of ideas, efficient use of resources and expands opportunities for further use
of research results. Using the Scopus database, selected publications of Nobel
Prize winners, which were written in collaboration with scientists who worked
in Ukrainian institutions. The data obtained indicate that the employees of
scientific institutions of Ukraine published very few papers in collaborations
with Nobel Prize winners of 1994-2018 in comparison with employees of
institutions in leading countries in publishing activity.
"
3125,"Graphing Contributions in Natural Language Processing Research:
  Intra-Annotator Agreement on a Trial Dataset","  Purpose: To stabilize the NLPContributionGraph scheme for the surface
structuring of contributions information in Natural Language Processing (NLP)
scholarly articles via a two-stage annotation methodology: first stage - to
define the scheme; and second stage - to stabilize the graphing model.
  Approach: Re-annotate, a second time, the contributions-pertinent information
across 50 prior-annotated NLP scholarly articles in terms of a data pipeline
comprising: contribution-centered sentences, phrases, and triples. To this end
specifically, care was taken in the second annotation stage to reduce
annotation noise while formulating the guidelines for our proposed novel NLP
contributions structuring scheme.
  Findings: The application of NLPContributionGraph on the 50 articles resulted
in finally in a dataset of 900 contribution-focused sentences, 4,702
contribution-information-centered phrases, and 2,980 surface-structured
triples. The intra-annotation agreement between the first and second stages, in
terms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for
triples indicating that with an increased granularity of the information, the
annotation decision variance is greater.
  Practical Implications: Demonstrate NLPContributionGraph data integrated in
the Open Research Knowledge Graph (ORKG), a next-generation KG-based digital
library with compute enabled over structured scholarly knowledge, as a viable
aid to assist researchers in their day-to-day tasks.
  Value: NLPContributionGraph is a novel scheme to obtain research
contribution-centered graphs from NLP articles which to the best of our
knowledge does not exist in the community. And our quantitative evaluations
over the two-stage annotation tasks offer insights into task difficulty.
"
3126,"Paying down metadata debt: learning the representation of concepts using
  topic models","  We introduce a data management problem called metadata debt, to identify the
mapping between data concepts and their logical representations. We describe
how this mapping can be learned using semisupervised topic models based on
low-rank matrix factorizations that account for missing and noisy labels,
coupled with sparsity penalties to improve localization and interpretability.
We introduce a gauge transformation approach that allows us to construct
explicit associations between topics and concept labels, and thus assign
meaning to topics. We also show how to use this topic model for semisupervised
learning tasks like extrapolating from known labels, evaluating possible errors
in existing labels, and predicting missing features. We show results from this
topic model in predicting subject tags on over 25,000 datasets from Kaggle.com,
demonstrating the ability to learn semantically meaningful features.
"
3127,ArXiving Before Submission Helps Everyone,"  We claim, and present evidence, that allowing arXiv publication before a
conference or journal submission benefits researchers, especially early career,
as well as the whole scientific community. Specifically, arXiving helps
professional identity building, protects against independent re-discovery, idea
theft and gate-keeping; it facilitates open research result distribution and
reduces inequality. The advantages dwarf the drawbacks -- mainly the relative
increase in acceptance rate of papers of well-known authors -- which studies
show to be marginal. Analyzing the pros and cons of arXiving papers, we
conclude that requiring preprints be anonymous is nearly as detrimental as not
allowing them. We see no reasons why anyone but the authors should decide
whether to arXiv or not.
"
3128,"Towards International Relations Data Science: Mining the CIA World
  Factbook","  This paper presents a three-component work. The first component sets the
overall theoretical context which lies in the argument that the increasing
complexity of the world has made it more difficult for International Relations
(IR) to succeed both in theory and practice. The era of information and the
events of the 21st century have moved IR theory and practice away from real
policy making (Walt, 2016) and have made it entrenched in opinions and
political theories difficult to prove. At the same time, the rise of the
""Fourth Paradigm - Data Intensive Scientific Discovery"" (Hey et al., 2009) and
the strengthening of data science offer an alternative: ""Computational
International Relations"" (Unver, 2018). The use of traditional and contemporary
data-centered tools can help to update the field of IR by making it more
relevant to reality (Koutsoupias, Mikelis, 2020). The ""wedding"" between Data
Science and IR is no panacea though. Changes are required both in perceptions
and practices. Above all, for Data Science to enter IR, the relevant data must
exist. This is where the second component comes into play. I mine the CIA World
Factbook which provides cross-domain data covering all countries of the world.
Then, I execute various data preprocessing tasks peaking in simple machine
learning which imputes missing values providing with a more complete dataset.
Lastly, the third component presents various projects making use of the
produced dataset in order to illustrate the relevance of Data Science to IR
through practical examples. Then, ideas regarding the future development of
this project are discussed in order to optimize it and ensure continuity.
Overall, I hope to contribute to the ""fourth paradigm"" discussion in IR by
providing practical examples while providing at the same time the fuel for
future research.
"
3129,"Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer
  of Scientific Concepts across Text Corpora","  What kind of basic research ideas are more likely to get applied in practice?
There is a long line of research investigating patterns of knowledge transfer,
but it generally focuses on documents as the unit of analysis and follow their
transfer into practice for a specific scientific domain. Here we study
translational research at the level of scientific concepts for all scientific
fields. We do this through text mining and predictive modeling using three
corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28
million clinical trials. We extract scientific concepts (i.e., phrases) from
corpora as instantiations of ""research ideas"", create concept-level features as
motivated by literature, and then follow the trajectories of over 450,000 new
concepts (emerged from 1995-2014) to identify factors that lead only a small
proportion of these ideas to be used in inventions and drug trials. Results
from our analysis suggest several mechanisms that distinguish which scientific
concept will be adopted in practice, and which will not. We also demonstrate
that our derived features can be used to explain and predict knowledge transfer
with high accuracy. Our work provides greater understanding of knowledge
transfer for researchers, practitioners, and government agencies interested in
encouraging translational research.
"
3130,NwQM: A neural quality assessment framework for Wikipedia,"  Millions of people irrespective of socioeconomic and demographic backgrounds,
depend on Wikipedia articles everyday for keeping themselves informed regarding
popular as well as obscure topics. Articles have been categorized by editors
into several quality classes, which indicate their reliability as encyclopedic
content. This manual designation is an onerous task because it necessitates
profound knowledge about encyclopedic language, as well navigating circuitous
set of wiki guidelines. In this paper we propose Neural wikipedia
QualityMonitor (NwQM), a novel deep learning model which accumulates signals
from several key information sources such as article text, meta data and images
to obtain improved Wikipedia article representation. We present comparison of
our approach against a plethora of available solutions and show 8% improvement
over state-of-the-art approaches with detailed ablation studies.
"
3131,"Predicting the popularity of scientific publications by an age-based
  diffusion model","  Predicting the popularity of scientific publications has attracted many
attentions from various disciplines. In this paper, we focus on the popularity
prediction problem of scientific papers, and propose an age-based diffusion
(AD) model to identify which paper will receive more citations in the near
future and will be popular. The AD model is a mimic of the attention diffusion
process along the citation networks. The experimental study shows that the AD
model can achieve better prediction accuracy than other networkbased methods.
For some newly published papers that have not accumulated many citations but
will be popular in the near future, the AD model can substantially improve
their rankings. This is really critical, because identifying the future highly
cited papers from massive numbers of new papers published each month would
provide very valuable references for researchers.
"
3132,The network structure of scientific revolutions,"  Philosophers of science have long postulated how collective scientific
knowledge grows. Empirical validation has been challenging due to limitations
in collecting and systematizing large historical records. Here, we capitalize
on the largest online encyclopedia to formulate knowledge as growing networks
of articles and their hyperlinked inter-relations. We demonstrate that concept
networks grow not by expanding from their core but rather by creating and
filling knowledge gaps, a process which produces discoveries that are more
frequently awarded Nobel prizes than others. Moreover, we operationalize
paradigms as network modules to reveal a temporal signature in structural
stability across scientific subjects. In a network formulation of scientific
discovery, data-driven conditions underlying breakthroughs depend just as much
on identifying uncharted gaps as on advancing solutions within scientific
communities.
"
3133,The Leaky Pipeline in Physics Publishing,"  Women make up a shrinking portion of physics faculty in senior positions, a
phenomenon known as a ""leaky pipeline."" While fixing this problem has been a
priority in academic institutions, efforts have been stymied by the diverse
sources of leaks. In this paper we identify a bias potentially contributing to
the leaky pipeline. We analyze bibliographic data provided by the American
Physical Society (APS), a leading publisher of physics research. By inferring
the gender of authors from names, we are able to measure the fraction of women
authors over past decades. We show that the more selective, higher impact APS
journals have lower fractions of women authors compared to other APS journals.
Correcting this bias may help more women publish in prestigious APS journals,
and in turn help improve their academic promotion cases.
"
3134,"Poincare: Recommending Publication Venues via Treatment Effect
  Estimation","  Choosing a publication venue for an academic paper is a crucial step in the
research process. However, in many cases, decisions are based on the experience
of researchers, which often leads to suboptimal results. Although some existing
methods recommend publication venues, they just recommend venues where a paper
is likely to be published. In this study, we aim to recommend publication
venues from a different perspective. We estimate the number of citations a
paper will receive if the paper is published in each venue and recommend the
venue where the paper has the most potential impact. However, there are two
challenges to this task. First, a paper is published in only one venue, and
thus, we cannot observe the number of citations the paper would receive if the
paper were published in another venue. Secondly, the contents of a paper and
the publication venue are not statistically independent; that is, there exist
selection biases in choosing publication venues. In this paper, we propose to
use a causal inference method to estimate the treatment effects of choosing a
publication venue effectively and to recommend venues based on the potential
influence of papers.
"
3135,"The (re-)instrumentalization of the Diagnostic and Statistical Manual of
  Mental Disorders (DSM) in psychological publications: a citation context
  analysis","  Research instruments play significant roles in the construction of scientific
knowledge, even though we have only acquired very limited knowledge about their
lifecycles from quantitative studies. This paper aims to address this gap by
quantitatively examining the citation contexts of an exemplary research
instrument, the Diagnostic and Statistical Manual of Mental Disorders (DSM), in
full-text psychological publications. We investigated the relationship between
the citation contexts of the DSM and its status as a valid instrument being
used and described by psychological researchers. We specifically focused on how
this relationship has changed over the DSM's citation histories, especially
through the temporal framework of its versions. We found that a new version of
the DSM is increasingly regarded as a valid instrument after its publication;
this is reflected in various key citation contexts, such as the use of hedges,
attention markers, and the verb profile in sentences where the DSM is cited. We
call this process the re-instrumentalization of the DSM in the space of
scientific publications. Our findings bridge an important gap between
quantitative and qualitative science studies and shed light on an aspect of the
social process of scientific instrument development that is not addressed by
the current qualitative literature.
"
3136,Topic Space Trajectories: A case study on machine learning literature,"  The annual number of publications at scientific venues, for example,
conferences and journals, is growing quickly. Hence, even for researchers it
becomes harder and harder to keep track of research topics and their progress.
In this task, researchers can be supported by automated publication analysis.
Yet, many such methods result in uninterpretable, purely numerical
representations. As an attempt to support human analysts, we present
\emph{topic space trajectories}, a structure that allows for the comprehensible
tracking of research topics. We demonstrate how these trajectories can be
interpreted based on eight different analysis approaches. To obtain
comprehensible results, we employ non-negative matrix factorization as well as
suitable visualization techniques. We show the applicability of our approach on
a publication corpus spanning 50 years of machine learning research from 32
publication venues. Our novel analysis method may be employed for paper
classification, for the prediction of future research topics, and for the
recommendation of fitting conferences and journals for submitting unpublished
work.
"
3137,"A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and
  SARS-CoV-2","  The number of unique terms in the scientific literature used to refer to
either SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase
rapidly despite well-established standardized terms. This high degree of term
variation makes high recall identification of these important entities
difficult. In this manuscript we present an extensive dictionary of terms used
in the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based
approach to iteratively generate new term variants, then locate these variants
in a large text corpus. We compare our dictionary to an extensive collection of
terminological resources, demonstrating that our resource provides a
substantial number of additional terms. We use our dictionary to analyze the
usage of SARS-CoV-2 and COVID-19 terms over time and show that the number of
unique terms continues to grow rapidly. Our dictionary is freely available at
https://github.com/ncbi-nlp/CovidTermVar.
"
3138,Improving Text Relationship Modeling with Artificial Data,"  Data augmentation uses artificially-created examples to support supervised
machine learning, adding robustness to the resulting models and helping to
account for limited availability of labelled data. We apply and evaluate a
synthetic data approach to relationship classification in digital libraries,
generating artificial books with relationships that are common in digital
libraries but not easier inferred from existing metadata. We find that for
classification on whole-part relationships between books, synthetic data
improves a deep neural network classifier by 91%. Further, we consider the
ability of synthetic data to learn a useful new text relationship class from
fully artificial training data.
"
3139,"The Journal Coverage of Web of Science, Scopus and Dimensions: A
  Comparative Analysis","  Traditionally, Web of Science and Scopus have been the two most widely used
databases for bibliometric analyses. However, during the last few years some
new scholarly databases, such as Dimensions, have come up. Several previous
studies have compared different databases, either through a direct comparison
of article coverage or by comparing the citations across the databases. This
article attempts to compare the journal coverage of the three databases: Web of
Science, Scopus and Dimensions. The most recent master journal lists of the
three databases have been used for the purpose of identifying the overlapping
and unique journals covered in the databases. The results indicate that the
databases have significantly different journal coverage, with the Web of
Science being most selective and Dimensions being the most exhaustive. About
99.11% and 96.61% of the journals indexed in Web of Science are also indexed in
Scopus and Dimensions, respectively. Scopus has 96.42% of its indexed journals
also covered by Dimensions. Dimensions database has the most exhaustive
coverage, with 82.22% more journals covered as compared to Web of Science and
48.17% more journals covered as compared to Scopus. We also analysed the
research outputs for 20 highly productive countries for the 2010-2019 period,
as indexed in the three databases, and identified database-induced variations
in research output volume, rank and global share of different countries. In
addition to variations in overall coverage of research output from different
countries, the three databases appear to have differential coverage of
different disciplines.
"
3140,"Comment on `Open is not forever: a study of vanished open access
  journals'","  This is a comment to an article by Laakso, Matthias and Jahn
(arXiv:2008.11933).
"
3141,Growth and dynamics of Econophysics: A bibliometric and network analysis,"  Digitization of publications, advancement in communication technology, and
the availability of bibliographic data have made it easier for the researchers
to study the growth and dynamics of any discipline. We present a study on
""Econophysics"" metadata extracted from the Web of Science managed by the
Clarivate Analytics from 2000-2019. The study highlights the growth and
dynamics of the discipline by measures of a number of publications, citations
on publications, other disciplines contribution, institutions participation,
country-wise spread, etc. We investigate the impact of self-citations on
citations with every five-year interval. Also, we find the contribution of
other disciplines by analyzing the cited references. Results emerged from
micro, meso and macro-level analysis of collaborations show that the
distributions among authors collaboration and affiliations of authors follow a
power law. Thus, very few authors keep producing most of the papers and are
from a few institutions. We find that China is leading in the production of a
number of authors and a number of papers; however, shares more of national
collaboration rather than international, whereas the USA shares more
international collaboration. Finally, we demonstrate the evolution of the
author's collaborations and affiliations networks from 2000-2019. Overall the
analysis reveals the ""small-world"" property of the network with average path
length 5. As a consequence of our analysis, this study can serve as in-depth
knowledge to understand the growth and dynamics of the Econophysics network
both qualitatively and quantitatively.
"
3142,Exhaustive Entity Recognition for Coptic: Challenges and Solutions,"  Entity recognition provides semantic access to ancient materials in the
Digital Humanities: itexposes people and places of interest in texts that
cannot be read exhaustively, facilitates linkingresources and can provide a
window into text contents, even for texts with no translations. Inthis paper we
present entity recognition for Coptic, the language of Hellenistic era Egypt.
Weevaluate NLP approaches to the task and lay out difficulties in applying them
to a low-resource,morphologically complex language. We present solutions for
named and non-named nested en-tity recognition and semi-automatic entity
linking to Wikipedia, relying on robust dependencyparsing, feature-based CRF
models, and hand-crafted knowledge base resources, enabling highaccuracy NER
with orders of magnitude less data than those used for high resource
languages.The results suggest avenues for research on other languages in
similar settings.
"
3143,A SIR epidemic model for citation dynamics,"  The study of citations in the scientific literature crosses the boundaries
between the traditional branches of science and stands on its own as a most
profitable research field dubbed the `science of science'. Although the
understanding of the citation histories of individual papers involves many
intangible factors, the basic assumption that citations beget citations can
explain most features of the empirical citation patterns. Here we use the SIR
epidemic model as a mechanistic model for the citation dynamics of well-cited
papers published in selected journals of the American Physical Society. The
estimated epidemiological parameters offer insight on unknown quantities as the
size of the community that could cite a paper and its ultimate impact on that
community. We find a good, though imperfect, agreement between the rank of the
journals obtained using the epidemiological parameters and the impact factor
rank.
"
3144,"Heavy-tailed distribution of the number of publications within
  scientific journals","  The community of scientists is characterized by their need to publish in
peer-reviewed journals, in an attempt to avoid the ""perish"" side of the famous
maxim. Accordingly, almost all researchers authored some scientific articles.
Scholarly publications represent at least two benefits for the study of the
scientific community as a social group. First, they attest of some form of
relation between scientists (collaborations, mentoring, heritage,...), useful
to determine and analyze social subgroups. Second, most of them are recorded in
large data bases, easily accessible and including a lot of pertinent
information, easing the quantitative and qualitative study of the scientific
community. Understanding the underlying dynamics driving the creation of
knowledge in general, and of scientific publication in particular, in addition
to its interest from the social science point of view, can contribute to
maintaining a high level of research, by identifying good and bad practices in
science. In this manuscript, we attempt to advance this understanding by a
statistical analysis of publications within peer-reviewed journals. Namely, we
show that the distribution of the number of articles published by an author in
a given journal is heavy-tailed, but has lighter tail than a power law.
Moreover, we observe some anomalies in the data that pinpoint underlying
dynamics of the scholarly publication process.
"
3145,"Collective authorship in Ukrainian science: marginal effect or new
  phenomenon?","  One of the features of modern science is the formation of stable large
collaborations of researchers working together within the projects that require
the concentration of huge financial and human resources. Results of such common
work are published in scientific papers by large co-authorship teams that
include sometimes thousands of names. The goal of this work is to study the
influence of such publications on the values of scientometric indicators
calculated for individuals, research groups and science of Ukraine in general.
Bibliometric data related to Ukraine, some academic institutions and selected
individual researchers were collected from Scopus database and used for our
study. It is demonstrated that while the relative share of publications by
collective authors is comparatively small, their presence in a general pool can
lead to statistically significant effects. The obtained results clearly show
that traditional quantitative approaches for research assessment should be
changed in order to take into account this phenomenon. Keywords: collective
authorship, scientometrics, group science, Ukraine.
"
3146,Similarity network fusion for scholarly journals,"  This paper explores intellectual and social proximity among scholarly
journals by using network fusion techniques. Similarities among journals are
initially represented by means of a three-layer network based on co-citations,
common authors and common editors. The information contained in the three
layers is combined by implementing a fused similarity network. Subsequently,
partial distance correlations are adopted for measuring the contribution of
each layer to the structure of the fused network. Finally, the community
morphology of the fused network is explored by using modularity. In the three
fields considered (i.e. economics, information and library sciences and
statistics) the major contribution to the structure of the fused network arises
from editors. This result suggests that the role of editors as gatekeepers of
journals is the most relevant in defining the boundaries of scholarly
communities. As to information and library sciences and statistics, the
clusters of journals reflect sub-field specializations. As to economics,
clusters of journals appear to be better interpreted in terms of alternative
methodological approaches. Thus, the graphs representing the clusters of
journals in the fused network are powerful explorative instruments for
exploring research fields.
"
3147,Software must be recognised as an important output of scholarly research,"  Software now lies at the heart of scholarly research. Here we argue that as
well as being important from a methodological perspective, software should, in
many instances, be recognised as an output of research, equivalent to an
academic paper. The article discusses the different roles that software may
play in research and highlights the relationship between software and research
sustainability and reproducibility. It describes the challenges associated with
the processes of citing and reviewing software, which differ from those used
for papers. We conclude that whilst software outputs do not necessarily fit
comfortably within the current publication model, there is a great deal of
positive work underway that is likely to make an impact in addressing this.
"
3148,"Linking Publications to Funding at Project Level: A curated dataset of
  publications reported by FP7 projects","  Datasets explicitly linking publications to funding at project level are the
basis of evaluative bibliometric analysis of funding programmes. Analysis of
the impact of the EU funding programmes has been often frustrated by the lack
of data on publications to which the funding has contributed. Here we present a
dataset 2 of scholarly publications reported by the projects funded by the
European Union under the 7th Framework Programme. The dataset was created by
first consolidating data from different reporting channels and validating the
records by systematically matching them to external authoritative sources and
assigning them external identifiers.
  The initial dataset had 299.000 records linked to one or more projects out of
which 68% had a digital object identify (doi). Through the data quality
assurance, we validate 92% of the initial records (277000) and assign a doi to
89% of them of them (267000). The resulting dataset has 240000 unique dois. It
is, to our knowledge, the first comprehensive and curated dataset of scholarly
outputs of the Framework Programme.
  The dataset could only be created thanks to significant improvements and
investments made in the reporting systems used by EU funded projects.
  The dataset is available on zenodo: https://doi.org/10.5281/zenodo.4275528
"
3149,"In Search of Outstanding Research Advances: Prototyping the creation of
  an open dataset of ""editorial highlights""","  A long-standing research question in bibliometrics is how one identifies
publications, which represent major advances in their fields, making high
impact in there and other areas. In this context, the term ""Breakthrough"" is
often used and commonly used approaches rely on citation links between
publications implicitly positing that peers who use or build upon previously
published results collectively inform about their standing in terms of
advancing the research frontiers.
  Here we argue that the ""Breakthrough"" concept is rooted in the Kuhnian model
of scientific revolution which has been both conceptually and empirically
challenged. A more fruitful approach is to consider various ways in which
authoritative actors in scholarly communication system signal the importance of
research results. We bring to discussions different ""recognition channels"" and
pilot the creation of an open dataset of editorial highlights from regular
lists of notable research advances. The dataset covers the last ten years and
includes: the ""discoveries of the year"" from Science magazine and La Recherche
and weekly editorial highlights from Nature (""research highlights"") and Science
(""editor's choice""). The final dataset includes 230 entries in the ""discoveries
of the years"" (with over 720 references) and about 9,000 weekly highlights
(with over 8,000 references).
"
3150,"Deep Learning -- A first Meta-Survey of selected Reviews across
  Scientific Disciplines and their Research Impact","  Deep learning belongs to the field of artificial intelligence, where machines
perform tasks that typically require some kind of human intelligence. Deep
learning tries to achieve this by mimicking the learning of a human brain.
Similar to the basic structure of a brain, which consists of (billions of)
neurons and connections between them, a deep learning algorithm consists of an
artificial neural network, which resembles the biological brain structure.
Mimicking the learning process of humans with their senses, deep learning
networks are fed with (sensory) data, like texts, images, videos or sounds.
These networks outperform the state-of-the-art methods in different tasks and,
because of this, the whole field saw an exponential growth during the last
years. This growth resulted in way over 10 000 publications per year in the
last years. For example, the search engine PubMed alone, which covers only a
sub-set of all publications in the medical field, provides over 11 000 results
for the search term $'$deep learning$'$ in Q3 2020, and ~90% of these results
are from the last three years. Consequently, a complete overview over the field
of deep learning is already impossible to obtain and, in the near future, it
will potentially become difficult to obtain an overview over a subfield.
However, there are several review articles about deep learning, which are
focused on specific scientific fields or applications, for example deep
learning advances in computer vision or in specific tasks like object
detection. With these surveys as a foundation, the aim of this contribution is
to provide a first high-level, categorized meta-analysis of selected reviews on
deep learning across different scientific disciplines and outline the research
impact that they already have during a short period of time.
"
3151,"A bibliometric methodology to unveil territorial inequities in the
  scientific wealth to combat COVID-19","  In this paper we develop a methodology to assess the scientific wealth of
territories at field level. Our methodology uses a bibliometric approach based
on the observation of academic research performance and overall scientific
production in each territory. We apply it to assess disparities in the Italian
territories in the medical specialties at the front line of the COVID-19
emergency. Italy has been the first among western countries to be severely
affected by the onset of the COVID-19 pandemic. The study reveals remarkable
inequities across territories, with scientific weaknesses concentrated in the
south. Policies for rebalancing the north-south divide should also consider, in
addition to tangible assets, the gap in production and availability of quality
medical knowledge.
"
3152,"Are University Rankings Statistically Significant? A Comparison among
  Chinese Universities and with the USA","  Purpose: We address the question of whether differences are statistically
significant in the rankings of universities. We propose methods measuring the
statistical significance among different universities and illustrate the
results by empirical data. Design/methodology/approach: Based on z-testing and
overlapping confidence intervals, and using data about 205 Chinese universities
included in the Leiden Rankings 2020, we argue that three main groups of
Chinese research universities can be distinguished.
  Findings: When the sample of 205 Chinese universities is merged with the 197
US universities included in Leiden Rankings 2020, the results similarly
indicate three main groups: high, middle, low. Using this data (Leiden Rankings
and Web-of-Science), the z-scores of the Chinese universities are significantly
below those of the US universities albeit with some overlap.
  Research limitations: We show empirically that differences in ranking may be
due to changes in the data, the models, or the modeling effects on the data.
The scientometric groupings are not always stable when we use different
methods.
  R&D policy implications: Differences among universities can be tested for
their statistical significance. The statistics relativize the values of
decimals in the rankings. One can operate with a scheme of low/middle/high in
policy debates and leave the more fine-grained rankings of individual
universities to operational management and local settings.
  Originality/value: In the discussion about the rankings of universities, the
question of whether differences are statistically significant, is, in our
opinion, insufficiently addressed.
"
3153,A Tale of Two Referees,"  Success in academia hinges on publishing in top tier journals. This requires
innovative results. And this requires clear and convincing presentation of said
results. Presentation can make the difference of one tier in journal level. A
lot of useful advice on this topic is available online from well-respected
outlets; see, for example, El-Omar (2014); Gould (2014); Neiles et al. (2015);
Notz and Kafadar (2011); or Sachdeva (2020). This text provides a different
angle.
"
3154,"A large-scale comparison of social media coverage and mentions captured
  by the two altmetric aggregators- Altmetric.com and PlumX","  The increased social media attention to scholarly articles has resulted in
efforts to create platforms & services to track and measure the social media
transactions around scholarly articles in different social platforms (such as
Twitter, Blog, Facebook) and academic social networks (such as Mendeley,
Academia and ResearchGate). Altmetric.com and PlumX are two popular aggregators
that track social media activity around scholarly articles from a variety of
social platforms and provide the coverage and transaction data to researchers
for various purposes. However, some previous studies have shown that the social
media data captured by the two aggregators have differences in terms of
coverage and magnitude of mentions. This paper aims to revisit the question by
doing a large-scale analysis of social media mentions of a data sample of
1,785,149 publication records (drawn from multiple disciplines, demographies,
publishers). Results obtained show that PlumX tracks more wide sources and more
articles as compared to Altmetric.com. However, the coverage and average
mentions of the two aggregators vary across different social media platforms,
with Altmetric.com recording higher mentions in Twitter and Blog, and PlumX
recording higher mentions in Facebook and Mendeley, for the same set of
articles. The coverage and average mentions captured by the two aggregators
across different document types, disciplines and publishers is also analyzed.
"
3155,"Do 'altmetric mentions' follow Power Laws? Evidence from social media
  mention data in Altmetric.com","  Power laws are a characteristic distribution that are ubiquitous, in that
they are found almost everywhere, in both natural as well as in man-made
systems. They tend to emerge in large, connected and self-organizing systems,
for example, scholarly publications. Citations to scientific papers have been
found to follow a power law, i.e., the number of papers having a certain level
of citation x are proportional to x raised to some negative power. The
distributional character of altmetrics has not been studied yet as altmetrics
are among the newest indicators related to scholarly publications. Here we
select a data sample from the altmetrics aggregator Altmetrics.com containing
records from the platforms Facebook, Twitter, News, Blogs, etc., and the
composite variable Alt-score for the period 2016. The individual and the
composite data series of 'mentions' on the various platforms are fit to a power
law distribution, and the parameters and goodness of fit determined using least
squares regression. The log-log plot of the data, 'mentions' vs. number of
papers, falls on an approximately linear line, suggesting the plausibility of a
power law distribution. The fit is not very good in all cases due to large
fluctuations in the tail. We show that fit to the power law can be improved by
truncating the data series to eliminate large fluctuations in the tail. We
conclude that altmetric distributions also follow power laws with a fairly good
fit over a wide range of values. More rigorous methods of determination may not
be necessary at present.
"
3156,"The 80-year development of Vietnam mathematical research: Preliminary
  insights from the SciMath database on mathematicians, their works and their
  networks","  Starting with the first international publication of Le Van Thiem in 1947,
modern mathematics in Vietnam is a longstanding research field. However, what
is known about its development usually comes from discrete essays such as
anecdotes or interviews of renowned mathematicians. We introduce SciMath-a
database on publications of Vietnamese mathematicians. To ensure this database
covers as many publications as possible, data entries are manually collected
from scientists' publication records, journals' websites, universities, and
research institutions. Collected data went through various verification steps
to ensure data quality and minimize errors. At the time of this report, the
database covered 8372 publications, profiles of 1566 Vietnamese, and 1492
foreign authors since 1947. We found a growing capability in mathematics
research in Vietnam in various aspects: scientific output, publications on
influential journals, or collaboration. The database and preliminary results
were presented to the Scientific Council of Vietnam Institute for Advanced
Study in Mathematics (VIASM) on November 13th, 2020.
"
3157,"Science User Scenarios for a Virtual Observatory Design Reference
  Mission: Science Requirements for Data Mining","  The knowledge discovery potential of the new large astronomical databases is
vast. When these are used in conjunction with the rich legacy data archives,
the opportunities for scientific discovery multiply rapidly. A Virtual
Observatory (VO) framework will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed. The
development of a VO should address the data mining needs of various
astronomical research constituencies. By way of example, two user scenarios are
presented which invoke applications and linkages of data across the catalog and
image domains in order to address specific astrophysics research problems.
These illustrate a subset of the desired capabilities and power of the VO, and
as such they represent potential components of a VO Design Reference Mission.
"
3158,Data Mining in Astronomical Databases,"  A Virtual Observatory (VO) will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed.
"
3159,The Future of Technical Libraries,"  Technical libraries are currently experiencing very rapid change. In the near
future their mission will change, their physical nature will change, and the
skills of their employees will change. While some will not be able to make
these changes, and will fail, others will lead us into a new era.
"
3160,CoRR: A Computing Research Repository,"  Discusses how CoRR was set up and some policy issues involved with setting up
such a repository.
"
3161,A response to the commentaries on CoRR,"  This is a response to the commentaries on ""CoRR: A Computing Research
Repository"".
"
3162,"Centroid-based summarization of multiple documents: sentence extraction,
  utility-based evaluation, and user studies","  We present a multi-document summarizer, called MEAD, which generates
summaries using cluster centroids produced by a topic detection and tracking
system. We also describe two new techniques, based on sentence utility and
subsumption, which we have applied to the evaluation of both single and
multiple document summaries. Finally, we describe two user studies that test
our models of multi-document summarization.
"
3163,Using compression to identify acronyms in text,"  Text mining is about looking for patterns in natural language text, and may
be defined as the process of analyzing text to extract information from it for
particular purposes. In previous work, we claimed that compression is a key
technology for text mining, and backed this up with a study that showed how
particular kinds of lexical tokens---names, dates, locations, etc.---can be
identified and located in running text, using compression models to provide the
leverage necessary to distinguish different token types (Witten et al., 1999)
"
3164,A usage based analysis of CoRR,"  Based on an empirical analysis of author usage of CoRR, and of its
predecessor in the Los Alamos eprint archives, it is shown that CoRR has not
yet been able to match the early growth of the Los Alamos physics archives.
Some of the reasons are implicit in Halpern's paper, and we explore them
further here. In particular we refer to the need to promote CoRR more
effectively for its intended community - computer scientists in universities,
industrial research labs and in government. We take up some points of detail on
this new world of open archiving concerning central versus distributed
self-archiving, publication, the restructuring of the journal publishers'
niche, peer review and copyright.
"
3165,Combining Linguistic and Spatial Information for Document Analysis,"  We present a framework to analyze color documents of complex layout. In
addition, no assumption is made on the layout. Our framework combines in a
content-driven bottom-up approach two different sources of information: textual
and spatial. To analyze the text, shallow natural language processing tools,
such as taggers and partial parsers, are used. To infer relations of the
logical layout we resort to a qualitative spatial calculus closely related to
Allen's calculus. We evaluate the system against documents from a color journal
and present the results of extracting the reading order from the journal's
pages. In this case, our analysis is successful as it extracts the intended
reading order from the document.
"
3166,"Open Archives Initiative protocol development and implementation at
  arXiv","  I outline the involvement of the Los Alamos e-print archive (arXiv) within
the Open Archives Initiative (OAI) and describe the implementation of the data
provider side of the OAI protocol v1.0. I highlight the ways in which we map
the existing structure of arXiv onto elements of the protocol.
"
3167,The OLAC Metadata Set and Controlled Vocabularies,"  As language data and associated technologies proliferate and as the language
resources community rapidly expands, it has become difficult to locate and
reuse existing resources. Are there any lexical resources for such-and-such a
language? What tool can work with transcripts in this particular format? What
is a good format to use for linguistic data of this type? Questions like these
dominate many mailing lists, since web search engines are an unreliable way to
find language resources. This paper describes a new digital infrastructure for
language resource discovery, based on the Open Archives Initiative, and called
OLAC -- the Open Language Archives Community. The OLAC Metadata Set and the
associated controlled vocabularies facilitate consistent description and
focussed searching. We report progress on the metadata set and controlled
vocabularies, describing current issues and soliciting input from the language
resources community.
"
3168,"Exposing and harvesting metadata using the OAI metadata harvesting
  protocol: A tutorial","  In this article I outline the ideas behind the Open Archives Initiative
metadata harvesting protocol (OAIMH), and attempt to clarify some common
misconceptions. I then consider how the OAIMH protocol can be used to expose
and harvest metadata. Perl code examples are given as practical illustration.
"
3169,"Semantic Web Content Accessibility Guidelines for Current Research
  Information Systems (CRIS)","  The most exciting challenge for CRIS is to create a service for research
information which should be wide-spread, distributed and actual like Google,
but at the same time structured, trusted, with a complex search and navigation
similar to today CRIS application. The core technology for such a ""new"" CRIS is
the semantic web technology to integrate database contents with HTML and XML
web pages for being provided to the research interested public. One (at the
moment the best) possible way is to use RDF (Resource Description Framework)
which is also recommended by the W3 consortium.
"
3170,Links tell us about lexical and semantic Web content,"  The latest generation of Web search tools is beginning to exploit hypertext
link information to improve ranking\cite{Brin98,Kleinberg98} and
crawling\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden
assumption behind such approaches, a correlation between the graph structure of
the Web and its content, has not been tested explicitly despite increasing
research on Web topology\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I
formalize and quantitatively validate two conjectures drawing connections from
link information to lexical and semantic Web content. The clink-content
conjecture states that a page is similar to the pages that link to it, i.e.,
one can infer the lexical content of a page by looking at the pages that link
to it. I also show that lexical inferences based on link cues are quite
heterogeneous across Web communities. The link-cluster conjecture states that
pages about the same topic are clustered together, i.e., one can infer the
meaning of a page by looking at its neighbours. These results explain the
success of the newest search technologies and open the way for more dynamic and
scalable methods to locate information in a topic or user driven way.
"
3171,The Open Language Archives Community and Asian Language Resources,"  The Open Language Archives Community (OLAC) is a new project to build a
worldwide system of federated language archives based on the Open Archives
Initiative and the Dublin Core Metadata Initiative. This paper aims to
disseminate the OLAC vision to the language resources community in Asia, and to
show language technologists and linguists how they can document their tools and
data in such a way that others can easily discover them. We describe OLAC and
the OLAC Metadata Set, then discuss two key issues in the Asian context:
language classification and multilingual resource classification.
"
3172,Information retrieval in Current Research Information Systems,"  In this paper we describe the requirements for research information systems
and problems which arise in the development of such system. Here is shown which
problems could be solved by using of knowledge markup technologies. Ontology
for Research Information System offered. Architecture for collecting research
data and providing access to it is described.
"
3173,"The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data","  The SkyServer provides Internet access to the public Sloan Digital Sky Survey
(SDSS) data for both astronomers and for science education. This paper
describes the SkyServer goals and architecture. It also describes our
experience operating the SkyServer on the Internet. The SDSS data is public and
well-documented so it makes a good test platform for research on database
algorithms and performance.
"
3174,Using Structural Metadata to Localize Experience of Digital Content,"  With the increasing technical sophistication of both information consumers
and providers, there is increasing demand for more meaningful experiences of
digital information. We present a framework that separates digital object
experience, or rendering, from digital object storage and manipulation, so the
rendering can be tailored to particular communities of users. Our framework
also accommodates extensible digital object behaviors and interoperability. The
two key components of our approach are 1) exposing structural metadata
associated with digital objects -- metadata about the labeled access points
within a digital object and 2) information intermediaries called context
brokers that match structural characteristics of digital objects with
mechanisms that produce behaviors. These context brokers allow for localized
rendering of digital information stored externally.
"
3175,"Core Services in the Architecture of the National Digital Library for
  Science Education (NSDL)","  We describe the core components of the architecture for the (NSDL) National
Science, Mathematics, Engineering, and Technology Education Digital Library.
Over time the NSDL will include heterogeneous users, content, and services. To
accommodate this, a design for a technical and organization infrastructure has
been formulated based on the notion of a spectrum of interoperability. This
paper describes the first phase of the interoperability infrastructure
including the metadata repository, search and discovery services, rights
management services, and user interface portal facilities.
"
3176,Components of an NSDL Architecture: Technical Scope and Functional Model,"  We describe work leading toward specification of a technical architecture for
the National Science, Mathematics, Engineering, and Technology Education
Digital Library (NSDL). This includes a technical scope and a functional model,
with some elaboration on the particularly rich set of library services that
NSDL is expected eventually to encompass.
"
3177,The SDSS SkyServer: Public Access to the Sloan Digital Sky Server Data,"  The SkyServer provides Internet access to the public Sloan Digi-tal Sky
Survey (SDSS) data for both astronomers and for science education. This paper
describes the SkyServer goals and archi-tecture. It also describes our
experience operating the SkyServer on the Internet. The SDSS data is public and
well-documented so it makes a good test platform for research on database
algorithms and performance.
"
3178,Data Mining the SDSS SkyServer Database,"  An earlier paper (Szalay et. al. ""Designing and Mining MultiTerabyte
Astronomy Archives: The Sloan Digital Sky Survey,"" ACM SIGMOD 2000) described
the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty
database queries and twelve data visualization tasks that a good data
management system should support. We built a database and interfaces to support
both the query load and also a website for ad-hoc access. This paper reports on
the database design, describes the data loading pipeline, and reports on the
query implementation and performance. The queries typically translated to a
single SQL statement. Most queries run in less than 20 seconds, allowing
scientists to interactively explore the database. This paper is an in-depth
tour of those queries. Readers should first have studied the companion overview
paper Szalay et. al. ""The SDSS SkyServer, Public Access to the Sloan Digital
Sky Server Data"" ACM SIGMOND 2002.
"
3179,The structure of broad topics on the Web,"  The Web graph is a giant social network whose properties have been measured
and modeled extensively in recent years. Most such studies concentrate on the
graph structure alone, and do not consider textual properties of the nodes.
Consequently, Web communities have been characterized purely in terms of graph
structure and not on page content. We propose that a topic taxonomy such as
Yahoo! or the Open Directory provides a useful framework for understanding the
structure of content-based clusters and communities. In particular, using a
topic taxonomy and an automatic classifier, we can measure the background
distribution of broad topics on the Web, and analyze the capability of recent
random walk algorithms to draw samples which follow such distributions. In
addition, we can measure the probability that a page about one broad topic will
link to another broad topic. Extending this experiment, we can measure how
quickly topic context is lost while walking randomly on the Web graph.
Estimates of this topic mixing distance may explain why a global PageRank is
still meaningful in the context of broad queries. In general, our measurements
may prove valuable in the design of community-specific crawlers and link-based
ranking systems.
"
3180,"Seven Dimensions of Portability for Language Documentation and
  Description","  The process of documenting and describing the world's languages is undergoing
radical transformation with the rapid uptake of new digital technologies for
capture, storage, annotation and dissemination. However, uncritical adoption of
new tools and technologies is leading to resources that are difficult to reuse
and which are less portable than the conventional printed resources they
replace. We begin by reviewing current uses of software tools and digital
technologies for language documentation and description. This sheds light on
how digital language documentation and description are created and managed,
leading to an analysis of seven portability problems under the following
headings: content, format, discovery, access, citation, preservation and
rights. After characterizing each problem we provide a series of value
statements, and this provides the framework for a broad range of best practice
recommendations.
"
3181,"A Scalable Architecture for Harvest-Based Digital Libraries - The
  ODU/Southampton Experiments","  This paper discusses the requirements of current and emerging applications
based on the Open Archives Initiative (OAI) and emphasizes the need for a
common infrastructure to support them. Inspired by HTTP proxy, cache, gateway
and web service concepts, a design for a scalable and reliable infrastructure
that aims at satisfying these requirements is presented. Moreover it is shown
how various applications can exploit the services included in the proposed
infrastructure. The paper concludes by discussing the current status of several
prototype implementations.
"
3182,"Using the Annotated Bibliography as a Resource for Indicative
  Summarization","  We report on a language resource consisting of 2000 annotated bibliography
entries, which is being analyzed as part of our research on indicative document
summarization. We show how annotated bibliographies cover certain aspects of
summarization that have not been well-covered by other summary corpora, and
motivate why they constitute an important form to study for information
retrieval. We detail our methodology for collecting the corpus, and overview
our document feature markup that we introduced to facilitate summary analysis.
We present the characteristics of the corpus, methods of collection, and show
its use in finding the distribution of types of information included in
indicative summaries and their relative ordering within the summaries.
"
3183,TerraService.NET: An Introduction to Web Services,"  This article explores the design and construction of a geo-spatial Internet
web service application from the host web site perspective and from the
perspective of an application using the web service. The TerraService.NET web
service was added to the popular TerraServer database and web site with no
major structural changes to the database. The article discusses web service
design, implementation, and deployment concepts and design guidelines. Web
services enable applications that aggregate and interact with information and
resources from Internet-scale distributed servers. The article presents the
design of two USDA applications that interoperate with database and web service
resources in Fort Collins Colorado and the TerraService web service located in
Tukwila Washington.
"
3184,"Online Scientific Data Curation, Publication, and Archiving","  Science projects are data publishers. The scale and complexity of current and
future science data changes the nature of the publication process. Publication
is becoming a major project component. At a minimum, a project must preserve
the ephemeral data it gathers. Derived data can be reconstructed from metadata,
but metadata is ephemeral. Longer term, a project should expect some archive to
preserve the data. We observe that pub-lished scientific data needs to be
available forever ? this gives rise to the data pyramid of versions and to data
inflation where the derived data volumes explode. As an example, this article
describes the Sloan Digital Sky Survey (SDSS) strategies for data publication,
data access, curation, and preservation.
"
3185,Web Services for the Virtual Observatory,"  Web Services form a new, emerging paradigm to handle distributed access to
resources over the Internet. There are platform independent standards (SOAP,
WSDL), which make the developers? task considerably easier. This article
discusses how web services could be used in the context of the Virtual
Observatory. We envisage a multi-layer architecture, with interoperating
services. A well-designed lower layer consisting of simple, standard services
implemented by most data providers will go a long way towards establishing a
modular architecture. More complex applications can be built upon this core
layer. We present two prototype applications, the SdssCutout and the SkyQuery
as examples of this layered architecture.
"
3186,A Virtual Library of Technical Publications,"  Through a collaborative effort, the Fermilab Information Resources Department
and Computing Division have created a ""virtual library"" of technical
publications that provides public access to electronic full-text documents.
This paper will discuss the vision, planning and milestones of the project, as
well as the hardware, software and interdepartmental cooperation components.
"
3187,"Reconciling MPEG-7 and MPEG-21 Semantics through a Common Event-Aware
  Metadata Model","  The ""event"" concept appears repeatedly when developing metadata models for
the description and management of multimedia content. During the typical life
cycle of multimedia content, events occur at many different levels - from the
events which happen during content creation (directing, acting, camera panning
and zooming) to the events which happen to the physical form (acquisition,
relocation, damage of film or video) to the digital conversion, reformatting,
editing and repackaging events, to the events which are depicted in the actual
content (political, news, sporting) to the usage, ownership and copyright
agreement events and even the metadata attribution events. Support is required
within both MPEG-7 and MPEG-21 for the clear and unambiguous description of all
of these event types which may occur at widely different levels of nesting and
granularity. In this paper we first describe an event-aware model (the ABC
model) which is capable of modeling and yet clearly differentiating between all
of these, often recursive and overlapping events. We then illustrate how this
model can be used as the foundation to facilitate semantic interoperability
between MPEG-7 and MPEG-21. By expressing the semantics of both MPEG-7 and
MPEG-21 metadata terms in RDF Schema (and some DAML+OIL extensions) and
attaching the MPEG-7 and MPEG-21 class and property hierarchies to the
appropriate top-level classes and properties of the ABC model, we are
essentially able to define a single distributed machine-understandable
ontology, which will enable interoperability of data and services across the
entire multimedia content delivery chain.
"
3188,"Integration and interoperability accessing electronic information
  resources in science and technology: the proposal of Brazilian Digital
  Library","  This paper describes technological and methodological options to achieve
interoperability in accessing electronic information resources, available in
Internet, in the scope of Brazilian Digital Library in Science and Technology
Project - BDL, developed by Brazilian Institute for Scientific and Technical
Information - IBICT. It stresses the impact of the Web in the publishing and
communication processes in science and technology and also in the information
systems and libraries. The work points out the two major objectives of the BDL
Project: facilitates electronic publishing of different full text materials
such as theses, journal articles, conference papers,grey literature - by
Brazilian scientific community, so amplifying their nationally and
internationally visibility; and achieving, through a unified gateway, thus
avoiding a user to navigate and query across different information resources
individually. The work explains technological options and standards that will
assure interoperability in this context.
"
3189,"An Approach to Automatic Indexing of Scientific Publications in High
  Energy Physics for Database SPIRES HEP","  We introduce an approach to automatic indexing of e-prints based on a
pattern-matching technique making extensive use of an Associative Patterns
Dictionary (APD), developed by us. Entries in the APD consist of natural
language phrases with the same semantic interpretation as a set of keywords
from a controlled vocabulary. The method also allows to recognize within
e-prints formulae written in TeX notations that might also appear as keywords.
We present an automatic indexing system, AUTEX, which we have applied to
keyword index e-prints in selected areas in high energy physics (HEP) making
use of the DESY-HEPI thesaurus as a controlled vocabulary.
"
3190,Building an Open Language Archives Community on the OAI Foundation,"  The Open Language Archives Community (OLAC) is an international partnership
of institutions and individuals who are creating a worldwide virtual library of
language resources. The Dublin Core (DC) Element Set and the OAI Protocol have
provided a solid foundation for the OLAC framework. However, we need more
precision in community-specific aspects of resource description than is offered
by DC. Furthermore, many of the institutions and individuals who might
participate in OLAC do not have the technical resources to support the OAI
protocol. This paper presents our solutions to these two problems. To address
the first, we have developed an extensible application profile for language
resource metadata. To address the second, we have implemented Vida (the virtual
data provider) and Viser (the virtual service provider), which permit community
members to provide data and services without having to implement the OAI
protocol. These solutions are generic and could be adopted by other specialized
subcommunities.
"
3191,Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS,"  The LOCKSS project has developed and deployed in a world-wide test a
peer-to-peer system for preserving access to journals and other archival
information published on the Web. It consists of a large number of independent,
low-cost, persistent web caches that cooperate to detect and repair damage to
their content by voting in ""opinion polls."" Based on this experience, we
present a design for and simulations of a novel protocol for voting in systems
of this kind. It incorporates rate limitation and intrusion detection to ensure
that even some very powerful adversaries attacking over many years have only a
small probability of causing irrecoverable damage before being detected.
"
3192,A Digital Preservation Appliance Based on OpenBSD,"  The LOCKSS program has developed and deployed in a world-wide test a system
for preserving access to academic journals published on the Web. The
fundamental problem for any digital preservation system is that it must be
affordable for the long term. To reduce the cost of ownership, the LOCKSS
system uses generic PC hardware, open source software, and peer-to-peer
technology. It is packaged as a ``network appliance'', a single-function box
that can be connected to the Internet, configured and left alone to do its job
with minimal monitoring or administration. The first version of this system was
based on a Linux boot floppy. After three years of testing it was replaced by a
second version, based on OpenBSD and booting from CD-ROM.
  We focus in this paper on the design, implementation and deployment of a
network appliance based on an open source operating system. We provide an
overview of the LOCKSS application and describe the experience of deploying and
supporting its first version. We list the requirements we took from this to
drive the design of the second version, describe how we satisfied them in the
OpenBSD environment, and report on the initial
"
3193,The NPC Framework for Building Information Dissemination Networks,"  Numerous systems for dissemination, retrieval, and archiving of documents
have been developed in the past. Those systems often focus on one of these
aspects and are hard to extend and combine. Typically, the transmission
protocols, query and filtering languages are fixed as well as the interfaces to
other systems. We rather envisage the seamless establishment of networks among
the providers, repositories and consumers of information, supporting
information retrieval and dissemination while being highly interoperable and
extensible.
  We propose a framework with a single event-based mechanism that unifies
document storage, retrieval, and dissemination. This framework offers complete
openness with respect to document and metadata formats, transmission protocols,
and filtering mechanisms. It specifies a high-level building kit, by which
arbitrary processors for document streams can be incorporated to support the
retrieval, transformation, aggregation and disaggregation of documents. Using
the same kit, interfaces for different transmission protocols can be added
easily to enable the communication with various information sources and
information consumers.
"
3194,Developing Open Data Models for Linguistic Field Data,"  The UQ Flint Archive houses the field notes and elicitation recordings made
by Elwyn Flint in the 1950's and 1960's during extensive linguistic survey work
across Queensland, Australia.
  The process of digitizing the contents of the UQ Flint Archive provides a
number of interesting challenges in the context of EMELD. Firstly, all of the
linguistic data is for languages which are either endangered or extinct, and as
such forms a valuable ethnographic repository. Secondly, the physical format of
the data is itself in danger of decline, and as such digitization is an
important preservation task in the short to medium term. Thirdly, the adoption
of open standards for the encoding and presentation of text and audio data for
linguistic field data, whilst enabling preservation, represents a new field of
research in itself where best practice has yet to be formalised. Fourthly, the
provision of this linguistic data online as a new data source for future
research introduces concerns of data portability and longevity.
  This paper will outline the origins of the data model, the content creation
components, presentation forms based on the data model, data capture tools and
media conversion components. It will also address some of the larger questions
regarding the digitization and annotation of linguistic field work based on
experience gained through work with the Flint Archive contents.
"
3195,"The Open Language Archives Community: An infrastructure for distributed
  archiving of language resources","  New ways of documenting and describing language via electronic media coupled
with new ways of distributing the results via the World-Wide Web offer a degree
of access to language resources that is unparalleled in history. At the same
time, the proliferation of approaches to using these new technologies is
causing serious problems relating to resource discovery and resource creation.
This article describes the infrastructure that the Open Language Archives
Community (OLAC) has built in order to address these problems. Its technical
and usage infrastructures address problems of resource discovery by
constructing a single virtual library of distributed resources. Its governance
infrastructure addresses problems of resource creation by providing a mechanism
through which the language-resource community can express its consensus on
recommended best practices.
"
3196,Eprints and the Open Archives Initiative,"  The Open Archives Initiative (OAI) was created as a practical way to promote
interoperability between eprint repositories. Although the scope of the OAI has
been broadened, eprint repositories still represent a significant fraction of
OAI data providers. In this article I present a brief survey of OAI eprint
repositories, and of services using metadata harvested from eprint repositories
using the OAI protocol for metadata harvesting (OAI-PMH). I then discuss
several situations where metadata harvesting may be used to further improve the
utility of eprint archives as a component of the scholarly communication
infrastructure.
"
3197,Limit groups and groups acting freely on $\bbR^n$-trees,"  We give a simple proof of the finite presentation of Sela's limit groups by
using free actions on $\bbR^n$-trees. We first prove that Sela's limit groups
do have a free action on an $\bbR^n$-tree. We then prove that a finitely
generated group having a free action on an $\bbR^n$-tree can be obtained from
free abelian groups and surface groups by a finite sequence of free products
and amalgamations over cyclic groups. As a corollary, such a group is finitely
presented, has a finite classifying space, its abelian subgroups are finitely
generated and contains only finitely many conjugacy classes of non-cyclic
maximal abelian subgroups.
"
3198,"Extending Dublin Core Metadata to Support the Description and Discovery
  of Language Resources","  As language data and associated technologies proliferate and as the language
resources community expands, it is becoming increasingly difficult to locate
and reuse existing resources. Are there any lexical resources for such-and-such
a language? What tool works with transcripts in this particular format? What is
a good format to use for linguistic data of this type? Questions like these
dominate many mailing lists, since web search engines are an unreliable way to
find language resources. This paper reports on a new digital infrastructure for
discovering language resources being developed by the Open Language Archives
Community (OLAC). At the core of OLAC is its metadata format, which is designed
to facilitate description and discovery of all kinds of language resources,
including data, tools, or advice. The paper describes OLAC metadata, its
relationship to Dublin Core metadata, and its dissemination using the metadata
harvesting protocol of the Open Archives Initiative.
"
3199,EqRank: A Self-Consistent Equivalence Relation on Graph Vertexes,"  A new method of hierarchical clustering of graph vertexes is suggested. In
the method, the graph partition is determined with an equivalence relation
satisfying a recursive definition stating that vertexes are equivalent if the
vertexes they point to (or vertexes pointing to them) are equivalent. Iterative
application of the partitioning yields a hierarchical clustering of graph
vertexes. The method is applied to the citation graph of hep-th. The outcome is
a two-level classification scheme for the subject field presented in hep-th,
and indexing of the papers from hep-th in this scheme. A number of tests show
that the classification obtained is adequate.
"
3200,"Semi-metric Behavior in Document Networks and its Application to
  Recommendation Systems","  Recommendation systems for different Document Networks (DN) such as the World
Wide Web (WWW) and Digital Libraries, often use distance functions extracted
from relationships among documents and keywords. For instance, documents in the
WWW are related via a hyperlink network, while documents in bibliographic
databases are related by citation and collaboration networks. Furthermore,
documents are related to keyterms. The distance functions computed from these
relations establish associative networks among items of the DN, referred to as
Distance Graphs, which allow recommendation systems to identify relevant
associations for individual users. However, modern recommendation systems need
to integrate associative data from multiple sources such as different
databases, web sites, and even other users. Thus, we are presented with a
problem of combining evidence (about associations between items) from different
sources characterized by distance functions. In this paper we describe our work
on (1) inferring relevant associations from, as well as characterizing,
semi-metric distance graphs and (2) combining evidence from different distance
graphs in a recommendation system. Regarding (1), we present the idea of
semi-metric distance graphs, and introduce ratios to measure semi-metric
behavior. We compute these ratios for several DN such as digital libraries and
web sites and show that they are useful to identify implicit associations.
Regarding (2), we describe an algorithm to combine evidence from distance
graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy
Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed
for a recommendation system named TalkMine.
"
3201,Efficient Algorithms for Citation Network Analysis,"  In the paper very efficient, linear in number of arcs, algorithms for
determining Hummon and Doreian's arc weights SPLC and SPNP in citation network
are proposed, and some theoretical properties of these weights are presented.
The nonacyclicity problem in citation networks is discussed. An approach to
identify on the basis of arc weights an important small subnetwork is proposed
and illustrated on the citation networks of SOM (self organizing maps)
literature and US patents.
"
3202,A Dynamic Programming Algorithm for the Segmentation of Greek Texts,"  In this paper we introduce a dynamic programming algorithm to perform linear
text segmentation by global minimization of a segmentation cost function which
consists of: (a) within-segment word similarity and (b) prior information about
segment length. The evaluation of the segmentation accuracy of the algorithm on
a text collection consisting of Greek texts showed that the algorithm achieves
high segmentation accuracy and appears to be very innovating and promissing.
"
3203,On The Cost Distribution of a Memory Bound Function,"  Memory Bound Functions have been proposed for fighting spam, resisting Sybil
attacks and other purposes. A particular implementation of such functions has
been proposed in which the average effort required to generate a proof of
effort is set by parameters E and l to E * l. The distribution of effort
required to generate an individual proof about this average is fairly broad.
When particular uses of these functions are envisaged, the choice of E and l,
and the system design surrounding the generation and verification of proofs of
effort, need to take the breadth of the distribution into account.
  We show the distribution for this implementation, discuss the system design
issues in the context of two proposed applications, and suggest an improved
implementation.
"
3204,Make search become the internal function of Internet,"  Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a distributed information retrieval system, which will solve problems like poor
coverage, long update interval in current web search system. The most distinct
character of DRIS is that it's a public opening system, and acts as an internal
component of Internet, but not the production of a company. The implementation
of DRIS is also represented.
"
3205,Copyright and Creativity: Authors and Photographers,"  The history of the occupations ""author"" and ""photographer"" provides an
insightful perspective on copyright and creativity. The concept of the romantic
author, associated with personal creative genius, gained prominence in the
eighteenth century. However, in the U.S. in 1900 only about three thousand
persons professed their occupation to be ""author."" Self-professed
""photographers"" were then about ten times as numerous as authors. Being a
photographer was associated with manufacturing and depended only on mastering
technical skills and making a living. Being an author, in contrast, was an
elite status associated with science and literature. Across the twentieth
century, the number of writers and authors grew much more rapidly than the
number of photographers. The relative success of writers and authors in
creating jobs seems to have depended not on differences in copyright or
possibilities for self-production, but on greater occupational innovation.
Creativity in organizing daily work is an important form of creativity.
"
3206,Designing of a Community-based Translation Center,"  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
"
3207,Evolution: Google vs. DRIS,"  This paper gives an absolute new search system that builds the information
retrieval infrastructure for Internet. Now most search engine companies are
mainly concerned with how to make profit from company users by advertisement
and ranking prominence, but never consider what its real customers will feel.
Few web search engines can sell billions dollars just at the cost of
inconvenience of most Internet users, but not its high quality of search
service. When we have to bear the bothersome advertisements in the awful
results and have no choices, Internet as the kind of public good will surely be
undermined. If current Internet can't fully ensure our right to know, it may
need some sound improvements or a revolution.
"
3208,Using sensors in the web crawling process,"  This paper offers a short description of an Internet information field
monitoring system, which places a special module-sensor on the side of the
Web-server to detect changes in information resources and subsequently
reindexes only the resources signalized by the corresponding sensor. Concise
results of simulation research and an implementation attempt of the given
""sensors"" concept are provided.
"
3209,"Initial Experiences Re-Exporting Duplicate and Similarity Computation
  with an OAI-PMH aggregator","  The proliferation of the Open Archive Initiative Protocol for Metadata
Harvesting (OAI-PMH) has resulted in the creation of a large number of service
providers, all harvesting from either data providers or aggregators. If data
were available regarding the similarity of metadata records, service providers
could track redundant records across harvests from multiple sources as well as
provide additional end-user services. Due to the large number of metadata
formats and the diverse mapping strategies employed by data providers,
similarity calculation requirements necessitate the use of information
retrieval strategies. We describe an OAI-PMH aggregator implementation that
uses the optional ``<about>'' container to re-export the results of similarity
calculations. Metadata records (3751) were harvested from a NASA data provider
and similarities for the records were computed. The results were useful for
detecting duplicates, similarities and metadata errors.
"
3210,Design of a Community-based Translation Center,"  Interfaces that support multi-lingual content can reach a broader community.
We wish to extend the reach of CITIDEL, a digital library for computing
education materials, to support multiple languages. By doing so, we hope that
it will increase the number of users, and in turn the number of resources. This
paper discusses three approaches to translation (automated translation,
developer-based, and community-based), and a brief evaluation of these
approaches. It proposes a design for an online community translation center
where volunteers help translate interface components and educational materials
available in CITIDEL.
"
3211,Automated Resolution of Noisy Bibliographic References,"  We describe a system used by the NASA Astrophysics Data System to identify
bibliographic references obtained from scanned article pages by OCR methods
with records in a bibliographic database. We analyze the process generating the
noisy references and conclude that the three-step procedure of correcting the
OCR results, parsing the corrected string and matching it against the database
provides unsatisfactory results. Instead, we propose a method that allows a
controlled merging of correction, parsing and matching, inspired by dependency
grammars. We also report on the effectiveness of various heuristics that we
have employed to improve recall.
"
3212,"Dynamic Linking of Smart Digital Objects Based on User Navigation
  Patterns","  We discuss a methodology to dynamically generate links among digital objects
by means of an unsupervised learning mechanism which analyzes user link
traversal patterns. We performed an experiment with a test bed of 150 complex
data objects, referred to as buckets. Each bucket manages its own content,
provides methods to interact with users and individually maintains a set of
links to other buckets. We demonstrate that buckets were capable of dynamically
adjusting their links to other buckets according to user link selections,
thereby generating a meaningful network of bucket relations. Our results
indicate such adaptive networks of linked buckets approximate the collective
link preferences of a community of user
"
3213,"Automatically Generating Interfaces for Personalized Interaction with
  Digital Libraries","  We present an approach to automatically generate interfaces supporting
personalized interaction with digital libraries; these interfaces augment the
user-DL dialog by empowering the user to (optionally) supply out-of-turn
information during an interaction, flatten or restructure the dialog, and
enquire about dialog options. Interfaces generated using this approach for
CITIDEL are described.
"
3214,Domain resource integration system,"  Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is
a hierarchical distributed Internet information retrieval system. This system
will solve some bottleneck problems such as long update interval, poor coverage
in current web search system. DRIS will build the information retrieval
infrastructure of Internet, but not a commercial search engine. The protocol
series of DRIS are also detailed in this paper.
"
3215,Providing Authentic Long-term Archival Access to Complex Relational Data,"  We discuss long-term preservation of and access to relational databases. The
focus is on national archives and science data archives which have to ingest
and integrate data from a broad spectrum of vendor-specific relational database
management systems (RDBMS). Furthermore, we present our solution SIARD which
analyzes and extracts data and data logic from almost any RDBMS. It enables, to
a reasonable level of authenticity, complete detachment of databases from their
vendor-specific environment. The user can add archival descriptive metadata
according to a customizable schema. A SIARD database archive integrates data,
data logic, technical metadata, and archival descriptive information in one
archival information package, independent of any specific software and
hardware, based upon plain text files and the standardized languages SQL and
XML. For usage purposes, a SIARD archive can be reloaded into any current or
future RDBMS which supports standard SQL. In addition, SIARD contains a client
that enables 'on demand' reload of archives into a target RDBMS, and multi-user
remote access for querying and browsing the data together with its technical
and descriptive metadata in one graphical user interface.
"
3216,"The Building of Online Communities: An approach for learning
  organizations, with a particular focus on the museum sector","  This paper considers the move toward and potential of building online
communities, with a particular focus on the museum sector. For instance, the
increase in the use of `personalized' toolkits that are becoming an integral
part of the online presence for learning organizations, like museums, can
provide a basis for creating and sustaining communities. A set of case studies
further illustrates working examples of the ways in which personalization and
specific tools are developing collaborative spaces, community channels and
group interactions.
"
3217,"A knowledge-based approach to semi-automatic annotation of multimedia
  documents via user adaptation","  Current approaches to the annotation process focus on annotation schemas,
languages for annotation, or are very application driven. In this paper it is
proposed that a more flexible architecture for annotation requires a knowledge
component to allow for flexible search and navigation of the annotated
material. In particular, it is claimed that a general approach must take into
account the needs, competencies, and goals of the producers, annotators, and
consumers of the annotated material. We propose that a user-model based
approach is, therefore, necessary.
"
3218,An argumentative annotation schema for meeting discussions,"  In this article, we are interested in the annotation of transcriptions of
human-human dialogue taken from meeting records. We first propose a meeting
content model where conversational acts are interpreted with respect to their
argumentative force and their role in building the argumentative structure of
the meeting discussion. Argumentation in dialogue describes the way
participants take part in the discussion and argue their standpoints. Then, we
propose an annotation scheme based on such an argumentative dialogue model as
well as the evaluation of its adequacy. The obtained higher-level semantic
annotations are exploited in the conceptual indexing of the information
contained in meeting discussions.
"
3219,"Automatic Keyword Extraction from Spoken Text. A Comparison of two
  Lexical Resources: the EDR and WordNet","  Lexical resources such as WordNet and the EDR electronic dictionary have been
used in several NLP tasks. Probably, partly due to the fact that the EDR is not
freely available, WordNet has been used far more often than the EDR. We have
used both resources on the same task in order to make a comparison possible.
The task is automatic assignment of keywords to multi-party dialogue episodes
(i.e. thematically coherent stretches of spoken text). We show that the use of
lexical resources in such a task results in slightly higher performances than
the use of a purely statistically based method.
"
3220,Transparent Format Migration of Preserved Web Content,"  The LOCKSS digital preservation system collects content by crawling the web
and preserves it in the format supplied by the publisher. Eventually, browsers
will no longer understand that format. A process called format migration
converts it to a newer format that the browsers do understand. The LOCKSS
program has designed and tested an initial implementation of format migration
for Web content that is transparent to readers, building on the content
negotiation capabilities of HTTP.
"
3221,Notes On The Design Of An Internet Adversary,"  The design of the defenses Internet systems can deploy against attack,
especially adaptive and resilient defenses, must start from a realistic model
of the threat. This requires an assessment of the capabilities of the
adversary. The design typically evolves through a process of simulating both
the system and the adversary. This requires the design and implementation of a
simulated adversary based on the capability assessment. Consensus on the
capabilities of a suitable adversary is not evident. Part of the recent
redesign of the protocol used by peers in the LOCKSS digital preservation
system included a conservative assessment of the adversary's capabilities. We
present our assessment and the implications we drew from it as a step towards a
reusable adversary specification.
"
3222,Principles for Digital Preservation,"  The immense investments in creating and disseminating digitally represented
information have not been accompanied by commensurate effort to ensure the
longevity of information of permanent interest. Asserted difficulties with
long-term digital preservation prove to be largely underestimation of what
technology can provide. We show how to clarify prominent misunderstandings and
sketch a 'Trustworthy Digital Object (TDO)' method that solves all the
published technical challenges.
"
3223,"Trustworthy 100-Year Digital Objects: Durable Encoding for When It's Too
  Late to Ask","  How can an author store digital information so that it will be reliably
useful, even years later when he is no longer available to answer questions?
Methods that might work are not good enough; what is preserved today should be
reliably useful whenever someone wants it. Prior proposals fail because they
confound saved data with irrelevant details of today's information
technology--details that are difficult to define, extract, and save completely
and accurately.
  We use a virtual machine to represent and eventually to render any data
whatsoever. We focus on a case of intermediate difficulty--an executable
procedure--and identify a variant for every other data type. This solution
might be more elaborate than needed to render some text, image, audio, or video
data. Simple data can be preserved as representations using well-known
standards. We sketch practical methods for files ranging from simple structures
to those containing computer programs, treating simple cases here and deferring
complex cases for future work. Enough of the complete solution is known to
enable practical aggressive preservation programs today.
"
3224,EURYDICE : A platform for unified access to documents,"  In this paper we present Eurydice, a platform dedicated to provide a unified
gateway to documents. Its basic functionalities about collecting documents have
been designed based on a long experience about the management of scientific
documentation among large and demanding academic communities such as IMAG and
INRIA. Besides the basic problem of accessing documents - which was of course
the original and main motivation of the project - a great effort has been
dedicated to the development of management functionalities which could help
institutions to control, analyse the current situation about the use of the
documentation, and finally to set a better ground for a documentation policy.
Finally a great emphasis - and corresponding technical investment - has been
put on the protection of property and reproduction rights both from the users'
intitution side and from the editors' side.
"
3225,A Link Clustering Based Approach for Clustering Categorical Data,"  Categorical data clustering (CDC) and link clustering (LC) have been
considered as separate research and application areas. The main focus of this
paper is to investigate the commonalities between these two problems and the
uses of these commonalities for the creation of new clustering algorithms for
categorical data based on cross-fertilization between the two disjoint research
fields. More precisely, we formally transform the CDC problem into an LC
problem, and apply LC approach for clustering categorical data. Experimental
results on real datasets show that LC based clustering method is competitive
with existing CDC algorithms with respect to clustering accuracy.
"
3226,Fedora: An Architecture for Complex Objects and their Relationships,"  The Fedora architecture is an extensible framework for the storage,
management, and dissemination of complex objects and the relationships among
them. Fedora accommodates the aggregation of local and distributed content into
digital objects and the association of services with objects. This al-lows an
object to have several accessible representations, some of them dy-namically
produced. The architecture includes a generic RDF-based relation-ship model
that represents relationships among objects and their components. Queries
against these relationships are supported by an RDF triple store. The
architecture is implemented as a web service, with all aspects of the complex
object architecture and related management functions exposed through REST and
SOAP interfaces. The implementation is available as open-source soft-ware,
providing the foundation for a variety of end-user applications for digital
libraries, archives, institutional repositories, and learning object systems.
"
3227,Clustering SPIRES with EqRank,"  SPIRES is the largest database of scientific papers in the subject field of
high energy and nuclear physics. It contains information on the citation graph
of more than half a million of papers (vertexes of the citation graph). We
outline the EqRank algorithm designed to cluster vertexes of directed graphs,
and present the results of EqRank application to the SPIRES citation graph. The
hierarchical clustering of SPIRES yielded by EqRank is used to set up a web
service, which is also outlined.
"
3228,An Information Network Overlay Architecture for the NSDL,"  We describe the underlying data model and implementation of a new
architecture for the National Science Digital Library (NSDL) by the Core
Integration Team (CI). The architecture is based on the notion of an
information network overlay. This network, implemented as a graph of digital
objects in a Fedora repository, allows the representation of multiple
information entities and their relationships. The architecture provides the
framework for contextualization and reuse of resources, which we argue is
essential for the utility of the NSDL as a tool for teaching and learning.
"
3229,Orchestrating Metadata Enhancement Services: Introducing Lenny,"  Harvested metadata often suffers from uneven quality to the point that
utility is compromised. Although some aggregators have developed methods for
evaluating and repairing specific metadata problems, it has been unclear how
these methods might be scaled into services that can be used within an
automated production environment. The National Science Digital Library (NSDL),
as part of its work with INFOMINE, has developed a model of ser-vice
interaction that enables loosely-coupled third party services to provide
metadata enhancements to a central repository, with interactions orchestrated
by a centralized software application.
"
3230,"aDORe: a modular, standards-based Digital Object Repository","  This paper describes the aDORe repository architecture, designed and
implemented for ingesting, storing, and accessing a vast collection of Digital
Objects at the Research Library of the Los Alamos National Laboratory. The
aDORe architecture is highly modular and standards-based. In the architecture,
the MPEG-21 Digital Item Declaration Language is used as the XML-based format
to represent Digital Objects that can consist of multiple datastreams as Open
Archival Information System Archival Information Packages (OAIS AIPs).Through
an ingestion process, these OAIS AIPs are stored in a multitude of autonomous
repositories. A Repository Index keeps track of the creation and location of
all the autonomous repositories, whereas an Identifier Locator registers in
which autonomous repository a given Digital Object or OAIS AIP resides. A
front-end to the complete environment, the OAI-PMH Federator, is introduced for
requesting OAIS Dissemination Information Packages (OAIS DIPs). These OAIS DIPs
can be the stored OAIS AIPs themselves, or transformations thereof. This
front-end allows OAI-PMH harvesters to recurrently and selectively collect
batches of OAIS DIPs from aDORe, and hence to create multiple, parallel
services using the collected objects. Another front-end, the OpenURL Resolver,
is introduced for requesting OAIS Result Sets. An OAIS Result Set is a
dissemination of an individual Digital Object or of its constituent
datastreams. Both front-ends make use of an MPEG-21 Digital Item Processing
Engine to apply services to OAIS AIPs, Digital Objects, or constituent
datastreams that were specified in a dissemination request.
"
3231,Co-Authorship Networks in the Digital Library Research Community,"  The field of digital libraries (DLs) coalesced in 1994: the first digital
library conferences were held that year, awareness of the World Wide Web was
accelerating, and the National Science Foundation awarded $24 Million (U.S.)
for the Digital Library Initiative (DLI). In this paper we examine the state of
the DL domain after a decade of activity by applying social network analysis to
the co-authorship network of the past ACM, IEEE, and joint ACM/IEEE digital
library conferences. We base our analysis on a common binary undirectional
network model to represent the co-authorship network, and from it we extract
several established network measures. We also introduce a weighted directional
network model to represent the co-authorship network, for which we define
$AuthorRank$ as an indicator of the impact of an individual author in the
network. The results are validated against conference program committee members
in the same period. The results show clear advantages of PageRank and
AuthorRank over degree, closeness and betweenness centrality metrics. We also
investigate the amount and nature of international participation in Joint
Conference on Digital Libraries (JCDL).
"
3232,"Toward alternative metrics of journal impact: A comparison of download
  and citation data","  We generated networks of journal relationships from citation and download
data, and determined journal impact rankings from these networks using a set of
social network centrality metrics. The resulting journal impact rankings were
compared to the ISI IF. Results indicate that, although social network metrics
and ISI IF rankings deviate moderately for citation-based journal networks,
they differ considerably for journal networks derived from download data. We
believe the results represent a unique aspect of general journal impact that is
not captured by the ISI IF. These results furthermore raise questions regarding
the validity of the ISI IF as the sole assessment of journal impact, and
suggest the possibility of devising impact metrics based on usage information
in general.
"
3233,"File-based storage of Digital Objects and constituent datastreams:
  XMLtapes and Internet Archive ARC files","  This paper introduces the write-once/read-many XMLtape/ARC storage approach
for Digital Objects and their constituent datastreams. The approach combines
two interconnected file-based storage mechanisms that are made accessible in a
protocol-based manner. First, XML-based representations of multiple Digital
Objects are concatenated into a single file named an XMLtape. An XMLtape is a
valid XML file; its format definition is independent of the choice of the
XML-based complex object format by which Digital Objects are represented. The
creation of indexes for both the identifier and the creation datetime of the
XML-based representation of the Digital Objects facilitates OAI-PMH-based
access to Digital Objects stored in an XMLtape. Second, ARC files, as
introduced by the Internet Archive, are used to contain the constituent
datastreams of the Digital Objects in a concatenated manner. An index for the
identifier of the datastream facilitates OpenURL-based access to an ARC file.
The interconnection between XMLtapes and ARC files is provided by conveying the
identifiers of ARC files associated with an XMLtape as administrative
information in the XMLtape, and by including OpenURL references to constituent
datastreams of a Digital Object in the XML-based representation of that Digital
Object.
"
3234,The Effect of Use and Access on Citations,"  It has been shown (S. Lawrence, 2001, Nature, 411, 521) that journal articles
which have been posted without charge on the internet are more heavily cited
than those which have not been. Using data from the NASA Astrophysics Data
System (ads.harvard.edu) and from the ArXiv e-print archive at Cornell
University (arXiv.org) we examine the causes of this effect.
"
3235,mod_oai: An Apache Module for Metadata Harvesting,"  We describe mod_oai, an Apache 2.0 module that implements the Open Archives
Initiative Protocol for Metadata Harvesting (OAI-PMH). OAIPMH is the de facto
standard for metadata exchange in digital libraries and allows repositories to
expose their contents in a structured, application-neutral format with
semantics optimized for accurate incremental harvesting. Current
implementations of OAI-PMH are either separate applications that access an
existing repository, or are built-in to repository software packages. mod_oai
is different in that it optimizes harvesting web content by building OAI-PMH
capability into the Apache server. We discuss the implications of adding
harvesting capability to an Apache server and describe our initial experimental
results accessing a departmental web site using both web crawling and OAIPMH
harvesting techniques.
"
3236,"Scientific impact quantity and quality: Analysis of two sources of
  bibliographic data","  Attempts to understand the consequence of any individual scientist's activity
within the long-term trajectory of science is one of the most difficult
questions within the philosophy of science. Because scientific publications
play such as central role in the modern enterprise of science, bibliometric
techniques which measure the ``impact'' of an individual publication as a
function of the number of citations it receives from subsequent authors have
provided some of the most useful empirical data on this question. Until
recently, Thompson/ISI has provided the only source of large-scale ``inverted''
bibliographic data of the sort required for impact analysis. In the end of
2004, Google introduced a new service, GoogleScholar, making much of this same
data available. Here we analyze 203 publications, collectively cited by more
than 4000 other publications. We show surprisingly good agreement between data
citation counts provided by the two services. Data quality across the systems
is analyzed, and potentially useful complementarities between are considered.
The additional robustness offered by multiple sources of such data promises to
increase the utility of these measurements as open citation protocols and open
access increase their impact on electronic scientific publication practices.
"
3237,The Convergence of Digital-Libraries and the Peer-Review Process,"  Pre-print repositories have seen a significant increase in use over the past
fifteen years across multiple research domains. Researchers are beginning to
develop applications capable of using these repositories to assist the
scientific community above and beyond the pure dissemination of information.
The contribution set forth by this paper emphasizes a deconstructed publication
model in which the peer-review process is mediated by an OAI-PMH peer-review
service. This peer-review service uses a social-network algorithm to determine
potential reviewers for a submitted manuscript and for weighting the relative
influence of each participating reviewer's evaluations. This paper also
suggests a set of peer-review specific metadata tags that can accompany a
pre-print's existing metadata record. The combinations of these contributions
provide a unique repository-centric peer-review model that fits within the
widely deployed OAI-PMH framework.
"
3238,The OAI Data-Provider Registration and Validation Service,"  I present a summary of recent use of the Open Archives Initiative (OAI)
registration and validation services for data-providers. The registration
service has seen a steady stream of registrations since its launch in 2002, and
there are now over 220 registered repositories. I examine the validation logs
to produce a breakdown of reasons why repositories fail validation. This
breakdown highlights some common problems and will be used to guide work to
improve the validation service.
"
3239,The Hyper-Cortex of Human Collective-Intelligence Systems,"  Individual-intelligence research, from a neurological perspective, discusses
the hierarchical layers of the cortex as a structure that performs conceptual
abstraction and specification. This theory has been used to explain how
motor-cortex regions responsible for different behavioral modalities such as
writing and speaking can be utilized to express the same general concept
represented higher in the cortical hierarchy. For example, the concept of a
dog, represented across a region of high-level cortical-neurons, can either be
written or spoken about depending on the individual's context. The higher-layer
cortical areas project down the hierarchy, sending abstract information to
specific regions of the motor-cortex for contextual implementation. In this
paper, this idea is expanded to incorporate collective-intelligence within a
hyper-cortical construct. This hyper-cortex is a multi-layered network used to
represent abstract collective concepts. These ideas play an important role in
understanding how collective-intelligence systems can be engineered to handle
problem abstraction and solution specification. Finally, a collection of common
problems in the scientific community are solved using an artificial
hyper-cortex generated from digital-library metadata.
"
3240,Dictionaries merger for text expansion in question answering,"  This paper presents an original way to add new data in a reference dictionary
from several other lexical resources, without loosing any consistence. This
operation is carried in order to get lexical information classified by the
sense of the entry. This classification makes it possible to enrich utterances
(in QA: the queries) following the meaning, and to reduce noise. An analysis of
the experienced problems shows the interest of this method, and insists on the
points that have to be tackled.
"
3241,"Exploitation de dictionnaires \'{e}lectroniques pour la
  d\'{e}sambigu\""{i}sation s\'{e}mantique lexicale","  This paper presents a lexical disambiguation system, initially developed for
English and now adapted to French. This system associates a word with its
meaning in a given context using electronic dictionaries as semantically
annotated corpora in order to extract semantic disambiguation rules. We
describe the rule extraction and application process as well as the evaluation
of the system. The results for French give us insight information on some
possible improvments of the nature and content of lexical resources adapted for
disambiguation in this framework.
"
3242,Adapting CBPP platforms for instructional use,"  Commons based peer-production (CBPP) is the de-centralized, net-based
approach to the creation and dissemination of information resources. Underlying
every CBPP system is a virtual community brought together by an internet tool
(such as a web site) and structured by a specific collaboration protocol. In
this talk we will argue that the value of such platforms can be leveraged by
adapting them for pedagogical purposes.
  We report on one such recent adaptation. The Noosphere system is a web-based
collaboration environment that underlies the popular Planetmath website, a
collaboratively written encyclopedia of mathematics licensed under the GNU Free
Documentation License (FDL). Recently, the system was used to host a
graduate-level mathematics course at Dalhousie University, in Halifax, Canada.
The course consisted of regular lectures and assignment problems. The students
in the course collaborated on a set of course notes, encapsulating the lecture
content and giving solutions of assigned problems. The successful outcome of
this experiment demonstrated that a dedicated Noosphere system is well suited
for classroom applications. We argue that this ``proof of concept'' experience
also strongly suggests that every successful CBPP platform possesses latent
pedagogical value.
"
3243,Representing Digital Assets using MPEG-21 Digital Item Declaration,"  Various XML-based approaches aimed at representing compound digital assets
have emerged over the last several years. Approaches that are of specific
relevance to the digital library community include the Metadata Encoding and
Transmission Standard (METS), the IMS Content Packaging XML Binding, and the
XML Formatted Data Units (XFDU) developed by CCSDS Panel 2. The MPEG-21 Digital
Item Declaration (MPEG-21 DID) is another standard specifying the
representation of digital assets in XML that, so far, has received little
attention in the digital library community. This article gives a brief insight
into the MPEG-21 standardization effort, highlights the major characteristics
of the MPEG-21 DID Abstract Model, and describes the MPEG-21 Digital Item
Declaration Language (MPEG-21 DIDL), an XML syntax for the representation of
digital assets based on the MPEG-21 DID Abstract Model. Also, it briefly
demonstrates the potential relevance of MPEG-21 DID to the digital library
community by describing its use in the aDORe repository environment at the
Research Library of the Los Alamos National Laboratory (LANL) for the
representation of digital assets.
"
3244,"Can Small Museums Develop Compelling, Educational and Accessible Web
  Resources? The Case of Accademia Carrara","  Due to the lack of budget, competence, personnel and time, small museums are
often unable to develop compelling, educational and accessible web resources
for their permanent collections or temporary exhibitions. In an attempt to
prove that investing in these types of resources can be very fruitful even for
small institutions, we will illustrate the case of Accademia Carrara, a museum
in Bergamo, northern Italy, which, for a current temporary exhibition on
Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed
a series of multimedia applications, including an accessible website, rich in
content and educational material [www.cezannerenoir.it].
"
3245,Copyright and Promotion: Oxymoron or Opportunity?,"  Copyright in the cultural sphere can act as a barrier to the dissemination of
high-quality information. On the other hand it protects works of art that might
not be made available otherwise. This dichotomy makes the area of copyright
difficult, especially when it applies to the digital arena of the web where
copying is so easy and natural. Here we present a snapshot of the issues for
online copyright, with particular emphasis on the relevance to cultural
institutions. We concentrate on Europe and the US; as an example we include a
special section dedicated to the situation in Italy.
"
3246,The Structure of Collaborative Tagging Systems,"  Collaborative tagging describes the process by which many users add metadata
in the form of keywords to shared content. Recently, collaborative tagging has
grown in popularity on the web, on sites that allow users to tag bookmarks,
photographs and other content. In this paper we analyze the structure of
collaborative tagging systems as well as their dynamical aspects. Specifically,
we discovered regularities in user activity, tag frequencies, kinds of tags
used, bursts of popularity in bookmarking and a remarkable stability in the
relative proportions of tags within a given url. We also present a dynamical
model of collaborative tagging that predicts these stable patterns and relates
them to imitation and shared knowledge.
"
3247,A Fresh Look at the Reliability of Long-term Digital Storage,"  Many emerging Web services, such as email, photo sharing, and web site
archives, need to preserve large amounts of quickly-accessible data
indefinitely into the future. In this paper, we make the case that these
applications' demands on large scale storage systems over long time horizons
require us to re-evaluate traditional storage system designs. We examine
threats to long-lived data from an end-to-end perspective, taking into account
not just hardware and software faults but also faults due to humans and
organizations. We present a simple model of long-term storage failures that
helps us reason about the various strategies for addressing these threats in a
cost-effective manner. Using this model we show that the most important
strategies for increasing the reliability of long-term storage are detecting
latent faults quickly, automating fault repair to make it faster and cheaper,
and increasing the independence of data replicas.
"
3248,Requirements for Digital Preservation Systems: A Bottom-Up Approach,"  The field of digital preservation is being defined by a set of standards
developed top-down, starting with an abstract reference model (OAIS) and
gradually adding more specific detail. Systems claiming conformance to these
standards are entering production use. Work is underway to certify that systems
conform to requirements derived from OAIS.
  We complement these requirements derived top-down by presenting an alternate,
bottom-up view of the field. The fundamental goal of these systems is to ensure
that the information they contain remains accessible for the long term. We
develop a parallel set of requirements based on observations of how existing
systems handle this task, and on an analysis of the threats to achieving the
goal. On this basis we suggest disclosures that systems should provide as to
how they satisfy their goals.
"
3249,Folksonomy as a Complex Network,"  Folksonomy is an emerging technology that works to classify the information
over WWW through tagging the bookmarks, photos or other web-based contents. It
is understood to be organized by every user while not limited to the authors of
the contents and the professional editors. This study surveyed the folksonomy
as a complex network. The result indicates that the network, which is composed
of the tags from the folksonomy, displays both properties of small world and
scale-free. However, the statistics only shows a local and static slice of the
vast body of folksonomy which is still evolving.
"
3250,Representing Digital Assets for Long-Term Preservation using MPEG-21 DID,"  Various efforts aimed at representing digital assets have emerged from
several communities over the last years, including the Metadata Encoding and
Transmission Standard (METS), the IMS Content Packaging (IMS-CP) XML Binding
and the XML Formatted Data Units (XFDU). The MPEG-21 Digital Item Declaration
(MPEG-21 DID) is another approach that can be used for the representation of
digital assets in XML. This paper will explore the potential of the MPEG-21 DID
in a Digital Preservation context, by looking at the core building blocks of
the OAIS Information Model and the way in which they map to the MPEG-21 DID
abstract model and the MPEG-21 DIDL XML syntax.
"
3251,"Access Interfaces for Open Archival Information Systems based on the
  OAI-PMH and the OpenURL Framework for Context-Sensitive Services","  In recent years, a variety of digital repository and archival systems have
been developed and adopted. All of these systems aim at hosting a variety of
compound digital assets and at providing tools for storing, managing and
accessing those assets. This paper will focus on the definition of common and
standardized access interfaces that could be deployed across such diverse
digital respository and archival systems. The proposed interfaces are based on
the two formal specifications that have recently emerged from the Digital
Library community: The Open Archive Initiative Protocol for Metadata Harvesting
(OAI-PMH) and the NISO OpenURL Framework for Context-Sensitive Services
(OpenURL Standard). As will be described, the former allows for the retrieval
of batches of XML-based representations of digital assets, while the latter
facilitates the retrieval of disseminations of a specific digital asset or of
one or more of its constituents. The core properties of the proposed interfaces
are explained in terms of the Reference Model for an Open Archival Information
System (OAIS).
"
3252,"Telling Great Stories: An NSDL Content and Communications System for
  Aggregation, Display, and Distribution of News and Features","  Education digital libraries contain cataloged resources as well as contextual
information about innovations in the use of educational technology, exemplar
stories about community activities, and news from various user communities that
include teachers, students, scholars, and developers. Long-standing library
traditions of service, preservation, democratization of knowledge, rich
discourse, equal access, and fair use are evident in library communications
models that both pull in and push out contextual information from multiple
sources integrated with editorial production processes. This paper argues that
a dynamic narrative flow [1] is enabled by effective management of complex
content and communications in a decentralized web-based education digital
library making publishing objects such as aggregations of resources, or
selected parts of objects [4] accessible through a Content and Communications
System. Providing services that encourage patrons to reuse, reflect out, and
contribute resources back [5] to the Library increases the reach and impact of
the National Science Digital Library (NSDL). This system is a model for
distributed content development and effective communications for education
digital libraries in general.
"
3253,Bibliographic Classification using the ADS Databases,"  We discuss two techniques used to characterize bibliographic records based on
their similarity to and relationship with the contents of the NASA Astrophysics
Data System (ADS) databases. The first method has been used to classify input
text as being relevant to one or more subject areas based on an analysis of the
frequency distribution of its individual words. The second method has been used
to classify existing records as being relevant to one or more databases based
on the distribution of the papers citing them. Both techniques have proven to
be valuable tools in assigning new and existing bibliographic records to
different disciplines within the ADS databases.
"
3254,The Availability and Persistence of Web References in D-Lib Magazine,"  We explore the availability and persistence of URLs cited in articles
published in D-Lib Magazine. We extracted 4387 unique URLs referenced in 453
articles published from July 1995 to August 2004. The availability was checked
three times a week for 25 weeks from September 2004 to February 2005. We found
that approximately 28% of those URLs failed to resolve initially, and 30%
failed to resolve at the last check. A majority of the unresolved URLs were due
to 404 (page not found) and 500 (internal server error) errors. The content
pointed to by the URLs was relatively stable; only 16% of the content
registered more than a 1 KB change during the testing period. We explore
possible factors which may cause a URL to fail by examining its age, path
depth, top-level domain and file extension. Based on the data collected, we
found the half-life of a URL referenced in a D-Lib Magazine article is
approximately 10 years. We also found that URLs were more likely to be
unavailable if they pointed to resources in the .net, .edu or country-specific
top-level domain, used non-standard ports (i.e., not port 80), or pointed to
resources with uncommon or deprecated extensions (e.g., .shtml, .ps, .txt).
"
3255,Dynamic Web File Format Transformations with Grace,"  Web accessible content stored in obscure, unpopular or obsolete formats
represents a significant problem for digital preservation. The file formats
that encode web content represent the implicit and explicit choices of web site
maintainers at a particular point in time. Older file formats that have fallen
out of favor are obviously a problem, but so are new file formats that have not
yet been fully supported by browsers. Often browsers use plug-in software for
displaying old and new formats, but plug-ins can be difficult to find, install
and replicate across all environments that one may use. We introduce Grace, an
http proxy server that transparently converts browser-incompatible and obsolete
web content into web content that a browser is able to display without the use
of plug-ins. Grace is configurable on a per user basis and can be expanded to
provide an array of conversion services. We illustrate how the Grace prototype
transforms several image formats (XBM, PNG with various alpha channels, and
JPEG 2000) so they are viewable in Internet Explorer.
"
3256,EqRank: Theme Evolution in Citation Graphs,"  Time evolution of the classification scheme generated by the EqRank algorithm
is studied with hep-th citation graph as an example. Intuitive expectations
about evolution of an adequate classification scheme for a growing set of
objects are formulated. Evolution compliant with these expectations is called
natural. It is demonstrated that EqRank yields a naturally evolving
classification scheme. We conclude that EqRank can be used as a means to detect
new scientific themes, and to track their development.
"
3257,Collaborative tagging as a tripartite network,"  We describe online collaborative communities by tripartite networks, the
nodes being persons, items and tags. We introduce projection methods in order
to uncover the structures of the networks, i.e. communities of users, genre
families...
  To do so, we focus on the correlations between the nodes, depending on their
profiles, and use percolation techniques that consist in removing less
correlated links and observing the shaping of disconnected islands. The
structuring of the network is visualised by using a tree representation. The
notion of diversity in the system is also discussed.
"
3258,Journal Status,"  The status of an actor in a social context is commonly defined in terms of
two factors: the total number of endorsements the actor receives from other
actors and the prestige of the endorsing actors. These two factors indicate the
distinction between popularity and expert appreciation of the actor,
respectively. We refer to the former as popularity and to the latter as
prestige. These notions of popularity and prestige also apply to the domain of
scholarly assessment. The ISI Impact Factor (ISI IF) is defined as the mean
number of citations a journal receives over a 2 year period. By merely counting
the amount of citations and disregarding the prestige of the citing journals,
the ISI IF is a metric of popularity, not of prestige. We demonstrate how a
weighted version of the popular PageRank algorithm can be used to obtain a
metric that reflects prestige. We contrast the rankings of journals according
to their ISI IF and their weighted PageRank, and we provide an analysis that
reveals both significant overlaps and differences. Furthermore, we introduce
the Y-factor which is a simple combination of both the ISI IF and the weighted
PageRank, and find that the resulting journal rankings correspond well to a
general understanding of journal status.
"
3259,"A Multi-Relational Network to Support the Scholarly Communication
  Process","  The general pupose of the scholarly communication process is to support the
creation and dissemination of ideas within the scientific community. At a finer
granularity, there exists multiple stages which, when confronted by a member of
the community, have different requirements and therefore different solutions.
In order to take a researcher's idea from an initial inspiration to a community
resource, the scholarly communication infrastructure may be required to 1)
provide a scientist initial seed ideas; 2) form a team of well suited
collaborators; 3) located the most appropriate venue to publish the formalized
idea; 4) determine the most appropriate peers to review the manuscript; and 5)
disseminate the end product to the most interested members of the community.
Through the various delinieations of this process, the requirements of each
stage are tied soley to the multi-functional resources of the community: its
researchers, its journals, and its manuscritps. It is within the collection of
these resources and their inherent relationships that the solutions to
scholarly communication are to be found. This paper describes an associative
network composed of multiple scholarly artifacts that can be used as a medium
for supporting the scholarly communication process.
"
3260,"Metadata aggregation and ""automated digital libraries"": A retrospective
  on the NSDL experience","  Over three years ago, the Core Integration team of the National Science
Digital Library (NSDL) implemented a digital library based on metadata
aggregation using Dublin Core and OAI-PMH. The initial expectation was that
such low-barrier technologies would be relatively easy to automate and
administer. While this architectural choice permitted rapid deployment of a
production NSDL, our three years of experience have contradicted our original
expectations of easy automation and low people cost. We have learned that
alleged ""low-barrier"" standards are often harder to deploy than expected. In
this paper we report on this experience and comment on the general cost, the
functionality, and the ultimate effectiveness of this architecture.
"
3261,D2D: Digital Archive to MPEG-21 DIDL,"  Digital Archive to MPEG-21 DIDL (D2D) analyzes the contents of the digital
archive and produces an MPEG-21 Digital Item Declaration Language (DIDL)
encapsulating the analysis results. DIDL is an extensible XML-based language
that aggregates resources and the metadata. We provide a brief report on
several analysis techniques applied on the digital archive by the D2D and
provide an evaluation of its run-time performance.
"
3262,"eJournal interface can influence usage statistics: implications for
  libraries, publishers, and Project COUNTER","  The design of a publisher's electronic interface can have a measurable effect
on electronic journal usage statistics. A study of journal usage from six
COUNTER-compliant publishers at thirty-two research institutions in the United
States, the United Kingdom and Sweden indicates that the ratio of PDF to HTML
views is not consistent across publisher interfaces, even after controlling for
differences in publisher content. The number of fulltext downloads may be
artificially inflated when publishers require users to view HTML versions
before accessing PDF versions or when linking mechanisms, such as CrossRef,
direct users to the full text, rather than the abstract, of each article. These
results suggest that usage reports from COUNTER-compliant publishers are not
directly comparable in their current form. One solution may be to modify
publisher numbers with adjustment factors deemed to be representative of the
benefit or disadvantage due to its interface. Standardization of some interface
and linking protocols may obviate these differences and allow for more accurate
cross-publisher comparisons.
"
3263,Exploring term-document matrices from matrix models in text mining,"  We explore a matrix-space model, that is a natural extension to the vector
space model for Information Retrieval. Each document can be represented by a
matrix that is based on document extracts (e.g. sentences, paragraphs,
sections). We focus on the performance of this model for the specific case in
which documents are originally represented as term-by-sentence matrices. We use
the singular value decomposition to approximate the term-by-sentence matrices
and assemble these results to form the pseudo-``term-document'' matrix that
forms the basis of a text mining method alternative to traditional VSM and LSI.
We investigate the singular values of this matrix and provide experimental
evidence suggesting that the method can be particularly effective in terms of
accuracy for text collections with multi-topic documents, such as web pages
with news.
"
3264,"Digital Libraries: From Process Modelling to Grid-based Service Oriented
  Architecture","  Graphical Business Process Modelling Languages (BPML) like Role Activity
Diagrams (RAD) provide ease and flexibility for modelling business behaviour.
However, these languages show limited applicability in terms of enactment over
distributed systems paradigms like Service Oriented Architecture (SOA) based
grid computing. This paper investigates RAD modelling of a Scientific
Publishing Process (SPP) for Digital Libraries (DL) and tries to determine the
suitability of Pi-Calculus based formal approaches to enact SOA based grid
computing. In order to achieve this purpose, the Pi-Calculus based formal
transformation from a RAD model of SPP for DL draws attention towards a number
of challenging issues including issues that require particular design
considerations for appropriate enactment in a SOA based grid system.
"
3265,Representing Contextualized Information in the NSDL,"  The NSDL (National Science Digital Library) is funded by the National Science
Foundation to advance science and match education. The inital product was a
metadata-based digital library providing search and access to distributed
resources. Our recent work recognizes the importance of context - relations,
metadata, annotations - for the pedagogical value of a digital library. This
new architecture uses Fedora, a tool for representing complex content, data,
metadata, web-based services, and semantic relationships, as the basis of an
information network overlay (INO). The INO provides an extensible knowl-edge
base for an expanding suite of digital library services.
"
3266,"Does the arXiv lead to higher citations and reduced publisher downloads
  for mathematics articles?","  An analysis of 2,765 articles published in four math journals from 1997 to
2005 indicate that articles deposited in the arXiv received 35% more citations
on average than non-deposited articles (an advantage of about 1.1 citations per
article), and that this difference was most pronounced for highly-cited
articles. Open Access, Early View, and Quality Differential were examined as
three non-exclusive postulates for explaining the citation advantage. There was
little support for a universal Open Access explanation, and no empirical
support for Early View. There was some inferential support for a Quality
Differential brought about by more highly-citable articles being deposited in
the arXiv. In spite of their citation advantage, arXiv-deposited articles
received 23% fewer downloads from the publisher's website (about 10 fewer
downloads per article) in all but the most recent two years after publication.
The data suggest that arXiv and the publisher's website may be fulfilling
distinct functional needs of the reader.
"
3267,VXA: A Virtual Architecture for Durable Compressed Archives,"  Data compression algorithms change frequently, and obsolete decoders do not
always run on new hardware and operating systems, threatening the long-term
usability of content archived using those algorithms. Re-encoding content into
new formats is cumbersome, and highly undesirable when lossy compression is
involved. Processor architectures, in contrast, have remained comparatively
stable over recent decades. VXA, an archival storage system designed around
this observation, archives executable decoders along with the encoded content
it stores. VXA decoders run in a specialized virtual machine that implements an
OS-independent execution environment based on the standard x86 architecture.
The VXA virtual machine strictly limits access to host system services, making
decoders safe to run even if an archive contains malicious code. VXA's adoption
of a ""native"" processor architecture instead of type-safe language technology
allows reuse of existing ""hand-optimized"" decoders in C and assembly language,
and permits decoders access to performance-enhancing architecture features such
as vector processing instructions. The performance cost of VXA's virtualization
is typically less than 15% compared with the same decoders running natively.
The storage cost of archived decoders, typically 30-130KB each, can be
amortized across many archived files sharing the same compression method.
"
3268,Collaborative thesaurus tagging the Wikipedia way,"  This paper explores the system of categories that is used to classify
articles in Wikipedia. It is compared to collaborative tagging systems like
del.icio.us and to hierarchical classification like the Dewey Decimal
Classification (DDC). Specifics and commonalitiess of these systems of subject
indexing are exposed. Analysis of structural and statistical properties
(descriptors per record, records per descriptor, descriptor levels) shows that
the category system of Wikimedia is a thesaurus that combines collaborative
tagging and hierarchical subject indexing in a special way.
"
3269,Effect of E-printing on Citation Rates in Astronomy and Physics,"  In this report we examine the change in citation behavior since the
introduction of the arXiv e-print repository (Ginsparg, 2001). It has been
observed that papers that initially appear as arXiv e-prints get cited more
than papers that do not (Lawrence, 2001; Brody et al., 2004; Schwarz &
Kennicutt, 2004; Kurtz et al., 2005a, Metcalfe, 2005). Using the citation
statistics from the NASA-Smithsonian Astrophysics Data System (ADS; Kurtz et
al., 1993, 2000), we confirm the findings from other studies, we examine the
average citation rate to e-printed papers in the Astrophysical Journal, and we
show that for a number of major astronomy and physics journals the most
important papers are submitted to the arXiv e-print repository first.
"
3270,Designing a Multi-petabyte Database for LSST,"  The 3.2 giga-pixel LSST camera will produce approximately half a petabyte of
archive images every month. These data need to be reduced in under a minute to
produce real-time transient alerts, and then added to the cumulative catalog
for further analysis. The catalog is expected to grow about three hundred
terabytes per year. The data volume, the real-time transient alerting
requirements of the LSST, and its spatio-temporal aspects require innovative
techniques to build an efficient data access system at reasonable cost. As
currently envisioned, the system will rely on a database for catalogs and
metadata. Several database systems are being evaluated to understand how they
perform at these data rates, data volumes, and access patterns. This paper
describes the LSST requirements, the challenges they impose, the data access
philosophy, results to date from evaluating available database technologies
against LSST requirements, and the proposed database architecture to meet the
data challenges.
"
3271,Collaborative Tagging and Semiotic Dynamics,"  Collaborative tagging has been quickly gaining ground because of its ability
to recruit the activity of web users into effectively organizing and sharing
vast amounts of information. Here we collect data from a popular system and
investigate the statistical properties of tag co-occurrence. We introduce a
stochastic model of user behavior embodying two main aspects of collaborative
tagging: (i) a frequency-bias mechanism related to the idea that users are
exposed to each other's tagging activity; (ii) a notion of memory - or aging of
resources - in the form of a heavy-tailed access to the past state of the
system. Remarkably, our simple modeling is able to account quantitatively for
the observed experimental features, with a surprisingly high accuracy. This
points in the direction of a universal behavior of users, who - despite the
complexity of their own cognitive processes and the uncoordinated and selfish
nature of their tagging activity - appear to follow simple activity patterns.
"
3272,Toward a Collection-based Metadata Maintenance Model,"  In this paper, the authors identify key entities and relationships in the
operational management of metadata catalogs that describe digital collections,
and they draft a data model to support the administration of metadata
maintenance for collections. Further, they consider this proposed model in
light of other data schemes to which it relates and discuss the implications of
the model for library metadata maintenance operations.
"
3273,Mapping the Bid Behavior of Conference Referees,"  The peer-review process, in its present form, has been repeatedly criticized.
Of the many critiques ranging from publication delays to referee bias, this
paper will focus specifically on the issue of how submitted manuscripts are
distributed to qualified referees. Unqualified referees, without the proper
knowledge of a manuscript's domain, may reject a perfectly valid study or
potentially more damaging, unknowingly accept a faulty or fraudulent result. In
this paper, referee competence is analyzed with respect to referee bid data
collected from the 2005 Joint Conference on Digital Libraries (JCDL). The
analysis of the referee bid behavior provides a validation of the intuition
that referees are bidding on conference submissions with regards to the subject
domain of the submission. Unfortunately, this relationship is not strong and
therefore suggests that there exists other factors beyond subject domain that
may be influencing referees to bid for particular submissions.
"
3274,A Metadata Registry from Vocabularies UP: The NSDL Registry Project,"  The NSDL Metadata Registry is designed to provide humans and machines with
the means to discover, create, access and manage metadata schemes, schemas,
application profiles, crosswalks and concept mappings. This paper describes the
general goals and architecture of the NSDL Metadata Registry as well as issues
encountered during the first year of the project's implementation.
"
3275,An Algorithm to Determine Peer-Reviewers,"  The peer-review process is the most widely accepted certification mechanism
for officially accepting the written results of researchers within the
scientific community. An essential component of peer-review is the
identification of competent referees to review a submitted manuscript. This
article presents an algorithm to automatically determine the most appropriate
reviewers for a manuscript by way of a co-authorship network data structure and
a relative-rank particle-swarm algorithm. This approach is novel in that it is
not limited to a pre-selected set of referees, is computationally efficient,
requires no human-intervention, and, in some instances, can automatically
identify conflict of interest situations. A useful application of this
algorithm would be to open commentary peer-review systems because it provides a
weighting for each referee with respects to their expertise in the domain of a
manuscript. The algorithm is validated using referee bid data from the 2005
Joint Conference on Digital Libraries.
"
3276,An Architecture for the Aggregation and Analysis of Scholarly Usage Data,"  Although recording of usage data is common in scholarly information services,
its exploitation for the creation of value-added services remains limited due
to concerns regarding, among others, user privacy, data validity, and the lack
of accepted standards for the representation, sharing and aggregation of usage
data. This paper presents a technical, standards-based architecture for sharing
usage information, which we have designed and implemented. In this
architecture, OpenURL-compliant linking servers aggregate usage information of
a specific user community as it navigates the distributed information
environment that it has access to. This usage information is made OAI-PMH
harvestable so that usage information exposed by many linking servers can be
aggregated to facilitate the creation of value-added services with a reach
beyond that of a single community or a single information service. This paper
also discusses issues that were encountered when implementing the proposed
approach, and it presents preliminary results obtained from analyzing a usage
data set containing about 3,500,000 requests aggregated by a federation of
linking servers at the California State University system over a 20 month
period.
"
3277,Analyzing Large Collections of Electronic Text Using OLAP,"  Computer-assisted reading and analysis of text has various applications in
the humanities and social sciences. The increasing size of many electronic text
archives has the advantage of a more complete analysis but the disadvantage of
taking longer to obtain results. On-Line Analytical Processing is a method used
to store and quickly analyze multidimensional data. By storing text analysis
information in an OLAP system, a user can obtain solutions to inquiries in a
matter of seconds as opposed to minutes, hours, or even days. This analysis is
user-driven allowing various users the freedom to pursue their own direction of
research.
"
3278,Repository Replication Using NNTP and SMTP,"  We present the results of a feasibility study using shared, existing,
network-accessible infrastructure for repository replication. We investigate
how dissemination of repository contents can be ``piggybacked'' on top of
existing email and Usenet traffic. Long-term persistence of the replicated
repository may be achieved thanks to current policies and procedures which
ensure that mail messages and news posts are retrievable for evidentiary and
other legal purposes for many years after the creation date. While the
preservation issues of migration and emulation are not addressed with this
approach, it does provide a simple method of refreshing content with unknown
partners.
"
3279,"Ten-Year Cross-Disciplinary Comparison of the Growth of Open Access and
  How it Increases Research Citation Impact","  Lawrence (2001)found computer science articles that were openly accessible
(OA) on the Web were cited more. We replicated this in physics. We tested
1,307,038 articles published across 12 years (1992-2003) in 10 disciplines
(Biology, Psychology, Sociology, Health, Political Science, Economics,
Education, Law, Business, Management). A robot trawls the Web for full-texts
using reference metadata ISI citation data (signal detectability d'=2.45; bias
= 0.52). Percentage OA (relative to total OA + NOA) articles varies from 5%-16%
(depending on discipline, year and country) and is slowly climbing annually
(correlation r=.76, sample size N=12, probability p < 0.005). Comparing OA and
NOA articles in the same journal/year, OA articles have consistently more
citations, the advantage varying from 36%-172% by discipline and year.
Comparing articles within six citation ranges (0, 1, 2-3, 4-7, 8-15, 16+
citations), the annual percentage of OA articles is growing significantly
faster than NOA within every citation range (r > .90, N=12, p < .0005) and the
effect is greater with the more highly cited articles (r = .98, N=6, p < .005).
Causality cannot be determined from these data, but our prior finding of a
similar pattern in physics, where percent OA is much higher (and even
approaches 100% in some subfields), makes it unlikely that the OA citation
advantage is merely or mostly a self-selection bias (for making only one's
better articles OA). Further research will analyze the effect's timing, causal
components and relation to other variables.
"
3280,Generalized h-index for Disclosing Latent Facts in Citation Networks,"  What is the value of a scientist and its impact upon the scientific thinking?
How can we measure the prestige of a journal or of a conference? The evaluation
of the scientific work of a scientist and the estimation of the quality of a
journal or conference has long attracted significant interest, due to the
benefits from obtaining an unbiased and fair criterion. Although it appears to
be simple, defining a quality metric is not an easy task. To overcome the
disadvantages of the present metrics used for ranking scientists and journals,
J.E. Hirsch proposed a pioneering metric, the now famous h-index. In this
article, we demonstrate several inefficiencies of this index and develop a pair
of generalizations and effective variants of it to deal with scientist ranking
and with publication forum ranking. The new citation indices are able to
disclose trendsetters in scientific research, as well as researchers that
constantly shape their field with their influential work, no matter how old
they are. We exhibit the effectiveness and the benefits of the new indices to
unfold the full potential of the h-index, with extensive experimental results
obtained from DBLP, a widely known on-line digital library.
"
3281,Citation as a Representation Process,"  The presented work proposes a novel approach to model the citation rate. The
paper begins with a brief introduction into informetrics studies and highlights
drawbacks of the contemporary approaches to modeling the citation process as a
product of social interactions. An alternative modeling framework based on
results obtained in cognitive psychology is then introduced and applied in an
experiment to investigate properties of the citation process, as they are
revealed by a large collection of citation statistics. Major research findings
are discussed, and a summary is given.
"
3282,Separating the articles of authors with the same name,"  I describe a method to separate the articles of different authors with the
same name. It is based on a distance between any two publications, defined in
terms of the probability that they would have as many coincidences if they were
drawn at random from all published documents. Articles with a given author name
are then clustered according to their distance, so that all articles in a
cluster belong very likely to the same author. The method has proven very
useful in generating groups of papers that are then selected manually. This
simplifies considerably citation analysis when the author publication lists are
not available.
"
3283,"myADS-arXiv - a Tailor-Made, Open Access, Virtual Journal","  The myADS-arXiv service provides the scientific community with a one stop
shop for staying up-to-date with a researcher's field of interest. The service
provides a powerful and unique filter on the enormous amount of bibliographic
information added to the ADS on a daily basis. It also provides a complete view
with the most relevant papers available in the subscriber's field of interest.
With this service, the subscriber will get to know the lastest developments,
popular trends and the most important papers. This makes the service not only
unique from a technical point of view, but also from a content point of view.
On this poster we will argue why myADS-arXiv is a tailor-made, open access,
virtual journal and we will illustrate its unique character.
"
3284,GDF - A general dataformat for biosignals,"  Biomedical signals are stored in many different data formats. Most formats
have been developed for a specific purpose of a specialized community for ECG
research, EEG analysis, sleep research, etc. So far none of the existing
formats can be considered a general purpose data format for biomedical signals.
In order to solve this problem and to unify the various needs of the various
biomedical signal processing fields, the so-called ""General Data Format for
biomedical signals"" (GDF) is developed. This GDF format is fully described and
specified. Software for reading and writing GDF data is implemented in
Octave/Matlab and C/C++ and provided through BioSig - an free and open source
software library for biomedical signal processing. BioSig privides also
converters from various data formats to GDF, and a viewing and scoring
software.
"
3285,E-prints and Journal Articles in Astronomy: a Productive Co-existence,"  Are the e-prints (electronic preprints) from the arXiv repository being used
instead of the journal articles? In this paper we show that the e-prints have
not undermined the usage of journal papers in the astrophysics community. As
soon as the journal article is published, the astronomical community prefers to
read the journal article and the use of e-prints through the NASA Astrophysics
Data System drops to zero. This suggests that the majority of astronomers have
access to institutional subscriptions and that they choose to read the journal
article when given the choice. Within the NASA Astrophysics Data System they
are given this choice, because the e-print and the journal article are treated
equally, since both are just one click away. In other words, the e-prints have
not undermined journal use in the astrophysics community and thus currently do
not pose a financial threat to the publishers. We present readership data for
the arXiv category ""astro-ph"" and the 4 core journals in astronomy
(Astrophysical Journal, Astronomical Journal, Monthly Notices of the Royal
Astronomical Society and Astronomy & Astrophysics). Furthermore, we show that
the half-life (the point where the use of an article drops to half the use of a
newly published article) for an e-print is shorter than for a journal paper.
  The ADS is funded by NASA Grant NNG06GG68G. arXiv receives funding from NSF
award #0404553
"
3286,Full Text Searching in the Astrophysics Data System,"  The Smithsonian/NASA Astrophysics Data System (ADS) provides a search system
for the astronomy and physics scholarly literature. All major and many smaller
astronomy journals that were published on paper have been scanned back to
volume 1 and are available through the ADS free of charge. All scanned pages
have been converted to text and can be searched through the ADS Full Text
Search System. In addition, searches can be fanned out to several external
search systems to include the literature published in electronic form. Results
from the different search systems are combined into one results list.
  The ADS Full Text Search System is available at:
http://adsabs.harvard.edu/fulltext_service.html
"
3287,Connectivity in the Astronomy Digital Library,"  The Astrophysics Data System (ADS) provides an extensive system of links
between the literature and other on-line information. Recently, the journals of
the American Astronomical Society (AAS) and a group of NASA data centers have
collaborated to provide more links between on-line data obtained by space
missions and the on-line journals. Authors can now specify which data sets they
have used in their article. This information is used by the participants to
provide the links between the literature and the data.
  The ADS is available at: http://ads.harvard.edu
"
3288,Creation and use of Citations in the ADS,"  With over 20 million records, the ADS citation database is regularly used by
researchers and librarians to measure the scientific impact of individuals,
groups, and institutions. In addition to the traditional sources of citations,
the ADS has recently added references extracted from the arXiv e-prints on a
nightly basis. We review the procedures used to harvest and identify the
reference data used in the creation of citations, the policies and procedures
that we follow to avoid double-counting and to eliminate contributions which
may not be scholarly in nature. Finally, we describe how users and institutions
can easily obtain quantitative citation data from the ADS, both interactively
and via web-based programming tools.
  The ADS is available at http://ads.harvard.edu.
"
3289,Data in the ADS -- Understanding How to Use it Better,"  The Smithsonian/NASA ADS Abstract Service contains a wealth of data for
astronomers and librarians alike, yet the vast majority of usage consists of
rudimentary searches. Hints on how to obtain more focused search results by
using more of the various capabilities of the ADS are presented, including
searching by affiliation. We also discuss the classification of articles by
content and by referee status.
  The ADS is funded by NASA Grant NNG06GG68G-16613687.
"
3290,Paper to Screen: Processing Historical Scans in the ADS,"  The NASA Astrophysics Data System in conjunction with the Wolbach Library at
the Harvard-Smithsonian Center for Astrophysics is working on a project to
microfilm historical observatory publications. The microfilm is then scanned
for inclusion in the ADS. The ADS currently contains over 700,000 scanned pages
of volumes of historical literature. Many of these volumes lack clear
pagination or other bibliographic data that are necessary to take advantage of
the searching capabilities of the ADS. This paper will address some of the
interesting challenges that needed to be resolved during the processing of the
Observatory Reports included in the ADS.
"
3291,Pathways: Augmenting interoperability across scholarly repositories,"  In the emerging eScience environment, repositories of papers, datasets,
software, etc., should be the foundation of a global and natively-digital
scholarly communications system. The current infrastructure falls far short of
this goal. Cross-repository interoperability must be augmented to support the
many workflows and value-chains involved in scholarly communication. This will
not be achieved through the promotion of single repository architecture or
content representation, but instead requires an interoperability framework to
connect the many heterogeneous systems that will exist.
  We present a simple data model and service architecture that augments
repository interoperability to enable scholarly value-chains to be implemented.
We describe an experiment that demonstrates how the proposed infrastructure can
be deployed to implement the workflow involved in the creation of an overlay
journal over several different repository systems (Fedora, aDORe, DSpace and
arXiv).
"
3292,Constructing experimental indicators for Open Access documents,"  The ongoing paradigm change in the scholarly publication system ('science is
turning to e-science') makes it necessary to construct alternative evaluation
criteria/metrics which appropriately take into account the unique
characteristics of electronic publications and other research output in digital
formats. Today, major parts of scholarly Open Access (OA) publications and the
self-archiving area are not well covered in the traditional citation and
indexing databases. The growing share and importance of freely accessible
research output demands new approaches/metrics for measuring and for evaluating
of these new types of scientific publications. In this paper we propose a
simple quantitative method which establishes indicators by measuring the
access/download pattern of OA documents and other web entities of a single web
server. The experimental indicators (search engine, backlink and direct access
indicator) are constructed based on standard local web usage data. This new
type of web-based indicator is developed to model the specific demand for
better study/evaluation of the accessibility, visibility and interlinking of
open accessible documents. We conclude that e-science will need new stable
e-indicators.
"
3293,"Usage Impact Factor: the effects of sample characteristics on
  usage-based impact metrics","  There exist ample demonstrations that indicators of scholarly impact
analogous to the citation-based ISI Impact Factor can be derived from usage
data. However, contrary to the ISI IF which is based on citation data generated
by the global community of scholarly authors, so far usage can only be
practically recorded at a local level leading to community-specific assessments
of scholarly impact that are difficult to generalize to the global scholarly
community. We define a journal Usage Impact Factor which mimics the definition
of the Thomson Scientific's ISI Impact Factor. Usage Impact Factor rankings are
calculated on the basis of a large-scale usage data set recorded for the
California State University system from 2003 to 2005. The resulting journal
rankings are then compared to Thomson Scientific's ISI Impact Factor which is
used as a baseline indicator of general impact. Our results indicate that
impact as derived from California State University usage reflects the
particular scientific and demographic characteristics of its communities.
"
3294,Protocols for Scholarly Communication,"  CERN, the European Organization for Nuclear Research, has operated an
institutional preprint repository for more than 10 years. The repository
contains over 850,000 records of which more than 450,000 are full-text OA
preprints, mostly in the field of particle physics, and it is integrated with
the library's holdings of books, conference proceedings, journals and other
grey literature. In order to encourage effective propagation and open access to
scholarly material, CERN is implementing a range of innovative library services
into its document repository: automatic keywording, reference extraction,
collaborative management tools and bibliometric tools. Some of these services,
such as user reviewing and automatic metadata extraction, could make up an
interesting testbed for future publishing solutions and certainly provide an
exciting environment for e-science possibilities. The future protocol for
scientific communication should naturally guide authors towards OA publication
and CERN wants to help reach a full open access publishing environment for the
particle physics community and the related sciences in the next few years.
"
3295,"Intra-site Level Cultural Heritage Documentation: Combination of Survey,
  Modeling and Imagery Data in a Web Information System","  Cultural heritage documentation induces the use of computerized techniques to
manage and preserve the information produced. Geographical information systems
have proved their potentialities in this scope, but they are not always adapted
for the management of features at the scale of a particular archaeological
site. Moreover, computer applications in archaeology are often technology
driven and software constrained. Thus, we propose a tool that tries to avoid
these difficulties. We are developing an information system that works over the
Internet and that is joined with a web site. Aims are to assist the work of
archaeological sites managers and to be a documentation tool about these sites,
dedicated to everyone. We devote therefore our system both to the professionals
who are in charge of the site, and to the general public who visits it or who
wants to have information on it. The system permits to do exploratory analyses
of the data, especially at spatial and temporal levels. We propose to record
metadata about the archaeological features in XML and to access these features
through interactive 2D and 3D representations, and through queries systems
(keywords and images). The 2D images, photos, or vectors are generated in SVG,
while 3D models are generated in X3D. Archaeological features are also
automatically integrated in a MySQL database. The web site is an exchange
platform with the information system and is written in PHP. Our first
application case is the medieval castle of Vianden, Luxembourg.
"
3296,"The effect of 'Open Access' upon citation impact: An analysis of ArXiv's
  Condensed Matter Section","  This article statistically analyses how the citation impact of articles
deposited in the Condensed Matter section of the preprint server ArXiv (hosted
by Cornell University), and subsequently published in a scientific journal,
compares to that of articles in the same journal that were not deposited in
that archive. Its principal aim is to further illustrate and roughly estimate
the effect of two factors, 'early view' and 'quality bias', upon differences in
citation impact between these two sets of papers, using citation data from
Thomson Scientific's Web of Science. It presents estimates for a number of
journals in the field of condensed matter physics. In order to discriminate
between an 'open access' effect and an early view effect, longitudinal citation
data was analysed covering a time period as long as 7 years. Quality bias was
measured by calculating ArXiv citation impact differentials at the level of
individual authors publishing in a journal, taking into account co-authorship.
The analysis provided evidence of a strong quality bias and early view effect.
Correcting for these effects, there is in a sample of 6 condensed matter
physics journals studied in detail, no sign of a general 'open access
advantage' of papers deposited in ArXiv. The study does provide evidence that
ArXiv accelerates citation, due to the fact that that ArXiv makes papers
earlier available rather than that it makes papers freely available.
"
3297,Wikipedia: organisation from a bottom-up approach,"  Wikipedia can be considered as an extreme form of a self-managing team, as a
means of labour division. One could expect that this bottom-up approach, with
the absense of top-down organisational control, would lead to a chaos, but our
analysis shows that this is not the case. In the Dutch Wikipedia, an integrated
and coherent data structure is created, while at the same time users succeed in
distributing roles by self-selection. Some users focus on an area of expertise,
while others edit over the whole encyclopedic range. This constitutes our
conclusion that Wikipedia, in general, is a successful example of a
self-managing team.
"
3298,Quantitative Analysis of the Publishing Landscape in High-Energy Physics,"  World-wide collaboration in high-energy physics (HEP) is a tradition which
dates back several decades, with scientific publications mostly coauthored by
scientists from different countries. This coauthorship phenomenon makes it
difficult to identify precisely the ``share'' of each country in HEP scientific
production. One year's worth of HEP scientific articles published in
peer-reviewed journals is analysed and their authors are uniquely assigned to
countries. This method allows the first correct estimation on a ``pro rata''
basis of the share of HEP scientific publishing among several countries and
institutions. The results provide an interesting insight into the geographical
collaborative patterns of the HEP community. The HEP publishing landscape is
further analysed to provide information on the journals favoured by the HEP
community and on the geographical variation of their author bases. These
results provide quantitative input to the ongoing debate on the possible
transition of HEP publishing to an Open Access model.
"
3299,"Why is Open Access Development so Successful? Stigmergic organization
  and the economics of information","  The explosive development of ""free"" or ""open source"" information goods
contravenes the conventional wisdom that markets and commercial organizations
are necessary to efficiently supply products. This paper proposes a theoretical
explanation for this phenomenon, using concepts from economics and theories of
self-organization. Once available on the Internet, information is intrinsically
not a scarce good, as it can be replicated virtually without cost. Moreover,
freely distributing information is profitable to its creator, since it improves
the quality of the information, and enhances the creator's reputation. This
provides a sufficient incentive for people to contribute to open access
projects. Unlike traditional organizations, open access communities are open,
distributed and self-organizing. Coordination is achieved through stigmergy:
listings of ""work-in-progress"" direct potential contributors to the tasks where
their contribution is most likely to be fruitful. This obviates the need both
for centralized planning and for the ""invisible hand"" of the market.
"
3300,Bias in the journal impact factor,"  The ISI journal impact factor (JIF) is based on a sample that may represent
half the whole-of-life citations to some journals, but a small fraction (<10%)
of the citations accruing to other journals. This disproportionate sampling
means that the JIF provides a misleading indication of the true impact of
journals, biased in favour of journals that have a rapid rather than a
prolonged impact. Many journals exhibit a consistent pattern of citation
accrual from year to year, so it may be possible to adjust the JIF to provide a
more reliable indication of a journal's impact.
"
3301,"Electronic Laboratory Notebook Assisting Reflectance Spectrometry in
  Legal Medicine","  Reflectance spectrometry is a fast and reliable method for the
characterisation of human skin if the spectra are analysed with respect to a
physical model describing the optical properties of human skin. For a field
study performed at the Institute of Legal Medicine and the Freiburg Materials
Research Center of the University of Freiburg an electronic laboratory notebook
has been developed, which assists in the recording, management, and analysis of
reflectance spectra. The core of the electronic laboratory notebook is a MySQL
database. It is filled with primary data via a web interface programmed in
Java, which also enables the user to browse the database and access the results
of data analysis. These are carried out by Matlab, Tcl and
  Python scripts, which retrieve the primary data from the electronic
laboratory notebook, perform the analysis, and store the results in the
database for further usage.
"
3302,"A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus,
  and Google Scholar","  Academic institutions, federal agencies, publishers, editors, authors, and
librarians increasingly rely on citation analysis for making hiring, promotion,
tenure, funding, and/or reviewer and journal evaluation and selection
decisions. The Institute for Scientific Information's (ISI) citation databases
have been used for decades as a starting point and often as the only tools for
locating citations and/or conducting citation analyses. ISI databases (or Web
of Science), however, may no longer be adequate as the only or even the main
sources of citations because new databases and tools that allow citation
searching are now available. Whether these new databases and tools complement
or represent alternatives to Web of Science (WoS) is important to explore.
Using a group of 15 library and information science faculty members as a case
study, this paper examines the effects of using Scopus and Google Scholar (GS)
on the citation counts and rankings of scholars as measured by WoS. The paper
discusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap
and uniqueness, quality and language of the citations, and the implications of
the findings for citation analysis. The project involved citation searching for
approximately 1,100 scholarly works published by the study group and over 200
works by a test group (an additional 10 faculty members). Overall, more than
10,000 citing and purportedly citing documents were examined. WoS data took
about 100 hours of collecting and processing time, Scopus consumed 200 hours,
and GS a grueling 3,000 hours.
"
3303,Finding Astronomical Communities Through Co-readership Analysis,"  Whenever a large group of people are engaged in an activity, communities will
form. The nature of these communities depends on the relationship considered.
In the group of people who regularly use scholarly literature, a relationship
like ``person i and person j have cited the same paper'' might reveal
communities of people working in a particular field. On this poster, we will
investigate the relationship ``person i and person j have read the same
paper''. Using the data logs of the NASA/Smithsonian Astrophysics Data System
(ADS), we first determine the population that will participate by requiring
that a user queries the ADS at a certain rate. Next, we apply the relationship
to this population. The result of this will be an abstract ``relationship
space'', which we will describe in terms of various ``representations''.
Examples of such ``representations'' are the projection of co-read vectors onto
Principal Components and the spectral density of the co-read network. We will
show that the co-read relationship results in structure, we will describe this
structure and we will provide a first attempt in the classification of this
structure in terms of astronomical communities.
  The ADS is funded by NASA Grant NNG06GG68G.
"
3304,On the robustness of the h-index,"  The h-index (Hirsch, 2005) is robust, remaining relatively unaffected by
errors in the long tails of the citations-rank distribution, such as
typographic errors that short-change frequently-cited papers and create bogus
additional records. This robustness, and the ease with which h-indices can be
verified, support the use of a Hirsch-type index over alternatives such as the
journal impact factor. These merits of the h-index apply to both individuals
and to journals.
"
3305,"Citation advantage of Open Access articles likely explained by quality
  differential and media effects","  In a study of articles published in the Proceedings of the National Academy
of Sciences, Gunther Eysenbach discovered a significant citation advantage for
those articles made freely-available upon publication (Eysenbach 2006). While
the author attempted to control for confounding factors that may have explained
the citation differential, the study was unable to control for characteristics
of the article that may have led some authors to pay the additional page
charges ($1,000) for immediate OA status. OA articles published in PNAS were
more than twice as likely to be featured on the front cover of the journal
(3.3% vs. 1.4%), nearly twice as likely to be picked up by the media (15% vs.
8%) and when cited reached, on average, nearly twice as many news outlets as
subscription-based articles (4.2 vs. 2.6). The citation advantage of Open
Access articles in PNAS may likely be explained by a quality differential and
the amplification of media effects.
"
3306,Why is a new Journal of Informetrics needed?,"  In our study we analysed 3.889 records which were indexed in the Library and
Information Science Abstracts (LISA) database in the research field of
informetrics. We can show the core journals of the field via a Bradford (power
law) distribution and corroborate on the basis of the restricted LISA data set
that it was the appropriate time to found a new specialized journal dedicated
to informetrics. According to Bradford's Law of scattering (pure quantitative
calculation), Egghe's Journal of Informetrics (JOI) first issue to appear in
2007, comes most probable at the right time.
"
3307,"Citation Advantage For OA Self-Archiving Is Independent of Journal
  Impact Factor, Article Age, and Number of Co-Authors","  Eysenbach has suggested that the OA (Green) self-archiving advantage might
just be an artifact of potential uncontrolled confounding factors such as
article age (older articles may be both more cited and more likely to be
self-archived), number of authors (articles with more authors might be more
cited and more self-archived), subject matter (the subjects that are cited
more, self-archive more), country (same thing), number of authors, citation
counts of authors, etc. Chawki Hajjem (doctoral candidate, UQaM) had already
shown that the OA advantage was present in all cases when articles were
analysed separately by age, subject matter or country. He has now done a
multiple regression analysis jointly testing (1) article age, (2) journal
impact factor, (3) number of authors, and (4) OA self-archiving as separate
factors for 442,750 articles in 576 (biomedical) journals across 11 years, and
has shown that each of the four factors contributes an independent,
statistically significant increment to the citation counts. The
OA-self-archiving advantage remains a robust, independent factor. Having
successfully responded to his challenge, we now challenge Eysenbach to
demonstrate -- by testing a sufficiently broad and representative sample of
journals at all levels of the journal quality, visibility and prestige
hierarchy -- that his finding of a citation advantage for Gold OA (articles
published OA on the high-profile website of the only journal he tested (PNAS)
over Green OA articles in the same journal (self-archived on the author's
website) was not just an artifact of having tested only one very high-profile
journal.
"
3308,The Open Access Citation Advantage: Quality Advantage Or Quality Bias?,"  Many studies have now reported the positive correlation between Open Access
(OA) self-archiving and citation counts (""OA Advantage,"" OAA). But does this
OAA occur because (QB) authors are more likely to self-selectively self-archive
articles that are more likely to be cited (self-selection ""Quality Bias"": QB)?
or because (QA) articles that are self-archived are more likely to be cited
(""Quality Advantage"": QA)? The probable answer is both. Three studies [by (i)
Kurtz and co-workers in astrophysics, (ii) Moed in condensed matter physics,
and (iii) Davis & Fromerth in mathematics] had reported the OAA to be due to QB
[plus Early Advantage, EA, from self-archiving the preprint before publication,
in (i) and (ii)] rather than QA. These three fields, however, (1) have less of
a postprint access problem than most other fields and (i) and (ii) also happen
to be among the minority of fields that (2) make heavy use of prepublication
preprints. Chawki Hajjem has now analyzed preliminary evidence based on over
100,000 articles from multiple fields, comparing self-selected self-archiving
with mandated self-archiving to estimate the contributions of QB and QA to the
OAA. Both factors contribute, and the contribution of QA is greater.
"
3309,"On the Software and Knowledge Engineering Aspects of the Educational
  Process","  The Hellenic Open University has embarked on a large-scale effort to enhance
its textbook-based material with content that demonstrably supports the basic
tenets of distance learning. The challenge is to set up a framework that allows
for the production-level creation, distribution and consumption of content, and
at the same time, evaluate the effort in terms of technological, educational
and organizational knowledge gained. This paper presents a model of the
educational process that is used as a development backbone and argues about its
conceptual and technical practicality at large.
"
3310,Plagiarism Detection in arXiv,"  We describe a large-scale application of methods for finding plagiarism in
research document collections. The methods are applied to a collection of
284,834 documents collected by arXiv.org over a 14 year period, covering a few
different research disciplines. The methodology efficiently detects a variety
of problematic author behaviors, and heuristics are developed to reduce the
number of false positives. The methods are also efficient enough to implement
as a real-time submission screen for a collection many times larger.
"
3311,Exploring the academic invisible web,"  Purpose: To provide a critical review of Bergman's 2001 study on the Deep
Web. In addition, we bring a new concept into the discussion, the Academic
Invisible Web (AIW). We define the Academic Invisible Web as consisting of all
databases and collections relevant to academia but not searchable by the
general-purpose internet search engines. Indexing this part of the Invisible
Web is central to scientific search engines. We provide an overview of
approaches followed thus far. Design/methodology/approach: Discussion of
measures and calculations, estimation based on informetric laws. Literature
review on approaches for uncovering information from the Invisible Web.
Findings: Bergman's size estimate of the Invisible Web is highly questionable.
We demonstrate some major errors in the conceptual design of the Bergman paper.
A new (raw) size estimate is given. Research limitations/implications: The
precision of our estimate is limited due to a small sample size and lack of
reliable data. Practical implications: We can show that no single library alone
will be able to index the Academic Invisible Web. We suggest collaboration to
accomplish this task. Originality/value: Provides library managers and those
interested in developing academic search engines with data on the size and
attributes of the Academic Invisible Web.
"
3312,Assessing the Value of Coooperation in Wikipedia,"  Since its inception six years ago, the online encyclopedia Wikipedia has
accumulated 6.40 million articles and 250 million edits, contributed in a
predominantly undirected and haphazard fashion by 5.77 million unvetted
volunteers. Despite the apparent lack of order, the 50 million edits by 4.8
million contributors to the 1.5 million articles in the English-language
Wikipedia follow strong certain overall regularities. We show that the
accretion of edits to an article is described by a simple stochastic mechanism,
resulting in a heavy tail of highly visible articles with a large number of
edits. We also demonstrate a crucial correlation between article quality and
number of edits, which validates Wikipedia as a successful collaborative
effort.
"
3313,A Comparison of On-Line Computer Science Citation Databases,"  This paper examines the difference and similarities between the two on-line
computer science citation databases DBLP and CiteSeer. The database entries in
DBLP are inserted manually while the CiteSeer entries are obtained autonomously
via a crawl of the Web and automatic processing of user submissions. CiteSeer's
autonomous citation database can be considered a form of self-selected on-line
survey. It is important to understand the limitations of such databases,
particularly when citation information is used to assess the performance of
authors, institutions and funding bodies.
  We show that the CiteSeer database contains considerably fewer single author
papers. This bias can be modeled by an exponential process with intuitive
explanation. The model permits us to predict that the DBLP database covers
approximately 24% of the entire literature of Computer Science. CiteSeer is
also biased against low-cited papers.
  Despite their difference, both databases exhibit similar and significantly
different citation distributions compared with previous analysis of the Physics
community. In both databases, we also observe that the number of authors per
paper has been increasing over time.
"
3314,Characterization of Search Engine Caches,"  Search engines provide cached copies of indexed content so users will have
something to ""click on"" if the remote resource is temporarily or permanently
unavailable. Depending on their proprietary caching strategies, search engines
will purge their indexes and caches of resources that exceed a threshold of
unavailability. Although search engine caches are provided only as an aid to
the interactive user, we are interested in building reliable preservation
services from the aggregate of these limited caching services. But first, we
must understand the contents of search engine caches. In this paper, we have
examined the cached contents of Ask, Google, MSN and Yahoo to profile such
things as overlap between index and cache, size, MIME type and ""staleness"" of
the cached resources. We also examined the overlap of the various caches with
the holdings of the Internet Archive.
"
3315,"We cite as we communicate: A communication model for the citation
  process","  Building on ideas from linguistics, psychology, and social sciences about the
possible mechanisms of human decision-making, we propose a novel theoretical
framework for the citation analysis. Given the existing trend to investigate
citation statistics in the context of various forms of power and Zipfian laws,
we show that the popular models of citation have poor predictive ability and
can hardly provide for an adequate explanation of the observed behavior of the
empirical data. An alternative model is then derived, using the apparatus of
statistical mechanics. The model is applied to approximate the citation
frequencies of scientific articles from two large collections, and it
demonstrates a predictive potential much superior to the one of any of the
citation models known to the authors from the literature. Some analytical
properties of the developed model are discussed, and conclusions are drawn.
Directions for future work are also given at the paper's end.
"
3316,Open Access Scientometrics and the UK Research Assessment Exercise,"  Scientometric predictors of research performance need to be validated by
showing that they have a high correlation with the external criterion they are
trying to predict. The UK Research Assessment Exercise (RAE), together with the
growing movement toward making the full-texts of research articles freely
available on the web -- offer a unique opportunity to test and validate a
wealth of old and new scientometric predictors, through multiple regression
analysis: Publications, journal impact factors, citations, co-citations,
citation chronometrics (age, growth, latency to peak, decay rate),
hub/authority scores, h-index, prior funding, student counts, co-authorship
scores, endogamy/exogamy, textual proximity, download/co-downloads and their
chronometrics, etc. can all be tested and validated jointly, discipline by
discipline, against their RAE panel rankings in the forthcoming parallel
panel-based and metric RAE in 2008. The weights of each predictor can be
calibrated to maximize the joint correlation with the rankings. Open Access
Scientometrics will provide powerful new means of navigating, evaluating,
predicting and analyzing the growing Open Access database, as well as powerful
incentives for making it grow faster. ~
"
3317,Microsoft TerraServer,"  The Microsoft TerraServer stores aerial and satellite images of the earth in
a SQL Server Database served to the public via the Internet. It is the world's
largest atlas, combining five terabytes of image data from the United States
Geodetic Survey, Sovinformsputnik, and Encarta Virtual Globe. Internet browsers
provide intuitive spatial and gazetteer interfaces to the data. The TerraServer
is also an E-Commerce application. Users can buy the right to use the imagery
using Microsoft Site Servers managed by the USGS and Aerial Images. This paper
describes the TerraServer's design and implementation.
"
3318,Novelty and Social Search in the World Wide Web,"  The World Wide Web is fast becoming a source of information for a large part
of the world's population. Because of its sheer size and complexity users often
resort to recommendations from others to decide which sites to visit. We
present a dynamical theory of recommendations which predicts site visits by
users of the World Wide Web. We show that it leads to a universal power law for
the number of users that visit given sites over periods of time, with an
exponent related to the rate at which users discover new sites on their own. An
extensive empirical study of user behavior in the Web that we conducted
confirms the existence of this law of influence while yielding bounds on the
rate of novelty encountered by users.
"
3319,The Asilomar Report on Database Research,"  The database research community is rightly proud of success in basic
research, and its remarkable record of technology transfer. Now the field needs
to radically broaden its research focus to attack the issues of capturing,
storing, analyzing, and presenting the vast array of online data. The database
research community should embrace a broader research agenda -- broadening the
definition of database management to embrace all the content of the Web and
other online data stores, and rethinking our fundamental assumptions in light
of technology shifts. To accelerate this transition, we recommend changing the
way research results are evaluated and presented. In particular, we advocate
encouraging more speculative and long-range work, moving conferences to a
poster format, and publishing all research literature on the Web.
"
3320,Digitizing Legacy Documents: A Knowledge-Base Preservation Project,"  This paper addresses the issue of making legacy information (that material
held in paper format only) electronically searchable and retrievable. We used
proprietary software and commercial hardware to create a process for scanning,
cataloging, archiving and electronically disseminating full-text documents.
This process is relatively easy to implement and reasonably affordable.
"
3321,"Vocal Access to a Newspaper Archive: Design Issues and Preliminary
  Investigation","  This paper presents the design and the current prototype implementation of an
interactive vocal Information Retrieval system that can be used to access
articles of a large newspaper archive using a telephone. The results of
preliminary investigation into the feasibility of such a system are also
presented.
"
3322,Making the most of electronic journals,"  As most electronic journals available today have been derived from print
originals, print journals have become a vital element in the broad development
of electronic journals publishing. Further dependence on the print publishing
model, however, will be a constraint on the continuing development of
e-journals, and a series of conflicts are likely to arise. Making the most of
e-journals requires that a distinctive new publishing model is developed. We
consider some of the issues that will be fundamental in this new model,
starting with user motivations and some reported publisher experiences, both of
which suggest a broadening desire for comprehensive linked archives. This leads
in turn to questions about the impact of rights assignment by authors, in
particular the common practice of giving exlusive rights to publishers for
individual works. Some non-prescriptive solutions are suggested, and four steps
towards optimum e-journals are proposed.
"
3323,"The Computing Research Repository: Promoting the Rapid Dissemination and
  Archiving of Computer Science Research","  We describe the Computing Research Repository (CoRR), a new electronic
archive for rapid dissemination and archiving of computer science research
results. CoRR was initiated in September 1998 through the cooperation of ACM,
LANL (Los Alamos National Laboratory) e-Print archive, and NCSTRL (Networked
Computer Science Technical Research Library. Through its implementation of the
Dienst protocol, CoRR combines the open and extensible architecture of NCSTRL
with the reliable access and well-established management practices of the LANL
XXX e-Print repository. This architecture will allow integration with other
e-Print archives and provides a foundation for a future broad-based scholarly
digital library. We describe the decisions that were made in creating CoRR, the
architecture of the CoRR/NCSTRL interoperation, and issues that have arisen
during the operation of CoRR.
"
3324,"Competition and cooperation: Libraries and publishers in the transition
  to electronic scholarly journals","  The conversion of scholarly journals to digital format is proceeding rapidly,
especially for those from large commercial and learned society publishers. This
conversion offers the best hope for survival for such publishers. The infamous
""journal crisis"" is more of a library cost crisis than a publisher pricing
problem, with internal library costs much higher than the amount spent on
purchasing books and journals. Therefore publishers may be able to retain or
even increase their revenues and profits, while at the same time providing a
superior service. To do this, they will have to take over many of the function
of libraries, and they can do that only in the digital domain. This paper
examines publishers' strategies, how they are likely to evolve, and how they
will affect libraries.
"
3325,"Automatic Identification of Subjects for Textual Documents in Digital
  Libraries","  The amount of electronic documents in the Internet grows very quickly. How to
effectively identify subjects for documents becomes an important issue. In
past, the researches focus on the behavior of nouns in documents. Although
subjects are composed of nouns, the constituents that determine which nouns are
subjects are not only nouns. Based on the assumption that texts are
well-organized and event-driven, nouns and verbs together contribute the
process of subject identification. This paper considers four factors: 1) word
importance, 2) word frequency, 3) word co-occurrence, and 4) word distance and
proposes a model to identify subjects for textual documents. The preliminary
experiments show that the performance of the proposed model is close to that of
human beings.
"
3326,"MyLibrary: A Model for Implementing a User-centered, Customizable
  Interface to a Library's Collection of Information Resources","  The paper describes an extensible model for implementing a user-centered,
customizable interface to a library's collection of information resources. This
model, called MyLibrary, integrates the principles of librarianship
(collection, organization, dissemination, and evaluation) with globally
networked computing resources creating a dynamic, customer-driven front-end to
any library's set of materials. The model supports a framework for libraries to
provide enhanced access to local and remote sets of data, information, and
knowledge. At the same, the model does not overwhelm its users with too much
information because the users control exactly how much information is displayed
to them at any given time. The model is active and not passive; direct human
interaction, computer mediated guidance and communication technologies, as well
as current awareness services all play indispensable roles in this system.
"
3327,"The Alex Catalogue, A Collection of Digital Texts with Automatic Methods
  for Acquisition and Cataloging, User-Defined Typography, Cross-searching of
  Indexed Content, and a Sense of Community","  This paper describes the Alex Catalogue of Electronic Texts, the only
Internet-accessible collection of digital documents allowing the user to 1)
dynamically create customized, typographically readable documents on demand, 2)
search the content of one or more documents from the collection simultaneously,
3) create sets of documents from the collection for review and annotation, and
4) publish these sets of annotated documents in turn fostering a sense of
community around the Catalogue. More than a just a collection of links that
will break over time, Alex is an archive of electronic texts providing
unprecedented access to its content and features allowing it to meet the needs
of a wide variety of users and settings. Furthermore, the process of
maintaining the Catalogue is streamlined with tools for automatic acquisition
and cataloging making it possible to sustain the service with a minimum of
personnel.
"
3328,KEA: Practical Automatic Keyphrase Extraction,"  Keyphrases provide semantic metadata that summarize and characterize
documents. This paper describes Kea, an algorithm for automatically extracting
keyphrases from text. Kea identifies candidate keyphrases using lexical
methods, calculates feature values for each candidate, and uses a
machine-learning algorithm to predict which candidates are good keyphrases. The
machine learning scheme first builds a prediction model using training
documents with known keyphrases, and then uses the model to find keyphrases in
new documents. We use a large test corpus to evaluate Kea's effectiveness in
terms of how many author-assigned keyphrases are correctly identified. The
system is simple, robust, and publicly available.
"
3329,Quality of OCR for Degraded Text Images,"  Commercial OCR packages work best with high-quality scanned images. They
often produce poor results when the image is degraded, either because the
original itself was poor quality, or because of excessive photocopying. The
ability to predict the word failure rate of OCR from a statistical analysis of
the image can help in making decisions in the trade-off between the success
rate of OCR and the cost of human correction of errors. This paper describes an
investigation of OCR of degraded text images using a standard OCR engine (Adobe
Capture). The documents were selected from those in the archive at Los Alamos
National Laboratory. By introducing noise in a controlled manner into perfect
documents, we show how the quality of OCR can be predicted from the nature of
the noise. The preliminary results show that a simple noise model can give good
prediction of the number of OCR errors.
"
3330,Content-Based Book Recommending Using Learning for Text Categorization,"  Recommender systems improve access to relevant products and information by
making personalized suggestions based on previous examples of a user's likes
and dislikes. Most existing recommender systems use social filtering methods
that base recommendations on other users' preferences. By contrast,
content-based methods use information about an item itself to make suggestions.
This approach has the advantage of being able to recommended previously unrated
items to users with unique interests and to provide explanations for its
recommendations. We describe a content-based book recommending system that
utilizes information extraction and a machine-learning algorithm for text
categorization. Initial experimental results demonstrate that this approach can
produce accurate recommendations.
"
3331,Digital Library Technology for Locating and Accessing Scientific Data,"  In this paper we describe our efforts to bring scientific data into the
digital library. This has required extension of the standard WWW, and also the
extension of metadata standards far beyond the Dublin Core. Our system
demonstrates this technology for real scientific data from astronomy.
"
3332,Use and usability in a digital library search system,"  Digital libraries must reach out to users from all walks of life, serving
information needs at all levels. To do this, they must attain high standards of
usability over an extremely broad audience. This paper details the evolution of
one important digital library component as it has grown in functionality and
usefulness over several years of use by a live, unrestricted community. Central
to its evolution have been user studies, analysis of use patterns, and
formative usability evaluation. We extrapolate that all three components are
necessary in the production of successful digital library systems.
"
3333,Resource Discovery in Trilogy,"  Trilogy is a collaborative project whose key aim is the development of an
integrated virtual laboratory to support research training within each
institution and collaborative projects between the partners. In this paper, the
architecture and underpinning platform of the system is described with
particular emphasis being placed on the structure and the integration of the
distributed database. A key element is the ontology that provides the
multi-agent system with a conceptualisation specification of the domain; this
ontology is explained, accompanied by a discussion how such a system is
integrated and used within the virtual laboratory. Although in this paper,
Telecommunications and in particular Broadband networks are used as exemplars,
the underlying system principles are applicable to any domain where a
combination of experimental and literature-based resources are required.
"
3334,"Multimedia Description Framework (MDF) for Content Description of
  Audio/Video Documents","  MPEG is undertaking a new initiative to standardize content description of
audio and video data/documents. When it is finalized in 2001, MPEG-7 is
expected to provide standardized description schemes for concise and
unambiguous content description of data/documents of complex media types.
Meanwhile, other meta-data or description schemes, such as Dublin Core, XML,
etc., are becoming popular in different application domains. In this paper, we
propose the Multimedia Description Framework (MDF), which is designated to
accommodate multiple description (meta-data) schemes, both MPEG-7 and
non-MPEG-7, into integrated architecture. We will use examples to show how MDF
description makes use of combined strength of different description schemes to
enhance its expression power and flexibility. We conclude the paper with
discussion of using MDF description of a movie video to search/retrieve
required scene clips from the movie, on the MDF prototype system we have
implemented.
"
3335,Not Available,"  withdrawn by author
"
3336,ZBroker: A Query Routing Broker for Z39.50 Databases,"  A query routing broker is a software agent that determines from a large set
of accessing information sources the ones most relevant to a user's information
need. As the number of information sources on the Internet increases
dramatically, future users will have to rely on query routing brokers to decide
a small number of information sources to query without incurring too much query
processing overheads. In this paper, we describe a query routing broker known
as ZBroker developed for bibliographic database servers that support the Z39.50
protocol. ZBroker samples the content of each bibliographic database by using
training queries and their results, and summarizes the bibliographic database
content into a knowledge base. We present the design and implementation of
ZBroker and describe its Web-based user interface.
"
3337,Multimodal Surrogates for Video Browsing,"  Three types of video surrogates - visual (keyframes), verbal
(keywords/phrases), and combination of the two - were designed and studied in a
qualitative investigation of user cognitive processes. The results favor the
combined surrogates in which verbal information and images reinforce each
other, lead to better comprehension, and may actually require less processing
time. The results also highlight image features users found most helpful. These
findings will inform the interface design and video representation for video
retrieval and browsing.
"
3338,"Using Query Mediators for Distributed Searching in Federated Digital
  Libraries","  We describe an architecture and investigate the characteristics of
distributed searching in federated digital libraries. We introduce the notion
of a query mediator as a digital library service responsible for selecting
among available search engines, routing queries to those search engines, and
aggregating results. We examine operational data from the NCSTRL distributed
digital library that reveals a number of characteristics of distributed
resource discovery. These include availability and response time of indexers
and the distinction between the query mediator view of these characteristics
and the indexer view.
"
3339,Visualization of Retrieved Documents using a Presentation Server,"  In any search-based digital library (DL) systems dealing with a non-trivial
number of documents, users are often required to go through a long list of
short document descriptions in order to identify what they are looking for. To
tackle the problem, a variety of document organization algorithms and/or
visualization techniques have been used to guide users in selecting relevant
documents. Since these techniques require heavy computations, however, we
developed a presentation server designed to serve as an intermediary between
retrieval servers and clients equipped with a visualization interface. In
addition, we designed our own visual interface by which users can view a set of
documents from different perspectives through layers of document maps. We
finally ran experiments to show that the visual interface, in conjunction with
the presentation server, indeed helps users in selecting relevant documents
from the retrieval results.
"
3340,Semi-Automatic Indexing of Multilingual Documents,"  With the growing significance of digital libraries and the Internet, more and
more electronic texts become accessible to a wide and geographically disperse
public. This requires adequate tools to facilitate indexing, storage, and
retrieval of documents written in different languages. We present a method for
semi-automatic indexing of electronic documents and construction of a
multilingual thesaurus, which can be used for query formulation and information
retrieval. We use special dictionaries and user interaction in order to solve
ambiguities and find adequate canonical terms in the language and adequate
abstract language-independent terms. The abstract thesaurus is updated
incrementally by new indexed documents and is used to search document
concerning terms in a query to the document base.
"
3341,A New Ranking Principle for Multimedia Information Retrieval,"  A theoretic framework for multimedia information retrieval is introduced
which guarantees optimal retrieval effectiveness. In particular, a Ranking
Principle for Distributed Multimedia-Documents (RPDM) is described together
with an algorithm that satisfies this principle. Finally, the RPDM is shown to
be a generalization of the Probability Ranking principle (PRP) which guarantees
optimal retrieval effectiveness in the case of text document retrieval. The PRP
justifies theoretically the relevance ranking adopted by modern search engines.
In contrast to the classical PRP, the new RPDM takes into account transmission
and inspection time, and most importantly, aspectual recall rather than simple
recall.
"
3342,"A Proposal for the Establishment of Review Boards - a flexible approach
  to the selection of academic knowledge","  Paper journals use a small number of trusted academics to select information
on behalf of all their readers. This inflexibility in the selection was
justified due to the expense of publishing. The advent of cheap distribution
via the internet allows a new trade-off between time and expense and the
flexibility of the selection process. This paper explores one such possible
process one where the role of mark-up and archiving is separated from that of
review. The idea is that authors publish their papers on their own web pages or
in a public paper archive, a board of reviewers judge that paper on a number of
different criteria. The detailed results of the reviews are stored in such a
way as to enable readers to use these judgements to find the papers they want
using search engines on the web. Thus instead of journals using generic
selection criteria readers can set their own to suit their needs. The resulting
system might be even cheaper than web-journals to implement.
"
3343,"Designing and Mining Multi-Terabyte Astronomy Archives: The Sloan
  Digital Sky Survey","  The next-generation astronomy digital archives will cover most of the
universe at fine resolution in many wave-lengths, from X-rays to ultraviolet,
optical, and infrared. The archives will be stored at diverse geographical
locations. One of the first of these projects, the Sloan Digital Sky Survey
(SDSS) will create a 5-wavelength catalog over 10,000 square degrees of the sky
(see http://www.sdss.org/). The 200 million objects in the multi-terabyte
database will have mostly numerical attributes, defining a space of 100+
dimensions. Points in this space have highly correlated distributions.
  The archive will enable astronomers to explore the data interactively. Data
access will be aided by a multidimensional spatial index and other indices. The
data will be partitioned in many ways. Small tag objects consisting of the most
popular attributes speed up frequent searches. Splitting the data among
multiple servers enables parallel, scalable I/O and applies parallel processing
to the data. Hashing techniques allow efficient clustering and pair-wise
comparison algorithms that parallelize nicely. Randomly sampled subsets allow
debugging otherwise large queries at the desktop. Central servers will operate
a data pump that supports sweeping searches that touch most of the data. The
anticipated queries require special operators related to angular distances and
complex similarity tests of object properties, like shapes, colors, velocity
vectors, or temporal behaviors. These issues pose interesting data management
challenges.
"
3344,Microsoft TerraServer: A Spatial Data Warehouse,"  The TerraServer stores aerial, satellite, and topographic images of the earth
in a SQL database available via the Internet. It is the world's largest online
atlas, combining five terabytes of image data from the United States Geological
Survey (USGS) and SPIN-2. This report describes the system-redesign based on
our experience over the last year. It also reports usage and operations results
over the last year -- over 2 billion web hits and over 20 Terabytes of imagry
served over the Internet. Internet browsers provide intuitive spatial and text
interfaces to the data. Users need no special hardware, software, or knowledge
to locate and browse imagery. This paper describes how terabytes of ""Internet
unfriendly"" geo-spatial images were scrubbed and edited into hundreds of
millions of ""Internet friendly"" image tiles and loaded into a SQL data
warehouse. Microsoft TerraServer demonstrates that general-purpose relational
database technology can manage large scale image repositories, and shows that
web browsers can be a good geospatial image presentation system.
"
3345,"Raising Reliability of Web Search Tool Research through Replication and
  Chaos Theory","  Because the World Wide Web is a dynamic collection of information, the Web
search tools (or ""search engines"") that index the Web are dynamic. Traditional
information retrieval evaluation techniques may not provide reliable results
when applied to the Web search tools. This study is the result of ten
replications of the classic 1996 Ding and Marchionini Web search tool research.
It explores the effects that replication can have on transforming unreliable
results from one iteration into replicable and therefore reliable results after
multiple iterations.
"
3346,Safe Deals Between Strangers,"  E-business, information serving, and ubiquitous computing will create heavy
request traffic from strangers or even incognitos. Such requests must be
managed automatically. Two ways of doing this are well known: giving every
incognito consumer the same treatment, and rendering service in return for
money. However, different behavior will be often wanted, e.g., for a university
library with different access policies for undergraduates, graduate students,
faculty, alumni, citizens of the same state, and everyone else.
  For a data or process server contacted by client machines on behalf of users
not previously known, we show how to provide reliable automatic access
administration conforming to service agreements. Implementations scale well
from very small collections of consumers and producers to immense client/server
networks. Servers can deliver information, effect state changes, and control
external equipment.
  Consumer privacy is easily addressed by the same protocol. We support
consumer privacy, but allow servers to deny their resources to incognitos. A
protocol variant even protects against statistical attacks by consortia of
service organizations.
  One e-commerce application would put the consumer's tokens on a smart card
whose readers are in vending kiosks. In e-business we can simplify supply chain
administration. Our method can also be used in sensitive networks without
introducing new security loopholes.
"
3347,"Representing Scholarly Claims in Internet Digital Libraries: A Knowledge
  Modelling Approach","  This paper is concerned with tracking and interpreting scholarly documents in
distributed research communities. We argue that current approaches to document
description, and current technological infrastructures particularly over the
World Wide Web, provide poor support for these tasks. We describe the design of
a digital library server which will enable authors to submit a summary of the
contributions they claim their documents makes, and its relations to the
literature. We describe a knowledge-based Web environment to support the
emergence of such a community-constructed semantic hypertext, and the services
it could provide to assist the interpretation of an idea or document in the
context of its literature. The discussion considers in detail how the approach
addresses usability issues associated with knowledge structuring environments.
"
3348,A Geometric Model for Information Retrieval Systems,"  This decade has seen a great deal of progress in the development of
information retrieval systems. Unfortunately, we still lack a systematic
understanding of the behavior of the systems and their relationship with
documents. In this paper we present a completely new approach towards the
understanding of the information retrieval systems. Recently, it has been
observed that retrieval systems in TREC 6 show some remarkable patterns in
retrieving relevant documents. Based on the TREC 6 observations, we introduce a
geometric linear model of information retrieval systems. We then apply the
model to predict the number of relevant documents by the retrieval systems. The
model is also scalable to a much larger data set. Although the model is
developed based on the TREC 6 routing test data, I believe it can be readily
applicable to other information retrieval systems. In Appendix, we explained a
simple and efficient way of making a better system from the existing systems.
"
3349,"Lattices for Dynamic, Hierarchic & Overlapping Categorization: the Case
  of Epistemic Communities","  We present a method for hierarchic categorization and taxonomy evolution
description. We focus on the structure of epistemic communities (ECs), or
groups of agents sharing common knowledge concerns. Introducing a formal
framework based on Galois lattices, we categorize ECs in an automated and
hierarchically structured way and propose criteria for selecting the most
relevant epistemic communities - for instance, ECs gathering a certain
proportion of agents and thus prototypical of major fields. This process
produces a manageable, insightful taxonomy of the community. Then, the
longitudinal study of these static pictures makes possible an historical
description. In particular, we capture stylized facts such as field progress,
decline, specialization, interaction (merging or splitting), and paradigm
emergence. The detection of such patterns in social networks could fruitfully
be applied to other contexts.
"
3350,On the genre-fication of Music: a percolation approach (long version),"  In this paper, we analyze web-downloaded data on people sharing their music
library. By attributing to each music group usual music genres (Rock, Pop...),
and analysing correlations between music groups of different genres with
percolation-idea based methods, we probe the reality of these subdivisions and
construct a music genre cartography, with a tree representation. We also show
the diversity of music genres with Shannon entropy arguments, and discuss an
alternative objective way to classify music, that is based on the complex
structure of the groups audience. Finally, a link is drawn with the theory of
hidden variables in complex networks.
"
3351,Ranking Scientific Publications Using a Simple Model of Network Traffic,"  To account for strong aging characteristics of citation networks, we modify
Google's PageRank algorithm by initially distributing random surfers
exponentially with age, in favor of more recent publications. The output of
this algorithm, which we call CiteRank, is interpreted as approximate traffic
to individual publications in a simple model of how researchers find new
information. We develop an analytical understanding of traffic flow in terms of
an RPA-like model and optimize parameters of our algorithm to achieve the best
performance. The results are compared for two rather different citation
networks: all American Physical Society publications and the set of high-energy
physics theory (hep-th) preprints. Despite major differences between these two
networks, we find that their optimal parameters for the CiteRank algorithm are
remarkably similar.
"
3352,The Rise and Rise of Citation Analysis,"  With the vast majority of scientific papers now available online, this paper
describes how the Web is allowing physicists and information providers to
measure more accurately the impact of these papers and their authors. Provides
a historical background of citation analysis, impact factor, new citation data
sources (e.g., Google Scholar, Scopus, NASA's Astrophysics Data System Abstract
Service, MathSciNet, ScienceDirect, SciFinder Scholar, Scitation/SPIN, and
SPIRES-HEP), as well as h-index, g-index, and a-index.
"
3353,Numerical simulations of mixed states quantum computation,"  We describe quantum-octave package of functions useful for simulations of
quantum algorithms and protocols. Presented package allows to perform
simulations with mixed states. We present numerical implementation of important
quantum mechanical operations - partial trace and partial transpose. Those
operations are used as building blocks of algorithms for analysis of
entanglement and quantum error correction codes. Simulation of Shor's algorithm
is presented as an example of package capabilities.
"
