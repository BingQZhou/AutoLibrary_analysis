,title,abstract
0,The on-line shortest path problem under partial monitoring,"  The on-line shortest path problem is considered under various models of
partial monitoring. Given a weighted directed acyclic graph whose edge weights
can change in an arbitrary (adversarial) way, a decision maker has to choose in
each round of a game a path between two distinguished vertices such that the
loss of the chosen path (defined as the sum of the weights of its composing
edges) be as small as possible. In a setting generalizing the multi-armed
bandit problem, after choosing a path, the decision maker learns only the
weights of those edges that belong to the chosen path. For this problem, an
algorithm is given whose average cumulative loss in n rounds exceeds that of
the best path, matched off-line to the entire sequence of the edge weights, by
a quantity that is proportional to 1/\sqrt{n} and depends only polynomially on
the number of edges of the graph. The algorithm can be implemented with linear
complexity in the number of rounds n and in the number of edges. An extension
to the so-called label efficient setting is also given, in which the decision
maker is informed about the weights of the edges corresponding to the chosen
path at a total of m << n time instances. Another extension is shown where the
decision maker competes against a time-varying path, a generalization of the
problem of tracking the best expert. A version of the multi-armed bandit
setting for shortest path is also discussed where the decision maker learns
only the total weight of the chosen path but not the weights of the individual
edges on the path. Applications to routing in packet switched networks along
with simulation results are also presented.
"
1,The Invar Tensor Package,"  The Invar package is introduced, a fast manipulator of generic scalar
polynomial expressions formed from the Riemann tensor of a four-dimensional
metric-compatible connection. The package can maximally simplify any polynomial
containing tensor products of up to seven Riemann tensors within seconds. It
has been implemented both in Mathematica and Maple algebraic systems.
"
2,"Parallel computation of the rank of large sparse matrices from algebraic
  K-theory","  This paper deals with the computation of the rank and of some integer Smith
forms of a series of sparse matrices arising in algebraic K-theory. The number
of non zero entries in the considered matrices ranges from 8 to 37 millions.
The largest rank computation took more than 35 days on 50 processors. We report
on the actual algorithms we used to build the matrices, their link to the
motivic cohomology and the linear algebra and parallelizations required to
perform such huge computations. In particular, these results are part of the
first computation of the cohomology of the linear group GL_7(Z).
"
3,"Towards an exact adaptive algorithm for the determinant of a rational
  matrix","  In this paper we propose several strategies for the exact computation of the
determinant of a rational matrix. First, we use the Chinese Remaindering
Theorem and the rational reconstruction to recover the rational determinant
from its modular images. Then we show a preconditioning for the determinant
which allows us to skip the rational reconstruction process and reconstruct an
integer result. We compare those approaches with matrix preconditioning which
allow us to treat integer instead of rational matrices. This allows us to
introduce integer determinant algorithms to the rational determinant problem.
In particular, we discuss the applicability of the adaptive determinant
algorithm of [9] and compare it with the integer Chinese Remaindering scheme.
We present an analysis of the complexity of the strategies and evaluate their
experimental performance on numerous examples. This experience allows us to
develop an adaptive strategy which would choose the best solution at the run
time, depending on matrix properties. All strategies have been implemented in
LinBox linear algebra library.
"
4,Tropical Implicitization and Mixed Fiber Polytopes,"  The software TrIm offers implementations of tropical implicitization and
tropical elimination, as developed by Tevelev and the authors. Given a
polynomial map with generic coefficients, TrIm computes the tropical variety of
the image. When the image is a hypersurface, the output is the Newton polytope
of the defining polynomial. TrIm can thus be used to compute mixed fiber
polytopes, including secondary polytopes.
"
5,A Proof of a Recursion for Bessel Moments,"  We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall
2007) on the existence and form of linear recursions for moments of powers of
the Bessel function $K_0$.
"
6,"Obstructions to Genericity in Study of Parametric Problems in Control
  Theory","  We investigate systems of equations, involving parameters from the point of
view of both control theory and computer algebra. The equations might involve
linear operators such as partial (q-)differentiation, (q-)shift, (q-)difference
as well as more complicated ones, which act trivially on the parameters. Such a
system can be identified algebraically with a certain left module over a
non-commutative algebra, where the operators commute with the parameters. We
develop, implement and use in practice the algorithm for revealing all the
expressions in parameters, for which e.g. homological properties of a system
differ from the generic properties. We use Groebner bases and Groebner basics
in rings of solvable type as main tools. In particular, we demonstrate an
optimized algorithm for computing the left inverse of a matrix over a ring of
solvable type. We illustrate the article with interesting examples. In
particular, we provide a complete solution to the ""two pendula, mounted on a
cart"" problem from the classical book of Polderman and Willems, including the
case, where the friction at the joints is essential . To the best of our
knowledge, the latter example has not been solved before in a complete way.
"
7,Moderate Growth Time Series for Dynamic Combinatorics Modelisation,"  Here, we present a family of time series with a simple growth constraint.
This family can be the basis of a model to apply to emerging computation in
business and micro-economy where global functions can be expressed from local
rules. We explicit a double statistics on these series which allows to
establish a one-to-one correspondence between three other ballot-like
strunctures.
"
8,"Bounding the Betti numbers and computing the Euler-Poincar\'e
  characteristic of semi-algebraic sets defined by partly quadratic systems of
  polynomials","  Let $\R$ be a real closed field, $ {\mathcal Q} \subset
\R[Y_1,...,Y_\ell,X_1,...,X_k], $ with $ \deg_{Y}(Q) \leq 2, \deg_{X}(Q) \leq
d, Q \in {\mathcal Q}, #({\mathcal Q})=m,$ and $ {\mathcal P} \subset
\R[X_1,...,X_k] $ with $\deg_{X}(P) \leq d, P \in {\mathcal P}, #({\mathcal
P})=s$, and $S \subset \R^{\ell+k}$ a semi-algebraic set defined by a Boolean
formula without negations, with atoms $P=0, P \geq 0, P \leq 0, P \in {\mathcal
P} \cup {\mathcal Q}$. We prove that the sum of the Betti numbers of $S$ is
bounded by \[ \ell^2 (O(s+\ell+m)\ell d)^{k+2m}. \] This is a common
generalization of previous results on bounding the Betti numbers of closed
semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.
  We also describe an algorithm for computing the Euler-Poincar\'e
characteristic of such sets, generalizing similar algorithms known before. The
complexity of the algorithm is bounded by $(\ell s m d)^{O(m(m+k))}$.
"
9,"Implicitization of Bihomogeneous Parametrizations of Algebraic Surfaces
  via Linear Syzygies","  We show that the implicit equation of a surface in 3-dimensional projective
space parametrized by bi-homogeneous polynomials of bi-degree (d,d), for a
given positive integer d, can be represented and computed from the linear
syzygies of its parametrization if the base points are isolated and form
locally a complete intersection.
"
10,Improved Linear Parallel Interference Cancellers,"  In this paper, taking the view that a linear parallel interference canceller
(LPIC) can be seen as a linear matrix filter, we propose new linear matrix
filters that can result in improved bit error performance compared to other
LPICs in the literature. The motivation for the proposed filters arises from
the possibility of avoiding the generation of certain interference and noise
terms in a given stage that would have been present in a conventional LPIC
(CLPIC). In the proposed filters, we achieve such avoidance of the generation
of interference and noise terms in a given stage by simply making the diagonal
elements of a certain matrix in that stage equal to zero. Hence, the proposed
filters do not require additional complexity compared to the CLPIC, and they
can allow achieving a certain error performance using fewer LPIC stages. We
also extend the proposed matrix filter solutions to a multicarrier DS-CDMA
system, where we consider two types of receivers. In one receiver (referred to
as Type-I receiver), LPIC is performed on each subcarrier first, followed by
multicarrier combining (MCC). In the other receiver (called Type-II receiver),
MCC is performed first, followed by LPIC. We show that in both Type-I and
Type-II receivers, the proposed matrix filters outperform other matrix filters.
Also, Type-II receiver performs better than Type-I receiver because of enhanced
accuracy of the interference estimates achieved due to frequency diversity
offered by MCC.
"
11,"An Extension to an Algebraic Method for Linear Time-Invariant System and
  Network Theory: The full AC-Calculus","  Being inspired by phasor analysis in linear circuit theory, and its algebraic
counterpart - the AC-(operational)-calculus for sinusoids developed by W.
Marten and W. Mathis - we define a complex structure on several spaces of
real-valued elementary functions. This is used to algebraize inhomogeneous
linear ordinary differential equations with inhomogenities stemming from these
spaces. Thus we deduce an effective method to calculate particular solutions of
these ODEs in a purely algebraic way.
"
12,Q-adic Transform revisited,"  We present an algorithm to perform a simultaneous modular reduction of
several residues. This algorithm is applied fast modular polynomial
multiplication. The idea is to convert the $X$-adic representation of modular
polynomials, with $X$ an indeterminate, to a $q$-adic representation where $q$
is an integer larger than the field characteristic. With some control on the
different involved sizes it is then possible to perform some of the $q$-adic
arithmetic directly with machine integers or floating points. Depending also on
the number of performed numerical operations one can then convert back to the
$q$-adic or $X$-adic representation and eventually mod out high residues. In
this note we present a new version of both conversions: more tabulations and a
way to reduce the number of divisions involved in the process are presented.
The polynomial multiplication is then applied to arithmetic in small finite
field extensions.
"
13,"Differential invariants of a Lie group action: syzygies on a generating
  set","  Given a group action, known by its infinitesimal generators, we exhibit a
complete set of syzygies on a generating set of differential invariants. For
that we elaborate on the reinterpretation of Cartan's moving frame by Fels and
Olver (1999). This provides constructive tools for exploring algebras of
differential invariants.
"
14,A Numerical Algorithm for Zero Counting. I: Complexity and Accuracy,"  We describe an algorithm to count the number of distinct real zeros of a
polynomial (square) system f. The algorithm performs O(n D kappa(f)) iterations
where n is the number of polynomials (as well as the dimension of the ambient
space), D is a bound on the polynomials' degree, and kappa(f) is a condition
number for the system. Each iteration uses an exponential number of operations.
The algorithm uses finite-precision arithmetic and a polynomial bound for the
precision required to ensure the returned output is correct is exhibited. This
bound is a major feature of our algorithm since it is in contrast with the
exponential precision required by the existing (symbolic) algorithms for
counting real zeros. The algorithm parallelizes well in the sense that each
iteration can be computed in parallel polynomial time with an exponential
number of processors.
"
15,SWI-Prolog and the Web,"  Where Prolog is commonly seen as a component in a Web application that is
either embedded or communicates using a proprietary protocol, we propose an
architecture where Prolog communicates to other components in a Web application
using the standard HTTP protocol. By avoiding embedding in external Web servers
development and deployment become much easier. To support this architecture, in
addition to the transfer protocol, we must also support parsing, representing
and generating the key Web document types such as HTML, XML and RDF.
  This paper motivates the design decisions in the libraries and extensions to
Prolog for handling Web documents and protocols. The design has been guided by
the requirement to handle large documents efficiently. The described libraries
support a wide range of Web applications ranging from HTML and XML documents to
Semantic Web RDF processing.
  To appear in Theory and Practice of Logic Programming (TPLP)
"
16,Some properties of finite meadows,"  The aim of this note is to describe the structure of finite meadows. We will
show that the class of finite meadows is the closure of the class of finite
fields under finite products. As a corollary, we obtain a unique representation
of minimal meadows in terms of prime fields.
"
17,"On the equations of the moving curve ideal of a rational algebraic plane
  curve","  Given a parametrization of a rational plane algebraic curve C, some explicit
adjoint pencils on C are described in terms of determinants. Moreover, some
generators of the Rees algebra associated to this parametrization are
presented. The main ingredient developed in this paper is a detailed study of
the elimination ideal of two homogeneous polynomials in two homogeneous
variables that form a regular sequence.
"
18,Faster polynomial multiplication via multipoint Kronecker substitution,"  We give several new algorithms for dense polynomial multiplication based on
the Kronecker substitution method. For moderately sized input polynomials, the
new algorithms improve on the performance of the standard Kronecker
substitution by a sizeable constant, both in theory and in empirical tests.
"
19,Introduction to the Galois Theory of Linear Differential Equations,"  This is an expanded version of the 10 lectures given as the 2006 London
Mathematical Society Invited Lecture Series at the Heriot-Watt University 31
July - 4 August 2006.
"
20,Computer algebra in systems biology,"  Systems biology focuses on the study of entire biological systems rather than
on their individual components. With the emergence of high-throughput data
generation technologies for molecular biology and the development of advanced
mathematical modeling techniques, this field promises to provide important new
insights. At the same time, with the availability of increasingly powerful
computers, computer algebra has developed into a useful tool for many
applications. This article illustrates the use of computer algebra in systems
biology by way of a well-known gene regulatory network, the Lac Operon in the
bacterium E. coli.
"
21,On sign conditions over real multivariate polynomials,"  We present a new probabilistic algorithm to find a finite set of points
intersecting the closure of each connected component of the realization of
every sign condition over a family of real polynomials defining regular
hypersurfaces that intersect transversally. This enables us to show a
probabilistic procedure to list all feasible sign conditions over the
polynomials. In addition, we extend these results to the case of closed sign
conditions over an arbitrary family of real multivariate polynomials. The
complexity bounds for these procedures improve the known ones.
"
22,"Factorization in categories of systems of linear partial differential
  equations","  We start with elementary algebraic theory of factorization of linear ordinary
differential equations developed in the period 1880-1930. After exposing these
classical results we sketch more sophisticated algorithmic approaches developed
in the last 20 years.
  The main part of this paper is devoted to modern generalizations of the
notion of factorization to the case of systems of linear partial differential
equations and their relation with explicit solvability of nonlinear partial
differential equations based on some constructions from the theory of abelian
categories.
"
23,Fast Integer Multiplication using Modular Arithmetic,"  We give an $O(N\cdot \log N\cdot 2^{O(\log^*N)})$ algorithm for multiplying
two $N$-bit integers that improves the $O(N\cdot \log N\cdot \log\log N)$
algorithm by Sch\""{o}nhage-Strassen. Both these algorithms use modular
arithmetic. Recently, F\""{u}rer gave an $O(N\cdot \log N\cdot 2^{O(\log^*N)})$
algorithm which however uses arithmetic over complex numbers as opposed to
modular arithmetic. In this paper, we use multivariate polynomial
multiplication along with ideas from F\""{u}rer's algorithm to achieve this
improvement in the modular setting. Our algorithm can also be viewed as a
$p$-adic version of F\""{u}rer's algorithm. Thus, we show that the two seemingly
different approaches to integer multiplication, modular and complex arithmetic,
are similar.
"
24,"Analyzing the Topology Types arising in a Family of Algebraic Curves
  Depending On Two Parameters","  Given the implicit equation $F(x,y,t,s)$ of a family of algebraic plane
curves depending on the parameters $t,s$, we provide an algorithm for studying
the topology types arising in the family. For this purpose, the algorithm
computes a finite partition of the parameter space so that the topology type of
the family stays invariant over each element of the partition. The ideas
contained in the paper can be seen as a generalization of the ideas in
\cite{JGRS}, where the problem is solved for families of algebraic curves
depending on one parameter, to the two-parameters case.
"
25,"Hopf Algebras in General and in Combinatorial Physics: a practical
  introduction","  This tutorial is intended to give an accessible introduction to Hopf
algebras. The mathematical context is that of representation theory, and we
also illustrate the structures with examples taken from combinatorics and
quantum physics, showing that in this latter case the axioms of Hopf algebra
arise naturally. The text contains many exercises, some taken from physics,
aimed at expanding and exemplifying the concepts introduced.
"
26,Approximate substitutions and the normal ordering problem,"  In this paper, we show that the infinite generalised Stirling matrices
associated with boson strings with one annihilation operator are projective
limits of approximate substitutions, the latter being characterised by a finite
set of algebraic equations.
"
27,The Invar tensor package: Differential invariants of Riemann,"  The long standing problem of the relations among the scalar invariants of the
Riemann tensor is computationally solved for all 6x10^23 objects with up to 12
derivatives of the metric. This covers cases ranging from products of up to 6
undifferentiated Riemann tensors to cases with up to 10 covariant derivatives
of a single Riemann. We extend our computer algebra system Invar to produce
within seconds a canonical form for any of those objects in terms of a basis.
The process is as follows: (1) an invariant is converted in real time into a
canonical form with respect to the permutation symmetries of the Riemann
tensor; (2) Invar reads a database of more than 6x10^5 relations and applies
those coming from the cyclic symmetry of the Riemann tensor; (3) then applies
the relations coming from the Bianchi identity, (4) the relations coming from
commutations of covariant derivatives, (5) the dimensionally-dependent
identities for dimension 4, and finally (6) simplifies invariants that can be
expressed as product of dual invariants. Invar runs on top of the tensor
computer algebra systems xTensor (for Mathematica) and Canon (for Maple).
"
28,Kolmogorov Complexity Theory over the Reals,"  Kolmogorov Complexity constitutes an integral part of computability theory,
information theory, and computational complexity theory -- in the discrete
setting of bits and Turing machines. Over real numbers, on the other hand, the
BSS-machine (aka real-RAM) has been established as a major model of
computation. This real realm has turned out to exhibit natural counterparts to
many notions and results in classical complexity and recursion theory; although
usually with considerably different proofs. The present work investigates
similarities and differences between discrete and real Kolmogorov Complexity as
introduced by Montana and Pardo (1998).
"
29,Reconstruction of eye movements during blinks,"  In eye movement research in reading, the amount of data plays a crucial role
for the validation of results. A methodological problem for the analysis of the
eye movement in reading are blinks, when readers close their eyes. Blinking
rate increases with increasing reading time, resulting in high data losses,
especially for older adults or reading impaired subjects. We present a method,
based on the symbolic sequence dynamics of the eye movements, that reconstructs
the horizontal position of the eyes while the reader blinks. The method makes
use of an observed fact that the movements of the eyes before closing or after
opening contain information about the eyes movements during blinks. Test
results indicate that our reconstruction method is superior to methods that use
simpler interpolation approaches. In addition, analyses of the reconstructed
data show no significant deviation from the usual behavior observed in readers.
"
30,Using Alloy to model-check visual design notations,"  This paper explores the process of validation for the abstract syntax of a
graphical notation. We define an unified specification for five of the UML
diagrams used by the Discovery Method and, in this document, we illustrate how
diagrams can be represented in Alloy and checked against our specification in
order to know if these are valid under the Discovery notation.
"
31,xPerm: fast index canonicalization for tensor computer algebra,"  We present a very fast implementation of the Butler-Portugal algorithm for
index canonicalization with respect to permutation symmetries. It is called
xPerm, and has been written as a combination of a Mathematica package and a C
subroutine. The latter performs the most demanding parts of the computations
and can be linked from any other program or computer algebra system. We
demonstrate with tests and timings the effectively polynomial performance of
the Butler-Portugal algorithm with respect to the number of indices, though we
also show a case in which it is exponential. Our implementation handles generic
tensorial expressions with several dozen indices in hundredths of a second, or
one hundred indices in a few seconds, clearly outperforming all other current
canonicalizers. The code has been already under intensive testing for several
years and has been essential in recent investigations in large-scale tensor
computer algebra.
"
32,On the Computation of the Topology of a Non-Reduced Implicit Space Curve,"  An algorithm is presented for the computation of the topology of a
non-reduced space curve defined as the intersection of two implicit algebraic
surfaces. It computes a Piecewise Linear Structure (PLS) isotopic to the
original space curve. The algorithm is designed to provide the exact result for
all inputs. It's a symbolic-numeric algorithm based on subresultant
computation. Simple algebraic criteria are given to certify the output of the
algorithm. The algorithm uses only one projection of the non-reduced space
curve augmented with adjacency information around some ""particular points"" of
the space curve. The algorithm is implemented with the Mathemagix Computer
Algebra System (CAS) using the SYNAPS library as a backend.
"
33,Compressed Modular Matrix Multiplication,"  We propose to store several integers modulo a small prime into a single
machine word. Modular addition is performed by addition and possibly
subtraction of a word containing several times the modulo. Modular
Multiplication is not directly accessible but modular dot product can be
performed by an integer multiplication by the reverse integer. Modular
multiplication by a word containing a single residue is a also possible.
Therefore matrix multiplication can be performed on such a compressed storage.
We here give bounds on the sizes of primes and matrices for which such a
compression is possible. We also explicit the details of the required
compressed arithmetic routines.
"
34,"Two Algorithms for Solving A General Backward Pentadiagonal Linear
  Systems","  In this paper we present an efficient computational and symbolic algorithms
for solving a backward pentadiagonal linear systems. The implementation of the
algorithms using Computer Algebra Systems (CAS) such as MAPLE, MACSYMA,
MATHEMATICA, and MATLAB are straightforward. An examples are given in order to
illustrate the algorithms. The symbolic algorithm is competitive the other
methods for solving a backward pentadiagonal linear systems.
"
35,"Towards a Symbolic-Numeric Method to Compute Puiseux Series: The Modular
  Part","  We have designed a new symbolic-numeric strategy to compute efficiently and
accurately floating point Puiseux series defined by a bivariate polynomial over
an algebraic number field. In essence, computations modulo a well chosen prime
$p$ are used to obtain the exact information required to guide floating point
computations. In this paper, we detail the symbolic part of our algorithm:
First of all, we study modular reduction of Puiseux series and give a good
reduction criterion to ensure that the information required by the numerical
part is preserved. To establish our results, we introduce a simple modification
of classical Newton polygons, that we call ""generic Newton polygons"", which
happen to be very convenient. Then, we estimate the arithmetic complexity of
computing Puiseux series over finite fields and improve known bounds. Finally,
we give bit-complexity bounds for deterministic and randomized versions of the
symbolic part. The details of the numerical part will be described in a
forthcoming paper.
"
36,Decomposing replicable functions,"  We describe an algorithm to decompose rational functions from which we
determine the poset of groups fixing these functions.
"
37,Twenty-Five Moves Suffice for Rubik's Cube,"  How many moves does it take to solve Rubik's Cube? Positions are known that
require 20 moves, and it has already been shown that there are no positions
that require 27 or more moves; this is a surprisingly large gap. This paper
describes a program that is able to find solutions of length 20 or less at a
rate of more than 16 million positions a second. We use this program, along
with some new ideas and incremental improvements in other techniques, to show
that there is no position that requires 26 moves.
"
38,Preferred extensions as stable models,"  Given an argumentation framework AF, we introduce a mapping function that
constructs a disjunctive logic program P, such that the preferred extensions of
AF correspond to the stable models of P, after intersecting each stable model
with the relevant atoms. The given mapping function is of polynomial size
w.r.t. AF. In particular, we identify that there is a direct relationship
between the minimal models of a propositional formula and the preferred
extensions of an argumentation framework by working on representing the
defeated arguments. Then we show how to infer the preferred extensions of an
argumentation framework by using UNSAT algorithms and disjunctive stable model
solvers. The relevance of this result is that we define a direct relationship
between one of the most satisfactory argumentation semantics and one of the
most successful approach of non-monotonic reasoning i.e., logic programming
with the stable model semantics.
"
39,On Ritt's decomposition Theorem in the case of finite fields,"  A classical theorem by Ritt states that all the complete decomposition chains
of a univariate polynomial satisfying a certain tameness condition have the
same length. In this paper we present our conclusions about the generalization
of these theorem in the case of finite coefficient fields when the tameness
condition is dropped.
"
40,Differentiation of Kaltofen's division-free determinant algorithm,"  Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix
determinants. The algorithm is based on a baby steps/giant steps construction
of Krylov subspaces, and computes the determinant as the constant term of a
characteristic polynomial. For matrices over an abstract field and by the
results of Baur and Strassen 1983, the determinant algorithm, actually a
straight-line program, leads to an algorithm with the same complexity for
computing the adjoint of a matrix [Kaltofen 1992]. However, the latter is
obtained by the reverse mode of automatic differentiation and somehow is not
``explicit''. We study this adjoint algorithm, show how it can be implemented
(without resorting to an automatic transformation), and demonstrate its use on
polynomial matrices.
"
41,On decomposition of tame polynomials and rational functions,"  In this paper we present algorithmic considerations and theoretical results
about the relation between the orders of certain groups associated to the
components of a polynomial and the order of the group that corresponds to the
polynomial, proving it for arbitrary tame polynomials, and considering the case
of rational functions.
"
42,Computation of unirational fields,"  One of the main contributions which Volker Weispfenning made to mathematics
is related to Groebner bases theory. In this paper we present an algorithm for
computing all algebraic intermediate subfields in a separably generated
unirational field extension (which in particular includes the zero
characteristic case). One of the main tools is Groebner bases theory. Our
algorithm also requires computing primitive elements and factoring over
algebraic extensions. Moreover, the method can be extended to finitely
generated K-algebras.
"
43,"Building counterexamples to generalizations for rational functions of
  Ritt's decomposition theorem","  The classical Ritt's Theorems state several properties of univariate
polynomial decomposition. In this paper we present new counterexamples to
Ritt's first theorem, which states the equality of length of decomposition
chains of a polynomial, in the case of rational functions. Namely, we provide
an explicit example of a rational function with coefficients in Q and two
decompositions of different length.
  Another aspect is the use of some techniques that could allow for other
counterexamples, namely, relating groups and decompositions and using the fact
that the alternating group A_4 has two subgroup chains of different lengths;
and we provide more information about the generalizations of another property
of polynomial decomposition: the stability of the base field. We also present
an algorithm for computing the fixing group of a rational function providing
the complexity over Q.
"
44,Computation of unirational fields (extended abstract),"  In this paper we present an algorithm for computing all algebraic
intermediate subfields in a separably generated unirational field extension
(which in particular includes the zero characteristic case). One of the main
tools is Groebner bases theory. Our algorithm also requires computing computing
primitive elements and factoring over algebraic extensions. Moreover, the
method can be extended to finitely generated K-algebras.
"
45,Schemes for Deterministic Polynomial Factoring,"  In this work we relate the deterministic complexity of factoring polynomials
(over finite fields) to certain combinatorial objects we call m-schemes. We
extend the known conditional deterministic subexponential time polynomial
factoring algorithm for finite fields to get an underlying m-scheme. We
demonstrate how the properties of m-schemes relate to improvements in the
deterministic complexity of factoring polynomials over finite fields assuming
the generalized Riemann Hypothesis (GRH). In particular, we give the first
deterministic polynomial time algorithm (assuming GRH) to find a nontrivial
factor of a polynomial of prime degree n where (n-1) is a smooth number.
"
46,"Products of Ordinary Differential Operators by Evaluation and
  Interpolation","  It is known that multiplication of linear differential operators over ground
fields of characteristic zero can be reduced to a constant number of matrix
products. We give a new algorithm by evaluation and interpolation which is
faster than the previously-known one by a constant factor, and prove that in
characteristic zero, multiplication of differential operators and of matrices
are computationally equivalent problems. In positive characteristic, we show
that differential operators can be multiplied in nearly optimal time.
Theoretical results are validated by intensive experiments.
"
47,Power Series Composition and Change of Basis,"  Efficient algorithms are known for many operations on truncated power series
(multiplication, powering, exponential, ...). Composition is a more complex
task. We isolate a large class of power series for which composition can be
performed efficiently. We deduce fast algorithms for converting polynomials
between various bases, including Euler, Bernoulli, Fibonacci, and the
orthogonal Laguerre, Hermite, Jacobi, Krawtchouk, Meixner and
Meixner-Pollaczek.
"
48,Fast Conversion Algorithms for Orthogonal Polynomials,"  We discuss efficient conversion algorithms for orthogonal polynomials. We
describe a known conversion algorithm from an arbitrary orthogonal basis to the
monomial basis, and deduce a new algorithm of the same complexity for the
converse operation.
"
49,"""E pluribus unum"" or How to Derive Single-equation Descriptions for
  Output-quantities in Nonlinear Circuits using Differential Algebra","  In this paper we describe by a number of examples how to deduce one single
characterizing higher order differential equation for output quantities of an
analog circuit.
  In the linear case, we apply basic ""symbolic"" methods from linear algebra to
the system of differential equations which is used to model the analog circuit.
For nonlinear circuits and their corresponding nonlinear differential
equations, we show how to employ computer algebra tools implemented in Maple,
which are based on differential algebra.
"
50,Experiments in Model-Checking Optimistic Replication Algorithms,"  This paper describes a series of model-checking experiments to verify
optimistic replication algorithms based on Operational Transformation (OT)
approach used for supporting collaborative edition. We formally define, using
tool UPPAAL, the behavior and the main consistency requirement (i.e.
convergence property) of the collaborative editing systems, as well as the
abstract behavior of the environment where these systems are supposed to
operate. Due to data replication and the unpredictable nature of user
interactions, such systems have infinitely many states. So, we show how to
exploit some features of the UPPAAL specification language to attenuate the
severe state explosion problem. Two models are proposed. The first one, called
concrete model, is very close to the system implementation but runs up against
a severe explosion of states. The second model, called symbolic model, aims to
overcome the limitation of the concrete model by delaying the effective
selection and execution of editing operations until the construction of
symbolic execution traces of all sites is completed. Experimental results have
shown that the symbolic model allows a significant gain in both space and time.
Using the symbolic model, we have been able to show that if the number of sites
exceeds 2 then the convergence property is not satisfied for all OT algorithms
considered here. A counterexample is provided for every algorithm.
"
51,Symbolic computations in differential geometry,"  We introduce the C++ library Wedge, based on GiNaC, for symbolic computations
in differential geometry. We show how Wedge makes it possible to use the
language C++ to perform such computations, and illustrate some advantages of
this approach with explicit examples. In particular, we describe a short
program to determine whether a given linear exterior differential system is
involutive.
"
52,Hochschild Homology and Cohomology of Klein Surfaces,"  Within the framework of deformation quantization, a first step towards the
study of star-products is the calculation of Hochschild cohomology. The aim of
this article is precisely to determine the Hochschild homology and cohomology
in two cases of algebraic varieties. On the one hand, we consider singular
curves of the plane; here we recover, in a different way, a result proved by
Fronsdal and make it more precise. On the other hand, we are interested in
Klein surfaces. The use of a complex suggested by Kontsevich and the help of
Groebner bases allow us to solve the problem.
"
53,"A Simple Dynamic Mind-map Framework To Discover Associative
  Relationships in Transactional Data Streams","  In this paper, we informally introduce dynamic mind-maps that represent a new
approach on the basis of a dynamic construction of connectionist structures
during the processing of a data stream. This allows the representation and
processing of recursively defined structures and avoids the problem of a more
traditional, fixed-size architecture with the processing of input structures of
unknown size. For a data stream analysis with association discovery, the
incremental analysis of data leads to results on demand. Here, we describe a
framework that uses symbolic cells to calculate associations based on
transactional data streams as it exists in e.g. bibliographic databases. We
follow a natural paradigm of applying simple operations on cells yielding on a
mind-map structure that adapts over time.
"
54,Grammatical Evolution with Restarts for Fast Fractal Generation,"  In a previous work, the authors proposed a Grammatical Evolution algorithm to
automatically generate Lindenmayer Systems which represent fractal curves with
a pre-determined fractal dimension. This paper gives strong statistical
evidence that the probability distributions of the execution time of that
algorithm exhibits a heavy tail with an hyperbolic probability decay for long
executions, which explains the erratic performance of different executions of
the algorithm. Three different restart strategies have been incorporated in the
algorithm to mitigate the problems associated to heavy tail distributions: the
first assumes full knowledge of the execution time probability distribution,
the second and third assume no knowledge. These strategies exploit the fact
that the probability of finding a solution in short executions is
non-negligible and yield a severe reduction, both in the expected execution
time (up to one order of magnitude) and in its variance, which is reduced from
an infinite to a finite value.
"
55,"Aplicacion de la descomposicion racional univariada a monstrous
  moonshine (in Spanish)","  This paper shows how to use Computational Algebra techniques, namely the
decomposition of rational functions in one variable, to explore a certain set
of modular functions, called replicable functions, that arise in Monstrous
Moonshine. In particular, we have computed all the rational relations with
coefficients in Z between pairs of replicable functions.
  -----
  En este articulo mostramos como usar tecnicas de Algebra Computacional,
concretamente la descomposcion de funciones racionales univariadas, para
estudiar un cierto conjunto de funciones modulares, llamadas funciones
replicables, que aparecen en Monstrous Moonshine. En concreto, hemos calculado
todas las relaciones racionales con coeficientes en Z entre pares de funciones
replicables.
"
56,Computing the fixing group of a rational function,"  Let G=Aut_K (K(x)) be the Galois group of the transcendental degree one pure
field extension K(x)/K. In this paper we describe polynomial time algorithms
for computing the field Fix(H) fixed by a subgroup H < G and for computing the
fixing group G_f of a rational function f in K(x).
"
57,"Unirational fields of transcendence degree one and functional
  decomposition","  In this paper we present an algorithm to compute all unirational fields of
transcendence degree one containing a given finite set of multivariate rational
functions. In particular, we provide an algorithm to decompose a multivariate
rational function f of the form f=g(h), where g is a univariate rational
function and h a multivariate one.
"
58,"Determination of the basis of the space of all root functionals of a
  system of polynomial equations and of the basis of its ideal by the operation
  of the extension of bounded root functionals","  It is proposed the algorithm that find a basis of the ideal and a basis of
the space of all root functionals by using the extension operation for bounded
root functionals, when the number of polynomials is equal to the number of
variables, if it is known that the ideal of polynomials is 0-dimensional. The
asyptotic complexity of this algorithm is d^{O(n)} operations, where n is the
number of polynomials and the number of variables, d is the maximal degree of
polynomials. The extension operation has connection with the multivariate
Bezoutian construction.
"
59,"Checking the Quality of Clinical Guidelines using Automated Reasoning
  Tools","  Requirements about the quality of clinical guidelines can be represented by
schemata borrowed from the theory of abductive diagnosis, using temporal logic
to model the time-oriented aspects expressed in a guideline. Previously, we
have shown that these requirements can be verified using interactive theorem
proving techniques. In this paper, we investigate how this approach can be
mapped to the facilities of a resolution-based theorem prover, Otter, and a
complementary program that searches for finite models of first-order
statements, Mace. It is shown that the reasoning required for checking the
quality of a guideline can be mapped to such fully automated theorem-proving
facilities. The medical quality of an actual guideline concerning diabetes
mellitus 2 is investigated in this way.
"
60,Subresultants in Recursive Polynomial Remainder Sequence,"  We introduce concepts of ""recursive polynomial remainder sequence (PRS)"" and
""recursive subresultant,"" and investigate their properties. In calculating PRS,
if there exists the GCD (greatest common divisor) of initial polynomials, we
calculate ""recursively"" with new PRS for the GCD and its derivative, until a
constant is derived. We call such a PRS a recursive PRS. We define recursive
subresultants to be determinants representing the coefficients in recursive PRS
by coefficients of initial polynomials. Finally, we discuss usage of recursive
subresultants in approximate algebraic computation, which motivates the present
work.
"
61,Recursive Polynomial Remainder Sequence and the Nested Subresultants,"  We give two new expressions of subresultants, nested subresultant and reduced
nested subresultant, for the recursive polynomial remainder sequence (PRS)
which has been introduced by the author. The reduced nested subresultant
reduces the size of the subresultant matrix drastically compared with the
recursive subresultant proposed by the authors before, hence it is much more
useful for investigation of the recursive PRS. Finally, we discuss usage of the
reduced nested subresultant in approximate algebraic computation, which
motivates the present work.
"
62,Recursive Polynomial Remainder Sequence and its Subresultants,"  We introduce concepts of ""recursive polynomial remainder sequence (PRS)"" and
""recursive subresultant,"" along with investigation of their properties. A
recursive PRS is defined as, if there exists the GCD (greatest common divisor)
of initial polynomials, a sequence of PRSs calculated ""recursively"" for the GCD
and its derivative until a constant is derived, and recursive subresultants are
defined by determinants representing the coefficients in recursive PRS as
functions of coefficients of initial polynomials. We give three different
constructions of subresultant matrices for recursive subresultants; while the
first one is built-up just with previously defined matrices thus the size of
the matrix increases fast as the recursion deepens, the last one reduces the
size of the matrix drastically by the Gaussian elimination on the second one
which has a ""nested"" expression, i.e. a Sylvester matrix whose elements are
themselves determinants.
"
63,"Consistency and Completeness of Rewriting in the Calculus of
  Constructions","  Adding rewriting to a proof assistant based on the Curry-Howard isomorphism,
such as Coq, may greatly improve usability of the tool. Unfortunately adding an
arbitrary set of rewrite rules may render the underlying formal system
undecidable and inconsistent. While ways to ensure termination and confluence,
and hence decidability of type-checking, have already been studied to some
extent, logical consistency has got little attention so far. In this paper we
show that consistency is a consequence of canonicity, which in turn follows
from the assumption that all functions defined by rewrite rules are complete.
We provide a sound and terminating, but necessarily incomplete algorithm to
verify this property. The algorithm accepts all definitions that follow
dependent pattern matching schemes presented by Coquand and studied by McBride
in his PhD thesis. It also accepts many definitions by rewriting, containing
rules which depart from standard pattern matching.
"
64,Round Trip Time Prediction Using the Symbolic Function Network Approach,"  In this paper, we develop a novel approach to model the Internet round trip
time using a recently proposed symbolic type neural network model called
symbolic function network. The developed predictor is shown to have good
generalization performance and simple representation compared to the multilayer
perceptron based predictors.
"
65,The implicit equation of a canal surface,"  A canal surface is an envelope of a one parameter family of spheres. In this
paper we present an efficient algorithm for computing the implicit equation of
a canal surface generated by a rational family of spheres. By using Laguerre
and Lie geometries, we relate the equation of the canal surface to the equation
of a dual variety of a certain curve in 5-dimensional projective space. We
define the \mu-basis for arbitrary dimension and give a simple algorithm for
its computation. This is then applied to the dual variety, which allows us to
deduce the implicit equations of the the dual variety, the canal surface and
any offset to the canal surface.
"
66,Executable Set Theory and Arithmetic Encodings in Prolog,"  The paper is organized as a self-contained literate Prolog program that
implements elements of an executable finite set theory with focus on
combinatorial generation and arithmetic encodings. The complete Prolog code is
available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First,
ranking and unranking functions for some ""mathematically elegant"" data types in
the universe of Hereditarily Finite Sets with Urelements are provided,
resulting in arithmetic encodings for powersets, hypergraphs, ordinals and
choice functions. After implementing a digraph representation of Hereditarily
Finite Sets we define {\em decoration functions} that can recover well-founded
sets from encodings of their associated acyclic digraphs. We conclude with an
encoding of arbitrary digraphs and discuss a concept of duality induced by the
set membership relation. In the process, we uncover the surprising possibility
of internally sharing isomorphic objects, independently of their language level
types and meanings.
"
67,"Pairing Functions, Boolean Evaluation and Binary Decision Diagrams in
  Prolog","  A ""pairing function"" J associates a unique natural number z to any two
natural numbers x,y such that for two ""unpairing functions"" K and L, the
equalities K(J(x,y))=x, L(J(x,y))=y and J(K(z),L(z))=z hold. Using pairing
functions on natural number representations of truth tables, we derive an
encoding for Binary Decision Diagrams with the unique property that its boolean
evaluation faithfully mimics its structural conversion to a a natural number
through recursive application of a matching pairing function. We then use this
result to derive {\em ranking} and {\em unranking} functions for BDDs and
reduced BDDs. The paper is organized as a self-contained literate Prolog
program, available at http://logic.csci.unt.edu/tarau/research/2008/pBDD.zip
  Keywords: logic programming and computational mathematics, pairing/unpairing
functions, encodings of boolean functions, binary decision diagrams, natural
number representations of truth tables
"
68,"Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite
  Datatypes","  Using specializations of unfold and fold on a generic tree data type we
derive unranking and ranking functions providing natural number encodings for
various Hereditarily Finite datatypes.
  In this context, we interpret unranking operations as instances of a generic
anamorphism and ranking operations as instances of the corresponding
catamorphism.
  Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural
Numbers we define pairings and tuple encodings that provide building blocks for
a theory of Hereditarily Finite Functions.
  The more difficult problem of ranking and unranking Hereditarily Finite
Permutations is then tackled using Lehmer codes and factoradics.
  The self-contained source code of the paper, as generated from a literate
Haskell program, is available at
\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}.
  Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding,
hereditarily finite sets, hereditarily finite functions, permutations and
factoradics, computational mathematics, Haskell data representations
"
69,"A Novel Symbolic Type Neural Network Model- Application to River Flow
  Forecasting","  In this paper we introduce a new symbolic type neural tree network called
symbolic function network (SFN) that is based on using elementary functions to
model systems in a symbolic form. The proposed formulation permits feature
selection, functional selection, and flexible structure. We applied this model
on the River Flow forecasting problem. The results found to be superior in both
fitness and sparsity.
"
70,A Refined Difference Field Theory for Symbolic Summation,"  In this article we present a refined summation theory based on Karr's
difference field approach. The resulting algorithms find sum representations
with optimal nested depth. For instance, the algorithms have been applied
successively to evaluate Feynman integrals from Perturbative Quantum Field
Theory.
"
71,Parameterized Telescoping Proves Algebraic Independence of Sums,"  Usually creative telescoping is used to derive recurrences for sums. In this
article we show that the non-existence of a creative telescoping solution, and
more generally, of a parameterized telescoping solution, proves algebraic
independence of certain types of sums. Combining this fact with
summation-theory shows transcendence of whole classes of sums. Moreover, this
result throws new light on the question why, e.g., Zeilberger's algorithm fails
to find a recurrence with minimal order.
"
72,Tschirnhaus-Weierstrass curves,"  We define the concept of Tschirnhaus-Weierstrass curve, named after the
Weierstrass form of an elliptic curve and Tschirnhaus transformations. Every
pointed curve has a Tschirnhaus-Weierstrass form, and this representation is
unique up to a scaling of variables. This is useful for computing isomorphisms
between curves.
"
73,"Simultaneous Modular Reduction and Kronecker Substitution for Small
  Finite Fields","  We present algorithms to perform modular polynomial multiplication or modular
dot product efficiently in a single machine word. We pack polynomials into
integers and perform several modular operations with machine integer or
floating point arithmetic. The modular polynomials are converted into integers
using Kronecker substitution (evaluation at a sufficiently large integer). With
some control on the sizes and degrees, arithmetic operations on the polynomials
can be performed directly with machine integers or floating point numbers and
the number of conversions can be reduced. We also present efficient ways to
recover the modular values of the coefficients. This leads to practical gains
of quite large constant factors for polynomial multiplication, prime field
linear algebra and small extension field arithmetic.
"
74,Obtaining Exact Interpolation Multivariate Polynomial by Approximation,"  In some fields such as Mathematics Mechanization, automated reasoning and
Trustworthy Computing etc., exact results are needed. Symbolic computations are
used to obtain the exact results. Symbolic computations are of high complexity.
In order to improve the situation, exactly interpolating methods are often
proposed for the exact results and approximate interpolating methods for the
approximate ones. In this paper, we study how to obtain exact interpolation
polynomial with rational coefficients by approximate interpolating methods.
"
75,How to Integrate a Polynomial over a Simplex,"  This paper settles the computational complexity of the problem of integrating
a polynomial function f over a rational simplex. We prove that the problem is
NP-hard for arbitrary polynomials via a generalization of a theorem of Motzkin
and Straus. On the other hand, if the polynomial depends only on a fixed number
of variables, while its degree and the dimension of the simplex are allowed to
vary, we prove that integration can be done in polynomial time. As a
consequence, for polynomials of fixed total degree, there is a polynomial time
algorithm as well. We conclude the article with extensions to other polytopes,
discussion of other available methods and experimental results.
"
76,An Unified Definition of Data Mining,"  Since many years, theoretical concepts of Data Mining have been developed and
improved. Data Mining has become applied to many academic and industrial
situations, and recently, soundings of public opinion about privacy have been
carried out. However, a consistent and standardized definition is still
missing, and the initial explanation given by Frawley et al. has pragmatically
often changed over the years. Furthermore, alternative terms like Knowledge
Discovery have been conjured and forged, and a necessity of a Data Warehouse
has been endeavoured to persuade the users. In this work, we pick up current
definitions and introduce an unified definition that covers existing attempted
explanations. For this, we appeal to the natural original of chemical states of
aggregation.
"
77,A local construction of the Smith normal form of a matrix polynomial,"  We present an algorithm for computing a Smith form with multipliers of a
regular matrix polynomial over a field. This algorithm differs from previous
ones in that it computes a local Smith form for each irreducible factor in the
determinant separately and then combines them into a global Smith form, whereas
other algorithms apply a sequence of unimodular row and column operations to
the original matrix. The performance of the algorithm in exact arithmetic is
reported for several test cases.
"
78,Poisson Homology in Degree 0 for some Rings of Symplectic Invariants,"  Let $\go{g}$ be a finite-dimensional semi-simple Lie algebra, $\go{h}$ a
Cartan subalgebra of $\go{g}$, and $W$ its Weyl group. The group $W$ acts
diagonally on $V:=\go{h}\oplus\go{h}^*$, as well as on $\mathbb{C}[V]$. The
purpose of this article is to study the Poisson homology of the algebra of
invariants $\mathbb{C}[V]^W$ endowed with the standard symplectic bracket. To
begin with, we give general results about the Poisson homology space in degree
0, denoted by $HP_0(\mathbb{C}[V]^W)$, in the case where $\go{g}$ is of type
$B_n-C_n$ or $D_n$, results which support Alev's conjecture. Then we are
focusing the interest on the particular cases of ranks 2 and 3, by computing
the Poisson homology space in degree 0 in the cases where $\go{g}$ is of type
$B_2$ ($\go{so}_5$), $D_2$ ($\go{so}_4$), then $B_3$ ($\go{so}_7$), and
$D_3=A_3$ ($\go{so}_6\simeq\go{sl}_4$). In order to do this, we make use of a
functional equation introduced by Y. Berest, P. Etingof and V. Ginzburg. We
recover, by a different method, the result established by J. Alev and L.
Foissy, according to which the dimension of $HP_0(\mathbb{C}[V]^W)$ equals 2
for $B_2$. Then we calculate the dimension of this space and we show that it is
equal to 1 for $D_2$. We also calculate it for the rank 3 cases, we show that
it is equal to 3 for $B_3-C_3$ and 1 for $D_3=A_3$.
"
79,An Evidential Path Logic for Multi-Relational Networks,"  Multi-relational networks are used extensively to structure knowledge.
Perhaps the most popular instance, due to the widespread adoption of the
Semantic Web, is the Resource Description Framework (RDF). One of the primary
purposes of a knowledge network is to reason; that is, to alter the topology of
the network according to an algorithm that uses the existing topological
structure as its input. There exist many such reasoning algorithms. With
respect to the Semantic Web, the bivalent, monotonic reasoners of the RDF
Schema (RDFS) and the Web Ontology Language (OWL) are the most prevalent.
However, nothing prevents other forms of reasoning from existing in the
Semantic Web. This article presents a non-bivalent, non-monotonic, evidential
logic and reasoner that is an algebraic ring over a multi-relational network
equipped with two binary operations that can be composed to execute various
forms of inference. Given its multi-relational grounding, it is possible to use
the presented evidential framework as another method for structuring knowledge
and reasoning in the Semantic Web. The benefits of this framework are that it
works with arbitrary, partial, and contradictory knowledge while, at the same
time, it supports a tractable approximate reasoning process.
"
80,Liouvillian Solutions of Difference-Differential Equations,"  For a field k$with an automorphism \sigma and a derivation \delta, we
introduce the notion of liouvillian solutions of linear difference-differential
systems {\sigma(Y) = AY, \delta(Y) = BY} over k and characterize the existence
of liouvillian solutions in terms of the Galois group of the systems. We will
give an algorithm to decide whether such a system has liouvillian solutions
when k = C(x,t), \sigma(x) = x+1, \delta = d/dt$ and the size of the system is
a prime.
"
81,"Characterizing 1-Dof Henneberg-I graphs with efficient configuration
  spaces","  We define and study exact, efficient representations of realization spaces of
a natural class of underconstrained 2D Euclidean Distance Constraint
Systems(EDCS) or Frameworks based on 1-dof Henneberg-I graphs. Each
representation corresponds to a choice of parameters and yields a different
parametrized configuration space. Our notion of efficiency is based on the
algebraic complexities of sampling the configuration space and of obtaining a
realization from the sample (parametrized) configuration. Significantly, we
give purely combinatorial characterizations that capture (i) the class of
graphs that have efficient configuration spaces and (ii) the possible choices
of representation parameters that yield efficient configuration spaces for a
given graph. Our results automatically yield an efficient algorithm for
sampling realizations, without missing extreme or boundary realizations. In
addition, our results formally show that our definition of efficient
configuration space is robust and that our characterizations are tight. We
choose the class of 1-dof Henneberg-I graphs in order to take the next step in
a systematic and graded program of combinatorial characterizations of efficient
configuration spaces. In particular, the results presented here are the first
characterizations that go beyond graphs that have connected and convex
configuration spaces.
"
82,A cache-friendly truncated FFT,"  We describe a cache-friendly version of van der Hoeven's truncated FFT and
inverse truncated FFT, focusing on the case of `large' coefficients, such as
those arising in the Schonhage--Strassen algorithm for multiplication in Z[x].
We describe two implementations and examine their performance.
"
83,Rational Hadamard products via Quantum Diagonal Operators,"  We use the remark that, through Bargmann-Fock representation, diagonal
operators of the Heisenberg-Weyl algebra are scalars for the Hadamard product
to give some properties (like the stability of periodic fonctions) of the
Hadamard product by a rational fraction. In particular, we provide through this
way explicit formulas for the multiplication table of the Hadamard product in
the algebra of rational functions in $\C[[z]]$.
"
84,"Kaltofen's division-free determinant algorithm differentiated for matrix
  adjoint computation","  Kaltofen has proposed a new approach in 1992 for computing matrix
determinants without divisions. The algorithm is based on a baby steps/giant
steps construction of Krylov subspaces, and computes the determinant as the
constant term of a characteristic polynomial. For matrices over an abstract
ring, by the results of Baur and Strassen, the determinant algorithm, actually
a straight-line program, leads to an algorithm with the same complexity for
computing the adjoint of a matrix. However, the latter adjoint algorithm is
obtained by the reverse mode of automatic differentiation, hence somehow is not
""explicit"". We present an alternative (still closely related) algorithm for the
adjoint thatcan be implemented directly, we mean without resorting to an
automatic transformation. The algorithm is deduced by applying program
differentiation techniques ""by hand"" to Kaltofen's method, and is completely
decribed. As subproblem, we study the differentiation of programs that compute
minimum polynomials of lineraly generated sequences, and we use a lazy
polynomial evaluation mechanism for reducing the cost of Strassen's avoidance
of divisions in our case.
"
85,Interpolation of Shifted-Lacunary Polynomials,"  Given a ""black box"" function to evaluate an unknown rational polynomial f in
Q[x] at points modulo a prime p, we exhibit algorithms to compute the
representation of the polynomial in the sparsest shifted power basis. That is,
we determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 <
e2 < ... < et, and the coefficients c1,...,ct in Q\{0} such that f(x) =
c1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely
minimal over any shifted power basis. The novelty of our algorithm is that the
complexity is polynomial in the (sparse) representation size, and in particular
is logarithmic in deg(f). Our method combines previous celebrated results on
sparse interpolation and computing sparsest shifts, and provides a way to
handle polynomials with extremely high degree which are, in some sense, sparse
in information.
"
86,"How to turn a scripting language into a domain specific language for
  computer algebra","  We have developed two computer algebra systems, meditor [Jolly:2007] and JAS
[Kredel:2006]. These CAS systems are available as Java libraries. For the
use-case of interactively entering and manipulating mathematical expressions,
there is a need of a scripting front-end for our libraries. Most other CAS
invent and implement their own scripting interface for this purpose. We,
however, do not want to reinvent the wheel and propose to use a contemporary
scripting language with access to Java code. In this paper we discuss the
requirements for a scripting language in computer algebra and check whether the
languages Python, Ruby, Groovy and Scala meet these requirements. We conclude
that, with minor problems, any of these languages is suitable for our purpose.
"
87,"Trading GRH for algebra: algorithms for factoring polynomials and
  related structures","  In this paper we develop techniques that eliminate the need of the
Generalized Riemann Hypothesis (GRH) from various (almost all) known results
about deterministic polynomial factoring over finite fields. Our main result
shows that given a polynomial f(x) of degree n over a finite field k, we can
find in deterministic poly(n^{\log n},\log |k|) time ""either"" a nontrivial
factor of f(x) ""or"" a nontrivial automorphism of k[x]/(f(x)) of order n. This
main tool leads to various new GRH-free results, most striking of which are:
  (1) Given a noncommutative algebra over a finite field, we can find a zero
divisor in deterministic subexponential time.
  (2) Given a positive integer r such that either 8|r or r has at least two
distinct odd prime factors. There is a deterministic polynomial time algorithm
to find a nontrivial factor of the r-th cyclotomic polynomial over a finite
field.
  In this paper, following the seminal work of Lenstra (1991) on constructing
isomorphisms between finite fields, we further generalize classical Galois
theory constructs like cyclotomic extensions, Kummer extensions, Teichmuller
subgroups, to the case of commutative semisimple algebras with automorphisms.
These generalized constructs help eliminate the dependence on GRH.
"
88,Craig Interpolation for Quantifier-Free Presburger Arithmetic,"  Craig interpolation has become a versatile algorithmic tool for improving
software verification. Interpolants can, for instance, accelerate the
convergence of fixpoint computations for infinite-state systems. They also help
improve the refinement of iteratively computed lazy abstractions. Efficient
interpolation procedures have been presented only for a few theories. In this
paper, we introduce a complete interpolation method for the full range of
quantifier-free Presburger arithmetic formulas. We propose a novel convex
variable projection for integer inequalities and a technique to combine them
with equalities. The derivation of the interpolant has complexity low-degree
polynomial in the size of the refutation proof and is typically fast in
practice.
"
89,Benchmarking the solar dynamo with Maxima,"  Recently, Jouve et al(A&A, 2008) published the paper that presents the
numerical benchmark for the solar dynamo models. Here, I would like to show a
way how to get it with help of computer algebra system Maxima. This way was
used in our paper (Pipin & Seehafer, A&A 2008, in print) to test some new ideas
in the large-scale stellar dynamos. In the present paper I complement the
dynamo benchmark with the standard test that address the problem of the
free-decay modes in the sphere which is submerged in vacuum.
"
90,Automated Induction for Complex Data Structures,"  We propose a procedure for automated implicit inductive theorem proving for
equational specifications made of rewrite rules with conditions and
constraints. The constraints are interpreted over constructor terms
(representing data values), and may express syntactic equality, disequality,
ordering and also membership in a fixed tree language. Constrained equational
axioms between constructor terms are supported and can be used in order to
specify complex data structures like sets, sorted lists, trees, powerlists...
  Our procedure is based on tree grammars with constraints, a formalism which
can describe exactly the initial model of the given specification (when it is
sufficiently complete and terminating). They are used in the inductive proofs
first as an induction scheme for the generation of subgoals at induction steps,
second for checking validity and redundancy criteria by reduction to an
emptiness problem, and third for defining and solving membership constraints.
  We show that the procedure is sound and refutationally complete. It
generalizes former test set induction techniques and yields natural proofs for
several non-trivial examples presented in the paper, these examples are
difficult to specify and carry on automatically with related induction
procedures.
"
91,Stable normal forms for polynomial system solving,"  This paper describes and analyzes a method for computing border bases of a
zero-dimensional ideal $I$. The criterion used in the computation involves
specific commutation polynomials and leads to an algorithm and an
implementation extending the one provided in [MT'05]. This general border basis
algorithm weakens the monomial ordering requirement for \grob bases
computations. It is up to date the most general setting for representing
quotient algebras, embedding into a single formalism Gr\""obner bases, Macaulay
bases and new representation that do not fit into the previous categories. With
this formalism we show how the syzygies of the border basis are generated by
commutation relations. We also show that our construction of normal form is
stable under small perturbations of the ideal, if the number of solutions
remains constant. This new feature for a symbolic algorithm has a huge impact
on the practical efficiency as it is illustrated by the experiments on
classical benchmark polynomial systems, at the end of the paper.
"
92,"Moment matrices, trace matrices and the radical of ideals","  Let $f_1,...,f_s \in \mathbb{K}[x_1,...,x_m]$ be a system of polynomials
generating a zero-dimensional ideal $\I$, where $\mathbb{K}$ is an arbitrary
algebraically closed field. Assume that the factor algebra
$\A=\mathbb{K}[x_1,...,x_m]/\I$ is Gorenstein and that we have a bound
$\delta>0$ such that a basis for $\A$ can be computed from multiples of
$f_1,...,f_s$ of degrees at most $\delta$. We propose a method using Sylvester
or Macaulay type resultant matrices of $f_1,...,f_s$ and $J$, where $J$ is a
polynomial of degree $\delta$ generalizing the Jacobian, to compute moment
matrices, and in particular matrices of traces for $\A$. These matrices of
traces in turn allow us to compute a system of multiplication matrices
$\{M_{x_i}|i=1,...,m\}$ of the radical $\sqrt{\I}$, following the approach in
the previous work by Janovitz-Freireich, R\'{o}nyai and Sz\'ant\'o.
Additionally, we give bounds for $\delta$ for the case when $\I$ has finitely
many projective roots in $\mathbb{P}^m_\CC$.
"
93,Polynomial relations among principal minors of a 4x4-matrix,"  The image of the principal minor map for n x n-matrices is shown to be
closed. In the 19th century, Nansen and Muir studied the implicitization
problem of finding all relations among principal minors when n=4. We complete
their partial results by constructing explicit polynomials of degree 12 that
scheme-theoretically define this affine variety and also its projective closure
in $\PP^{15}$. The latter is the main component in the singular locus of the 2
x 2 x 2 x 2-hyperdeterminant.
"
94,A Sparse Flat Extension Theorem for Moment Matrices,"  In this note we prove a generalization of the flat extension theorem of Curto
and Fialkow for truncated moment matrices. It applies to moment matrices
indexed by an arbitrary set of monomials and its border, assuming that this set
is connected to 1. When formulated in a basis-free setting, this gives an
equivalent result for truncated Hankel operators.
"
95,On the total order of reducibility of a pencil of algebraic plane curves,"  In this paper, the problem of bounding the number of reducible curves in a
pencil of algebraic plane curves is addressed. Unlike most of the previous
related works, each reducible curve of the pencil is here counted with its
appropriate multiplicity. It is proved that this number of reducible curves,
counted with multiplicity, is bounded by d^2-1 where d is the degree of the
pencil. Then, a sharper bound is given by taking into account the Newton's
polygon of the pencil.
"
96,"Using a computer algebra system to simplify expressions for
  Titchmarsh-Weyl m-functions associated with the Hydrogen Atom on the half
  line","  In this paper we give simplified formulas for certain polynomials which arise
in some new Titchmarsh-Weyl m-functions for the radial part of the separated
Hydrogen atom on the half line and two independent programs for generating them
using the symbolic manipulator Mathematica.
"
97,Detecting lacunary perfect powers and computing their roots,"  We consider solutions to the equation f = h^r for polynomials f and h and
integer r > 1. Given a polynomial f in the lacunary (also called sparse or
super-sparse) representation, we first show how to determine if f can be
written as h^r and, if so, to find such an r. This is a Monte Carlo randomized
algorithm whose cost is polynomial in the number of non-zero terms of f and in
log(deg f), i.e., polynomial in the size of the lacunary representation, and it
works over GF(q)[x] (for large characteristic) as well as Q[x]. We also give
two deterministic algorithms to compute the perfect root h given f and r. The
first is output-sensitive (based on the sparsity of h) and works only over
Q[x]. A sparsity-sensitive Newton iteration forms the basis for the second
approach to computing h, which is extremely efficient and works over both
GF(q)[x] (for large characteristic) and Q[x], but depends on a number-theoretic
conjecture. Work of Erdos, Schinzel, Zannier, and others suggests that both of
these algorithms are unconditionally polynomial-time in the lacunary size of
the input polynomial f. Finally, we demonstrate the efficiency of the
randomized detection algorithm and the latter perfect root computation
algorithm with an implementation in the C++ library NTL.
"
98,Some Open Problems in Combinatorial Physics,"  We point out four problems which have arisen during the recent research in
the domain of Combinatorial Physics.
"
99,On the Computation of Matrices of Traces and Radicals of Ideals,"  Let $f_1,...,f_s \in \mathbb{K}[x_1,...,x_m]$ be a system of polynomials
generating a zero-dimensional ideal $\I$, where $\mathbb{K}$ is an arbitrary
algebraically closed field. We study the computation of ""matrices of traces""
for the factor algebra $\A := \CC[x_1, ..., x_m]/ \I$, i.e. matrices with
entries which are trace functions of the roots of $\I$. Such matrices of traces
in turn allow us to compute a system of multiplication matrices
$\{M_{x_i}|i=1,...,m\}$ of the radical $\sqrt{\I}$. We first propose a method
using Macaulay type resultant matrices of $f_1,...,f_s$ and a polynomial $J$ to
compute moment matrices, and in particular matrices of traces for $\A$. Here
$J$ is a polynomial generalizing the Jacobian. We prove bounds on the degrees
needed for the Macaulay matrix in the case when $\I$ has finitely many
projective roots in $\mathbb{P}^m_\CC$. We also extend previous results which
work only for the case where $\A$ is Gorenstein to the non-Gorenstein case. The
second proposed method uses Bezoutian matrices to compute matrices of traces of
$\A$. Here we need the assumption that $s=m$ and $f_1,...,f_m$ define an affine
complete intersection. This second method also works if we have higher
dimensional components at infinity. A new explicit description of the
generators of $\sqrt{\I}$ are given in terms of Bezoutians.
"
100,Homotopy methods for multiplication modulo triangular sets,"  We study the cost of multiplication modulo triangular families of
polynomials. Following previous work by Li, Moreno Maza and Schost, we propose
an algorithm that relies on homotopy and fast evaluation-interpolation
techniques. We obtain a quasi-linear time complexity for substantial families
of examples, for which no such result was known before. Applications are given
to notably addition of algebraic numbers in small characteristic.
"
101,Symmetric tensor decomposition,"  We present an algorithm for decomposing a symmetric tensor, of dimension n
and order d as a sum of rank-1 symmetric tensors, extending the algorithm of
Sylvester devised in 1886 for binary forms. We recall the correspondence
between the decomposition of a homogeneous polynomial in n variables of total
degree d as a sum of powers of linear forms (Waring's problem), incidence
properties on secant varieties of the Veronese Variety and the representation
of linear forms as a linear combination of evaluations at distinct points. Then
we reformulate Sylvester's approach from the dual point of view. Exploiting
this duality, we propose necessary and sufficient conditions for the existence
of such a decomposition of a given rank, using the properties of Hankel (and
quasi-Hankel) matrices, derived from multivariate polynomials and normal form
computations. This leads to the resolution of polynomial equations of small
degree in non-generic cases. We propose a new algorithm for symmetric tensor
decomposition, based on this characterization and on linear algebra
computations with these Hankel matrices. The impact of this contribution is
two-fold. First it permits an efficient computation of the decomposition of any
tensor of sub-generic rank, as opposed to widely used iterative algorithms with
unproved global convergence (e.g. Alternate Least Squares or gradient
descents). Second, it gives tools for understanding uniqueness conditions, and
for detecting the rank.
"
102,Fast algorithms for differential equations in positive characteristic,"  We address complexity issues for linear differential equations in
characteristic $p>0$: resolution and computation of the $p$-curvature. For
these tasks, our main focus is on algorithms whose complexity behaves well with
respect to $p$. We prove bounds linear in $p$ on the degree of polynomial
solutions and propose algorithms for testing the existence of polynomial
solutions in sublinear time $\tilde{O}(p^{1/2})$, and for determining a whole
basis of the solution space in quasi-linear time $\tilde{O}(p)$; the
$\tilde{O}$ notation indicates that we hide logarithmic factors. We show that
for equations of arbitrary order, the $p$-curvature can be computed in
subquadratic time $\tilde{O}(p^{1.79})$, and that this can be improved to
$O(\log(p))$ for first order equations and to $\tilde{O}(p)$ for classes of
second order equations.
"
103,"Performance of Buchberger's Improved Algorithm using Prime Based
  Ordering","  Prime-based ordering which is proved to be admissible, is the encoding of
indeterminates in power-products with prime numbers and ordering them by using
the natural number order. Using Eiffel, four versions of Buchberger's improved
algorithm for obtaining Groebner Bases have been developed: two total degree
versions, representing power products as strings and the other two as integers
based on prime-based ordering. The versions are further distinguished by
implementing coefficients as 64-bit integers and as multiple-precision
integers. By using primebased power product coding, iterative or recursive
operations on power products are replaced with integer operations. It is found
that on a series of example polynomial sets, significant reductions in
computation time of 30% or more are almost always obtained.
"
104,"On finding multiplicities of characteristic polynomial factors of
  black-box matrices","  We present algorithms and heuristics to compute the characteristic polynomial
of a matrix given its minimal polynomial. The matrix is represented as a
black-box, i.e., by a function to compute its matrix-vector product. The
methods apply to matrices either over the integers or over a large enough
finite field. Experiments show that these methods perform efficiently in
practice. Combined in an adaptive strategy, these algorithms reach significant
speedups in practice for some integer matrices arising in an application from
graph theory.
"
105,"The acoustic wave equation in the expanding universe. Sachs-Wolfe
  theorem","  In this paper the acoustic field propagating in the early hot ($p=\epsilon/$)
universe of arbitrary space curvature ($K=0, \pm 1$) is considered. The field
equations are reduced to the d'Alembert equation in an auxiliary static
Roberson-Walker space-time. Symbolic computation in {\em Mathematica} is
applied.
"
106,Graphical Reasoning in Compact Closed Categories for Quantum Computation,"  Compact closed categories provide a foundational formalism for a variety of
important domains, including quantum computation. These categories have a
natural visualisation as a form of graphs. We present a formalism for
equational reasoning about such graphs and develop this into a generic proof
system with a fixed logical kernel for equational reasoning about compact
closed categories. Automating this reasoning process is motivated by the slow
and error prone nature of manual graph manipulation. A salient feature of our
system is that it provides a formal and declarative account of derived results
that can include `ellipses'-style notation. We illustrate the framework by
instantiating it for a graphical language of quantum computation and show how
this can be used to perform symbolic computation.
"
107,Finding Exact Minimal Polynomial by Approximations,"  We present a new algorithm for reconstructing an exact algebraic number from
its approximate value using an improved parameterized integer relation
construction method. Our result is consistent with the existence of error
controlling on obtaining an exact rational number from its approximation. The
algorithm is applicable for finding exact minimal polynomial by its approximate
root. This also enables us to provide an efficient method of converting the
rational approximation representation to the minimal polynomial representation,
and devise a simple algorithm to factor multivariate polynomials with rational
coefficients.
  Compared with other methods, this method has the numerical computation
advantage of high efficiency. The experimental results show that the method is
more efficient than \emph{identify} in \emph{Maple} 11 for obtaining an exact
algebraic number from its approximation. In this paper, we completely implement
how to obtain exact results by numerical approximate computations.
"
108,"A baby steps/giant steps Monte Carlo algorithm for computing roadmaps in
  smooth compact real hypersurfaces","  We consider the problem of constructing roadmaps of real algebraic sets. The
problem was introduced by Canny to answer connectivity questions and solve
motion planning problems. Given $s$ polynomial equations with rational
coefficients, of degree $D$ in $n$ variables, Canny's algorithm has a Monte
Carlo cost of $s^n\log(s) D^{O(n^2)}$ operations in $\mathbb{Q}$; a
deterministic version runs in time $s^n \log(s) D^{O(n^4)}$. The next
improvement was due to Basu, Pollack and Roy, with an algorithm of
deterministic cost $s^{d+1} D^{O(n^2)}$ for the more general problem of
computing roadmaps of semi-algebraic sets ($d \le n$ is the dimension of an
associated object). We give a Monte Carlo algorithm of complexity
$(nD)^{O(n^{1.5})}$ for the problem of computing a roadmap of a compact
hypersurface $V$ of degree $D$ in $n$ variables; we also have to assume that
$V$ has a finite number of singular points. Even under these extra assumptions,
no previous algorithm featured a cost better than $D^{O(n^2)}$.
"
109,Abstraction and Refinement in Static Model-Checking,"  interpretation is a general methodology for building static analyses of
programs. It was introduced by P. and R. Cousot in \cite{cc}. We present, in
this paper, an application of a generic abstract interpretation to domain of
model-checking. Dynamic checking are usually easier to use, because the concept
are establishe d and wide well know. But they are usually limited to systems
whose states space is finite. In an other part, certain faults cannot be
detected dynamically, even by keeping track of the history of the states
space.Indeed, the classical problem of finding the right test cases is far from
trivial and limit the abilities of dynamic checkers further. Static checking
have the advantage that they work on a more abstract level than dynamic checker
and can verify system properties for all inputs. Problem, it is hard to
guarantee that a violation of a modeled property corresponds to a fault in the
concrete system. We propose an approach, in which we generate counter-examples
dynamically using the abstract interpretation techniques.
"
110,Group-Theoretic Partial Matrix Multiplication,"  A generalization of recent group-theoretic matrix multiplication algorithms
to an analogue of the theory of partial matrix multiplication is presented. We
demonstrate that the added flexibility of this approach can in some cases
improve upper bounds on the exponent of matrix multiplication yielded by
group-theoretic full matrix multiplication. The group theory behind our partial
matrix multiplication algorithms leads to the problem of maximizing a quantity
representing the ""fullness"" of a given partial matrix pattern. This problem is
shown to be NP-hard, and two algorithms, one optimal and another non-optimal
but polynomial-time, are given for solving it.
"
111,A formal calculus on the Riordan near algebra,"  The Riordan group is the semi-direct product of a multiplicative group of
invertible series and a group, under substitution, of non units. The Riordan
near algebra, as introduced in this paper, is the Cartesian product of the
algebra of formal power series and its principal ideal of non units, equipped
with a product that extends the multiplication of the Riordan group. The later
is naturally embedded as a subgroup of units into the former. In this paper, we
prove the existence of a formal calculus on the Riordan algebra. This formal
calculus plays a role similar to those of holomorphic calculi in the Banach or
Fr\'echet algebras setting, but without the constraint of a radius of
convergence. Using this calculus, we define \emph{en passant} a notion of
generalized powers in the Riordan group.
"
112,ASF+ --- eine ASF-aehnliche Spezifikationssprache,"  Maintaining the main aspects of the algebraic specification language ASF as
presented in [Bergstra&al.89] we have extend ASF with the following concepts:
While once exported names in ASF must stay visible up to the top the module
hierarchy, ASF+ permits a more sophisticated hiding of signature names. The
erroneous merging of distinct structures that occurs when importing different
actualizations of the same parameterized module in ASF is avoided in ASF+ by a
more adequate form of parameter binding. The new ``Namensraum''-concept of ASF+
permits the specifier on the one hand directly to identify the origin of hidden
names and on the other to decide whether an imported module is only to be
accessed or whether an important property of it is to be modified. In the first
case he can access one single globally provided version; in the second he has
to import a copy of the module. Finally ASF+ permits semantic conditions on
parameters and the specification of tasks for a theorem prover.
"
113,"A bound on the minimum of a real positive polynomial over the standard
  simplex","  We consider the problem of bounding away from 0 the minimum value m taken by
a polynomial P of Z[X_1,...,X_k] over the standard simplex, assuming that m>0.
Recent algorithmic developments in real algebraic geometry enable us to obtain
a positive lower bound on m in terms of the dimension k, the degree d and the
bitsize of the coefficients of P. The bound is explicit, and obtained without
any extra assumption on P, in contrast with previous results reported in the
literature.
"
114,"Determining the closed forms of the $O(a_s^3)$ anomalous dimensions and
  Wilson coefficients from Mellin moments by means of computer algebra","  Single scale quantities, as anomalous dimensions and hard scattering cross
sections, in renormalizable Quantum Field Theories are found to obey difference
equations of finite order in Mellin space. It is often easier to calculate
fixed moments for these quantities compared to a direct attempt to derive them
in terms of harmonic sums and their generalizations involving the Mellin
parameter $N$. Starting from a sufficiently large number of given moments, we
establish linear recurrence relations of lowest possible order with polynomial
coefficients of usually high degree. Then these recurrence equations are solved
in terms of d'Alembertian solutions where the involved nested sums are
represented in optimal nested depth. Given this representation, it is then an
easy task to express the result in terms of harmonic sums. In this process we
compactify the result such that no algebraic relations occur among the sums
involved. We demonstrate the method for the QCD unpolarized anomalous
dimensions and massless Wilson coefficients to 3--loop order treating the
contributions for individual color coefficients. For the most complicated
subproblem 5114 moments were needed in order to produce a recurrence of order
35 whose coefficients have degrees up to 938. About four months of CPU time
were needed to establish and solve the recurrences for the anomalous dimensions
and Wilson coefficients on a 2 GHz machine requiring less than 10 GB of memory.
No algorithm is known yet to provide such a high number of moments for 3--loop
quantities. Yet the method presented shows that it is possible to establish and
solve recurrences of rather large order and and degree, occurring in physics
problems, uniquely, fast and reliably with computer algebra.
"
115,From Moments to Functions in Quantum Chromodynamics,"  Single-scale quantities, like the QCD anomalous dimensions and Wilson
coefficients, obey difference equations. Therefore their analytic form can be
determined from a finite number of moments. We demonstrate this in an explicit
calculation by establishing and solving large scale recursions by means of
computer algebra for the anomalous dimensions and Wilson coefficients in
unpolarized deeply inelastic scattering from their Mellin moments to 3-loop
order.
"
116,A Graph Analysis of the Linked Data Cloud,"  The Linked Data community is focused on integrating Resource Description
Framework (RDF) data sets into a single unified representation known as the Web
of Data. The Web of Data can be traversed by both man and machine and shows
promise as the \textit{de facto} standard for integrating data world wide much
like the World Wide Web is the \textit{de facto} standard for integrating
documents. On February 27$^\text{th}$ of 2009, an updated Linked Data cloud
visualization was made publicly available. This visualization represents the
various RDF data sets currently in the Linked Data cloud and their interlinking
relationships. For the purposes of this article, this visual representation was
manually transformed into a directed graph and analyzed.
"
117,Heuristic Reasoning on Graph and Game Complexity of Sudoku,"  The Sudoku puzzle has achieved worldwide popularity recently, and attracted
great attention of the computational intelligence community. Sudoku is always
considered as Satisfiability Problem or Constraint Satisfaction Problem. In
this paper, we propose to focus on the essential graph structure underlying the
Sudoku puzzle. First, we formalize Sudoku as a graph. Then a solving algorithm
based on heuristic reasoning on the graph is proposed. The related r-Reduction
theorem, inference theorem and their properties are proved, providing the
formal basis for developments of Sudoku solving systems. In order to evaluate
the difficulty levels of puzzles, a quantitative measurement of the complexity
level of Sudoku puzzles based on the graph structure and information theory is
proposed. Experimental results show that all the puzzles can be solved fast
using the proposed heuristic reasoning, and that the proposed game complexity
metrics can discriminate difficulty levels of puzzles perfectly.
"
118,Combinatorial Deformations of Algebras: Twisting and Perturbations,"  The framework used to prove the multiplicative law deformation of the algebra
of Feynman-Bender diagrams is a \textit{twisted shifted dual law} (in fact,
twice). We give here a clear interpretation of its two parameters. The crossing
parameter is a deformation of the tensor structure whereas the superposition
parameters is a perturbation of the shuffle coproduct of Hoffman type which, in
turn, can be interpreted as the diagonal restriction of a superproduct. Here,
we systematically detail these constructions.
"
119,Supernodal Analysis Revisited,"  In this paper we show how to extend the known algorithm of nodal analysis in
such a way that, in the case of circuits without nullors and controlled sources
(but allowing for both, independent current and voltage sources), the system of
nodal equations describing the circuit is partitioned into one part, where the
nodal variables are explicitly given as linear combinations of the voltage
sources and the voltages of certain reference nodes, and another, which
contains the node variables of these reference nodes only and which moreover
can be read off directly from the given circuit. Neither do we need
preparational graph transformations, nor do we need to introduce additional
current variables (as in MNA). Thus this algorithm is more accessible to
students, and consequently more suitable for classroom presentations.
"
120,Computations modulo regular chains,"  The computation of triangular decompositions are based on two fundamental
operations: polynomial GCDs modulo regular chains and regularity test modulo
saturated ideals. We propose new algorithms for these core operations relying
on modular methods and fast polynomial arithmetic. Our strategies take also
advantage of the context in which these operations are performed. We report on
extensive experimentation, comparing our code to pre-existing Maple
implementations, as well as more optimized Magma functions. In most cases, our
new code outperforms the other packages by several orders of magnitude.
"
121,"Computing Cylindrical Algebraic Decomposition via Triangular
  Decomposition","  Cylindrical algebraic decomposition is one of the most important tools for
computing with semi-algebraic sets, while triangular decomposition is among the
most important approaches for manipulating constructible sets. In this paper,
for an arbitrary finite set $F \subset {\R}[y_1, ..., y_n]$ we apply
comprehensive triangular decomposition in order to obtain an $F$-invariant
cylindrical decomposition of the $n$-dimensional complex space, from which we
extract an $F$-invariant cylindrical algebraic decomposition of the
$n$-dimensional real space. We report on an implementation of this new approach
for constructing cylindrical algebraic decompositions.
"
122,"Differential reduction of generalized hypergeometric functions from
  Feynman diagrams: One-variable case","  The differential-reduction algorithm, which allows one to express generalized
hypergeometric functions with parameters of arbitrary values in terms of such
functions with parameters whose values differ from the original ones by
integers, is discussed in the context of evaluating Feynman diagrams. Where
this is possible, we compare our results with those obtained using standard
techniques. It is shown that the criterion of reducibility of multiloop Feynman
integrals can be reformulated in terms of the criterion of reducibility of
hypergeometric functions. The relation between the numbers of master integrals
obtained by differential reduction and integration by parts is discussed.
"
123,Dependency Pairs and Polynomial Path Orders,"  We show how polynomial path orders can be employed efficiently in conjunction
with weak innermost dependency pairs to automatically certify polynomial
runtime complexity of term rewrite systems and the polytime computability of
the functions computed. The established techniques have been implemented and we
provide ample experimental data to assess the new method.
"
124,"On the Cooperation of the Constraint Domains H, R and FD in CFLP","  This paper presents a computational model for the cooperation of constraint
domains and an implementation for a particular case of practical importance.
The computational model supports declarative programming with lazy and possibly
higher-order functions, predicates, and the cooperation of different constraint
domains equipped with their respective solvers, relying on a so-called
Constraint Functional Logic Programming (CFLP) scheme. The implementation has
been developed on top of the CFLP system TOY, supporting the cooperation of the
three domains H, R and FD, which supply equality and disequality constraints
over symbolic terms, arithmetic constraints over the real numbers, and finite
domain constraints over the integers, respectively. The computational model has
been proved sound and complete w.r.t. the declarative semantics provided by the
$CFLP$ scheme, while the implemented system has been tested with a set of
benchmarks and shown to behave quite efficiently in comparison to the closest
related approach we are aware of.
  To appear in Theory and Practice of Logic Programming (TPLP)
"
125,A Symbolic Summation Approach to Find Optimal Nested Sum Representations,"  We consider the following problem: Given a nested sum expression, find a sum
representation such that the nested depth is minimal. We obtain a symbolic
summation framework that solves this problem for sums defined, e.g., over
hypergeometric, $q$-hypergeometric or mixed hypergeometric expressions.
Recently, our methods have found applications in quantum field theory.
"
126,Effective Bounds for P-Recursive Sequences,"  We describe an algorithm that takes as input a complex sequence $(u_n)$ given
by a linear recurrence relation with polynomial coefficients along with initial
values, and outputs a simple explicit upper bound $(v_n)$ such that $|u_n| \leq
v_n$ for all $n$. Generically, the bound is tight, in the sense that its
asymptotic behaviour matches that of $u_n$. We discuss applications to the
evaluation of power series with guaranteed precision.
"
127,A Non-Holonomic Systems Approach to Special Function Identities,"  We extend Zeilberger's approach to special function identities to cases that
are not holonomic. The method of creative telescoping is thus applied to
definite sums or integrals involving Stirling or Bernoulli numbers, incomplete
Gamma function or polylogarithms, which are not covered by the holonomic
framework. The basic idea is to take into account the dimension of appropriate
ideals in Ore algebras. This unifies several earlier extensions and provides
algorithms for summation and integration in classes that had not been
accessible to computer algebra before.
"
128,La R\'esolvante de Lagrange et ses Applications,"  In this paper, the changes of representations of a group are used in order to
describe its action as algebraic Galois group of an univariate polynomial on
the roots of factors of any Lagrange resolvent. By this way, the Galois group
of resolvent factors are pre-determinated. In follows, different applications
are exposed; in particular, some classical results of algebraic Galois theory.
"
129,"Counting Complex Disordered States by Efficient Pattern Matching:
  Chromatic Polynomials and Potts Partition Functions","  Counting problems, determining the number of possible states of a large
system under certain constraints, play an important role in many areas of
science. They naturally arise for complex disordered systems in physics and
chemistry, in mathematical graph theory, and in computer science. Counting
problems, however, are among the hardest problems to access computationally.
Here, we suggest a novel method to access a benchmark counting problem, finding
chromatic polynomials of graphs. We develop a vertex-oriented symbolic pattern
matching algorithm that exploits the equivalence between the chromatic
polynomial and the zero-temperature partition function of the Potts
antiferromagnet on the same graph. Implementing this bottom-up algorithm using
appropriate computer algebra, the new method outperforms standard top-down
methods by several orders of magnitude, already for moderately sized graphs. As
a first application, we compute chromatic polynomials of samples of the simple
cubic lattice, for the first time computationally accessing three-dimensional
lattices of physical relevance. The method offers straightforward
generalizations to several other counting problems.
"
130,"Successive Difference Substitution Based on Column Stochastic Matrix and
  Mechanical Decision for Positive Semi-definite Forms","  The theory part of this paper is sketched as follows. Based on column
stochastic average matrix $T_n$ selected as a basic substitution matrix, the
method of advanced successive difference substitution is established. Then, a
set of necessary and sufficient conditions for deciding positive semi-definite
form on $\R^n_+$ is derived from this method. And furthermore, it is proved
that the sequence of SDS sets of a positive definite form is positively
terminating.
  Worked out according to these results, the Maple program TSDS3 not only
automatically proves the polynomial inequalities, but also outputs counter
examples for the false. Sometimes TSDS3 does not halt, but it is very useful by
experimenting on so many examples.
"
131,Multihomogeneous Resultant Formulae for Systems with Scaled Support,"  Constructive methods for matrices of multihomogeneous (or multigraded)
resultants for unmixed systems have been studied by Weyman, Zelevinsky,
Sturmfels, Dickenstein and Emiris. We generalize these constructions to mixed
systems, whose Newton polytopes are scaled copies of one polytope, thus taking
a step towards systems with arbitrary supports. First, we specify matrices
whose determinant equals the resultant and characterize the systems that admit
such formulae. Bezout-type determinantal formulae do not exist, but we describe
all possible Sylvester-type and hybrid formulae. We establish tight bounds for
all corresponding degree vectors, and specify domains that will surely contain
such vectors; the latter are new even for the unmixed case. Second, we make use
of multiplication tables and strong duality theory to specify resultant
matrices explicitly, for a general scaled system, thus including unmixed
systems. The encountered matrices are classified; these include a new type of
Sylvester-type matrix as well as Bezout-type matrices, known as partial
Bezoutians. Our public-domain Maple implementation includes efficient storage
of complexes in memory, and construction of resultant matrices.
"
132,"Castelnuovo-Mumford Regularity and Computing the de Rham Cohomology of
  Smooth Projective Varieties","  We describe a parallel polynomial time algorithm for computing the
topological Betti numbers of a smooth complex projective variety $X$. It is the
first single exponential time algorithm for computing the Betti numbers of a
significant class of complex varieties of arbitrary dimension. Our main
theoretical result is that the Castelnuovo-Mumford regularity of the sheaf of
differential $p$-forms on $X$ is bounded by $p(em+1)D$, where $e$, $m$, and $D$
are the maximal codimension, dimension, and degree, respectively, of all
irreducible components of $X$. It follows that, for a union $V$ of generic
hyperplane sections in $X$, the algebraic de Rham cohomology of $X\setminus V$
is described by differential forms with poles along $V$ of single exponential
order. This yields a similar description of the de Rham cohomology of $X$,
which allows its efficient computation. Furthermore, we give a parallel
polynomial time algorithm for testing whether a projective variety is smooth.
"
133,"Programming Realization of Symbolic Computations for Non-linear
  Commutator Superalgebras over the Heisenberg--Weyl Superalgebra: Data
  Structures and Processing Methods","  We suggest a programming realization of an algorithm for verifying a given
set of algebraic relations in the form of a supercommutator multiplication
table for the Verma module, which is constructed according to a generalized
Cartan procedure for a quadratic superalgebra and whose elements are realized
as a formal power series with respect to non-commuting elements. To this end,
we propose an algebraic procedure of Verma module construction and its
realization in terms of non-commuting creation and annihilation operators of a
given Heisenberg--Weyl superalgebra. In doing so, we set up a problem which
naturally arises within a Lagrangian description of higher-spin fields in
anti-de-Sitter (AdS) spaces: to verify the fact that the resulting Verma module
elements obey the given commutator multiplication for the original non-linear
superalgebra. The problem setting is based on a restricted principle of
mathematical induction, in powers of inverse squared radius of the AdS-space.
For a construction of an algorithm resolving this problem, we use a two-level
data model within the object-oriented approach, which is realized on a basis of
the programming language C#. The program allows one to consider objects (of a
less general nature than non-linear commutator superalgebras) that fall under
the class of so-called $GR$-algebras, for whose treatment one widely uses the
module \emph{Plural} of the system \emph{Singular} of symbolic computations for
polynomials.
"
134,Continued Fraction Expansion of Real Roots of Polynomial Systems,"  We present a new algorithm for isolating the real roots of a system of
multivariate polynomials, given in the monomial basis. It is inspired by
existing subdivision methods in the Bernstein basis; it can be seen as
generalization of the univariate continued fraction algorithm or alternatively
as a fully analog of Bernstein subdivision in the monomial basis. The
representation of the subdivided domains is done through homographies, which
allows us to use only integer arithmetic and to treat efficiently unbounded
regions. We use univariate bounding functions, projection and preconditionning
techniques to reduce the domain of search. The resulting boxes have optimized
rational coordinates, corresponding to the first terms of the continued
fraction expansion of the real roots. An extension of Vincent's theorem to
multivariate polynomials is proved and used for the termination of the
algorithm. New complexity bounds are provided for a simplified version of the
algorithm. Examples computed with a preliminary C++ implementation illustrate
the approach.
"
135,Spherical Distribution of 5 Points with Maximal Distance Sum,"  In this paper, we mainly consider the problem of spherical distribution of 5
points, that is, how to configure 5 points on a sphere such that the mutual
distance sum attains the maximum. It is conjectured that the sum of distances
is maximal if 5 points form a bipyramid configuration in which case two points
are positioned at two poles of the sphere and the other three are positioned
uniformly on the equator. We study this problem using interval methods and
related technics, and give a proof for the conjecture through computers in
finite time.
"
136,"Eliminating Human Insight: An Algorithmic Proof of Stembridge's TSPP
  Theorem","  We present a new proof of Stembridge's theorem about the enumeration of
totally symmetric plane partitions using the methodology suggested in the
recent Koutschan-Kauers-Zeilberger semi-rigorous proof of the Andrews-Robbins
q-TSPP conjecture. Our proof makes heavy use of computer algebra and is
completely automatic. We describe new methods that make the computations
feasible in the first place. The tantalizing aspect of this work is that the
same methods can be applied to prove the q-TSPP conjecture (that is a
q-analogue of Stembridge's theorem and open for more than 25 years); the only
hurdle here is still the computational complexity.
"
137,The alternative operad is not Koszul,"  Using computer calculations, we prove the statement in the title.
"
138,Symbolic Script Programming for Java,"  Computer algebra in Java is a promising field of development. It has not yet
reached an industrial strength, in part because of a lack of good user
interfaces. Using a general purpose scripting language can bring a natural
mathematical notation, akin to the one of specialized interfaces included in
most computer algebra systems. We present such an interface for Java computer
algebra libraries, using scripts available in the JSR 223 framework. We
introduce the concept of `symbolic programming' and show its usefulness by
prototypes of symbolic polynomials and polynomial rings.
"
139,Chebyshev Expansions for Solutions of Linear Differential Equations,"  A Chebyshev expansion is a series in the basis of Chebyshev polynomials of
the first kind. When such a series solves a linear differential equation, its
coefficients satisfy a linear recurrence equation. We interpret this equation
as the numerator of a fraction of linear recurrence operators. This
interpretation lets us give a simple view of previous algorithms, analyze their
complexity, and design a faster one for large orders.
"
140,"Noether's forms for the study of non-composite rational functions and
  their spectrum","  In this paper, the spectrum and the decomposability of a multivariate
rational function are studied by means of the effective Noether's
irreducibility theorem given by Ruppert. With this approach, some new effective
results are obtained. In particular, we show that the reduction modulo p of the
spectrum of a given integer multivariate rational function r coincides with the
spectrum of the reduction of r modulo p for p a prime integer greater or equal
to an explicit bound. This bound is given in terms of the degree, the height
and the number of variables of r. With the same strategy, we also study the
decomposability of r modulo p. Some similar explicit results are also provided
for the case of polynomials with coefficients in a polynomial ring.
"
141,"Real Solution Isolation with Multiplicity of Zero-Dimensional Triangular
  Systems","  Existing algorithms for isolating real solutions of zero-dimensional
polynomial systems do not compute the multiplicities of the solutions. In this
paper, we define in a natural way the multiplicity of solutions of
zero-dimensional triangular polynomial systems and prove that our definition is
equivalent to the classical definition of local (intersection) multiplicity.
Then we present an effective and complete algorithm for isolating real
solutions with multiplicities of zero-dimensional triangular polynomial systems
using our definition. The algorithm is based on interval arithmetic and
square-free factorization of polynomials with real algebraic coefficients. The
computational results on some examples from the literature are presented.
"
142,On computing the Hermite form of a matrix of differential polynomials,"  Given an n x n matrix over the ring of differential polynomials
F(t)[\D;\delta], we show how to compute the Hermite form H of A, and a
unimodular matrix U such that UA=H. The algorithm requires a polynomial number
of operations in terms of n, deg_D(A), and deg_t(A). When F is the field of
rational numbers, it also requires time polynomial in the bit-length of the
coefficients.
"
143,On the minimum of a positive polynomial over the standard simplex,"  We present a new positive lower bound for the minimum value taken by a
polynomial P with integer coefficients in k variables over the standard simplex
of R^k, assuming that P is positive on the simplex. This bound depends only on
the number of variables, the degree and the bitsize of the coefficients of P
and improves all previous bounds for arbitrary polynomials which are positive
over the simplex.
"
144,Simultaneous Integer Relation Detection and Its an Application,"  Let $\mathbf{x_1}, ..., \mathbf{x_t} \in \mathbb{R}^{n}$. A simultaneous
integer relation (SIR) for $\mathbf{x_1}, ..., \mathbf{x_t}$ is a vector
$\mathbf{m} \in \mathbb{Z}^{n}\setminus\{\textbf{0}\}$ such that
$\mathbf{x_i}^T\mathbf{m} = 0$ for $i = 1, ..., t$. In this paper, we propose
an algorithm SIRD to detect an SIR for real vectors, which constructs an SIR
within $\mathcal {O}(n^4 + n^3 \log \lambda(X))$ arithmetic operations, where
$\lambda(X)$ is the least Euclidean norm of SIRs for $\mathbf{x_1}, >...,
\mathbf{x_t}$. One can easily generalize SIRD to complex number field.
Experimental results show that SIRD is practical and better than another
detecting algorithm in the literature. In its application, we present a new
algorithm for finding the minimal polynomial of an arbitrary complex algebraic
number from its an approximation, which is not based on LLL. We also provide a
sufficient condition on the precision of the approximate value, which depends
only on the height and the degree of the algebraic number.
"
145,"An Improved Algorithm based on Shannon-Happ Formula for Calculating
  Transfer Function from Signal Flow Graph and Its Visualization","  A new method based on Shannon-Happ formula to calculate transfer function
from Signal Flow Graph (SFG) is presented. The algorithm provides an explicit
approach to get the transfer function in a format with both numerical and
symbolic expressions. The adoption of the symbolic variable in SFG, which could
represent the nonlinear item or the independent sub-system, is achieved by
variable separation approach. An investigation is given for the solutions of
several special conditions of SFG. To improve the efficiency of the algorithm,
a new technique combined with Johnson method for generating the combinations of
the non-touching loops is developed. It uses the previous combinations in lower
order to get the ones in higher order. There is an introduction about the
visualization of SFG and the subroutines for system performance analysis in the
software, AVANT.
"
146,Generating functions of Chebyshev-like polynomials,"  In this short note, we give simple proofs of several results and conjectures
formulated by Stolarsky and Tran concerning generating functions of some
families of Chebyshev-like polynomials.
"
147,The Piranha algebraic manipulator,"  In this paper we present a specialised algebraic manipulation package devoted
to Celestial Mechanics. The system, called Piranha, is built on top of a
generic and extensible framework, which allows to treat efficiently and in a
unified way the algebraic structures most commonly encountered in Celestial
Mechanics (such as multivariate polynomials and Poisson series). In this
contribution we explain the architecture of the software, with special focus on
the implementation of series arithmetics, show its current capabilities, and
present benchmarks indicating that Piranha is competitive, performance-wise,
with other specialised manipulators.
"
148,"An Efficient Algorithm for Factoring Polynomials over Algebraic
  Extension Field","  A new efficient algorithm is proposed for factoring polynomials over an
algebraic extension field. The extension field is defined by a polynomial ring
modulo a maximal ideal. If the maximal ideal is given by its Groebner basis, no
extra Groebner basis computation is needed for factoring a polynomial over this
extension field. Nothing more than linear algebraic technique is used to get a
polynomial over the ground field by a generic linear map. Then this polynomial
is factorized over the ground field. From these factors, the factorization of
the polynomial over the extension field is obtained. The new algorithm has been
implemented and computer experiments indicate that the new algorithm is very
efficient, particularly in complicated examples.
"
149,An Explicit Construction of Gauss-Jordan Elimination Matrix,"  A constructive approach to get the reduced row echelon form of a given matrix
$A$ is presented. It has been shown that after the $k$th step of the
Gauss-Jordan procedure, each entry $a^k_{ij}(i<>j; j > k)$ in the new matrix
$A^k$ can always be expressed as a ratio of two determinants whose entries are
from the original matrix $A$. The new method also gives a more general
generalization of Cramer's rule than existing methods.
"
150,Proof Theory at Work: Complexity Analysis of Term Rewrite Systems,"  This thesis is concerned with investigations into the ""complexity of term
rewriting systems"". Moreover the majority of the presented work deals with the
""automation"" of such a complexity analysis. The aim of this introduction is to
present the main ideas in an easily accessible fashion to make the result
presented accessible to the general public. Necessarily some technical points
are stated in an over-simplified way.
"
151,Ladder Operators and Endomorphisms in Combinatorial Physics,"  Starting with the Heisenberg-Weyl algebra, fundamental to quantum physics, we
first show how the ordering of the non-commuting operators intrinsic to that
algebra gives rise to generalizations of the classical Stirling Numbers of
Combinatorics. These may be expressed in terms of infinite, but {\em
row-finite}, matrices, which may also be considered as endomorphisms of
$\C[[x]]$. This leads us to consider endomorphisms in more general spaces, and
these in turn may be expressed in terms of generalizations of the
ladder-operators familiar in physics.
"
152,The complete Generating Function for Gessel Walks is Algebraic,"  Gessel walks are lattice walks in the quarter plane $\set N^2$ which start at
the origin $(0,0)\in\set N^2$ and consist only of steps chosen from the set
$\{\leftarrow,\swarrow,\nearrow,\to\}$. We prove that if $g(n;i,j)$ denotes the
number of Gessel walks of length $n$ which end at the point $(i,j)\in\set N^2$,
then the trivariate generating series $G(t;x,y)=\sum_{n,i,j\geq 0} g(n;i,j)x^i
y^j t^n$ is an algebraic function.
"
153,Topology of 2D and 3D Rational Curves,"  In this paper we present algorithms for computing the topology of planar and
space rational curves defined by a parametrization. The algorithms given here
work directly with the parametrization of the curve, and do not require to
compute or use the implicit equation of the curve (in the case of planar
curves) or of any projection (in the case of space curves). Moreover, these
algorithms have been implemented in Maple; the examples considered and the
timings obtained show good performance skills.
"
154,"Implementing Gr\""obner bases for operads","  We present an implementation of the algorithm for computing Groebner bases
for operads due to the first author and A. Khoroshkin. We discuss the actual
algorithms, the choices made for the implementation platform and the data
representation, and strengths and weaknesses of our approach.
"
155,"On the Different Shapes Arising in a Family of Rational Curves Depending
  on a Parameter","  Given a family of rational curves depending on a real parameter, defined by
its parametric equations, we provide an algorithm to compute a finite partition
of the parameter space (${\Bbb R}$, in general) so that the shape of the family
stays invariant along each element of the partition. So, from this partition
the topology types in the family can be determined. The algorithm is based on a
geometric interpretation of previous work (\cite{JGRS}) for the implicit case.
However, in our case the algorithm works directly with the parametrization of
the family, and the implicit equation does not need to be computed. Timings
comparing the algorithm in the implicit and the parametric cases are given;
these timings show that the parametric algorithm developed here provides in
general better results than the known algorithm for the implicit case.
"
156,Local Shape of Generalized Offsets to Algebraic Curves,"  In this paper we study the local behavior of an algebraic curve under a
geometric construction which is a variation of the usual offsetting
construction, namely the {\it generalized} offsetting process (\cite {SS99}).
More precisely, here we discuss when and how this geometric construction may
cause local changes in the shape of an algebraic curve, and we compare our
results with those obtained for the case of classical offsets (\cite{JGS07}).
For these purposes, we use well-known notions of Differential Geometry, and
also the notion of {\it local shape} introduced in \cite{JGS07}.
"
157,Faster algorithms for the square root and reciprocal of power series,"  We give new algorithms for the computation of square roots and reciprocals of
power series in C[[x]]. If M(n) denotes the cost of multiplying polynomials of
degree n, the square root to order n costs (1.333... + o(1)) M(n) and the
reciprocal costs (1.444... + o(1)) M(n). These improve on the previous best
results, respectively (1.8333... + o(1)) M(n) and (1.5 + o(1)) M(n).
"
158,Decreasing Diagrams and Relative Termination,"  In this paper we use the decreasing diagrams technique to show that a
left-linear term rewrite system R is confluent if all its critical pairs are
joinable and the critical pair steps are relatively terminating with respect to
R. We further show how to encode the rule-labeling heuristic for decreasing
diagrams as a satisfiability problem. Experimental data for both methods are
presented.
"
159,"Computing rational points in convex semi-algebraic sets and SOS
  decompositions","  Let ${\cal P}=\{h_1, ..., h_s\}\subset \Z[Y_1, ..., Y_k]$, $D\geq \deg(h_i)$
for $1\leq i \leq s$, $\sigma$ bounding the bit length of the coefficients of
the $h_i$'s, and $\Phi$ be a quantifier-free ${\cal P}$-formula defining a
convex semi-algebraic set. We design an algorithm returning a rational point in
${\cal S}$ if and only if ${\cal S}\cap \Q\neq\emptyset$. It requires
$\sigma^{\bigO(1)}D^{\bigO(k^3)}$ bit operations. If a rational point is
outputted its coordinates have bit length dominated by $\sigma D^{\bigO(k^3)}$.
Using this result, we obtain a procedure deciding if a polynomial $f\in \Z[X_1,
>..., X_n]$ is a sum of squares of polynomials in $\Q[X_1, ..., X_n]$. Denote
by $d$ the degree of $f$, $\tau$ the maximum bit length of the coefficients in
$f$, $D={{n+d}\choose{n}}$ and $k\leq D(D+1)-{{n+2d}\choose{n}}$. This
procedure requires $\tau^{\bigO(1)}D^{\bigO(k^3)}$ bit operations and the
coefficients of the outputted polynomials have bit length dominated by $\tau
D^{\bigO(k^3)}$.
"
160,Computing modular correspondences for abelian varieties,"  The aim of this paper is to give a higher dimensional equivalent of the
classical modular polynomials $\Phi_\ell(X,Y)$. If $j$ is the $j$-invariant
associated to an elliptic curve $E_k$ over a field $k$ then the roots of
$\Phi_\ell(j,X)$ correspond to the $j$-invariants of the curves which are
$\ell$-isogeneous to $E_k$. Denote by $X_0(N)$ the modular curve which
parametrizes the set of elliptic curves together with a $N$-torsion subgroup.
It is possible to interpret $\Phi_\ell(X,Y)$ as an equation cutting out the
image of a certain modular correspondence $X_0(\ell) \to X_0(1) \times X_0(1)$
in the product $X_0(1) \times X_0(1)$. Let $g$ be a positive integer and
$\overn \in \N^g$. We are interested in the moduli space that we denote by
$\Mn$ of abelian varieties of dimension $g$ over a field $k$ together with an
ample symmetric line bundle $\pol$ and a symmetric theta structure of type
$\overn$. If $\ell$ is a prime and let $\overl=(\ell, ..., \ell)$, there exists
a modular correspondence $\Mln \to \Mn \times \Mn$. We give a system of
algebraic equations defining the image of this modular correspondence.
"
161,Minimal Polynomial Algorithms for Finite Sequences,"  We show that a straightforward rewrite of a known minimal polynomial
algorithm yields a simpler version of a recent algorithm of A. Salagean.
"
162,"Bounding the radii of balls meeting every connected component of
  semi-algebraic sets","  We prove explicit bounds on the radius of a ball centered at the origin which
is guaranteed to contain all bounded connected components of a semi-algebraic
set $S \subset \mathbbm{R}^k$ defined by a quantifier-free formula involving
$s$ polynomials in $\mathbbm{Z}[X_1, ..., X_k]$ having degrees at most $d$, and
whose coefficients have bitsizes at most $\tau$. Our bound is an explicit
function of $s, d, k$ and $\tau$, and does not contain any undetermined
constants. We also prove a similar bound on the radius of a ball guaranteed to
intersect every connected component of $S$ (including the unbounded
components). While asymptotic bounds of the form $2^{\tau d^{O (k)}}$ on these
quantities were known before, some applications require bounds which are
explicit and which hold for all values of $s, d, k$ and $\tau$. The bounds
proved in this paper are of this nature.
"
163,From Abstract Rewriting Systems to Abstract Proof Systems,"  Some personal recollections on the introduction of `abstract proof systems'
as a framework for formulating syntax-independent, general results about rule
derivability and admissibility. With a particular eye on the inspiration I owe
to Roel de Vrijer: the analogy with abstract rewriting systems.
"
164,Faster exponentials of power series,"  We describe a new algorithm for computing exp(f) where f is a power series in
C[[x]]. If M(n) denotes the cost of multiplying polynomials of degree n, the
new algorithm costs (2.1666... + o(1)) M(n) to compute exp(f) to order n. This
improves on the previous best result, namely (2.333... + o(1)) M(n).
"
165,The Hilbert scheme of points and its link with border basis,"  In this paper, we give new explicit representations of the Hilbert scheme of
$\mu$ points in $\PP^{r}$ as a projective subvariety of a Grassmanniann
variety. This new explicit description of the Hilbert scheme is simpler than
the previous ones and global. It involves equations of degree $2$. We show how
these equations are deduced from the commutation relations characterizing
border bases. Next, we consider infinitesimal perturbations of an input system
of equations on this Hilbert scheme and describe its tangent space. We propose
an effective criterion to test if it is a flat deformation, that is if the
perturbed system remains on the Hilbert scheme of the initial equations. This
criterion involves in particular formal reduction with respect to border bases.
"
166,"Deciding Regularity of the Set of Instances of a Set of Terms with
  Regular Constraints is EXPTIME-Complete","  Finite-state tree automata are a well studied formalism for representing term
languages. This paper studies the problem of determining the regularity of the
set of instances of a finite set of terms with variables, where each variable
is restricted to instantiations of a regular set given by a tree automaton. The
problem was recently proved decidable, but with an unknown complexity. Here,
the exact complexity of the problem is determined by proving
EXPTIME-completeness. The main contribution is a new, exponential time
algorithm that performs various exponential transformations on the involved
terms and tree automata, and decides regularity by analyzing formulas over
inequality and height predicates.
"
167,A computational definition of the notion of vectorial space,"  We usually define an algebraic structure by a set, some operations defined on
this set and some propositions that the algebraic structure must validate. In
some cases, we can replace these propositions by an algorithm on terms
constructed upon these operations that the algebraic structure must validate.
We show in this note that this is the case for the notions of vectorial space
and bilinear operation. KEYWORDS: Rewrite system, vector space, bilinear
operation, tensorial product, semantics, quantum programming languages,
probabilistic programming languages.
"
168,"Automated Proofs in Geometry : Computing Upper Bounds for the Heilbronn
  Problem for Triangles","  We propose a method for computing upper bounds for the Heilbronn problem for
triangles.
"
169,"Efficient Higher Order Derivatives of Objective Functions Composed of
  Matrix Operations","  This paper is concerned with the efficient evaluation of higher-order
derivatives of functions $f$ that are composed of matrix operations. I.e., we
want to compute the $D$-th derivative tensor $\nabla^D f(X) \in \mathbb
R^{N^D}$, where $f:\mathbb R^{N} \to \mathbb R$ is given as an algorithm that
consists of many matrix operations. We propose a method that is a combination
of two well-known techniques from Algorithmic Differentiation (AD): univariate
Taylor propagation on scalars (UTPS) and first-order forward and reverse on
matrices. The combination leads to a technique that we would like to call
univariate Taylor propagation on matrices (UTPM). The method inherits many
desirable properties: It is easy to implement, it is very efficient and it
returns not only $\nabla^D f$ but yields in the process also the derivatives
$\nabla^d f$ for $d \leq D$. As performance test we compute the gradient
$\nabla f(X)$ % and the Hessian $\nabla_A^2 f(A)$ by a combination of forward
and reverse mode of $f(X) = \trace (X^{-1})$ in the reverse mode of AD for $X
\in \mathbb R^{n \times n}$. We observe a speedup of about 100 compared to
UTPS. Due to the nature of the method, the memory footprint is also small and
therefore can be used to differentiate functions that are not accessible by
standard methods due to limited physical memory.
"
170,Shortest Two-way Linear Recurrences,"  Let $s$ be a finite sequence over a field of length $n$. It is well-known
that if $s$ satisfies a linear recurrence of order $d$ with non-zero constant
term, then the reverse of $s$ also satisfies a recurrence of order $d$ (with
coefficients in reverse order). A recent article of A. Salagean proposed an
algorithm to find such a shortest 'two-way' recurrence -- which may be longer
than a linear recurrence for $s$ of shortest length $\LC_n$.
  We give a new and simpler algorithm to compute a shortest two-way linear
recurrence. First we show that the pairs of polynomials we use to construct a
minimal polynomial iteratively are always relatively prime; we also give the
extended multipliers. Then we combine degree lower bounds with a
straightforward rewrite of a published algorithm due to the author to obtain
our simpler algorithm. The increase in shortest length is
$\max\{n+1-2\LC_n,0\}$.
"
171,Linear Solving for Sign Determination,"  We give a specific method to solve with quadratic complexity the linear
systems arising in known algorithms to deal with the sign determination
problem. In particular, this enable us to improve the complexity bound for sign
determination in the univariate case.
"
172,Completeness of the WDS method in Checking Positivity of Integral Forms,"  Examples show that integral forms can be efficiently proved positive
semidefinite by the WDS method, but it was unknown that how many steps of
substitutions are needed, or furthermore, which integral forms is this method
applicable for. In this paper, we give upper bounds of step numbers of WDS
required in proving that an integral form is positive definite, positive
semidefinite, or not positive semidefinite, thus deducing that the WDS method
is complete.
"
173,"Parallelizing Deadlock Resolution in Symbolic Synthesis of Distributed
  Programs","  Previous work has shown that there are two major complexity barriers in the
synthesis of fault-tolerant distributed programs: (1) generation of fault-span,
the set of states reachable in the presence of faults, and (2) resolving
deadlock states, from where the program has no outgoing transitions. Of these,
the former closely resembles with model checking and, hence, techniques for
efficient verification are directly applicable to it. Hence, we focus on
expediting the latter with the use of multi-core technology.
  We present two approaches for parallelization by considering different design
choices. The first approach is based on the computation of equivalence classes
of program transitions (called group computation) that are needed due to the
issue of distribution (i.e., inability of processes to atomically read and
write all program variables). We show that in most cases the speedup of this
approach is close to the ideal speedup and in some cases it is superlinear. The
second approach uses traditional technique of partitioning deadlock states
among multiple threads. However, our experiments show that the speedup for this
approach is small. Consequently, our analysis demonstrates that a simple
approach of parallelizing the group computation is likely to be the effective
method for using multi-core computing in the context of deadlock resolution.
"
174,Simplex Subdivisions and Nonnegativity Decision of Forms,"  This paper mainly studies nonnegativity decision of forms based on variable
substitutions. Unlike existing research, the paper regards simplex subdivisions
as new perspectives to study variable substitutions, gives some subdivisions of
the simplex T_n, introduces the concept of convergence of the subdivision
sequence, and presents a sufficient and necessary condition for the convergent
self-similar subdivision sequence. Then the relationships between subdivisions
and their corresponding substitutions are established. Moreover, it is proven
that if the form F is indefinite on T_n and the sequence of the successive
L-substitution sets is convergent, then the sequence of sets {SLS^(m)(F)} is
negatively terminating, and an algorithm for deciding indefinite forms with a
counter-example is obtained. Thus, various effective substitutions for deciding
positive semi-definite forms and indefinite forms are gained, which are beyond
the weighted difference substitutions characterized by ""difference"".
"
175,"The weighted difference substitutions and Nonnegativity Decision of
  Forms","  In this paper, we study the weighted difference substitutions from
geometrical views. First, we give the geometric meanings of the weighted
difference substitutions, and introduce the concept of convergence of the
sequence of substitution sets. Then it is proven that the sequence of the
successive weighted difference substitution sets is convergent. Based on the
convergence of the sequence of the successive weighted difference sets, a new,
simpler method to prove that if the form F is positive definite on T_n, then
the sequence of sets {SDS^m(F)} is positively terminating is presented, which
is different from the one given in [11]. That is, we can decide the
nonnegativity of a positive definite form by successively running the weighted
difference substitutions finite times. Finally, an algorithm for deciding an
indefinite form with a counter-example is obtained, and some examples are
listed by using the obtained algorithm.
"
176,"A Complete Method for Checking Hurwitz Stability of a Polytope of
  Matrices","  We present a novel method for checking the Hurwitz stability of a polytope of
matrices. First we prove that the polytope matrix is stable if and only if two
homogenous polynomials are positive on a simplex, then through a newly proposed
method, i.e., the weighted difference substitution method, the latter can be
checked in finite steps. Examples show the efficiency of our method.
"
177,A complete algorithm to find exact minimal polynomial by approximations,"  We present a complete algorithm for finding an exact minimal polynomial from
its approximate value by using an improved parameterized integer relation
construction method. Our result is superior to the existence of error
controlling on obtaining an exact rational number from its approximation. The
algorithm is applicable for finding exact minimal polynomial of an algebraic
number by its approximate root. This also enables us to provide an efficient
method of converting the rational approximation representation to the minimal
polynomial representation, and devise a simple algorithm to factor multivariate
polynomials with rational coefficients.
  Compared with the subsistent methods, our method combines advantage of high
efficiency in numerical computation, and exact, stable results in symbolic
computation. we also discuss some applications to some transcendental numbers
by approximations. Moreover, the Digits of our algorithm is far less than the
LLL-lattice basis reduction technique in theory. In this paper, we completely
implement how to obtain exact results by numerical approximate computations.
"
178,The Berlekamp-Massey Algorithm via Minimal Polynomials,"  We present a recursive minimal polynomial theorem for finite sequences over a
commutative integral domain $D$. This theorem is relative to any element of
$D$. The ingredients are: the arithmetic of Laurent polynomials over $D$, a
recursive 'index function' and simple mathematical induction. Taking
reciprocals gives a 'Berlekamp-Massey theorem' i.e. a recursive construction of
the polynomials arising in the Berlekamp-Massey algorithm, relative to any
element of $D$. The recursive theorem readily yields the iterative minimal
polynomial algorithm due to the author and a transparent derivation of the
iterative Berlekamp-Massey algorithm.
  We give an upper bound for the sum of the linear complexities of $s$ which is
tight if $s$ has a perfect linear complexity profile. This implies that over a
field, both iterative algorithms require at most $2\lfloor
\frac{n^2}{4}\rfloor$ multiplications.
"
179,"Parallel computation of real solving bivariate polynomial systems by
  zero-matching method","  We present a new algorithm for solving the real roots of a bivariate
polynomial system $\Sigma=\{f(x,y),g(x,y)\}$ with a finite number of solutions
by using a zero-matching method. The method is based on a lower bound for
bivariate polynomial system when the system is non-zero. Moreover, the
multiplicities of the roots of $\Sigma=0$ can be obtained by a given
neighborhood. From this approach, the parallelization of the method arises
naturally. By using a multidimensional matching method this principle can be
generalized to the multivariate equation systems.
"
180,"Gr\""obner Bases of Bihomogeneous Ideals generated by Polynomials of
  Bidegree (1,1): Algorithms and Complexity","  Solving multihomogeneous systems, as a wide range of structured algebraic
systems occurring frequently in practical problems, is of first importance.
Experimentally, solving these systems with Gr\""obner bases algorithms seems to
be easier than solving homogeneous systems of the same degree. Nevertheless,
the reasons of this behaviour are not clear. In this paper, we focus on
bilinear systems (i.e. bihomogeneous systems where all equations have bidegree
(1,1)). Our goal is to provide a theoretical explanation of the aforementionned
experimental behaviour and to propose new techniques to speed up the Gr\""obner
basis computations by using the multihomogeneous structure of those systems.
The contributions are theoretical and practical. First, we adapt the classical
F5 criterion to avoid reductions to zero which occur when the input is a set of
bilinear polynomials. We also prove an explicit form of the Hilbert series of
bihomogeneous ideals generated by generic bilinear polynomials and give a new
upper bound on the degree of regularity of generic affine bilinear systems.
This leads to new complexity bounds for solving bilinear systems. We propose
also a variant of the F5 Algorithm dedicated to multihomogeneous systems which
exploits a structural property of the Macaulay matrix which occurs on such
inputs. Experimental results show that this variant requires less time and
memory than the classical homogeneous F5 Algorithm.
"
181,Generic design of Chinese remaindering schemes,"  We propose a generic design for Chinese remainder algorithms. A Chinese
remainder computation consists in reconstructing an integer value from its
residues modulo non coprime integers. We also propose an efficient linear data
structure, a radix ladder, for the intermediate storage and computations. Our
design is structured into three main modules: a black box residue computation
in charge of computing each residue; a Chinese remaindering controller in
charge of launching the computation and of the termination decision; an integer
builder in charge of the reconstruction computation. We then show that this
design enables many different forms of Chinese remaindering (e.g.
deterministic, early terminated, distributed, etc.), easy comparisons between
these forms and e.g. user-transparent parallelism at different parallel grains.
"
182,"Proceedings Ninth International Workshop on Reduction Strategies in
  Rewriting and Programming","  This volume contains selected papers presented at the 9th International
Workshop on Reduction Strategies in Rewriting and Programming, WRS2009, which
was held in Brasilia on the 28th June 2009, associated to RTA 2009 (the 20th
International Conference on Rewriting Techniques and Applications) at RDP, the
Federated Conference on Rewriting, Deduction and Programming. Reduction
strategies define which (sub)expression(s) should be selected for evaluation
and which rule(s) should be applied. These choices affect fundamental
properties of reductions, such as completeness, laziness and efficiency in
general. The WRS workshops promote research and collaboration in the area of
reduction strategies and their applications in specification and programming,
theorem proving, software engineering, etc.
"
183,Some Algebraic Properties of a Subclass of Finite Normal Form Games,"  We study the problem of computing all Nash equilibria of a subclass of finite
normal form games. With algebraic characterization of the games, we present a
method for computing all its Nash equilibria. Further, we present a method for
deciding membership to the class of games with its related results. An
appendix, containing an example to show working of each of the presented
methods, concludes the work.
"
184,"An in-place truncated Fourier transform and applications to polynomial
  multiplication","  The truncated Fourier transform (TFT) was introduced by van der Hoeven in
2004 as a means of smoothing the ""jumps"" in running time of the ordinary FFT
algorithm that occur at power-of-two input sizes. However, the TFT still
introduces these jumps in memory usage. We describe in-place variants of the
forward and inverse TFT algorithms, achieving time complexity O(n log n) with
only O(1) auxiliary space. As an application, we extend the second author's
results on space-restricted FFT-based polynomial multiplication to polynomials
of arbitrary degree.
"
185,"Stability and Bifurcation Analysis of Coupled Fitzhugh-Nagumo
  Oscillators","  Neurons are the central biological objects in understanding how the brain
works. The famous Hodgkin-Huxley model, which describes how action potentials
of a neuron are initiated and propagated, consists of four coupled nonlinear
differential equations. Because these equations are difficult to deal with,
there also exist several simplified models, of which many exhibit
polynomial-like non-linearity. Examples of such models are the Fitzhugh-Nagumo
(FHN) model, the Hindmarsh-Rose (HR) model, the Morris-Lecar (ML) model and the
Izhikevich model. In this work, we first prescribe the biologically relevant
parameter ranges for the FHN model and subsequently study the dynamical
behaviour of coupled neurons on small networks of two or three nodes. To do
this, we use a computational real algebraic geometry method called the
Discriminant Variety (DV) method to perform the stability and bifurcation
analysis of these small networks. A time series analysis of the FHN model can
be found elsewhere in related work[15].
"
186,The Power of Vocabulary: The Case of Cyclotomic Polynomials,"  We observe that the vocabulary used to construct the ""answer"" to problems in
computer algebra can have a dramatic effect on the computational complexity of
solving that problem. We recall a formalization of this observation and explain
the classic example of sparse polynomial arithmetic. For this case, we show
that it is possible to extend the vocabulary so as reap the benefits of
conciseness whilst avoiding the obvious pitfall of repeating the problem
statement as the ""solution"".
  It is possible to extend the vocabulary either by irreducible cyclotomics or
by $x^n-1$: we look at the options and suggest that the pragmatist might opt
for both.
"
187,B\'{e}zout Identities Associated to a Finite Sequence,"  We consider finite sequences $s\in D^n$ where $D$ is a commutative, unital,
integral domain. We prove three sets of identities (possibly with repetitions),
each involving $2n$ polynomials associated to $s$. The right-hand side of these
identities is a recursively-defined (non-zero) 'product-of-discrepancies'.
There are implied iterative algorithms (of quadratic complexity) for the
left-hand side coefficients; when the ground domain is factorial, the
identities are in effect B\'ezout identities.
  We give a number of applications: an algorithm to compute B\'ezout
coefficients over a field; the outputs of the Berlekamp-Massey algorithm;
sequences with perfect linear complexity profile; annihilating polynomials
which do not vanish at zero and have minimal degree: we simplify and extend an
algorithm of Salagean to sequences over $D$. In the Appendix, we give a new
proof of a theorem of Imamura and Yoshida on the linear complexity of reverse
sequences, initially proved using Hankel matrices over a field and now valid
for sequences over a factorial domain.
"
188,"Gradual sub-lattice reduction and a new complexity for factoring
  polynomials","  We present a lattice algorithm specifically designed for some classical
applications of lattice reduction. The applications are for lattice bases with
a generalized knapsack-type structure, where the target vectors are boundably
short. For such applications, the complexity of the algorithm improves
traditional lattice reduction by replacing some dependence on the bit-length of
the input vectors by some dependence on the bound for the output vectors. If
the bit-length of the target vectors is unrelated to the bit-length of the
input, then our algorithm is only linear in the bit-length of the input
entries, which is an improvement over the quadratic complexity floating-point
LLL algorithms. To illustrate the usefulness of this algorithm we show that a
direct application to factoring univariate polynomials over the integers leads
to the first complexity bound improvement since 1984. A second application is
algebraic number reconstruction, where a new complexity bound is obtained as
well.
"
189,Fast Arithmetics in Artin-Schreier Towers over Finite Fields,"  An Artin-Schreier tower over the finite field F_p is a tower of field
extensions generated by polynomials of the form X^p - X - a. Following Cantor
and Couveignes, we give algorithms with quasi-linear time complexity for
arithmetic operations in such towers. As an application, we present an
implementation of Couveignes' algorithm for computing isogenies between
elliptic curves using the p-torsion.
"
190,"NumGfun: a Package for Numerical and Analytic Computation with D-finite
  Functions","  This article describes the implementation in the software package NumGfun of
classical algorithms that operate on solutions of linear differential equations
or recurrence relations with polynomial coefficients, including what seems to
be the first general implementation of the fast high-precision numerical
evaluation algorithms of Chudnovsky & Chudnovsky. In some cases, our
descriptions contain improvements over existing algorithms. We also provide
references to relevant ideas not currently used in NumGfun.
"
191,Factorization of Non-Commutative Polynomials,"  We describe an algorithm for the factorization of non-commutative polynomials
over a field. The first sketch of this algorithm appeared in an unpublished
manuscript (literally hand written notes) by James H. Davenport more than 20
years ago. This version of the algorithm contains some improvements with
respect to the original sketch. An improved version of the algorithm has been
fully implemented in the Axiom computer algebra system.
"
192,Proof of George Andrews's and David Robbins's q-TSPP Conjecture,"  The conjecture that the orbit-counting generating function for totally
symmetric plane partitions can be written as an explicit product formula, has
been stated independently by George Andrews and David Robbins around 1983. We
present a proof of this long-standing conjecture.
"
193,Triangular Decomposition of Semi-algebraic Systems,"  Regular chains and triangular decompositions are fundamental and
well-developed tools for describing the complex solutions of polynomial
systems. This paper proposes adaptations of these tools focusing on solutions
of the real analogue: semi-algebraic systems. We show that any such system can
be decomposed into finitely many {\em regular semi-algebraic systems}. We
propose two specifications of such a decomposition and present corresponding
algorithms. Under some assumptions, one type of decomposition can be computed
in singly exponential time w.r.t.\ the number of variables. We implement our
algorithms and the experimental results illustrate their effectiveness.
"
194,"Stable polynomial division and essential normality of graded Hilbert
  modules","  The purpose of this paper is to initiate a new attack on Arveson's resistant
conjecture, that all graded submodules of the $d$-shift Hilbert module $H^2$
are essentially normal. We introduce the stable division property for modules
(and ideals): a normed module $M$ over the ring of polynomials in $d$ variables
has the stable division property if it has a generating set $\{f_1, ..., f_k\}$
such that every $h \in M$ can be written as $h = \sum_i a_i f_i$ for some
polynomials $a_i$ such that $\sum \|a_i f_i\| \leq C\|h\|$. We show that
certain classes of modules have this property, and that the stable
decomposition $h = \sum a_i f_i$ may be obtained by carefully applying
techniques from computational algebra. We show that when the algebra of
polynomials in $d$ variables is given the natural $\ell^1$ norm, then every
ideal is linearly equivalent to an ideal that has the stable division property.
We then show that a module $M$ that has the stable division property (with
respect to the appropriate norm) is $p$-essentially normal for $p > \dim(M)$,
as conjectured by Douglas. This result is used to give a new, unified proof
that certain classes of graded submodules are essentially normal. Finally, we
reduce the problem of determining whether all graded submodules of the
$d$-shift Hilbert module are essentially normal, to the problem of determining
whether all ideals generated by quadratic scalar valued polynomials are
essentially normal.
"
195,"Stability Analysis of Linear Uncertain Systems via Checking Positivity
  of Forms on Simplices","  In this paper, we mainly study the robust stability of linear continuous
systems with parameter uncertainties, a more general kind of uncertainties for
system matrices is considered, i.e., entries of system matrices are rational
functions of uncertain parameters which are varying in intervals. we present a
method which can check the robust Hurwitz stability of such uncertain systems
in finite steps. Examples show the efficiency of our approach.
"
196,"Algorithms for Checking Rational Roots of $b$-functions and their
  Applications","  Bernstein-Sato polynomial of a hypersurface is an important object with
numerous applications. It is known, that it is complicated to obtain it
computationally, as a number of open questions and challenges indicate. In this
paper we propose a family of algorithms called \texttt{checkRoot} for optimized
check of whether a given rational number is a root of Bernstein-Sato polynomial
and the computations of its multiplicity. This algorithms are used in the new
approach to compute the whole global or local Bernstein-Sato polynomial and
$b$-function of a holonomic ideal with respect to weights. They are applied in
numerous situations, where there is a possibility to compute an upper bound for
the polynomial. Namely, it can be achieved by means of embedded resolution, for
topologically equivalent singularities or using the formula of A'Campo and
spectral numbers. We also present approaches to the logarithmic comparison
problem and the intersection homology D-module. Several applications are
presented as well as solutions to some challenges which were intractable with
the classical methods. One of the main applications consists of computing of a
stratification of affine space with the local $b$-function being constant on
each stratum. Notably, the algorithm we propose does not employ primary
decomposition. Also we apply our results for the computation of Bernstein-Sato
polynomials for varieties. The methods from this paper have been implemented in
{\sc Singular:Plural} as libraries {\tt dmod.lib} and {\tt bfun.lib}. All the
examples from the paper have been computed with this implementation.
"
197,"Computing diagonal form and Jacobson normal form of a matrix using
  Gr\""obner bases","  In this paper we present two algorithms for the computation of a diagonal
form of a matrix over non-commutative Euclidean domain over a field with the
help of Gr\""obner bases. This can be viewed as the pre-processing for the
computation of Jacobson normal form and also used for the computation of Smith
normal form in the commutative case. We propose a general framework for
handling, among other, operator algebras with rational coefficients. We employ
special ""polynomial"" strategy in Ore localizations of non-commutative
$G$-algebras and show its merits. In particular, for a given matrix $M$ we
provide an algorithm to compute $U,V$ and $D$ with fraction-free entries such
that $UMV=D$ holds. The polynomial approach allows one to obtain more precise
information, than the rational one e. g. about singularities of the system.
  Our implementation of polynomial strategy shows very impressive performance,
compared with methods, which directly use fractions. In particular, we
experience quite moderate swell of coefficients and obtain uncomplicated
transformation matrices. This shows that this method is well suitable for
solving nontrivial practical problems. We present an implementation of
algorithms in SINGULAR:PLURAL and compare it with other available systems. We
leave questions on the algorithmic complexity of this algorithm open, but we
stress the practical applicability of the proposed method to a bigger class of
non-commutative algebras.
"
198,A Modal Logic for Termgraph Rewriting,"  We propose a modal logic tailored to describe graph transformations and
discuss some of its properties. We focus on a particular class of graphs called
termgraphs. They are first-order terms augmented with sharing and cycles.
Termgraphs allow one to describe classical data-structures (possibly with
pointers) such as doubly-linked lists, circular lists etc. We show how the
proposed logic can faithfully describe (i) termgraphs as well as (ii) the
application of a termgraph rewrite rule (i.e. matching and replacement) and
(iii) the computation of normal forms with respect to a given rewrite system.
We also show how the proposed logic, which is more expressive than
propositional dynamic logic, can be used to specify shapes of classical
data-structures (e.g. binary trees, circular lists etc.).
"
199,Protocol indepedence through disjoint encryption under Exclusive-OR,"  Multi-protocol attacks due to protocol interaction has been a notorious
problem for security. Gutman-Thayer proved that they can be prevented by
ensuring that encrypted messages are distinguishable across protocols, under a
free algebra. In this paper, we prove that a similar suggestion prevents these
attacks under commonly used operators such as Exclusive-OR, that induce
equational theories, breaking the free algebra assumption.
"
200,"Disabling equational theories in unification for cryptographic protocol
  analysis through tagging","  In this paper, we show a new tagging scheme for cryptographic protocol
messages. Under this tagging, equational theories of operators such as
exclusive-or, binary addition etc. are effectively disabled, when terms are
unified. We believe that this result has a significant impact on protocol
analysis and security, since unification is at the heart of symbolic protocol
analysis. Hence, disabling equational theories in unification implies disabling
them altogether in protocol analysis for most operators and theories.
"
201,A New Proof for the Correctness of F5 (F5-Like) Algorithm,"  The famous F5 algorithm for computing Gr\""obner basis was presented by
Faug\`ere in 2002 without complete proofs for its correctness. The current
authors have simplified the original F5 algorithm into an F5 algorithm in
Buchberger's style (F5B algorithm), which is equivalent to original F5
algorithm and may deduce some F5-like versions. In this paper, the F5B
algorithm is briefly revisited and a new complete proof for the correctness of
F5B algorithm is proposed. This new proof is not limited to homogeneous systems
and does not depend on the strategy of selecting critical pairs (i.e. the
strategy deciding which critical pair is computed first) such that any strategy
could be utilized in F5B (F5) algorithm. From this new proof, we find that the
special reduction procedure (F5-reduction) is the key of F5 algorithm, so
maintaining this special reduction, various variation algorithms become
available. A natural variation of F5 algorithm, which transforms original F5
algorithm to a non-incremental algorithm, is presented and proved in this paper
as well. This natural variation has been implemented over the Boolean ring. The
two revised criteria in this natural variation are also able to reject almost
all unnecessary computations and few polynomials reduce to 0 in most examples.
"
202,Syntactic Abstraction of B Models to Generate Tests,"  In a model-based testing approach as well as for the verification of
properties, B models provide an interesting solution. However, for industrial
applications, the size of their state space often makes them hard to handle. To
reduce the amount of states, an abstraction function can be used, often
combining state variable elimination and domain abstractions of the remaining
variables. This paper complements previous results, based on domain abstraction
for test generation, by adding a preliminary syntactic abstraction phase, based
on variable elimination. We define a syntactic transformation that suppresses
some variables from a B event model, in addition to a method that chooses
relevant variables according to a test purpose. We propose two methods to
compute an abstraction A of an initial model M. The first one computes A as a
simulation of M, and the second one computes A as a bisimulation of M. The
abstraction process produces a finite state system. We apply this abstraction
computation to a Model Based Testing process.
"
203,On Computing Groebner Basis in the Rings of Differential Operators,"  Insa and Pauer presented a basic theory of Groebner basis for differential
operators with coefficients in a commutative ring in 1998, and a criterion was
proposed to determine if a set of differential operators is a Groebner basis.
In this paper, we will give a new criterion such that Insa and Pauer's
criterion could be concluded as a special case and one could compute the
Groebner basis more efficiently by this new criterion.
"
204,Exact linear modeling using Ore algebras,"  Linear exact modeling is a problem coming from system identification: Given a
set of observed trajectories, the goal is find a model (usually, a system of
partial differential and/or difference equations) that explains the data as
precisely as possible. The case of operators with constant coefficients is well
studied and known in the systems theoretic literature, whereas the operators
with varying coefficients were addressed only recently. This question can be
tackled either using Gr\""obner bases for modules over Ore algebras or by
following the ideas from differential algebra and computing in commutative
rings. In this paper, we present algorithmic methods to compute ""most powerful
unfalsified models"" (MPUM) and their counterparts with variable coefficients
(VMPUM) for polynomial and polynomial-exponential signals. We also study the
structural properties of the resulting models, discuss computer algebraic
techniques behind algorithms and provide several examples.
"
205,A Fast Approach to Creative Telescoping,"  In this note we reinvestigate the task of computing creative telescoping
relations in differential-difference operator algebras. Our approach is based
on an ansatz that explicitly includes the denominators of the delta parts. We
contribute several ideas of how to make an implementation of this approach
reasonably fast and provide such an implementation. A selection of examples
shows that it can be superior to existing methods by a large factor.
"
206,"Exact Sparse Matrix-Vector Multiplication on GPU's and Multicore
  Architectures","  We propose different implementations of the sparse matrix--dense vector
multiplication (\spmv{}) for finite fields and rings $\Zb/m\Zb$. We take
advantage of graphic card processors (GPU) and multi-core architectures. Our
aim is to improve the speed of \spmv{} in the \linbox library, and henceforth
the speed of its black box algorithms. Besides, we use this and a new
parallelization of the sigma-basis algorithm in a parallel block Wiedemann rank
implementation over finite fields.
"
207,"Multiplication of sparse Laurent polynomials and Poisson series on
  modern hardware architectures","  In this paper we present two algorithms for the multiplication of sparse
Laurent polynomials and Poisson series (the latter being algebraic structures
commonly arising in Celestial Mechanics from the application of perturbation
theories). Both algorithms first employ the Kronecker substitution technique to
reduce multivariate multiplication to univariate multiplication, and then use
the schoolbook method to perform the univariate multiplication. The first
algorithm, suitable for moderately-sparse multiplication, uses the exponents of
the monomials resulting from the univariate multiplication as trivial hash
values in a one dimensional lookup array of coefficients. The second algorithm,
suitable for highly-sparse multiplication, uses a cache-optimised hash table
which stores the coefficient-exponent pairs resulting from the multiplication
using the exponents as keys. Both algorithms have been implemented with
attention to modern computer hardware architectures. Particular care has been
devoted to the efficient exploitation of contemporary memory hierarchies
through cache-blocking techniques and cache-friendly term ordering. The first
algorithm has been parallelised for shared-memory multicore architectures,
whereas the second algorithm is in the process of being parallelised. We
present benchmarks comparing our algorithms to the routines of other computer
algebra systems, both in sequential and parallel mode.
"
208,Chunky and Equal-Spaced Polynomial Multiplication,"  Finding the product of two polynomials is an essential and basic problem in
computer algebra. While most previous results have focused on the worst-case
complexity, we instead employ the technique of adaptive analysis to give an
improvement in many ""easy"" cases. We present two adaptive measures and methods
for polynomial multiplication, and also show how to effectively combine them to
gain both advantages. One useful feature of these algorithms is that they
essentially provide a gradient between existing ""sparse"" and ""dense"" methods.
We prove that these approaches provide significant improvements in many cases
but in the worst case are still comparable to the fastest existing algorithms.
"
209,Formal Proof of SCHUR Conjugate Function,"  The main goal of our work is to formally prove the correctness of the key
commands of the SCHUR software, an interactive program for calculating with
characters of Lie groups and symmetric functions. The core of the computations
relies on enumeration and manipulation of combinatorial structures. As a first
""proof of concept"", we present a formal proof of the conjugate function,
written in C. This function computes the conjugate of an integer partition. To
formally prove this program, we use the Frama-C software. It allows us to
annotate C functions and to generate proof obligations, which are proved using
several automated theorem provers. In this paper, we also draw on methodology,
discussing on how to formally prove this kind of program.
"
210,"Nearly Optimal Algorithms for the Decomposition of Multivariate Rational
  Functions and the Extended L\""uroth's Theorem","  The extended L\""uroth's Theorem says that if the transcendence degree of
$\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)/\KK$ is 1 then there exists $f \in
\KK(\underline{X})$ such that $\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)$ is equal
to $\KK(f)$. In this paper we show how to compute $f$ with a probabilistic
algorithm. We also describe a probabilistic and a deterministic algorithm for
the decomposition of multivariate rational functions. The probabilistic
algorithms proposed in this paper are softly optimal when $n$ is fixed and $d$
tends to infinity. We also give an indecomposability test based on gcd
computations and Newton's polytope. In the last section, we show that we get a
polynomial time algorithm, with a minor modification in the exponential time
decomposition algorithm proposed by Gutierez-Rubio-Sevilla in 2001.
"
211,Some Results on the Functional Decomposition of Polynomials,"  If g and h are functions over some field, we can consider their composition f
= g(h). The inverse problem is decomposition: given f, determine the ex-
istence of such functions g and h. In this thesis we consider functional decom-
positions of univariate and multivariate polynomials, and rational functions
over a field F of characteristic p. In the polynomial case, ""wild"" behaviour
occurs in both the mathematical and computational theory of the problem if p
divides the degree of g. We consider the wild case in some depth, and deal with
those polynomials whose decompositions are in some sense the ""wildest"": the
additive polynomials. We determine the maximum number of decompositions and
show some polynomial time algorithms for certain classes of polynomials with
wild decompositions. For the rational function case we present a definition of
the problem, a normalised version of the problem to which the general problem
reduces, and an exponential time solution to the normal problem.
"
212,Symbolic Domain Decomposition,"  Decomposing the domain of a function into parts has many uses in mathematics.
A domain may naturally be a union of pieces, a function may be defined by
cases, or different boundary conditions may hold on different regions. For any
particular problem the domain can be given explicitly, but when dealing with a
family of problems given in terms of symbolic parameters, matters become more
difficult. This article shows how hybrid sets, that is multisets allowing
negative multiplicity, may be used to express symbolic domain decompositions in
an efficient, elegant and uniform way, simplifying both computation and
reasoning. We apply this theory to the arithmetic of piecewise functions and
symbolic matrices and show how certain operations may be reduced from
exponential to linear complexity.
"
213,When can we decide that a P-finite sequence is positive?,"  We consider two algorithms which can be used for proving positivity of
sequences that are defined by a linear recurrence equation with polynomial
coefficients (P-finite sequences). Both algorithms have in common that while
they do succeed on a great many examples, there is no guarantee for them to
terminate, and they do in fact not terminate for every input. For some
restricted classes of P-finite recurrence equations of order up to three we
provide a priori criteria that assert the termination of the algorithms.
"
214,Partial Denominator Bounds for Partial Linear Difference Equations,"  We investigate which polynomials can possibly occur as factors in the
denominators of rational solutions of a given partial linear difference
equation (PLDE). Two kinds of polynomials are to be distinguished, we call them
/periodic/ and /aperiodic/. The main result is a generalization of a well-known
denominator bounding technique for univariate equations to PLDEs. This
generalization is able to find all the aperiodic factors of the denominators
for a given PLDE.
"
215,Integrating multiple sources to answer questions in Algebraic Topology,"  We present in this paper an evolution of a tool from a user interface for a
concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to
a front-end allowing the interoperability among different sources for
computation and deduction. The architecture allows the system not only to
interface several systems, but also to make them cooperate in shared
calculations.
"
216,Generic design of Chinese remaindering schemes,"  We propose a generic design for Chinese remainder algorithms. A Chinese
remainder computation consists in reconstructing an integer value from its
residues modulo non coprime integers. We also propose an efficient linear data
structure, a radix ladder, for the intermediate storage and computations. Our
design is structured into three main modules: a black box residue computation
in charge of computing each residue; a Chinese remaindering controller in
charge of launching the computation and of the termination decision; an integer
builder in charge of the reconstruction computation. We then show that this
design enables many different forms of Chinese remaindering (e.g.
deterministic, early terminated, distributed, etc.), easy comparisons between
these forms and e.g. user-transparent parallelism at different parallel grains.
"
217,Polynomial integration on regions defined by a triangle and a conic,"  We present an efficient solution to the following problem, of relevance in a
numerical optimization scheme: calculation of integrals of the type \[\iint_{T
\cap \{f\ge0\}} \phi_1\phi_2 \, dx\,dy\] for quadratic polynomials
$f,\phi_1,\phi_2$ on a plane triangle $T$. The naive approach would involve
consideration of the many possible shapes of $T\cap\{f\geq0\}$ (possibly after
a convenient transformation) and parameterizing its border, in order to
integrate the variables separately. Our solution involves partitioning the
triangle into smaller triangles on which integration is much simpler.
"
218,Composition collisions and projective polynomials,"  The functional decomposition of polynomials has been a topic of great
interest and importance in pure and computer algebra and their applications.
The structure of compositions of (suitably normalized) polynomials f=g(h) over
finite fields is well understood in many cases, but quite poorly when the
degrees of both components are divisible by the characteristic p. This work
investigates the decomposition of polynomials whose degree is a power of p.
"
219,How to correctly prune tropical trees,"  We present tropical games, a generalization of combinatorial min-max games
based on tropical algebras. Our model breaks the traditional symmetry of
rational zero-sum games where players have exactly opposed goals (min vs. max),
is more widely applicable than min-max and also supports a form of pruning,
despite it being less effective than alpha-beta. Actually, min-max games may be
seen as particular cases where both the game and its dual are tropical: when
the dual of a tropical game is also tropical, the power of alpha-beta is
completely recovered. We formally develop the model and prove that the tropical
pruning strategy is correct, then conclude by showing how the problem of
approximated parsing can be modeled as a tropical game, profiting from pruning.
"
220,"Random polynomials and expected complexity of bisection methods for real
  solving","  Our probabilistic analysis sheds light to the following questions: Why do
random polynomials seem to have few, and well separated real roots, on the
average? Why do exact algorithms for real root isolation may perform
comparatively well or even better than numerical ones? We exploit results by
Kac, and by Edelman and Kostlan in order to estimate the real root separation
of degree $d$ polynomials with i.i.d.\ coefficients that follow two zero-mean
normal distributions: for SO(2) polynomials, the $i$-th coefficient has
variance ${d \choose i}$, whereas for Weyl polynomials its variance is
${1/i!}$. By applying results from statistical physics, we obtain the expected
(bit) complexity of \func{sturm} solver, $\sOB(r d^2 \tau)$, where $r$ is the
number of real roots and $\tau$ the maximum coefficient bitsize. Our bounds are
two orders of magnitude tighter than the record worst case ones. We also derive
an output-sensitive bound in the worst case. The second part of the paper shows
that the expected number of real roots of a degree $d$ polynomial in the
Bernstein basis is $\sqrt{2d}\pm\OO(1)$, when the coefficients are i.i.d.\
variables with moderate standard deviation. Our paper concludes with
experimental results which corroborate our analysis.
"
221,Constructive $D$-module Theory with \textsc{Singular},"  We overview numerous algorithms in computational $D$-module theory together
with the theoretical background as well as the implementation in the computer
algebra system \textsc{Singular}. We discuss new approaches to the computation
of Bernstein operators, of logarithmic annihilator of a polynomial, of
annihilators of rational functions as well as complex powers of polynomials. We
analyze algorithms for local Bernstein-Sato polynomials and also algorithms,
recovering any kind of Bernstein-Sato polynomial from partial knowledge of its
roots. We address a novel way to compute the Bernstein-Sato polynomial for an
affine variety algorithmically. All the carefully selected nontrivial examples,
which we present, have been computed with our implementation. We address such
applications as the computation of a zeta-function for certain integrals and
revealing the algebraic dependence between pairwise commuting elements.
"
222,Nominal Unification from a Higher-Order Perspective,"  Nominal Logic is a version of first-order logic with equality, name-binding,
renaming via name-swapping and freshness of names. Contrarily to higher-order
logic, bindable names, called atoms, and instantiable variables are considered
as distinct entities. Moreover, atoms are capturable by instantiations,
breaking a fundamental principle of lambda-calculus. Despite these differences,
nominal unification can be seen from a higher-order perspective. From this
view, we show that nominal unification can be reduced to a particular fragment
of higher-order unification problems: Higher-Order Pattern Unification. This
reduction proves that nominal unification can be decided in quadratic
deterministic time, using the linear algorithm for Higher-Order Pattern
Unification. We also prove that the translation preserves most generality of
unifiers.
"
223,"Holonomic Gradient Descent and its Application to Fisher-Bingham
  Integral","  We give a new algorithm to find local maximum and minimum of a holonomic
function and apply it for the Fisher-Bingham integral on the sphere $S^n$,
which is used in the directional statistics. The method utilizes the theory and
algorithms of holonomic systems.
"
224,"An Algebraic Approach for Computing Equilibria of a Subclass of Finite
  Normal Form Games","  A Nash equilibrium has become important solution concept for analyzing the
decision making in Game theory. In this paper, we consider the problem of
computing Nash equilibria of a subclass of generic finite normal form games. We
define ""rational payoff irrational equilibria games"" to be the games with all
rational payoffs and all irrational equilibria. We present a purely algebraic
method for computing all Nash equilibria of these games that uses knowledge of
Galois groups. Some results, showing properties of the class of games, and an
example to show working of the method concludes the paper.
"
225,The DMM bound: multivariate (aggregate) separation bounds,"  In this paper we derive aggregate separation bounds, named after
Davenport-Mahler-Mignotte (\dmm), on the isolated roots of polynomial systems,
specifically on the minimum distance between any two such roots. The bounds
exploit the structure of the system and the height of the sparse (or toric)
resultant by means of mixed volume, as well as recent advances on aggregate
root bounds for univariate polynomials, and are applicable to arbitrary
positive dimensional systems. We improve upon Canny's gap theorem
\cite{c-crmp-87} by a factor of $\OO(d^{n-1})$, where $d$ bounds the degree of
the polynomials, and $n$ is the number of variables. One application is to the
bitsize of the eigenvalues and eigenvectors of an integer matrix, which also
yields a new proof that the problem is polynomial. We also compare against
recent lower bounds on the absolute value of the root coordinates by Brownawell
and Yap \cite{by-issac-2009}, obtained under the hypothesis there is a
0-dimensional projection. Our bounds are in general comparable, but exploit
sparseness; they are also tighter when bounding the value of a positive
polynomial over the simplex. For this problem, we also improve upon the bounds
in \cite{bsr-arxix-2009,jp-arxiv-2009}. Our analysis provides a precise
asymptotic upper bound on the number of steps that subdivision-based algorithms
perform in order to isolate all real roots of a polynomial system. This leads
to the first complexity bound of Milne's algorithm \cite{Miln92} in 2D.
"
226,"The 1958 Pekeris-Accad-WEIZAC Ground-Breaking Collaboration that
  Computed Ground States of Two-Electron Atoms (and its 2010 Redux)","  In order to appreciate how well off we mathematicians and scientists are
today, with extremely fast hardware and lots and lots of memory, as well as
with powerful software, both for numeric and symbolic computation, it may be a
good idea to go back to the early days of electronic computers and compare how
things went then. We have chosen, as a case study, a problem that was
considered a huge challenge at the time. Namely, we looked at C.L. Pekeris's
seminal 1958 work on the ground state energies of two-electron atoms. We went
through all the computations ab initio with today's software and hardware, with
a special emphasis on the symbolic computations which in 1958 had to be made by
hand, and which nowadays can be automated and generalized.
"
227,Parallel versions of the symbolic manipulation system FORM,"  The symbolic manipulation program FORM is specialized to handle very large
algebraic expressions. Some specific features of its internal structure make
FORM very well suited for parallelization.
  We have now two parallel versions of FORM, one is based on POSIX threads and
is optimal for modern multicore computers while another one uses MPI and can be
used to parallelize FORM on clusters and Massive Parallel Processing systems.
Most existing FORM programs will be able to take advantage of the parallel
execution without the need for modifications.
"
228,"Sparse approaches for the exact distribution of patterns in long state
  sequences generated by a Markov source","  We present two novel approaches for the computation of the exact distribution
of a pattern in a long sequence. Both approaches take into account the sparse
structure of the problem and are two-part algorithms. The first approach relies
on a partial recursion after a fast computation of the second largest
eigenvalue of the transition matrix of a Markov chain embedding. The second
approach uses fast Taylor expansions of an exact bivariate rational
reconstruction of the distribution. We illustrate the interest of both
approaches on a simple toy-example and two biological applications: the
transcription factors of the Human Chromosome 5 and the PROSITE signatures of
functional motifs in proteins. On these example our methods demonstrate their
complementarity and their hability to extend the domain of feasibility for
exact computations in pattern problems to a new level.
"
229,FORM facts,"  Some of the new features of the symbolic manipulation system FORM are
discussed. Then some recent results running its multithreaded version TFORM are
shown. Finally the plans for the future are presented.
"
230,The F5 Algorithm in Buchberger's Style,"  The famous F5 algorithm for computing \gr basis was presented by Faug\`ere in
2002. The original version of F5 is given in programming codes, so it is a bit
difficult to understand. In this paper, the F5 algorithm is simplified as F5B
in a Buchberger's style such that it is easy to understand and implement. In
order to describe F5B, we introduce F5-reduction, which keeps the signature of
labeled polynomials unchanged after reduction. The equivalence between F5 and
F5B is also shown. At last, some versions of the F5 algorithm are illustrated.
"
231,The MAPLE package for calculating Poincar\'e series,"  We offer a Maple package {\tt Poincare\_Series} for calculating the
Poincar\'e series for the algebras of invariants/covariants of binary forms,
for the algebras of joint invariants/covariants of several binary forms, for
the kernel of Weitzenb\""ock derivations and for the multivariate Poincar\'e
series of algebras of joint invariants/covariants of several binary forms.
"
232,"A Unified Formal Description of Arithmetic and Set Theoretical Data
  Types","  We provide a ""shared axiomatization"" of natural numbers and hereditarily
finite sets built around a polymorphic abstraction of bijective base-2
arithmetics.
  The ""axiomatization"" is described as a progressive refinement of Haskell type
classes with examples of instances converging to an efficient implementation in
terms of arbitrary length integers and bit operations. As an instance, we
derive algorithms to perform arithmetic operations efficiently directly with
hereditarily finite sets.
  The self-contained source code of the paper is available at
http://logic.cse.unt.edu/tarau/research/2010/unified.hs .
"
233,"From matrix interpretations over the rationals to matrix interpretations
  over the naturals","  Matrix interpretations generalize linear polynomial interpretations and have
been proved useful in the implementation of tools for automatically proving
termination of Term Rewriting Systems. In view of the successful use of
rational coefficients in polynomial interpretations, we have recently
generalized traditional matrix interpretations (using natural numbers in the
matrix entries) to incorporate real numbers. However, existing results which
formally prove that polynomials over the reals are more powerful than
polynomials over the naturals for proving termination of rewrite systems failed
to be extended to matrix interpretations. In this paper we get deeper into this
problem. We show that, under some conditions, it is possible to transform a
matrix interpretation over the rationals satisfying a set of symbolic
constraints into a matrix interpretation over the naturals (using bigger
matrices) which still satisfies the constraints.
"
234,Model Counting in Product Configuration,"  We describe how to use propositional model counting for a quantitative
analysis of product configuration data. Our approach computes valuable meta
information such as the total number of valid configurations or the relative
frequency of components. This information can be used to assess the severity of
documentation errors or to measure documentation quality. As an application
example we show how we apply these methods to product documentation formulas of
the Mercedes-Benz line of vehicles. In order to process these large formulas we
developed and implemented a new model counter for non-CNF formulas. Our model
counter can process formulas, whose CNF representations could not be processed
up till now.
"
235,"GPGCD, an Iterative Method for Calculating Approximate GCD of Univariate
  Polynomials, with the Complex Coefficients","  We present an extension of our GPGCD method, an iterative method for
calculating approximate greatest common divisor (GCD) of univariate
polynomials, to polynomials with the complex coefficients. For a given pair of
polynomials and a degree, our algorithm finds a pair of polynomials which has a
GCD of the given degree and whose coefficients are perturbed from those in the
original inputs, making the perturbations as small as possible, along with the
GCD. In our GPGCD method, the problem of approximate GCD is transfered to a
constrained minimization problem, then solved with a so-called modified Newton
method, which is a generalization of the gradient-projection method, by
searching the solution iteratively. While our original method is designed for
polynomials with the real coefficients, we extend it to accept polynomials with
the complex coefficients in this paper.
"
236,"GPGCD, an Iterative Method for Calculating Approximate GCD, for Multiple
  Univariate Polynomials","  We present an extension of our GPGCD method, an iterative method for
calculating approximate greatest common divisor (GCD) of univariate
polynomials, to multiple polynomial inputs. For a given pair of polynomials and
a degree, our algorithm finds a pair of polynomials which has a GCD of the
given degree and whose coefficients are perturbed from those in the original
inputs, making the perturbations as small as possible, along with the GCD. In
our GPGCD method, the problem of approximate GCD is transferred to a
constrained minimization problem, then solved with the so-called modified
Newton method, which is a generalization of the gradient-projection method, by
searching the solution iteratively. In this paper, we extend our method to
accept more than two polynomials with the real coefficients as an input.
"
237,"Strassen's Matrix Multiplication Algorithm for Matrices of Arbitrary
  Order","  The well known algorithm of Volker Strassen for matrix multiplication can
only be used for $(m2^k \times m2^k)$ matrices. For arbitrary $(n \times n)$
matrices one has to add zero rows and columns to the given matrices to use
Strassen's algorithm. Strassen gave a strategy of how to set $m$ and $k$ for
arbitrary $n$ to ensure $n\leq m2^k$. In this paper we study the number $d$ of
additional zero rows and columns and the influence on the number of flops used
by the algorithm in the worst case ($d=n/16$), best case ($d=1$) and in the
average case ($d\approx n/48$). The aim of this work is to give a detailed
analysis of the number of additional zero rows and columns and the additional
work caused by Strassen's bad parameters. Strassen used the parameters $m$ and
$k$ to show that his matrix multiplication algorithm needs less than
$4.7n^{\log_2 7}$ flops. We can show in this paper, that these parameters cause
an additional work of approx. 20 % in the worst case in comparison to the
optimal strategy for the worst case. This is the main reason for the search for
better parameters.
"
238,"Connecting Gr\""obner Bases Programs with Coq to do Proofs in Algebra,
  Geometry and Arithmetics","  We describe how we connected three programs that compute Groebner bases to
Coq, to do automated proofs on algebraic, geometrical and arithmetical
expressions. The result is a set of Coq tactics and a certificate mechanism
(downloadable at http://www-sop.inria.fr/marelle/Loic.Pottier/gb-keappa.tgz).
The programs are: F4, GB \, and gbcoq. F4 and GB are the fastest (up to our
knowledge) available programs that compute Groebner bases. Gbcoq is slow in
general but is proved to be correct (in Coq), and we adapted it to our specific
problem to be efficient. The automated proofs concern equalities and
non-equalities on polynomials with coefficients and indeterminates in R or Z,
and are done by reducing to Groebner computation, via Hilbert's
Nullstellensatz. We adapted also the results of Harrison, to allow to prove
some theorems about modular arithmetics. The connection between Coq and the
programs that compute Groebner bases is done using the ""external"" tactic of Coq
that allows to call arbitrary programs accepting xml inputs and outputs. We
also produce certificates in order to make the proof scripts independant from
the external programs.
"
239,Open Graphs and Computational Reasoning,"  We present a form of algebraic reasoning for computational objects which are
expressed as graphs. Edges describe the flow of data between primitive
operations which are represented by vertices. These graphs have an interface
made of half-edges (edges which are drawn with an unconnected end) and enjoy
rich compositional principles by connecting graphs along these half-edges. In
particular, this allows equations and rewrite rules to be specified between
graphs. Particular computational models can then be encoded as an axiomatic set
of such rules. Further rules can be derived graphically and rewriting can be
used to simulate the dynamics of a computational system, e.g. evaluating a
program on an input. Examples of models which can be formalised in this way
include traditional electronic circuits as well as recent categorical accounts
of quantum information.
"
240,"Symmetric Determinantal Representation of Formulas and Weakly Skew
  Circuits","  We deploy algebraic complexity theoretic techniques for constructing
symmetric determinantal representations of for00504925mulas and weakly skew
circuits. Our representations produce matrices of much smaller dimensions than
those given in the convex geometry literature when applied to polynomials
having a concise representation (as a sum of monomials, or more generally as an
arithmetic formula or a weakly skew circuit). These representations are valid
in any field of characteristic different from 2. In characteristic 2 we are led
to an almost complete solution to a question of B\""urgisser on the
VNP-completeness of the partial permanent. In particular, we show that the
partial permanent cannot be VNP-complete in a finite field of characteristic 2
unless the polynomial hierarchy collapses.
"
241,Dominance in the family of Sugeno-Weber t-norms,"  The dominance relationship between two members of the family of Sugeno Weber
t-norms is proven by using a quantifer elimination algorithm. Further it is
shown that dominance is a transitive, and therefore also an order relation, on
this family of t-norms.
"
242,Bit-size estimates for triangular sets in positive dimension,"  We give bit-size estimates for the coefficients appearing in triangular sets
describing positive-dimensional algebraic sets defined over Q. These estimates
are worst case upper bounds; they depend only on the degree and height of the
underlying algebraic sets. We illustrate the use of these results in the
context of a modular algorithm. This extends results by the first and last
author, which were confined to the case of dimension 0. Our strategy is to get
back to dimension 0 by evaluation and inter- polation techniques. Even though
the main tool (height theory) remains the same, new difficulties arise to
control the growth of the coefficients during the interpolation process.
"
243,Exact Bivariate Polynomial Factorization in Q by Approximation of Roots,"  Factorization of polynomials is one of the foundations of symbolic
computation. Its applications arise in numerous branches of mathematics and
other sciences. However, the present advanced programming languages such as C++
and J++, do not support symbolic computation directly. Hence, it leads to
difficulties in applying factorization in engineering fields. In this paper, we
present an algorithm which use numerical method to obtain exact factors of a
bivariate polynomial with rational coefficients. Our method can be directly
implemented in efficient programming language such C++ together with the GNU
Multiple-Precision Library. In addition, the numerical computation part often
only requires double precision and is easily parallelizable.
"
244,"Pattern Classification In Symbolic Streams via Semantic Annihilation of
  Information","  We propose a technique for pattern classification in symbolic streams via
selective erasure of observed symbols, in cases where the patterns of interest
are represented as Probabilistic Finite State Automata (PFSA). We define an
additive abelian group for a slightly restricted subset of probabilistic finite
state automata (PFSA), and the group sum is used to formulate pattern-specific
semantic annihilators. The annihilators attempt to identify pre-specified
patterns via removal of essentially all inter-symbol correlations from observed
sequences, thereby turning them into symbolic white noise. Thus a perfect
annihilation corresponds to a perfect pattern match. This approach of
classification via information annihilation is shown to be strictly
advantageous, with theoretical guarantees, for a large class of PFSA models.
The results are supported by simulation experiments.
"
245,Quartic Curves and Their Bitangents,"  A smooth quartic curve in the complex projective plane has 36 inequivalent
representations as a symmetric determinant of linear forms and 63
representations as a sum of three squares. These correspond to Cayley octads
and Steiner complexes respectively. We present exact algorithms for computing
these objects from the 28 bitangents. This expresses Vinnikov quartics as
spectrahedra and positive quartics as Gram matrices. We explore the geometry of
Gram spectrahedra and we find equations for the variety of Cayley octads.
Interwoven is an exposition of much of the 19th century theory of plane
quartics.
"
246,Towards Solving the Inverse Protein Folding Problem,"  Accurately assigning folds for divergent protein sequences is a major
obstacle to structural studies and underlies the inverse protein folding
problem. Herein, we outline our theories for fold-recognition in the
""twilight-zone"" of sequence similarity (<25% identity). Our analyses
demonstrate that structural sequence profiles built using Position-Specific
Scoring Matrices (PSSMs) significantly outperform multiple popular
homology-modeling algorithms for relating and predicting structures given only
their amino acid sequences. Importantly, structural sequence profiles
reconstitute SCOP fold classifications in control and test datasets. Results
from our experiments suggest that structural sequence profiles can be used to
rapidly annotate protein folds at proteomic scales. We propose that encoding
the entire Protein DataBank (~1070 folds) into structural sequence profiles
would extract interoperable information capable of improving most if not all
methods of structural modeling.
"
247,"A Geometric Index Reduction Method for Implicit Systems of Differential
  Algebraic Equations","  This paper deals with the index reduction problem for the class of
quasi-regular DAE systems. It is shown that any of these systems can be
transformed to a generically equivalent first order DAE system consisting of a
single purely algebraic (polynomial) equation plus an under-determined ODE
(that is, a semi-explicit DAE system of differentiation index 1) in as many
variables as the order of the input system. This can be done by means of a
Kronecker-type algorithm with bounded complexity.
"
248,"Intersection Theory in Differential Algebraic Geometry: Generic
  Intersections and the Differential Chow Form","  In this paper, an intersection theory for generic differential polynomials is
presented. The intersection of an irreducible differential variety of dimension
$d$ and order $h$ with a generic differential hypersurface of order $s$ is
shown to be an irreducible variety of dimension $d-1$ and order $h+s$. As a
consequence, the dimension conjecture for generic differential polynomials is
proved. Based on the intersection theory, the Chow form for an irreducible
differential variety is defined and most of the properties of the Chow form in
the algebraic case are established for its differential counterpart.
Furthermore, the generalized differential Chow form is defined and its
properties are proved. As an application of the generalized differential Chow
form, the differential resultant of $n+1$ generic differential polynomials in
$n$ variables is defined and properties similar to that of the Macaulay
resultant for multivariate polynomials are proved.
"
249,System Description: H-PILoT (Version 1.9),"  This system description provides an overview of H-PILoT (Hierarchical Proving
by Instantiation in Local Theory extensions), a program for hierarchical
reasoning in extensions of logical theories. H-PILoT reduces deduction problems
in the theory extension to deduction problems in the base theory. Specialized
provers and standard SMT solvers can be used for testing the satisfiability of
the formulae obtained after the reduction. For a certain type of theory
extension (namely for local theory extensions) this hierarchical reduction is
sound and complete and -- if the formulae obtained this way belong to a
fragment decidable in the base theory -- H-PILoT provides a decision procedure
for testing satisfiability of ground formulae, and can also be used for model
generation.
"
250,"LinBox founding scope allocation, parallel building blocks, and separate
  compilation","  To maximize efficiency in time and space, allocations and deallocations, in
the exact linear algebra library \linbox, must always occur in the founding
scope. This provides a simple lightweight allocation model. We present this
model and its usage for the rebinding of matrices between different coefficient
domains. We also present automatic tools to speed-up the compilation of
template libraries and a software abstraction layer for the introduction of
transparent parallelism at the algorithmic level.
"
251,"Computation of Darboux polynomials and rational first integrals with
  bounded degree in polynomial time","  In this paper we study planar polynomial differential systems of this form:
dX/dt=A(X, Y), dY/dt= B(X, Y), where A,B belongs to Z[X, Y], degA \leq d, degB
\leq d, and the height of A and B is smaller than H. A lot of properties of
planar polynomial differential systems are related to irreducible Darboux
polynomials of the corresponding derivation: D =A(X, Y)dX + B(X, Y)dY . Darboux
polynomials are usually computed with the method of undetermined coefficients.
With this method we have to solve a polynomial system. We show that this
approach can give rise to the computation of an exponential number of reducible
Darboux polynomials. Here we show that the Lagutinskii-Pereira's algorithm
computes irreducible Darboux polynomials with degree smaller than N, with a
polynomial number, relatively to d, log(H) and N, binary operations. We also
give a polynomial-time method to compute, if it exists, a rational first
integral with bounded degree.
"
252,Computing sparse multiples of polynomials,"  We consider the problem of finding a sparse multiple of a polynomial. Given f
in F[x] of degree d over a field F, and a desired sparsity t, our goal is to
determine if there exists a multiple h in F[x] of f such that h has at most t
non-zero terms, and if so, to find such an h. When F=Q and t is constant, we
give a polynomial-time algorithm in d and the size of coefficients in h. When F
is a finite field, we show that the problem is at least as hard as determining
the multiplicative order of elements in an extension field of F (a problem
thought to have complexity similar to that of factoring integers), and this
lower bound is tight when t=2.
"
253,"Proceedings Fourth International Workshop on Testing, Analysis and
  Verification of Web Software","  This volume contains the papers presented at the fourth international
workshop on Testing, Analysis and Verification of Software, which was
associated with the 25th IEEE/ACM International Conference on Automated
Software Engineering (ASE 2010). The collection of papers includes research on
formal specification, model-checking, testing, and debugging of Web software.
"
254,"An Elimination Method for Solving Bivariate Polynomial Systems:
  Eliminating the Usual Drawbacks","  We present an exact and complete algorithm to isolate the real solutions of a
zero-dimensional bivariate polynomial system. The proposed algorithm
constitutes an elimination method which improves upon existing approaches in a
number of points. First, the amount of purely symbolic operations is
significantly reduced, that is, only resultant computation and square-free
factorization is still needed. Second, our algorithm neither assumes generic
position of the input system nor demands for any change of the coordinate
system. The latter is due to a novel inclusion predicate to certify that a
certain region is isolating for a solution. Our implementation exploits
graphics hardware to expedite the resultant computation. Furthermore, we
integrate a number of filtering techniques to improve the overall performance.
Efficiency of the proposed method is proven by a comparison of our
implementation with two state-of-the-art implementations, that is, LPG and
Maple's isolate. For a series of challenging benchmark instances, experiments
show that our implementation outperforms both contestants.
"
255,"Improved complexity bounds for real root isolation using Continued
  Fractions","  We consider the problem of isolating the real roots of a square-free
polynomial with integer coefficients using (variants of) the continued fraction
algorithm (CF). We introduce a novel way to compute a lower bound on the
positive real roots of univariate polynomials. This allows us to derive a worst
case bound of $\sOB(d^6 + d^4\tau^2 + d^3\tau^2)$ for isolating the real roots
of a polynomial with integer coefficients using the classic variant of CF,
where $d$ is the degree of the polynomial and $\tau$ the maximum bitsize of its
coefficients. This improves the previous bound by Sharma \cite{sharma-tcs-2008}
by a factor of $d^3$ and matches the bound derived by Mehlhorn and Ray
\cite{mr-jsc-2009} for another variant of CF; it also matches the worst case
bound of the subdivision-based solvers. We present a new variant of CF, we call
it iCF, that isolates the real roots of a polynomial with integer coefficients
in $\sOB(d^5+d^4\tau)$, thus improving the current known bound for the problem
by a factor of $d$. If the polynomial has only real roots, then our bound
becomes $\sOB(d^4+d^3\tau+ d^2\tau^2)$, thus matching the bound of the
numerical algorithms by Reif \cite{r-focs-1993} and by Ben-Or and Tiwari
\cite{bt-joc-1990}. Actually the latter bound holds in a more general setting,
that is under the rather mild assumption that $\Omega(d/\lg^c{d})$, where
$c\geq 0$ is a constant, roots contribute to the sign variations of the
coefficient list of the polynomial. This is the only bound on exact algorithms
that matches the one of the numerical algorithms by Pan \cite{Pan02jsc} and
Sch\""onhage \cite{Sch82}. To our knowledge the presented bounds are the best
known for the problem of real root isolation for algorithms based on exact
computations.
"
256,Detecting Simultaneous Integer Relations for Several Real Vectors,"  An algorithm which either finds an nonzero integer vector ${\mathbf m}$ for
given $t$ real $n$-dimensional vectors ${\mathbf x}_1,...,{\mathbf x}_t$ such
that ${\mathbf x}_i^T{\mathbf m}=0$ or proves that no such integer vector with
norm less than a given bound exists is presented in this paper. The cost of the
algorithm is at most ${\mathcal O}(n^4 + n^3 \log \lambda(X))$ exact arithmetic
operations in dimension $n$ and the least Euclidean norm $\lambda(X)$ of such
integer vectors. It matches the best complexity upper bound known for this
problem. Experimental data show that the algorithm is better than an already
existing algorithm in the literature. In application, the algorithm is used to
get a complete method for finding the minimal polynomial of an unknown complex
algebraic number from its approximation, which runs even faster than the
corresponding \emph{Maple} built-in function.
"
257,"Improved complexity bounds for real root isolation using Continued
  Fractions","  We consider the problem of isolating the real roots of a square-free
polynomial with integer coefficients using (variants of) the continued fraction
algorithm (CF). We introduce a novel way to compute a lower bound on the
positive real roots of univariate polynomials. This allows us to derive a worst
case bound of $\sOB(d^6 + d^4\tau^2 + d^3\tau^2)$ for isolating the real roots
of a polynomial with integer coefficients using the classic variant
\cite{Akritas:implementation} of CF, where $d$ is the degree of the polynomial
and $\tau$ the maximum bitsize of its coefficients. This improves the previous
bound of Sharma \cite{sharma-tcs-2008} by a factor of $d^3$ and matches the
bound derived by Mehlhorn and Ray \cite{mr-jsc-2009} for another variant of CF;
it also matches the worst case bound of the subdivision-based solvers.
"
258,"Fast Gr\""obner Basis Computation for Boolean Polynomials","  We introduce the Macaulay2 package BooleanGB, which computes a Gr\""obner
basis for Boolean polynomials using a binary representation rather than
symbolic. We compare the runtime of several Boolean models from systems in
biology and give an application to Sudoku.
"
259,The assembly modes of rigid 11-bar linkages,"  Designing an m-bar linkage with a maximal number of assembly modes is
important in robot kinematics, and has further applications in structural
biology and computational geometry. A related question concerns the number of
assembly modes of rigid mechanisms as a function of their nodes n, which is
uniquely defined given m. Rigid 11-bar linkages, where n=7, are the simplest
planar linkages for which these questions were still open. It will be proven
that the maximal number of assembly modes of such linkages is exactly 56. The
rigidity of a linkage is captured by a polynomial system derived from distance,
or Cayley-Menger, matrices. The upper bound on the number of assembly modes is
obtained as the mixed volume of a 5x5 system. An 11-bar linkage admitting 56
configurations is constructed using stochastic optimisation methods. This
yields a general lower bound of $\Omega(2.3^n)$ on the number of assembly
modes, slightly improving the current record of $\Omega(2.289^n)$, while the
best known upper bound is roughly $4^n$. Our methods are straightforward and
have been implemented in Maple. They are described in general terms
illustrating the fact that they can be readily extended to other planar or
spatial linkages. The main results have been reported in conference publication
[EM11]. This version (2017) typesets correctly the last Figure 5 so as to
include all 28 configurations modulo reflection.
"
260,On the Complexity of Real Root Isolation,"  We introduce a new approach to isolate the real roots of a square-free
polynomial $F=\sum_{i=0}^n A_i x^i$ with real coefficients. It is assumed that
each coefficient of $F$ can be approximated to any specified error bound. The
presented method is exact, complete and deterministic. Due to its similarities
to the Descartes method, we also consider it practical and easy to implement.
Compared to previous approaches, our new method achieves a significantly better
bit complexity. It is further shown that the hardness of isolating the real
roots of $F$ is exclusively determined by the geometry of the roots and not by
the complexity or the size of the coefficients. For the special case where $F$
has integer coefficients of maximal bitsize $\tau$, our bound on the bit
complexity writes as $\tilde{O}(n^3\tau^2)$ which improves the best bounds
known for existing practical algorithms by a factor of $n=deg F$. The crucial
idea underlying the new approach is to run an approximate version of the
Descartes method, where, in each subdivision step, we only consider
approximations of the intermediate results to a certain precision. We give an
upper bound on the maximal precision that is needed for isolating the roots of
$F$. For integer polynomials, this bound is by a factor $n$ lower than that of
the precision needed when using exact arithmetic explaining the improved bound
on the bit complexity.
"
261,An interface between physics and number theory,"  We extend the Hopf algebra description of a simple quantum system given
previously, to a more elaborate Hopf algebra, which is rich enough to encompass
that related to a description of perturbative quantum field theory (pQFT). This
provides a {\em mathematical} route from an algebraic description of
non-relativistic, non-field theoretic quantum statistical mechanics to one of
relativistic quantum field theory. Such a description necessarily involves
treating the algebra of polyzeta functions, extensions of the Riemann Zeta
function, since these occur naturally in pQFT. This provides a link between
physics, algebra and number theory. As a by-product of this approach, we are
led to indicate {\it inter alia} a basis for concluding that the Euler gamma
constant $\gamma$ may be rational.
"
262,"A recombination algorithm for the decomposition of multivariate rational
  functions","  In this paper we show how we can compute in a deterministic way the
decomposition of a multivariate rational function with a recombination
strategy. The key point of our recombination strategy is the used of Darboux
polynomials. We study the complexity of this strategy and we show that this
method improves the previous ones. In appendix, we explain how the strategy
proposed recently by J. Berthomieu and G. Lecerf for the sparse factorization
can be used in the decomposition setting. Then we deduce a decomposition
algorithm in the sparse bivariate case and we give its complexity
"
263,"The $z$-Transform and Automata-Recognizable Systems of Nonhomogeneous
  Linear Recurrence Equations over Semirings","  A nonhomogeneous system of linear recurrence equations can be recognized by
an automaton $\mathcal{A}$ over a one-letter alphabet $A = \{z\}$. Conversely,
the automaton $\mathcal{A}$ generates precisely this nonhomogeneous system of
linear recurrence equations. We present the solutions of these systems and
apply the $z$-transform to these solutions to obtain their series
representation. Finally, we show some results that simplify the series
representation of the $z$-transform of these solutions. We consider single
systems as well as the composition of two systems.
"
264,"Zero Decomposition with Multiplicity of Zero-Dimensional Polynomial
  Systems","  We present a zero decomposition theorem and an algorithm based on Wu's
method, which computes a zero decomposition with multiplicity for a given
zero-dimensional polynomial system. If the system satisfies some condition, the
zero decomposition is of triangular form.
"
265,Group extensions over infinite words,"  We construct an extension $E(A,G)$ of a given group $G$ by infinite
non-Archimedean words over an discretely ordered abelian group like $Z^n$. This
yields an effective and uniform method to study various groups that ""behave
like $G$"". We show that the Word Problem for f.g. subgroups in the extension is
decidable if and only if and only if the Cyclic Membership Problem in $G$ is
decidable.
  The present paper embeds the partial monoid of infinite words as defined by
Myasnikov, Remeslennikov, and Serbin (Contemp. Math., Amer. Math. Soc.,
378:37-77, 2005) into $E(A,G)$. Moreover, we define the extension group
$E(A,G)$ for arbitrary groups $G$ and not only for free groups as done in
previous work. We show some structural results about the group (existence and
type of torsion elements, generation by elements of order 2) and we show that
some interesting HNN extensions of $G$ embed naturally in the larger group
$E(A,G)$.
"
266,An algorithm for determining copositive matrices,"  In this paper, we present an algorithm of simple exponential growth called
COPOMATRIX for determining the copositivity of a real symmetric matrix. The
core of this algorithm is a decomposition theorem, which is used to deal with
simplicial subdivision of $\hat{T}^{-}=\{y\in \Delta_{m}| \beta^Ty\leq 0\}$ on
the standard simplex $\Delta_m$, where each component of the vector $\beta$ is
-1, 0 or 1.
"
267,"A New Algorithm for Inverting General Cyclic Heptadiagonal Matrices
  Recursively","  In this paper, we describe a reliable symbolic computational algorithm for
inverting general cyclic heptadiagonal matrices by using parallel computing
along with recursion. The algorithm is implementable to the Computer Algebra
System(CAS) such as MAPLE, Matlab and Mathematica . An example is presented for
the sake of illustration.
"
268,A Symbolic Summation Approach to Feynman Integral Calculus,"  Given a Feynman parameter integral, depending on a single discrete variable
$N$ and a real parameter $\epsilon$, we discuss a new algorithmic framework to
compute the first coefficients of its Laurent series expansion in $\epsilon$.
In a first step, the integrals are expressed by hypergeometric multi-sums by
means of symbolic transformations. Given this sum format, we develop new
summation tools to extract the first coefficients of its series expansion
whenever they are expressible in terms of indefinite nested product-sum
expressions. In particular, we enhance the known multi-sum algorithms to derive
recurrences for sums with complicated boundary conditions, and we present new
algorithms to find formal Laurent series solutions of a given recurrence
relation.
"
269,"On the Inverse Of General Cyclic Heptadiagonal and Anti-Heptadiagonal
  Matrices","  In the current work, the author present a symbolic algorithm for finding the
determinant of any general nonsingular cyclic heptadiagonal matrices and
inverse of anti-cyclic heptadiagonal matrices. The algorithms are mainly based
on the work presented in [A. A. KARAWIA, A New Algorithm for Inverting General
Cyclic Heptadiagonal Matrices Recursively, arXiv:1011.2306v1 [cs.SC]]. The
symbolic algorithms are suited for implementation using Computer Algebra
Systems (CAS) such as MATLAB, MAPLE and MATHEMATICA. An illustrative example is
given.
"
270,"On Functional Decomposition of Multivariate Polynomials with
  Differentiation and Homogenization","  In this paper, we give a theoretical analysis for the algorithms to compute
functional decomposition for multivariate polynomials based on differentiation
and homogenization which are proposed by Ye, Dai, Lam (1999) and Faug$\mu$ere,
Perret (2006, 2008, 2009). We show that a degree proper functional
decomposition for a set of randomly decomposable quartic homogenous polynomials
can be computed using the algorithm with high probability. This solves a
conjecture proposed by Ye, Dai, and Lam (1999). We also propose a conjecture
such that the decomposition for a set of polynomials can be computed from that
of its homogenization with high probability. Finally, we prove that the right
decomposition factors for a set of polynomials can be computed from its right
decomposition factor space. Combining these results together, we prove that the
algorithm can compute a degree proper decomposition for a set of randomly
decomposable quartic polynomials with probability one when the base field is of
characteristic zero, and with probability close to one when the base field is a
finite field with sufficiently large number under the assumption that the
conjeture is correct.
"
271,"Efficient Characteristic Set Algorithms for Equation Solving in Finite
  Fields and Applications in Cryptanalysis","  Efficient characteristic set methods for computing solutions of polynomial
equation systems in a finite field are proposed. The concept of proper
triangular sets is introduced and an explicit formula for the number of
solutions of a proper and monic (or regular) triangular set is given. An
improved zero decomposition algorithm which can be used to reduce the zero set
of an equation system in general form to the union of zero sets of monic proper
triangular sets is proposed. As a consequence, we can give an explicit formula
for the number of solutions of an equation system. Bitsize complexity for the
algorithm is given in the case of Boolean polynomials. We also give a
multiplication free characteristic set method for Boolean polynomials, where
the sizes of the polynomials are effectively controlled. The algorithms are
implemented in the case of Boolean polynomials and extensive experiments show
that they are quite efficient for solving certain classes of Boolean equations.
"
272,Isomorphisms of Algebraic Number Fields,"  Let $\mathbb{Q}(\alpha)$ and $\mathbb{Q}(\beta)$ be algebraic number fields.
We describe a new method to find (if they exist) all isomorphisms,
$\mathbb{Q}(\beta) \rightarrow \mathbb{Q}(\alpha)$. The algorithm is
particularly efficient if the number of isomorphisms is one.
"
273,How to refine polynomial functions,"  Research on refinable functions in wavelet theory is mostly focused to
localized functions. However it is known, that polynomial functions are
refinable, too. In our paper we investigate on conversions between refinement
masks and polynomials and their uniqueness.
"
274,Th\'eorie de Galois effective : aide m\'emoire,"  This paper collects many results on galoisian ideals and Galois theory.
"
275,Unification modulo a partial theory of exponentiation,"  Modular exponentiation is a common mathematical operation in modern
cryptography. This, along with modular multiplication at the base and exponent
levels (to different moduli) plays an important role in a large number of key
agreement protocols. In our earlier work, we gave many decidability as well as
undecidability results for multiple equational theories, involving various
properties of modular exponentiation. Here, we consider a partial subtheory
focussing only on exponentiation and multiplication operators. Two main results
are proved. The first result is positive, namely, that the unification problem
for the above theory (in which no additional property is assumed of the
multiplication operators) is decidable. The second result is negative: if we
assume that the two multiplication operators belong to two different abelian
groups, then the unification problem becomes undecidable.
"
276,"On the Complexity of the Tiden-Arnborg Algorithm for Unification modulo
  One-Sided Distributivity","  We prove that the Tiden and Arnborg algorithm for equational unification
modulo one-sided distributivity is not polynomial time bounded as previously
thought. A set of counterexamples is developed that demonstrates that the
algorithm goes through exponentially many steps.
"
277,Modular absolute decomposition of equidimensional polynomial ideals,"  In this paper, we present a modular strategy which describes key properties
of the absolute primary decomposition of an equidimensional polynomial ideal
defined by polynomials with rational coefficients. The algorithm we design is
based on the classical technique of elimination of variables and colon ideals
and uses a tricky choice of prime integers to work with. Thanks to this
technique, we can obtain the number of absolute irreducible components, their
degree, multiplicity and also the affine Hilbert function of the reduced
components (namely, their initial ideal w.r.t. a degree-compatible term
ordering) .
"
278,"Computing Differential Equations for Integrals Associated to Smooth Fano
  Polytopes","  We give an approximate algorithm of computing holonomic systems of linear
differential equations for definite integrals with parameters. We show that
this algorithm gives a correct answer in finite steps, but we have no general
stopping condition. We apply the approximate method to find differential
equations for integrals associated to smooth Fano polytopes. They are
interested in the study of K3 surfaces and the toric mirror symmetry. In this
class of integrals, we can apply Stienstra's rank formula to our algorithm,
which gives a stopping condition of the approximate algorithm.
"
279,"A new conception for computing gr\""{o}bner basis and its applications","  This paper presents a conception for computing gr\""{o}bner basis. We convert
some of gr\""{o}bner-computing algorithms, e.g., F5, extended F5 and GWV
algorithms into a special type of algorithm. The new algorithm's finite
termination problem can be described by equivalent conditions, so all the above
algorithms can be determined when they terminate finitely. At last, a new
criterion is presented. It is an improvement for the Rewritten and Signature
Criterion.
"
280,"""On the engineers' new toolbox"" or Analog Circuit Design, using Symbolic
  Analysis, Computer Algebra, and Elementary Network Transformations","  In this paper, by way of three examples - a fourth order low pass active RC
filter, a rudimentary BJT amplifier, and an LC ladder - we show, how the
algebraic capabilities of modern computer algebra systems can, or in the last
example, might be brought to use in the task of designing analog circuits.
"
281,FORM development,"  I give an overview of FORM development based on a few pilot projects,
explaining how they have influenced the FORM capabilities. Next I explain what
is happnening right now in the field of Open Sourcing and the FORM Forum.
"
282,"A Refined Denominator Bounding Algorithm for Multivariate Linear
  Difference Equations","  We continue to investigate which polynomials can possibly occur as factors in
the denominators of rational solutions of a given partial linear difference
equation.
  In an earlier article we had introduced the distinction between periodic and
aperiodic factors in the denominator, and we gave an algorithm for predicting
the aperiodic ones. Now we extend this technique towards the periodic case and
present a refined algorithm which also finds most of the periodic factors.
"
283,"Deflation and Certified Isolation of Singular Zeros of Polynomial
  Systems","  We develop a new symbolic-numeric algorithm for the certification of singular
isolated points, using their associated local ring structure and certified
numerical computations. An improvement of an existing method to compute inverse
systems is presented, which avoids redundant computation and reduces the size
of the intermediate linear systems to solve. We derive a one-step deflation
technique, from the description of the multiplicity structure in terms of
differentials. The deflated system can be used in Newton-based iterative
schemes with quadratic convergence. Starting from a polynomial system and a
small-enough neighborhood, we obtain a criterion for the existence and
uniqueness of a singular root of a given multiplicity structure, applying a
well-chosen symbolic perturbation. Standard verification methods, based eg. on
interval arithmetic and a fixed point theorem, are employed to certify that
there exists a unique perturbed system with a singular root in the domain.
Applications to topological degree computation and to the analysis of real
branches of an implicit curve illustrate the method.
"
284,"A Symbolic Transformation Language and its Application to a Multiscale
  Method","  The context of this work is the design of a software, called MEMSALab,
dedicated to the automatic derivation of multiscale models of arrays of micro-
and nanosystems. In this domain a model is a partial differential equation.
Multiscale methods approximate it by another partial differential equation
which can be numerically simulated in a reasonable time. The challenge consists
in taking into account a wide range of geometries combining thin and periodic
structures with the possibility of multiple nested scales.
  In this paper we present a transformation language that will make the
development of MEMSALab more feasible. It is proposed as a Maple package for
rule-based programming, rewriting strategies and their combination with
standard Maple code. We illustrate the practical interest of this language by
using it to encode two examples of multiscale derivations, namely the two-scale
limit of the derivative operator and the two-scale model of the stationary heat
equation.
"
285,"A Generalized Criterion for Signature Related Gr\""obner Basis Algorithms","  A generalized criterion for signature related algorithms to compute Gr\""obner
basis is proposed in this paper. Signature related algorithms are a popular
kind of algorithms for computing Gr\""obner basis, including the famous F5
algorithm, the extended F5 algorithm and the GVW algorithm. The main purpose of
current paper is to study in theory what kind of criteria is correct in
signature related algorithms and provide a generalized method to develop new
criteria. For this purpose, a generalized criterion is proposed. The
generalized criterion only relies on a general partial order defined on a set
of polynomials. When specializing the partial order to appropriate specific
orders, the generalized criterion can specialize to almost all existing
criteria of signature related algorithms. For {\em admissible} partial orders,
a complete proof of the correctness of the algorithm based on this generalized
criterion is also presented. This proof has no extra requirements on the
computing order of critical pairs, and is also valid for non-homogeneous
polynomial systems. More importantly, the partial orders implied by existing
criteria are admissible. Besides, one can also check whether a new criterion is
correct in signature related algorithms or even develop new criteria by using
other admissible partial orders in the generalized criterion.
"
286,Signature-based algorithms to compute Groebner bases,"  This paper describes a Buchberger-style algorithm to compute a Groebner basis
of a polynomial ideal, allowing for a selection strategy based on ""signatures"".
We explain how three recent algorithms can be viewed as different strategies
for the new algorithm, and how other selection strategies can be formulated. We
describe a fourth as an example. We analyze the strategies both theoretically
and empirically, leading to some surprising results.
"
287,Multiplicity Preserving Triangular Set Decomposition of Two Polynomials,"  In this paper, a multiplicity preserving triangular set decomposition
algorithm is proposed for a system of two polynomials. The algorithm decomposes
the variety defined by the polynomial system into unmixed components
represented by triangular sets, which may have negative multiplicities. In the
bivariate case, we give a complete algorithm to decompose the system into
multiplicity preserving triangular sets with positive multiplicities. We also
analyze the complexity of the algorithm in the bivariate case. We implement our
algorithm and show the effectiveness of the method with extensive experiments.
"
288,Diversification improves interpolation,"  We consider the problem of interpolating an unknown multivariate polynomial
with coefficients taken from a finite field or as numerical approximations of
complex numbers. Building on the recent work of Garg and Schost, we improve on
the best-known algorithm for interpolation over large finite fields by
presenting a Las Vegas randomized algorithm that uses fewer black box
evaluations. Using related techniques, we also address numerical interpolation
of sparse polynomials with complex coefficients, and provide the first provably
stable algorithm (in the sense of relative error) for this problem, at the cost
of modestly more evaluations. A key new technique is a randomization which
makes all coefficients of the unknown polynomial distinguishable, producing
what we call a diverse polynomial. Another departure from most previous
approaches is that our algorithms do not rely on root finding as a subroutine.
We show how these improvements affect the practical performance with trial
implementations.
"
289,Univariate real root isolation in an extension field,"  We present algorithmic, complexity and implementation results for the problem
of isolating the real roots of a univariate polynomial in $B_{\alpha} \in
L[y]$, where $L=\QQ(\alpha)$ is a simple algebraic extension of the rational
numbers. We consider two approaches for tackling the problem. In the first
approach using resultant computations we perform a reduction to a polynomial
with integer coefficients. We compute separation bounds for the roots, and
using them we deduce that we can isolate the real roots of $B_{\alpha}$ in
$\sOB(N^{10})$, where $N$ is an upper bound on all the quantities (degree and
bitsize) of the input polynomials. In the second approach we isolate the real
roots working directly on the polynomial of the input. We compute improved
separation bounds for real roots and we prove that they are optimal, under mild
assumptions. For isolating the roots we consider a modified Sturm's algorithm,
and a modified version of \func{descartes}' algorithm introduced by Sagraloff.
For the former we prove a complexity bound of $\sOB(N^8)$ and for the latter a
bound of $\sOB(N^{7})$. We implemented the algorithms in \func{C} as part of
the core library of \mathematica and we illustrate their efficiency over
various data sets. Finally, we present complexity results for the general case
of the first approach, where the coefficients belong to multiple extensions.
"
290,"Independence of hyperlogarithms over function fields via algebraic
  combinatorics","  We obtain a necessary and sufficient condition for the linear independence of
solutions of differential equations for hyperlogarithms. The key fact is that
the multiplier (i.e. the factor $M$ in the differential equation $dS=MS$) has
only singularities of first order (Fuchsian-type equations) and this implies
that they freely span a space which contains no primitive. We give direct
applications where we extend the property of linear independence to the largest
known ring of coefficients.
"
291,A Note on the Group-theoretic Approach to Fast Matrix Multiplication,"  In 2003 COHN and UMANS introduced a group-theoretic approach to fast matrix
multiplication. This involves finding large subsets S, T and U of a group G
satisfying the Triple Product Property (TPP) as a means to bound the exponent
$\omega$ of the matrix multiplication. We show that S, T and U may be be
assumed to contain the identity and be otherwise disjoint. We also give a much
shorter proof of the upper bound |S|+|T|+|U| <= |G|+2.
"
292,Computing Semi-algebraic Invariants for Polynomial Dynamical Systems,"  In this paper, we consider an extended concept of invariant for polynomial
dynamical system (PDS) with domain and initial condition, and establish a sound
and complete criterion for checking semi-algebraic invariants (SAI) for such
PDSs. The main idea is encoding relevant dynamical properties as conditions on
the high order Lie derivatives of polynomials occurring in the SAI. A direct
consequence of this criterion is a relatively complete method of SAI generation
based on template assumption and semi-algebraic constraint solving. Relative
completeness means if there is an SAI in the form of a predefined template,
then our method can indeed find one using this template.
"
293,Generalized companion matrix for approximate GCD,"  We study a variant of the univariate approximate GCD problem, where the
coefficients of one polynomial f(x)are known exactly, whereas the coefficients
of the second polynomial g(x)may be perturbed. Our approach relies on the
properties of the matrix which describes the operator of multiplication by gin
the quotient ring C[x]=(f). In particular, the structure of the null space of
the multiplication matrix contains all the essential information about GCD(f;
g). Moreover, the multiplication matrix exhibits a displacement structure that
allows us to design a fast algorithm for approximate GCD computation with
quadratic complexity w.r.t. polynomial degrees.
"
294,"Root Isolation of Zero-dimensional Polynomial Systems with Linear
  Univariate Representation","  In this paper, a linear univariate representation for the roots of a
zero-dimensional polynomial equation system is presented, where the roots of
the equation system are represented as linear combinations of roots of several
univariate polynomial equations. The main advantage of this representation is
that the precision of the roots can be easily controlled. In fact, based on the
linear univariate representation, we can give the exact precisions needed for
roots of the univariate equations in order to obtain the roots of the equation
system to a given precision. As a consequence, a root isolation algorithm for a
zero-dimensional polynomial equation system can be easily derived from its
linear univariate representation.
"
295,SqFreeEVAL: An (almost) optimal real-root isolation algorithm,"  Let f be a univariate polynomial with real coefficients, f in R[X].
Subdivision algorithms based on algebraic techniques (e.g., Sturm or Descartes
methods) are widely used for isolating the real roots of f in a given interval.
In this paper, we consider a simple subdivision algorithm whose primitives are
purely numerical (e.g., function evaluation). The complexity of this algorithm
is adaptive because the algorithm makes decisions based on local data. The
complexity analysis of adaptive algorithms (and this algorithm in particular)
is a new challenge for computer science. In this paper, we compute the size of
the subdivision tree for the SqFreeEVAL algorithm.
  The SqFreeEVAL algorithm is an evaluation-based numerical algorithm which is
well-known in several communities. The algorithm itself is simple, but prior
attempts to compute its complexity have proven to be quite technical and have
yielded sub-optimal results. Our main result is a simple O(d(L+ln d)) bound on
the size of the subdivision tree for the SqFreeEVAL algorithm on the benchmark
problem of isolating all real roots of an integer polynomial f of degree d and
whose coefficients can be written with at most L bits.
  Our proof uses two amortization-based techniques: First, we use the algebraic
amortization technique of the standard Mahler-Davenport root bounds to
interpret the integral in terms of d and L. Second, we use a continuous
amortization technique based on an integral to bound the size of the
subdivision tree. This paper is the first to use the novel analysis technique
of continuous amortization to derive state of the art complexity bounds.
"
296,Special Values of Generalized Log-sine Integrals,"  We study generalized log-sine integrals at special values. At $\pi$ and
multiples thereof explicit evaluations are obtained in terms of Nielsen
polylogarithms at $\pm1$. For general arguments we present algorithmic
evaluations involving Nielsen polylogarithms at related arguments. In
particular, we consider log-sine integrals at $\pi/3$ which evaluate in terms
of polylogarithms at the sixth root of unity. An implementation of our results
for the computer algebra systems Mathematica and SAGE is provided.
"
297,Deciding trigonality of algebraic curves,"  Let C be a non-hyperelliptic algebraic curve of genus at least 3. Enriques
and Babbage proved that its canonical image is the intersection of the quadrics
that contain it, except when C is trigonal (that is, it has a linear system of
degree 3 and dimension 1) or C is isomorphic to a plane quintic (genus 6). We
present a method to decide whether a given algebraic curve is trigonal, and in
the affirmative case to compute a map from C to the projective line whose
fibers cut out the linear system. It is based on the Lie algebra method
presented in Schicho (2006). Our algorithm is part of a larger effort to
determine whether a given algebraic curve admits a radical parametrization.
"
298,Arrangement Computation for Planar Algebraic Curves,"  We present a new certified and complete algorithm to compute arrangements of
real planar algebraic curves. Our algorithm provides a geometric-topological
analysis of the decomposition of the plane induced by a finite number of
algebraic curves in terms of a cylindrical algebraic decomposition of the
plane. Compared to previous approaches, we improve in two main aspects:
Firstly, we significantly reduce the amount of exact operations, that is, our
algorithms only uses resultant and gcd as purely symbolic operations. Secondly,
we introduce a new hybrid method in the lifting step of our algorithm which
combines the usage of a certified numerical complex root solver and information
derived from the resultant computation. Additionally, we never consider any
coordinate transformation and the output is also given with respect to the
initial coordinate system. We implemented our algorithm as a prototypical
package of the C++-library CGAL. Our implementation exploits graphics hardware
to expedite the resultant and gcd computation. We also compared our
implementation with the current reference implementation, that is, CGAL's curve
analysis and arrangement for algebraic curves. For various series of
challenging instances, our experiments show that the new implementation
outperforms the existing one.
"
299,Generating and Searching Families of FFT Algorithms,"  A fundamental question of longstanding theoretical interest is to prove the
lowest exact count of real additions and multiplications required to compute a
power-of-two discrete Fourier transform (DFT). For 35 years the split-radix
algorithm held the record by requiring just 4n log n - 6n + 8 arithmetic
operations on real numbers for a size-n DFT, and was widely believed to be the
best possible. Recent work by Van Buskirk et al. demonstrated improvements to
the split-radix operation count by using multiplier coefficients or ""twiddle
factors"" that are not n-th roots of unity for a size-n DFT. This paper presents
a Boolean Satisfiability-based proof of the lowest operation count for certain
classes of DFT algorithms. First, we present a novel way to choose new yet
valid twiddle factors for the nodes in flowgraphs generated by common
power-of-two fast Fourier transform algorithms, FFTs. With this new technique,
we can generate a large family of FFTs realizable by a fixed flowgraph. This
solution space of FFTs is cast as a Boolean Satisfiability problem, and a
modern Satisfiability Modulo Theory solver is applied to search for FFTs
requiring the fewest arithmetic operations. Surprisingly, we find that there
are FFTs requiring fewer operations than the split-radix even when all twiddle
factors are n-th roots of unity.
"
300,Algorithms for Computing Triangular Decompositions of Polynomial Systems,"  We propose new algorithms for computing triangular decompositions of
polynomial systems incrementally. With respect to previous works, our
improvements are based on a {\em weakened} notion of a polynomial GCD modulo a
regular chain, which permits to greatly simplify and optimize the
sub-algorithms. Extracting common work from similar expensive computations is
also a key feature of our algorithms. In our experimental results the
implementation of our new algorithms, realized with the {\RegularChains}
library in {\Maple}, outperforms solvers with similar specifications by several
orders of magnitude on sufficiently difficult problems.
"
301,"Quantifier Elimination over Finite Fields Using Gr\""obner Bases","  We give an algebraic quantifier elimination algorithm for the first-order
theory over any given finite field using Gr\""obner basis methods. The algorithm
relies on the strong Nullstellensatz and properties of elimination ideals over
finite fields. We analyze the theoretical complexity of the algorithm and show
its application in the formal analysis of a biological controller model.
"
302,Root Refinement for Real Polynomials,"  We consider the problem of approximating all real roots of a square-free
polynomial $f$. Given isolating intervals, our algorithm refines each of them
to a width of $2^{-L}$ or less, that is, each of the roots is approximated to
$L$ bits after the binary point. Our method provides a certified answer for
arbitrary real polynomials, only considering finite approximations of the
polynomial coefficients and choosing a suitable working precision adaptively.
In this way, we get a correct algorithm that is simple to implement and
practically efficient. Our algorithm uses the quadratic interval refinement
method; we adapt that method to be able to cope with inaccuracies when
evaluating $f$, without sacrificing its quadratic convergence behavior. We
prove a bound on the bit complexity of our algorithm in terms of the degree of
the polynomial, the size and the separation of the roots, that is, parameters
exclusively related to the geometric location of the roots. Our bound is near
optimal and significantly improves previous work on integer polynomials.
Furthermore, it essentially matches the best known theoretical bounds on root
approximation which are obtained by very sophisticated algorithms. We also
investigate the practical behavior of the algorithm and demonstrate how closely
the practical performance matches our asymptotic bounds.
"
303,A Worst-case Bound for Topology Computation of Algebraic Curves,"  Computing the topology of an algebraic plane curve $\mathcal{C}$ means to
compute a combinatorial graph that is isotopic to $\mathcal{C}$ and thus
represents its topology in $\mathbb{R}^2$. We prove that, for a polynomial of
degree $n$ with coefficients bounded by $2^\rho$, the topology of the induced
curve can be computed with $\tilde{O}(n^8(n+\rho^2))$ bit operations
deterministically, and with $\tilde{O}(n^8\rho^2)$ bit operations with a
randomized algorithm in expectation. Our analysis improves previous best known
complexity bounds by a factor of $n^2$. The improvement is based on new
techniques to compute and refine isolating intervals for the real roots of
polynomials, and by the consequent amortized analysis of the critical fibers of
the algebraic curve.
"
304,"Effective partitioning method for computing weighted Moore-Penrose
  inverse","  We introduce a method and an algorithm for computing the weighted
Moore-Penrose inverse of multiple-variable polynomial matrix and the related
algorithm which is appropriated for sparse polynomial matrices. These methods
and algorithms are generalizations of algorithms developed in [M.B. Tasic, P.S.
Stanimirovic, M.D. Petkovic, Symbolic computation of weighted Moore-Penrose
inverse using partitioning method, Appl. Math. Comput. 189 (2007) 615-640] to
multiple-variable rational and polynomial matrices and improvements of these
algorithms on sparse matrices. Also, these methods are generalizations of the
partitioning method for computing the Moore-Penrose inverse of rational and
polynomial matrices introduced in [P.S. Stanimirovic, M.B. Tasic, Partitioning
method for rational and polynomial matrices, Appl. Math. Comput. 155 (2004)
137-163; M.D. Petkovic, P.S. Stanimirovic, Symbolic computation of the
Moore-Penrose inverse using partitioning method, Internat. J. Comput. Math. 82
(2005) 355-367] to the case of weighted Moore-Penrose inverse. Algorithms are
implemented in the symbolic computational package MATHEMATICA.
"
305,"Symbolic computation of weighted Moore-Penrose inverse using
  partitioning method","  We propose a method and algorithm for computing the weighted Moore-Penrose
inverse of one-variable rational matrices. Continuing this idea, we develop an
algorithm for computing the weighted Moore-Penrose inverse of one-variable
polynomial matrix. These methods and algorithms are generalizations of the
method for computing the weighted Moore-Penrose inverse for constant matrices,
originated in Wang and Chen [G.R. Wang, Y.L. Chen, A recursive algorithm for
computing the weighted Moore-Penrose inverse AMN, J. Comput. Math. 4 (1986)
74-85], and the partitioning method for computing the Moore-Penrose inverse of
rational and polynomial matrices introduced in Stanimirovic and Tasic [P.S.
Stanimirovic, M.B. Tasic, Partitioning method for rational and polynomial
matrices, Appl. Math. Comput. 155 (2004) 137-163]. Algorithms are implemented
in the symbolic computational package MATHEMATICA.
"
306,Computing generalized inverses using LU factorization of matrix product,"  An algorithm for computing {2, 3}, {2, 4}, {1, 2, 3}, {1, 2, 4} -inverses and
the Moore-Penrose inverse of a given rational matrix A is established. Classes
A(2, 3)s and A(2, 4)s are characterized in terms of matrix products (R*A)+R*
and T*(AT*)+, where R and T are rational matrices with appropriate dimensions
and corresponding rank. The proposed algorithm is based on these general
representations and the Cholesky factorization of symmetric positive matrices.
The algorithm is implemented in programming languages MATHEMATICA and DELPHI,
and illustrated via examples. Numerical results of the algorithm, corresponding
to the Moore-Penrose inverse, are compared with corresponding results obtained
by several known methods for computing the Moore-Penrose inverse.
"
307,About the generalized LM-inverse and the weighted Moore-Penrose inverse,"  The recursive method for computing the generalized LM-inverse of a constant
rectangular matrix augmented by a column vector is proposed in Udwadia and
Phohomsiri (2007) [16] and [17]. The corresponding algorithm for the sequential
determination of the generalized LM-inverse is established in the present
paper. We prove that the introduced algorithm for computing the generalized
LM-inverse and the algorithm for the computation of the weighted Moore-Penrose
inverse developed by Wang and Chen (1986) in [23] are equivalent algorithms.
Both of the algorithms are implemented in the present paper using the package
MATHEMATICA. Several rational test matrices and randomly generated constant
matrices are tested and the CPU time is compared and discussed.
"
308,Effective radical parametrization of trigonal curves,"  Let $C$ be a non-hyperelliptic algebraic curve. It is known that its
canonical image is the intersection of the quadrics that contain it, except
when $C$ is trigonal (that is, it has a linear system of degree 3 and dimension
1) or isomorphic to a plane quintic (genus 6). In this context, we present a
method to decide whether a given algebraic curve is trigonal, and in the
affirmative case to compute a map from $C$ to the projective line whose fibers
cut out the linear system.
"
309,Computing Border Bases without using a Term Ordering,"  Border bases, a generalization of Groebner bases, have actively been
researched during recent years due to their applicability to industrial
problems. A. Kehrein and M. Kreuzer formulated the so called Border Basis
Algorithm, an algorithm which allows the computation of border bases that
relate to a degree compatible term ordering. In this paper we extend the
original Border Basis Algorithm in such a way that also border bases that do
not relate to any term ordering can be computed by it.
"
310,Methods in Mathematica for Solving Ordinary Differential Equations,"  An overview of the solution methods for ordinary differential equations in
the Mathematica function DSolve is presented.
"
311,"Symbolic Computation of Recursion Operators for Nonlinear
  Differential-Difference equations","  An algorithm for the symbolic computation of recursion operators for systems
of nonlinear differential-difference equations (DDEs) is presented. Recursion
operators allow one to generate an infinite sequence of generalized symmetries.
The existence of a recursion operator therefore guarantees the complete
integrability of the DDE. The algo-rithm is based in part on the concept of
dilation invariance and uses our earlier algorithms for the symbolic
computation of conservation laws and generalized symmetries.
  The algorithm has been applied to a number of well-known DDEs, including the
Kac-van Moerbeke (Volterra), Toda, and Ablowitz-Ladik lattices, for which
recursion opera-tors are shown. The algorithm has been implemented in
Mathematica, a leading com-puter algebra system. The package
DDERecursionOperator.m is briefly discussed.
"
312,Automated Synthesis of Tableau Calculi,"  This paper presents a method for synthesising sound and complete tableau
calculi. Given a specification of the formal semantics of a logic, the method
generates a set of tableau inference rules that can then be used to reason
within the logic. The method guarantees that the generated rules form a
calculus which is sound and constructively complete. If the logic can be shown
to admit finite filtration with respect to a well-defined first-order semantics
then adding a general blocking mechanism provides a terminating tableau
calculus. The process of generating tableau rules can be completely automated
and produces, together with the blocking mechanism, an automated procedure for
generating tableau decision procedures. For illustration we show the
workability of the approach for a description logic with transitive roles and
propositional intuitionistic logic.
"
313,"Computer Algebra meets Finite Elements: an Efficient Implementation for
  Maxwell's Equations","  We consider the numerical discretization of the time-domain Maxwell's
equations with an energy-conserving discontinuous Galerkin finite element
formulation. This particular formulation allows for higher order approximations
of the electric and magnetic field. Special emphasis is placed on an efficient
implementation which is achieved by taking advantage of recurrence properties
and the tensor-product structure of the chosen shape functions. These
recurrences have been derived symbolically with computer algebra methods
reminiscent of the holonomic systems approach.
"
314,"Square root Bound on the Least Power Non-residue using a
  Sylvester-Vandermonde Determinant","  We give a new elementary proof of the fact that the value of the least
$k^{th}$ power non-residue in an arithmetic progression $\{bn+c\}_{n=0,1...}$,
over a prime field $\F_p$, is bounded by $7/\sqrt{5} \cdot b \cdot \sqrt{p/k} +
4b + c$. Our proof is inspired by the so called \emph{Stepanov method}, which
involves bounding the size of the solution set of a system of equations by
constructing a non-zero low degree auxiliary polynomial that vanishes with high
multiplicity on the solution set. The proof uses basic algebra and number
theory along with a determinant identity that generalizes both the Sylvester
and the Vandermonde determinant.
"
315,"Symbolic Computation of Conservation Laws, Generalized Symmetries, and
  Recursion Operators for Nonlinear Differential-Difference Equations","  Algorithms for the symbolic computation of polynomial conservation laws,
generalized symmetries, and recursion operators for systems of nonlinear
differential-difference equations (DDEs) are presented. The algorithms can be
used to test the complete integrability of nonlinear DDEs. The ubiquitous Toda
lattice illustrates the steps of the algorithms, which have been implemented in
{\em Mathematica}. The codes {\sc InvariantsSymmetries.m} and {\sc
DDERecursionOperator.m} can aid researchers interested in properties of
nonlinear DDEs.
"
316,On the Complexity of Solving a Bivariate Polynomial System,"  We study the complexity of computing the real solutions of a bivariate
polynomial system using the recently proposed algorithm BISOLVE. BISOLVE is a
classical elimination method which first projects the solutions of a system
onto the $x$- and $y$-axes and, then, selects the actual solutions from the so
induced candidate set. However, unlike similar algorithms, BISOLVE requires no
genericity assumption on the input nor it needs any change of the coordinate
system. Furthermore, extensive benchmarks from \cite{bes-bisolve-2011} confirm
that the algorithm outperforms state of the art approaches by a large factor.
In this work, we show that, for two polynomials $f,g\in\mathbb{Z}[x,y]$ of
total degree at most $n$ with integer coefficients bounded by $2^\tau$, BISOLVE
computes isolating boxes for all real solutions of the system $f=g=0$ using
$\Otilde(n^8\tau^{2})$ bit operations, thereby improving the previous record
bound by a factor of at least $n^{2}$.
"
317,Search and test algorithms for Triple Product Property triples,"  In 2003 COHN and UMANS introduced a group-theoretic approach to fast matrix
multiplication. This involves finding large subsets of a group $G$ satisfying
the Triple Product Property (TPP) as a means to bound the exponent $\omega$ of
matrix multiplication. We present two new characterizations of the TPP, which
are useful for theoretical considerations and for TPP test algorithms. With
this we describe all known TPP tests and implement them in GAP algorithms. We
also compare their runtime. Furthermore we show that the search for subgroup
TPP triples of nontrivial size in a nonabelian group can be restricted to the
set of all nonnormal subgroups of that group. Finally we describe brute-force
search algorithms for maximal subgroup and subset TPP triples. In addition we
present the results of the subset brute-force search for all groups of order
less than 25 and selected results of the subgroup brute-force search for
2-groups, $SL(n,q)$ and $PSL(2,q)$.
"
318,About a conjectured basis for Multiple Zeta Values,"  We confirm a conjecture about the construction of basis elements for the
multiple zeta values (MZVs) at weight 27 and weight 28. Both show as expected
one element that is twofold extended. This is done with some lengthy computer
algebra calculations using TFORM to determine explicit bases for the MZVs at
these weights.
"
319,"HYPERDIRE: HYPERgeometric functions DIfferential REduction: MATHEMATICA
  based packages for differential reduction of generalized hypergeometric
  functions pFq, F1,F2,F3,F4","  HYPERDIRE is a project devoted to the creation of a set of Mathematica based
programs for the differential reduction of hypergeometric functions. The
current version includes two parts: one, pfq, is relevant for manipulations of
hypergeometric functions_{p+1}F_p, and the second one, AppellF1F4, for
manipulations with Appell hypergeometric functions F_1,F_2,F_3,F_4 of two
variables.
"
320,On the Generation of Positivstellensatz Witnesses in Degenerate Cases,"  One can reduce the problem of proving that a polynomial is nonnegative, or
more generally of proving that a system of polynomial inequalities has no
solutions, to finding polynomials that are sums of squares of polynomials and
satisfy some linear equality (Positivstellensatz). This produces a witness for
the desired property, from which it is reasonably easy to obtain a formal proof
of the property suitable for a proof assistant such as Coq. The problem of
finding a witness reduces to a feasibility problem in semidefinite programming,
for which there exist numerical solvers. Unfortunately, this problem is in
general not strictly feasible, meaning the solution can be a convex set with
empty interior, in which case the numerical optimization method fails.
Previously published methods thus assumed strict feasibility; we propose a
workaround for this difficulty. We implemented our method and illustrate its
use with examples, including extractions of proofs to Coq.
"
321,Explicit formula for the generating series of diagonal 3D rook paths,"  Let $a_n$ denote the number of ways in which a chess rook can move from a
corner cell to the opposite corner cell of an $n \times n \times n$
three-dimensional chessboard, assuming that the piece moves closer to the goal
cell at each step. We describe the computer-driven \emph{discovery and proof}
of the fact that the generating series $G(x)= \sum_{n \geq 0} a_n x^n$ admits
the following explicit expression in terms of a Gaussian hypergeometric
function: \[ G(x) = 1 + 6 \cdot \int_0^x \frac{\,\pFq21{1/3}{2/3}{2} {\frac{27
w(2-3w)}{(1-4w)^3}}}{(1-4w)(1-64w)} \, dw.\]
"
322,"Symbolic-manipulation constructions of Hilbert-space metrics in quantum
  mechanics","  The problem of the determination of the Hilbert-space metric which renders a
given Hamiltonian $H$ self-adjoint is addressed from the point of view of
applicability of computer-assisted algebraic manipulations. An exactly solvable
example of the so called Gegenbauerian quantum-lattice oscillator is recalled
for the purpose. Both the construction of suitable metric (basically, the
solution of the Dieudonne's operator equation) and the determination of its
domain of positivity are shown facilitated by the symbolic algebraic
manipulations and by MAPLE-supported numerics and graphics.
"
323,Deciding Kleene Algebras in Coq,"  We present a reflexive tactic for deciding the equational theory of Kleene
algebras in the Coq proof assistant. This tactic relies on a careful
implementation of efficient finite automata algorithms, so that it solves
casual equations instantaneously and properly scales to larger expressions. The
decision procedure is proved correct and complete: correctness is established
w.r.t. any model by formalising Kozen's initiality theorem; a counter-example
is returned when the given equation does not hold. The correctness proof is
challenging: it involves both a precise analysis of the underlying automata
algorithms and a lot of algebraic reasoning. In particular, we have to
formalise the theory of matrices over a Kleene algebra. We build on the recent
addition of firstorder typeclasses in Coq in order to work efficiently with the
involved algebraic structures.
"
324,A note on Solving Parametric Polynomial Systems,"  Lazard and Rouillier in [9], by introducing the concept of discriminant
variety, have described a new and efficient algorithm for solving parametric
polynomial systems. In this paper we modify this algorithm, and we show that
with our improvements the output of our algorithm is always minimal and it does
not need to compute the radical of ideals.
"
325,PHCpack in Macaulay2,"  The Macaulay2 package PHCpack.m2 provides an interface to PHCpack, a
general-purpose polynomial system solver that uses homotopy continuation. The
main method is a numerical blackbox solver which is implemented for all Laurent
systems. The package also provides a fast mixed volume computation, the ability
to filter solutions, homotopy path tracking, and a numerical irreducible
decomposition method. As the size of many problems in applied algebraic
geometry often surpasses the capabilities of symbolic software, this package
will be of interest to those working on problems involving large polynomial
systems.
"
326,A parallel Buchberger algorithm for multigraded ideals,"  We demonstrate a method to parallelize the computation of a Gr\""obner basis
for a homogenous ideal in a multigraded polynomial ring. Our method uses
anti-chains in the lattice $\mathbb N^k$ to separate mutually independent
S-polynomials for reduction.
"
327,Harmonic Sums and Polylogarithms Generated by Cyclotomic Polynomials,"  The computation of Feynman integrals in massive higher order perturbative
calculations in renormalizable Quantum Field Theories requires extensions of
multiply nested harmonic sums, which can be generated as real representations
by Mellin transforms of Poincar\'e--iterated integrals including denominators
of higher cyclotomic polynomials. We derive the cyclotomic harmonic
polylogarithms and harmonic sums and study their algebraic and structural
relations. The analytic continuation of cyclotomic harmonic sums to complex
values of $N$ is performed using analytic representations. We also consider
special values of the cyclotomic harmonic polylogarithms at argument $x=1$,
resp., for the cyclotomic harmonic sums at $N \rightarrow \infty$, which are
related to colored multiple zeta values, deriving various of their relations,
based on the stuffle and shuffle algebras and three multiple argument
relations. We also consider infinite generalized nested harmonic sums at roots
of unity which are related to the infinite cyclotomic harmonic sums. Basis
representations are derived for weight {\sf w = 1,2} sums up to cyclotomy {\sf
l = 20}.
"
328,"A New Algorithm for Proving Global Asymptotic Stability of Rational
  Difference Equations","  Global asymptotic stability of rational difference equations is an area of
research that has been well studied. In contrast to the many current methods
for proving global asymptotic stability, we propose an algorithmic approach.
The algorithm we summarize here employs the idea of contractions. Given a
particular rational difference equation, defined by a function $Q$ which maps
the $k+1$ dimensional real numbers to itself, we attempt to find an integer,
$K$, for which $Q^K$ shrinks distances to the difference equation's equilibrium
point. We state some general results that our algorithm has been able to prove,
and also mention the implementation of our algorithm using Maple.
"
329,The MathScheme Library: Some Preliminary Experiments,"  We present some of the experiments we have performed to best test our design
for a library for MathScheme, the mechanized mathematics software system we are
building. We wish for our library design to use and reflect, as much as
possible, the mathematical structure present in the objects which populate the
library.
"
330,"A Generalized Criterion for Signature-based Algorithms to Compute
  Gr\""obner Bases","  A generalized criterion for signature-based algorithms to compute Gr\""obner
bases is proposed in this paper. This criterion is named by ""generalized
criterion"", because it can be specialized to almost all existing criteria for
signature-based algorithms which include the famous F5 algorithm, F5C, extended
F5, G$^2$V and the GVW algorithm. The main purpose of current paper is to study
in theory which kind of criteria is correct in signature-based algorithms and
provide a generalized method to develop new criteria. For this purpose, by
studying some key facts and observations of signature-based algorithms, a
generalized criterion is proposed. The generalized criterion only relies on a
partial order defined on a set of polynomials. When specializing the partial
order to appropriate specific orders, the generalized criterion can specialize
to almost all existing criteria of signature-based algorithms. For {\em
admissible} partial orders, a proof is presented for the correctness of the
algorithm that is based on this generalized criterion. And the partial orders
implied by the criteria of F5 and GVW are also shown to be admissible. More
importantly, the generalized criterion provides an effective method to check
whether a new criterion is correct as well as to develop new criteria for
signature-based algorithms.
"
331,Splitting full matrix algebras over algebraic number fields,"  Let K be an algebraic number field of degree d and discriminant D over Q. Let
A be an associative algebra over K given by structure constants such that A is
isomorphic to the algebra M_n(K) of n by n matrices over K for some positive
integer n. Suppose that d, n and D are bounded. Then an isomorphism of A with
M_n(K) can be constructed by a polynomial time ff-algorithm. (An ff-algorithm
is a deterministic procedure which is allowed to call oracles for factoring
integers and factoring univariate polynomials over finite fields.)
  As a consequence, we obtain a polynomial time ff-algorithm to compute
isomorphisms of central simple algebras of bounded degree over K.
"
332,Decomposition of Polynomials,"  This diploma thesis is concerned with functional decomposition $f = g \circ
h$ of polynomials. First an algorithm is described which computes
decompositions in polynomial time. This algorithm was originally proposed by
Zippel (1991). A bound for the number of minimal collisions is derived. Finally
a proof of a conjecture in von zur Gathen, Giesbrecht & Ziegler (2010) is
given, which states a classification for a special class of decomposable
polynomials.
"
333,Computing the Distance between Piecewise-Linear Bivariate Functions,"  We consider the problem of computing the distance between two
piecewise-linear bivariate functions $f$ and $g$ defined over a common domain
$M$. We focus on the distance induced by the $L_2$-norm, that is
$\|f-g\|_2=\sqrt{\iint_M (f-g)^2}$. If $f$ is defined by linear interpolation
over a triangulation of $M$ with $n$ triangles, while $g$ is defined over
another such triangulation, the obvious na\""ive algorithm requires
$\Theta(n^2)$ arithmetic operations to compute this distance. We show that it
is possible to compute it in $\O(n\log^4 n)$ arithmetic operations, by reducing
the problem to multi-point evaluation of a certain type of polynomials. We also
present an application to terrain matching.
"
334,Differential Chow Form for Projective Differential Variety,"  In this paper, a generic intersection theorem in projective differential
algebraic geometry is presented. Precisely, the intersection of an irreducible
projective differential variety of dimension d>0 and order h with a generic
projective differential hyperplane is shown to be an irreducible projective
differential variety of dimension d-1 and order h. Based on the generic
intersection theorem, the Chow form for an irreducible projective differential
variety is defined and most of the properties of the differential Chow form in
affine differential case are established for its projective differential
counterpart. Finally, we apply the differential Chow form to a result of linear
dependence over projective varieties given by Kolchin.
"
335,Computing the homology of groups: the geometric way,"  In this paper we present several algorithms related with the computation of
the homology of groups, from a geometric perspective (that is to say, carrying
out the calculations by means of simplicial sets and using techniques of
Algebraic Topology). More concretely, we have developed some algorithms which,
making use of the effective homology method, construct the homology groups of
Eilenberg-MacLane spaces K(G,1) for different groups G, allowing one in
particular to determine the homology groups of G.
  Our algorithms have been programmed as new modules for the Kenzo system,
enhancing it with the following new functionalities:
  - construction of the effective homology of K(G,1) from a given finite free
resolution of the group G;
  - construction of the effective homology of K(A,1) for every finitely
generated Abelian group A (as a consequence, the effective homology of K(A,n)
is also available in Kenzo, for all n);
  - computation of homology groups of some 2-types;
  - construction of the effective homology for central extensions.
  In addition, an inverse problem is also approached in this work: given a
group G such that K(G,1) has effective homology, can a finite free resolution
of the group G be obtained? We provide some algorithms to solve this problem,
based on a notion of norm of a group, allowing us to control the convergence of
the process when building such a resolution.
"
336,On Consensus under Polynomial Protocols,"  In this paper we explore the possibility of using computational algebraic
methods to analyze a class of consensus protocols. We state some necessary
conditions for convergence under consensus protocols that are polynomials.
"
337,Consistency Analysis of Finite Difference Approximations to PDE Systems,"  In the given paper we consider finite difference approximations to systems of
polynomially-nonlinear partial differential equations whose coefficients are
rational functions over rationals in the independent variables. The notion of
strong consistency which we introduced earlier for linear systems is extended
to nonlinear ones. For orthogonal and uniform grids we describe an algorithmic
procedure for verification of strong consistency based on computation of
difference standard bases. The concepts and algorithmic methods of the present
paper are illustrated by two finite difference approximations to the
two-dimensional Navier-Stokes equations. One of these approximations is
strongly consistent and another is not.
"
338,On Two-generated Non-commutative Algebras Subject to the Affine Relation,"  We consider algebras over a field K, generated by two variables x and y
subject to the single relation yx = qxy + ax + by + c for q in K^* and a, b, c
in K. We prove, that among such algebras there are precisely five isomorphism
classes. The representatives of these classes, which are ubiquitous operator
algebras, are called model algebras. We derive explicit multiplication formulas
for y^m*x^n in terms of standard monomials x^i*y^j for many algebras of the
considered type. Such formulas are used in establishing formulas of binomial
type and in implementing non-commutative multiplication in a computer algebra
system. By using the formulas we also study centers and ring-theoretic
properties of the non-commutative model algebras.
"
339,"Solving Detachability Problem for the Polynomial Ring by Signature-based
  Groebner Basis Algorithms","  Signature-based algorithms are a popular kind of algorithms for computing
Groebner basis, including the famous F5 algorithm, F5C, extended F5, G2V and
the GVW algorithm. In this paper, an efficient method is proposed to solve the
detachability problem. The new method only uses the outputs of signature-based
algorithms, and no extra Groebner basis computations are needed. When a
Groebner basis is obtained by signature-based algorithms, the detachability
problem can be settled in polynomial time.
"
340,A New Algorithmic Scheme for Computing Characteristic Sets,"  Ritt-Wu's algorithm of characteristic sets is the most representative for
triangularizing sets of multivariate polynomials. Pseudo-division is the main
operation used in this algorithm. In this paper we present a new algorithmic
scheme for computing generalized characteristic sets by introducing other
admissible reductions than pseudo-division. A concrete subalgorithm is designed
to triangularize polynomial sets using selected admissible reductions and
several effective elimination strategies and to replace the algorithm of basic
sets (used in Ritt-Wu's algorithm). The proposed algorithm has been implemented
and experimental results show that it performs better than Ritt-Wu's algorithm
in terms of computing time and simplicity of output for a number of non-trivial
test examples.
"
341,"Lattice Green's Functions of the Higher-Dimensional Face-Centered Cubic
  Lattices","  We study the face-centered cubic lattice (fcc) in up to six dimensions. In
particular, we are concerned with lattice Green's functions (LGF) and return
probabilities. Computer algebra techniques, such as the method of creative
telescoping, are used for deriving an ODE for a given LGF. For the four- and
five-dimensional fcc lattices, we give rigorous proofs of the ODEs that were
conjectured by Guttmann and Broadhurst. Additionally, we find the ODE of the
LGF of the six-dimensional fcc lattice, a result that was not believed to be
achievable with current computer hardware.
"
342,"A New General-Purpose Method to Multiply 3x3 Matrices Using Only 23
  Multiplications","  One of the most famous conjectures in computer algebra is that matrix
multiplication might be feasible in not much more than quadratic time. The best
known exponent is 2.376, due to Coppersmith and Winograd. Many attempts to
solve this problems in the literature work by solving, fixed-size problems and
then apply the solution recursively. This leads to pure combinatorial
optimisation problems with fixed size. These problems are unlikely to be
solvable in polynomial time.
  In 1976 Laderman published a method to multiply two 3x3 matrices using only
23 multiplications. This result is non-commutative, and therefore can be
applied recursively to smaller sub-matrices. In 35 years nobody was able to do
better and it remains an open problem if this can be done with 22
multiplications. We proceed by solving the so called Brent equations [7]. We
have implemented a method to converting this very hard problem to a SAT
problem, and we have attempted to solve it, with our portfolio of some 500 SAT
solvers. With this new method we were able to produce new solutions to the
Laderman's problem. We present a new fully general non-commutative solution
with 23 multiplications and show that this solution is new and is NOT an
equivalent variant of the Laderman's original solution. This result
demonstrates that the space of solutions to Laderman's problem is larger than
expected, and therefore it becomes now more plausible that a solution with 22
multiplications exists. If it exists, we might be able to find it soon just by
running our algorithms longer, or due to further improvements in the SAT solver
algorithms.
"
343,A Variant of Gerdt's Algorithm for Computing Involutive Bases,"  Ihe first author presented an efficient algorithm for computing involutive
(and reduced Groebner) bases. In this paper, we consider a modification of this
algorithm which simplifies matters to understand it and to implement. We prove
correctness and termination of the modified algorithm and also correctness of
the used criteria. The proposed algorithm has been implemented in Maple. We
present experimental comparison, via some examples, of performance of the
modified algorithm with its original form which has been implemented in Maple
too. In doing so, we have taken care to provide uniform implementation details
for the both algorithms.
"
344,The Parametric Solution of Underdetermined linear ODEs,"  The purpose of this paper is twofold. An immediate practical use of the
presented algorithm is its applicability to the parametric solution of
underdetermined linear ordinary differential equations (ODEs) with coefficients
that are arbitrary analytic functions in the independent variable. A second
conceptual aim is to present an algorithm that is in some sense dual to the
fundamental Euclids algorithm, and thus an alternative to the special case of a
Groebner basis algorithm as it is used for solving linear ODE-systems. In the
paper Euclids algorithm and the new `dual version' are compared and their
complementary strengths are analysed on the task of solving underdetermined
ODEs. An implementation of the described algorithm is interactively accessible
under http://lie.math.brocku.ca/crack/demo.
"
345,Trading Order for Degree in Creative Telescoping,"  We analyze the differential equations produced by the method of creative
telescoping applied to a hyperexponential term in two variables. We show that
equations of low order have high degree, and that higher order equations have
lower degree. More precisely, we derive degree bounding formulas which allow to
estimate the degree of the output equations from creative telescoping as a
function of the order. As an application, we show how the knowledge of these
formulas can be used to improve, at least in principle, the performance of
creative telescoping implementations, and we deduce bounds on the asymptotic
complexity of creative telescoping for hyperexponential terms.
"
346,A fast algorithm for reversion of power series,"  We give an algorithm for reversion of formal power series, based on an
efficient way to implement the Lagrange inversion formula. Our algorithm
requires $O(n^{1/2}(M(n) + MM(n^{1/2})))$ operations where $M(n)$ and $MM(n)$
are the costs of polynomial and matrix multiplication respectively. This
matches the asymptotic complexity of an algorithm of Brent and Kung, but we
achieve a constant factor speedup whose magnitude depends on the polynomial and
matrix multiplication algorithms used. Benchmarks confirm that the algorithm
performs well in practice.
"
347,"Algorithms for integrals of holonomic functions over domains defined by
  polynomial inequalities","  We present an algorithm for computing a holonomic system for a definite
integral of a holonomic function over a domain defined by polynomial
inequalities. If the integrand satisfies a holonomic difference-differential
system including parameters, then a holonomic difference-differential system
for the integral can also be computed. In the algorithm, holonomic
distributions (generalized functions in the sense of L. Schwartz) are
inevitably involved even if the integrand is a usual function.
"
348,"An Oracle-based, Output-sensitive Algorithm for Projections of Resultant
  Polytopes","  We design an algorithm to compute the Newton polytope of the resultant, known
as resultant polytope, or its orthogonal projection along a given direction.
The resultant is fundamental in algebraic elimination, optimization, and
geometric modeling. Our algorithm exactly computes vertex- and
halfspace-representations of the polytope using an oracle producing resultant
vertices in a given direction, thus avoiding walking on the polytope whose
dimension is alpha-n-1, where the input consists of alpha points in Z^n. Our
approach is output-sensitive as it makes one oracle call per vertex and facet.
It extends to any polytope whose oracle-based definition is advantageous, such
as the secondary and discriminant polytopes. Our publicly available
implementation uses the experimental CGAL package triangulation. Our method
computes 5-, 6- and 7-dimensional polytopes with 35K, 23K and 500 vertices,
respectively, within 2hrs, and the Newton polytopes of many important surface
equations encountered in geometric modeling in <1sec, whereas the corresponding
secondary polytopes are intractable. It is faster than tropical geometry
software up to dimension 5 or 6. Hashing determinantal predicates accelerates
execution up to 100 times. One variant computes inner and outer approximations
with, respectively, 90% and 105% of the true volume, up to 25 times faster.
"
349,"Quality Up in Polynomial Homotopy Continuation by Multithreaded Path
  Tracking","  Speedup measures how much faster we can solve the same problem using many
cores. If we can afford to keep the execution time fixed, then quality up
measures how much better the solution will be computed using many cores. In
this paper we describe our multithreaded implementation to track one solution
path defined by a polynomial homotopy. Limiting quality to accuracy and
confusing accuracy with precision, we strive to offset the cost of
multiprecision arithmetic running multithreaded code on many cores.
"
350,"A Majorization Order on Monomials and Termination of a Successive
  Difference Substitution Algorithm","  We introduce a majorization order on monomials. With the help of this order,
we derive a necessary condition on the positive termination of a general
successive difference substitution algorithm (KSDS) for an input form $f$.
"
351,"Solving large linear algebraic systems in the context of integrable
  non-abelian Laurent ODEs","  The paper reports on a computer algebra program LSSS (Linear Selective
Systems Solver) for solving linear algebraic systems with rational
coefficients. The program is especially efficient for very large sparse systems
that have a solution in which many variables take the value zero. The program
is applied to the symmetry investigation of a non-abelian Laurent ODE
introduced recently by M. Kontsevich. The computed symmetries confirmed that a
Lax pair found for this system earlier generates all first integrals of degree
at least up to 14.
"
352,On Kahan's Rules for Determining Branch Cuts,"  In computer algebra there are different ways of approaching the mathematical
concept of functions, one of which is by defining them as solutions of
differential equations. We compare different such approaches and discuss the
occurring problems. The main focus is on the question of determining possible
branch cuts. We explore the extent to which the treatment of branch cuts can be
rendered (more) algorithmic, by adapting Kahan's rules to the differential
equation setting.
"
353,Computing the Hermite Form of a Matrix of Ore Polynomials,"  Let R=F[D;sigma,delta] be the ring of Ore polynomials over a field (or skew
field) F, where sigma is a automorphism of F and delta is a sigma-derivation.
Given a an m by n matrix A over R, we show how to compute the Hermite form H of
A and a unimodular matrix U such that UA=H. The algorithm requires a polynomial
number of operations in F in terms of both the dimensions m and n, and the
degree of the entries in A. When F=k(z) for some field k, it also requires time
polynomial in the degree in z, and if k is the rational numbers Q, it requires
time polynomial in the bit length of the coefficients as well. Explicit
analyses are provided for the complexity, in particular for the important cases
of differential and shift polynomials over Q(z). To accomplish our algorithm,
we apply the Dieudonne determinant and quasideterminant theory for Ore
polynomial rings to get explicit bounds on the degrees and sizes of entries in
H and U.
"
354,"Symbolic integration with respect to the Haar measure on the unitary
  group","  We present IntU package for Mathematica computer algebra system. The
presented package performs a symbolic integration of polynomial functions over
the unitary group with respect to unique normalized Haar measure. We describe a
number of special cases which can be used to optimize the calculation speed for
some classes of integrals. We also provide some examples of usage of the
presented package.
"
355,On the complexity of computing with zero-dimensional triangular sets,"  We study the complexity of some fundamental operations for triangular sets in
dimension zero. Using Las-Vegas algorithms, we prove that one can perform such
operations as change of order, equiprojectable decomposition, or quasi-inverse
computation with a cost that is essentially that of modular composition. Over
an abstract field, this leads to a subquadratic cost (with respect to the
degree of the underlying algebraic set). Over a finite field, in a boolean RAM
model, we obtain a quasi-linear running time using Kedlaya and Umans' algorithm
for modular composition. Conversely, we also show how to reduce the problem of
modular composition to change of order for triangular sets, so that all these
problems are essentially equivalent. Our algorithms are implemented in Maple;
we present some experimental results.
"
356,"When Newton meets Descartes: A Simple and Fast Algorithm to Isolate the
  Real Roots of a Polynomial","  We introduce a new algorithm denoted DSC2 to isolate the real roots of a
univariate square-free polynomial f with integer coefficients. The algorithm
iteratively subdivides an initial interval which is known to contain all real
roots of f. The main novelty of our approach is that we combine Descartes' Rule
of Signs and Newton iteration. More precisely, instead of using a fixed
subdivision strategy such as bisection in each iteration, a Newton step based
on the number of sign variations for an actual interval is considered, and,
only if the Newton step fails, we fall back to bisection. Following this
approach, our analysis shows that, for most iterations, we can achieve
quadratic convergence towards the real roots. In terms of complexity, our
method induces a recursion tree of almost optimal size O(nlog(n tau)), where n
denotes the degree of the polynomial and tau the bitsize of its coefficients.
The latter bound constitutes an improvement by a factor of tau upon all
existing subdivision methods for the task of isolating the real roots. In
addition, we provide a bit complexity analysis showing that DSC2 needs only
\tilde{O}(n^3tau) bit operations to isolate all real roots of f. This matches
the best bound known for this fundamental problem. However, in comparison to
the much more involved algorithms by Pan and Sch\""onhage (for the task of
isolating all complex roots) which achieve the same bit complexity, DSC2
focuses on real root isolation, is very easy to access and easy to implement.
"
357,Asymptotic Methods of ODEs: Exploring Singularities of the Second Kind,"  We develop symbolic methods of asymptotic approximations for solutions of
linear ordinary differential equations and use to them stabilize numerical
calculations. Our method follows classical analysis for first-order systems and
higher-order scalar equations where growth behavior is expressed in terms of
elementary functions. We then recast our equations in mollified form - thereby
obtaining stability.
"
358,Affine solution sets of sparse polynomial systems,"  This paper focuses on the equidimensional decomposition of affine varieties
defined by sparse polynomial systems. For generic systems with fixed supports,
we give combinatorial conditions for the existence of positive dimensional
components which characterize the equidimensional decomposition of the
associated affine variety. This result is applied to design an equidimensional
decomposition algorithm for generic sparse systems. For arbitrary sparse
systems of n polynomials in n variables with fixed supports, we obtain an upper
bound for the degree of the affine variety defined and we present an algorithm
which computes finite sets of points representing its equidimensional
components.
"
359,"Structure of lexicographic Groebner bases in three variables of ideals
  of dimension zero","  We generalize the structural theorem of Lazard in 1985, from 2 variables to 3
variables. We use the Gianni-Kalkbrener result to do this, which implies some
restrictions inside which lies the case of a radical ideal.
"
360,How Can I Do That with ACL2? Recent Enhancements to ACL2,"  The last several years have seen major enhancements to ACL2 functionality,
largely driven by requests from its user community, including utilities now in
common use such as 'make-event', 'mbe', and trust tags. In this paper we
provide user-level summaries of some ACL2 enhancements introduced after the
release of Version 3.5 (in May, 2009, at about the time of the 2009 ACL2
workshop) up through the release of Version 4.3 in July, 2011, roughly a couple
of years later. Many of these features are not particularly well known yet, but
most ACL2 users could take advantage of at least some of them. Some of the
changes could affect existing proof efforts, such as a change that treats pairs
of functions such as 'member' and 'member-equal' as the same function.
"
361,Implementing an Automatic Differentiator in ACL2,"  The foundational theory of differentiation was developed as part of the
original release of ACL2(r). In work reported at the last ACL2 Workshop, we
presented theorems justifying the usual differentiation rules, including the
chain rule and the derivative of inverse functions. However, the process of
applying these theorems to formalize the derivative of a particular function is
completely manual. More recently, we developed a macro and supporting functions
that can automate this process. This macro uses the ACL2 table facility to keep
track of functions and their derivatives, and it also interacts with the macro
that introduces inverse functions in ACL2(r), so that their derivatives can
also be automated. In this paper, we present the implementation of this macro
and related functions.
"
362,"Improvement Of Barreto-Voloch Algorithm For Computing $r$th Roots Over
  Finite Fields","  Root extraction is a classical problem in computers algebra. It plays an
essential role in cryptosystems based on elliptic curves. In 2006, Barreto and
Voloch proposed an algorithm to compute $r$th roots in ${F}_{q^m} $ for certain
choices of $m$ and $q$. If $r\,||\,q-1$ and $ (m, r)=1, $ they proved that the
complexity of their method is $\widetilde{\mathcal {O}}(r(\log m+\log\log
q)m\log q) $. In this paper, we extend the Barreto-Voloch algorithm to the
general case that $r\,||\,q^m-1$, without the restrictions $r\,||\,q-1$ and
$(m, r)=1 $. We also specify the conditions that the Barreto-Voloch algorithm
can be preferably applied.
"
363,"Fraction-free algorithm for the computation of diagonal forms matrices
  over Ore domains using Gr{\""o}bner bases","  This paper is a sequel to ""Computing diagonal form and Jacobson normal form
of a matrix using Groebner bases"", J. of Symb. Computation, 46 (5), 2011. We
present a new fraction-free algorithm for the computation of a diagonal form of
a matrix over a certain non-commutative Euclidean domain over a computable
field with the help of Gr\""obner bases. This algorithm is formulated in a
general constructive framework of non-commutative Ore localizations of
$G$-algebras (OLGAs). We split the computation of a normal form of a matrix
into the diagonalization and the normalization processes. Both of them can be
made fraction-free. For a matrix $M$ over an OLGA we provide a diagonalization
algorithm to compute $U,V$ and $D$ with fraction-free entries such that $UMV=D$
holds and $D$ is diagonal. The fraction-free approach gives us more information
on the system of linear functional equations and its solutions, than the
classical setup of an operator algebra with rational functions coefficients. In
particular, one can handle distributional solutions together with, say,
meromorphic ones. We investigate Ore localizations of common operator algebras
over $K[x]$ and use them in the unimodularity analysis of transformation
matrices $U,V$. In turn, this allows to lift the isomorphism of modules over an
OLGA Euclidean domain to a polynomial subring of it. We discuss the relation of
this lifting with the solutions of the original system of equations. Moreover,
we prove some new results concerning normal forms of matrices over non-simple
domains. Our implementation in the computer algebra system {\sc
Singular:Plural} follows the fraction-free strategy and shows impressive
performance, compared with methods which directly use fractions. Since we
experience moderate swell of coefficients and obtain simple transformation
matrices, the method we propose is well suited for solving nontrivial practical
problems.
"
364,"Stability of Triangular Decomposition and Comprehensive Triangular
  Decomposition","  A new concept, decomposition-unstable (DU) variety of a parametric polynomial
system, is introduced in this paper and the stabilities of several triangular
decomposition methods, such as characteristic set decomposition, relatively
simplicial decomposition and regular chain decomposition, for parametric
polynomial systems are discussed in detail. The concept leads to a definition
of weakly comprehensive triangular decomposition (WCTD) and a new algorithm for
computing comprehensive triangular decomposition (CTD) which was first
introduced in [4] for computing an analogue of comprehensive Groebner systems
for parametric polynomial systems. Our algorithm takes advantage of a
hierarchical solving strategy and a self-adaptive order of parameters. The
algorithm has been implemented with Maple 15 and experimented with a number of
benchmarks from the literature. Comparison with the Maple package
RegularChains, which contains an implementation of the algorithm in [4], is
provided and the results illustrate that the time costs by our program for
computing CTDs of most examples are no more than those by RegularChains.
"
365,"Generating Loop Invariants by Computing Vanishing Ideals of Sample
  Points","  Loop invariants play a very important role in proving correctness of
programs. In this paper, we address the problem of generating invariants of
polynomial loop programs. We present a new approach, for generating polynomial
equation invariants of polynomial loop programs through computing vanishing
ideals of sample points. We apply rational function interpolation, based on
early termination technique, to generate invariants of loop programs with
symbolic initial values. Our approach avoids first-order quantifier elimination
and cylindrical algebraic decomposition(CAD). An algorithm for generating
polynomial invariants is proposed and some examples are given to illustrate the
algorithm. Furthermore, we demonstrate on a set of loop programs with symbolic
initial values that our algorithm can yield polynomial invariants with degrees
high up to 15.
"
366,Sparse Differential Resultant for Laurent Differential Polynomials,"  In this paper, we first introduce the concept of Laurent differentially
essential systems and give a criterion for Laurent differentially essential
systems in terms of their supports. Then the sparse differential resultant for
a Laurent differentially essential system is defined and its basic properties
are proved. In particular, order and degree bounds for the sparse differential
resultant are given. Based on these bounds, an algorithm to compute the sparse
differential resultant is proposed, which is single exponential in terms of the
number of indeterminates, the Jacobi number of the system, and the size of the
system.
"
367,Adleman-Manders-Miller Root Extraction Method Revisited,"  In 1977, Adleman, Manders and Miller had briefly described how to extend
their square root extraction method to the general $r$th root extraction over
finite fields, but not shown enough details. Actually, there is a dramatic
difference between the square root extraction and the general $r$th root
extraction because one has to solve discrete logarithms for $r$th root
extraction. In this paper, we clarify their method and analyze its complexity.
Our heuristic presentation is helpful to grasp the method entirely and deeply.
"
368,Advanced Computer Algebra for Determinants,"  We prove three conjectures concerning the evaluation of determinants, which
are related to the counting of plane partitions and rhombus tilings. One of
them was posed by George Andrews in 1980, the other two were by Guoce Xin and
Christian Krattenthaler. Our proofs employ computer algebra methods, namely,
the holonomic ansatz proposed by Doron Zeilberger and variations thereof. These
variations make Zeilberger's original approach even more powerful and allow for
addressing a wider variety of determinants. Finally, we present, as a challenge
problem, a conjecture about a closed-form evaluation of Andrews's determinant.
"
369,Mixed Discriminants,"  The mixed discriminant of n Laurent polynomials in n variables is the
irreducible polynomial in the coefficients which vanishes whenever two of the
roots coincide. The Cayley trick expresses the mixed discriminant as an
A-discriminant. We show that the degree of the mixed discriminant is a
piecewise linear function in the Plucker coordinates of a mixed Grassmannian.
An explicit degree formula is given for the case of plane curves.
"
370,Lie algebra conjugacy,"  We study the problem of matrix Lie algebra conjugacy. Lie algebras arise
centrally in areas as diverse as differential equations, particle physics,
group theory, and the Mulmuley--Sohoni Geometric Complexity Theory program. A
matrix Lie algebra is a set L of matrices such that $A, B\in L$ implies $AB -
BA \in L$. Two matrix Lie algebras are conjugate if there is an invertible
matrix $M$ such that $L_1 = M L_2 M^{-1}$.
  We show that certain cases of Lie algebra conjugacy are equivalent to graph
isomorphism. On the other hand, we give polynomial-time algorithms for other
cases of Lie algebra conjugacy, which allow us to essentially derandomize a
recent result of Kayal on affine equivalence of polynomials. Affine equivalence
is related to many complexity problems such as factoring integers, graph
isomorphism, matrix multiplication, and permanent versus determinant.
  Specifically, we show:
  Abelian Lie algebra conjugacy is equivalent to the code equivalence problem,
and hence is as hard as graph isomorphism.
  Abelian Lie algebra conjugacy of $n \times n$ matrices can be solved in
poly(n) time when the Lie algebras have dimension O(1).
  Semisimple Lie algebra conjugacy is equivalent to graph isomorphism. A Lie
algebra is semisimple if it is a direct sum of simple Lie algebras.
  Semisimple Lie algebra conjugacy of $n \times n$ matrices can be solved in
polynomial time when the Lie algebras consist of only $O(\log n)$ simple direct
summands.
  Conjugacy of completely reducible Lie algebras---that is, a direct sum of an
abelian and a semisimple Lie algebra---can be solved in polynomial time when
the abelian part has dimension O(1) and the semisimple part has $O(\log n)$
simple direct summands.
"
371,The fundamental invariants of 3 x 3 x 3 arrays,"  We determine the three fundamental invariants in the entries of a $3 \times 3
\times 3$ array over $\mathbb{C}$ as explicit polynomials in the 27 variables
$x_{ijk}$ for $1 \le i, j, k \le 3$. By the work of Vinberg on $\theta$-groups,
it is known that these homogeneous polynomials have degrees 6, 9 and 12; they
freely generate the algebra of invariants for the Lie group $SL_3(\mathbb{C})
\times SL_3(\mathbb{C}) \times SL_3(\mathbb{C})$ acting irreducibly on its
natural representation $\mathbb{C}^3 \otimes \mathbb{C}^3 \otimes
\mathbb{C}^3$. These generators have respectively 1152, 9216 and 209061 terms;
we find compact expressions in terms of the orbits of the finite group $(S_3
\times S_3 \times S_3) \rtimes S_3$ acting on monomials of weight zero for the
action of the Lie algebra $\mathfrak{sl}_3(\mathbb{C}) \oplus
\mathfrak{sl}_3(\mathbb{C}) \oplus \mathfrak{sl}_3(\mathbb{C})$.
"
372,Note on fast division algorithm for polynomials using Newton iteration,"  The classical division algorithm for polynomials requires $O(n^2)$ operations
for inputs of size $n$. Using reversal technique and Newton iteration, it can
be improved to $O({M}(n))$, where ${M}$ is a multiplication time. But the
method requires that the degree of the modulo, $x^l$, should be the power of 2.
If $l$ is not a power of 2 and $f(0)=1$, Gathen and Gerhard suggest to compute
the inverse,$f^{-1}$, modulo $x^{\lceil l/2^r\rceil}, x^{\lceil
l/2^{r-1}\rceil},..., x^{\lceil l/2\rceil}, x^l$, separately. But they did not
specify the iterative step. In this note, we show that the original Newton
iteration formula can be directly used to compute $f^{-1}\,{mod}\,x^{l}$
without any additional cost, when $l$ is not a power of 2.
"
373,On the Complexity of the Generalized MinRank Problem,"  We study the complexity of solving the \emph{generalized MinRank problem},
i.e. computing the set of points where the evaluation of a polynomial matrix
has rank at most $r$. A natural algebraic representation of this problem gives
rise to a \emph{determinantal ideal}: the ideal generated by all minors of size
$r+1$ of the matrix. We give new complexity bounds for solving this problem
using Gr\""obner bases algorithms under genericity assumptions on the input
matrix. In particular, these complexity bounds allow us to identify families of
generalized MinRank problems for which the arithmetic complexity of the solving
process is polynomial in the number of solutions. We also provide an algorithm
to compute a rational parametrization of the variety of a 0-dimensional and
radical system of bi-degree $(D,1)$. We show that its complexity can be bounded
by using the complexity bounds for the generalized MinRank problem.
"
374,"Complexity and Algorithms for Euler Characteristic of Simplicial
  Complexes","  We consider the problem of computing the Euler characteristic of an abstract
simplicial complex given by its vertices and facets. We show that this problem
is #P-complete and present two new practical algorithms for computing Euler
characteristic. The two new algorithms are derived using combinatorial
commutative algebra and we also give a second description of them that requires
no algebra. We present experiments showing that the two new algorithms can be
implemented to be faster than previous Euler characteristic implementations by
a large margin.
"
375,Abstracting Path Conditions for Effective Symbolic Execution,"  We present an algorithm for tests generation tools based on symbolic
execution. The algorithm is supposed to help in situations, when a tool is
repeatedly failing to cover some code by tests. The algorithm then provides the
tool a necessary condition strongly narrowing space of program paths, which
must be checked for reaching the uncovered code. We also discuss integration of
the algorithm into the tools and we provide experimental results showing a
potential of the algorithm to be valuable in the tools, when properly
implemented there.
"
376,Computing zeta functions of sparse nondegenerate hypersurfaces,"  Using the cohomology theory of Dwork, as developed by Adolphson and Sperber,
we exhibit a deterministic algorithm to compute the zeta function of a
nondegenerate hypersurface defined over a finite field. This algorithm is
particularly well-suited to work with polynomials in small characteristic that
have few monomials (relative to their dimension). Our method covers toric,
affine, and projective hypersurfaces and also can be used to compute the
L-function of an exponential sum.
"
377,"Rank-profile revealing Gaussian elimination and the CUP matrix
  decomposition","  Transforming a matrix over a field to echelon form, or decomposing the matrix
as a product of structured matrices that reveal the rank profile, is a
fundamental building block of computational exact linear algebra. This paper
surveys the well known variations of such decompositions and transformations
that have been proposed in the literature. We present an algorithm to compute
the CUP decomposition of a matrix, adapted from the LSP algorithm of Ibarra,
Moran and Hui (1982), and show reductions from the other most common Gaussian
elimination based matrix transformations and decompositions to the CUP
decomposition. We discuss the advantages of the CUP algorithm over other
existing algorithms by studying time and space complexities: the asymptotic
time complexity is rank sensitive, and comparing the constants of the leading
terms, the algorithms for computing matrix invariants based on the CUP
decomposition are always at least as good except in one case. We also show that
the CUP algorithm, as well as the computation of other invariants such as
transformation to reduced column echelon form using the CUP algorithm, all work
in place, allowing for example to compute the inverse of a matrix on the same
storage as the input matrix.
"
378,On the Complexity of Solving Quadratic Boolean Systems,"  A fundamental problem in computer science is to find all the common zeroes of
$m$ quadratic polynomials in $n$ unknowns over $\mathbb{F}_2$. The
cryptanalysis of several modern ciphers reduces to this problem. Up to now, the
best complexity bound was reached by an exhaustive search in $4\log_2 n\,2^n$
operations. We give an algorithm that reduces the problem to a combination of
exhaustive search and sparse linear algebra. This algorithm has several
variants depending on the method used for the linear algebra step. Under
precise algebraic assumptions on the input system, we show that the
deterministic variant of our algorithm has complexity bounded by
$O(2^{0.841n})$ when $m=n$, while a probabilistic variant of the Las Vegas type
has expected complexity $O(2^{0.792n})$. Experiments on random systems show
that the algebraic assumptions are satisfied with probability very close to~1.
We also give a rough estimate for the actual threshold between our method and
exhaustive search, which is as low as~200, and thus very relevant for
cryptographic applications.
"
379,"Computational Tutorial on Gr\""obner bases embedding Sage in LaTeX with
  SageTEX","  Elementary tutorial on implementation aspects of Gr\""obner bases computation.
"
380,Exact Symbolic-Numeric Computation of Planar Algebraic Curves,"  We present a novel certified and complete algorithm to compute arrangements
of real planar algebraic curves. It provides a geometric-topological analysis
of the decomposition of the plane induced by a finite number of algebraic
curves in terms of a cylindrical algebraic decomposition. From a high-level
perspective, the overall method splits into two main subroutines, namely an
algorithm denoted Bisolve to isolate the real solutions of a zero-dimensional
bivariate system, and an algorithm denoted GeoTop to analyze a single algebraic
curve.
  Compared to existing approaches based on elimination techniques, we
considerably improve the corresponding lifting steps in both subroutines. As a
result, generic position of the input system is never assumed, and thus our
algorithm never demands for any change of coordinates. In addition, we
significantly limit the types of involved exact operations, that is, we only
use resultant and gcd computations as purely symbolic operations. The latter
results are achieved by combining techniques from different fields such as
(modular) symbolic computation, numerical analysis and algebraic geometry.
  We have implemented our algorithms as prototypical contributions to the
C++-project CGAL. They exploit graphics hardware to expedite the symbolic
computations. We have also compared our implementation with the current
reference implementations, that is, LGP and Maple's Isolate for polynomial
system solving, and CGAL's bivariate algebraic kernel for analyses and
arrangement computations of algebraic curves. For various series of challenging
instances, our exhaustive experiments show that the new implementations
outperform the existing ones.
"
381,Telescopers for Rational and Algebraic Functions via Residues,"  We show that the problem of constructing telescopers for functions of m
variables is equivalent to the problem of constructing telescopers for
algebraic functions of m -1 variables and present a new algorithm to construct
telescopers for algebraic functions of two variables. These considerations are
based on analyzing the residues of the input. According to experiments, the
resulting algorithm for rational functions of three variables is faster than
known algorithms, at least in some examples of combinatorial interest. The
algorithm for algebraic functions implies a new bound on the order of the
telescopers.
"
382,Order-Degree Curves for Hypergeometric Creative Telescoping,"  Creative telescoping applied to a bivariate proper hypergeometric term
produces linear recurrence operators with polynomial coefficients, called
telescopers. We provide bounds for the degrees of the polynomials appearing in
these operators. Our bounds are expressed as curves in the (r,d)-plane which
assign to every order r a bound on the degree d of the telescopers. These
curves are hyperbolas, which reflect the phenomenon that higher order
telescopers tend to have lower degree, and vice versa.
"
383,"An algorithm to compute the differential equations for the logarithm of
  a polynomial","  We present an algorithm to compute the annihilator of (i.e., the linear
differential equations for) the logarithm of a polynomial in the ring of
differential operators with polynomial coefficients. The algorithm consists of
differentiation with respect to the parameter s of the annihilator of f^s for a
polynomial f and quotient computation. More generally, the annihilator of
f^s(log f)^m for a complex number s and a positive integer m can be computed,
which constitutes what is called a holonomic system in D-module theory. This
enables us to compute a holonomic system for the integral of a function
involving the logarithm of a polynomial by using integration algorithm for
D-modules.
"
384,Twisting q-holonomic sequences by complex roots of unity,"  A sequence $f_n(q)$ is $q$-holonomic if it satisfies a nontrivial linear
recurrence with coefficients polynomials in $q$ and $q^n$. Our main theorems
state that $q$-holonomicity is preserved under twisting, i.e., replacing $q$ by
$\omega q$ where $\omega$ is a complex root of unity, and under the
substitution $q \to q^{\alpha}$ where $\alpha$ is a rational number. Our proofs
are constructive, work in the multivariate setting of $\partial$-finite
sequences and are implemented in the Mathematica package HolonomicFunctions.
Our results are illustrated by twisting natural $q$-holonomic sequences which
appear in quantum topology, namely the colored Jones polynomial of pretzel
knots and twist knots. The recurrence of the twisted colored Jones polynomial
can be used to compute the asymptotics of the Kashaev invariant of a knot at an
arbitrary complex root of unity.
"
385,Computing Puiseux Series for Algebraic Surfaces,"  In this paper we outline an algorithmic approach to compute Puiseux series
expansions for algebraic surfaces. The series expansions originate at the
intersection of the surface with as many coordinate planes as the dimension of
the surface. Our approach starts with a polyhedral method to compute cones of
normal vectors to the Newton polytopes of the given polynomial system that
defines the surface. If as many vectors in the cone as the dimension of the
surface define an initial form system that has isolated solutions, then those
vectors are potential tropisms for the initial term of the Puiseux series
expansion. Our preliminary methods produce exact representations for solution
sets of the cyclic $n$-roots problem, for $n = m^2$, corresponding to a result
of Backelin.
"
386,On the Shape of Curves that are Rational in Polar Coordinates,"  In this paper we provide a computational approach to the shape of curves
which are rational in polar coordinates, i.e. which are defined by means of a
parametrization (r(t),\theta(t)) where both r(t),\theta(t) are rational
functions. Our study includes theoretical aspects on the shape of these curves,
and algorithmic results which eventually lead to an algorithm for plotting the
""interesting parts"" of the curve, i.e. the parts showing the main geometrical
features of it. On the theoretical side, we prove that these curves, with the
exceptions of lines and circles, cannot be algebraic (in cartesian
coordinates), we characterize the existence of infinitely many
self-intersections, and we connect this with certain phenomena which are not
possible in the algebraic world, namely the existence of limit circles, limit
points, or spiral branches. On the practical side, we provide an algorithm
which has been implemented in the computer algebra system Maple to visualize
this kind of curves. Our implementation makes use (and improves some aspects
of) the command polarplot currently available in Maple for plotting curves in
polar form.
"
387,"Exact Safety Verification of Hybrid Systems Based on Bilinear SOS
  Representation","  In this paper, we address the problem of safety verification of nonlinear
hybrid systems. A hybrid symbolic-numeric method is presented to compute exact
inequality invariants of hybrid systems efficiently. Some numerical invariants
of a hybrid system can be obtained by solving a bilinear SOS programming via
PENBMI solver or iterative method, then the modified Newton refinement and
rational vector recovery techniques are applied to obtain exact polynomial
invariants with rational coefficients, which {\it exactly} satisfy the
conditions of invariants. Experiments on some benchmarks are given to
illustrate the efficiency of our algorithm.
"
388,Generating Program Invariants via Interpolation,"  This article focuses on automatically generating polynomial equations that
are inductive loop invariants of computer programs. We propose a new algorithm
for this task, which is based on polynomial interpolation. Though the proposed
algorithm is not complete, it is efficient and can be applied to a broader
range of problems compared to existing methods targeting similar problems. The
efficiency of our approach is testified by experiments on a large collection of
programs. The current implementation of our method is based on dense
interpolation, for which a total degree bound is needed. On the theoretical
front, we study the degree and dimension of the invariant ideal of loops which
have no branches and where the assignments define a P-solvable recurrence. In
addition, we obtain sufficient conditions for non-trivial polynomial equation
invariants to exist (resp. not to exist).
"
389,Zeilberger's Holonomic Ansatz for Pfaffians,"  A variation of Zeilberger's holonomic ansatz for symbolic determinant
evaluations is proposed which is tailored to deal with Pfaffians. The method is
also applicable to determinants of skew-symmetric matrices, for which the
original approach does not work. As Zeilberger's approach is based on the
Laplace expansion (cofactor expansion) of the determinant, we derive our
approach from the cofactor expansion of the Pfaffian. To demonstrate the power
of our method, we prove, using computer algebra algorithms, some conjectures
proposed in the paper ""Pfaffian decomposition and a Pfaffian analogue of
q-Catalan Hankel determinants"" by Ishikawa, Tagawa, and Zeng. A minor summation
formula related to partitions and Motzkin paths follows as a corollary.
"
390,Fast Computation of Smith Forms of Sparse Matrices Over Local Rings,"  We present algorithms to compute the Smith Normal Form of matrices over two
families of local rings.
  The algorithms use the \emph{black-box} model which is suitable for sparse
and structured matrices. The algorithms depend on a number of tools, such as
matrix rank computation over finite fields, for which the best-known time- and
memory-efficient algorithms are probabilistic.
  For an $\nxn$ matrix $A$ over the ring $\Fzfe$, where $f^e$ is a power of an
irreducible polynomial $f \in \Fz$ of degree $d$, our algorithm requires
$\bigO(\eta de^2n)$ operations in $\F$, where our black-box is assumed to
require $\bigO(\eta)$ operations in $\F$ to compute a matrix-vector product by
a vector over $\Fzfe$ (and $\eta$ is assumed greater than $\Pden$). The
algorithm only requires additional storage for $\bigO(\Pden)$ elements of $\F$.
In particular, if $\eta=\softO(\Pden)$, then our algorithm requires only
$\softO(n^2d^2e^3)$ operations in $\F$, which is an improvement on known dense
methods for small $d$ and $e$.
  For the ring $\ZZ/p^e\ZZ$, where $p$ is a prime, we give an algorithm which
is time- and memory-efficient when the number of nontrivial invariant factors
is small. We describe a method for dimension reduction while preserving the
invariant factors. The time complexity is essentially linear in $\mu n r e \log
p,$ where $\mu$ is the number of operations in $\ZZ/p\ZZ$ to evaluate the
black-box (assumed greater than $n$) and $r$ is the total number of non-zero
invariant factors.
  To avoid the practical cost of conditioning, we give a Monte Carlo
certificate, which at low cost, provides either a high probability of success
or a proof of failure. The quest for a time- and memory-efficient solution
without restrictions on the number of nontrivial invariant factors remains
open. We offer a conjecture which may contribute toward that end.
"
391,A General Solver Based on Sparse Resultants,"  Sparse (or toric) elimination exploits the structure of polynomials by
measuring their complexity in terms of Newton polytopes instead of total
degree. The sparse, or Newton, resultant generalizes the classical homogeneous
resultant and its degree is a function of the mixed volumes of the Newton
polytopes. We sketch the sparse resultant constructions of Canny and Emiris and
show how they reduce the problem of root-finding to an eigenproblem. A novel
method for achieving this reduction is presented which does not increase the
dimension of the problem. Together with an implementation of the sparse
resultant construction, it provides a general solver for polynomial systems. We
discuss the overall implementation and illustrate its use by applying it to
concrete problems from vision, robotics and structural biology. The high
efficiency and accuracy of the solutions suggest that sparse elimination may be
the method of choice for systems of moderate size.
"
392,"An iterative algorithm for parametrization of shortest length shift
  registers over finite rings","  The construction of shortest feedback shift registers for a finite sequence
S_1,...,S_N is considered over the finite ring Z_{p^r}. A novel algorithm is
presented that yields a parametrization of all shortest feedback shift
registers for the sequence of numbers S_1,...,S_N, thus solving an open problem
in the literature. The algorithm iteratively processes each number, starting
with S_1, and constructs at each step a particular type of minimal Gr\""obner
basis. The construction involves a simple update rule at each step which leads
to computational efficiency. It is shown that the algorithm simultaneously
computes a similar parametrization for the reciprocal sequence S_N,...,S_1.
"
393,"List Decoding Algorithms based on Groebner Bases for General One-Point
  AG Codes","  We generalize the list decoding algorithm for Hermitian codes proposed by Lee
and O'Sullivan based on Gr\""obner bases to general one-point AG codes, under an
assumption weaker than one used by Beelen and Brander. By using the same
principle, we also generalize the unique decoding algorithm for one-point AG
codes over the Miura-Kamiya $C_{ab}$ curves proposed by Lee, Bras-Amor\'os and
O'Sullivan to general one-point AG codes, without any assumption. Finally we
extend the latter unique decoding algorithm to list decoding, modify it so that
it can be used with the Feng-Rao improved code construction, prove equality
between its error correcting capability and half the minimum distance lower
bound by Andersen and Geil that has not been done in the original proposal, and
remove the unnecessary computational steps so that it can run faster.
"
394,A baby step-giant step roadmap algorithm for general algebraic sets,"  Let $\mathrm{R}$ be a real closed field and $\mathrm{D} \subset \mathrm{R}$
an ordered domain. We give an algorithm that takes as input a polynomial $Q \in
\mathrm{D}[X_1,\ldots,X_k]$, and computes a description of a roadmap of the set
of zeros, $\mathrm{Zer}(Q,\mathrm{R}^k)$, of $Q$ in $\mathrm{R}^k$. The
complexity of the algorithm, measured by the number of arithmetic operations in
the ordered domain $\mathrm{D}$, is bounded by $d^{O(k \sqrt{k})}$, where $d =
\mathrm{deg}(Q)\ge 2$. As a consequence, there exist algorithms for computing
the number of semi-algebraically connected components of a real algebraic set,
$\mathrm{Zer}(Q,\mathrm{R}^k)$, whose complexity is also bounded by $d^{O(k
\sqrt{k})}$, where $d = \mathrm{deg}(Q)\ge 2$. The best previously known
algorithm for constructing a roadmap of a real algebraic subset of
$\mathrm{R}^k$ defined by a polynomial of degree $d$ has complexity
$d^{O(k^2)}$.
"
395,"Critical Points and Gr\""obner Bases: the Unmixed Case","  We consider the problem of computing critical points of the restriction of a
polynomial map to an algebraic variety. This is of first importance since the
global minimum of such a map is reached at a critical point. Thus, these points
appear naturally in non-convex polynomial optimization which occurs in a wide
range of scientific applications (control theory, chemistry, economics,...).
Critical points also play a central role in recent algorithms of effective real
algebraic geometry. Experimentally, it has been observed that Gr\""obner basis
algorithms are efficient to compute such points. Therefore, recent software
based on the so-called Critical Point Method are built on Gr\""obner bases
engines. Let $f_1,..., f_p$ be polynomials in $ \Q[x_1,..., x_n]$ of degree
$D$, $V\subset\C^n$ be their complex variety and $\pi_1$ be the projection map
$(x_1,.., x_n)\mapsto x_1$. The critical points of the restriction of $\pi_1$
to $V$ are defined by the vanishing of $f_1,..., f_p$ and some maximal minors
of the Jacobian matrix associated to $f_1,..., f_p$. Such a system is
algebraically structured: the ideal it generates is the sum of a determinantal
ideal and the ideal generated by $f_1,..., f_p$. We provide the first
complexity estimates on the computation of Gr\""obner bases of such systems
defining critical points. We prove that under genericity assumptions on
$f_1,..., f_p$, the complexity is polynomial in the generic number of critical
points, i.e. $D^p(D-1)^{n-p}{{n-1}\choose{p-1}}$. More particularly, in the
quadratic case D=2, the complexity of such a Gr\""obner basis computation is
polynomial in the number of variables $n$ and exponential in $p$. We also give
experimental evidence supporting these theoretical results.
"
396,"Counting and computing regions of $D$-decomposition: algebro-geometric
  approach","  New methods for $D$-decomposition analysis are presented. They are based on
topology of real algebraic varieties and computational real algebraic geometry.
The estimate of number of root invariant regions for polynomial parametric
families of polynomial and matrices is given. For the case of two parametric
family more sharp estimate is proven. Theoretic results are supported by
various numerical simulations that show higher precision of presented methods
with respect to traditional ones. The presented methods are inherently global
and could be applied for studying $D$-decomposition for the space of parameters
as a whole instead of some prescribed regions. For symbolic computations the
Maple v.14 software and its package RegularChains are used.
"
397,Evaluation of Multi-Sums for Large Scale Problems,"  A big class of Feynman integrals, in particular, the coefficients of their
Laurent series expansion w.r.t.\ the dimension parameter $\ep$ can be
transformed to multi-sums over hypergeometric terms and harmonic sums. In this
article, we present a general summation method based on difference fields that
simplifies these multi--sums by transforming them from inside to outside to
representations in terms of indefinite nested sums and products. In particular,
we present techniques that assist in the task to simplify huge expressions of
such multi-sums in a completely automatic fashion. The ideas are illustrated on
new calculations coming from 3-loop topologies of gluonic massive operator
matrix elements containing two fermion lines, which contribute to the
transition matrix elements in the variable flavor scheme.
"
398,Computable Hilbert Schemes,"  In this PhD thesis we propose an algorithmic approach to the study of the
Hilbert scheme. Developing algorithmic methods, we also obtain general results
about Hilbert schemes. In Chapter 1 we discuss the equations defining the
Hilbert scheme as subscheme of a suitable Grassmannian and in Chapter 5 we
determine a new set of equations of degree lower than the degree of equations
known so far. In Chapter 2 we study the most important objects used to project
algorithmic techniques, namely Borel-fixed ideals. We determine an algorithm
computing all the saturated Borel-fixed ideals with Hilbert polynomial assigned
and we investigate their combinatorial properties. In Chapter 3 we show a new
type of flat deformations of Borel-fixed ideals which lead us to give a new
proof of the connectedness of the Hilbert scheme. In Chapter 4 we construct
families of ideals that generalize the notion of family of ideals sharing the
same initial ideal with respect to a fixed term ordering. Some of these
families correspond to open subsets of the Hilbert scheme and can be used to a
local study of the Hilbert scheme. In Chapter 6 we deal with the problem of the
connectedness of the Hilbert scheme of locally Cohen-Macaulay curves in the
projective 3-space. We show that one of the Hilbert scheme considered a ""good""
candidate to be non-connected, is instead connected. Moreover there are three
appendices that present and explain how to use the implementations of the
algorithms proposed.
"
399,Proceedings First Workshop on CTP Components for Educational Software,"  The THedu'11 workshop received thirteen submissions, twelve of which were
accepted and presented during the workshop. For the post-conference proceedings
nine submission where received and accepted. The submissions are within the
scope of the following points, which have been announced in the call of papers:
CTP-based software tools for education; CTP technology combined with novel
interfaces, drag and drop, etc.; technologies to access ITP knowledge relevant
for a certain step of problem solving; usability considerations on representing
ITP knowledge; combination of deduction and computation; formal problem
specifications; effectiveness of ATP in checking user input; formats for
deductive content in proof documents, geometric constructions, etc; formal
domain models for e-learning in mathematics and applications.
"
400,Towards an Intelligent Tutor for Mathematical Proofs,"  Computer-supported learning is an increasingly important form of study since
it allows for independent learning and individualized instruction. In this
paper, we discuss a novel approach to developing an intelligent tutoring system
for teaching textbook-style mathematical proofs. We characterize the
particularities of the domain and discuss common ITS design models. Our
approach is motivated by phenomena found in a corpus of tutorial dialogs that
were collected in a Wizard-of-Oz experiment. We show how an intelligent tutor
for textbook-style mathematical proofs can be built on top of an adapted
assertion-level proof assistant by reusing representations and proof search
strategies originally developed for automated and interactive theorem proving.
The resulting prototype was successfully evaluated on a corpus of tutorial
dialogs and yields good results.
"
401,Automatic Deduction in Dynamic Geometry using Sage,"  We present a symbolic tool that provides robust algebraic methods to handle
automatic deduction tasks for a dynamic geometry construction. The main
prototype has been developed as two different worksheets for the open source
computer algebra system Sage, corresponding to two different ways of coding a
geometric construction. In one worksheet, diagrams constructed with the open
source dynamic geometry system GeoGebra are accepted. In this worksheet,
Groebner bases are used to either compute the equation of a geometric locus in
the case of a locus construction or to determine the truth of a general
geometric statement included in the GeoGebra construction as a boolean
variable. In the second worksheet, locus constructions coded using the common
file format for dynamic geometry developed by the Intergeo project are accepted
for computation. The prototype and several examples are provided for testing.
Moreover, a third Sage worksheet is presented in which a novel algorithm to
eliminate extraneous parts in symbolically computed loci has been implemented.
The algorithm, based on a recent work on the Groebner cover of parametric
systems, identifies degenerate components and extraneous adherence points in
loci, both natural byproducts of general polynomial algebraic methods. Detailed
examples are discussed.
"
402,Formalization and Implementation of Algebraic Methods in Geometry,"  We describe our ongoing project of formalization of algebraic methods for
geometry theorem proving (Wu's method and the Groebner bases method), their
implementation and integration in educational tools. The project includes
formal verification of the algebraic methods within Isabelle/HOL proof
assistant and development of a new, open-source Java implementation of the
algebraic methods. The project should fill-in some gaps still existing in this
area (e.g., the lack of formal links between algebraic methods and synthetic
geometry and the lack of self-contained implementations of algebraic methods
suitable for integration with dynamic geometry tools) and should enable new
applications of theorem proving in education.
"
403,"Computer-Assisted Program Reasoning Based on a Relational Semantics of
  Programs","  We present an approach to program reasoning which inserts between a program
and its verification conditions an additional layer, the denotation of the
program expressed in a declarative form. The program is first translated into
its denotation from which subsequently the verification conditions are
generated. However, even before (and independently of) any verification
attempt, one may investigate the denotation itself to get insight into the
""semantic essence"" of the program, in particular to see whether the denotation
indeed gives reason to believe that the program has the expected behavior.
Errors in the program and in the meta-information may thus be detected and
fixed prior to actually performing the formal verification. More concretely,
following the relational approach to program semantics, we model the effect of
a program as a binary relation on program states. A formal calculus is devised
to derive from a program a logic formula that describes this relation and is
subject for inspection and manipulation. We have implemented this idea in a
comprehensive form in the RISC ProgramExplorer, a new program reasoning
environment for educational purposes which encompasses the previously developed
RISC ProofNavigator as an interactive proving assistant.
"
404,Compositions and collisions at degree p^2,"  A univariate polynomial f over a field is decomposable if f = g o h = g(h)
for nonlinear polynomials g and h. In order to count the decomposables, one
wants to know, under a suitable normalization, the number of equal-degree
collisions of the form f = g o h = g^* o h^* with (g, h) = (g^*, h^*) and deg g
= deg g^*. Such collisions only occur in the wild case, where the field
characteristic p divides deg f. Reasonable bounds on the number of
decomposables over a finite field are known, but they are less sharp in the
wild case, in particular for degree p^2.
  We provide a classification of all polynomials of degree p^2 with a
collision. It yields the exact number of decomposable polynomials of degree p^2
over a finite field of characteristic p. We also present an efficient algorithm
that determines whether a given polynomial of degree p^2 has a collision or
not.
"
405,"Effective Differential L\""uroth's Theorem","  This paper focuses on effectivity aspects of the L\""uroth's theorem in
differential fields. Let $\mathcal{F}$ be an ordinary differential field of
characteristic 0 and $\mathcal{F}<u>$ be the field of differential rational
functions generated by a single indeterminate $u$. Let be given non constant
rational functions $v_1,...,v_n\in \mathcal{F}<u>$ generating a differential
subfield $\mathcal{G}\subseteq \mathcal{F}<e u>$. The differential L\""uroth's
theorem proved by Ritt in 1932 states that there exists $v\in \mathcal G$ such
that $\mathcal{G}= \mathcal{F}<v>$. Here we prove that the total order and
degree of a generator $v$ are bounded by $\min_j \textrm{ord} (v_j)$ and
$(nd(e+1)+1)^{2e+1}$, respectively, where $e:=\max_j \textrm{ord} (v_j)$ and
$d:=\max_j \textrm{deg} (v_j)$. As a byproduct, our techniques enable us to
compute a L\""uroth generator by dealing with a polynomial ideal in a polynomial
ring in finitely many variables.
"
406,"On the asymptotic and practical complexity of solving bivariate systems
  over the reals","  This paper is concerned with exact real solving of well-constrained,
bivariate polynomial systems. The main problem is to isolate all common real
roots in rational rectangles, and to determine their intersection
multiplicities. We present three algorithms and analyze their asymptotic bit
complexity, obtaining a bound of $\sOB(N^{14})$ for the purely projection-based
method, and $\sOB(N^{12})$ for two subresultant-based methods: this notation
ignores polylogarithmic factors, where $N$ bounds the degree and the bitsize of
the polynomials. The previous record bound was $\sOB(N^{14})$.
  Our main tool is signed subresultant sequences. We exploit recent advances on
the complexity of univariate root isolation, and extend them to sign evaluation
of bivariate polynomials over two algebraic numbers, and real root counting for
polynomials over an extension field. Our algorithms apply to the problem of
simultaneous inequalities; they also compute the topology of real plane
algebraic curves in $\sOB(N^{12})$, whereas the previous bound was
$\sOB(N^{14})$.
  All algorithms have been implemented in MAPLE, in conjunction with numeric
filtering. We compare them against FGB/RS, system solvers from SYNAPS, and
MAPLE libraries INSULATE and TOP, which compute curve topology. Our software is
among the most robust, and its runtimes are comparable, or within a small
constant factor, with respect to the C/C++ libraries.
  Key words: real solving, polynomial systems, complexity, MAPLE software
"
407,"Can the Eureqa symbolic regression program, computer algebra and
  numerical analysis help each other?","  The Eureqa symbolic regression program has recently received extensive press
praise. A representative quote is
  ""There are very clever 'thinking machines' in existence today, such as
Watson, the IBM computer that conquered Jeopardy! last year. But next to
Eureqa, Watson is merely a glorified search engine.""
  The program was designed to work with noisy experimental data. However, if
the data is generated from an expression for which there exists more concise
equivalent expressions, sometimes some of the Eureqa results are one or more of
those more concise equivalents. If not, perhaps one or more of the returned
Eureqa results might be a sufficiently accurate approximation that is more
concise than the given expression. Moreover, when there is no known closed form
expression, the data points can be generated by numerical methods, enabling
Eureqa to find expressions that concisely fit those data points with sufficient
accuracy. In contrast to typical regression software, the user does not have to
explicitly or implicitly provide a specific expression or class of expressions
containiing unknown constants for the software to determine.
  Is Eureqa useful enough in these regards to provide an additional tool for
experimental mathematics, computer algebra users and numerical analysis? Yes if
used carefully. Can computer algebra and numerical methods help Eureqa?
Definitely.
"
408,An Algorithmic Characterization of Polynomial Functions over $Z_{p^n}$,"  In this paper we consider polynomial representability of functions defined
over $Z_{p^n}$, where $p$ is a prime and $n$ is a positive integer. Our aim is
to provide an algorithmic characterization that (i) answers the decision
problem: to determine whether a given function over $Z_{p^n}$ is polynomially
representable or not, and (ii) finds the polynomial if it is polynomially
representable. The previous characterizations given by Kempner (1921) and
Carlitz (1964) are existential in nature and only lead to an exhaustive search
method, i.e., algorithm with complexity exponential in size of the input. Our
characterization leads to an algorithm whose running time is linear in size of
input. We also extend our result to the multivariate case.
"
409,"Subtotal ordering -- a pedagogically advantageous algorithm for
  computing total degree reverse lexicographic order","  Total degree reverse lexicographic order is currently generally regarded as
most often fastest for computing Groebner bases. This article describes an
alternate less mysterious algorithm for computing this order using exponent
subtotals and describes why it should be very nearly the same speed the
traditional algorithm, all other things being equal. However, experimental
evidence suggests that subtotal order is actually slightly faster for the
Mathematica Groebner basis implementation more often than not. This is probably
because the weight vectors associated with the natural subtotal weight matrix
and with the usual total degree reverse lexicographic weight matrix are
different, and Mathematica also uses those the corresponding weight vectors to
help select successive S polynomials and divisor polynomials: Those selection
heuristics appear to work slightly better more often with subtotal weight
vectors.
  However, the most important advantage of exponent subtotals is pedagogical.
It is easier to understand than the total degree reverse lexicographic
algorithm, and it is more evident why the resulting order is often the fastest
known order for computing Groebner bases.
  Keywords: Term order, Total degree reverse lexicographic, tdeg, grevlex,
Groebner basis
"
410,Simplifying products of fractional powers of powers,"  Most computer algebra systems incorrectly simplify (z - z)/(sqrt(w^2)/w^3 -
1/(w*sqrt(w^2))) to 0 rather than to 0/0. The reasons for this are:
  1. The default simplification doesn't succeed in simplifying the denominator
to 0.
  2. There is a rule that 0 is the result of 0 divided by anything that doesn't
simplify to either 0 or 0/0.
  Try it on your computer algebra systems!
  This article describes how to simplify products of the form w^a*(w^b1)^g1 ...
(w^bn)^gn correctly and well, where w is any real or complex expression and the
exponents are rational numbers.
  It might seem that correct good simplification of such a restrictive
expression class must already be published and/or built into at least one
widely used computer-algebra system, but apparently this issue has been
overlooked. Default and relevant optional simplification was tested with 86
examples for n=1 on Derive, Maple, Mathematica, Maxima and TI-CAS. Totaled over
all five systems, 11% of the results were not equivalent to the input
everywhere, 50% of the results did not simplify to 0 a result that was
equivalent to 0, and at least 16% of the results exhibited one or more of four
additional flaw types. There was substantial room for improvement in all five
systems, including the two for which I was a co-author.
  The good news is: These flaws are easy to fix.
"
411,Series misdemeanors,"  Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series --
perhaps generalized to allow coefficients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer-algebra systems
also offer functions that can compute more-general truncated recursive
hierarchical series. However, for all of these kinds of truncated series there
are important implementation details that haven't been addressed before in the
published literature and in current implementations.
  For implementers this article contains ideas for designing more convenient,
correct, and efficient implementations or improving existing ones. For users,
this article is a warning about some of these limitations. Many of the ideas in
this article have been implemented in the computer-algebra within the TI-Nspire
calculator, Windows and Macintosh products.
"
412,Residues and Telescopers for Rational Functions,"  We give necessary and sufficient conditions for the existence of telescopers
for rational functions of two variables in the continuous, discrete and
q-discrete settings and characterize which operators can occur as telescopers.
Using this latter characterization, we reprove results of Furstenberg and
Zeilberger concerning diagonals of power series representing rational
functions. The key concept behind these considerations is a generalization of
the notion of residue in the continuous case to an analogous concept in the
discrete and q-discrete cases.
"
413,Series Crimes,"  Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series.
Some are generalized to allow coefficients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer algebra systems
also have built-in or loadable functions that compute infinite Puiseux series.
Unfortunately, there are some little-known pitfalls in computing Puiseux
series. The most serious of these is expansions within branch cuts or at branch
points that are incorrect for some directions in the complex plane. For example
with each series implementation accessible to you:
  Compare the value of (z^2 + z^3)^(3/2) with that of its truncated series
expansion about z = 0, approximated at z = -0.01. Does the series converge to a
value that is the negative of the correct value?
  Compare the value of ln(z^2 + z^3) with its truncated series expansion about
z = 0, approximated at z = -0.01 + 0.1i. Does the series converge to a value
that is incorrect by 2pi i?
  Compare arctanh(-2 + ln(z)z) with its truncated series expansion about z = 0,
approximated at z = -0.01. Does the series converge to a value that is
incorrect by about pi i?
  At the time of this writing, most implementations that accommodate such
series exhibit such errors. This article describes how to avoid these errors
both for manual derivation of series and when implementing series packages.
"
414,"A ""Hybrid"" Approach for Synthesizing Optimal Controllers of Hybrid
  Systems: A Case Study of the Oil Pump Industrial Example","  In this paper, we propose an approach to reduce the optimal controller
synthesis problem of hybrid systems to quantifier elimination; furthermore, we
also show how to combine quantifier elimination with numerical computation in
order to make it more scalable but at the same time, keep arising errors due to
discretization manageable and within bounds. A major advantage of our approach
is not only that it avoids errors due to numerical computation, but it also
gives a better optimal controller. In order to illustrate our approach, we use
the real industrial example of an oil pump provided by the German company HYDAC
within the European project Quasimodo as a case study throughout this paper,
and show that our method improves (up to 7.5%) the results reported in [3]
based on game theory and model checking.
"
415,"List Decoding Algorithm based on Voting in Groebner Bases for General
  One-Point AG Codes","  We generalize the unique decoding algorithm for one-point AG codes over the
Miura-Kamiya Cab curves proposed by Lee, Bras-Amor\'os and O'Sullivan (2012) to
general one-point AG codes, without any assumption. We also extend their unique
decoding algorithm to list decoding, modify it so that it can be used with the
Feng-Rao improved code construction, prove equality between its error
correcting capability and half the minimum distance lower bound by Andersen and
Geil (2008) that has not been done in the original proposal except for
one-point Hermitian codes, remove the unnecessary computational steps so that
it can run faster, and analyze its computational complexity in terms of
multiplications and divisions in the finite field. As a unique decoding
algorithm, the proposed one is empirically and theoretically as fast as the BMS
algorithm for one-point Hermitian codes. As a list decoding algorithm,
extensive experiments suggest that it can be much faster for many moderate
size/usual inputs than the algorithm by Beelen and Brander (2010). It should be
noted that as a list decoding algorithm the proposed method seems to have
exponential worst-case computational complexity while the previous proposals
(Beelen and Brander, 2010; Guruswami and Sudan, 1999) have polynomial ones, and
that the proposed method is expected to be slower than the previous proposals
for very large/special inputs.
"
416,"Generalization of the Lee-O'Sullivan List Decoding for One-Point AG
  Codes","  We generalize the list decoding algorithm for Hermitian codes proposed by Lee
and O'Sullivan based on Gr\""obner bases to general one-point AG codes, under an
assumption weaker than one used by Beelen and Brander. Our generalization
enables us to apply the fast algorithm to compute a Gr\""obner basis of a module
proposed by Lee and O'Sullivan, which was not possible in another
generalization by Lax.
"
417,FORM version 4.0,"  We present version 4.0 of the symbolic manipulation system FORM. The most
important new features are manipulation of rational polynomials and the
factorization of expressions. Many other new functions and commands are also
added; some of them are very general, while others are designed for building
specific high level packages, such as one for Groebner bases. New is also the
checkpoint facility, that allows for periodic backups during long calculations.
Lastly, FORM 4.0 has become available as open source under the GNU General
Public License version 3.
"
418,State Space Exploration of RT Systems in the Cloud,"  The growing availability of distributed and cloud computing frameworks make
it possible to face complex computational problems in a more effective and
convenient way. A notable example is state-space exploration of discrete-event
systems specified in a formal way. The exponential complexity of this task is a
major limitation to the usage of consolidated analysis techniques and tools. We
present and compare two different approaches to state-space explosion, relying
on distributed and cloud frameworks, respectively. These approaches were
designed and implemented following the same computational schema, a sort of map
& fold. They are applied on symbolic state-space exploration of real-time
systems specified by (a timed extension of) Petri Nets, by readapting a
sequential algorithm implemented as a command-line Java tool. The outcome of
several tests performed on a benchmarking specification are presented, thus
showing the convenience of cloud approaches.
"
419,Theory Presentation Combinators,"  We motivate and give semantics to theory presentation combinators as the
foundational building blocks for a scalable library of theories. The key
observation is that the category of contexts and fibered categories are the
ideal theoretical tools for this purpose.
"
420,Faster Algorithms for Rectangular Matrix Multiplication,"  Let {\alpha} be the maximal value such that the product of an n x n^{\alpha}
matrix by an n^{\alpha} x n matrix can be computed with n^{2+o(1)} arithmetic
operations. In this paper we show that \alpha>0.30298, which improves the
previous record \alpha>0.29462 by Coppersmith (Journal of Complexity, 1997).
More generally, we construct a new algorithm for multiplying an n x n^k matrix
by an n^k x n matrix, for any value k\neq 1. The complexity of this algorithm
is better than all known algorithms for rectangular matrix multiplication. In
the case of square matrix multiplication (i.e., for k=1), we recover exactly
the complexity of the algorithm by Coppersmith and Winograd (Journal of
Symbolic Computation, 1990).
  These new upper bounds can be used to improve the time complexity of several
known algorithms that rely on rectangular matrix multiplication. For example,
we directly obtain a O(n^{2.5302})-time algorithm for the all-pairs shortest
paths problem over directed graphs with small integer weights, improving over
the O(n^{2.575})-time algorithm by Zwick (JACM 2002), and also improve the time
complexity of sparse square matrix multiplication.
"
421,"New techniques for computing the ideal class group and a system of
  fundamental units in number fields","  We describe a new algorithm for computing the ideal class group, the
regulator and a system of fundamental units in number fields under the
generalized Riemann hypothesis. We use sieving techniques adapted from the
number field sieve algorithm to derive relations between elements of the ideal
class group, and $p$-adic approximations to manage the loss of precision during
the computation of units. This new algorithm is particularily efficient for
number fields of small degree for which a speed-up of an order of magnitude is
achieved with respect to the standard methods.
"
422,"A polynomial time algorithm for computing the HNF of a module over the
  integers of a number field","  We present a variation of the modular algorithm for computing the Hermite
Normal Form of an $\OK$-module presented by Cohen, where $\OK$ is the ring of
integers of a number field K. The modular strategy was conjectured to run in
polynomial time by Cohen, but so far, no such proof was available in the
literature. In this paper, we provide a new method to prevent the coefficient
explosion and we rigorously assess its complexity with respect to the size of
the input and the invariants of the field K.
"
423,"Improvements in the computation of ideal class groups of imaginary
  quadratic number fields","  We investigate improvements to the algorithm for the computation of ideal
class groups described by Jacobson in the imaginary quadratic case. These
improvements rely on the large prime strategy and a new method for performing
the linear algebra phase. We achieve a significant speed-up and are able to
compute ideal class groups with discriminants of 110 decimal digits in less
than a week.
"
424,Synthesis of Minimal Error Control Software,"  Software implementations of controllers for physical systems are at the core
of many embedded systems. The design of controllers uses the theory of
dynamical systems to construct a mathematical control law that ensures that the
controlled system has certain properties, such as asymptotic convergence to an
equilibrium point, while optimizing some performance criteria. However, owing
to quantization errors arising from the use of fixed-point arithmetic, the
implementation of this control law can only guarantee practical stability:
under the actions of the implementation, the trajectories of the controlled
system converge to a bounded set around the equilibrium point, and the size of
the bounded set is proportional to the error in the implementation. The problem
of verifying whether a controller implementation achieves practical stability
for a given bounded set has been studied before. In this paper, we change the
emphasis from verification to automatic synthesis. Using synthesis, the need
for formal verification can be considerably reduced thereby reducing the design
time as well as design cost of embedded control software.
  We give a methodology and a tool to synthesize embedded control software that
is Pareto optimal w.r.t. both performance criteria and practical stability
regions. Our technique is a combination of static analysis to estimate
quantization errors for specific controller implementations and stochastic
local search over the space of possible controllers using particle swarm
optimization. The effectiveness of our technique is illustrated using examples
of various standard control systems: in most examples, we achieve controllers
with close LQR-LQG performance but with implementation errors, hence regions of
practical stability, several times as small.
"
425,Delta-Complete Decision Procedures for Satisfiability over the Reals,"  We introduce the notion of ""\delta-complete decision procedures"" for solving
SMT problems over the real numbers, with the aim of handling a wide range of
nonlinear functions including transcendental functions and solutions of
Lipschitz-continuous ODEs. Given an SMT problem \varphi and a positive rational
number \delta, a \delta-complete decision procedure determines either that
\varphi is unsatisfiable, or that the ""\delta-weakening"" of \varphi is
satisfiable. Here, the \delta-weakening of \varphi is a variant of \varphi that
allows \delta-bounded numerical perturbations on \varphi. We prove the
existence of \delta-complete decision procedures for bounded SMT over reals
with functions mentioned above. For functions in Type 2 complexity class C,
under mild assumptions, the bounded \delta-SMT problem is in NP^C.
\delta-Complete decision procedures can exploit scalable numerical methods for
handling nonlinearity, and we propose to use this notion as an ideal
requirement for numerically-driven decision procedures. As a concrete example,
we formally analyze the DPLL<ICP> framework, which integrates Interval
Constraint Propagation (ICP) in DPLL(T), and establish necessary and sufficient
conditions for its \delta-completeness. We discuss practical applications of
\delta-complete decision procedures for correctness-critical applications
including formal verification and theorem proving.
"
426,Computational linear algebra over finite fields,"  We present here algorithms for efficient computation of linear algebra
problems over finite fields.
"
427,"Matrix Formula of Differential Resultant for First Order Generic
  Ordinary Differential Polynomials","  In this paper, a matrix representation for the differential resultant of two
generic ordinary differential polynomials $f_1$ and $f_2$ in the differential
indeterminate $y$ with order one and arbitrary degree is given. That is, a
non-singular matrix is constructed such that its determinant contains the
differential resultant as a factor. Furthermore, the algebraic sparse resultant
of $f_1, f_2, \delta f_1, \delta f_2$ treated as polynomials in $y, y', y""$ is
shown to be a non-zero multiple of the differential resultant of $f_1, f_2$.
Although very special, this seems to be the first matrix representation for a
class of nonlinear generic differential polynomials.
"
428,Change-Of-Bases Abstractions for Non-Linear Systems,"  We present abstraction techniques that transform a given non-linear dynamical
system into a linear system or an algebraic system described by polynomials of
bounded degree, such that, invariant properties of the resulting abstraction
can be used to infer invariants for the original system. The abstraction
techniques rely on a change-of-basis transformation that associates each state
variable of the abstract system with a function involving the state variables
of the original system. We present conditions under which a given change of
basis transformation for a non-linear system can define an abstraction.
Furthermore, the techniques developed here apply to continuous systems defined
by Ordinary Differential Equations (ODEs), discrete systems defined by
transition systems and hybrid systems that combine continuous as well as
discrete subsystems. The techniques presented here allow us to discover, given
a non-linear system, if a change of bases transformation involving
degree-bounded polynomials yielding an algebraic abstraction exists. If so, our
technique yields the resulting abstract system, as well. This approach is
further extended to search for a change of bases transformation that abstracts
a given non-linear system into a system of linear differential inclusions. Our
techniques enable the use of analysis techniques for linear systems to infer
invariants for non-linear systems. We present preliminary evidence of the
practical feasibility of our ideas using a prototype implementation.
"
429,"An efficient implementation of the algorithm computing the Borel-fixed
  points of a Hilbert scheme","  Borel-fixed ideals play a key role in the study of Hilbert schemes. Indeed
each component and each intersection of components of a Hilbert scheme contains
at least one Borel-fixed point, i.e. a point corresponding to a subscheme
defined by a Borel-fixed ideal. Moreover Borel-fixed ideals have good
combinatorial properties, which make them very interesting in an algorithmic
perspective. In this paper, we propose an implementation of the algorithm
computing all the saturated Borel-fixed ideals with number of variables and
Hilbert polynomial assigned, introduced from a theoretical point of view in the
paper ""Segment ideals and Hilbert schemes of points"", Discrete Mathematics 311
(2011).
"
430,"Fast Computation of Common Left Multiples of Linear Ordinary
  Differential Operators","  We study tight bounds and fast algorithms for LCLMs of several linear
differential operators with polynomial coefficients. We analyze the arithmetic
complexity of existing algorithms for LCLMs, as well as the size of their
outputs. We propose a new algorithm that recasts the LCLM computation in a
linear algebra problem on a polynomial matrix. This algorithm yields sharp
bounds on the coefficient degrees of the LCLM, improving by one order of
magnitude the best bounds obtained using previous algorithms. The complexity of
the new algorithm is almost optimal, in the sense that it nearly matches the
arithmetic size of the output.
"
431,"Proving Inequalities and Solving Global Optimization Problems via
  Simplified CAD Projection","  Let $\xx_n=(x_1,\ldots,x_n)$ and $f\in \R[\xx_n,k]$. The problem of finding
all $k_0$ such that $f(\xx_n,k_0)\ge 0$ on $\mathbb{R}^n$ is considered in this
paper, which obviously takes as a special case the problem of computing the
global infimum or proving the semi-definiteness of a polynomial.
  For solving the problems, we propose a simplified Brown's CAD projection
operator, \Nproj, of which the projection scale is always no larger than that
of Brown's. For many problems, the scale is much smaller than that of Brown's.
As a result, the lifting phase is also simplified. Some new algorithms based on
\Nproj\ for solving those problems are designed and proved to be correct.
Comparison to some existing tools on some examples is reported to illustrate
the effectiveness of our new algorithms.
"
432,Faster arithmetic for number-theoretic transforms,"  We show how to improve the efficiency of the computation of fast Fourier
transforms over F_p where p is a word-sized prime. Our main technique is
optimisation of the basic arithmetic, in effect decreasing the total number of
reductions modulo p, by making use of a redundant representation for integers
modulo p. We give performance results showing a significant improvement over
Shoup's NTL library.
"
433,Power Series Solutions of Singular (q)-Differential Equations,"  We provide algorithms computing power series solutions of a large class of
differential or $q$-differential equations or systems. Their number of
arithmetic operations grows linearly with the precision, up to logarithmic
terms.
"
434,A Domain-Specific Compiler for Linear Algebra Operations,"  We present a prototypical linear algebra compiler that automatically exploits
domain-specific knowledge to generate high-performance algorithms. The input to
the compiler is a target equation together with knowledge of both the structure
of the problem and the properties of the operands. The output is a variety of
high-performance algorithms, and the corresponding source code, to solve the
target equation. Our approach consists in the decomposition of the input
equation into a sequence of library-supported kernels. Since in general such a
decomposition is not unique, our compiler returns not one but a number of
algorithms. The potential of the compiler is shown by means of its application
to a challenging equation arising within the genome-wide association study. As
a result, the compiler produces multiple ""best"" algorithms that outperform the
best existing libraries.
"
435,"Speeding up Cylindrical Algebraic Decomposition by Gr\""obner Bases","  Gr\""obner Bases and Cylindrical Algebraic Decomposition are generally thought
of as two, rather different, methods of looking at systems of equations and, in
the case of Cylindrical Algebraic Decomposition, inequalities. However, even
for a mixed system of equalities and inequalities, it is possible to apply
Gr\""obner bases to the (conjoined) equalities before invoking CAD. We see that
this is, quite often but not always, a beneficial preconditioning of the CAD
problem.
  It is also possible to precondition the (conjoined) inequalities with respect
to the equalities, and this can also be useful in many cases.
"
436,Quasi-Stability versus Genericity,"  Quasi-stable ideals appear as leading ideals in the theory of Pommaret bases.
We show that quasi-stable leading ideals share many of the properties of the
generic initial ideal. In contrast to genericity, quasi-stability is a
characteristic independent property that can be effectively verified. We also
relate Pommaret bases to some invariants associated with local cohomology,
exhibit the existence of linear quotients in Pommaret bases and prove some
results on componentwise linear ideals.
"
437,Computing arithmetic Kleinian groups,"  Arithmetic Kleinian groups are arithmetic lattices in PSL_2(C). We present an
algorithm which, given such a group Gamma, returns a fundamental domain and a
finite presentation for Gamma with a computable isomorphism.
"
438,Comprehensive Involutive Systems,"  In this paper, we consider parametric ideals and introduce a notion of
comprehensive involutive system. This notion plays the same role in theory of
involutive bases as the notion of comprehensive Groebner system in theory of
Groebner bases. Given a parametric ideal, the space of parameters is decomposed
into a finite set of cells. Each cell yields the corresponding involutive basis
of the ideal for the values of parameters in that cell. Using the Gerdt-Blinkov
algorithm for computing involutive bases and also the Montes algorithm for
computing comprehensive Groebner systems, we present an algorithm for
construction of comprehensive involutive systems. The proposed algorithm has
been implemented in Maple, and we provide an illustrative example showing the
step-by-step construction of comprehensive involutive system by our algorithm.
"
439,First Steps Towards Radical Parametrization of Algebraic Surfaces,"  We introduce the notion of radical parametrization of a surface, and we
provide algorithms to compute such type of parametrizations for families of
surfaces, like: Fermat surfaces, surfaces with a high multiplicity (at least
the degree minus 4) singularity, all irreducible surfaces of degree at most 5,
all irreducible singular surfaces of degree 6, and surfaces containing a pencil
of low-genus curves. In addition, we prove that radical parametrizations are
preserved under certain type of geometric constructions that include offset and
conchoids.
"
440,"Power Series Solution to Non-Linear Partial Differential equations of
  Mathematical Physics","  Power Series Solution method has been used traditionally for to solve Linear
Differential Equations, in Ordinary and Partial form. But this method has been
limited to this kind of problems. We present the solution of problems of Non
Linear Partial Differential equations of Physical Mathematical using power
series.
"
441,Computation of Difference Groebner Bases,"  To compute difference Groebner bases of ideals generated by linear
polynomials we adopt to difference polynomial rings the involutive algorithm
based on Janet-like division. The algorithm has been implemented in Maple in
the form of the package LDA (Linear Difference Algebra) and we describe the
main features of the package. Its applications are illustrated by generation of
finite difference approximations to linear partial differential equations and
by reduction of Feynman integrals. We also present the algorithm for an ideal
generated by a finite set of nonlinear difference polynomials. If the algorithm
terminates, then it constructs a Groebner basis of the ideal.
"
442,Factoring bivariate lacunary polynomials without heights,"  We present an algorithm which computes the multilinear factors of bivariate
lacunary polynomials. It is based on a new Gap Theorem which allows to test
whether a polynomial of the form P(X,X+1) is identically zero in time
polynomial in the number of terms of P(X,Y). The algorithm we obtain is more
elementary than the one by Kaltofen and Koiran (ISSAC'05) since it relies on
the valuation of polynomials of the previous form instead of the height of the
coefficients. As a result, it can be used to find some linear factors of
bivariate lacunary polynomials over a field of large finite characteristic in
probabilistic polynomial time.
"
443,"A Reduction Method for Higher Order Variational Equations of Hamiltonian
  Systems","  Let $\mathbf{k}$ be a differential field and let $[A]\,:\,Y'=A\,Y$ be a
linear differential system where $A\in\mathrm{Mat}(n\,,\,\mathbf{k})$. We say
that $A$ is in a reduced form if $A\in\mathfrak{g}(\bar{\mathbf{k}})$ where
$\mathfrak{g}$ is the Lie algebra of $[A]$ and $\bar{\mathbf{k}}$ denotes the
algebraic closure of $\mathbf{k}$. We owe the existence of such reduced forms
to a result due to Kolchin and Kovacic \cite{Ko71a}. This paper is devoted to
the study of reduced forms, of (higher order) variational equations along a
particular solution of a complex analytical hamiltonian system $X$. Using a
previous result \cite{ApWea}, we will assume that the first order variational
equation has an abelian Lie algebra so that, at first order, there are no
Galoisian obstructions to Liouville integrability. We give a strategy to
(partially) reduce the variational equations at order $m+1$ if the variational
equations at order $m$ are already in a reduced form and their Lie algebra is
abelian. Our procedure stops when we meet obstructions to the meromorphic
integrability of $X$. We make strong use both of the lower block triangular
structure of the variational equations and of the notion of associated Lie
algebra of a linear differential system (based on the works of Wei and Norman
in \cite{WeNo63a}). Obstructions to integrability appear when at some step we
obtain a non-trivial commutator between a diagonal element and a nilpotent
(subdiagonal) element of the associated Lie algebra. We use our method coupled
with a reasoning on polylogarithms to give a new and systematic proof of the
non-integrability of the H\'enon-Heiles system. We conjecture that our method
is not only a partial reduction procedure but a complete reduction algorithm.
In the context of complex Hamiltonian systems, this would mean that our method
would be an effective version of the Morales-Ramis-Sim\'o theorem.
"
444,A Characterization of Reduced Forms of Linear Differential Systems,"  A differential system $[A] : \; Y'=AY$, with $A\in \mathrm{Mat}(n, \bar{k})$
is said to be in reduced form if $A\in \mathfrak{g}(\bar{k})$ where
$\mathfrak{g}$ is the Lie algebra of the differential Galois group $G$ of
$[A]$. In this article, we give a constructive criterion for a system to be in
reduced form. When $G$ is reductive and unimodular, the system $[A]$ is in
reduced form if and only if all of its invariants (rational solutions of
appropriate symmetric powers) have constant coefficients (instead of rational
functions). When $G$ is non-reductive, we give a similar characterization via
the semi-invariants of $G$. In the reductive case, we propose a decision
procedure for putting the system into reduced form which, in turn, gives a
constructive proof of the classical Kolchin-Kovacic reduction theorem.
"
445,Practical Groebner Basis Computation,"  We report on our experiences exploring state of the art Groebner basis
computation. We investigate signature based algorithms in detail. We also
introduce new practical data structures and computational techniques for use in
both signature based Groebner basis algorithms and more traditional variations
of the classic Buchberger algorithm. Our conclusions are based on experiments
using our new freely available open source standalone C++ library.
"
446,"GPGCD: An iterative method for calculating approximate GCD of univariate
  polynomials","  We present an iterative algorithm for calculating approximate greatest common
divisor (GCD) of univariate polynomials with the real or the complex
coefficients. For a given pair of polynomials and a degree, our algorithm finds
a pair of polynomials which has a GCD of the given degree and whose
coefficients are perturbed from those in the original inputs, making the
perturbations as small as possible, along with the GCD. The problem of
approximate GCD is transfered to a constrained minimization problem, then
solved with the so-called modified Newton method, which is a generalization of
the gradient-projection method, by searching the solution iteratively. We
demonstrate that, in some test cases, our algorithm calculates approximate GCD
with perturbations as small as those calculated by a method based on the
structured total least norm (STLN) method and the UVGCD method, while our
method runs significantly faster than theirs by approximately up to 30 or 10
times, respectively, compared with their implementation. We also show that our
algorithm properly handles some ill-conditioned polynomials which have a GCD
with small or large leading coefficient.
"
447,Metric Problems for Quadrics in Multidimensional Space,"  Given the equations of the first and the second order surfaces in
multidimensional space, our goal is to construct a univariate polynomial one of
the zeros of which coincides with the square of the distance between these
surfaces. To achieve this goal we employ Elimination Theory methods. The
proposed approach is also extended for the case of parameter dependent
surfaces.
"
448,Real Root Isolation of Polynomial Equations Based on Hybrid Computation,"  A new algorithm for real root isolation of polynomial equations based on
hybrid computation is presented in this paper. Firstly, the approximate
(complex) zeros of the given polynomial equations are obtained via homotopy
continuation method. Then, for each approximate zero, an initial box relying on
the Kantorovich theorem is constructed, which contains the corresponding
accurate zero. Finally, the Krawczyk interval iteration with interval
arithmetic is applied to the initial boxes so as to check whether or not the
corresponding approximate zeros are real and to obtain the real root isolation
boxes. Meanwhile, an empirical construction of initial box is provided for
higher performance. Our experiments on many benchmarks show that the new hybrid
method is more efficient, compared with the traditional symbolic approaches.
"
449,"On lexicographic Groebner bases of radical ideals in dimension zero:
  interpolation and structure","  Due to the elimination property held by the lexicographic monomial order, the
corresponding Groebner bases display strong structural properties from which
meaningful informations can easily be extracted. We study these properties for
radical ideals of (co)dimension zero. The proof presented relies on a
combinatorial decomposition of the finite set of points whereby iterated
Lagrange interpolation formulas permit to reconstruct a minimal Groebner basis.
This is the first fully explicit interpolation formula for polynomials forming
a lexicographic Groebner basis, from which the structure property can easily be
read off. The inductive nature of the proof also yield as a byproduct a
triangular decomposition algorithm from the Groebner basis.
"
450,Detecting Symmetries of Rational Plane and Space Curves,"  This paper addresses the problem of determining the symmetries of a plane or
space curve defined by a rational parametrization. We provide effective methods
to compute the involution and rotation symmetries for the planar case. As for
space curves, our method finds the involutions in all cases, and all the
rotation symmetries in the particular case of Pythagorean-hodograph curves. Our
algorithms solve these problems without converting to implicit form. Instead,
we make use of a relationship between two proper parametrizations of the same
curve, which leads to algorithms that involve only univariate polynomials.
These algorithms have been implemented and tested in the Sage system.
"
451,Improving multivariate Horner schemes with Monte Carlo tree search,"  Optimizing the cost of evaluating a polynomial is a classic problem in
computer science. For polynomials in one variable, Horner's method provides a
scheme for producing a computationally efficient form. For multivariate
polynomials it is possible to generalize Horner's method, but this leaves
freedom in the order of the variables. Traditionally, greedy schemes like
most-occurring variable first are used. This simple textbook algorithm has
given remarkably efficient results. Finding better algorithms has proved
difficult. In trying to improve upon the greedy scheme we have implemented
Monte Carlo tree search, a recent search method from the field of artificial
intelligence. This results in better Horner schemes and reduces the cost of
evaluating polynomials, sometimes by factors up to two.
"
452,Quasi-optimal multiplication of linear differential operators,"  We show that linear differential operators with polynomial coefficients over
a field of characteristic zero can be multiplied in quasi-optimal time. This
answers an open question raised by van der Hoeven.
"
453,Hybrid Automata and \epsilon-Analysis on a Neural Oscillator,"  In this paper we propose a hybrid model of a neural oscillator, obtained by
partially discretizing a well-known continuous model. Our construction points
out that in this case the standard techniques, based on replacing sigmoids with
step functions, is not satisfactory. Then, we study the hybrid model through
both symbolic methods and approximation techniques. This last analysis, in
particular, allows us to show the differences between the considered
approximation approaches. Finally, we focus on approximations via
epsilon-semantics, proving how these can be computed in practice.
"
454,Generic Regular Decompositions for Generic Zero-Dimensional Systems,"  Two new concepts, generic regular decomposition and
regular-decomposition-unstable (RDU) variety for generic zero-dimensional
systems, are introduced in this paper and an algorithm is proposed for
computing a generic regular decomposition and the associated RDU variety of a
given generic zero-dimensional system simultaneously. The solutions of the
given system can be expressed by finitely many zero-dimensional regular chains
if the parameter value is not on the RDU variety.
  The so called weakly relatively simplicial decomposition plays a crucial role
in the algorithm, which is based on the theories of subresultant chains.
Furthermore, the algorithm can be naturally adopted to compute a non-redundant
Wu's decomposition and the decomposition is stable at any parameter value that
is not on the RDU variety. The algorithm has been implemented with Maple 15 and
experimented with a number of benchmarks from the literature. Empirical results
are also presented to show the good performance of the algorithm.
"
455,"Reducing the size and number of linear programs in a dynamic Gr\""obner
  basis algorithm","  The dynamic algorithm to compute a Gr\""obner basis is nearly twenty years
old, yet it seems to have arrived stillborn; aside from two initial
publications, there have been no published followups. One reason for this may
be that, at first glance, the added overhead seems to outweigh the benefit; the
algorithm must solve many linear programs with many linear constraints. This
paper describes two methods of reducing the cost substantially, answering the
problem effectively.
"
456,The expansion of real forms on the simplex and applications,"  If n points B_1,---,B_n$ in the standard simplex \Delta_n are affinely
independent, then they can span an (n-1)-simplex denoted by
\Lambda=Con(B_1,---,B_n). Here \Lambda corresponds to an n*n matrix [\Lambda]
whose columns are B_1,---,B_n. In this paper, we firstly proved that if \Lambda
of diameter sufficiently small contains a point $P$, and f(P)>0 (<0) for a form
f in R[X], then the coefficients of f([\Lambda] X) are all positive (negative).
Next, as an application of this result, a necessary and sufficient condition
for determining the real zeros on \Delta_n of a system of homogeneous algebraic
equations with integral coefficients is established.
"
457,logcf: An Efficient Tool for Real Root Isolation,"  This paper revisits an algorithm for isolating real roots of univariate
polynomials based on continued fractions. It follows the work of Vincent,
Uspen- sky, Collins and Akritas, Johnson and Krandick. We use some tricks,
especially a new algorithm for computing an upper bound of positive roots. In
this way, the algorithm of isolating real roots is improved. The complexity of
our method for computing an upper bound of positive roots is O(n log(u+1))
where u is the optimal upper bound satisfying Theorem 3 and n is the degree of
the polynomial. Our method has been implemented as a software package logcf
using C++ language. For many benchmarks logcf is two or three times faster than
the function RootIntervals of Mathematica. And it is much faster than another
continued fractions based software CF, which seems to be one of the fastest
available open software for exact real root isolation. For those benchmarks
which have only real roots, logcf is much faster than Sleeve and eigensolve
which are based on numerical computation.
"
458,A Note on the Space Complexity of Fast D-Finite Function Evaluation,"  We state and analyze a generalization of the ""truncation trick"" suggested by
Gourdon and Sebah to improve the performance of power series evaluation by
binary splitting. It follows from our analysis that the values of D-finite
functions (i.e., functions described as solutions of linear differential
equations with polynomial coefficients) may be computed with error bounded by
2^(-p) in time O(p*(lg p)^(3+o(1))) and space O(p). The standard fast algorithm
for this task, due to Chudnovsky and Chudnovsky, achieves the same time
complexity bound but requires \Theta(p*lg p) bits of memory.
"
459,"A proposal to first principles electronic structure calculation:
  Symbolic-Numeric method","  This study proposes an approach toward the first principles electronic
structure calculation with the aid of symbolic-numeric solving. The symbolic
computation enables us to express the Hartree-Fock-Roothaan equation and the
molecular integrals in analytic forms and approximate them as a set of
polynomial equations. By use of the Grobner bases technique, the polynomial
equations are transformed into other ones which have identical roots. The
converted equations take more convenient forms which will simplify numerical
procedures, from which we can derive necessary physical properties in order, in
an a la carte way. This method enables us to solve the electronic structure
calculation, the optimization of any kind, or the inverse problem as a forward
problem in a unified way, in which there is no need for iterative
self-consistent procedures with trials and errors.
"
460,"A new edge selection heuristic for computing the Tutte polynomial of an
  undirected graph","  We present a new edge selection heuristic and vertex ordering heuristic that
together enable one to compute the Tutte polynomial of much larger sparse
graphs than was previously doable. As a specific example, we are able to
compute the Tutte polynomial of the truncated icosahedron graph using our Maple
implementation in under 4 minutes on a single CPU. This compares with a recent
result of Haggard, Pearce and Royle whose special purpose C++ software took one
week on 150 computers.
"
461,"On Newton-Raphson iteration for multiplicative inverses modulo prime
  powers","  We study algorithms for the fast computation of modular inverses.
Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation
computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve
the recurrence to obtain an explicit formula for the inverse. Then we study
different implementation variants of this iteration and show that our explicit
formula is interesting for small exponent values but slower or large exponent,
say of more than $700$ bits. Overall we thus propose a hybrid combination of
our explicit formula and the best asymptotic variants. This hybrid combination
yields then a constant factor improvement, also for large exponents.
"
462,On the Complexity of the Multivariate Resultant,"  The multivariate resultant is a fundamental tool of computational algebraic
geometry. It can in particular be used to decide whether a system of n
homogeneous equations in n variables is satisfiable (the resultant is a
polynomial in the system's coefficients which vanishes if and only if the
system is satisfiable). In this paper, we investigate the complexity of
computing the multivariate resultant.
  First, we study the complexity of testing the multivariate resultant for
zero. Our main result is that this problem is NP-hard under deterministic
reductions in any characteristic, for systems of low-degree polynomials with
coefficients in the ground field (rather than in an extension). In
characteristic zero, we observe that this problem is in the Arthur-Merlin class
AM if the generalized Riemann hypothesis holds true, while the best known upper
bound in positive characteristic remains PSPACE.
  Second, we study the classical algorithms to compute the resultant. They
usually rely on the computation of the determinant of an exponential-size
matrix, known as Macaulay matrix. We show that this matrix belongs to a class
of succinctly representable matrices, for which testing the determinant for
zero is proved PSPACE-complete. This means that improving Canny's PSPACE upper
bound requires either to look at the fine structure of the Macaulay matrix to
find an ad hoc algorithm for computing its determinant, or to use altogether
different techniques.
"
463,"Advanced Computer Algebra Algorithms for the Expansion of Feynman
  Integrals","  Two-point Feynman parameter integrals, with at most one mass and containing
local operator insertions in $4+\ep$-dimensional Minkowski space, can be
transformed to multi-integrals or multi-sums over hyperexponential and/or
hypergeometric functions depending on a discrete parameter $n$. Given such a
specific representation, we utilize an enhanced version of the multivariate
Almkvist--Zeilberger algorithm (for multi-integrals) and a common summation
framework of the holonomic and difference field approach (for multi-sums) to
calculate recurrence relations in $n$. Finally, solving the recurrence we can
decide efficiently if the first coefficients of the Laurent series expansion of
a given Feynman integral can be expressed in terms of indefinite nested sums
and products; if yes, the all $n$ solution is returned in compact
representations, i.e., no algebraic relations exist among the occurring sums
and products.
"
464,"Symbolic Analysis for Boundary Problems: From Rewriting to Parametrized
  Gr\""obner Bases","  We review our algebraic framework for linear boundary problems (concentrating
on ordinary differential equations). Its starting point is an appropriate
algebraization of the domain of functions, which we have named
integro-differential algebras. The algebraic treatment of boundary problems
brings up two new algebraic structures whose symbolic representation and
computational realization is based on canonical forms in certain commutative
and noncommutative polynomial domains. The first of these, the ring of
integro-differential operators, is used for both stating and solving linear
boundary problems. The other structure, called integro-differential
polynomials, is the key tool for describing extensions of integro-differential
algebras. We use the canonical simplifier for integro-differential polynomials
for generating an automated proof establishing a canonical simplifier for
integro-differential operators. Our approach is fully implemented in the
Theorema system; some code fragments and sample computations are included.
"
465,Regular and Singular Boundary Problems in Maple,"  We describe a new Maple package for treating boundary problems for linear
ordinary differential equations, allowing two-/multipoint as well as Stieltjes
boundary conditions. For expressing differential operators, boundary
conditions, and Green's operators, we employ the algebra of
integro-differential operators. The operations implemented for regular boundary
problems include computing Green's operators as well as composing and factoring
boundary problems. Our symbolic approach to singular boundary problems is new;
it provides algorithms for computing compatibility conditions and generalized
Green's operators.
"
466,A New Recursive Algorithm For Inverting A General Comrade Matrix,"  In this paper, the author present a reliable symbolic computational algorithm
for inverting a general comrade matrix by using parallel computing along with
recursion. The computational cost of our algorithm is O(n^2). The algorithm is
implementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and
MATHEMATICA. Three examples are presented for the sake of illustration.
"
467,A new Truncated Fourier Transform algorithm,"  Truncated Fourier Transforms (TFTs), first introduced by Van der Hoeven,
refer to a family of algorithms that attempt to smooth ""jumps"" in complexity
exhibited by FFT algorithms. We present an in-place TFT whose time complexity,
measured in terms of ring operations, is comparable to existing not-in-place
TFT methods. We also describe a transformation that maps between two families
of TFT algorithms that use different sets of evaluation points.
"
468,"An Incremental Algorithm for Computing Cylindrical Algebraic
  Decompositions","  In this paper, we propose an incremental algorithm for computing cylindrical
algebraic decompositions. The algorithm consists of two parts: computing a
complex cylindrical tree and refining this complex tree into a cylindrical tree
in real space. The incrementality comes from the first part of the algorithm,
where a complex cylindrical tree is constructed by refining a previous complex
cylindrical tree with a polynomial constraint. We have implemented our
algorithm in Maple. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
"
469,On the Summability of Bivariate Rational Functions,"  We present criteria for deciding whether a bivariate rational function in two
variables can be written as a sum of two (q-)differences of bivariate rational
functions. Using these criteria, we show how certain double sums can be
evaluated, first, in terms of single sums and, finally, in terms of values of
special functions.
"
470,On the Existence of Telescopers for Mixed Hypergeometric Terms,"  We present a criterion for the existence of telescopers for mixed
hypergeometric terms, which is based on multiplicative and additive
decompositions. The criterion enables us to determine the termination of
Zeilberger's algorithms for mixed hypergeometric inputs.
"
471,"How to compute the constant term of a power of a Laurent polynomial
  efficiently","  We present an algorithm for efficient computation of the constant term of a
power of a multivariate Laurent polynomial. The algorithm is based on
univariate interpolation, does not require the storage of intermediate data and
can be easily parallelized. As an application we compute the power series
expansion of the principal period of some toric Calabi-Yau varieties and find
previously unknown differential operators of Calabi-Yau type.
"
472,"Unified Form Language: A domain-specific language for weak formulations
  of partial differential equations","  We present the Unified Form Language (UFL), which is a domain-specific
language for representing weak formulations of partial differential equations
with a view to numerical approximation. Features of UFL include support for
variational forms and functionals, automatic differentiation of forms and
expressions, arbitrary function space hierarchies for multi-field problems,
general differential operators and flexible tensor algebra. With these
features, UFL has been used to effortlessly express finite element methods for
complex systems of partial differential equations in near-mathematical
notation, resulting in compact, intuitive and readable programs. We present in
this work the language and its construction. An implementation of UFL is freely
available as an open-source software library. The library generates abstract
syntax tree representations of variational problems, which are used by other
software libraries to generate concrete low-level implementations. Some
application examples are presented and libraries that support UFL are
highlighted.
"
473,"Confusion of Tagged Perturbations in Forward Automatic Differentiation
  of Higher-Order Functions","  Forward Automatic Differentiation (AD) is a technique for augmenting programs
to compute derivatives. The essence of Forward AD is to attach perturbations to
each number, and propagate these through the computation. When derivatives are
nested, the distinct derivative calculations, and their associated
perturbations, must be distinguished. This is typically accomplished by
creating a unique tag for each derivative calculation, tagging the
perturbations, and overloading the arithmetic operators. We exhibit a subtle
bug, present in fielded implementations, in which perturbations are confused
despite the tagging machinery. The essence of the bug is this: each invocation
of a derivative creates a unique tag but a unique tag is needed for each
derivative calculation. When taking derivatives of higher-order functions,
these need not correspond! The derivative of a higher-order function $f$ that
returns a function $g$ will be a function $f'$ that returns a function
$\bar{g}$ that performs a derivative calculation. A single invocation of $f'$
will create a single fresh tag but that same tag will be used for each
derivative calculation resulting from an invocation of $\bar{g}$. This
situation arises when taking derivatives of curried functions. Two potential
solutions are presented, and their serious deficiencies discussed. One requires
eta expansion to delay the creation of fresh tags from the invocation of $f'$
to the invocation of $\bar{g}$, which can be difficult or even impossible in
some circumstances. The other requires $f'$ to wrap $\bar{g}$ with tag
renaming, which is difficult to implement without violating the desirable
complexity properties of forward AD.
"
474,Theano: new features and speed improvements,"  Theano is a linear algebra compiler that optimizes a user's
symbolically-specified mathematical computations to produce efficient low-level
implementations. In this paper, we present new features and efficiency
improvements to Theano, and benchmarks demonstrating Theano's performance
relative to Torch7, a recently introduced machine learning library, and to
RNNLM, a C++ library targeted at recurrent neural networks.
"
475,Irreducibility of q-difference operators and the knot 7_4,"  Our goal is to compute the minimal-order recurrence of the colored Jones
polynomial of the 7_4 knot, as well as for the first four double twist knots.
As a corollary, we verify the AJ Conjecture for the simplest knot 7_4 with
reducible non-abelian SL(2,C) character variety. To achieve our goal, we use
symbolic summation techniques of Zeilberger's holonomic systems approach and an
irreducibility criterion for q-difference operators. For the latter we use an
improved version of the qHyper algorithm of Abramov-Paule-Petkovsek to show
that a given q-difference operator has no linear right factors. En route, we
introduce exterior power Adams operations on the ring of bivariate polynomials
and on the corresponding affine curves.
"
476,Sparse Difference Resultant,"  In this paper, the concept of sparse difference resultant for a Laurent
transformally essential system of difference polynomials is introduced and a
simple criterion for the existence of sparse difference resultant is given. The
concept of transformally homogenous polynomial is introduced and the sparse
difference resultant is shown to be transformally homogenous. It is shown that
the vanishing of the sparse difference resultant gives a necessary condition
for the corresponding difference polynomial system to have non-zero solutions.
The order and degree bounds for sparse difference resultant are given. Based on
these bounds, an algorithm to compute the sparse difference resultant is
proposed, which is single exponential in terms of the number of variables, the
Jacobi number, and the size of the Laurent transformally essential system.
Furthermore, the precise order and degree, a determinant representation, and a
Poisson-type product formula for the difference resultant are given.
"
477,Relative parametrization of linear multidimensional systems,"  In the last chapter of his book ""The Algebraic Theory of Modular Systems ""
published in 1916, F. S. Macaulay developped specific techniques for dealing
with "" unmixed polynomial ideals "" by introducing what he called "" inverse
systems "". The purpose of this paper is to extend such a point of view to
differential modules defined by linear multidimensional systems, that is by
linear systems of ordinary differential (OD) or partial differential (PD)
equations of any order, with any number of independent variables, any number of
unknowns and even with variable coefficients in a differential field. The first
and main idea is to replace unmixed polynomial ideals by "" pure differential
modules "". The second idea is to notice that a module is 0-pure if and only if
it is torsion-free and thus if and only if it admits an "" absolute
parametrization "" by means of arbitrary potential like functions, or,
equivalently, if it can be embedded into a free module by means of an ""
absolute localization "". The third idea is to refer to a difficult theorem of
algebraic analysis saying that an r-pure module can be embedded into a module
of projective dimension equal to r, that is a module admitting a projective
resolution with exactly r operators. The fourth and final idea is to establish
a link between the use of extension modules for such a purpose and specific
formal properties of the underlying multidimensional system through the use of
involution and a ""relative localization "" leading to a ""relative
parametrization "", that is to the use of potential-like functions satisfying a
kind of ""minimum differential constraint "" limiting, in some sense, the number
of independent variables appearing in these functions, in a way similar to the
situation met in the Cartan-K\""ahler theorem of analysis. The paper is written
in a rather effective self-contained way and we provide many explicit examples
that should become test examples for a future use of computer algebra.
"
478,"Multiple precision evaluation of the Airy Ai function with reduced
  cancellation","  The series expansion at the origin of the Airy function Ai(x) is alternating
and hence problematic to evaluate for x > 0 due to cancellation. Based on a
method recently proposed by Gawronski, M\""uller, and Reinhard, we exhibit two
functions F and G, both with nonnegative Taylor expansions at the origin, such
that Ai(x) = G(x)/F(x). The sums are now well-conditioned, but the Taylor
coefficients of G turn out to obey an ill-conditioned three-term recurrence. We
use the classical Miller algorithm to overcome this issue. We bound all errors
and our implementation allows an arbitrary and certified accuracy, that can be
used, e.g., for providing correct rounding in arbitrary precision.
"
479,"Program Verification in the presence of complex numbers, functions with
  branch cuts etc","  In considering the reliability of numerical programs, it is normal to ""limit
our study to the semantics dealing with numerical precision"" (Martel, 2005). On
the other hand, there is a great deal of work on the reliability of programs
that essentially ignores the numerics. The thesis of this paper is that there
is a class of problems that fall between these two, which could be described as
""does the low-level arithmetic implement the high-level mathematics"". Many of
these problems arise because mathematics, particularly the mathematics of the
complex numbers, is more difficult than expected: for example the complex
function log is not continuous, writing down a program to compute an inverse
function is more complicated than just solving an equation, and many algebraic
simplification rules are not universally valid.
  The good news is that these problems are theoretically capable of being
solved, and are practically close to being solved, but not yet solved, in
several real-world examples. However, there is still a long way to go before
implementations match the theoretical possibilities.
"
480,"Pfaffian Systems of A-Hypergeometric Equations I: Bases of Twisted
  Cohomology Groups","  This is the third revision. We study bases of Pfaffian systems for
$A$-hypergeometric system. Gr\""obner deformations give bases. These bases also
give those for twisted cohomology groups. For hypergeometric system associated
to a class of order polytopes, these bases have a combinatorial description.
The size of the bases associated to a subclass of the order polytopes have the
growth rate of the polynomial order. Bases associated to two chain posets and
bouquets are studied.
"
481,Desingularization Explains Order-Degree Curves for Ore Operators,"  Desingularization is the problem of finding a left multiple of a given Ore
operator in which some factor of the leading coefficient of the original
operator is removed. An order-degree curve for a given Ore operator is a curve
in the $(r,d)$-plane such that for all points $(r,d)$ above this curve, there
exists a left multiple of order $r$ and degree $d$ of the given operator. We
give a new proof of a desingularization result by Abramov and van Hoeij for the
shift case, and show how desingularization implies order-degree curves which
are extremely accurate in examples.
"
482,"Finding Hyperexponential Solutions of Linear ODEs by Numerical
  Evaluation","  We present a new algorithm for computing hyperexponential solutions of
ordinary linear differential equations with polynomial coefficients. The
algorithm relies on interpreting formal series solutions at the singular points
as analytic functions and evaluating them numerically at some common ordinary
point. The numerical data is used to determine a small number of combinations
of the formal series that may give rise to hyperexponential solutions.
"
483,Generic Regular Decompositions for Parametric Polynomial Systems,"  This paper presents a generalization of our earlier work in [19]. In this
paper, the two concepts, generic regular decomposition (GRD) and
regular-decomposition-unstable (RDU) variety introduced in [19] for generic
zero-dimensional systems, are extended to the case where the parametric systems
are not necessarily zero-dimensional. An algorithm is provided to compute GRDs
and the associated RDU varieties of parametric systems simultaneously on the
basis of the algorithm for generic zero-dimensional systems proposed in [19].
Then the solutions of any parametric system can be represented by the solutions
of finitely many regular systems and the decomposition is stable at any
parameter value in the complement of the associated RDU variety of the
parameter space. The related definitions and the results presented in [19] are
also generalized and a further discussion on RDU varieties is given from an
experimental point of view. The new algorithm has been implemented on the basis
of DISCOVERER with Maple 16 and experimented with a number of benchmarks from
the literature.
"
484,Classification of Angle-Symmetric 6R Linkage,"  In this paper, we consider a special kind of overconstrained 6R closed
linkages which we call angle-symmetric 6R linkages. These are linkages with the
property that the rotation angles are equal for each of the three pairs of
opposite joints. We give a classification of these linkages. It turns that
there are three types. First, we have the linkages with line symmetry. The
second type is new. The third type is related to cubic motion polynomials.
"
485,"Creative telescoping for rational functions using the Griffiths-Dwork
  method","  Creative telescoping algorithms compute linear differential equations
satisfied by multiple integrals with parameters. We describe a precise and
elementary algorithmic version of the Griffiths-Dwork method for the creative
telescoping of rational functions. This leads to bounds on the order and degree
of the coefficients of the differential equation, and to the first complexity
result which is simply exponential in the number of variables. One of the
important features of the algorithm is that it does not need to compute
certificates. The approach is vindicated by a prototype implementation.
"
486,"From Approximate Factorization to Root Isolation with Application to
  Cylindrical Algebraic Decomposition","  We present an algorithm for isolating the roots of an arbitrary complex
polynomial $p$ that also works for polynomials with multiple roots provided
that the number $k$ of distinct roots is given as part of the input. It outputs
$k$ pairwise disjoint disks each containing one of the distinct roots of $p$,
and its multiplicity. The algorithm uses approximate factorization as a
subroutine.
  In addition, we apply the new root isolation algorithm to a recent algorithm
for computing the topology of a real planar algebraic curve specified as the
zero set of a bivariate integer polynomial and for isolating the real solutions
of a bivariate polynomial system. For input polynomials of degree $n$ and
bitsize $\tau$, we improve the currently best running time from
$\tO(n^{9}\tau+n^{8}\tau^{2})$ (deterministic) to $\tO(n^{6}+n^{5}\tau)$
(randomized) for topology computation and from $\tO(n^{8}+n^{7}\tau)$
(deterministic) to $\tO(n^{6}+n^{5}\tau)$ (randomized) for solving bivariate
systems.
"
487,"Closed form solutions of linear difference equations in terms of
  symmetric products","  In this paper we show how to find a closed form solution for third order
difference operators in terms of solutions of second order operators. This work
is an extension of previous results on finding closed form solutions of
recurrence equations and a counterpart to existing results on differential
equations. As motivation and application for this work, we discuss the problem
of proving positivity of sequences given merely in terms of their defining
recurrence relation. The main advantage of the present approach to earlier
methods attacking the same problem is that our algorithm provides
human-readable and verifiable, i.e., certified proofs.
"
488,"Hermite Reduction and Creative Telescoping for Hyperexponential
  Functions","  We present a reduction algorithm that simultaneously extends Hermite's
reduction for rational functions and the Hermite-like reduction for
hyperexponential functions. It yields a unique additive decomposition and
allows to decide hyperexponential integrability. Based on this reduction
algorithm, we design a new method to compute minimal telescopers for bivariate
hyperexponential functions. One of its main features is that it can avoid the
costly computation of certificates. Its implementation outperforms Maple's
function DEtools[Zeilberger]. Moreover, we derive an order bound on minimal
telescopers, which is more general and tighter than the known one.
"
489,Complexity of Creative Telescoping for Bivariate Rational Functions,"  The long-term goal initiated in this work is to obtain fast algorithms and
implementations for definite integration in Almkvist and Zeilberger's framework
of (differential) creative telescoping. Our complexity-driven approach is to
obtain tight degree bounds on the various expressions involved in the method.
To make the problem more tractable, we restrict to bivariate rational
functions. By considering this constrained class of inputs, we are able to
blend the general method of creative telescoping with the well-known Hermite
reduction. We then use our new method to compute diagonals of rational power
series arising from combinatorics.
"
490,On the Structure of Compatible Rational Functions,"  A finite number of rational functions are compatible if they satisfy the
compatibility conditions of a first-order linear functional system involving
differential, shift and q-shift operators. We present a theorem that describes
the structure of compatible rational functions. The theorem enables us to
decompose a solution of such a system as a product of a rational function,
several symbolic powers, a hyperexponential function, a hypergeometric term,
and a q-hypergeometric term. We outline an algorithm for computing this
product, and present an application.
"
491,Complexity Estimates for Two Uncoupling Algorithms,"  Uncoupling algorithms transform a linear differential system of first order
into one or several scalar differential equations. We examine two approaches to
uncoupling: the cyclic-vector method (CVM) and the
Danilevski-Barkatou-Z\""urcher algorithm (DBZ). We give tight size bounds on the
scalar equations produced by CVM, and design a fast variant of CVM whose
complexity is quasi-optimal with respect to the output size. We exhibit a
strong structural link between CVM and DBZ enabling to show that, in the
generic case, DBZ has polynomial complexity and that it produces a single
equation, strongly related to the output of CVM. We prove that algorithm CVM is
faster than DBZ by almost two orders of magnitude, and provide experimental
results that validate the theoretical complexity analyses.
"
492,"On the Complexity of Computing Gr\""obner Bases for Quasi-homogeneous
  Systems","  Let $\K$ be a field and $(f_1, \ldots, f_n)\subset \K[X_1, \ldots, X_n]$ be a
sequence of quasi-homogeneous polynomials of respective weighted degrees $(d_1,
\ldots, d_n)$ w.r.t a system of weights $(w_{1},\dots,w_{n})$. Such systems are
likely to arise from a lot of applications, including physics or cryptography.
We design strategies for computing Gr\""obner bases for quasi-homogeneous
systems by adapting existing algorithms for homogeneous systems to the
quasi-homogeneous case. Overall, under genericity assumptions, we show that for
a generic zero-dimensional quasi-homogeneous system, the complexity of the full
strategy is polynomial in the weighted B\'ezout bound $\prod_{i=1}^{n}d_{i} /
\prod_{i=1}^{n}w_{i}$. We provide some experimental results based on generic
systems as well as systems arising from a cryptography problem. They show that
taking advantage of the quasi-homogeneous structure of the systems allow us to
solve systems that were out of reach otherwise.
"
493,Superfast solution of Toeplitz systems based on syzygy reduction,"  We present a new superfast algorithm for solving Toeplitz systems. This
algorithm is based on a relation between the solution of such problems and
syzygies of polynomials or moving lines. We show an explicit connection between
the generators of a Toeplitz matrix and the generators of the corresponding
module of syzygies. We show that this module is generated by two elements and
the solution of a Toeplitz system T u=g can be reinterpreted as the remainder
of a vector depending on g, by these two generators. We obtain these generators
and this remainder with computational complexity O(n log^2 n) for a Toeplitz
matrix of size nxn.
"
494,A simple and fast algorithm for computing exponentials of power series,"  As was initially shown by Brent, exponentials of truncated power series can
be computed using a constant number of polynomial multiplications. This note
gives a relatively simple algorithm with a low constant factor.
"
495,Fast algorithms for ell-adic towers over finite fields,"  Inspired by previous work of Shoup, Lenstra-De Smit and Couveignes-Lercier,
we give fast algorithms to compute in (the first levels of) the ell-adic
closure of a finite field. In many cases, our algorithms have quasi-linear
complexity.
"
496,"Analytic and Algorithmic Aspects of Generalized Harmonic Sums and
  Polylogarithms","  In recent three--loop calculations of massive Feynman integrals within
Quantum Chromodynamics (QCD) and, e.g., in recent combinatorial problems the
so-called generalized harmonic sums (in short $S$-sums) arise. They are
characterized by rational (or real) numerator weights also different from $\pm
1$. In this article we explore the algorithmic and analytic properties of these
sums systematically. We work out the Mellin and inverse Mellin transform which
connects the sums under consideration with the associated Poincar\'{e} iterated
integrals, also called generalized harmonic polylogarithms. In this regard, we
obtain explicit analytic continuations by means of asymptotic expansions of the
$S$-sums which started to occur frequently in current QCD calculations. In
addition, we derive algebraic and structural relations, like differentiation
w.r.t. the external summation index and different multi-argument relations, for
the compactification of $S$-sum expressions. Finally, we calculate algebraic
relations for infinite $S$-sums, or equivalently for generalized harmonic
polylogarithms evaluated at special values. The corresponding algorithms and
relations are encoded in the computer algebra package {\tt HarmonicSums}.
"
497,"Introduction to Redberry: a computer algebra system designed for tensor
  manipulation","  In this paper we introduce Redberry --- an open source computer algebra
system with native support of tensorial expressions. It provides basic computer
algebra tools (algebraic manipulations, substitutions, basic simplifications
etc.) which are aware of specific features of indexed expressions: contractions
of indices, permutational symmetries, multiple index types etc. Redberry
supports conventional \LaTeX-style input notation for tensorial expressions.
The high energy physics package includes tools for Feynman diagrams
calculation: Dirac and SU(N) algebra, Levi-Civita simplifications and tools for
one-loop calculations in quantum field theory. In the paper we give detailed
overview of Redberry features: from basic manipulations with tensors to real
Feynman diagrams calculation, accompanied by many examples. Redberry is written
in Java 7 and provides convenient Groovy-based user interface inside the
high-level general purpose programming language environment. Redberry is
available from http://redberry.cc
"
498,"Representation, simplification and display of fractional powers of
  rational numbers in computer algebra","  Simplification of fractional powers of positive rational numbers and of sums,
products and powers of such numbers is taught in beginning algebra. Such
numbers can often be expressed in many ways, as this article discusses in some
detail. Since they are such a restricted subset of algebraic numbers, it might
seem that good simplification of them must already be implemented in all widely
used computer algebra systems. However, the algorithm taught in beginning
algebra uses integer factorization, which can consume unacceptable time for the
large numbers that often arise within computer algebra. Therefore some systems
apparently use various ad hoc techniques that can return an incorrect result
because of not simplifying to 0 the difference between two equivalent such
expressions. Even systems that avoid this flaw often do not return the same
result for all equivalent such input forms, or return an unnecessarily bulky
result that does not have any other compensating useful property. This article
identifies some of these deficiencies, then describes the advantages and
disadvantages of various alternative forms and how to overcome the deficiencies
without costly integer factorization.
"
499,Computer-Aided Derivation of Multi-scale Models: A Rewriting Framework,"  We introduce a framework for computer-aided derivation of multi-scale models.
It relies on a combination of an asymptotic method used in the field of partial
differential equations with term rewriting techniques coming from computer
science.
  In our approach, a multi-scale model derivation is characterized by the
features taken into account in the asymptotic analysis. Its formulation
consists in a derivation of a reference model associated to an elementary
nominal model, and in a set of transformations to apply to this proof until it
takes into account the wanted features. In addition to the reference model
proof, the framework includes first order rewriting principles designed for
asymptotic model derivations, and second order rewriting principles dedicated
to transformations of model derivations. We apply the method to generate a
family of homogenized models for second order elliptic equations with periodic
coefficients that could be posed in multi-dimensional domains, with possibly
multi-domains and/or thin domains.
"
500,"An Algorithm for Computing the Limit Points of the Quasi-component of a
  Regular Chain","  For a regular chain $R$, we propose an algorithm which computes the
(non-trivial) limit points of the quasi-component of $R$, that is, the set
$\bar{W(R)} \setminus W(R)$. Our procedure relies on Puiseux series expansions
and does not require to compute a system of generators of the saturated ideal
of $R$. We focus on the case where this saturated ideal has dimension one and
we discuss extensions of this work in higher dimensions. We provide
experimental results illustrating the benefits of our algorithms.
"
501,Combinatorics of $\phi$-deformed stuffle Hopf algebras,"  In order to extend the Sch\""utzenberger's factorization to general
perturbations, the combinatorial aspects of the Hopf algebra of the
$\phi$-deformed stuffle product is developed systematically in a parallel way
with those of the shuffle product.
"
502,Factorization of Z-homogeneous polynomials in the First (q)-Weyl Algebra,"  We present algorithms to factorize weighted homogeneous elements in the first
polynomial Weyl algebra and $q$-Weyl algebra, which are both viewed as a
$\mathbb{Z}$-graded rings. We show, that factorization of homogeneous
polynomials can be almost completely reduced to commutative univariate
factorization over the same base field with some additional uncomplicated
combinatorial steps. This allows to deduce the complexity of our algorithms in
detail. Furthermore, we will show for homogeneous polynomials that
irreducibility in the polynomial first Weyl algebra also implies irreducibility
in the rational one, which is of interest for practical reasons. We report on
our implementation in the computer algebra system \textsc{Singular}. It
outperforms for homogeneous polynomials currently available implementations
dealing with factorization in the first Weyl algebra both in speed and elegancy
of the results.
"
503,"Exact Safety Verification of Interval Hybrid Systems Based on
  Symbolic-Numeric Computation","  In this paper, we address the problem of safety verification of interval
hybrid systems in which the coefficients are intervals instead of explicit
numbers. A hybrid symbolic-numeric method, based on SOS relaxation and interval
arithmetic certification, is proposed to generate exact inequality invariants
for safety verification of interval hybrid systems. As an application, an
approach is provided to verify safety properties of non-polynomial hybrid
systems. Experiments on the benchmark hybrid systems are given to illustrate
the efficiency of our method.
"
504,"Proceedings 7th International Workshop on Computing with Terms and
  Graphs","  This volume contains the proceedings of the Seventh International Workshop on
Computing with Terms and Graphs (TERMGRAPH 2013). The workshop took place in
Rome, Italy, on March 23rd, 2013, as part of the sixteenth edition of the
European Joint Conferences on Theory and Practice of Software (ETAPS 2013).
  Research in term and graph rewriting ranges from theoretical questions to
practical issues. Computing with graphs handles the sharing of common
subexpressions in a natural and seamless way, and improves the efficiency of
computations in space and time. Sharing is ubiquitous in several research
areas, as witnessed by the modelling of first- and higher-order term rewriting
by (acyclic or cyclic) graph rewriting, the modelling of biological or chemical
abstract machines, and the implementation techniques of programming languages:
many implementations of functional, logic, object-oriented, concurrent and
mobile calculi are based on term graphs. Term graphs are also used in automated
theorem proving and symbolic computation systems working on shared structures.
The aim of this workshop is to bring together researchers working in different
domains on term and graph transformation and to foster their interaction, to
provide a forum for presenting new ideas and work in progress, and to enable
newcomers to learn about current activities in term graph rewriting.
  These proceedings contain six accepted papers and the abstracts of two
invited talks. All submissions were subject to careful refereeing. The topics
of accepted papers range over a wide spectrum, including theoretical aspects of
term graph rewriting, concurrency, semantics as well as application issues of
term graph transformation.
"
505,Module Border Bases,"  In this paper, we generalize the notion of border bases of zero-dimensional
polynomial ideals to the module setting. To this end, we introduce order
modules as a generalization of order ideals and module border bases of
submodules with finite codimension in a free module as a generalization of
border bases of zero-dimensional ideals in the first part of this paper. In
particular, we extend the division algorithm for border bases to the module
setting, show the existence and uniqueness of module border bases, and
characterize module border bases analogously like border bases via the special
generation property, border form modules, rewrite rules, commuting matrices,
and liftings of border syzygies. Furthermore, we deduce Buchberger's Criterion
for Module Border Bases and give an algorithm for the computation of module
border bases that uses linear algebra techniques. In the second part, we
further generalize the notion of module border bases to quotient modules. We
then show the connection between quotient module border bases and special
module border bases and deduce characterizations similar to the ones for module
border bases. Moreover, we give an algorithm for the computation of quotient
module border bases using linear algebra techniques, again. At last, we prove
that subideal border bases are isomorphic to special quotient module border
bases. This isomorphy immediately yields characterizations and an algorithm for
the computation of subideal border bases.
"
506,An implementation of CAD in Maple utilising McCallum projection,"  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets. Originally introduced by Collins in the
1970s for use in quantifier elimination it has since found numerous
applications within algebraic geometry and beyond. Following from his original
work in 1988, McCallum presented an improved algorithm, CADW, which offered a
huge increase in the practical utility of CAD. In 2009 a team based at the
University of Western Ontario presented a new and quite separate algorithm for
CAD, which was implemented and included in the computer algebra system Maple.
As part of a wider project at Bath investigating CAD and its applications,
Collins and McCallum's CAD algorithms have been implemented in Maple. This
report details these implementations and compares them to Qepcad and the
Ontario algorithm.
  The implementations were originally undertaken to facilitate research into
the connections between the algorithms. However, the ability of the code to
guarantee order-invariant output has led to its use in new research on CADs
which are minimal for certain problems. In addition, the implementation
described here is of interest as the only full implementation of CADW, (since
Qepcad does not currently make use of McCallum's delineating polynomials), and
hence can solve problems not admissible to other CAD implementations.
"
507,"Normalization of Polynomials in Algebraic Invariants of
  Three-Dimensional Orthogonal Geometry","  In classical invariant theory, the Gr\""obner base of the ideal of syzygies
and the normal forms of polynomials of invariants are two core contents. To
improve the performance of invariant theory in symbolic computing of classical
geometry, advanced invariants are introduced via Clifford product. This paper
addresses and solves the two key problems in advanced invariant theory: the
Gr\""obner base of the ideal of syzygies among advanced invariants, and the
normal forms of polynomials of advanced invariants. These results beautifully
extend the straightening of Young tableaux to advanced invariants.
"
508,"Explicit Noether Normalization for Simultaneous Conjugation via
  Polynomial Identity Testing","  Mulmuley recently gave an explicit version of Noether's Normalization lemma
for ring of invariants of matrices under simultaneous conjugation, under the
conjecture that there are deterministic black-box algorithms for polynomial
identity testing (PIT). He argued that this gives evidence that constructing
such algorithms for PIT is beyond current techniques. In this work, we show
this is not the case. That is, we improve Mulmuley's reduction and
correspondingly weaken the conjecture regarding PIT needed to give explicit
Noether Normalization. We then observe that the weaker conjecture has recently
been nearly settled by the authors, who gave quasipolynomial size hitting sets
for the class of read-once oblivious algebraic branching programs (ROABPs).
This gives the desired explicit Noether Normalization unconditionally, up to
quasipolynomial factors.
  As a consequence of our proof we give a deterministic parallel
polynomial-time algorithm for deciding if two matrix tuples have intersecting
orbit closures, under simultaneous conjugation.
  We also study the strength of conjectures that Mulmuley requires to obtain
similar results as ours. We prove that his conjectures are stronger, in the
sense that the computational model he needs PIT algorithms for is equivalent to
the well-known algebraic branching program (ABP) model, which is provably
stronger than the ROABP model.
  Finally, we consider the depth-3 diagonal circuit model as defined by Saxena,
as PIT algorithms for this model also have implications in Mulmuley's work.
Previous work have given quasipolynomial size hitting sets for this model. In
this work, we give a much simpler construction of such hitting sets, using
techniques of Shpilka and Volkovich.
"
509,Elimination for generic sparse polynomial systems,"  We present a new probabilistic symbolic algorithm that, given a variety
defined in an n-dimensional affine space by a generic sparse system with fixed
supports, computes the Zariski closure of its projection to an l-dimensional
coordinate affine space with l < n. The complexity of the algorithm depends
polynomially on combinatorial invariants associated to the supports.
"
510,Domain-of-Attraction Estimation for Uncertain Non-polynomial Systems,"  In this paper, we consider the problem of computing estimates of the
domain-of-attraction for non-polynomial systems. A polynomial approximation
technique, based on multivariate polynomial interpolation and error analysis
for remaining functions, is applied to compute an uncertain polynomial system,
whose set of trajectories contains that of the original non-polynomial system.
Experiments on the benchmark non-polynomial systems show that our approach
gives better estimates of the domain-of-attraction.
"
511,"New Symbolic Algorithms For Solving A General Bordered Tridiagonal
  Linear System","  In this paper, the author present reliable symbolic algorithms for solving a
general bordered tridiagonal linear system. The first algorithm is based on the
LU decomposition of the coefficient matrix and the computational cost of it is
O(n). The second is based on The Sherman-Morrison-Woodbury formula. The
algorithms are implementable to the Computer Algebra System (CAS) such as
MAPLE, MATLAB and MATHEMATICA. Three examples are presented for the sake of
illustration.
"
512,Tropicalization of classical moduli spaces,"  The image of the complement of a hyperplane arrangement under a monomial map
can be tropicalized combinatorially using matroid theory. We apply this to
classical moduli spaces that are associated with complex reflection
arrangements. Starting from modular curves, we visit the Segre cubic, the Igusa
quartic, and moduli of marked del Pezzo surfaces of degrees 2 and 3. Our
primary example is the Burkhardt quartic, whose tropicalization is a
3-dimensional fan in 39-dimensional space. This effectuates a synthesis of
concrete and abstract approaches to tropical moduli of genus 2 curves.
"
513,"New modular multiplication and division algorithms based on continued
  fraction expansion","  In this paper, we apply results on number systems based on continued fraction
expansions to modular arithmetic. We provide two new algorithms in order to
compute modular multiplication and modular division. The presented algorithms
are based on the Euclidean algorithm and are of quadratic complexity.
"
514,Separating linear forms for bivariate systems,"  We present an algorithm for computing a separating linear form of a system of
bivariate polynomials with integer coefficients, that is a linear combination
of the variables that takes different values when evaluated at distinct
(complex) solutions of the system. In other words, a separating linear form
defines a shear of the coordinate system that sends the algebraic system in
generic position, in the sense that no two distinct solutions are vertically
aligned. The computation of such linear forms is at the core of most algorithms
that solve algebraic systems by computing rational parameterizations of the
solutions and, moreover, the computation a separating linear form is the
bottleneck of these algorithms, in terms of worst-case bit complexity. Given
two bivariate polynomials of total degree at most $d$ with integer coefficients
of bitsize at most~$\tau$, our algorithm computes a separating linear form in
$\sOB(d^{8}+d^7\tau)$ bit operations in the worst case, where the previously
known best bit complexity for this problem was $\sOB(d^{10}+d^9\tau)$ (where
$\sO$ refers to the complexity where polylogarithmic factors are omitted and
$O_B$ refers to the bit complexity).
"
515,"Rational Univariate Representations of Bivariate Systems and
  Applications","  We address the problem of solving systems of two bivariate polynomials of
total degree at most $d$ with integer coefficients of maximum bitsize $\tau$.
It is known that a linear separating form, that is a linear combination of the
variables that takes different values at distinct solutions of the system, can
be computed in $\sOB(d^{8}+d^7\tau)$ bit operations (where $O_B$ refers to bit
complexities and $\sO$ to complexities where polylogarithmic factors are
omitted) and we focus here on the computation of a Rational Univariate
Representation (RUR) given a linear separating form. We present an algorithm
for computing a RUR with worst-case bit complexity in $\sOB(d^7+d^6\tau)$ and
bound the bitsize of its coefficients by $\sO(d^2+d\tau)$. We show in addition
that isolating boxes of the solutions of the system can be computed from the
RUR with $\sOB(d^{8}+d^7\tau)$ bit operations. Finally, we show how a RUR can
be used to evaluate the sign of a bivariate polynomial (of degree at most $d$
and bitsize at most $\tau$) at one real solution of the system in
$\sOB(d^{8}+d^7\tau)$ bit operations and at all the $\Theta(d^2)$ {real}
solutions in only $O(d)$ times that for one solution.
"
516,"Numerical method for real root isolation of semi-algebraic system and
  its applications","  In this paper, based on the homotopy continuation method and the interval
Newton method, an efficient algorithm is introduced to isolate the real roots
of semi-algebraic system.
  Tests on some random examples and a variety of problems including
transcendental functions arising in many applications show that the new
algorithm reduces the cost substantially compared with the traditional symbolic
approaches.
"
517,"A Variant of the Gr\""obner Basis Algorithm for Computing Hilbert Bases","  Gr\""obner bases can be used for computing the Hilbert basis of a numerical
submonoid. By using these techniques, we provide an algorithm that calculates a
basis of a subspace of a finite-dimensional vector space over a finite prime
field given as a matrix kernel.
"
518,"Highly Scalable Multiplication for Distributed Sparse Multivariate
  Polynomials on Many-core Systems","  We present a highly scalable algorithm for multiplying sparse multivariate
polynomials represented in a distributed format. This algo- rithm targets not
only the shared memory multicore computers, but also computers clusters or
specialized hardware attached to a host computer, such as graphics processing
units or many-core coprocessors. The scal- ability on the large number of cores
is ensured by the lacks of synchro- nizations, locks and false-sharing during
the main parallel step.
"
519,"Synthesizing Switching Controllers for Hybrid Systems by Continuous
  Invariant Generation","  We extend a template-based approach for synthesizing switching controllers
for semi-algebraic hybrid systems, in which all expressions are polynomials.
This is achieved by combining a QE (quantifier elimination)-based method for
generating continuous invariants with a qualitative approach for predefining
templates. Our synthesis method is relatively complete with regard to a given
family of predefined templates. Using qualitative analysis, we discuss
heuristics to reduce the numbers of parameters appearing in the templates. To
avoid too much human interaction in choosing templates as well as the high
computational complexity caused by QE, we further investigate applications of
the SOS (sum-of-squares) relaxation approach and the template polyhedra
approach in continuous invariant generation, which are both well supported by
efficient numerical solvers.
"
520,Sparse FGLM algorithms,"  Given a zero-dimensional ideal I in K[x1,...,xn] of degree D, the
transformation of the ordering of its Groebner basis from DRL to LEX is a key
step in polynomial system solving and turns out to be the bottleneck of the
whole solving process. Thus it is of crucial importance to design efficient
algorithms to perform the change of ordering.
  The main contributions of this paper are several efficient methods for the
change of ordering which take advantage of the sparsity of multiplication
matrices in the classical FGLM algorithm. Combing all these methods, we propose
a deterministic top-level algorithm that automatically detects which method to
use depending on the input. As a by-product, we have a fast implementation that
is able to handle ideals of degree over 40000. Such an implementation
outperforms the Magma and Singular ones, as shown by our experiments.
  First for the shape position case, two methods are designed based on the
Wiedemann algorithm: the first is probabilistic and its complexity to complete
the change of ordering is O(D(N1+nlog(D))), where N1 is the number of nonzero
entries of a multiplication matrix; the other is deterministic and computes the
LEX Groebner basis of the radical of I via Chinese Remainder Theorem. Then for
the general case, the designed method is characterized by the
Berlekamp-Massey-Sakata algorithm from Coding Theory to handle the
multi-dimensional linearly recurring relations. Complexity analyses of all
proposed methods are also provided.
  Furthermore, for generic polynomial systems, we present an explicit formula
for the estimation of the sparsity of one main multiplication matrix, and prove
its construction is free. With the asymptotic analysis of such sparsity, we are
able to show for generic systems the complexity above becomes $O(\sqrt{6/n \pi}
D^{2+(n-1)/n}})$.
"
521,"A probabilistic algorithm to compute the real dimension of a
  semi-algebraic set","  Let $\RR$ be a real closed field (e.g. the field of real numbers) and
$\mathscr{S} \subset \RR^n$ be a semi-algebraic set defined as the set of
points in $\RR^n$ satisfying a system of $s$ equalities and inequalities of
multivariate polynomials in $n$ variables, of degree at most $D$, with
coefficients in an ordered ring $\ZZ$ contained in $\RR$. We consider the
problem of computing the {\em real dimension}, $d$, of $\mathscr{S}$. The real
dimension is the first topological invariant of interest; it measures the
number of degrees of freedom available to move in the set. Thus, computing the
real dimension is one of the most important and fundamental problems in
computational real algebraic geometry. The problem is ${\rm
NP}_{\mathbb{R}}$-complete in the Blum-Shub-Smale model of computation. The
current algorithms (probabilistic or deterministic) for computing the real
dimension have complexity $(s \, D)^{O(d(n-d))}$, that becomes $(s \,
D)^{O(n^2)}$ in the worst-case. The existence of a probabilistic or
deterministic algorithm for computing the real dimension with single
exponential complexity with a factor better than ${O(n^2)}$ in the exponent in
the worst-case, is a longstanding open problem. We provide a positive answer to
this problem by introducing a probabilistic algorithm for computing the real
dimension of a semi-algebraic set with complexity $(s\, D)^{O(n)}$.
"
522,Symbolic Arithmetic and Integer Factorization,"  In this paper, we create a systematic and automatic procedure for
transforming the integer factorization problem into the problem of solving a
system of Boolean equations. Surprisingly, the resulting system of Boolean
equations takes on a ""life of its own"" and becomes a new type of integer, which
we call a generic integer.
  We then proceed to use the newly found algebraic structure of the ring of
generic integers to create two new integer factoring algorithms, called
respectively the Boolean factoring (BF) algorithm, and the multiplicative
Boolean factoring (MBF) algorithm. Although these two algorithms are not
competitive with current classical integer factoring algorithms, it is hoped
that they will become stepping stones to creating much faster and more
competitive algorithms, and perhaps be precursors of a new quantum algorithm
for integer factoring.
"
523,Faster sparse interpolation of straight-line programs,"  We give a new probabilistic algorithm for interpolating a ""sparse"" polynomial
f given by a straight-line program. Our algorithm constructs an approximation
f* of f, such that their difference probably has at most half the number of
terms of f, then recurses on their difference. Our approach builds on previous
work by Garg and Schost (2009), and Giesbrecht and Roche (2011), and is
asymptotically more efficient in terms of the total cost of the probes required
than previous methods, in many cases.
"
524,Simplifying Multiple Sums in Difference Fields,"  In this survey article we present difference field algorithms for symbolic
summation. Special emphasize is put on new aspects in how the summation
problems are rephrased in terms of difference fields, how the problems are
solved there, and how the derived results in the given difference field can be
reinterpreted as solutions of the input problem. The algorithms are illustrated
with the Mathematica package \SigmaP\ by discovering and proving new harmonic
number identities extending those from (Paule and Schneider, 2003). In
addition, the newly developed package \texttt{EvaluateMultiSums} is introduced
that combines the presented tools. In this way, large scale summation problems
for the evaluation of Feynman diagrams in QCD (Quantum ChromoDynamics) can be
solved completely automatically.
"
525,"Efficient Calculation of Determinants of Symbolic Matrices with Many
  Variables","  Efficient matrix determinant calculations have been studied since the 19th
century. Computers expand the range of determinants that are practically
calculable to include matrices with symbolic entries. However, the fastest
determinant algorithms for numerical matrices are often not the fastest for
symbolic matrices with many variables. We compare the performance of two
algorithms, fraction-free Gaussian elimination and minor expansion, on symbolic
matrices with many variables. We show that, under a simplified theoretical
model, minor expansion is faster in most situations. We then propose
optimizations for minor expansion and demonstrate their effectiveness with
empirical data.
"
526,Intrinsic complexity estimates in polynomial optimization,"  It is known that point searching in basic semialgebraic sets and the search
for globally minimal points in polynomial optimization tasks can be carried out
using $(s\,d)^{O(n)}$ arithmetic operations, where $n$ and $s$ are the numbers
of variables and constraints and $d$ is the maximal degree of the polynomials
involved.\spar \noindent We associate to each of these problems an intrinsic
system degree which becomes in worst case of order $(n\,d)^{O(n)}$ and which
measures the intrinsic complexity of the task under consideration.\spar
\noindent We design non-uniformly deterministic or uniformly probabilistic
algorithms of intrinsic, quasi-polynomial complexity which solve these
problems.
"
527,Plane mixed discriminants and toric jacobians,"  Polynomial algebra offers a standard approach to handle several problems in
geometric modeling. A key tool is the discriminant of a univariate polynomial,
or of a well-constrained system of polynomial equations, which expresses the
existence of a multiple root. We concentrate on bivariate polynomials and
establish an original formula that relates the mixed discriminant of two
bivariate Laurent polynomials with fixed support, with the sparse resultant of
these polynomials and their toric Jacobian. This allows us to obtain a new
proof for the bidegree of the mixed discriminant as well as to establish
multipicativity formulas arising when one polynomial can be factored.
"
528,Polynomial Systems Solving by Fast Linear Algebra,"  Polynomial system solving is a classical problem in mathematics with a wide
range of applications. This makes its complexity a fundamental problem in
computer science. Depending on the context, solving has different meanings. In
order to stick to the most general case, we consider a representation of the
solutions from which one can easily recover the exact solutions or a certified
approximation of them. Under generic assumption, such a representation is given
by the lexicographical Gr\""obner basis of the system and consists of a set of
univariate polynomials. The best known algorithm for computing the
lexicographical Gr\""obner basis is in $\widetilde{O}(d^{3n})$ arithmetic
operations where $n$ is the number of variables and $d$ is the maximal degree
of the equations in the input system. The notation $\widetilde{O}$ means that
we neglect polynomial factors in $n$. We show that this complexity can be
decreased to $\widetilde{O}(d^{\omega n})$ where $2 \leq \omega < 2.3727$ is
the exponent in the complexity of multiplying two dense matrices. Consequently,
when the input polynomial system is either generic or reaches the B\'ezout
bound, the complexity of solving a polynomial system is decreased from
$\widetilde{O}(D^3)$ to $\widetilde{O}(D^\omega)$ where $D$ is the number of
solutions of the system. To achieve this result we propose new algorithms which
rely on fast linear algebra. When the degree of the equations are bounded
uniformly by a constant we propose a deterministic algorithm. In the unbounded
case we present a Las Vegas algorithm.
"
529,"Reduced Gr\""obner Bases and Macaulay-Buchberger Basis Theorem over
  Noetherian Rings","  In this paper, we extend the characterization of $\mathbb{Z}[x]/\ < f \ >$,
where $f \in \mathbb{Z}[x]$ to be a free $\mathbb{Z}$-module to multivariate
polynomial rings over any commutative Noetherian ring, $A$. The
characterization allows us to extend the Gr\""obner basis method of computing a
$\Bbbk$-vector space basis of residue class polynomial rings over a field
$\Bbbk$ (Macaulay-Buchberger Basis Theorem) to rings, i.e.
$A[x_1,\ldots,x_n]/\mathfrak{a}$, where $\mathfrak{a} \subseteq
A[x_1,\ldots,x_n]$ is an ideal. We give some insights into the characterization
for two special cases, when $A = \mathbb{Z}$ and $A =
\Bbbk[\theta_1,\ldots,\theta_m]$. As an application of this characterization,
we show that the concept of border bases can be extended to rings when the
corresponding residue class ring is a finitely generated, free $A$-module.
"
530,"Harmonic Sums, Polylogarithms, Special Numbers, and their
  Generalizations","  In these introductory lectures we discuss classes of presently known nested
sums, associated iterated integrals, and special constants which hierarchically
appear in the evaluation of massless and massive Feynman diagrams at higher
loops. These quantities are elements of stuffle and shuffle algebras implying
algebraic relations being widely independent of the special quantities
considered. They are supplemented by structural relations. The generalizations
are given in terms of generalized harmonic sums, (generalized) cyclotomic sums,
and sums containing in addition binomial and inverse-binomial weights. To all
these quantities iterated integrals and special numbers are associated. We also
discuss the analytic continuation of nested sums of different kind to complex
values of the external summation bound N.
"
531,Optimising Problem Formulation for Cylindrical Algebraic Decomposition,"  Cylindrical algebraic decomposition (CAD) is an important tool for the study
of real algebraic geometry with many applications both within mathematics and
elsewhere. It is known to have doubly exponential complexity in the number of
variables in the worst case, but the actual computation time can vary greatly.
It is possible to offer different formulations for a given problem leading to
great differences in tractability. In this paper we suggest a new measure for
CAD complexity which takes into account the real geometry of the problem. This
leads to new heuristics for choosing: the variable ordering for a CAD problem,
a designated equational constraint, and formulations for truth-table invariant
CADs (TTICADs). We then consider the possibility of using Groebner bases to
precondition TTICAD and when such formulations constitute the creation of a new
problem.
"
532,Understanding Branch Cuts of Expressions,"  We assume some standard choices for the branch cuts of a group of functions
and consider the problem of then calculating the branch cuts of expressions
involving those functions. Typical examples include the addition formulae for
inverse trigonometric functions. Understanding these cuts is essential for
working with the single-valued counterparts, the common approach to encoding
multi-valued functions in computer algebra systems. While the defining choices
are usually simple (typically portions of either the real or imaginary axes)
the cuts induced by the expression may be surprisingly complicated. We have
made explicit and implemented techniques for calculating the cuts in the
computer algebra programme Maple. We discuss the issues raised, classifying the
different cuts produced. The techniques have been gathered in the BranchCuts
package, along with tools for visualising the cuts. The package is included in
Maple 17 as part of the FunctionAdvisor tool.
"
533,"A Symbolic Approach to Boundary Problems for Linear Partial Differential
  Equations: Applications to the Completely Reducible Case of the Cauchy
  Problem with Constant Coefficients","  We introduce a general algebraic setting for describing linear boundary
problems in a symbolic computation context, with emphasis on the case of
partial differential equations. The general setting is then applied to the
Cauchy problem for completely reducible partial differential equations with
constant coefficients. While we concentrate on the theoretical features in this
paper, the underlying operator ring is implemented and provides a sufficient
basis for all methods presented here.
"
534,Cylindrical Algebraic Decompositions for Boolean Combinations,"  This article makes the key observation that when using cylindrical algebraic
decomposition (CAD) to solve a problem with respect to a set of polynomials, it
is not always the signs of those polynomials that are of paramount importance
but rather the truth values of certain quantifier free formulae involving them.
This motivates our definition of a Truth Table Invariant CAD (TTICAD). We
generalise the theory of equational constraints to design an algorithm which
will efficiently construct a TTICAD for a wide class of problems, producing
stronger results than when using equational constraints alone. The algorithm is
implemented fully in Maple and we present promising results from
experimentation.
"
535,Abstract Stobjs and Their Application to ISA Modeling,"  We introduce a new ACL2 feature, the abstract stobj, and show how to apply it
to modeling the instruction set architecture of a microprocessor. Benefits of
abstract stobjs over traditional (""concrete"") stobjs can include faster
execution, support for symbolic simulation, more efficient reasoning, and
resilience of proof developments under modeling optimization.
"
536,Fast Approximate Polynomial Multipoint Evaluation and Applications,"  It is well known that, using fast algorithms for polynomial multiplication
and division, evaluation of a polynomial $F \in \mathbb{C}[x]$ of degree $n$ at
$n$ complex-valued points can be done with $\tilde{O}(n)$ exact field
operations in $\mathbb{C},$ where $\tilde{O}(\cdot)$ means that we omit
polylogarithmic factors. We complement this result by an analysis of
approximate multipoint evaluation of $F$ to a precision of $L$ bits after the
binary point and prove a bit complexity of $\tilde{O}(n(L + \tau + n\Gamma)),$
where $2^\tau$ and $2^\Gamma,$ with $\tau, \Gamma \in \mathbb{N}_{\ge 1},$ are
bounds on the magnitude of the coefficients of $F$ and the evaluation points,
respectively. In particular, in the important case where the precision demand
dominates the other input parameters, the complexity is soft-linear in $n$ and
$L$.
  Our result on approximate multipoint evaluation has some interesting
consequences on the bit complexity of further approximation algorithms which
all use polynomial evaluation as a key subroutine. Of these applications, we
discuss in detail an algorithm for polynomial interpolation and for computing a
Taylor shift of a polynomial. Furthermore, our result can be used to derive
improved complexity bounds for algorithms to refine isolating intervals for the
real roots of a polynomial. For all of the latter algorithms, we derive
near-optimal running times.
"
537,Generalization of Risch's Algorithm to Special Functions,"  Symbolic integration deals with the evaluation of integrals in closed form.
We present an overview of Risch's algorithm including recent developments. The
algorithms discussed are suited for both indefinite and definite integration.
They can also be used to compute linear relations among integrals and to find
identities for special functions given by parameter integrals. The aim of this
presentation is twofold: to introduce the reader to some basic ideas of
differential algebra in the context of integration and to raise awareness in
the physics community of computer algebra algorithms for indefinite and
definite integration.
"
538,Numerical Reparametrization of Rational Parametric Plane Curves,"  In this paper, we present an algorithm for reparametrizing algebraic plane
curves from a numerical point of view. That is, we deal with mathematical
objects that are assumed to be given approximately. More precisely, given a
tolerance $\epsilon>0$ and a rational parametrization $\cal P$ with perturbed
float coefficients of a plane curve $\cal C$, we present an algorithm that
computes a parametrization $\cal Q$ of a new plane curve $\cal D$ such that
${\cal Q}$ is an {\it $\epsilon$--proper reparametrization} of $\cal D$. In
addition, the error bound is carefully discussed and we present a formula that
measures the ""closeness"" between the input curve $\cal C$ and the output curve
$\cal D$.
"
539,Characterization of Rational Ruled Surfaces,"  The ruled surface is a typical modeling surface in computer aided geometric
design. It is usually given in the standard parametric form. However, it can
also be in the forms than the standard one. For these forms, it is necessary to
determine and find the standard form. In this paper, we present algorithms to
determine whether a given implicit surface is a rational ruled surface. A
parametrization of the surface is computed for the affirmative case. We also
consider the parametric situation. More precisely, after a given rational
parametric surface is determined as a ruled one, we reparameterize it to the
standard form.
"
540,Determination and (re)parametrization of rational developable surfaces,"  The developable surface is an important surface in computer aided design,
geometric modeling and industrial manufactory. It is often given in the stan-
dard parametric form, but it can also be in the implicit form which is commonly
used in algebraic geometry. Not all algebraic developable surfaces have
rational parametrizations. In this paper, we focus on the rational developable
surfaces. For a given algebraic surface, we first determine whether it is
developable by geometric inspection, and we give a rational proper
parametrization for the af- firmative case. For a rational parametric surface,
we can also determine the developability and give a proper reparametrization
for the developable surface.
"
541,A computer algebra user interface manifesto,"  Many computer algebra systems have more than 1000 built-in functions, making
expertise difficult. Using mock dialog boxes, this article describes a proposed
interactive general-purpose wizard for organizing optional transformations and
allowing easy fine grain control over the form of the result even by amateurs.
This wizard integrates ideas including:
  * flexible subexpression selection;
  * complete control over the ordering of variables and commutative operands,
with well-chosen defaults;
  * interleaving the choice of successively less main variables with applicable
function choices to provide detailed control without incurring a combinatorial
number of applicable alternatives at any one level;
  * quick applicability tests to reduce the listing of inapplicable
transformations;
  * using an organizing principle to order the alternatives in a helpful
manner;
  * labeling quickly-computed alternatives in dialog boxes with a preview of
their results,
  * using ellipsis elisions if necessary or helpful;
  * allowing the user to retreat from a sequence of choices to explore other
branches of the tree of alternatives or to return quickly to branches already
visited;
  * allowing the user to accumulate more than one of the alternative forms;
  * integrating direct manipulation into the wizard; and
  * supporting not only the usual input-result pair mode, but also the useful
alternative derivational and in situ replacement modes in a unified window.
"
542,Zero-nonzero and real-nonreal sign determination,"  We consider first the zero-nonzero determination problem, which consists in
determining the list of zero-nonzero conditions realized by a finite list of
polynomials on a finite set Z included in C^k with C an algebraic closed field.
We describe an algorithm to solve the zero-nonzero determination problem and we
perform its bit complexity analysis. This algorithm, which is in many ways an
adaptation of the methods used to solve the more classical sign determination
problem, presents also new ideas which can be used to improve sign
determination. Then, we consider the real-nonreal sign determination problem,
which deals with both the sign determination and the zero-nonzero determination
problem. We describe an algorithm to solve the real-nonreal sign determination
problem, we perform its bit complexity analysis and we discuss this problem in
a parametric context.
"
543,"Dual bases for non commutative symmetric and quasi-symmetric functions
  via monoidal factorization","  In this work, an effective construction, via Sch\""utzenberger's monoidal
factorization, of dual bases for the non commutative symmetric and
quasi-symmetric functions is proposed.
"
544,"Sch\""utzenberger's factorization on the (completed) Hopf algebra of
  $q-$stuffle product","  In order to extend the Sch\""utzenberger's factorization, the combinatorial
Hopf algebra of the $q$-stuffles product is developed systematically in a
parallel way with that of the shuffle product and and in emphasizing the Lie
elements as studied by Ree. In particular, we will give here an effective
construction of pair of bases in duality.
"
545,"Computer-Assisted Proofs of Some Identities for Bessel Functions of
  Fractional Order","  We employ computer algebra algorithms to prove a collection of identities
involving Bessel functions with half-integer orders and other special
functions. These identities appear in the famous Handbook of Mathematical
Functions, as well as in its successor, the DLMF, but their proofs were lost.
We use generating functions and symbolic summation techniques to produce new
proofs for them.
"
546,"Effective Differential Nullstellensatz for Ordinary DAE Systems with
  Constant Coefficients","  We give upper bounds for the differential Nullstellensatz in the case of
ordinary systems of differential algebraic equations over any field of
constants $K$ of characteristic $0$. Let $\vec{x}$ be a set of $n$ differential
variables, $\vec{f}$ a finite family of differential polynomials in the ring
$K\{\vec{x}\}$ and $f\in K\{\vec{x}\}$ another polynomial which vanishes at
every solution of the differential equation system $\vec{f}=0$ in any
differentially closed field containing $K$. Let $d:=\max\{\deg(\vec{f}),
\deg(f)\}$ and $\epsilon:=\max\{2,{\rm{ord}}(\vec{f}), {\rm{ord}}(f)\}$. We
show that $f^M$ belongs to the algebraic ideal generated by the successive
derivatives of $\vec{f}$ of order at most $L = (n\epsilon
d)^{2^{c(n\epsilon)^3}}$, for a suitable universal constant $c>0$, and
$M=d^{n(\epsilon +L+1)}$. The previously known bounds for $L$ and $M$ are not
elementary recursive.
"
547,"Intertwining Laplace Transformations of Linear Partial Differential
  Equations","  We propose a generalization of Laplace transformations to the case of linear
partial differential operators (LPDOs) of arbitrary order in R^n. Practically
all previously proposed differential transformations of LPDOs are particular
cases of this transformation (intertwining Laplace transformation, ILT). We
give a complete algorithm of construction of ILT and describe the classes of
operators in R^n suitable for this transformation.
  Keywords: Integration of linear partial differential equations, Laplace
transformation, differential transformation
"
548,"Relativistic Coulomb Integrals and Zeilberger's Holonomic Systems
  Approach II","  We derive the recurrence relations for relativistic Coulomb integrals
directly from the integral representations with the help of computer algebra
methods. In order to manage the computational complexity of this problem, we
employ holonomic closure properties in a sophisticated way.
"
549,"An implementation of CAD in Maple utilising problem formulation,
  equational constraints and truth-table invariance","  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications within algebraic
geometry and beyond. We recently reported on a new implementation of CAD in
Maple which implemented the original algorithm of Collins and the subsequent
improvement to projection by McCallum. Our implementation was in contrast to
Maple's in-built CAD command, based on a quite separate theory. Although
initially developed as an investigative tool to compare the algorithms, we
found and reported that our code offered functionality not currently available
in any other existing implementations. One particularly important piece of
functionality is the ability to produce order-invariant CADs. This has allowed
us to extend the implementation to produce CADs invariant with respect to
either equational constraints (ECCADs) or the truth-tables of sequences of
formulae (TTICADs). This new functionality is contained in the second release
of our code, along with commands to consider problem formulation which can be a
major factor in the tractability of a CAD. In the report we describe the new
functionality and some theoretical discoveries it prompted. We describe how the
CADs produced using equational constraints are able to take advantage of not
just improved projection but also improvements in the lifting phase. We also
present an extension to the original TTICAD algorithm which increases both the
applicability of TTICAD and its relative benefit over other algorithms. The
code and an introductory Maple worksheet / pdf demonstrating the full
functionality of the package are freely available online.
"
550,Degeneracy loci and polynomial equation solving,"  Let V be a smooth equidimensional quasi-affine variety of dimension r over
the complex numbers $C$ and let $F$ be a $(p\times s)$-matrix of coordinate
functions of $C[V]$, where $s\ge p+r$. The pair $(V,F)$ determines a vector
bundle $E$ of rank $s-p$ over $W:=\{x\in V:\mathrm{rk} F(x)=p\}$. We associate
with $(V,F)$ a descending chain of degeneracy loci of E (the generic polar
varieties of $V$ represent a typical example of this situation).
  The maximal degree of these degeneracy loci constitutes the essential
ingredient for the uniform, bounded error probabilistic pseudo-polynomial time
algorithm which we are going to design and which solves a series of
computational elimination problems that can be formulated in this framework. We
describe applications to polynomial equation solving over the reals and to the
computation of a generic fiber of a dominant endomorphism of an affine space.
"
551,"An approach to first principles electronic structure calculation by
  symbolic-numeric computation","  This article is an introduction to a new approach to first principles
electronic structure calculation. The starting point is the
Hartree-Fock-Roothaan equation, in which molecular integrals are approximated
by polynomials by way of Taylor expansion with respect to atomic coordinates
and other variables. It leads to a set of polynomial equations whose solutions
are eigenstate, which is designated as algebraic molecular orbital equation.
Symbolic computation, especially, Gr\""obner bases theory, enables us to rewrite
the polynomial equations into more trimmed and tractable forms with identical
roots, from which we can unravel the relationship between physical parameters
(wave function, atomic coordinates, and others) and numerically evaluate them
one by one in order. Furthermore, this method is a unified way to solve the
electronic structure calculation, the optimization of physical parameters, and
the inverse problem as a forward problem.
"
552,Deciding Nonnegativity of Polynomials by MAPLE,"  There have been some effective tools for solving (constant/parametric)
semi-algebraic systems in Maple's library RegularChains since Maple 13. By
using the functions of the library, e.g., RealRootClassfication, one can prove
and discover polynomial inequalities. This paper is more or less a user guide
on using RealRootClassfication to prove the nonnegativity of polynomials. We
show by examples how to use this powerful tool to prove a polynomial is
nonnegative under some polynomial inequality and/or equation constraints. Some
tricks for using the tool are also provided.
"
553,Ore Polynomials in Sage,"  We present a Sage implementation of Ore algebras. The main features for the
most common instances include basic arithmetic and actions; gcrd and lclm;
D-finite closure properties; natural transformations between related algebras;
guessing; desingularization; solvers for polynomials, rational functions and
(generalized) power series. This paper is a tutorial on how to use the package.
"
554,Detecting Similarity of Rational Plane Curves,"  A novel and deterministic algorithm is presented to detect whether two given
rational plane curves are related by means of a similarity, which is a central
question in Pattern Recognition. As a by-product it finds all such
similarities, and the particular case of equal curves yields all symmetries. A
complete theoretical description of the method is provided, and the method has
been implemented and tested in the Sage system for curves of moderate degrees.
"
555,Software for Evaluating Relevance of Steps in Algebraic Transformations,"  Students of our department solve algebraic exercises in mathematical logic in
a computerized environment. They construct transformations step by step and the
program checks the syntax, equivalence of expressions and completion of the
task. With our current project, we add a program component for checking
relevance of the steps.
"
556,Involutive Bases Algorithm Incorporating F5 Criterion,"  Faugere's F5 algorithm is the fastest known algorithm to compute Groebner
bases. It has a signature-based and an incremental structure that allow to
apply the F5 criterion for deletion of unnecessary reductions. In this paper,
we present an involutive completion algorithm which outputs a minimal
involutive basis. Our completion algorithm has a nonincremental structure and
in addition to the involutive form of Buchberger's criteria it applies the F5
criterion whenever this criterion is applicable in the course of completion to
involution. In doing so, we use the G2V form of the F5 criterion developed by
Gao, Guan and Volny IV. To compare the proposed algorithm, via a set of
benchmarks, with the Gerdt-Blinkov involutive algorithm (which does not apply
the F5 criterion) we use implementations of both algorithms done on the same
platform in Maple.
"
557,Computing Puiseux Expansions at Cusps of the Modular Curve X0(N),"  The goal in this preprint is to give an efficient algorithm to compute
Puiseux expansions at cusps of X0(N). It is based on a relation with a
hypergeometric function that holds for any N.
"
558,"Theorema 2.0: A Graphical User Interface for a Mathematical Assistant
  System","  Theorema 2.0 stands for a re-design including a complete re-implementation of
the Theorema system, which was originally designed, developed, and implemented
by Bruno Buchberger and his Theorema group at RISC. In this paper, we present
the first prototype of a graphical user interface (GUI) for the new system. It
heavily relies on powerful interactive capabilities introduced in recent
releases of the underlying Mathematica system, most importantly the possibility
of having dynamic objects connected to interface elements like sliders, menus,
check-boxes, radio-buttons and the like. All these features are fully
integrated into the Mathematica programming environment and allow the
implementation of a modern user interface.
"
559,An Efficient Multiplication Algorithm Using Nikhilam Method,"  Multiplication is one of the most important operation in computer arithmetic.
Many integer operations such as squaring, division and computing reciprocal
require same order of time as multiplication whereas some other operations such
as computing GCD and residue operation require at most a factor of $\log n$
time more than multiplication. We propose an integer multiplication algorithm
using Nikhilam method of Vedic mathematics which can be used to multiply two
binary numbers efficiently.
"
560,Certification of Bounds of Non-linear Functions: the Templates Method,"  The aim of this work is to certify lower bounds for real-valued multivariate
functions, defined by semialgebraic or transcendental expressions. The
certificate must be, eventually, formally provable in a proof system such as
Coq. The application range for such a tool is widespread; for instance Hales'
proof of Kepler's conjecture yields thousands of inequalities. We introduce an
approximation algorithm, which combines ideas of the max-plus basis method (in
optimal control) and of the linear templates method developed by Manna et al.
(in static analysis). This algorithm consists in bounding some of the
constituents of the function by suprema of quadratic forms with a well chosen
curvature. This leads to semialgebraic optimization problems, solved by
sum-of-squares relaxations. Templates limit the blow up of these relaxations at
the price of coarsening the approximation. We illustrate the efficiency of our
framework with various examples from the literature and discuss the interfacing
with Coq.
"
561,Creative Telescoping for Holonomic Functions,"  The aim of this article is twofold: on the one hand it is intended to serve
as a gentle introduction to the topic of creative telescoping, from a practical
point of view; for this purpose its application to several problems is
exemplified. On the other hand, this chapter has the flavour of a survey
article: the developments in this area during the last two decades are sketched
and a selection of references is compiled in order to highlight the impact of
creative telescoping in numerous contexts.
"
562,Functional framework for representing and transforming quantum channels,"  We develop a framework which aims to simplify the analysis of quantum states
and quantum operations by harnessing the potential of function programming
paradigm. We show that the introduced framework allows a seamless manipulation
of quantum channels, in particular to convert between different representations
of quantum channels, and thus that the use of functional programming concepts
facilitates the manipulation of abstract objects used in the language of
quantum theory.
  For the purpose of our presentation we will use Mathematica computer algebra
system. This choice is motivated twofold. First, it offers a rich programming
language based on the functional paradigm. Second, this programming language is
combined with powerful symbolic and numeric manipulation capabilities.
"
563,"Polynomial-Time Algorithms for Quadratic Isomorphism of Polynomials: The
  Regular Case","  Let $\mathbf{f}=(f\_1,\ldots,f\_m)$ and $\mathbf{g}=(g\_1,\ldots,g\_m)$ be
two sets of $m\geq 1$ nonlinear polynomials over $\mathbb{K}[x\_1,\ldots,x\_n]$
($\mathbb{K}$ being a field). We consider the computational problem of finding
-- if any -- an invertible transformation on the variables mapping $\mathbf{f}$
to $\mathbf{g}$. The corresponding equivalence problem is known as {\tt
Isomorphism of Polynomials with one Secret} ({\tt IP1S}) and is a fundamental
problem in multivariate cryptography. The main result is a randomized
polynomial-time algorithm for solving {\tt IP1S} for quadratic instances, a
particular case of importance in cryptography and somewhat justifying {\it a
posteriori} the fact that {\it Graph Isomorphism} reduces to only cubic
instances of {\tt IP1S} (Agrawal and Saxena). To this end, we show that {\tt
IP1S} for quadratic polynomials can be reduced to a variant of the classical
module isomorphism problem in representation theory, which involves to test the
orthogonal simultaneous conjugacy of symmetric matrices. We show that we can
essentially {\it linearize} the problem by reducing quadratic-{\tt IP1S} to
test the orthogonal simultaneous similarity of symmetric matrices; this latter
problem was shown by Chistov, Ivanyos and Karpinski to be equivalent to finding
an invertible matrix in the linear space $\mathbb{K}^{n \times n}$ of $n \times
n$ matrices over $\mathbb{K}$ and to compute the square root in a matrix
algebra. While computing square roots of matrices can be done efficiently using
numerical methods, it seems difficult to control the bit complexity of such
methods. However, we present exact and polynomial-time algorithms for computing
the square root in $\mathbb{K}^{n \times n}$ for various fields (including
finite fields). We then consider \\#{\tt IP1S}, the counting version of {\tt
IP1S} for quadratic instances. In particular, we provide a (complete)
characterization of the automorphism group of homogeneous quadratic
polynomials. Finally, we also consider the more general {\it Isomorphism of
Polynomials} ({\tt IP}) problem where we allow an invertible linear
transformation on the variables \emph{and} on the set of polynomials. A
randomized polynomial-time algorithm for solving {\tt IP} when
\(\mathbf{f}=(x\_1^d,\ldots,x\_n^d)\) is presented. From an algorithmic point
of view, the problem boils down to factoring the determinant of a linear matrix
(\emph{i.e.}\ a matrix whose components are linear polynomials). This extends
to {\tt IP} a result of Kayal obtained for {\tt PolyProj}.
"
564,"On Computing the Elimination Ideal Using Resultants with Applications to
  Gr\""obner Bases","  Resultants and Gr\""obner bases are crucial tools in studying polynomial
elimination theory. We investigate relations between the variety of the
resultant of two polynomials and the variety of the ideal they generate. Then
we focus on the bivariate case, in which the elimination ideal is principal. We
study - by means of elementary tools - the difference between the multiplicity
of the factors of the generator of the elimination ideal and the multiplicity
of the factors of the resultant.
"
565,Fast polynomial evaluation and composition,"  The library \emph{fast\_polynomial} for Sage compiles multivariate
polynomials for subsequent fast evaluation. Several evaluation schemes are
handled, such as H\""orner, divide and conquer and new ones can be added easily.
Notably, a new scheme is introduced that improves the classical divide and
conquer scheme when the number of terms is not a pure power of two. Natively,
the library handles polynomials over gmp big integers, boost intervals, python
numeric types. And any type that supports addition and multiplication can
extend the library thanks to the template design. Finally, the code is
parallelized for the divide and conquer schemes, and memory allocation is
localized and optimized for the different evaluation schemes. This extended
abstract presents the concepts behind the \emph{fast\_polynomial} library. The
sage package can be downloaded at
\url{http://trac.sagemath.org/sage_trac/ticket/13358}.
"
566,"A nearly optimal algorithm for deciding connectivity queries in smooth
  and bounded real algebraic sets","  A roadmap for a semi-algebraic set $S$ is a curve which has a non-empty and
connected intersection with all connected components of $S$. Hence, this kind
of object, introduced by Canny, can be used to answer connectivity queries
(with applications, for instance, to motion planning) but has also become of
central importance in effective real algebraic geometry, since it is used in
higher-level algorithms. In this paper, we provide a probabilistic algorithm
which computes roadmaps for smooth and bounded real algebraic sets. Its output
size and running time are polynomial in $(nD)^{n\log(d)}$, where $D$ is the
maximum of the degrees of the input polynomials, $d$ is the dimension of the
set under consideration and $n$ is the number of variables. More precisely, the
running time of the algorithm is essentially subquadratic in the output size.
Even under our assumptions, it is the first roadmap algorithm with output size
and running time polynomial in $(nD)^{n\log(d)}$.
"
567,"Fast Algorithms for Refined Parameterized Telescoping in Difference
  Fields","  Parameterized telescoping (including telescoping and creative telescoping)
and refined versions of it play a central role in the research area of symbolic
summation. Karr introduced 1981 $\Pi\Sigma$-fields, a general class of
difference fields, that enables one to consider this problem for indefinite
nested sums and products covering as special cases, e.g., the
($q$--)hypergeometric case and their mixed versions. This survey article
presents the available algorithms in the framework of $\Pi\Sigma$-extensions
and elaborates new results concerning efficiency.
"
568,"Proceedings Fourth International Symposium on Symbolic Computation in
  Software Science","  Symbolic computation is the science of computing with symbolic objects
(terms, formulae, programs, algebraic objects, geometrical objects, etc).
Powerful symbolic algorithms have been developed during the past decades and
have played an influential role in theorem proving, automated reasoning,
software verification, model checking, rewriting, formalisation of mathematics,
network security, Groebner bases, characteristic sets, etc.
  The international Symposium on ""Symbolic Computation in Software Science"" is
the fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the
Research Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,
SCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal
workshops that bring together researchers from: a) SCORE (Symbolic Computation
Research Group) at the University of Tsukuba, Japan, b) Theorema Group at the
Research Institute for Symbolic Computation, Johannes Kepler University Linz,
Austria, c) SSFG (Software Science Foundation Group) at Kyoto University,
Japan, and d) Sup'Com (Higher School of Communication of Tunis) at the
University of Carthage, Tunisia.
"
569,"Probabilistic Algorithm for Polynomial Optimization over a Real
  Algebraic Set","  Let $f, f_1, \ldots, f_\nV$ be polynomials with rational coefficients in the
indeterminates $\bfX=X_1, \ldots, X_n$ of maximum degree $D$ and $V$ be the set
of common complex solutions of $\F=(f_1,\ldots, f_\nV)$. We give an algorithm
which, up to some regularity assumptions on $\F$, computes an exact
representation of the global infimum $f^\star=\inf_{x\in V\cap\R^n} f\Par{x}$,
i.e. a univariate polynomial vanishing at $f^\star$ and an isolating interval
for $f^\star$. Furthermore, this algorithm decides whether $f^\star$ is reached
and if so, it returns $x^\star\in V\cap\R^n$ such that
$f\Par{x^\star}=f^\star$. This algorithm is probabilistic. It makes use of the
notion of polar varieties. Its complexity is essentially cubic in $\Par{\nV
D}^n$ and linear in the complexity of evaluating the input. This fits within
the best known deterministic complexity class $D^{O(n)}$. We report on some
practical experiments of a first implementation that is available as a Maple
package. It appears that it can tackle global optimization problems that were
unreachable by previous exact algorithms and can manage instances that are hard
to solve with purely numeric techniques. As far as we know, even under the
extra genericity assumptions on the input, it is the first probabilistic
algorithm that combines practical efficiency with good control of complexity
for this problem.
"
570,"Signature-Based Gr\""obner Basis Algorithms --- Extended MMM Algorithm
  for computing Gr\""obner bases","  Signature-based algorithms is a popular kind of algorithms for computing
Gr\""obner bases, and many related papers have been published recently. In this
paper, no new signature-based algorithms and no new proofs are presented.
Instead, a view of signature-based algorithms is given, that is,
signature-based algorithms can be regarded as an extended version of the famous
MMM algorithm. By this view, this paper aims to give an easier way to
understand signature-based Gr\""obner basis algorithms.
"
571,xTras: a field-theory inspired xAct package for Mathematica,"  We present the tensor computer algebra package xTras, which provides
functions and methods frequently needed when doing (classical) field theory.
Amongst others, it can compute contractions, make Ans\""atze, and solve
tensorial equations. It is built upon the tensor computer algebra system xAct,
a collection of packages for Mathematica.
"
572,Computing Real Roots of Real Polynomials,"  Computing the roots of a univariate polynomial is a fundamental and
long-studied problem of computational algebra with applications in mathematics,
engineering, computer science, and the natural sciences. For isolating as well
as for approximating all complex roots, the best algorithm known is based on an
almost optimal method for approximate polynomial factorization, introduced by
Pan in 2002. Pan's factorization algorithm goes back to the splitting circle
method from Schoenhage in 1982. The main drawbacks of Pan's method are that it
is quite involved and that all roots have to be computed at the same time. For
the important special case, where only the real roots have to be computed, much
simpler methods are used in practice; however, they considerably lag behind
Pan's method with respect to complexity.
  In this paper, we resolve this discrepancy by introducing a hybrid of the
Descartes method and Newton iteration, denoted ANEWDSC, which is simpler than
Pan's method, but achieves a run-time comparable to it. Our algorithm computes
isolating intervals for the real roots of any real square-free polynomial,
given by an oracle that provides arbitrary good approximations of the
polynomial's coefficients. ANEWDSC can also be used to only isolate the roots
in a given interval and to refine the isolating intervals to an arbitrary small
size; it achieves near optimal complexity for the latter task.
"
573,"Computing Equilibria of Semi-algebraic Economies Using Triangular
  Decomposition and Real Solution Classification","  In this paper, we are concerned with the problem of determining the existence
of multiple equilibria in economic models. We propose a general and complete
approach for identifying multiplicities of equilibria in semi-algebraic
economies, which may be expressed as semi-algebraic systems. The approach is
based on triangular decomposition and real solution classification, two
powerful tools of algebraic computation. Its effectiveness is illustrated by
two examples of application.
"
574,Branch Cuts in Maple 17,"  Accurate and comprehensible knowledge about the position of branch cuts is
essential for correctly working with multi-valued functions, such as the square
root and logarithm. We discuss the new tools in Maple 17 for calculating and
visualising the branch cuts of such functions, and others built up from them.
The cuts are described in an intuitive and accurate form, offering substantial
improvement on the descriptions previously available.
"
575,"A ""Piano Movers"" Problem Reformulated","  It has long been known that cylindrical algebraic decompositions (CADs) can
in theory be used for robot motion planning. However, in practice even the
simplest examples can be too complicated to tackle. We consider in detail a
""Piano Mover's Problem"" which considers moving an infinitesimally thin piano
(or ladder) through a right-angled corridor.
  Producing a CAD for the original formulation of this problem is still
infeasible after 25 years of improvements in both CAD theory and computer
hardware. We review some alternative formulations in the literature which use
differing levels of geometric analysis before input to a CAD algorithm. Simpler
formulations allow CAD to easily address the question of the existence of a
path. We provide a new formulation for which both a CAD can be constructed and
from which an actual path could be determined if one exists, and analyse the
CADs produced using this approach for variations of the problem.
  This emphasises the importance of the precise formulation of such problems
for CAD. We analyse the formulations and their CADs considering a variety of
heuristics and general criteria, leading to conclusions about tackling other
problems of this form.
"
576,"On the Complexity of Computing Critical Points with Gr\""obner Bases","  Computing the critical points of a polynomial function $q\in\mathbb
Q[X_1,\ldots,X_n]$ restricted to the vanishing locus $V\subset\mathbb R^n$ of
polynomials $f_1,\ldots, f_p\in\mathbb Q[X_1,\ldots, X_n]$ is of first
importance in several applications in optimization and in real algebraic
geometry. These points are solutions of a highly structured system of
multivariate polynomial equations involving maximal minors of a Jacobian
matrix. We investigate the complexity of solving this problem by using
Gr\""obner basis algorithms under genericity assumptions on the coefficients of
the input polynomials. The main results refine known complexity bounds (which
depend on the maximum $D=\max(deg(f_1),\ldots,deg(f_p),deg(q))$) to bounds
which depend on the list of degrees $(deg(f_1),\ldots,deg(f_p),deg(q))$: we
prove that the Gr\""obner basis computation can be performed in
$\delta^{O(\log(A)/\log(G))}$ arithmetic operations in $\mathbb Q$, where
$\delta$ is the algebraic degree of the ideal vanishing on the critical points,
and $A$ and $G$ are the arithmetic and geometric average of a multiset
constructed from the sequence of degrees. As a by-product, we prove that
solving such generic optimization problems with Gr\""obner bases requires at
most $D^{O(n)}$ arithmetic operations in $\mathbb Q$, which meets the best
known complexity bound for this problem. Finally, we illustrate these
complexity results with experiments, giving evidence that these bounds are
relevant for applications.
"
577,"HYPERgeometric functions DIfferential REduction: MATHEMATICA based
  packages for differential reduction of generalized hypergeometric functions:
  Horn hypergeometric functions of two variables","  HYPERDIRE is a project devoted to the creation of a set of Mathematica-based
programs for the differential reduction of hypergeometric functions. The
current version allows for manipulations involving the full set of Horn-type
hypergeometric functions of two variables, including 30 functions.
"
578,"Rigorous high-precision computation of the Hurwitz zeta function and its
  derivatives","  We study the use of the Euler-Maclaurin formula to numerically evaluate the
Hurwitz zeta function $\zeta(s,a)$ for $s, a \in \mathbb{C}$, along with an
arbitrary number of derivatives with respect to $s$, to arbitrary precision
with rigorous error bounds. Techniques that lead to a fast implementation are
discussed. We present new record computations of Stieltjes constants, Keiper-Li
coefficients and the first nontrivial zero of the Riemann zeta function,
obtained using an open source implementation of the algorithms described in
this paper.
"
579,"A probabilistic and deterministic modular algorithm for computing
  Groebner basis over $\Q$","  Modular algorithm are widely used in computer algebra systems (CAS), for
example to compute efficiently the gcd of multivariate polynomials. It is known
to work to compute Groebner basis over $\Q$, but it does not seem to be popular
among CAS implementers. In this paper, I will show how to check a candidate
Groebner basis (obtained by reconstruction of several Groebner basis modulo
distinct prime numbers) with a given error probability, that may be 0 if a
certified Groebner basis is desired. This algorithm is now the default
algorithm used by the Giac/Xcas computer algebra system with competitive
timings, thanks to a trick that can accelerate computing Groebner basis modulo
a prime once the computation has been done modulo another prime.
"
580,"Higher-order Reverse Automatic Differentiation with emphasis on the
  third-order","  It is commonly assumed that calculating third order information is too
expensive for most applications. But we show that the directional derivative of
the Hessian ($D^3f(x)\cdot d$) can be calculated at a cost proportional to that
of a state-of-the-art method for calculating the Hessian matrix. We do this by
first presenting a simple procedure for designing high order reverse methods
and applying it to deduce several methods including a reverse method that
calculates $D^3f(x)\cdot d$. We have implemented this method taking into
account symmetry and sparsity, and successfully calculated this derivative for
functions with a million variables. These results indicate that the use of
third order information in a general nonlinear solver, such as Halley-Chebyshev
methods, could be a practical alternative to Newton's method.
"
581,"Applications of Continuous Amortization to Bisection-based Root
  Isolation","  Continuous amortization is a technique for computing the complexity of
algorithms, and it was first presented by the author in Burr, Krahmer, & Yap
(2009). Continuous amortization can result in simpler and more straight-forward
complexity analyses, and it was used in Burr, Krahmer, & Yap (2009), Burr &
Krahmer (2012), and Sharma & Yap (2012) to provide complexity bounds for simple
root isolation algorithms. This paper greatly extends the reach of continuous
amortization to serve as an overarching technique which can be used to compute
complexity of many root isolation techniques in a straight-forward manner.
Additionally, the technique of continuous amortization is extended to higher
dimensions and to the computation of the bit-complexity of algorithms. In this
paper, six continuous amortization calculations are performed to compute
complexity bounds (on either the size of the subdivision tree or the bit
complexity) for several algorithms (including algorithms based on Sturm
sequences, Descartes' rule of signs, and polynomial evaluation); in each case,
continuous amortization achieves an optimal complexity bound.
"
582,Introduction to the Symbolic Integration System,"  Symbolic integration is an important module of a typical Computer Algebra
System. As for now, Mathematica, Matlab, Maple and Sage are all mainstream CAS.
They share the same framework for symbolic integration at some points. In this
book first we review the state of the art in the field of CAS. Then we focus on
typical frameworks of the current symbolic integration systems and summarize
the main mathematical theories behind these frameworks. Based on the
open-source computer algebra system maTHmU developed by our team in our
university, we propose a potential framework to improve the performance of the
current symbolic integration system.
"
583,"Finding Linear Dependencies in Integration-By-Parts Equations: A Monte
  Carlo Approach","  The reduction of a large number of scalar integrals to a small set of master
integrals via Laporta's algorithm is common practice in multi-loop
calculations. It is also a major bottleneck in terms of running time and memory
consumption. It involves solving a large set of linear equations where many of
the equations are linearly dependent. We propose a simple algorithm that
eliminates all linearly dependent equations from a given system, reducing the
time and space requirements of a subsequent run of Laporta's algorithm.
"
584,Modernizing PHCpack through phcpy,"  PHCpack is a large software package for solving systems of polynomial
equations. The executable phc is menu driven and file oriented. This paper
describes the development of phcpy, a Python interface to PHCpack. Instead of
navigating through menus, users of phcpy solve systems in the Python shell or
via scripts. Persistent objects replace intermediate files.
"
585,"Modern Summation Methods for Loop Integrals in Quantum Field Theory: The
  Packages Sigma, EvaluateMultiSums and SumProduction","  A large class of Feynman integrals, like e.g., two-point parameter integrals
with at most one mass and containing local operator insertions, can be
transformed to multi-sums over hypergeometric expressions. In this survey
article we present a difference field approach for symbolic summation that
enables one to simplify such definite nested sums to indefinite nested sums. In
particular, the simplification is given -if possible- in terms of harmonic
sums, generalized harmonic sums, cyclotomic harmonic sums or binomial sums.
Special emphasis is put on the developed packages Sigma, EvaluateMultiSums and
SumProduction that assist in the task to perform these simplifications
completely automatically for huge input expressions.
"
586,"On the Parameterized Complexity of Associative and Commutative
  Unification","  This paper studies the unification problem with associative, commutative, and
associative-commutative functions mainly from a viewpoint of the parameterized
complexity on the number of variables. It is shown that both associative and
associative-commutative unification problems are $W[1]$-hard. A fixed-parameter
algorithm and a polynomial-time algorithm are presented for special cases of
commutative unification in which one input term is variable-free and the number
of variables is bounded by a constant, respectively. Related results including
those on the string and tree edit distance problems with variables are shown
too.
"
587,Logspace computations for Garside groups of spindle type,"  M. Picantin introduced the notion of Garside groups of spindle type,
generalizing the 3-strand braid group. We show that, for linear Garside groups
of spindle type, a normal form and a solution to the conjugacy problem are
logspace computable. For linear Garside groups of spindle type with homogenous
presentation we compute a geodesic normal form in logspace.
"
588,"Differential elimination by differential specialization of Sylvester
  style matrices","  Differential resultant formulas are defined, for a system $\mathcal{P}$ of
$n$ ordinary Laurent differential polynomials in $n-1$ differential variables.
These are determinants of coefficient matrices of an extended system of
polynomials obtained from $\mathcal{P}$ through derivations and multiplications
by Laurent monomials. To start, through derivations, a system $ps(\mathcal{P})$
of $L$ polynomials in $L-1$ algebraic variables is obtained, which is non
sparse in the order of derivation. This enables the use of existing formulas
for the computation of algebraic resultants, of the multivariate sparse
algebraic polynomials in $ps(\mathcal{P})$, to obtain polynomials in the
differential elimination ideal generated by $\mathcal{P}$. The formulas
obtained are multiples of the sparse differential resultant defined by Li, Yuan
and Gao, and provide order and degree bounds in terms of mixed volumes in the
generic case.
"
589,Middle-Solving F4 to Compute Grobner bases for Cryptanalysis over GF(2),"  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Faugere's F4 is a well-known Grobner bases algorithm to
solve this problem. However, a serious drawback exists in the Grobner bases
based algebraic attacks, namely, any information won't be got if we couldn't
work out the Grobner bases of the polynomial equations system. In this paper,
we in-depth research the F4 algorithm over GF(2). By using S-polynomials to
replace critical pairs and computing the normal form of the productions with
respect to the field equations in certain steps, many ""redundant"" reductors are
avoided during the computation process of the F4 algorithm. By slightly
modifying the logic of F4 algorithm, we solve the univariate polynomials
appeared in the algorithm and then back-substitute the values of the solved
variables at each iteration of the algorithm. We call our improvements
Middle-Solving F4. The heuristic strategy of Middle-Solving overcomes the
drawback of algebraic attacks and well suits algebraic attacks. It has never
been applied to the Grobner bases algorithm before. Experiments to some Hidden
Field Equation instances and some classical benchmarks (Cyclic 6, Gonnet83)
show that Middle-Solving F4 is faster and uses less memory than Faugere's F4.
"
590,"Efficient Algorithms for Computing Rational First Integrals and Darboux
  Polynomials of Planar Polynomial Vector Fields","  We present fast algorithms for computing rational first integrals with
bounded degree of a planar polynomial vector field. Our approach is inspired by
an idea of Ferragut and Giacomini. We improve upon their work by proving that
rational first integrals can be computed via systems of linear equations
instead of systems of quadratic equations. This leads to a probabilistic
algorithm with arithmetic complexity $\bigOsoft(N^{2 \omega})$ and to a
deterministic algorithm solving the problem in $\bigOsoft(d^2N^{2 \omega+1})$
arithmetic operations, where $N$ denotes the given bound for the degree of the
rational first integral, and where $d \leq N$ is the degree of the vector
field, and $\omega$ the exponent of linear algebra. We also provide a fast
heuristic variant which computes a rational first integral, or fails, in
$\bigOsoft(N^{\omega+2})$ arithmetic operations. By comparison, the best
previous algorithm uses at least $d^{\omega+1}\, N^{4\omega +4}$ arithmetic
operations. We then show how to apply a similar method to the computation of
Darboux polynomials. The algorithms are implemented in a Maple package which is
available to interested readers with examples showing its efficiency.
"
591,"The 3 x 3 x 3 hyperdeterminant as a polynomial in the fundamental
  invariants for SL(3,C) x SL(3,C) x SL(3,C)","  We briefly review previous work on the invariant theory of 3 x 3 x 3 arrays.
We then recall how to generate arrays of arbitrary size m_1 x ... x m_k with
hyperdeterminant 0. Our main result is an explicit formula for the 3 x 3 x 3
hyperdeterminant as a polynomial in the fundamental invariants of degrees 6, 9
and 12 for the action of the Lie group SL(3,C) x SL(3,C) x SL(3,C). We apply
our calculations to Nurmiev's classification of normal forms for 3 x 3 x 3
arrays.
"
592,Evaluating parametric holonomic sequences using rectangular splitting,"  We adapt the rectangular splitting technique of Paterson and Stockmeyer to
the problem of evaluating terms in holonomic sequences that depend on a
parameter. This approach allows computing the $n$-th term in a recurrent
sequence of suitable type using $O(n^{1/2})$ ""expensive"" operations at the cost
of an increased number of ""cheap"" operations.
  Rectangular splitting has little overhead and can perform better than either
naive evaluation or asymptotically faster algorithms for ranges of $n$
encountered in applications. As an example, fast numerical evaluation of the
gamma function is investigated. Our work generalizes two previous algorithms of
Smith.
"
593,"A Polyhedral Method to Compute All Affine Solution Sets of Sparse
  Polynomial Systems","  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. We present a polyhedral
method to compute all affine solution sets of a polynomial system. The method
enumerates all factors contributing to a generalized permanent. Toric solution
sets are recovered as a special case of this enumeration. For sparse systems as
adjacent 2-by-2 minors our methods scale much better than the techniques from
numerical algebraic geometry.
"
594,SymbolicData:SDEval - Benchmarking for Everyone,"  In this paper we will present SDeval, a software project that contains tools
for creating and running benchmarks with a focus on problems in computer
algebra. It is built on top of the Symbolic Data project, able to translate
problems in the database into executable code for various computer algebra
systems. The included tools are designed to be very flexible to use and to
extend, such that they can be utilized even in contexts of other communities.
With the presentation of SDEval, we will also address particularities of
benchmarking in the field of computer algebra. Furthermore, with SDEval, we
provide a feasible and automatizable way of reproducing benchmarks published in
current research works, which appears to be a difficult task in general due to
the customizability of the available programs. We will simultaneously present
the current developments in the Symbolic Data project.
"
595,On Jacobian group arithmetic for typical divisors on curves,"  In a previous joint article with F. Abu Salem, we gave efficient algorithms
for Jacobian group arithmetic of ""typical"" divisor classes on C_{3,4} curves,
improving on similar results by other authors. At that time, we could only
state that a generic divisor was typical, and hence unlikely to be encountered
if one implemented these algorithms over a very large finite field. This
article pins down an explicit characterization of these typical divisors, for
an arbitrary smooth projective curve of genus g >= 1 having at least one
rational point. We give general algorithms for Jacobian group arithmetic with
these typical divisors, and prove not only that the algorithms are correct if
various divisors are typical, but also that the success of our algorithms
provides a guarantee that the resulting output is correct and that the
resulting input and/or output divisors are also typical. These results apply in
particular to our earlier algorithms for C_{3,4} curves. As a byproduct, we
obtain a further speedup of approximately 15% on our previous algorithms for
C_{3,4} curves.
"
596,Code Optimization in FORM,"  We describe the implementation of output code optimization in the open source
computer algebra system FORM. This implementation is based on recently
discovered techniques of Monte Carlo tree search to find efficient multivariate
Horner schemes, in combination with other optimization algorithms, such as
common subexpression elimination. For systems for which no specific knowledge
is provided it performs significantly better than other methods we could
compare with. Because the method has a number of free parameters, we also show
some methods by which to tune them to different types of problems.
"
597,"Composing and Factoring Generalized Green's Operators and Ordinary
  Boundary Problems","  We consider solution operators of linear ordinary boundary problems with ""too
many"" boundary conditions, which are not always solvable. These generalized
Green's operators are a certain kind of generalized inverses of differential
operators. We answer the question when the product of two generalized Green's
operators is again a generalized Green's operator for the product of the
corresponding differential operators and which boundary problem it solves.
Moreover, we show that---provided a factorization of the underlying
differential operator---a generalized boundary problem can be factored into
lower order problems corresponding to a factorization of the respective Green's
operators. We illustrate our results by examples using the Maple package
IntDiffOp, where the presented algorithms are implemented.
"
598,Automatic congruences for diagonals of rational functions,"  In this paper we use the framework of automatic sequences to study
combinatorial sequences modulo prime powers. Given a sequence whose generating
function is the diagonal of a rational power series, we provide a method, based
on work of Denef and Lipshitz, for computing a finite automaton for the
sequence modulo $p^\alpha$, for all but finitely many primes $p$. This method
gives completely automatic proofs of known results, establishes a number of new
theorems for well-known sequences, and allows us to resolve some conjectures
regarding the Ap\'ery numbers. We also give a second method, which applies to
an algebraic sequence modulo $p^\alpha$ for all primes $p$, but is
significantly slower. Finally, we show that a broad range of multidimensional
sequences possess Lucas products modulo $p$.
"
599,Exact Solutions in Structured Low-Rank Approximation,"  Structured low-rank approximation is the problem of minimizing a weighted
Frobenius distance to a given matrix among all matrices of fixed rank in a
linear space of matrices. We study exact solutions to this problem by way of
computational algebraic geometry. A particular focus lies on Hankel matrices,
Sylvester matrices and generic linear spaces.
"
600,On the length of integers in telescopers for proper hypergeometric terms,"  We show that the number of digits in the integers of a creative telescoping
relation of expected minimal order for a bivariate proper hypergeometric term
has essentially cubic growth with the problem size. For telescopers of higher
order but lower degree we obtain a quintic bound. Experiments suggest that
these bounds are tight. As applications of our results, we give an improved
bound on the maximal possible integer root of the leading coefficient of a
telescoper, and the first discussion of the bit complexity of creative
telescoping.
"
601,Chapter 10: Algebraic Algorithms,"  Our Chapter in the upcoming Volume I: Computer Science and Software
Engineering of Computing Handbook (Third edition), Allen Tucker, Teo Gonzales
and Jorge L. Diaz-Herrera, editors, covers Algebraic Algorithms, both symbolic
and numerical, for matrix computations and root-finding for polynomials and
systems of polynomials equations. We cover part of these large subjects and
include basic bibliography for further study. To meet space limitation we cite
books, surveys, and comprehensive articles with pointers to further references,
rather than including all the original technical papers.
"
602,"Computing the multilinear factors of lacunary polynomials without
  heights","  We present a deterministic algorithm which computes the multilinear factors
of multivariate lacunary polynomials over number fields. Its complexity is
polynomial in $\ell^n$ where $\ell$ is the lacunary size of the input
polynomial and $n$ its number of variables, that is in particular polynomial in
the logarithm of its degree. We also provide a randomized algorithm for the
same problem of complexity polynomial in $\ell$ and $n$.
  Over other fields of characteristic zero and finite fields of large
characteristic, our algorithms compute the multilinear factors having at least
three monomials of multivariate polynomials. Lower bounds are provided to
explain the limitations of our algorithm. As a by-product, we also design
polynomial-time deterministic polynomial identity tests for families of
polynomials which were not known to admit any.
  Our results are based on so-called Gap Theorem which reduce high-degree
factorization to repeated low-degree factorizations. While previous algorithms
used Gap Theorems expressed in terms of the heights of the coefficients, our
Gap Theorems only depend on the exponents of the polynomials. This makes our
algorithms more elementary and general, and faster in most cases.
"
603,"Analyzing Multiplicities of a Zero-dimensional Regular Set's Zeros Using
  Pseudo Squarefree Decomposition","  In this paper, we are concerned with the problem of counting the
multiplicities of a zero-dimensional regular set's zeros. We generalize the
squarefree decomposition of univariate polynomials to the so-called pseudo
squarefree decomposition of multivariate polynomials, and then propose an
algorithm for decomposing a regular set into a finite number of simple sets.
From the output of this algorithm, the multiplicities of zeros could be
directly read out, and the real solution isolation with multiplicity can also
be easily produced. Experiments with a preliminary implementation show the
efficiency of our method.
"
604,"Comprehensive Border Bases for Zero Dimensional Parametric Polynomial
  Ideals","  In this paper, we extend the idea of comprehensive Gr\""{o}bner bases given by
Weispfenning (1992) to border bases for zero dimensional parametric polynomial
ideals. For this, we introduce a notion of comprehensive border bases and
border system, and prove their existence even in the cases where they do not
correspond to any term order. We further present algorithms to compute
comprehensive border bases and border system. Finally, we study the relation
between comprehensive Gr\""{o}bner bases and comprehensive border bases w.r.t. a
term order and give an algorithm to compute such comprehensive border bases
from comprehensive Gr\""{o}bner bases.
"
605,"A Generic Position Based Method for Real Root Isolation of
  Zero-Dimensional Polynomial Systems","  We improve the local generic position method for isolating the real roots of
a zero-dimensional bivariate polynomial system with two polynomials and extend
the method to general zero-dimensional polynomial systems. The method mainly
involves resultant computation and real root isolation of univariate polynomial
equations. The roots of the system have a linear univariate representation. The
complexity of the method is $\tilde{O}_B(N^{10})$ for the bivariate case, where
$N=\max(d,\tau)$, $d$ resp., $\tau$ is an upper bound on the degree, resp., the
maximal coefficient bitsize of the input polynomials. The algorithm is
certified with probability 1 in the multivariate case. The implementation shows
that the method is efficient, especially for bivariate polynomial systems.
"
606,"On the Complexity of the F5 Gr\""obner basis Algorithm","  We study the complexity of Gr\""obner bases computation, in particular in the
generic situation where the variables are in simultaneous Noether position with
respect to the system.
  We give a bound on the number of polynomials of degree $d$ in a Gr\""obner
basis computed by Faug\`ere's $F_5$ algorithm~(Fau02) in this generic case for
the grevlex ordering (which is also a bound on the number of polynomials for a
reduced Gr\""obner basis, independently of the algorithm used). Next, we analyse
more precisely the structure of the polynomials in the Gr\""obner bases with
signatures that $F_5$ computes and use it to bound the complexity of the
algorithm.
  Our estimates show that the version of~$F_5$ we analyse, which uses only
standard Gaussian elimination techniques, outperforms row reduction of the
Macaulay matrix with the best known algorithms for moderate degrees, and even
for degrees up to the thousands if Strassen's multiplication is used. The
degree being fixed, the factor of improvement grows exponentially with the
number of variables.
"
607,"Special Algorithm for Stability Analysis of Multistable Biological
  Regulatory Systems","  We consider the problem of counting (stable) equilibriums of an important
family of algebraic differential equations modeling multistable biological
regulatory systems. The problem can be solved, in principle, using real
quantifier elimination algorithms, in particular real root classification
algorithms. However, it is well known that they can handle only very small
cases due to the enormous computing time requirements. In this paper, we
present a special algorithm which is much more efficient than the general
methods. Its efficiency comes from the exploitation of certain interesting
structures of the family of differential equations.
"
608,Large Galois groups with applications to Zariski density,"  We introduce the first provably efficient algorithm to check if a finitely
generated subgroup of an almost simple semi-simple group over the rationals is
Zariski-dense. We reduce this question to one of computing Galois groups, and
to this end we describe efficient algorithms to check if the Galois group of a
polynomial $p$ with integer coefficients is ""generic"" (which, for arbitrary
polynomials of degree $n$ means the full symmetric group $S_n,$ while for
reciprocal polynomials of degree $2n$ it means the hyperoctahedral group $C_2
\wr S_n.$). We give efficient algorithms to verify that a polynomial has Galois
group $S_n,$ and that a reciprocal polynomial has Galois group $C_2 \wr S_n.$
We show how these algorithms give efficient algorithms to check if a set of
matrices $\mathcal{G}$ in $\mathop{SL}(n, \mathbb{Z})$ or $\mathop{Sp}(2n,
\mathbb{Z})$ generate a \emph{Zariski dense} subgroup.
  The complexity of doing this in$\mathop{SL}(n, \mathbb{Z})$ is of order
$O(n^4 \log n \log \|\mathcal{G}\|)\log \epsilon$ and in $\mathop{Sp}(2n,
\mathbb{Z})$ the complexity is of order $O(n^8 \log n\log \|\mathcal{G}\|)\log
\epsilon$ In general semisimple groups we show that Zariski density can be
confirmed or denied in time of order $O(n^14 \log \|\mathcal{G}\|\log
\epsilon),$ where $\epsilon$ is the probability of a wrong ""NO"" answer, while
$\|\mathcal{G}\|$ is the measure of complexity of the input (the maximum of the
Frobenius norms of the generating matrices). The algorithms work essentially
without change over algebraic number fields, and in other semi-simple groups.
However, we restrict to the case of the special linear and symplectic groups
and rational coefficients in the interest of clarity.
"
609,"Misfortunes of a mathematicians' trio using Computer Algebra Systems:
  Can we trust?","  Computer algebra systems are a great help for mathematical research but
sometimes unexpected errors in the software can also badly affect it. As an
example, we show how we have detected an error of Mathematica computing
determinants of matrices of integer numbers: not only it computes the
determinants wrongly, but also it produces different results if one evaluates
the same determinant twice.
"
610,"Hrushovski's Algorithm for Computing the Galois Group of a Linear
  Differential Equation","  We present a detailed and simplified version of Hrushovski's algorithm that
determines the Galois group of a linear differential equation. There are three
major ingredients in this algorithm. The first is to look for a degree bound
for proto-Galois groups, which enables one to compute one of them. The second
is to determine the identity component of the Galois group that is the pullback
of a torus to the proto-Galois group. The third is to recover the Galois group
from its identity component and a finite Galois group.
"
611,"HYPERDIRE: HYPERgeometric functions DIfferential REduction: MATHEMATICA
  based packages for differential reduction of generalized hypergeometric
  functions: $F_D$ and $F_S$ Horn-type hypergeometric functions of three
  variables","  HYPERDIRE is a project devoted to the creation of a set of Mathematica based
programs for the differential reduction of hypergeometric functions. The
current version includes two parts: the first one, FdFunction, for
manipulations with Appell hypergeometric functions $F_D$ of $r$ variables; and
the second one, FsFunction, for manipulations with Lauricella-Saran
hypergeometric functions $F_S$ of three variables. Both functions are related
with one-loop Feynman diagrams.
  The published version includes also Chapter 5 with two theorems about
structure of coefficients of epsilon-expansion of the Horn-type hypergeometric
functions. As illustration, the first three coefficients of epsilon-expansion
for the Appell hypergeometric function FD of r-variables are explicitly
evaluated.
"
612,On modular computation of Groebner bases with integer coefficients,"  Let $I_1\subset I_2\subset\dots$ be an increasing sequence of ideals of the
ring $\Bbb Z[X]$, $X=(x_1,\dots,x_n)$ and let $I$ be their union. We propose an
algorithm to compute the Gr\""obner base of $I$ under the assumption that the
Gr\""obner bases of the ideal $\Bbb Q I$ of the ring $\Bbb Q[X]$ and the the
ideals $I\otimes(\Bbb Z/m\Bbb Z)$ of the rings $(\Bbb Z/m\Bbb Z)[X]$ are known.
  Such an algorithmic problem arises, for example, in the construction of
Markov and semi-Markov traces on cubic Hecke algebras.
"
613,"A Quadratically Convergent Algorithm for Structured Low-Rank
  Approximation","  Structured Low-Rank Approximation is a problem arising in a wide range of
applications in Numerical Analysis and Engineering Sciences. Given an input
matrix $M$, the goal is to compute a matrix $M'$ of given rank $r$ in a linear
or affine subspace $E$ of matrices (usually encoding a specific structure) such
that the Frobenius distance $\lVert M-M'\rVert$ is small. We propose a
Newton-like iteration for solving this problem, whose main feature is that it
converges locally quadratically to such a matrix under mild transversality
assumptions between the manifold of matrices of rank $r$ and the linear/affine
subspace $E$. We also show that the distance between the limit of the iteration
and the optimal solution of the problem is quadratic in the distance between
the input matrix and the manifold of rank $r$ matrices in $E$. To illustrate
the applicability of this algorithm, we propose a Maple implementation and give
experimental results for several applicative problems that can be modeled by
Structured Low-Rank Approximation: univariate approximate GCDs (Sylvester
matrices), low-rank Matrix completion (coordinate spaces) and denoising
procedures (Hankel matrices). Experimental results give evidence that this
all-purpose algorithm is competitive with state-of-the-art numerical methods
dedicated to these problems.
"
614,Truth Table Invariant Cylindrical Algebraic Decomposition,"  When using cylindrical algebraic decomposition (CAD) to solve a problem with
respect to a set of polynomials, it is likely not the signs of those
polynomials that are of paramount importance but rather the truth values of
certain quantifier free formulae involving them. This observation motivates our
article and definition of a Truth Table Invariant CAD (TTICAD).
  In ISSAC 2013 the current authors presented an algorithm that can efficiently
and directly construct a TTICAD for a list of formulae in which each has an
equational constraint. This was achieved by generalising McCallum's theory of
reduced projection operators. In this paper we present an extended version of
our theory which can be applied to an arbitrary list of formulae, achieving
savings if at least one has an equational constraint. We also explain how the
theory of reduced projection operators can allow for further improvements to
the lifting phase of CAD algorithms, even in the context of a single equational
constraint.
  The algorithm is implemented fully in Maple and we present both promising
results from experimentation and a complexity analysis showing the benefits of
our contributions.
"
615,Cylindrical Algebraic Sub-Decompositions,"  Cylindrical algebraic decompositions (CADs) are a key tool in real algebraic
geometry, used primarily for eliminating quantifiers over the reals and
studying semi-algebraic sets. In this paper we introduce cylindrical algebraic
sub-decompositions (sub-CADs), which are subsets of CADs containing all the
information needed to specify a solution for a given problem.
  We define two new types of sub-CAD: variety sub-CADs which are those cells in
a CAD lying on a designated variety; and layered sub-CADs which have only those
cells of dimension higher than a specified value. We present algorithms to
produce these and describe how the two approaches may be combined with each
other and the recent theory of truth-table invariant CAD.
  We give a complexity analysis showing that these techniques can offer
substantial theoretical savings, which is supported by experimentation using an
implementation in Maple.
"
616,Relating $p$-adic eigenvalues and the local Smith normal form,"  Conditions are established under which the $p$-adic valuations of the
invariant factors (diagonal entries of the Smith form) of an integer matrix are
equal to the $p$-adic valuations of the eigenvalues. It is then shown that this
correspondence is the typical case for ""most"" matrices; precise density bounds
are given for when the property holds, as well as easy transformations to this
typical case.
"
617,"A Fast Algorithm for the Inversion of Quasiseparable Vandermonde-like
  Matrices","  The results on Vandermonde-like matrices were introduced as a generalization
of polynomial Vandermonde matrices, and the displacement structure of these
matrices was used to derive an inversion formula. In this paper we first
present a fast Gaussian elimination algorithm for the polynomial
Vandermonde-like matrices. Later we use the said algorithm to derive fast
inversion algorithms for quasiseparable, semiseparable and well-free
Vandermonde-like matrices having $\mathcal{O}(n^2)$ complexity. To do so we
identify structures of displacement operators in terms of generators and the
recurrence relations(2-term and 3-term) between the columns of the basis
transformation matrices for quasiseparable, semiseparable and well-free
polynomials. Finally we present an $\mathcal{O}(n^2)$ algorithm to compute the
inversion of quasiseparable Vandermonde-like matrices.
"
618,Melikyan algebra is a deformation of a Poisson algebra,"  We prove, using computer, that the restricted Melikyan algebra of dimension
125 is a deformation of a Poisson algebra.
"
619,Essentially optimal interactive certificates in linear algebra,"  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a---possibly randomized---verification
algorithm that proves the correctness of each output. The certificates are
essentially optimal if the time (and space) complexity of verification is
essentially linear in the input size $N$, meaning $N$ times a factor
$N^{o(1)}$, i.e., a factor $N^{\eta(N)}$ with $\lim\_{N\to \infty} \eta(N)$ $=$
$0$. We give algorithms that compute essentially optimal certificates for the
positive semidefiniteness, Frobenius form, characteristic and minimal
polynomial of an $n\times n$ dense integer matrix $A$. Our certificates can be
verified in Monte-Carlo bit complexity $(n^2 \log\|A\|)^{1+o(1)}$, where
$\log\|A\|$ is the bit size of the integer entries, solving an open problem in
[Kaltofen, Nehring, Saunders, Proc.\ ISSAC 2011] subject to computational
hardness assumptions. Second, we give algorithms that compute certificates for
the rank of sparse or structured $n\times n$ matrices over an abstract field,
whose Monte Carlo verification complexity is $2$ matrix-times-vector products
$+$ $n^{1+o(1)}$ arithmetic operations in the field. For example, if the
$n\times n$ input matrix is sparse with $n^{1+o(1)}$ non-zero entries, our rank
certificate can be verified in $n^{1+o(1)}$ field operations. This extends also
to integer matrices with only an extra $\|A\|^{1+o(1)}$ factor. All our
certificates are based on interactive verification protocols with the
interaction removed by a Fiat-Shamir identification heuristic. The validity of
our verification procedure is subject to standard computational hardness
assumptions from cryptography.
"
620,Parallel Telescoping and Parameterized Picard--Vessiot Theory,"  Parallel telescoping is a natural generalization of differential
creative-telescoping for single integrals to line integrals. It computes a
linear ordinary differential operator $L$, called a parallel telescoper, for
several multivariate functions, such that the applications of $L$ to the
functions yield antiderivatives of a single function. We present a necessary
and sufficient condition guaranteeing the existence of parallel telescopers for
differentially finite functions, and develop an algorithm to compute minimal
ones for compatible hyperexponential functions. Besides computing annihilators
of parametric line integrals, we use the parallel telescoping for determining
Galois groups of parameterized partial differential systems of first order.
"
621,"Computing low-degree factors of lacunary polynomials: a Newton-Puiseux
  approach","  We present a new algorithm for the computation of the irreducible factors of
degree at most $d$, with multiplicity, of multivariate lacunary polynomials
over fields of characteristic zero. The algorithm reduces this computation to
the computation of irreducible factors of degree at most $d$ of univariate
lacunary polynomials and to the factorization of low-degree multivariate
polynomials. The reduction runs in time polynomial in the size of the input
polynomial and in $d$. As a result, we obtain a new polynomial-time algorithm
for the computation of low-degree factors, with multiplicity, of multivariate
lacunary polynomials over number fields, but our method also gives partial
results for other fields, such as the fields of $p$-adic numbers or for
absolute or approximate factorization for instance.
  The core of our reduction uses the Newton polygon of the input polynomial,
and its validity is based on the Newton-Puiseux expansion of roots of bivariate
polynomials. In particular, we bound the valuation of $f(X,\phi)$ where $f$ is
a lacunary polynomial and $\phi$ a Puiseux series whose vanishing polynomial
has low degree.
"
622,Sparse interpolation over finite fields via low-order roots of unity,"  We present a new Monte Carlo algorithm for the interpolation of a
straight-line program as a sparse polynomial $f$ over an arbitrary finite field
of size $q$. We assume a priori bounds $D$ and $T$ are given on the degree and
number of terms of $f$. The approach presented in this paper is a hybrid of the
diversified and recursive interpolation algorithms, the two previous fastest
known probabilistic methods for this problem. By making effective use of the
information contained in the coefficients themselves, this new algorithm
improves on the bit complexity of previous methods by a ""soft-Oh"" factor of
$T$, $\log D$, or $\log q$.
"
623,Constructing Fewer Open Cells by GCD Computation in CAD Projection,"  A new projection operator based on cylindrical algebraic decomposition (CAD)
is proposed. The new operator computes the intersection of projection factor
sets produced by different CAD projection orders. In other words, it computes
the gcd of projection polynomials in the same variables produced by different
CAD projection orders. We prove that the new operator still guarantees
obtaining at least one sample point from every connected component of the
highest dimension, and therefore, can be used for testing semi-definiteness of
polynomials. Although the complexity of the new method is still doubly
exponential, in many cases, the new operator does produce smaller projection
factor sets and fewer open cells. Some examples of testing semi-definiteness of
polynomials, which are difficult to be solved by existing tools, have been
worked out efficiently by our program based on the new method.
"
624,Over-constrained Weierstrass iteration and the nearest consistent system,"  We propose a generalization of the Weierstrass iteration for over-constrained
systems of equations and we prove that the proposed method is the Gauss-Newton
iteration to find the nearest system which has at least $k$ common roots and
which is obtained via a perturbation of prescribed structure. In the univariate
case we show the connection of our method to the optimization problem
formulated by Karmarkar and Lakshman for the nearest GCD. In the multivariate
case we generalize the expressions of Karmarkar and Lakshman, and give
explicitly several iteration functions to compute the optimum.
  The arithmetic complexity of the iterations is detailed.
"
625,On the Complexity of Computing with Planar Algebraic Curves,"  In this paper, we give improved bounds for the computational complexity of
computing with planar algebraic curves. More specifically, for arbitrary
coprime polynomials $f$, $g \in \mathbb{Z}[x,y]$ and an arbitrary polynomial $h
\in \mathbb{Z}[x,y]$, each of total degree less than $n$ and with integer
coefficients of absolute value less than $2^\tau$, we show that each of the
following problems can be solved in a deterministic way with a number of bit
operations bounded by $\tilde{O}(n^6+n^5\tau)$, where we ignore polylogarithmic
factors in $n$ and $\tau$:
  (1) The computation of isolating regions in $\mathbb{C}^2$ for all complex
solutions of the system $f = g = 0$,
  (2) the computation of a separating form for the solutions of $f = g = 0$,
  (3) the computation of the sign of $h$ at all real valued solutions of $f = g
= 0$, and
  (4) the computation of the topology of the planar algebraic curve
$\mathcal{C}$ defined as the real valued vanishing set of the polynomial $f$.
  Our bound improves upon the best currently known bounds for the first three
problems by a factor of $n^2$ or more and closes the gap to the
state-of-the-art randomized complexity for the last problem.
"
626,"Applications of the Gauss-Jordan algorithm, done right","  Computer Algebra systems are widely spread because of some of their
remarkable features such as their ease of use and performance. Nonetheless,
this focus on performance sometimes leads to unwanted consequences: algorithms
and computations are implemented and carried out in a way which is sometimes
not transparent to the users, and that can lead to unexpected failures. In this
paper we present a formalisation in a proof assistant system of a \emph{naive}
version of the Gauss-Jordan algorithm, with explicit proofs of some of its
applications, and additionally a process to obtain versions of this algorithm
in two different functional languages (SML and Haskell) by means of code
generation techniques from the verified algorithm. The obtained programs are
then applied to test cases, which, despite the simplicity of the original
algorithm, have shown remarkable features in comparison to some Computer
Algebra systems, such as Mathematica\textsuperscript{\textregistered} (where
some of these computations are even incorrect), or Sage (in comparison to which
the generated programs show a compelling performance). The aim of the paper is
to show that, with the current technology in Theorem Proving, formalising
Linear Algebra procedures is a challenging but rewarding task, which provides
programs that can be compared in some aspects to \emph{state of the art}
procedures in Computer Algebra systems, and whose correctness is formally
proved.
"
627,"The Differential Dimension Polynomial for Characterizable Differential
  Ideals","  We generalize the differential dimension polynomial from prime differential
ideals to characterizable differential ideals. Its computation is algorithmic,
its degree and leading coefficient remain differential birational invariants,
and it decides equality of characterizable differential ideals contained in
each other.
"
628,A Near-Optimal Algorithm for Computing Real Roots of Sparse Polynomials,"  Let $p\in\mathbb{Z}[x]$ be an arbitrary polynomial of degree $n$ with $k$
non-zero integer coefficients of absolute value less than $2^\tau$. In this
paper, we answer the open question whether the real roots of $p$ can be
computed with a number of arithmetic operations over the rational numbers that
is polynomial in the input size of the sparse representation of $p$. More
precisely, we give a deterministic, complete, and certified algorithm that
determines isolating intervals for all real roots of $p$ with
$O(k^3\cdot\log(n\tau)\cdot \log n)$ many exact arithmetic operations over the
rational numbers.
  When using approximate but certified arithmetic, the bit complexity of our
algorithm is bounded by $\tilde{O}(k^4\cdot n\tau)$, where $\tilde{O}(\cdot)$
means that we ignore logarithmic. Hence, for sufficiently sparse polynomials
(i.e. $k=O(\log^c (n\tau))$ for a positive constant $c$), the bit complexity is
$\tilde{O}(n\tau)$. We also prove that the latter bound is optimal up to
logarithmic factors.
"
629,"Truth Table Invariant Cylindrical Algebraic Decomposition by Regular
  Chains","  A new algorithm to compute cylindrical algebraic decompositions (CADs) is
presented, building on two recent advances. Firstly, the output is truth table
invariant (a TTICAD) meaning given formulae have constant truth value on each
cell of the decomposition. Secondly, the computation uses regular chains theory
to first build a cylindrical decomposition of complex space (CCD) incrementally
by polynomial. Significant modification of the regular chains technology was
used to achieve the more sophisticated invariance criteria. Experimental
results on an implementation in the RegularChains Library for Maple verify that
combining these advances gives an algorithm superior to its individual
components and competitive with the state of the art.
"
630,"Ranks of Quotients, Remainders and $p$-Adic Digits of Matrices","  For a prime $p$ and a matrix $A \in \mathbb{Z}^{n \times n}$, write $A$ as $A
= p (A \,\mathrm{quo}\, p) + (A \,\mathrm{rem}\, p)$ where the remainder and
quotient operations are applied element-wise. Write the $p$-adic expansion of
$A$ as $A = A^{[0]} + p A^{[1]} + p^2 A^{[2]} + \cdots$ where each $A^{[i]} \in
\mathbb{Z}^{n \times n}$ has entries between $[0, p-1]$. Upper bounds are
proven for the $\mathbb{Z}$-ranks of $A \,\mathrm{rem}\, p$, and $A
\,\mathrm{quo}\, p$. Also, upper bounds are proven for the
$\mathbb{Z}/p\mathbb{Z}$-rank of $A^{[i]}$ for all $i \ge 0$ when $p = 2$, and
a conjecture is presented for odd primes.
"
631,"Multivariate sparse interpolation using randomized Kronecker
  substitutions","  We present new techniques for reducing a multivariate sparse polynomial to a
univariate polynomial. The reduction works similarly to the classical and
widely-used Kronecker substitution, except that we choose the degrees randomly
based on the number of nonzero terms in the multivariate polynomial, that is,
its sparsity. The resulting univariate polynomial often has a significantly
lower degree than the Kronecker substitution polynomial, at the expense of a
small number of term collisions. As an application, we give a new algorithm for
multivariate interpolation which uses these new techniques along with any
existing univariate interpolation algorithm.
"
632,The MMO problem,"  We consider a two polynomials analogue of the polynomial interpolation
problem. Namely, we consider the Mixing Modular Operations (MMO) problem of
recovering two polynomials $f\in \Z_p[x]$ and $g\in \Z_q[x]$ of known degree,
where $p$ and $q$ are two (un)known positive integers, from the values of
$f(t)\bmod p + g(t)\bmod q$ at polynomially many points $t \in \Z$. We show
that if $p$ and $q$ are known, the MMO problem is equivalent to computing a
close vector in a lattice with respect to the infinity norm. We also
implemented in the SAGE system a heuristic polynomial-time algorithm. If $p$
and $q$ are kept secret, we do not know how to solve this problem. This problem
is motivated by several potential cryptographic applications.
"
633,Powers of Tensors and Fast Matrix Multiplication,"  This paper presents a method to analyze the powers of a given trilinear form
(a special kind of algebraic constructions also called a tensor) and obtain
upper bounds on the asymptotic complexity of matrix multiplication. Compared
with existing approaches, this method is based on convex optimization, and thus
has polynomial-time complexity. As an application, we use this method to study
powers of the construction given by Coppersmith and Winograd [Journal of
Symbolic Computation, 1990] and obtain the upper bound $\omega<2.3728639$ on
the exponent of square matrix multiplication, which slightly improves the best
known upper bound.
"
634,Divide-And-Conquer Computation of Cylindrical Algebraic Decomposition,"  We present a divide-and-conquer version of the Cylindrical Algebraic
Decomposition (CAD) algorithm. The algorithm represents the input as a Boolean
combination of subformulas, computes cylindrical algebraic decompositions of
solution sets of the subformulas, and combines the results. We propose a
graph-based heuristic to find a suitable partitioning of the input and present
empirical comparison with direct CAD computation.
"
635,"Faster Algorithms for Multivariate Interpolation with Multiplicities and
  Simultaneous Polynomial Approximations","  The interpolation step in the Guruswami-Sudan algorithm is a bivariate
interpolation problem with multiplicities commonly solved in the literature
using either structured linear algebra or basis reduction of polynomial
lattices. This problem has been extended to three or more variables; for this
generalization, all fast algorithms proposed so far rely on the lattice
approach. In this paper, we reduce this multivariate interpolation problem to a
problem of simultaneous polynomial approximations, which we solve using fast
structured linear algebra. This improves the best known complexity bounds for
the interpolation step of the list-decoding of Reed-Solomon codes,
Parvaresh-Vardy codes, and folded Reed-Solomon codes. In particular, for
Reed-Solomon list-decoding with re-encoding, our approach has complexity
$\mathcal{O}\tilde{~}(\ell^{\omega-1}m^2(n-k))$, where $\ell,m,n,k$ are the
list size, the multiplicity, the number of sample points and the dimension of
the code, and $\omega$ is the exponent of linear algebra; this accelerates the
previously fastest known algorithm by a factor of $\ell / m$.
"
636,A Generalized Apagodu-Zeilberger Algorithm,"  The Apagodu-Zeilberger algorithm can be used for computing annihilating
operators for definite sums over hypergeometric terms, or for definite
integrals over hyperexponential functions. In this paper, we propose a
generalization of this algorithm which is applicable to arbitrary
$\partial$-finite functions. In analogy to the hypergeometric case, we
introduce the notion of proper $\partial$-finite functions. We show that the
algorithm always succeeds for these functions, and we give a tight a priori
bound for the order of the output operator.
"
637,"An Algorithm to Compute the Topological Euler Characteristic,
  Chern-Schwartz-MacPherson Class and Segre Class of Projective Varieties","  Let $V$ be a closed subscheme of a projective space $\mathbb{P}^n$. We give
an algorithm to compute the Chern-Schwartz-MacPherson class, Euler
characteristic and Segre class of $ V$. The algorithm can be implemented using
either symbolic or numerical methods. The algorithm is based on a new method
for calculating the projective degrees of a rational map defined by a
homogeneous ideal. Using this result and known formulas for the
Chern-Schwartz-MacPherson class of a projective hypersurface and the Segre
class of a projective variety in terms of the projective degrees of certain
rational maps we give algorithms to compute the Chern-Schwartz-MacPherson class
and Segre class of a projective variety. Since the Euler characteristic of $V$
is the degree of the zero dimensional component of the
Chern-Schwartz-MacPherson class of $V$ our algorithm also computes the Euler
characteristic $\chi(V)$. Relationships between the algorithm developed here
and other existing algorithms are discussed. The algorithm is tested on several
examples and performs favourably compared to current algorithms for computing
Chern-Schwartz-MacPherson classes, Segre classes and Euler characteristics.
"
638,Parallel computation of echelon forms,"  We propose efficient parallel algorithms and implementations on shared memory
architectures of LU factorization over a finite field. Compared to the
corresponding numerical routines, we have identified three main difficulties
specific to linear algebra over finite fields. First, the arithmetic complexity
could be dominated by modular reductions. Therefore, it is mandatory to delay
as much as possible these reductions while mixing fine-grain parallelizations
of tiled iterative and recursive algorithms. Second, fast linear algebra
variants, e.g., using Strassen-Winograd algorithm, never suffer from
instability and can thus be widely used in cascade with the classical
algorithms. There, trade-offs are to be made between size of blocks well suited
to those fast variants or to load and communication balancing. Third, many
applications over finite fields require the rank profile of the matrix (quite
often rank deficient) rather than the solution to a linear system. It is thus
important to design parallel algorithms that preserve and compute this rank
profile. Moreover, as the rank profile is only discovered during the algorithm,
block size has then to be dynamic. We propose and compare several block
decomposition: tile iterative with left-looking, right-looking and Crout
variants, slab and tile recursive. Experiments demonstrate that the tile
recursive variant performs better and matches the performance of reference
numerical software when no rank deficiency occur. Furthermore, even in the most
heterogeneous case, namely when all pivot blocks are rank deficient, we show
that it is possbile to maintain a high efficiency.
"
639,"A Technique for Deriving Equational Conditions on the Denavit-Hartenberg
  Parameters of 6R Linkages that are Necessary for Movability","  A closed 6R linkage is generically rigid. Special cases may be mobile. Many
families of mobile 6R linkages have been characterised in terms of the
invariant Denavit-Hartenberg parameters of the linkage. In other words, many
sufficient conditions for mobility are known. In this paper we give, for the
first time, equational conditions on the invariant Denavit-Hartenberg
parameters that are necessary for mobility. The method is based on the theory
of bonds. We illustrate the method by deriving the equational conditions for
various well-known linkages (Bricard's line symmetric linkage, Hooke's linkage,
Dietmaier's linkage, and recent a generalization of Bricard's orthogonal
linkage), starting from their bond diagrams; and by deriving the equations for
another bond diagram, thereby discovering a new mobile 6R linkage.
"
640,Tame Decompositions and Collisions,"  A univariate polynomial f over a field is decomposable if f = g o h = g(h)
for nonlinear polynomials g and h. It is intuitively clear that the
decomposable polynomials form a small minority among all polynomials over a
finite field. The tame case, where the characteristic p of Fq does not divide n
= deg f, is fairly well-understood, and we have reasonable bounds on the number
of decomposables of degree n. Nevertheless, no exact formula is known if $n$
has more than two prime factors. In order to count the decomposables, one wants
to know, under a suitable normalization, the number of collisions, where
essentially different (g, h) yield the same f. In the tame case, Ritt's Second
Theorem classifies all 2-collisions.
  We introduce a normal form for multi-collisions of decompositions of
arbitrary length with exact description of the (non)uniqueness of the
parameters. We obtain an efficiently computable formula for the exact number of
such collisions at degree n over a finite field of characteristic coprime to p.
This leads to an algorithm for the exact number of decomposable polynomials at
degree n over a finite field Fq in the tame case.
"
641,Tensor computations in computer algebra systems,"  This paper considers three types of tensor computations. On their basis, we
attempt to formulate criteria that must be satisfied by a computer algebra
system dealing with tensors. We briefly overview the current state of tensor
computations in different computer algebra systems. The tensor computations are
illustrated with appropriate examples implemented in specific systems: Cadabra
and Maxima.
"
642,Computing discrete logarithms in subfields of residue class rings,"  Recent breakthrough methods \cite{gggz,joux,bgjt} on computing discrete
logarithms in small characteristic finite fields share an interesting feature
in common with the earlier medium prime function field sieve method \cite{jl}.
To solve discrete logarithms in a finite extension of a finite field $\F$, a
polynomial $h(x) \in \F[x]$ of a special form is constructed with an
irreducible factor $g(x) \in \F[x]$ of the desired degree. The special form of
$h(x)$ is then exploited in generating multiplicative relations that hold in
the residue class ring $\F[x]/h(x)\F[x]$ hence also in the target residue class
field $\F[x]/g(x)\F[x]$. An interesting question in this context and addressed
in this paper is: when and how does a set of relations on the residue class
ring determine the discrete logarithms in the finite fields contained in it? We
give necessary and sufficient conditions for a set of relations on the residue
class ring to determine discrete logarithms in the finite fields contained in
it. We also present efficient algorithms to derive discrete logarithms from the
relations when the conditions are met. The derived necessary conditions allow
us to clearly identify structural obstructions intrinsic to the special
polynomial $h(x)$ in each of the aforementioned methods, and propose
modifications to the selection of $h(x)$ so as to avoid obstructions.
"
643,"Matrix-F5 algorithms and tropical Gr\""obner bases computation","  Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr\""obner bases taking into account the valuation
of $K$. Because of the use of the valuation, this theory is promising for
stable computations over polynomial rings over a $p$-adic fields.We design a
strategy to compute such tropical Gr\""obner bases by adapting the Matrix-F5
algorithm. Two variants of the Matrix-F5 algorithm, depending on how the
Macaulay matrices are built, are available to tropical computation with
respective modifications. The former is more numerically stable while the
latter is faster.Our study is performed both over any exact field with
valuation and some inexact fields like $\mathbb{Q}\_p$ or $\mathbb{F}\_q
\llbracket t \rrbracket.$ In the latter case, we track the loss in precision,
and show that the numerical stability can compare very favorably to the case of
classical Gr\""obner bases when the valuation is non-trivial. Numerical examples
are provided.
"
644,"Nonnegative Trigonometric Polynomials, Sturms Theorem, and Symbolic
  Computation","  In this paper, we explain a procedure based on a classical result of Sturm
that can be used to determine rigorously whether a given trigonometric
polynomial is nonnegative in a certain interval or not. Many examples are
given. This technique has been employed by the author in several recent works.
  The procedure often involves tedious computations that are time-consuming and
error-prone. Fortunately, symbolic computation software is available to
automate the procedure. In this paper, we give the details of its
implementation in MAPLE 13. Some who are strongly attached to a more
traditional theoretical research framework may find such details boring or even
consider computer-assisted proofs suspicious. However, we emphasize again that
the procedure is completely mathematically rigorous.
"
645,Tracking p-adic precision,"  We present a new method to propagate $p$-adic precision in computations,
which also applies to other ultrametric fields. We illustrate it with many
examples and give a toy application to the stable computation of the SOMOS 4
sequence.
"
646,"Sparse Gr\""obner Bases: the Unmixed Case","  Toric (or sparse) elimination theory is a framework developped during the
last decades to exploit monomial structures in systems of Laurent polynomials.
Roughly speaking, this amounts to computing in a \emph{semigroup algebra},
\emph{i.e.} an algebra generated by a subset of Laurent monomials. In order to
solve symbolically sparse systems, we introduce \emph{sparse Gr\""obner bases},
an analog of classical Gr\""obner bases for semigroup algebras, and we propose
sparse variants of the $F_5$ and FGLM algorithms to compute them. Our prototype
""proof-of-concept"" implementation shows large speed-ups (more than 100 for some
examples) compared to optimized (classical) Gr\""obner bases software. Moreover,
in the case where the generating subset of monomials corresponds to the points
with integer coordinates in a normal lattice polytope $\mathcal P\subset\mathbb
R^n$ and under regularity assumptions, we prove complexity bounds which depend
on the combinatorial properties of $\mathcal P$. These bounds yield new
estimates on the complexity of solving $0$-dim systems where all polynomials
share the same Newton polytope (\emph{unmixed case}). For instance, we
generalize the bound $\min(n_1,n_2)+1$ on the maximal degree in a Gr\""obner
basis of a $0$-dim. bilinear system with blocks of variables of sizes
$(n_1,n_2)$ to the multilinear case: $\sum n_i - \max(n_i)+1$. We also propose
a variant of Fr\""oberg's conjecture which allows us to estimate the complexity
of solving overdetermined sparse systems.
"
647,Matrix Methods for Solving Algebraic Systems,"  We present our public-domain software for the following tasks in sparse (or
toric) elimination theory, given a well-constrained polynomial system. First, C
code for computing the mixed volume of the system. Second, Maple code for
defining an overconstrained system and constructing a Sylvester-type matrix of
its sparse resultant. Third, C code for a Sylvester-type matrix of the sparse
resultant and a superset of all common roots of the initial well-constrained
system by computing the eigen-decomposition of a square matrix obtained from
the resultant matrix. We conclude with experiments in computing molecular
conformations.
"
648,"Design, Implementation and Evaluation of MTBDD based Fuzzy Sets and
  Binary Fuzzy Relations","  For fast and efficient analysis of large sets of fuzzy data, elimination of
redundancies in the memory representation is needed. We used MTBDDs as the
underlying data-structure to represent fuzzy sets and binary fuzzy relations.
This leads to elimination of redundancies in the representation, less
computations, and faster analyses. We have also extended a BDD package (BuDDy)
to support MTBDDs in general and fuzzy sets and relations in particular.
Different fuzzy operations such as max, min and max-min composition were
implemented based on our representation. Effectiveness of our representation is
shown by applying it on fuzzy connectedness and image segmentation problem.
Compared to a base implementation, the running time of our MTBDD based
implementation was faster (in our test cases) by a factor ranging from 2 to 27.
Also, when the MTBDD based data-structure was employed, the memory needed to
represent the final results was improved by a factor ranging from 37.9 to
265.5.
"
649,"Sparse Polynomial Interpolation Codes and their decoding beyond half the
  minimal distance","  We present algorithms performing sparse univariate polynomial interpolation
with errors in the evaluations of the polynomial. Based on the initial work by
Comer, Kaltofen and Pernet [Proc. ISSAC 2012], we define the sparse polynomial
interpolation codes and state that their minimal distance is precisely the
length divided by twice the sparsity. At ISSAC 2012, we have given a decoding
algorithm for as much as half the minimal distance and a list decoding
algorithm up to the minimal distance. Our new polynomial-time list decoding
algorithm uses sub-sequences of the received evaluations indexed by a linear
progression, allowing the decoding for a larger radius, that is, more errors in
the evaluations while returning a list of candidate sparse polynomials. We
quantify this improvement for all typically small values of number of terms and
number of errors, and provide a worst case asymptotic analysis of this
improvement. For instance, for sparsity T = 5 with up to 10 errors we can list
decode in polynomial-time from 74 values of the polynomial with unknown terms,
whereas our earlier algorithm required 2T (E + 1) = 110 evaluations. We then
propose two variations of these codes in characteristic zero, where appropriate
choices of values for the variable yield a much larger minimal distance: the
length minus twice the sparsity.
"
650,A Short Note on Zero-error Computation for Algebraic Numbers by IPSLQ,"  The PSLQ algorithm is one of the most popular algorithm for finding
nontrivial integer relations for several real numbers. In the present work, we
present an incremental version of PSLQ. For some applications needing to call
PSLQ many times, such as finding the minimal polynomial of an algebraic number
without knowing the degree, the incremental PSLQ algorithm is more efficient
than PSLQ, both theoretically and practically.
"
651,"Matrix-F5 algorithms over finite-precision complete discrete valuation
  fields","  Let $(f\_1,\dots, f\_s) \in \mathbb{Q}\_p [X\_1,\dots, X\_n]^s$ be a sequence
of homogeneous polynomials with $p$-adic coefficients. Such system may happen,
for example, in arithmetic geometry. Yet, since $\mathbb{Q}\_p$ is not an
effective field, classical algorithm does not apply.We provide a definition for
an approximate Gr{\""o}bner basis with respect to a monomial order $w.$ We
design a strategy to compute such a basis, when precision is enough and under
the assumption that the input sequence is regular and the ideals $\langle
f\_1,\dots,f\_i \rangle$ are weakly-$w$-ideals. The conjecture of Moreno-Socias
states that for the grevlex ordering, such sequences are generic.Two variants
of that strategy are available, depending on whether one lean more on precision
or time-complexity. For the analysis of these algorithms, we study the loss of
precision of the Gauss row-echelon algorithm, and apply it to an adapted
Matrix-F5 algorithm. Numerical examples are provided.Moreover, the fact that
under such hypotheses, Gr{\""o}bner bases can be computed stably has many
applications. Firstly, the mapping sending $(f\_1,\dots,f\_s)$ to the reduced
Gr{\""o}bner basis of the ideal they span is differentiable, and its
differential can be given explicitly. Secondly, these hypotheses allows to
perform lifting on the Grobner bases, from $\mathbb{Z}/p^k \mathbb{Z}$ to
$\mathbb{Z}/p^{k+k'} \mathbb{Z}$ or $\mathbb{Z}.$ Finally, asking for the same
hypotheses on the highest-degree homogeneous components of the entry
polynomials allows to extend our strategy to the affine case.
"
652,"Model-based construction of Open Non-uniform Cylindrical Algebraic
  Decompositions","  In this paper we introduce the notion of an Open Non-uniform Cylindrical
Algebraic Decomposition (NuCAD), and present an efficient model-based algorithm
for constructing an Open NuCAD from an input formula. A NuCAD is a
generalization of Cylindrical Algebraic Decomposition (CAD) as defined by
Collins in his seminal work from the early 1970s, and as extended in concepts
like Hong's partial CAD. A NuCAD, like a CAD, is a decomposition of
n-dimensional real space into cylindrical cells. But unlike a CAD, the cells in
a NuCAD need not be arranged cylindrically. It is in this sense that NuCADs are
not uniformly cylindrical. However, NuCADs--- like CADs --- carry a tree-like
structure that relates different cells. It is a very different tree but, as
with the CAD tree structure, it allows some operations to be performed
efficiently, for example locating the containing cell for an arbitrary input
point.
"
653,Procesamiento topo-geom\'etrico de im\'agenes neuronales,"  Fruit of the relationship of our research group with the team coordinated by
the biologist Miguel Morales (http://spineup.es), we have applied different
topo-geometric techniques for neuronal image processing. The images, captured
with a powerful confocal microscope, allow to study the evolution of synaptic
density under the influence of various substances, with the aim of studying
neurodegenerative diseases like Alzheimer.
  In the paper we make a brief review of the techniques that appear in our
bioinformatic problems, including the calculation of ordinary and persistent
homology (for which one can use the program Kenzo for symbolic computation in
algebraic topology ) and classical problems of digital topology as skeleton
location and path tracking. We focus on some particular cases of recent
application, with which we will illustrate the previous techniques.
"
654,Defining and computing persistent Z-homology in the general case,"  By general case we mean methods able to process simplicial sets and chain
complexes not of finite type. A filtration of the object to be studied is the
heart of both subjects persistent homology and spectral sequences. In this
paper we present the complete relation between them, both from theoretical and
computational points of view. One of the main contributions of this paper is
the observation that a slight modification of our previous programs computing
spectral sequences is enough to compute also persistent homology. By
inheritance from our spectral sequence programs, we obtain for free persistent
homology programs applicable to spaces not of finite type (provided they are
spaces with effective homology) and with Z-coefficients (significantly
generalizing the usual presentation of persistent homology over a field). As an
illustration, we compute some persistent homology groups (and the corresponding
integer barcodes) in the case of a Postnikov tower.
"
655,"Proceedings 1st International Workshop on Synthesis of Continuous
  Parameters","  This volume contains the proceedings of the 1st International Workshop on
Synthesis of Continuous Parameters (SynCoP'14). The workshop was held in
Grenoble, France on April 6th, 2014, as a satellite event of the 17th European
Joint Conferences on Theory and Practice of Software (ETAPS'14).
  SynCoP aims at bringing together researchers working on parameter synthesis
for systems with continuous variables, where the parameters consist of a
(usually dense) set of constant values. Synthesis problems for such parameters
arise for real-time, hybrid or probabilistic systems in a large variety
application domains. A parameter could be, e.g., a delay in a real-time system,
or a reaction rate in a biological cell model. The objective of the synthesis
problem is to identify suitable parameters to achieve desired behavior, or to
verify the behavior for a given range of parameter values.
  This volume contains seven contributions: two invited talks and five regular
papers.
"
656,Factoring Differential Operators in n Variables,"  In this paper, we present a new algorithm and an experimental implementation
for factoring elements in the polynomial n'th Weyl algebra, the polynomial n'th
shift algebra, and ZZ^n-graded polynomials in the n'th q-Weyl algebra.
  The most unexpected result is that this noncommutative problem of factoring
partial differential operators can be approached effectively by reducing it to
the problem of solving systems of polynomial equations over a commutative ring.
In the case where a given polynomial is ZZ^n-graded, we can reduce the problem
completely to factoring an element in a commutative multivariate polynomial
ring.
  The implementation in Singular is effective on a broad range of polynomials
and increases the ability of computer algebra systems to address this important
problem. We compare the performance and output of our algorithm with other
implementations in commodity computer algebra systems on nontrivial examples.
"
657,"Predicting zero reductions in Gr\""obner basis computations","  Since Buchberger's initial algorithm for computing Gr\""obner bases in 1965
many attempts have been taken to detect zero reductions in advance.
Buchberger's Product and Chain criteria may be known the most, especially in
the installaton of Gebauer and M\""oller. A relatively new approach are
signature-based criteria which were first used in Faug\`ere's F5 algorithm in
2002. For regular input sequences these criteria are known to compute no zero
reduction at all. In this paper we give a detailed discussion on zero
reductions and the corresponding syzygies. We explain how the different methods
to predict them compare to each other and show advantages and drawbacks in
theory and practice. With this a new insight into algebraic structures
underlying Gr\""obner bases and their computations might be achieved.
"
658,"An Improvement over the GVW Algorithm for Inhomogeneous Polynomial
  Systems","  The GVW algorithm is a signature-based algorithm for computing Gr\""obner
bases. If the input system is not homogeneous, some J-pairs with higher
signatures but lower degrees are rejected by GVW's Syzygy Criterion, instead,
GVW have to compute some J-pairs with lower signatures but higher degrees.
Consequently, degrees of polynomials appearing during the computations may
unnecessarily grow up higher and the computation become more expensive. In this
paper, a variant of the GVW algorithm, called M-GVW, is proposed and mutant
pairs are introduced to overcome inconveniences brought by inhomogeneous input
polynomials. Some techniques from linear algebra are used to improve the
efficiency. Both GVW and M-GVW have been implemented in C++ and tested by many
examples from boolean polynomial rings. The timings show M-GVW usually performs
much better than the original GVW algorithm when mutant pairs are found.
Besides, M-GVW is also compared with intrinsic Gr\""obner bases functions on
Maple, Singular and Magma. Due to the efficient routines from the M4RI library,
the experimental results show that M-GVW is very efficient.
"
659,"A survey on signature-based Gr\""obner basis computations","  This paper is a survey on the area of signature-based Gr\""obner basis
algorithms that was initiated by Faug\`ere's F5 algorithm in 2002. We explain
the general ideas behind the usage of signatures. We show how to classify the
various known variants by 3 different orderings. For this we give translations
between different notations and show that besides notations many approaches are
just the same. Moreover, we give a general description of how the idea of
signatures is quite natural when performing the reduction process using linear
algebra. This survey shall help to outline this field of active research.
"
660,"The Secant-Newton Map is Optimal Among Contracting $n^{th}$ Degree Maps
  for $n^{th}$ Root Computation","  Consider the problem: given a real number $x$ and an error bound $\epsilon$,
find an interval such that it contains the $\sqrt[n]{x}$ and its width is less
than $\epsilon$. One way to solve the problem is to start with an initial
interval and to repeatedly update it by applying an interval refinement map on
it until it becomes narrow enough. In this paper, we prove that the well known
Secant-Newton map is optimal among a certain family of natural generalizations.
"
661,Nearly Optimal Computations with Structured Matrices,"  We estimate the Boolean complexity of multiplication of structured matrices
by a vector and the solution of nonsingular linear systems of equations with
these matrices. We study four basic most popular classes, that is, Toeplitz,
Hankel, Cauchy and Van-der-monde matrices, for which the cited computational
problems are equivalent to the task of polynomial multiplication and division
and polynomial and rational multipoint evaluation and interpolation. The
Boolean cost estimates for the latter problems have been obtained by Kirrinnis
in \cite{kirrinnis-joc-1998}, except for rational interpolation, which we
supply now. All known Boolean cost estimates for these problems rely on using
Kronecker product. This implies the $d$-fold precision increase for the $d$-th
degree output, but we avoid such an increase by relying on distinct techniques
based on employing FFT. Furthermore we simplify the analysis and make it more
transparent by combining the representation of our tasks and algorithms in
terms of both structured matrices and polynomials and rational functions. This
also enables further extensions of our estimates to cover Trummer's important
problem and computations with the popular classes of structured matrices that
generalize the four cited basic matrix classes.
"
662,"Accelerated Approximation of the Complex Roots of a Univariate
  Polynomial (Extended Abstract)","  Highly efficient and even nearly optimal algorithms have been developed for
the classical problem of univariate polynomial root-finding (see, e.g.,
\cite{P95}, \cite{P02}, \cite{MNP13}, and the bibliography therein), but this
is still an area of active research. By combining some powerful techniques
developed in this area we devise new nearly optimal algorithms, whose
substantial merit is their simplicity, important for the implementation.
"
663,Computing periods of rational integrals,"  A period of a rational integral is the result of integrating, with respect to
one or several variables, a rational function over a closed path. This work
focuses particularly on periods depending on a parameter: in this case the
period under consideration satisfies a linear differential equation, the
Picard-Fuchs equation. I give a reduction algorithm that extends the
Griffiths-Dwork reduction and apply it to the computation of Picard-Fuchs
equations. The resulting algorithm is elementary and has been successfully
applied to problems that were previously out of reach.
"
664,Global Newton Iteration over Archimedean and non-Archimedean Fields,"  In this paper, we study iterative methods on the coefficients of the rational
univariate representation (RUR) of a given algebraic set, called global Newton
iteration. We compare two natural approaches to define locally quadratically
convergent iterations: the first one involves Newton iteration applied to the
approximate roots individually and then interpolation to find the RUR of these
approximate roots; the second one considers the coefficients in the exact RUR
as zeroes of a high dimensional map defined by polynomial reduction, and
applies Newton iteration on this map. We prove that over fields with a p-adic
valuation these two approaches give the same iteration function, but over
fields equipped with the usual Archimedean absolute value, they are not
equivalent. In the latter case, we give explicitly the iteration function for
both approaches. Finally, we analyze the parallel complexity of the different
versions of the global Newton iteration, compare them, and demonstrate that
they can be efficiently computed. The motivation for this study comes from the
certification of approximate roots of overdetermined and singular polynomial
systems via the recovery of an exact RUR from approximate numerical data.
"
665,"Applying machine learning to the problem of choosing a heuristic to
  select the variable ordering for cylindrical algebraic decomposition","  Cylindrical algebraic decomposition(CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. When using CAD, there is often a choice for the ordering placed on the
variables. This can be important, with some problems infeasible with one
variable ordering but easy with another. Machine learning is the process of
fitting a computer model to a complex function based on properties learned from
measured data. In this paper we use machine learning (specifically a support
vector machine) to select between heuristics for choosing a variable ordering,
outperforming each of the separate heuristics.
"
666,"Problem formulation for truth-table invariant cylindrical algebraic
  decomposition by incremental triangular decomposition","  Cylindrical algebraic decompositions (CADs) are a key tool for solving
problems in real algebraic geometry and beyond. We recently presented a new CAD
algorithm combining two advances: truth-table invariance, making the CAD
invariant with respect to the truth of logical formulae rather than the signs
of polynomials; and CAD construction by regular chains technology, where first
a complex decomposition is constructed by refining a tree incrementally by
constraint. We here consider how best to formulate problems for input to this
algorithm. We focus on a choice (not relevant for other CAD algorithms) about
the order in which constraints are presented. We develop new heuristics to help
make this choice and thus allow the best use of the algorithm in practice. We
also consider other choices of problem formulation for CAD, as discussed in
CICM 2013, revisiting these in the context of the new algorithm.
"
667,Automatic Differentiation of Algorithms for Machine Learning,"  Automatic differentiation---the mechanical transformation of numeric computer
programs to calculate derivatives efficiently and accurately---dates to the
origin of the computer age. Reverse mode automatic differentiation both
antedates and generalizes the method of backwards propagation of errors used in
machine learning. Despite this, practitioners in a variety of fields, including
machine learning, have been little influenced by automatic differentiation, and
make scant use of available tools. Here we review the technique of automatic
differentiation, describe its two main modes, and explain how it can benefit
machine learning practitioners. To reach the widest possible audience our
treatment assumes only elementary differential calculus, and does not assume
any knowledge of linear algebra.
"
668,Binomial Difference Ideal and Toric Difference Variety,"  In this paper, the concepts of binomial difference ideals and toric
difference varieties are defined and their properties are proved. Two canonical
representations for Laurent binomial difference ideals are given using the
reduced Groebner basis of Z[x]-lattices and regular and coherent difference
ascending chains, respectively. Criteria for a Laurent binomial difference
ideal to be reflexive, prime, well-mixed, perfect, and toric are given in terms
of their support lattices which are Z[x]-lattices. The reflexive, well-mixed,
and perfect closures of a Laurent binomial difference ideal are shown to be
binomial. Four equivalent definitions for toric difference varieties are
presented. Finally, algorithms are given to check whether a given Laurent
binomial difference ideal I is reflexive, prime, well-mixed, perfect, or toric,
and in the negative case, to compute the reflexive, well-mixed, and perfect
closures of I. An algorithm is given to decompose a finitely generated perfect
binomial difference ideal as the intersection of reflexive prime binomial
difference ideals.
"
669,Computing all Affine Solution Sets of Binomial Systems,"  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. For the problem of
computing solution sets in the intersection of some coordinate planes, the
direct application of a polyhedral method fails, because the original facial
structure of the Newton polytopes may alter completely when selected variables
become zero. Our new proposed method enumerates all factors contributing to a
generalized permanent and toric solutions as a special case of this
enumeration. For benchmark problems such as the adjacent 2-by-2 minors of a
general matrix, our methods scale much better than the witness set
representations of numerical algebraic geometry.
"
670,Border Bases for Polynomial Rings over Noetherian Rings,"  The theory of border bases for zero-dimensional ideals has attracted several
researchers in symbolic computation due to their numerical stability and
mathematical elegance. As shown in (Francis & Dukkipati, J. Symb. Comp., 2014),
one can extend the concept of border bases over Noetherian rings whenever the
corresponding residue class ring is finitely generated and free. In this paper
we address the following problem: Can the concept of border basis over
Noetherian rings exists for ideals when the corresponding residue class rings
are finitely generated but need not necessarily be free modules? We present a
border division algorithm and prove the termination of the algorithm for a
special class of border bases. We show the existence of such border bases over
Noetherian rings and present some characterizations in this regard. We also
show that certain reduced Gr\""{o}bner bases over Noetherian rings are contained
in this class of border bases.
"
671,Formulating problems for real algebraic geometry,"  We discuss issues of problem formulation for algorithms in real algebraic
geometry, focussing on quantifier elimination by cylindrical algebraic
decomposition. We recall how the variable ordering used can have a profound
effect on both performance and output and summarise what may be done to assist
with this choice. We then survey other questions of problem formulation and
algorithm optimisation that have become pertinent following advances in CAD
theory, including both work that is already published and work that is
currently underway. With implementations now in reach of real world
applications and new theory meaning algorithms are far more sensitive to the
input, our thesis is that intelligently formulating problems for algorithms,
and indeed choosing the correct algorithm variant for a problem, is key to
improving the practical use of both quantifier elimination and symbolic real
algebraic geometry in general.
"
672,"On the Efficiency of Solving Boolean Polynomial Systems with the
  Characteristic Set Method","  An improved characteristic set algorithm for solving Boolean polynomial
systems is proposed. This algorithm is based on the idea of converting all the
polynomials into monic ones by zero decomposition, and using additions to
obtain pseudo-remainders. Three important techniques are applied in the
algorithm. The first one is eliminating variables by new generated linear
polynomials. The second one is optimizing the strategy of choosing polynomial
for zero decomposition. The third one is to compute add-remainders to eliminate
the leading variable of new generated monic polynomials. By analyzing the depth
of the zero decomposition tree, we present some complexity bounds of this
algorithm, which are lower than the complexity bounds of previous
characteristic set algorithms. Extensive experimental results show that this
new algorithm is more efficient than previous characteristic set algorithms for
solving Boolean polynomial systems.
"
673,"Improved algorithm for computing separating linear forms for bivariate
  systems","  We address the problem of computing a linear separating form of a system of
two bivariate polynomials with integer coefficients, that is a linear
combination of the variables that takes different values when evaluated at the
distinct solutions of the system. The computation of such linear forms is at
the core of most algorithms that solve algebraic systems by computing rational
parameterizations of the solutions and this is the bottleneck of these
algorithms in terms of worst-case bit complexity. We present for this problem a
new algorithm of worst-case bit complexity $\sOB(d^7+d^6\tau)$ where $d$ and
$\tau$ denote respectively the maximum degree and bitsize of the input (and
where $\sO$ refers to the complexity where polylogarithmic factors are omitted
and $O_B$ refers to the bit complexity). This algorithm simplifies and
decreases by a factor $d$ the worst-case bit complexity presented for this
problem by Bouzidi et al. \cite{bouzidiJSC2014a}. This algorithm also yields,
for this problem, a probabilistic Las-Vegas algorithm of expected bit
complexity $\sOB(d^5+d^4\tau)$.
"
674,Una metodolog\'ia para realizar Diferenciaci\'on Autom\'atica Anidada,"  En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal\'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
Diferenciaci\'on Autom\'atica y se calculen el valor y la derivada que se
requiera.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required.
"
675,Cylindrical Algebraic Decomposition Using Local Projections,"  We present an algorithm which computes a cylindrical algebraic decomposition
of a semialgebraic set using projection sets computed for each cell separately.
Such local projection sets can be significantly smaller than the global
projection set used by the Cylindrical Algebraic Decomposition (CAD) algorithm.
This leads to reduction in the number of cells the algorithm needs to
construct. We give an empirical comparison of our algorithm and the classical
CAD algorithm.
"
676,Numerical Hilbert functions for Macaulay2,"  The NumericalHilbert package for Macaulay2 includes algorithms for computing
local dual spaces of polynomial ideals, and related local combinatorial data
about its scheme structure. These techniques are numerically stable, and can be
used with floating point arithmetic over the complex numbers. They provide a
viable alternative in this setting to purely symbolic methods such as standard
bases. In particular, these methods can be used to compute initial ideals,
local Hilbert functions and Hilbert regularity.
"
677,"A fast algorithm for computing the characteristic polynomial of the
  p-curvature","  We discuss theoretical and algorithmic questions related to the $p$-curvature
of differential operators in characteristic $p$. Given such an operator $L$,
and denoting by $\Chi(L)$ the characteristic polynomial of its $p$-curvature,
we first prove a new, alternative, description of $\Chi(L)$. This description
turns out to be particularly well suited to the fast computation of $\Chi(L)$
when $p$ is large: based on it, we design a new algorithm for computing
$\Chi(L)$, whose cost with respect to $p$ is $\softO(p^{0.5})$ operations in
the ground field. This is remarkable since, prior to this work, the fastest
algorithms for this task, and even for the subtask of deciding nilpotency of
the $p$-curvature, had merely slightly subquadratic complexity
$\softO(p^{1.79})$.
"
678,"Computing necessary integrability conditions for planar parametrized
  homogeneous potentials","  Let $V\in\mathbb{Q}(i)(\a_1,\dots,\a_n)(\q_1,\q_2)$ be a rationally
parametrized planar homogeneous potential of homogeneity degree $k\neq -2, 0,
2$. We design an algorithm that computes polynomial \emph{necessary} conditions
on the parameters $(\a_1,\dots,\a_n)$ such that the dynamical system associated
to the potential $V$ is integrable. These conditions originate from those of
the Morales-Ramis-Sim\'o integrability criterion near all Darboux points. The
implementation of the algorithm allows to treat applications that were out of
reach before, for instance concerning the non-integrability of polynomial
potentials up to degree $9$. Another striking application is the first complete
proof of the non-integrability of the \emph{collinear three body problem}.
"
679,Diferenciaci\'on Autom\'atica Anidada. Un enfoque algebraico,"  En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
diferenciaci\'on autom\'atica y se calculen el valor y la derivada que se
requiera. Tambi\'en se presenta un enfoque algebraico de la Diferenciaci\'on
Autom\'atica y en particular del espacio de los SuperAdoubles.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required. Also, an Automatic Differentiation algebraic point
of view is presented with particular emphasis in Nested Automatic
Differentiation.
"
680,A comparison of three heuristics to choose the variable ordering for CAD,"  Cylindrical algebraic decomposition (CAD) is a key tool for problems in real
algebraic geometry and beyond. When using CAD there is often a choice over the
variable ordering to use, with some problems infeasible in one ordering but
simple in another. Here we discuss a recent experiment comparing three
heuristics for making this choice on thousands of examples.
"
681,"Using the Regular Chains Library to build cylindrical algebraic
  decompositions by projecting and lifting","  Cylindrical algebraic decomposition (CAD) is an important tool, both for
quantifier elimination over the reals and a range of other applications.
Traditionally, a CAD is built through a process of projection and lifting to
move the problem within Euclidean spaces of changing dimension. Recently, an
alternative approach which first decomposes complex space using triangular
decomposition before refining to real space has been introduced and implemented
within the RegularChains Library of Maple. We here describe a freely available
package ProjectionCAD which utilises the routines within the RegularChains
Library to build CADs by projection and lifting. We detail how the projection
and lifting algorithms were modified to allow this, discuss the motivation and
survey the functionality of the package.
"
682,"Choosing a variable ordering for truth-table invariant cylindrical
  algebraic decomposition by incremental triangular decomposition","  Cylindrical algebraic decomposition (CAD) is a key tool for solving problems
in real algebraic geometry and beyond. In recent years a new approach has been
developed, where regular chains technology is used to first build a
decomposition in complex space. We consider the latest variant of this which
builds the complex decomposition incrementally by polynomial and produces CADs
on whose cells a sequence of formulae are truth-invariant. Like all CAD
algorithms the user must provide a variable ordering which can have a profound
impact on the tractability of a problem. We evaluate existing heuristics to
help with the choice for this algorithm, suggest improvements and then derive a
new heuristic more closely aligned with the mechanics of the new algorithm.
"
683,On the design of an expert help system for computer algebra systems,"  It is our intention here only to discuss the nature, complexity and tools
concerning the design of Smart Help, an expert help facility for aiding users
of Computer Algebra Systems. Although the expert help system presented here has
been particularly oriented to REDUCE (as a consequence of our former experience
with this system), we point out that the concept of Smart Help can be extended
to other Computer Algebra Systems. Technically, Smart Help is a Production
System on the top of a particular implementation of MANTRA, a hybrid knowledge
representation system, which has REDUCE integrated as an additional knowledge
representation module. Since the heuristic level of MANTRA has not yet been
implemented, being presently represented by the Lisp language itself, Smart
Help is coded in Lisp and resides in the same Lisp session of MANTRA. A
prototype of Smart Help is now running on a SUN work-station on an experimental
basis.
"
684,"Geometric involutive bases for positive dimensional polynomial ideals
  and SDP methods","  Geometric involutive bases for polynomial systems of equations have their
origin in the prolongation and projection methods of the geometers Cartan and
Kuranishi for systems of PDE. They are useful for numerical ideal membership
testing and the solution of polynomial systems. In this paper we further
develop our symbolic-numeric methods for such bases. We give methods to
explicitly extract and decrease the degree of intermediate systems and the
output basis. Algorithms for the numerical computation of involutivity criteria
for positive dimensional ideals are also discussed.
  We were also motivated by some remarkable recent work by Lasserre and
collaborators who employed our prolongation projection involutive criteria as a
part of their semi-definite based programming (SDP) method for identifying the
real radical of zero dimensional polynomial ideals. Consequently in this paper
we begin an exploration of the interaction between geometric involutive bases
and these methods particularly in the positive dimensional case. Motivated by
the extension of these methods to the positive dimensional case we explore the
interplay between geometric involutive bases and the new SDP methods.
"
685,"Fast K\""otter-Nielsen-H{\o}holdt Interpolation in the Guruswami-Sudan
  Algorithm","  The K\""otter-Nielsen-H{\o}holdt algorithm is a popular way to construct the
bivariate interpolation polynomial in the Guruswami-Sudan decoding algorithm
for Reed-Solomon codes. In this paper, we show how one can use Divide & Conquer
techniques to provide an asymptotic speed-up of the algorithm, rendering its
complexity quasi-linear in n. Several of our observations can also provide a
practical speed-up to the classical version of the algorithm.
"
686,Hierarchical Comprehensive Triangular Decomposition,"  The concept of comprehensive triangular decomposition (CTD) was first
introduced by Chen et al. in their CASC'2007 paper and could be viewed as an
analogue of comprehensive Grobner systems for parametric polynomial systems.
The first complete algorithm for computing CTD was also proposed in that paper
and implemented in the RegularChains library in Maple. Following our previous
work on generic regular decomposition for parametric polynomial systems, we
introduce in this paper a so-called hierarchical strategy for computing CTDs.
Roughly speaking, for a given parametric system, the parametric space is
divided into several sub-spaces of different dimensions and we compute CTDs
over those sub-spaces one by one. So, it is possible that, for some benchmarks,
it is difficult to compute CTDs in reasonable time while this strategy can
obtain some ""partial"" solutions over some parametric sub-spaces. The program
based on this strategy has been tested on a number of benchmarks from the
literature. Experimental results on these benchmarks with comparison to
RegularChains are reported and may be valuable for developing more efficient
triangularization tools.
"
687,Computing GCRDs of Approximate Differential Polynomials,"  Differential (Ore) type polynomials with approximate polynomial coefficients
are introduced. These provide a useful representation of approximate
differential operators with a strong algebraic structure, which has been used
successfully in the exact, symbolic, setting. We then present an algorithm for
the approximate Greatest Common Right Divisor (GCRD) of two approximate
differential polynomials, which intuitively is the differential operator whose
solutions are those common to the two inputs operators. More formally, given
approximate differential polynomials $f$ and $g$, we show how to find ""nearby""
polynomials $\widetilde f$ and $\widetilde g$ which have a non-trivial GCRD.
Here ""nearby"" is under a suitably defined norm. The algorithm is a
generalization of the SVD-based method of Corless et al. (1995) for the
approximate GCD of regular polynomials. We work on an appropriately
""linearized"" differential Sylvester matrix, to which we apply a block SVD. The
algorithm has been implemented in Maple and a demonstration of its robustness
is presented.
"
688,Covering Rational Ruled Surfaces,"  We present an algorithm that covers any given rational ruled surface with two
rational parametrizations. In addition, we present an algorithm that transforms
any rational surface parametrization into a new rational surface
parametrization without affine base points and such that the degree of the
corresponding maps is preserved.
"
689,"Solving the ""Isomorphism of Polynomials with Two Secrets"" Problem for
  all Pairs of Quadratic Forms","  We study the Isomorphism of Polynomial (IP2S) problem with m=2 homogeneous
quadratic polynomials of n variables over a finite field of odd characteristic:
given two quadratic polynomials (a, b) on n variables, we find two bijective
linear maps (s,t) such that b=t . a . s. We give an algorithm computing s and t
in time complexity O~(n^4) for all instances, and O~(n^3) in a dominant set of
instances.
  The IP2S problem was introduced in cryptography by Patarin back in 1996. The
special case of this problem when t is the identity is called the isomorphism
with one secret (IP1S) problem. Generic algebraic equation solvers (for example
using Gr\""obner bases) solve quite well random instances of the IP1S problem.
For the particular cyclic instances of IP1S, a cubic-time algorithm was later
given and explained in terms of pencils of quadratic forms over all finite
fields; in particular, the cyclic IP1S problem in odd characteristic reduces to
the computation of the square root of a matrix.
  We give here an algorithm solving all cases of the IP1S problem in odd
characteristic using two new tools, the Kronecker form for a singular quadratic
pencil, and the reduction of bilinear forms over a non-commutative algebra.
Finally, we show that the second secret in the IP2S problem may be recovered in
cubic time.
"
690,"Algorithm for computing the factor ring of an ideal in Dedekind domain
  with finite rank","  We give an algorithm for computing the factor ring of a given ideal in a
Dedekind domain with finite rank, which runs in deterministic and
polynomial-time. We provide two applications of the algorithm: judging whether
a given ideal is prime or prime power. The main algorithm is based on basis
representation of finite rings which is computed via Hermite and Smith normal
forms.
"
691,A novel approach to integration by parts reduction,"  Integration by parts reduction is a standard component of most modern
multi-loop calculations in quantum field theory. We present a novel strategy
constructed to overcome the limitations of currently available reduction
programs based on Laporta's algorithm. The key idea is to construct algebraic
identities from numerical samples obtained from reductions over finite fields.
We expect the method to be highly amenable to parallelization, show a low
memory footprint during the reduction step, and allow for significantly better
run-times.
"
692,"Gr\""obner Bases for Linearized Polynomials","  In this work we develop the theory of Gr\""obner bases for modules over the
ring of univariate linearized polynomials with coefficients from a finite
field.
"
693,Strongly stable ideals and Hilbert polynomials,"  The \texttt{StronglyStableIdeals} package for \textit{Macaulay2} provides a
method to compute all saturated strongly stable ideals in a given polynomial
ring with a fixed Hilbert polynomial. A description of the main method and
auxiliary tools is given.
"
694,"A New Primitive for a Diffie-Hellman-like Key Exchange Protocol Based on
  Multivariate Ore Polynomials","  In this paper we present a new primitive for a key exchange protocol based on
multivariate non-commutative polynomial rings, analogous to the classic
Diffie-Hellman method. Our technique extends the proposed scheme of Boucher et
al. from 2010. Their method was broken by Dubois and Kammerer in 2011, who
exploited the Euclidean domain structure of the chosen ring. However, our
proposal is immune against such attacks, without losing the advantages of
non-commutative polynomial rings as outlined by Boucher et al. Moreover, our
extension is not restricted to any particular ring, but is designed to allow
users to readily choose from a large class of rings when applying the protocol.
Our primitive can also be applied to other cryptographic paradigms. In
particular, we develop a three-pass protocol, a public key cryptosystem, a
digital signature scheme and a zero-knowledge proof protocol.
"
695,"Recent Symbolic Summation Methods to Solve Coupled Systems of
  Differential and Difference Equations","  We outline a new algorithm to solve coupled systems of differential equations
in one continuous variable $x$ (resp. coupled difference equations in one
discrete variable $N$) depending on a small parameter $\epsilon$: given such a
system and given sufficiently many initial values, we can determine the first
coefficients of the Laurent-series solutions in $\epsilon$ if they are
expressible in terms of indefinite nested sums and products. This systematic
approach is based on symbolic summation algorithms in the context of difference
rings/fields and uncoupling algorithms. The proposed method gives rise to new
interesting applications in connection with integration by parts (IBP) methods.
As an illustrative example, we will demonstrate how one can calculate the
$\epsilon$-expansion of a ladder graph with 6 massive fermion lines.
"
696,Determining surfaces of revolution from their implicit equations,"  Results of number of geometric operations (often used in technical practise,
as e.g. the operation of blending) are in many cases surfaces described
implicitly. Then it is a challenging task to recognize the type of the obtained
surface, find its characteristics and for the rational surfaces compute also
their parameterizations. In this contribution we will focus on surfaces of
revolution. These objects, widely used in geometric modelling, are generated by
rotating a generatrix around a given axis. If the generatrix is an algebraic
curve then so is also the resulting surface, described uniquely by a polynomial
which can be found by some well-established implicitation technique. However,
starting from a polynomial it is not known how to decide if the corresponding
algebraic surface is rotational or not. Motivated by this, our goal is to
formulate a simple and efficient algorithm whose input is a polynomial with the
coefficients from some subfield of $\mathbb{R}$ and the output is the answer
whether the shape is a surface of revolution. In the affirmative case we also
find the equations of its axis and generatrix. Furthermore, we investigate the
problem of rationality and unirationality of surfaces of revolution and show
that this question can be efficiently answered discussing the rationality of a
certain associated planar curve.
"
697,"Resultant of an equivariant polynomial system with respect to the
  symmetric group","  Given a system of n homogeneous polynomials in n variables which is
equivariant with respect to the canonical actions of the symmetric group of n
symbols on the variables and on the polynomials, it is proved that its
resultant can be decomposed into a product of several smaller resultants that
are given in terms of some divided differences. As an application, we obtain a
decomposition formula for the discriminant of a multivariate homogeneous
symmetric polynomial.
"
698,"Rigorous uniform approximation of D-finite functions using Chebyshev
  expansions","  A wide range of numerical methods exists for computing polynomial
approximations of solutions of ordinary differential equations based on
Chebyshev series expansions or Chebyshev interpolation polynomials. We consider
the application of such methods in the context of rigorous computing (where we
need guarantees on the accuracy of the result), and from the complexity point
of view. It is well-known that the order-n truncation of the Chebyshev
expansion of a function over a given interval is a near-best uniform polynomial
approximation of the function on that interval. In the case of solutions of
linear differential equations with polynomial coefficients, the coefficients of
the expansions obey linear recurrence relations with polynomial coefficients.
Unfortunately, these recurrences do not lend themselves to a direct recursive
computation of the coefficients, owing among other things to a lack of initial
conditions. We show how they can nevertheless be used, as part of a validated
process, to compute good uniform approximations of D-finite functions together
with rigorous error bounds, and we study the complexity of the resulting
algorithms. Our approach is based on a new view of a classical numerical method
going back to Clenshaw, combined with a functional enclosure method.
"
699,Survey on counting special types of polynomials,"  Most integers are composite and most univariate polynomials over a finite
field are reducible. The Prime Number Theorem and a classical result of
Gau{\ss} count the remaining ones, approximately and exactly.
  For polynomials in two or more variables, the situation changes dramatically.
Most multivariate polynomials are irreducible. This survey presents counting
results for some special classes of multivariate polynomials over a finite
field, namely the the reducible ones, the s-powerful ones (divisible by the
s-th power of a nonconstant polynomial), the relatively irreducible ones
(irreducible but reducible over an extension field), the decomposable ones, and
also for reducible space curves. These come as exact formulas and as
approximations with relative errors that essentially decrease exponentially in
the input size.
  Furthermore, a univariate polynomial f is decomposable if f = g o h for some
nonlinear polynomials g and h. It is intuitively clear that the decomposable
polynomials form a small minority among all polynomials. The tame case, where
the characteristic p of Fq does not divide n = deg f, is fairly
well-understood, and we obtain closely matching upper and lower bounds on the
number of decomposable polynomials. In the wild case, where p does divide n,
the bounds are less satisfactory, in particular when p is the smallest prime
divisor of n and divides n exactly twice. The crux of the matter is to count
the number of collisions, where essentially different (g, h) yield the same f.
We present a classification of all collisions at degree n = p^2 which yields an
exact count of those decomposable polynomials.
"
700,Elements of Design for Containers and Solutions in the LinBox Library,"  We describe in this paper new design techniques used in the \cpp exact linear
algebra library \linbox, intended to make the library safer and easier to use,
while keeping it generic and efficient. First, we review the new simplified
structure for containers, based on our \emph{founding scope allocation} model.
We explain design choices and their impact on coding: unification of our matrix
classes, clearer model for matrices and submatrices, \etc Then we present a
variation of the \emph{strategy} design pattern that is comprised of a
controller--plugin system: the controller (solution) chooses among plug-ins
(algorithms) that always call back the controllers for subtasks. We give
examples using the solution \mul. Finally we present a benchmark architecture
that serves two purposes: Providing the user with easier ways to produce
graphs; Creating a framework for automatically tuning the library and
supporting regression testing.
"
701,Even faster integer multiplication,"  We give a new proof of F\""urer's bound for the cost of multiplying n-bit
integers in the bit complexity model. Unlike F\""urer, our method does not
require constructing special coefficient rings with ""fast"" roots of unity.
Moreover, we prove the more explicit bound O(n log n K^(log^* n))$ with K = 8.
We show that an optimised variant of F\""urer's algorithm achieves only K = 16,
suggesting that the new algorithm is faster than F\""urer's by a factor of
2^(log^* n). Assuming standard conjectures about the distribution of Mersenne
primes, we give yet another algorithm that achieves K = 4.
"
702,Faster polynomial multiplication over finite fields,"  Let p be a prime, and let M_p(n) denote the bit complexity of multiplying two
polynomials in F_p[X] of degree less than n. For n large compared to p, we
establish the bound M_p(n) = O(n log n 8^(log^* n) log p), where log^* is the
iterated logarithm. This is the first known F\""urer-type complexity bound for
F_p[X], and improves on the previously best known bound M_p(n) = O(n log n log
log n log p).
"
703,"Nested (inverse) binomial sums and new iterated integrals for massive
  Feynman diagrams","  Nested sums containing binomial coefficients occur in the computation of
massive operator matrix elements. Their associated iterated integrals lead to
alphabets including radicals, for which we determined a suitable basis. We
discuss algorithms for converting between sum and integral representations,
mainly relying on the Mellin transform. To aid the conversion we worked out
dedicated rewrite rules, based on which also some general patterns emerging in
the process can be obtained.
"
704,"The package HarmonicSums: Computer Algebra and Analytic aspects of
  Nested Sums","  This paper summarizes the essential functionality of the computer algebra
package HarmonicSums. On the one hand HarmonicSums can work with nested sums
such as harmonic sums and their generalizations and on the other hand it can
treat iterated integrals of the Poincare and Chen-type, such as harmonic
polylogarithms and their generalizations. The interplay of these
representations and the analytic aspects are illustrated by concrete examples.
"
705,GCD Computation of n Integers,"  Greatest Common Divisor (GCD) computation is one of the most important
operation of algorithmic number theory. In this paper we present the algorithms
for GCD computation of $n$ integers. We extend the Euclid's algorithm and
binary GCD algorithm to compute the GCD of more than two integers.
"
706,"An Algorithm for Deciding the Summability of Bivariate Rational
  Functions","  Let $\Delta_x f(x,y)=f(x+1,y)-f(x,y)$ and $\Delta_y f(x,y)=f(x,y+1)-f(x,y)$
be the difference operators with respect to $x$ and $y$. A rational function
$f(x,y)$ is called summable if there exist rational functions $g(x,y)$ and
$h(x,y)$ such that $f(x,y)=\Delta_x g(x,y) + \Delta_y h(x,y)$. Recently, Chen
and Singer presented a method for deciding whether a rational function is
summable. To implement their method in the sense of algorithms, we need to
solve two problems. The first is to determine the shift equivalence of two
bivariate polynomials. We solve this problem by presenting an algorithm for
computing the dispersion sets of any two bivariate polynomials. The second is
to solve a univariate difference equation in an algebraically closed field. By
considering the irreducible factorization of the denominator of $f(x,y)$ in a
general field, we present a new criterion which requires only finding a
rational solution of a bivariate difference equation. This goal can be achieved
by deriving a universal denominator of the rational solutions and a degree
bound on the numerator. Combining these two algorithms, we can decide the
summability of a bivariate rational function.
"
707,"Certifying solutions to overdetermined and singular polynomial systems
  over Q","  This paper is concerned with certifying that a given point is near an exact
root of an overdetermined or singular polynomial system with rational
coefficients. The difficulty lies in the fact that consistency of
overdetermined systems is not a continuous property. Our certification is based
on hybrid symbolic-numeric methods to compute the exact ""rational univariate
representation"" (RUR) of a component of the input system from approximate
roots. For overdetermined polynomial systems with simple roots, we compute an
initial RUR from approximate roots. The accuracy of the RUR is increased via
Newton iterations until the exact RUR is found, which we certify using exact
arithmetic. Since the RUR is well-constrained, we can use it to certify the
given approximate roots using alpha-theory. To certify isolated singular roots,
we use a determinantal form of the ""isosingular deflation"", which adds new
polynomials to the original system without introducing new variables. The
resulting polynomial system is overdetermined, but the roots are now simple,
thereby reducing the problem to the overdetermined case. We prove that our
algorithms have complexity that are polynomial in the input plus the output
size upon successful convergence, and we use worst case upper bounds for
termination when our iteration does not converge to an exact RUR. Examples are
included to demonstrate the approach.
"
708,A Difference Ring Theory for Symbolic Summation,"  A summation framework is developed that enhances Karr's difference field
approach. It covers not only indefinite nested sums and products in terms of
transcendental extensions, but it can treat, e.g., nested products defined over
roots of unity. The theory of the so-called $R\Pi\Sigma^*$-extensions is
supplemented by algorithms that support the construction of such difference
rings automatically and that assist in the task to tackle symbolic summation
problems. Algorithms are presented that solve parameterized telescoping
equations, and more generally parameterized first-order difference equations,
in the given difference ring. As a consequence, one obtains algorithms for the
summation paradigms of telescoping and Zeilberger's creative telescoping. With
this difference ring theory one obtains a rigorous summation machinery that has
been applied to numerous challenging problems coming, e.g., from combinatorics
and particle physics.
"
709,"Solving Polynomial Equations with Equation Constraints: the
  Zero-dimensional Case","  A zero-dimensional polynomial ideal may have a lot of complex zeros. But
sometimes, only some of them are needed. In this paper, for a zero-dimensional
ideal $I$, we study its complex zeros that locate in another variety
$\textbf{V}(J)$ where $J$ is an arbitrary ideal.
  The main problem is that for a point in $\textbf{V}(I) \cap
\textbf{V}(J)=\textbf{V}(I+J)$, its multiplicities w.r.t. $I$ and $I+J$ may be
different. Therefore, we cannot get the multiplicity of this point w.r.t. $I$
by studying $I + J$. A straightforward way is that first compute the points of
$\textbf{V}(I + J)$, then study their multiplicities w.r.t. $I$. But the former
step is difficult to realize exactly.
  In this paper, we propose a natural geometric explanation of the localization
of a polynomial ring corresponding to a semigroup order. Then, based on this
view, using the standard basis method and the border basis method, we introduce
a way to compute the complex zeros of $I$ in $\textbf{V}(J)$ with their
multiplicities w.r.t. $I$. As an application, we compute the sum of Milnor
numbers of the singular points on a polynomial hypersurface and work out all
the singular points on the hypersurface with their Milnor numbers.
"
710,Computing Multiplicative Order and Primitive Root in Finite Cyclic Group,"  Multiplicative order of an element $a$ of group $G$ is the least positive
integer $n$ such that $a^n=e$, where $e$ is the identity element of $G$. If the
order of an element is equal to $|G|$, it is called generator or primitive
root. This paper describes the algorithms for computing multiplicative order
and primitive root in $\mathbb{Z}^*_{p}$, we also present a logarithmic
improvement over classical algorithms.
"
711,Desingularization of Ore Operators,"  We show that Ore operators can be desingularized by calculating a least
common left multiple with a random operator of appropriate order. Our result
generalizes a classical result about apparent singularities of linear
differential equations, and it gives rise to a surprisingly simple
desingularization algorithm.
"
712,Bounds for D-finite closure properties,"  We provide bounds on the size of operators obtained by algorithms for
executing D-finite closure properties. For operators of small order, we give
bounds on the degree and on the height (bit-size). For higher order operators,
we give degree bounds that are parameterized with respect to the order and
reflect the phenomenon that higher order operators may have lower degrees
(order-degree curves).
"
713,"Computing the determinant of a matrix with polynomial entries by
  approximation","  Computing the determinant of a matrix with the univariate and multivariate
polynomial entries arises frequently in the scientific computing and
engineering fields. In this paper, an effective algorithm is presented for
computing the determinant of a matrix with polynomial entries using hybrid
symbolic and numerical computation. The algorithm relies on the Newton's
interpolation method with error control for solving Vandermonde systems. It is
also based on a novel approach for estimating the degree of variables, and the
degree homomorphism method for dimension reduction. Furthermore, the
parallelization of the method arises naturally.
"
714,Algorithms in Real Algebraic Geometry: A Survey,"  We survey both old and new developments in the theory of algorithms in real
algebraic geometry -- starting from effective quantifier elimination in the
first order theory of reals due to Tarski and Seidenberg, to more recent
algorithms for computing topological invariants of semi-algebraic sets. We
emphasize throughout the complexity aspects of these algorithms and also
discuss the computational hardness of the underlying problems. We also describe
some recent results linking the computational hardness of decision problems in
the first order theory of the reals, with that of computing certain topological
invariants of semi-algebraic sets. Even though we mostly concentrate on exact
algorithms, we also discuss some numerical approaches involving semi-definite
programming that have gained popularity in recent times.
"
715,"Using the distribution of cells by dimension in a cylindrical algebraic
  decomposition","  We investigate the distribution of cells by dimension in cylindrical
algebraic decompositions (CADs). We find that they follow a standard
distribution which seems largely independent of the underlying problem or CAD
algorithm used. Rather, the distribution is inherent to the cylindrical
structure and determined mostly by the number of variables.
  This insight is then combined with an algorithm that produces only
full-dimensional cells to give an accurate method of predicting the number of
cells in a complete CAD. Since constructing only full-dimensional cells is
relatively inexpensive (involving no costly algebraic number calculations) this
leads to heuristics for helping with various questions of problem formulation
for CAD, such as choosing an optimal variable ordering. Our experiments
demonstrate that this approach can be highly effective.
"
716,On Solving Pentadiagonal Linear Systems via Transformations,"  Many authors studied numeric algorithms for solving the linear systems of the
pentadiagonal type. The well-known Fast Pentadiagonal System Solver algorithm
is an example of such algorithms. The current article are described new numeric
and symbolic algorithms for solving pentadiagonal linear systems via
transformations. New algorithms are natural generalization of the work
presented in [Moawwad El- Mikkawy and Faiz Atlan, Algorithms for Solving Linear
Systems of Equations of Tridiagonal Type via Transformations, Applied
Mathematics, 2014, 5, 413-422]. The symbolic algorithms remove the cases where
the numeric algorithms fail. The computational cost of our algorithms is given.
Some examples are given in order to illustrate the effectiveness of the
proposed algorithms. All of the experiments are performed on a computer with
the aid of programs written in MATLAB.
"
717,"Fast and deterministic computation of the determinant of a polynomial
  matrix","  Given a square, nonsingular matrix of univariate polynomials
$\mathbf{F}\in\mathbb{K}[x]^{n\times n}$ over a field $\mathbb{K}$, we give a
deterministic algorithm for finding the determinant of $\mathbf{F}$. The
complexity of the algorithm is $\bigO \left(n^{\omega}s\right)$ field
operations where $s$ is the average column degree or the average row degree of
$\mathbf{F}$. Here $\bigO$ notation is Big-$O$ with log factors omitted and
$\omega$ is the exponent of matrix multiplication.
"
718,"Tropical Effective Primary and Dual Nullstellens\""atze","  Tropical algebra is an emerging field with a number of applications in
various areas of mathematics. In many of these applications appeal to tropical
polynomials allows to study properties of mathematical objects such as
algebraic varieties and algebraic curves from the computational point of view.
This makes it important to study both mathematical and computational aspects of
tropical polynomials.
  In this paper we prove a tropical Nullstellensatz and moreover we show an
effective formulation of this theorem. Nullstellensatz is a natural step in
building algebraic theory of tropical polynomials and its effective version is
relevant for computational aspects of this field.
  On our way we establish a simple formulation of min-plus and tropical linear
dualities. We also observe a close connection between tropical and min-plus
polynomial systems.
"
719,"On Ideal Lattices and Gr\""obner Bases","  In this paper, we draw a connection between ideal lattices and Gr\""{o}bner
bases in the multivariate polynomial rings over integers. We study extension of
ideal lattices in $\mathbb{Z}[x]/\langle f \rangle$ (Lyubashevsky \&
Micciancio, 2006) to ideal lattices in
$\mathbb{Z}[x_1,\ldots,x_n]/\mathfrak{a}$, the multivariate case, where $f$ is
a polynomial in $\mathbb{Z}[X]$ and $\mathfrak{a}$ is an ideal in
$\mathbb{Z}[x_1,\ldots,x_n]$. Ideal lattices in univariate case are interpreted
as generalizations of cyclic lattices. We introduce a notion of multivariate
cyclic lattices and we show that multivariate ideal lattices are indeed a
generalization of them. We show that the fact that existence of ideal lattice
in univariate case if and only if $f$ is monic translates to short reduced
Gr\""obner basis (Francis \& Dukkipati, 2014) of $\mathfrak{a}$ is monic in
multivariate case. We, thereby, give a necessary and sufficient condition for
residue class polynomial rings over $\mathbb{Z}$ to have ideal lattices. We
also characterize ideals in $\mathbb{Z}[x_1,\ldots,x_n]$ that give rise to full
rank lattices.
"
720,"A Monomial-Oriented GVW for Computing Gr\""obner Bases","  The GVW algorithm, presented by Gao et al., is a signature-based algorithm
for computing Gr\""obner bases. In this paper, a variant of GVW is presented.
This new algorithm is called a monomial-oriented GVW algorithm or mo-GVW
algorithm for short. The mo-GVW algorithm presents a new frame of GVW and
regards {\em labeled monomials} instead of {\em labeled polynomials} as basic
elements of the algorithm. Being different from the original GVW algorithm, for
each labeled monomial, the mo-GVW makes efforts to find the smallest signature
that can generate this monomial. The mo-GVW algorithm also avoids generating
J-pairs, and uses efficient methods of searching reducers and checking
criteria. Thus, the mo-GVW algorithm has a better performance during practical
implementations.
"
721,Automatic Generation of Loop-Invariants for Matrix Operations,"  In recent years it has been shown that for many linear algebra operations it
is possible to create families of algorithms following a very systematic
procedure. We do not refer to the fine tuning of a known algorithm, but to a
methodology for the actual generation of both algorithms and routines to solve
a given target matrix equation. Although systematic, the methodology relies on
complex algebraic manipulations and non-obvious pattern matching, making the
procedure challenging to be performed by hand, our goal is the development of a
fully automated system that from the sole description of a target equation
creates multiple algorithms and routines. We present CL1ck, a symbolic system
written in Mathematica, that starts with an equation, decomposes it into
multiple equations, and returns a set of loop-invariants for the algorithms --
yet to be generated -- that will solve the equation. In a successive step each
loop-invariant is then mapped to its corresponding algorithm and routine. For a
large class of equations, the methodology generates known algorithms as well as
many previously unknown ones. Most interestingly, the methodology unifies
algorithms traditionally developed in isolation. As an example, the five well
known algorithms for the LU factorization are for the first time unified under
a common root.
"
722,Knowledge-Based Automatic Generation of Partitioned Matrix Expressions,"  In a series of papers it has been shown that for many linear algebra
operations it is possible to generate families of algorithms by following a
systematic procedure. Although powerful, such a methodology involves complex
algebraic manipulation, symbolic computations and pattern matching, making the
generation a process challenging to be performed by hand. We aim for a fully
automated system that from the sole description of a target operation creates
multiple algorithms without any human intervention. Our approach consists of
three main stages. The first stage yields the core object for the entire
process, the Partitioned Matrix Expression (PME), which establishes how the
target problem may be decomposed in terms of simpler sub-problems. In the
second stage the PME is inspected to identify predicates, the Loop-Invariants,
to be used to set up the skeleton of a family of proofs of correctness. In the
third and last stage the actual algorithms are constructed so that each of them
satisfies its corresponding proof of correctness. In this paper we focus on the
first stage of the process, the automatic generation of Partitioned Matrix
Expressions. In particular, we discuss the steps leading to a PME and the
knowledge necessary for a symbolic system to perform such steps. We also
introduce Cl1ck, a prototype system written in Mathematica that generates PMEs
automatically.
"
723,"On Ideal Lattices, Gr\""obner Bases and Generalized Hash Functions","  In this paper, we draw connections between ideal lattices and multivariate
polynomial rings over integers using Gr\""obner bases. Ideal lattices are ideals
in the residue class ring, $\mathbb{Z}[x]/\langle f \rangle$ (here $f$ is a
monic polynomial), and cryptographic primitives have been built based on these
objects. As ideal lattices in the univariate case are generalizations of cyclic
lattices, we introduce the notion of multivariate cyclic lattices and show that
multivariate ideal lattices are indeed a generalization of them. Based on
multivariate ideal lattices, we establish the existence of collision resistant
hash functions using Gr\""obner basis techniques. For the construction of hash
functions, we define a worst case problem, shortest substitution problem w.r.t.
an ideal in $\mathbb{Z}[x_1,\ldots, x_n]$, and establish hardness results using
functional fields.
"
724,Recognizing implicitly given rational canal surfaces,"  It is still a challenging task of today to recognize the type of a given
algebraic surface which is described only by its implicit representation.
In~this paper we will investigate in more detail the case of canal surfaces
that are often used in geometric modelling, Computer-Aided Design and technical
practice (e.g. as blending surfaces smoothly joining two parts with circular
ends). It is known that if the squared medial axis transform is a rational
curve then so is also the corresponding surface. However, starting from a
polynomial it is not known how to decide if the corresponding algebraic surface
is rational canal surface or not. Our goal is to formulate a simple and
efficient algorithm whose input is a~polynomial with the coefficients from some
subfield of real numbers and the output is the answer whether the surface is a
rational canal surface. In the affirmative case we also compute a rational
parameterization of the squared medial axis transform which can be then used
for finding a rational parameterization of the implicitly given canal surface.
"
725,"A Direct Algorithm to Compute the Topological Euler Characteristic and
  Chern-Schwartz-MacPherson Class of Projective Complete Intersection Varieties","  Let $V$ be a possibly singular scheme-theoretic complete intersection
subscheme of $\mathbb{P}^n$ over an algebraically closed field of
characteristic zero. Using a recent result of Fullwood (""On Milnor classes via
invariants of singular subschemes"", Journal of Singularities) we develop an
algorithm to compute the Chern-Schwartz-MacPherson class and Euler
characteristic of $V$. This algorithm complements existing algorithms by
providing performance improvements in the computation of the
Chern-Schwartz-MacPherson class and Euler characteristic for certain types of
complete intersection subschemes of $\mathbb{P}^n$.
"
726,Missing sets in rational parametrizations of surfaces of revolution,"  Parametric representations do not cover, in general, the whole geometric
object that they parametrize. This can be a problem in practical applications.
In this paper we analyze the question for surfaces of revolution generated by
real rational profile curves, and we describe a simple small superset of the
real zone of the surface not covered by the parametrization. This superset
consists, in the worst case, of the union of a circle and the mirror curve of
the profile curve.
"
727,"Gr\""obner Bases and Nullstellens\""atze for Graph-Coloring Ideals","  We revisit a well-known family of polynomial ideals encoding the problem of
graph-$k$-colorability. Our paper describes how the inherent combinatorial
structure of the ideals implies several interesting algebraic properties.
Specifically, we provide lower bounds on the difficulty of computing Gr\""obner
bases and Nullstellensatz certificates for the coloring ideals of general
graphs. For chordal graphs, however, we explicitly describe a Gr\""obner basis
for the coloring ideal, and provide a polynomial-time algorithm.
"
728,New effective differential Nullstellensatz,"  We show new upper and lower bounds for the effective differential
Nullstellensatz for differential fields of characteristic zero with several
commuting derivations. Seidenberg was the first to address this problem in
1956, without giving a complete solution. The first explicit bounds appeared in
2009 in a paper by Golubitsky, Kondratieva, Szanto, and Ovchinnikov, with the
upper bound expressed in terms of the Ackermann function. D'Alfonso, Jeronimo,
and Solern\'o, using novel ideas, obtained in 2014 a new bound if restricted to
the case of one derivation and constant coefficients. To obtain the bound in
the present paper without this restriction, we extend this approach and use the
new methods of Freitag and Le\'on S\'anchez and of Pierce from 2014, which
represent a model-theoretic approach to differential algebraic geometry.
"
729,"Exploiting chordal structure in polynomial ideals: a Gr\""obner bases
  approach","  Chordal structure and bounded treewidth allow for efficient computation in
numerical linear algebra, graphical models, constraint satisfaction and many
other areas. In this paper, we begin the study of how to exploit chordal
structure in computational algebraic geometry, and in particular, for solving
polynomial systems. The structure of a system of polynomial equations can be
described in terms of a graph. By carefully exploiting the properties of this
graph (in particular, its chordal completions), more efficient algorithms can
be developed. To this end, we develop a new technique, which we refer to as
chordal elimination, that relies on elimination theory and Gr\""obner bases. By
maintaining graph structure throughout the process, chordal elimination can
outperform standard Gr\""obner basis algorithms in many cases. The reason is
that all computations are done on ""smaller"" rings, of size equal to the
treewidth of the graph. In particular, for a restricted class of ideals, the
computational complexity is linear in the number of variables. Chordal
structure arises in many relevant applications. We demonstrate the suitability
of our methods in examples from graph colorings, cryptography, sensor
localization and differential equations.
"
730,"Sparse implicitization by interpolation: Geometric computations using
  matrix representations","  Based on the computation of a superset of the implicit support,
implicitization of a parametrically given hyper-surface is reduced to computing
the nullspace of a numeric matrix. Our approach exploits the sparseness of the
given parametric equations and of the implicit polynomial. In this work, we
study how this interpolation matrix can be used to reduce some key geometric
predicates on the hyper-surface to simple numerical operations on the matrix,
namely membership and sidedness for given query points. We illustrate our
results with examples based on our Maple implementation.
"
731,Sparse Univariate Polynomials with Many Roots Over Finite Fields,"  Suppose $q$ is a prime power and $f\in\mathbb{F}_q[x]$ is a univariate
polynomial with exactly $t$ monomial terms and degree $<q-1$. To establish a
finite field analogue of Descartes' Rule, Bi, Cheng, and Rojas (2013) proved an
upper bound of $2(q-1)^{\frac{t-2}{t-1}}$ on the number of cosets in
$\mathbb{F}^*_q$ needed to cover the roots of $f$ in $\mathbb{F}^*_q$. Here, we
give explicit $f$ with root structure approaching this bound: For $q$ a
$(t-1)$-st power of a prime we give an explicit $t$-nomial vanishing on
$q^{\frac{t-2}{t-1}}$ distinct cosets of $\mathbb{F}^*_q$. Over prime fields
$\mathbb{F}_p$, computational data we provide suggests that it is harder to
construct explicit sparse polynomials with many roots. Nevertheless, assuming
the Generalized Riemann Hypothesis, we find explicit trinomials having
$\Omega\left(\frac{\log p}{\log \log p}\right)$ distinct roots in
$\mathbb{F}_p$.
"
732,A canonical form for the continuous piecewise polynomial functions,"  We present in this paper a canonical form for the elements in the ring of
continuous piecewise polynomial functions. This new representation is based on
the use of a particular class of functions
$$\{C_i(P):P\in\Q[x],i=0,\ldots,\deg(P)\}$$ defined by $$C_i(P)(x)= \left\{
\begin{array}{cll}0 & \mbox{ if } & x \leq \alpha \\ P(x) & \mbox{ if } & x
\geq \alpha \end{array} \right.$$ where $\alpha$ is the $i$-th real root of the
polynomial $P$. These functions will allow us to represent and manipulate
easily every continuous piecewise polynomial function through the use of the
corresponding canonical form.
  It will be also shown how to produce a ""rational"" representation of each
function $C_{i}(P)$ allowing its evaluation by performing only operations in
$\Q$ and avoiding the use of any real algebraic number.
"
733,"Finding Semi-Analytic Solutions of Power System Differential-Algebraic
  Equations for Fast Transient Stability Simulation","  This paper studies the semi-analytic solution (SAS) of a power system's
differential-algebraic equation. A SAS is a closed-form function of symbolic
variables including time, the initial state and the parameters on system
operating conditions, and hence able to directly give trajectories on system
state variables, which are accurate for at least a certain time window. A
two-stage SAS-based approach for fast transient stability simulation is
proposed, which offline derives the SAS by the Adomian Decomposition Method and
online evaluates the SAS for each of sequential time windows until making up a
desired simulation period. When applied to fault simulation, the new approach
employs numerical integration only for the fault-on period to determine the
post-disturbance initial state of the SAS. The paper further analyzes the
maximum length of a time window for a SAS to keep its accuracy, and
accordingly, introduces a divergence indicator for adaptive time windows. The
proposed SAS-based new approach is validated on the IEEE 10-machine, 39-bus
system.
"
734,"A streamlined difference ring theory: Indefinite nested sums, the
  alternating sign and the parameterized telescoping problem","  We present an algebraic framework to represent indefinite nested sums over
hypergeometric expressions in difference rings. In order to accomplish this
task, parts of Karr's difference field theory have been extended to a ring
theory in which also the alternating sign can be expressed. The underlying
machinery relies on algorithms that compute all solutions of a given
parameterized telescoping equation. As a consequence, we can solve the
telescoping and creative telescoping problem in such difference rings.
"
735,"Numeric certified algorithm for the topology of resultant and
  discriminant curves","  Let $\mathcal C$ be a real plane algebraic curve defined by the resultant of
two polynomials (resp. by the discriminant of a polynomial). Geometrically such
a curve is the projection of the intersection of the surfaces
$P(x,y,z)=Q(x,y,z)=0$ (resp. $P(x,y,z)=\frac{\partial P}{\partial
z}(x,y,z)=0$), and generically its singularities are nodes (resp. nodes and
ordinary cusps). State-of-the-art numerical algorithms compute the topology of
smooth curves but usually fail to certify the topology of singular ones. The
main challenge is to find practical numerical criteria that guarantee the
existence and the uniqueness of a singularity inside a given box $B$, while
ensuring that $B$ does not contain any closed loop of $\mathcal{C}$. We solve
this problem by first providing a square deflation system, based on
subresultants, that can be used to certify numerically whether $B$ contains a
unique singularity $p$ or not. Then we introduce a numeric adaptive separation
criterion based on interval arithmetic to ensure that the topology of $\mathcal
C$ in $B$ is homeomorphic to the local topology at $p$. Our algorithms are
implemented and experiments show their efficiency compared to state-of-the-art
symbolic or homotopic methods.
"
736,Bounded-degree factors of lacunary multivariate polynomials,"  In this paper, we present a new method for computing bounded-degree factors
of lacunary multivariate polynomials. In particular for polynomials over number
fields, we give a new algorithm that takes as input a multivariate polynomial f
in lacunary representation and a degree bound d and computes the irreducible
factors of degree at most d of f in time polynomial in the lacunary size of f
and in d. Our algorithm, which is valid for any field of zero characteristic,
is based on a new gap theorem that enables reducing the problem to several
instances of (a) the univariate case and (b) low-degree multivariate
factorization.
  The reduction algorithms we propose are elementary in that they only
manipulate the exponent vectors of the input polynomial. The proof of
correctness and the complexity bounds rely on the Newton polytope of the
polynomial, where the underlying valued field consists of Puiseux series in a
single variable.
"
737,"Four Random Permutations Conjugated by an Adversary Generate $S_n$ with
  High Probability","  We prove a conjecture dating back to a 1978 paper of D.R.\
Musser~\cite{musserirred}, namely that four random permutations in the
symmetric group $\mathcal{S}_n$ generate a transitive subgroup with probability
$p_n > \epsilon$ for some $\epsilon > 0$ independent of $n$, even when an
adversary is allowed to conjugate each of the four by a possibly different
element of $\S_n$ (in other words, the cycle types already guarantee generation
of $\mathcal{S}_n$). This is closely related to the following random set model.
A random set $M \subseteq \mathbb{Z}^+$ is generated by including each $n \geq
1$ independently with probability $1/n$. The sumset $\text{sumset}(M)$ is
formed. Then at most four independent copies of $\text{sumset}(M)$ are needed
before their mutual intersection is no longer infinite.
"
738,Efficient edge-skeleton computation for polytopes defined by oracles,"  In general dimension, there is no known total polynomial algorithm for either
convex hull or vertex enumeration, i.e. an algorithm whose complexity depends
polynomially on the input and output sizes. It is thus important to identify
problems (and polytope representations) for which total polynomial-time
algorithms can be obtained. We offer the first total polynomial-time algorithm
for computing the edge-skeleton (including vertex enumeration) of a polytope
given by an optimization or separation oracle, where we are also given a
superset of its edge directions. We also offer a space-efficient variant of our
algorithm by employing reverse search. All complexity bounds refer to the
(oracle) Turing machine model. There is a number of polytope classes naturally
defined by oracles; for some of them neither vertex nor facet representation is
obvious. We consider two main applications, where we obtain (weakly) total
polynomial-time algorithms: Signed Minkowski sums of convex polytopes, where
polytopes can be subtracted provided the signed sum is a convex polytope, and
computation of secondary, resultant, and discriminant polytopes. Further
applications include convex combinatorial optimization and convex integer
programming, where we offer a new approach, thus removing the complexity's
exponential dependence in the dimension.
"
739,"Faster Sparse Multivariate Polynomial Interpolation of Straight-Line
  Programs","  Given a straight-line program whose output is a polynomial function of the
inputs, we present a new algorithm to compute a concise representation of that
unknown function. Our algorithm can handle any case where the unknown function
is a multivariate polynomial, with coefficients in an arbitrary finite field,
and with a reasonable number of nonzero terms but possibly very large degree.
It is competitive with previously known sparse interpolation algorithms that
work over an arbitrary finite field, and provides an improvement when there are
a large number of variables.
"
740,On the Inverting of A General Heptadiagonal Matrix,"  In this paper, we developed new numeric and symbolic algorithms to find the
inverse of any nonsingular heptadiagonal matrix. Symbolic algorithm will not
break and it is without setting any restrictive conditions. The computational
cost of our algorithms is $O(n)$. The algorithms are suitable for
implementation using computer algebra system such as MAPLE, MATLAB and
MATHEMATICA. Examples are given to illustrate the efficiency of the algorithms.
"
741,"A Successive Resultant Projection for Cylindrical Algebraic
  Decomposition","  This note shows the equivalence of two projection operators which both can be
used in cylindrical algebraic decomposition (CAD) . One is known as Brown's
Projection (C. W. Brown (2001)); the other was proposed by Lu Yang in his
earlier work (L.Yang and S.~H. Xia (2000)) that is sketched as follows: given a
polynomial $f$ in $x_1,\,x_2,\,\cdots$, by $f_1$ denote the resultant of $f$
and its partial derivative with respect to $x_1$ (removing the multiple
factors), by $f_2$ denote the resultant of $f_1$ and its partial derivative
with respect to $x_2$, (removing the multiple factors), $\cdots$, repeat this
procedure successively until the last resultant becomes a univariate
polynomial. Making use of an identity, the equivalence of these two projection
operators is evident.
"
742,"Probabilistic analysis of Wiedemann's algorithm for minimal polynomial
  computation","  Blackbox algorithms for linear algebra problems start with projection of the
sequence of powers of a matrix to a sequence of vectors (Lanczos), a sequence
of scalars (Wiedemann) or a sequence of smaller matrices (block methods). Such
algorithms usually depend on the minimal polynomial of the resulting sequence
being that of the given matrix. Here exact formulas are given for the
probability that this occurs. They are based on the generalized Jordan normal
form (direct sum of companion matrices of the elementary divisors) of the
matrix. Sharp bounds follow from this for matrices of unknown elementary
divisors. The bounds are valid for all finite field sizes and show that a small
blocking factor can give high probability of success for all cardinalities and
matrix dimensions.
"
743,Real root finding for determinants of linear matrices,"  Let $\A_0, \A_1, \ldots, \A_n$ be given square matrices of size $m$ with
rational coefficients. The paper focuses on the exact computation of one point
in each connected component of the real determinantal variety $\{\X \in\RR^n \:
:\: \det(\A_0+x_1\A_1+\cdots+x_n\A_n)=0\}$. Such a problem finds applications
in many areas such as control theory, computational geometry, optimization,
etc. Using standard complexity results this problem can be solved using
$m^{O(n)}$ arithmetic operations. Under some genericity assumptions on the
coefficients of the matrices, we provide an algorithm solving this problem
whose runtime is essentially quadratic in ${{n+m}\choose{n}}^{3}$. We also
report on experiments with a computer implementation of this algorithm. Its
practical performance illustrates the complexity estimates. In particular, we
emphasize that for subfamilies of this problem where $m$ is fixed, the
complexity is polynomial in $n$.
"
744,Generalization of Gabidulin Codes over Fields of Rational Functions,"  We transpose the theory of rank metric and Gabidulin codes to the case of
fields which are not finite fields. The Frobenius automorphism is replaced by
any element of the Galois group of a cyclic algebraic extension of a base
field. We use our framework to define Gabidulin codes over the field of
rational functions using algebraic function fields with a cyclic Galois group.
This gives a linear subspace of matrices whose coefficients are rational
function, such that the rank of each of this matrix is lower bounded, where the
rank is comprised in term of linear combination with rational functions. We
provide two examples based on Kummer and Artin-Schreier extensions.The matrices
that we obtain may be interpreted as generating matrices of convolutional
codes.
"
745,"On the complexity of computing Gr\""obner bases for weighted homogeneous
  systems","  Solving polynomial systems arising from applications is frequently made
easier by the structure of the systems. Weighted homogeneity (or
quasi-homogeneity) is one example of such a structure: given a system of
weights $W=(w\_{1},\dots,w\_{n})$, $W$-homogeneous polynomials are polynomials
which are homogeneous w.r.t the weighted degree
$\deg\_{W}(X\_{1}^{\alpha\_{1}},\dots,X\_{n}^{\alpha\_{n}}) = \sum
w\_{i}\alpha\_{i}$. Gr\""obner bases for weighted homogeneous systems can be
computed by adapting existing algorithms for homogeneous systems to the
weighted homogeneous case. We show that in this case, the complexity estimate
for Algorithm~\F5 $\left(\binom{n+\dmax-1}{\dmax}^{\omega}\right)$ can be
divided by a factor $\left(\prod w\_{i} \right)^{\omega}$. For zero-dimensional
systems, the complexity of Algorithm~\FGLM $nD^{\omega}$ (where $D$ is the
number of solutions of the system) can be divided by the same factor
$\left(\prod w\_{i} \right)^{\omega}$. Under genericity assumptions, for
zero-dimensional weighted homogeneous systems of $W$-degree
$(d\_{1},\dots,d\_{n})$, these complexity estimates are polynomial in the
weighted B\'ezout bound $\prod\_{i=1}^{n}d\_{i} / \prod\_{i=1}^{n}w\_{i}$.
Furthermore, the maximum degree reached in a run of Algorithm \F5 is bounded by
the weighted Macaulay bound $\sum (d\_{i}-w\_{i}) + w\_{n}$, and this bound is
sharp if we can order the weights so that $w\_{n}=1$. For overdetermined
semi-regular systems, estimates from the homogeneous case can be adapted to the
weighted case. We provide some experimental results based on systems arising
from a cryptography problem and from polynomial inversion problems. They show
that taking advantage of the weighted homogeneous structure yields substantial
speed-ups, and allows us to solve systems which were otherwise out of reach.
"
746,"Accurate solution of near-colliding Prony systems via decimation and
  homotopy continuation","  We consider polynomial systems of Prony type, appearing in many areas of
mathematics. Their robust numerical solution is considered to be difficult,
especially in ""near-colliding"" situations. We consider a case when the
structure of the system is a-priori fixed. We transform the nonlinear part of
the Prony system into a Hankel-type polynomial system. Combining this
representation with a recently discovered ""decimation"" technique, we present an
algorithm which applies homotopy continuation to an appropriately chosen
Hankel-type system as above. In this way, we are able to solve for the
nonlinear variables of the original system with high accuracy when the data is
perturbed.
"
747,Data-Discriminants of Likelihood Equations,"  Maximum likelihood estimation (MLE) is a fundamental computational problem in
statistics. The problem is to maximize the likelihood function with respect to
given data on a statistical model. An algebraic approach to this problem is to
solve a very structured parameterized polynomial system called likelihood
equations. For general choices of data, the number of complex solutions to the
likelihood equations is finite and called the ML-degree of the model. The only
solutions to the likelihood equations that are statistically meaningful are the
real/positive solutions. However, the number of real/positive solutions is not
characterized by the ML-degree. We use discriminants to classify data according
to the number of real/positive solutions of the likelihood equations. We call
these discriminants data-discriminants (DD). We develop a probabilistic
algorithm for computing DDs. Experimental results show that, for the benchmarks
we have tried, the probabilistic algorithm is more efficient than the standard
elimination algorithm. Based on the computational results, we discuss the real
root classification problem for the 3 by 3 symmetric matrix~model.
"
748,"Parallel degree computation for solution space of binomial systems with
  an application to the master space of $\mathcal{N}=1$ gauge theories","  The problem of solving a system of polynomial equations is one of the most
fundamental problems in applied mathematics. Among them, the problem of solving
a system of binomial equations form a important subclass for which specialized
techniques exist. For both theoretic and applied purposes, the degree of the
solution set of a system of binomial equations often plays an important role in
understanding the geometric structure of the solution set. Its computation,
however, is computationally intensive. This paper proposes a specialized
parallel algorithm for computing the degree on GPUs that takes advantage of the
massively parallel nature of GPU devices. The preliminary implementation shows
remarkable efficiency and scalability when compared to the closest CPU-based
counterpart. Applied to the ""master space problem of $\mathcal{N}=1$ gauge
theories"" the GPU-based implementation achieves nearly 30 fold speedup over its
CPU-only counterpart enabling the discovery of previously unknown results.
Equally important to note is the far superior scalability: with merely 3 GPU
devices on a single workstation, the GPU-based implementation shows better
performance, on certain problems, than a small cluster totaling 100 CPU cores.
"
749,Computation of Differential Chow Forms for Prime Differential Ideals,"  In this paper, we propose algorithms to compute differential Chow forms for
prime differential ideals which are given by their characteristic sets. The
main algorithm is based on an optimal bound for the order of a prime
differential ideal in terms of its characteristic set under an arbitrary
ranking, which shows the Jacobi bound conjecture holds in this case. Apart from
the order bound, we also give a degree bound for the differential Chow form. In
addition, for prime differential ideals given by their characteristic sets
under an orderly ranking, a much more simpler algorithm is given to compute its
differential Chow form. The computational complexity of both is single
exponential in terms of the Jacobi number, the maximal degree of the
differential polynomials in the characteristic set and the number of variables.
"
750,Integral D-Finite Functions,"  We propose a differential analog of the notion of integral closure of
algebraic function fields. We present an algorithm for computing the integral
closure of the algebra defined by a linear differential operator. Our algorithm
is a direct analog of van Hoeij's algorithm for computing integral bases of
algebraic function fields.
"
751,"Improving the use of equational constraints in cylindrical algebraic
  decomposition","  When building a cylindrical algebraic decomposition (CAD) savings can be made
in the presence of an equational constraint (EC): an equation logically implied
by a formula.
  The present paper is concerned with how to use multiple ECs, propagating
those in the input throughout the projection set. We improve on the approach of
McCallum in ISSAC 2001 by using the reduced projection theory to make savings
in the lifting phase (both to the polynomials we lift with and the cells lifted
over). We demonstrate the benefits with worked examples and a complexity
analysis.
"
752,A test for monomial containment,"  We present an algorithm to decide whether a given ideal in the polynomial
ring contains a monomial without using Gr\""obner bases, factorization or
sub-resultant computations.
"
753,"A Modified Abramov-Petkovsek Reduction and Creative Telescoping for
  Hypergeometric Terms","  The Abramov-Petkovsek reduction computes an additive decomposition of a
hypergeometric term, which extends the functionality of the Gosper algorithm
for indefinite hypergeometric summation. We modify the Abramov-Petkovsek
reduction so as to decompose a hypergeometric term as the sum of a summable
term and a non-summable one. The outputs of the Abramov-Petkovsek reduction and
our modified version share the same required properties. The modified reduction
does not solve any auxiliary linear difference equation explicitly. It is also
more efficient than the original reduction according to computational
experiments. Based on this reduction, we design a new algorithm to compute
minimal telescopers for bivariate hypergeometric terms. The new algorithm can
avoid the costly computation of certificates.
"
754,Subtropical Real Root Finding,"  We describe a new incomplete but terminating method for real root finding for
large multivariate polynomials. We take an abstract view of the polynomial as
the set of exponent vectors associated with sign information on the
coefficients. Then we employ linear programming to heuristically find roots.
There is a specialized variant for roots with exclusively positive coordinates,
which is of considerable interest for applications in chemistry and systems
biology. An implementation of our method combining the computer algebra system
Reduce with the linear programming solver Gurobi has been successfully applied
to input data originating from established mathematical models used in these
areas. We have solved several hundred problems with up to more than 800000
monomials in up to 10 variables with degrees up to 12. Our method has failed
due to its incompleteness in less than 8 percent of the cases.
"
755,Better Answers to Real Questions,"  We consider existential problems over the reals. Extended quantifier
elimination generalizes the concept of regular quantifier elimination by
providing in addition answers, which are descriptions of possible assignments
for the quantified variables. Implementations of extended quantifier
elimination via virtual substitution have been successfully applied to various
problems in science and engineering. So far, the answers produced by these
implementations included infinitesimal and infinite numbers, which are hard to
interpret in practice. We introduce here a post-processing procedure to
convert, for fixed parameters, all answers into standard real numbers. The
relevance of our procedure is demonstrated by application of our implementation
to various examples from the literature, where it significantly improves the
quality of the results.
"
756,Computing the Rank Profile Matrix,"  The row (resp. column) rank profile of a matrix describes the staircase shape
of its row (resp. column) echelon form. In an ISSAC'13 paper, we proposed a
recursive Gaussian elimination that can compute simultaneously the row and
column rank profiles of a matrix as well as those of all of its leading
sub-matrices, in the same time as state of the art Gaussian elimination
algorithms. Here we first study the conditions making a Gaus-sian elimination
algorithm reveal this information. Therefore, we propose the definition of a
new matrix invariant, the rank profile matrix, summarizing all information on
the row and column rank profiles of all the leading sub-matrices. We also
explore the conditions for a Gaussian elimination algorithm to compute all or
part of this invariant, through the corresponding PLUQ decomposition. As a
consequence, we show that the classical iterative CUP decomposition algorithm
can actually be adapted to compute the rank profile matrix. Used, in a Crout
variant, as a base-case to our ISSAC'13 implementation, it delivers a
significant improvement in efficiency. Second, the row (resp. column) echelon
form of a matrix are usually computed via different dedicated triangular
decompositions. We show here that, from some PLUQ decompositions, it is
possible to recover the row and column echelon forms of a matrix and of any of
its leading sub-matrices thanks to an elementary post-processing algorithm.
"
757,"Output-sensitive algorithms for sumset and sparse polynomial
  multiplication","  We present randomized algorithms to compute the sumset (Minkowski sum) of two
integer sets, and to multiply two univariate integer polynomials given by
sparse representations. Our algorithm for sumset has cost softly linear in the
combined size of the inputs and output. This is used as part of our sparse
multiplication algorithm, whose cost is softly linear in the combined size of
the inputs, output, and the sumset of the supports of the inputs. As a
subroutine, we present a new method for computing the coefficients of a sparse
polynomial, given a set containing its support. Our multiplication algorithm
extends to multivariate Laurent polynomials over finite fields and rational
numbers. Our techniques are based on sparse interpolation algorithms and
results from analytic number theory.
"
758,Numerically Safe Gaussian Elimination with No Pivoting,"  Gaussian elimination with no pivoting and block Gaussian elimination are
attractive alternatives to the customary but communication intensive Gaussian
elimination with partial pivoting (hereafter we use the acronyms GENP, BGE, and
GEPP} provided that the computations proceed safely and numerically safely},
that is, run into neither division by 0 nor numerical problems. Empirically,
safety and numerical safety of GENP have been consistently observed in a number
of papers where an input matrix was pre-processed with various structured
multipliers chosen ad hoc. Our present paper provides missing formal support
for this empirical observation and explains why it was elusive so far. Namely
we prove that GENP is numerically unsafe for a specific class of input matrices
in spite of its pre-processing with some well-known and well-tested structured
multipliers, but we also prove that GENP and BGE are safe and numerically safe
for the average input matrix pre-processed with any nonsingular and
well-conditioned multiplier. This should embolden search for sparse and
structured multipliers, and we list and test some new classes of them. We also
seek randomized pre-processing that universally (that is, for all input
matrices) supports (i) safe GENP and BGE with probability 1 and/or (ii)
numerically safe GENP and BGE with a probability close to 1.We achieve goal (i)
with a Gaussian structured multiplier and goal (ii) with a Gaussian
unstructured multiplier and alternatively with Gaussian structured
augmentation. We consistently confirm all these formal results with our tests
of GENP for benchmark inputs. We have extended our approach to other
fundamental matrix computations and keep working on further extensions.
"
759,"Real Polynomial Root-finding by Means of Matrix and Polynomial
  Iterations","  Univariate polynomial root-finding is a classical subject, still important
for modern computing. Frequently one seeks just the real roots of a polynomial
with real coefficients. They can be approximated at a low computational cost if
the polynomial has no nonreal roots, but for high degree polynomials, nonreal
roots are typically much more numerous than the real ones. The challenge is
known for a long time, and the subject has been intensively studied.
Nevertheless, we produce some novel ideas and techniques and obtain dramatic
acceleration of the known algorithms. In order to achieve our progress we
exploit the correlation between the computations with matrices and polynomials,
randomized matrix computations, and complex plane geometry, extend the
techniques of the matrix sign iterations, and use the structure of the
companion matrix of the input polynomial. The results of our extensive tests
with benchmark polynomials and random matrices are quite encouraging. In
particular in our tests the number of iterations required for convergence of
our algorithms grew very slowly (if at all) as we increased the degree of the
univariate input polynomials and the dimension of the input matrices from 64 to
1024.
"
760,"Accelerated Approximation of the Complex Roots and Factors of a
  Univariate Polynomial","  The algorithms of Pan (1995) and(2002) approximate the roots of a complex
univariate polynomial in nearly optimal arithmetic and Boolean time but require
precision of computing that exceeds the degree of the polynomial. This causes
numerical stability problems when the degree is large. We observe, however,
that such a difficulty disappears at the initial stage of the algorithms, and
in our present paper we extend this stage to root-finding within a nearly
optimal arithmetic and Boolean complexity bounds provided that some mild
initial isolation of the roots of the input polynomial has been ensured.
Furthermore our algorithm is nearly optimal for the approximation of the roots
isolated in a fixed disc, square or another region on the complex plane rather
than all complex roots of a polynomial. Moreover the algorithm can be applied
to a polynomial given by a black box for its evaluation (even if its
coefficients are not known); it promises to be of practical value for
polynomial root-finding and factorization, the latter task being of interest on
its own right. We also provide a new support for a winding number algorithm,
which enables extension of our progress to obtaining mild initial
approximations to the roots. We conclude with summarizing our algorithms and
their extension to the approximation of isolated multiple roots and root
clusters.
"
761,A Generalized Framework for Virtual Substitution,"  We generalize the framework of virtual substitution for real quantifier
elimination to arbitrary but bounded degrees. We make explicit the
representation of test points in elimination sets using roots of parametric
univariate polynomials described by Thom codes. Our approach follows an early
suggestion by Weispfenning, which has never been carried out explicitly.
Inspired by virtual substitution for linear formulas, we show how to
systematically construct elimination sets containing only test points
representing lower bounds.
"
762,Polyhedral Omega: A New Algorithm for Solving Linear Diophantine Systems,"  Polyhedral Omega is a new algorithm for solving linear Diophantine systems
(LDS), i.e., for computing a multivariate rational function representation of
the set of all non-negative integer solutions to a system of linear equations
and inequalities. Polyhedral Omega combines methods from partition analysis
with methods from polyhedral geometry. In particular, we combine MacMahon's
iterative approach based on the Omega operator and explicit formulas for its
evaluation with geometric tools such as Brion decompositions and Barvinok's
short rational function representations. In this way, we connect two recent
branches of research that have so far remained separate, unified by the concept
of symbolic cones which we introduce. The resulting LDS solver Polyhedral Omega
is significantly faster than previous solvers based on partition analysis and
it is competitive with state-of-the-art LDS solvers based on geometric methods.
Most importantly, this synthesis of ideas makes Polyhedral Omega the simplest
algorithm for solving linear Diophantine systems available to date. Moreover,
we provide an illustrated geometric interpretation of partition analysis, with
the aim of making ideas from both areas accessible to readers from a wide range
of backgrounds.
"
763,Near Optimal Subdivision Algorithms for Real Root Isolation,"  We describe a subroutine that improves the running time of any subdivision
algorithm for real root isolation. The subroutine first detects clusters of
roots using a result of Ostrowski, and then uses Newton iteration to converge
to them. Near a cluster, we switch to subdivision, and proceed recursively. The
subroutine has the advantage that it is independent of the predicates used to
terminate the subdivision. This gives us an alternative and simpler approach to
recent developments of Sagraloff (2012) and Sagraloff-Mehlhorn (2013), assuming
exact arithmetic.
  The subdivision tree size of our algorithm using predicates based on
Descartes's rule of signs is bounded by $O(n\log n)$, which is better by
$O(n\log L)$ compared to known results. Our analysis differs in two key
aspects. First, we use the general technique of continuous amortization from
Burr-Krahmer-Yap (2009), and second, we use the geometry of clusters of roots
instead of the Davenport-Mahler bound. The analysis naturally extends to other
predicates.
"
764,Real root finding for rank defects in linear Hankel matrices,"  Let $H\_0, ..., H\_n$ be $m \times m$ matrices with entries in $\QQ$ and
Hankel structure, i.e. constant skew diagonals. We consider the linear Hankel
matrix $H(\vecx)=H\_0+\X\_1H\_1+...+\X\_nH\_n$ and the problem of computing
sample points in each connected component of the real algebraic set defined by
the rank constraint ${\sf rank}(H(\vecx))\leq r$, for a given integer $r \leq
m-1$. Computing sample points in real algebraic sets defined by rank defects in
linear matrices is a general problem that finds applications in many areas such
as control theory, computational geometry, optimization, etc. Moreover, Hankel
matrices appear in many areas of engineering sciences. Also, since Hankel
matrices are symmetric, any algorithmic development for this problem can be
seen as a first step towards a dedicated exact algorithm for solving
semi-definite programming problems, i.e. linear matrix inequalities. Under some
genericity assumptions on the input (such as smoothness of an incidence
variety), we design a probabilistic algorithm for tackling this problem. It is
an adaptation of the so-called critical point method that takes advantage of
the special structure of the problem. Its complexity reflects this: it is
essentially quadratic in specific degree bounds on an incidence variety. We
report on practical experiments and analyze how the algorithm takes advantage
of this special structure. A first implementation outperforms existing
implementations for computing sample points in general real algebraic sets: it
tackles examples that are out of reach of the state-of-the-art.
"
765,Fast integer multiplication using generalized Fermat primes,"  For almost 35 years, Sch{\""o}nhage-Strassen's algorithm has been the fastest
algorithm known for multiplying integers, with a time complexity O(n $\times$
log n $\times$ log log n) for multiplying n-bit inputs. In 2007, F{\""u}rer
proved that there exists K > 1 and an algorithm performing this operation in
O(n $\times$ log n $\times$ K log n). Recent work by Harvey, van der Hoeven,
and Lecerf showed that this complexity estimate can be improved in order to get
K = 8, and conjecturally K = 4. Using an alternative algorithm, which relies on
arithmetic modulo generalized Fermat primes, we obtain conjecturally the same
result K = 4 via a careful complexity analysis in the deterministic multitape
Turing model.
"
766,"Extractions: Computable and Visible Analogues of Localizations for
  Polynomial Ideals","  When studying local properties of a polynomial ideal, one usually needs a
theoretic technique called localization. For most cases, in spite of its
importance, the computation in a localized ring cannot be algorithmically
preformed. On the other hand, the standard basis method is very effective for
the computation in a special kind of localized rings, but for a general
semigroup order the geometry of the localization of a positive-dimensional
ideal is difficult to interpret.
  In this paper, we introduce a new ideal operation called extraction. For an
ideal $I$ in a polynomial ring $K[x_1,\ldots,x_n]$ over a field $K$, we use
another ideal $J$ to control the primary components of $I$ and the result
$\beta(I,J)$ is called the extraction of $I$ by $J$. It is still a polynomial
ideal and has a concrete geometric meaning in $\bar{K}^n$, i.e., we keep the
branches of $\textbf{V}(I) \subset \bar{K}^n$ that intersect with
$\textbf{V}(J) \subset \bar{K}^n$ and delete others, where $\bar{K}$ is the
algebraic closure of $K$. This is what we mean by visible. On the other hand,
we can use the standard basis method to compute a localized ideal corresponding
to $\beta(I,J)$ without a complete primary decomposition, and can do further
computation in the localized ring such as determining the membership problem of
$\beta(I,J)$. Moreover, we prove that extractions are as powerful as
localizations in the sense that for any multiplicatively closed subset $S$ of
$K[x_1,\ldots,x_n]$ and any polynomial ideal $I$, there always exists a
polynomial ideal $J$ such that $\beta(I,J)=(S^{-1}I)^c$.
"
767,Planar Linkages Following a Prescribed Motion,"  Designing mechanical devices, called linkages, that draw a given plane curve
has been a topic that interested engineers and mathematicians for hundreds of
years, and recently also computer scientists. Already in 1876, Kempe proposed a
procedure for solving the problem in full generality, but his constructions
tend to be extremely complicated. We provide a novel algorithm that produces
much simpler linkages, but works only for parametric curves. Our approach is to
transform the problem into a factorization task over some noncommutative
algebra. We show how to compute such a factorization, and how to use it to
construct a linkage tracing a given curve.
"
768,Automatic differentiation in machine learning: a survey,"  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in
machine learning. Automatic differentiation (AD), also called algorithmic
differentiation or simply ""autodiff"", is a family of techniques similar to but
more general than backpropagation for efficiently and accurately evaluating
derivatives of numeric functions expressed as computer programs. AD is a small
but established field with applications in areas including computational fluid
dynamics, atmospheric sciences, and engineering design optimization. Until very
recently, the fields of machine learning and AD have largely been unaware of
each other and, in some cases, have independently discovered each other's
results. Despite its relevance, general-purpose AD has been missing from the
machine learning toolbox, a situation slowly changing with its ongoing adoption
under the names ""dynamic computational graphs"" and ""differentiable
programming"". We survey the intersection of AD and machine learning, cover
applications where AD has direct relevance, and address the main implementation
techniques. By precisely defining the main differentiation techniques and their
interrelationships, we aim to bring clarity to the usage of the terms
""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as
these are encountered more and more in machine learning settings.
"
769,"On Sequences, Rational Functions and Decomposition","  Our overall goal is to unify and extend some results in the literature
related to the approximation of generating functions of finite and infinite
sequences over a field by rational functions. In our approach, numerators play
a significant role. We revisit a theorem of Niederreiter on (i) linear
complexities and (ii) '$n^{th}$ minimal polynomials' of an infinite sequence,
proved using partial quotients. We prove (i) and its converse from first
principles and generalise (ii) to rational functions where the denominator need
not have minimal degree. We prove (ii) in two parts: firstly for geometric
sequences and then for sequences with a jump in linear complexity. The basic
idea is to decompose the denominator as a sum of polynomial multiples of two
polynomials of minimal degree; there is a similar decomposition for the
numerators. The decomposition is unique when the denominator has degree at most
the length of the sequence. The proof also applies to rational functions
related to finite sequences, generalising a result of Massey. We give a number
of applications to rational functions associated to sequences.
"
770,Groebner basis in Boolean rings is not polynomial-space,"  We give an example where the number of elements of a Groebner basis in a
Boolean ring is not polynomially bounded in terms of the bitsize and degrees of
the input.
"
771,Factorization of Motion Polynomials,"  In this paper, we consider the existence of a factorization of a monic,
bounded motion polynomial. We prove existence of factorizations, possibly after
multiplication with a real polynomial and provide algorithms for computing
polynomial factor and factorizations. The first algorithm is conceptually
simpler but may require a high degree of the polynomial factor. The second
algorithm gives an optimal degree.
"
772,Tropical differential equations,"  Tropical differential equations are introduced and an algorithm is designed
which tests solvability of a system of tropical linear differential equations
within the complexity polynomial in the size of the system and in its
coefficients. Moreover, we show that there exists a minimal solution, and the
algorithm constructs it (in case of solvability). This extends a similar
complexity bound established for tropical linear systems. In case of tropical
linear differential systems in one variable a polynomial complexity algorithm
for testing its solvability is designed.
  We prove also that the problem of solvability of a system of tropical
non-linear differential equations in one variable is $NP$-hard, and this
problem for arbitrary number of variables belongs to $NP$. Similar to tropical
algebraic equations, a tropical differential equation expresses the (necessary)
condition on the dominant term in the issue of solvability of a differential
equation in power series.
"
773,On the Computation of the Galois Group of Linear Difference Equations,"  We present an algorithm that determines the Galois group of linear difference
equations with rational function coefficients.
"
774,"A Triangular Decomposition Algorithm for Differential Polynomial Systems
  with Elementary Computation Complexity","  In this paper, a new triangular decomposition algorithm is proposed for
ordinary differential polynomial systems, which has triple exponential
computational complexity. The key idea is to eliminate one algebraic variable
from a set of polynomials in one step using the theory of multivariate
resultant. This seems to be the first differential triangular decomposition
algorithm with elementary computation complexity.
"
775,Polynomial complexity recognizing a tropical linear variety,"  A polynomial complexity algorithm is designed which tests whether a point
belongs to a given tropical linear variety.
"
776,An implementation of Sub-CAD in Maple,"  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications in algebraic geometry
and beyond. We have previously reported on an implementation of CAD in Maple
which offers the original projection and lifting algorithm of Collins along
with subsequent improvements.
  Here we report on new functionality: specifically the ability to build
cylindrical algebraic sub-decompositions (sub-CADs) where only certain cells
are returned. We have implemented algorithms to return cells of a prescribed
dimensions or higher (layered {\scad}s), and an algorithm to return only those
cells on which given polynomials are zero (variety {\scad}s). These offer
substantial savings in output size and computation time.
  The code described and an introductory Maple worksheet / pdf demonstrating
the full functionality of the package are freely available online at
http://opus.bath.ac.uk/43911/.
"
777,One-Step Stochastic Processes Simulation Software Package,"  Background. It is assumed that the introduction of stochastic in mathematical
model makes it more adequate. But there is virtually no methods of coordinated
(depended on structure of the system) stochastic introduction into
deterministic models. Authors have improved the method of stochastic models
construction for the class of one-step processes and illustrated by models of
population dynamics. Population dynamics was chosen for study because its
deterministic models were sufficiently well explored that allows to compare the
results with already known ones.
  Purpose. To optimize the models creation as much as possible some routine
operations should be automated. In this case, the process of drawing up the
model equations can be algorithmized and implemented in the computer algebra
system. Furthermore, on the basis of these results a set of programs for
numerical experiment can be obtained.
  Method. The computer algebra system Axiom is used for analytical calculations
implementation. To perform the numerical experiment FORTRAN and Julia languages
are used. The method Runge--Kutta method for stochastic differential equations
is used as numerical method.
  Results. The program compex for creating stochastic one-step processes models
is constructed. Its application is illustrated by the predator-prey population
dynamic system.
  Conclusions. Computer algebra systems are very convenient for the purposes of
rapid prototyping in mathematical models design and analysis.
"
778,"Gr\""obner Bases over Algebraic Number Fields","  Although Buchberger's algorithm, in theory, allows us to compute Gr\""obner
bases over any field, in practice, however, the computational efficiency
depends on the arithmetic of the ground field. Consider a field $K =
\mathbb{Q}(\alpha)$, a simple extension of $\mathbb{Q}$, where $\alpha$ is an
algebraic number, and let $f \in \mathbb{Q}[t]$ be the minimal polynomial of
$\alpha$. In this paper we present a new efficient method to compute Gr\""obner
bases in polynomial rings over the algebraic number field $K$. Starting from
the ideas of Noro [Noro, 2006], we proceed by joining $f$ to the ideal to be
considered, adding $t$ as an extra variable. But instead of avoiding
superfluous S-pair reductions by inverting algebraic numbers, we achieve the
same goal by applying modular methods as in [Arnold, 2003; B\""ohm et al., 2015;
Idrees et al., 2011], that is, by inferring information in characteristic zero
from information in characteristic $p > 0$. For suitable primes $p$, the
minimal polynomial $f$ is reducible over $\mathbb{F}_p$. This allows us to
apply modular methods once again, on a second level, with respect to the
factors of $f$. The algorithm thus resembles a divide and conquer strategy and
is in particular easily parallelizable. At current state, the algorithm is
probabilistic in the sense that, as for other modular Gr\""obner basis
computations, an effective final verification test is only known for
homogeneous ideals or for local monomial orderings. The presented timings show
that for most examples, our algorithm, which has been implemented in SINGULAR,
outperforms other known methods by far.
"
779,"Index reduction of differential algebraic equations by differential
  algebraic elimination","  High index differential algebraic equations (DAEs) are ordinary differential
equations (ODEs) with constraints and arise frequently from many mathematical
models of physical phenomenons and engineering fields. In this paper, we
generalize the idea of differential elimination with Dixon resultant to
polynomially nonlinear DAEs. We propose a new algorithm for index reduction of
DAEs and establish the notion of differential algebraic elimination, which can
provide the differential algebraic resultant of the enlarged system of original
equations. To make use of structure of DAEs, variable pencil technique is given
to determine the termination of differentiation. Moreover, we also provide a
heuristics method for removing the extraneous factors from differential
algebraic resultant. The experimentation shows that the proposed algorithm
outperforms existing ones for many examples taken from the literature.
"
780,Timed Orchestration for Component-based Systems,"  Individual machines in flexible production lines explicitly expose
capabilities at their interfaces by means of parametric skills. Given such a
set of configurable machines, a line integrator is faced with the problem of
finding and tuning parameters for each machine such that the overall production
line implements given safety and temporal requirements in an optimized and
robust fashion. We formalize this problem of configuring and orchestrating
flexible production lines as a parameter synthesis problem for systems of
parametric timed automata, where interactions are based on skills. Parameter
synthesis problems for interaction-level LTL properties are translated to
parameter synthesis problems for state-based safety properties. For safety
properties, synthesis problems are solved by checking satisfiability of
$\exists\forall$SMT constraints. For constraint generation, we provide a set of
computationally cheap over-approximations of the set of reachable states,
together with fence constructions as sufficient conditions for safety formulas.
We demonstrate the feasibility of our approach by solving typical machine
configuration problems as encountered in industrial automation.
"
781,Recent Advances in Real Geometric Reasoning,"  In the 1930s Tarski showed that real quantifier elimination was possible, and
in 1975 Collins gave a remotely practicable method, albeit with
doubly-exponential complexity, which was later shown to be inherent. We discuss
some of the recent major advances in Collins method: such as an alternative
approach based on passing via the complexes, and advances which come closer to
""solving the question asked"" rather than ""solving all problems to do with these
polynomials"".
"
782,On the last fall degree of zero-dimensional Weil descent systems,"  In this article we will discuss a new, mostly theoretical, method for solving
(zero-dimensional) polynomial systems, which lies in between Gr\""obner basis
computations and the heuristic first fall degree assumption and is not based on
any heuristic. This method relies on the new concept of last fall degree.
  Let $k$ be a finite field of cardinality $q^n$ and let $k'$ be its subfield
of cardinality $q$. Let $\mathcal{F} \subset k[X_0,\ldots,X_{m-1}]$ be a finite
subset generating a zero-dimensional ideal. We give an upper bound of the last
fall degree of the Weil descent system of $\mathcal{F}$, which depends on $q$,
$m$, the last fall degree of $\mathcal{F}$, the degree of $\mathcal{F}$ and the
number of solutions of $\mathcal{F}$, but not on $n$. This shows that such Weil
descent systems can be solved efficiently if $n$ grows. In particular, we apply
these results for multi-HFE and essentially show that multi-HFE is insecure.
  Finally, we discuss that the degree of regularity (or last fall degree) of
Weil descent systems coming from summation polynomials to solve the elliptic
curve discrete logarithm problem might depend on $n$, since such systems
without field equations are not zero-dimensional.
"
783,"Pfaffian Systems of A-Hypergeometric Systems II --- Holonomic Gradient
  Method","  We give two efficient methods to derive Pfaffian systems for A-hypergeometric
systems for the application to the holonomic gradient method for statistics. We
utilize the Hilbert driven Buchberger algorithm and Macaulay type matrices in
the two methods.
"
784,"Symbolic-numeric methods for improving structural analysis of
  differential-algebraic equation systems","  Systems of differential-algebraic equations (DAEs) are generated routinely by
simulation and modeling environments such as Modelica and MapleSim. Before a
simulation starts and a numerical solution method is applied, some kind of
structural analysis is performed to determine the structure and the index of a
DAE. Structural analysis methods serve as a necessary preprocessing stage, and
among them, Pantelides's algorithm is widely used.
  Recently Pryce's $\Sigma$-method is becoming increasingly popular, owing to
its straightforward approach and capability of analyzing high-order systems.
Both methods are equivalent in the sense that when one succeeds, producing a
nonsingular system Jacobian, the other also succeeds, and the two give the same
structural index.
  Although provably successful on fairly many problems of interest, the
structural analysis methods can fail on some simple, solvable DAEs and give
incorrect structural information including the index. In this report, we focus
on the $\Sigma$-method. We investigate its failures, and develop two
symbolic-numeric conversion methods for converting a DAE, on which the
$\Sigma$-method fails, to an equivalent problem on which this method succeeds.
Aimed at making structural analysis methods more reliable, our conversion
methods exploit structural information of a DAE, and require a symbolic tool
for their implementation.
"
785,On the Skolem Problem for Continuous Linear Dynamical Systems,"  The Continuous Skolem Problem asks whether a real-valued function satisfying
a linear differential equation has a zero in a given interval of real numbers.
This is a fundamental reachability problem for continuous linear dynamical
systems, such as linear hybrid automata and continuous-time Markov chains.
Decidability of the problem is currently open---indeed decidability is open
even for the sub-problem in which a zero is sought in a bounded interval. In
this paper we show decidability of the bounded problem subject to Schanuel's
Conjecture, a unifying conjecture in transcendental number theory. We
furthermore analyse the unbounded problem in terms of the frequencies of the
differential equation, that is, the imaginary parts of the characteristic
roots. We show that the unbounded problem can be reduced to the bounded problem
if there is at most one rationally linearly independent frequency, or if there
are two rationally linearly independent frequencies and all characteristic
roots are simple. We complete the picture by showing that decidability of the
unbounded problem in the case of two (or more) rationally linearly independent
frequencies would entail a major new effectiveness result in Diophantine
approximation, namely computability of the Diophantine-approximation types of
all real algebraic numbers.
"
786,An Elimination Method to Solve Interval Polynomial Systems,"  There are several efficient methods to solve linear interval polynomial
systems in the context of interval computations, however, the general case of
interval polynomial systems is not yet covered as well. In this paper we
introduce a new elimination method to solve and analyse interval polynomial
systems, in general case. This method is based on computational algebraic
geometry concepts such as polynomial ideals and Groebner basis computation.
Specially, we use the comprehensive Groebner system concept to keep the
dependencies between interval coefficients. At the end of paper, we will state
some applications of our method to evaluate its performance.
"
787,"Solving Polynomial Systems in the Cloud with Polynomial Homotopy
  Continuation","  Polynomial systems occur in many fields of science and engineering.
Polynomial homotopy continuation methods apply symbolic-numeric algorithms to
solve polynomial systems. We describe the design and implementation of our web
interface and reflect on the application of polynomial homotopy continuation
methods to solve polynomial systems in the cloud. Via the graph isomorphism
problem we organize and classify the polynomial systems we solved. The
classification with the canonical form of a graph identifies newly submitted
systems with systems that have already been solved.
"
788,Lacunaryx: Computing bounded-degree factors of lacunary polynomials,"  In this paper, we report on an implementation in the free software Mathemagix
of lacunary factorization algorithms, distributed as a library called
Lacunaryx. These algorithms take as input a polynomial in sparse
representation, that is as a list of nonzero monomials, and an integer $d$, and
compute its irreducible degree-$\le d$ factors. The complexity of these
algorithms is polynomial in the sparse size of the input polynomial and $d$.
"
789,Parallel sparse interpolation using small primes,"  To interpolate a supersparse polynomial with integer coefficients, two
alternative approaches are the Prony-based ""big prime"" technique, which acts
over a single large finite field, or the more recently-proposed ""small primes""
technique, which reduces the unknown sparse polynomial to many low-degree dense
polynomials. While the latter technique has not yet reached the same
theoretical efficiency as Prony-based methods, it has an obvious potential for
parallelization. We present a heuristic ""small primes"" interpolation algorithm
and report on a low-level C implementation using FLINT and MPI.
"
790,p-Adic Stability In Linear Algebra,"  Using the differential precision methods developed previously by the same
authors, we study the p-adic stability of standard operations on matrices and
vector spaces. We demonstrate that lattice-based methods surpass naive methods
in many applications, such as matrix multiplication and sums and intersections
of subspaces. We also analyze determinants , characteristic polynomials and LU
factorization using these differential methods. We supplement our observations
with numerical experiments.
"
791,A Fast Algorithm for Computing the p-Curvature,"  We design an algorithm for computing the $p$-curvature of a differential
system in positive characteristic $p$. For a system of dimension $r$ with
coefficients of degree at most $d$, its complexity is $\softO (p d r^\omega)$
operations in the ground field (where $\omega$ denotes the exponent of matrix
multiplication), whereas the size of the output is about $p d r^2$. Our
algorithm is then quasi-optimal assuming that matrix multiplication is
(\emph{i.e.} $\omega = 2$). The main theoretical input we are using is the
existence of a well-suited ring of series with divided powers for which an
analogue of the Cauchy--Lipschitz Theorem holds.
"
792,Real root finding for low rank linear matrices,"  We consider $m \times s$ matrices (with $m\geq s$) in a real affine subspace
of dimension $n$. The problem of finding elements of low rank in such spaces
finds many applications in information and systems theory, where low rank is
synonymous of structure and parsimony. We design computer algebra algorithms,
based on advanced methods for polynomial system solving, to solve this problem
efficiently and exactly: the input are the rational coefficients of the
matrices spanning the affine subspace as well as the expected maximum rank, and
the output is a rational parametrization encoding a finite set of points that
intersects each connected component of the low rank real algebraic set. The
complexity of our algorithm is studied thoroughly. It is polynomial in
$\binom{n+m(s-r)}{n}$. It improves on the state-of-the-art in computer algebra
and effective real algebraic geometry. Moreover, computer experiments show the
practical efficiency of our approach.
"
793,Symbolic Derivation of Mean-Field PDEs from Lattice-Based Models,"  Transportation processes, which play a prominent role in the life and social
sciences, are typically described by discrete models on lattices. For studying
their dynamics a continuous formulation of the problem via partial differential
equations (PDE) is employed. In this paper we propose a symbolic computation
approach to derive mean-field PDEs from a lattice-based model. We start with
the microscopic equations, which state the probability to find a particle at a
given lattice site. Then the PDEs are formally derived by Taylor expansions of
the probability densities and by passing to an appropriate limit as the time
steps and the distances between lattice sites tend to zero. We present an
implementation in a computer algebra system that performs this transition for a
general class of models. In order to rewrite the mean-field PDEs in a
conservative formulation, we adapt and implement symbolic integration methods
that can handle unspecified functions in several variables. To illustrate our
approach, we consider an application in crowd motion analysis where the
dynamics of bidirectional flows are studied. However, the presented approach
can be applied to various transportation processes of multiple species with
variable size in any dimension, for example, to confirm several proposed
mean-field models for cell motility.
"
794,"On the Connection Between Ritt Characteristic Sets and
  Buchberger-Gr\""obner Bases","  For any polynomial ideal $I$, let the minimal triangular set contained in the
reduced Buchberger-Gr\""obner basis of $I$ with respect to the purely
lexicographical term order be called the W-characteristic set of $I$. In this
paper, we establish a strong connection between Ritt's characteristic sets and
Buchberger's Gr\""obner bases of polynomial ideals by showing that the
W-characteristic set $C$ of $I$ is a Ritt characteristic set of $I$ whenever
$C$ is an ascending set, and a Ritt characteristic set of $I$ can always be
computed from $C$ with simple pseudo-division when $C$ is regular. We also
prove that under certain variable ordering, either the W-characteristic set of
$I$ is normal, or irregularity occurs for the $j$th, but not the $(j+1)$th,
elimination ideal of $I$ for some $j$. In the latter case, we provide explicit
pseudo-divisibility relations, which lead to nontrivial factorizations of
certain polynomials in the Buchberger-Gr\""obner basis and thus reveal the
structure of such polynomials. The pseudo-divisibility relations may be used to
devise an algorithm to decompose arbitrary polynomial sets into normal
triangular sets based on Buchberger-Gr\""obner bases computation.
"
795,"Interactive certificate for the verification of Wiedemann's Krylov
  sequence: application to the certification of the determinant, the minimal
  and the characteristic polynomials of sparse matrices","  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a-possibly randomized- verification
algorithm that proves the correctness of each output. Wiede-mann's algorithm
projects the Krylov sequence obtained by repeatedly multiplying a vector by a
matrix to obtain a linearly recurrent sequence. The minimal polynomial of this
sequence divides the minimal polynomial of the matrix. For instance, if the
$n\times n$ input matrix is sparse with n 1+o(1) non-zero entries, the
computation of the sequence is quadratic in the dimension of the matrix while
the computation of the minimal polynomial is n 1+o(1), once that projected
Krylov sequence is obtained. In this paper we give algorithms that compute
certificates for the Krylov sequence of sparse or structured $n\times n$
matrices over an abstract field, whose Monte Carlo verification complexity can
be made essentially linear. As an application this gives certificates for the
determinant, the minimal and characteristic polynomials of sparse or structured
matrices at the same cost.
"
796,(Pure) transcendence bases in $\phi$-deformed shuffle bialgebras,"  Computations with integro-differential operators are often carried out in an
associative algebra with unit, and they are essentially non-commutative
computations. By adjoining a cocommutative co-product, one can have those
operators perform act on a bialgebra isomorphic to an enveloping algebra. That
gives an adequate framework for a computer-algebra implementation via monoidal
factorization, (pure) transcendence bases and Poincar\'e--Birkhoff--Witt bases.
In this paper, we systematically study these deformations, obtaining necessary
and sufficient conditions for the operators to exist, and we give the most
general cocommutative deformations of the shuffle co-product and an effective
construction of pairs of bases in duality. The paper ends by the combinatorial
setting of local systems of coordinates on the group of group-like series.
"
797,Discovering and Proving Infinite Binomial Sums Identities,"  We consider binomial and inverse binomial sums at infinity and rewrite them
in terms of a small set of constants, such as powers of $\pi$ or $\log(2)$. In
order to perform these simplifications, we view the series as specializations
of generating series. For these generating series, we derive integral
representations in terms of root-valued iterated integrals. Using
substitutions, we express the interated integrals as cyclotomic harmonic
polylogarithms. Finally, by applying known relations among the cyclotomic
harmonic polylogarithms, we derive expressions in terms of several constants.
"
798,An algorithm for computing Grobner basis and the complexity evaluation,"  In this paper, we suggest a new efficient algorithm in order to compute
S-polynomial reduction rapidly in the known algorithm for computing Grobner
bases, and compare the complexity with others.
"
799,"Middle-Solving Grobner bases algorithm for cryptanalysis over finite
  fields","  Algebraic cryptanalysis usually requires to recover the secret key by solving
polynomial equations. Grobner bases algorithm is a well-known method to solve
this problem. However, a serious drawback exists in the Grobner bases based
algebraic attacks, namely, any information won't be got if we couldn't work out
the Grobner bases of the polynomial equations system. In this paper, firstly, a
generalized model of Grobner basis algorithms is presented, which provides us a
platform to analyze and solve common problems of the algorithms. Secondly, we
give and prove the degree bound of the polynomials appeared during the
computation of Grobner basis after field polynomials is added. Finally, by
detecting the temporary basis during the computation of Grobner bases and then
extracting the univariate polynomials contained unique solution in the
temporary basis, a heuristic strategy named Middle-Solving is presented to
solve these polynomials at each iteration of the algorithm. Farther, two
specific application mode of Middle-Solving strategy for the incremental and
non-incremental Grobner bases algorithms are presented respectively. By using
the Middle-Solving strategy, even though we couldn't work out the final Grobner
bases, some information of the variables still leak during the computational
process.
"
800,"Algorithm for Solving Massively Underdefined Systems of Multivariate
  Quadratic Equations over Finite Fields","  Solving systems of m multivariate quadratic equations in n variables
(MQ-problem) over finite fields is NP-hard. The security of many cryptographic
systems is based on this problem. Up to now, the best algorithm for solving the
underdefined MQ-problem is Hiroyuki Miura et al.'s algorithm, which is a
polynomial-time algorithm when \[n \ge m(m + 3)/2\] and the characteristic of
the field is even. In order to get a wider applicable range, we reduce the
underdefined MQ-problem to the problem of finding square roots over finite
field, and then combine with the guess and determine method. In this way, the
applicable range is extended to \[n \ge m(m + 1)/2\], which is the widest range
until now. Theory analysis indicates that the complexity of our algorithm is
\[O(q{n^\omega }m{(\log {\kern 1pt} {\kern 1pt} q)^2}){\kern 1pt} \] when
characteristic of the field is even and \[O(q{2^m}{n^\omega }m{(\log {\kern
1pt} {\kern 1pt} q)^2})\] when characteristic of the field is odd, where \[2
\le \omega \le 3\] is the complexity of Gaussian elimination.
"
801,Open Weak CAD and its Applications,"  The concept of open weak CAD is introduced. Every open CAD is an open weak
CAD. On the contrary, an open weak CAD is not necessarily an open CAD. An
algorithm for computing projection polynomials of open weak CADs is proposed.
The key idea is to compute the intersection of projection factor sets produced
by different projection orders. The resulting open weak CAD often has smaller
number of sample points than open CADs. The algorithm can be used for computing
sample points for all open connected components of $ f\neq0$ for a given
polynomial $f$. It can also be used for many other applications, such as
testing semi-definiteness of polynomials and copositive problems. In fact, we
solved several difficult semi-definiteness problems efficiently by using the
algorithm. Furthermore, applying the algorithm to copositive problems, we find
an explicit expression of the polynomials producing open weak CADs under some
conditions, which significantly improves the efficiency of solving copositive
problems.
"
802,Formulas for Continued Fractions. An Automated Guess and Prove Approach,"  We describe a simple method that produces automatically closed forms for the
coefficients of continued fractions expansions of a large number of special
functions. The function is specified by a non-linear differential equation and
initial conditions. This is used to generate the first few coefficients and
from there a conjectured formula. This formula is then proved automatically
thanks to a linear recurrence satisfied by some remainder terms. Extensive
experiments show that this simple approach and its straightforward
generalization to difference and $q$-difference equations capture a large part
of the formulas in the literature on continued fractions.
"
803,Proof of the Wilf-Zeilberger Conjecture for Mixed Hypergeometric Terms,"  In 1992, Wilf and Zeilberger conjectured that a hypergeometric term in
several discrete and continuous variables is holonomic if and only if it is
proper. Strictly speaking the conjecture does not hold, but it is true when
reformulated properly: Payne proved a piecewise interpretation in 1997, and
independently, Abramov and Petkovsek in 2002 proved a conjugate interpretation.
Both results address the pure discrete case of the conjecture. In this paper we
extend their work to hypergeometric terms in several discrete and continuous
variables and prove the conjugate interpretation of the Wilf-Zeilberger
conjecture in this mixed setting.
"
804,Resultants and subresultants of p-adic polynomials,"  We address the problem of the stability of the computations of resultants and
subresultants of polynomials defined over complete discrete valuation rings
(e.g. Zp or k[[t]] where k is a field). We prove that Euclide-like algorithms
are highly unstable on average and we explain, in many cases, how one can
stabilize them without sacrifying the complexity. On the way, we completely
determine the distribution of the valuation of the principal subresultants of
two random monic p-adic polynomials having the same degree.
"
805,Duality of Multiple Root Loci,"  The multiple root loci among univariate polynomials of degree $n$ are indexed
by partitions of $n$. We study these loci and their conormal varieties. The
projectively dual varieties are joins of such loci where the partitions are
hooks. Our emphasis lies on equations and parametrizations that are useful for
Euclidean distance optimization. We compute the ED degrees for hooks. Among the
dual hypersurfaces are those that demarcate the set of binary forms whose real
rank equals the generic complex rank.
"
806,Exact algorithms for linear matrix inequalities,"  Let $A(x)=A\_0+x\_1A\_1+...+x\_nA\_n$ be a linear matrix, or pencil,
generated by given symmetric matrices $A\_0,A\_1,...,A\_n$ of size $m$ with
rational entries. The set of real vectors x such that the pencil is positive
semidefinite is a convex semi-algebraic set called spectrahedron, described by
a linear matrix inequality (LMI). We design an exact algorithm that, up to
genericity assumptions on the input matrices, computes an exact algebraic
representation of at least one point in the spectrahedron, or decides that it
is empty. The algorithm does not assume the existence of an interior point, and
the computed point minimizes the rank of the pencil on the spectrahedron. The
degree $d$ of the algebraic representation of the point coincides
experimentally with the algebraic degree of a generic semidefinite program
associated to the pencil. We provide explicit bounds for the complexity of our
algorithm, proving that the maximum number of arithmetic operations that are
performed is essentially quadratic in a multilinear B\'ezout bound of $d$. When
$m$ (resp. $n$) is fixed, such a bound, and hence the complexity, is polynomial
in $n$ (resp. $m$). We conclude by providing results of experiments showing
practical improvements with respect to state-of-the-art computer algebra
algorithms.
"
807,Computing characteristic classes of subschemes of smooth toric varieties,"  Let $X_{\Sigma}$ be a smooth complete toric variety defined by a fan $\Sigma$
and let $V=V(I)$ be a subscheme of $X_{\Sigma}$ defined by an ideal $I$
homogeneous with respect to the grading on the total coordinate ring of
$X_{\Sigma}$. We show a new expression for the Segre class $s(V,X_{\Sigma})$ in
terms of the projective degrees of a rational map specified by the generators
of $I$ when each generator corresponds to a numerically effective (nef)
divisor. Restricting to the case where $X_{\Sigma}$ is a smooth projective
toric variety and dehomogenizing the total homogeneous coordinate ring of
$X_{\Sigma}$ via a dehomogenizing ideal we also give an expression for the
projective degrees of this rational map in terms of the dimension of an
explicit quotient ring. Under an additional technical assumption we construct
what we call a general dehomogenizing ideal and apply this construction to give
effective algorithms to compute the Segre class $s(V,X_{\Sigma})$, the
Chern-Schwartz-MacPherson class $c_{SM}(V)$ and the topological Euler
characteristic $\chi(V)$ of $V$. These algorithms can, in particular, be used
for subschemes of any product of projective spaces $\mathbb{P}^{n_1} \times
\cdots \times \mathbb{P}^{n_j}$ or for subschemes of many other projective
toric varieties. Running time bounds for several of the algorithms are given
and the algorithms are tested on a variety of examples. In all applicable cases
our algorithms to compute these characteristic classes are found to offer
significantly increased performance over other known algorithms.
"
808,"Algebraic Local Cohomology with Parameters and Parametric Standard Bases
  for Zero-Dimensional Ideals","  A computation method of algebraic local cohomology with parameters,
associated with zero-dimensional ideal with parameter, is introduced. This
computation method gives us in particular a decomposition of the parameter
space depending on the structure of algebraic local cohomology classes. This
decomposition informs us several properties of input ideals and the output of
our algorithm completely describes the multiplicity structure of input ideals.
An efficient algorithm for computing a parametric standard basis of a given
zero-dimensional ideal, with respect to an arbitrary local term order, is also
described as an application of the computation method. The algorithm can always
output ""reduced"" standard basis of a given zero-dimensional ideal, even if the
zero-dimensional ideal has parameters.
"
809,Computing the Chow variety of quadratic space curves,"  Quadrics in the Grassmannian of lines in 3-space form a 19-dimensional
projective space. We study the subvariety of coisotropic hypersurfaces.
Following Gel'fand, Kapranov and Zelevinsky, it decomposes into Chow forms of
plane conics, Chow forms of pairs of lines, and Hurwitz forms of quadric
surfaces. We compute the ideals of these loci.
"
810,"Computing explicit isomorphisms with full matrix algebras over
  $\mathbb{F}_q(x)$","  We propose a polynomial time $f$-algorithm (a deterministic algorithm which
uses an oracle for factoring univariate polynomials over $\mathbb{F}_q$) for
computing an isomorphism (if there is any) of a finite dimensional
$\mathbb{F}_q(x)$-algebra $A$ given by structure constants with the algebra of
$n$ by $n$ matrices with entries from $\mathbb{F}_q(x)$. The method is based on
computing a finite $\mathbb{F}_q$-subalgebra of $A$ which is the intersection
of a maximal $\mathbb{F}_q[x]$-order and a maximal $R$-order, where $R$ is the
subring of $\mathbb{F}_q(x)$ consisting of fractions of polynomials with
denominator having degree not less than that of the numerator.
"
811,Lagrangian Constraints and Differential Thomas Decomposition,"  In this paper we show how to compute algorithmically the full set of
algebraically independent constraints for singular mechanical and
field-theoretical models with polynomial Lagrangians. If a model under
consideration is not singular as a whole but has domains of dynamical (field)
variables where its Lagrangian becomes singular, then our approach allows to
detect such domains and compute the relevant constraints. In doing so, we
assume that the Lagrangian of a model is a differential polynomial and apply
the differential Thomas decomposition algorithm to the Euler-Lagrange
equations.
"
812,Computing isolated orbifolds in weighted flag varieties,"  Given a weighted flag variety $w\Sigma(\mu,u)$ corresponding to chosen fixed
parameters $\mu$ and $u$, we present an algorithm to compute lists of all
possible projectively Gorenstein $n$-folds, having canonical weight $k$ and
isolated orbifold points, appearing as weighted complete intersections in
$w\Sigma(\mu,u) $ or some projective cone(s) over $w\Sigma(\mu,u)$. We apply
our algorithm to compute lists of interesting classes of polarized 3-folds with
isolated orbifold points in the codimension 8 weighted $G_2$ variety. We also
show the existence of some families of log-terminal $\mathbb Q$-Fano 3-folds in
codimension 8 by explicitly constructing them as quasilinear sections of a
weighted $G_2$-variety.
"
813,"A Near-Optimal Subdivision Algorithm for Complex Root Isolation based on
  the Pellet Test and Newton Iteration","  We describe a subdivision algorithm for isolating the complex roots of a
polynomial $F\in\mathbb{C}[x]$. Given an oracle that provides approximations of
each of the coefficients of $F$ to any absolute error bound and given an
arbitrary square $\mathcal{B}$ in the complex plane containing only simple
roots of $F$, our algorithm returns disjoint isolating disks for the roots of
$F$ in $\mathcal{B}$. Our complexity analysis bounds the absolute error to
which the coefficients of $F$ have to be provided, the total number of
iterations, and the overall bit complexity. It further shows that the
complexity of our algorithm is controlled by the geometry of the roots in a
near neighborhood of the input square $\mathcal{B}$, namely, the number of
roots, their absolute values and pairwise distances. The number of subdivision
steps is near-optimal. For the \emph{benchmark problem}, namely, to isolate all
the roots of a polynomial of degree $n$ with integer coefficients of bit size
less than $\tau$, our algorithm needs $\tilde O(n^3+n^2\tau)$ bit operations,
which is comparable to the record bound of Pan (2002). It is the first time
that such a bound has been achieved using subdivision methods, and independent
of divide-and-conquer techniques such as Sch\""onhage's splitting circle
technique. Our algorithm uses the quadtree construction of Weyl (1924) with two
key ingredients: using Pellet's Theorem (1881) combined with Graeffe iteration,
we derive a ""soft-test"" to count the number of roots in a disk. Using
Schr\""oder's modified Newton operator combined with bisection, in a form
inspired by the quadratic interval method from Abbot (2006), we achieve
quadratic convergence towards root clusters. Relative to the divide-conquer
algorithms, our algorithm is quite simple with the potential of being
practical. This paper is self-contained: we provide pseudo-code for all
subroutines used by our algorithm.
"
814,"Calculating Three Loop Ladder and V-Topologies for Massive Operator
  Matrix Elements by Computer Algebra","  Three loop ladder and $V$-topology diagrams contributing to the massive
operator matrix element $A_{Qg}$ are calculated. The corresponding objects can
all be expressed in terms of nested sums and recurrences depending on the
Mellin variable $N$ and the dimensional parameter $\varepsilon$. Given these
representations, the desired Laurent series expansions in $\varepsilon$ can be
obtained with the help of our computer algebra toolbox. Here we rely on
generalized hypergeometric functions and Mellin-Barnes representations, on
difference ring algorithms for symbolic summation, on an optimized version of
the multivariate Almkvist-Zeilberger algorithm for symbolic integration, and on
new methods to calculate Laurent series solutions of coupled systems of
differential equations. The solutions can be computed for general coefficient
matrices directly for any basis also performing the expansion in the
dimensional parameter in case it is expressible in terms of indefinite nested
product-sum expressions. This structural result is based on new results of our
difference ring theory. In the cases discussed we deal with iterative sum- and
integral-solutions over general alphabets. The final results are expressed in
terms of special sums, forming quasi-shuffle algebras, such as nested harmonic
sums, generalized harmonic sums, and nested binomially weighted (cyclotomic)
sums. Analytic continuations to complex values of $N$ are possible through the
recursion relations obeyed by these quantities and their analytic asymptotic
expansions. The latter lead to a host of new constants beyond the multiple zeta
values, the infinite generalized harmonic and cyclotomic sums in the case of
$V$-topologies.
"
815,On a Conjecture of Cusick Concerning the Sum of Digits of n and n + t,"  For a nonnegative integer $t$, let $c_t$ be the asymptotic density of natural
numbers $n$ for which $s(n + t) \geq s(n)$, where $s(n)$ denotes the sum of
digits of $n$ in base $2$. We prove that $c_t > 1/2$ for $t$ in a set of
asymptotic density $1$, thus giving a partial solution to a conjecture of T. W.
Cusick stating that $c_t > 1/2$ for all t. Interestingly, this problem has
several equivalent formulations, for example that the polynomial $X(X +
1)\cdots(X + t - 1)$ has less than $2^t$ zeros modulo $2^{t+1}$. The proof of
the main result is based on Chebyshev's inequality and the asymptotic analysis
of a trivariate rational function, using methods from analytic combinatorics.
"
816,Kempe's Universality Theorem for Rational Space Curves,"  We prove that every bounded rational space curve of degree d and circularity
c can be drawn by a linkage with 9/2 d - 6c + 1 revolute joints. Our proof is
based on two ingredients. The first one is the factorization theory of motion
polynomials. The second one is the construction of a motion polynomial of
minimum degree with given orbit. Our proof also gives the explicity
construction of the linkage.
"
817,Liaison Linkages,"  The complete classification of hexapods - also known as Stewart Gough
platforms - of mobility one is still open. To tackle this problem, we can
associate to each hexapod of mobility one an algebraic curve, called the
configuration curve. In this paper we establish an upper bound for the degree
of this curve, assuming the hexapod is general enough. Moreover, we provide a
construction of hexapods with curves of maximal degree, which is based on
liaison, a technique used in the theory of algebraic curves.
"
818,"Algebraic independence of sequences generated by (cyclotomic) harmonic
  sums","  An expression in terms of (cyclotomic) harmonic sums can be simplified by the
quasi-shuffle algebra in terms of the so-called basis sums. By construction,
these sums are algebraically independent within the quasi-shuffle algebra. In
this article we show that the basis sums can be represented within a tower of
difference ring extensions where the constants remain unchanged. This property
enables one to embed this difference ring for the (cyclotomic) harmonic sums
into the ring of sequences. This construction implies that the sequences
produced by the basis sums are algebraically independent over the rational
sequences adjoined with the alternating sequence.
"
819,Algebraic Diagonals and Walks,"  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
Generically, it is its minimal polynomial and is obtained in time quasi-linear
in its size. We show that this minimal polynomial has an exponential size with
respect to the degree of the input rational function. We then address the
related problem of enumerating directed lattice walks. The insight given by our
study leads to a new method for expanding the generating power series of
bridges, excursions and meanders. We show that their first N terms can be
computed in quasi-linear complexity in N, without first computing a very large
polynomial equation.
"
820,"Algebraic Diagonals and Walks: Algorithms, Bounds, Complexity","  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
We give a precise bound on the size of this polynomial and show that
generically, this polynomial is the minimal polynomial and that its size
reaches the bound. The algorithm runs in time quasi-linear in this bound, which
grows exponentially with the degree of the input rational function. We then
address the related problem of enumerating directed lattice walks. The insight
given by our study leads to a new method for expanding the generating power
series of bridges, excursions and meanders. We show that their first N terms
can be computed in quasi-linear complexity in N, without first computing a very
large polynomial equation.
"
821,On Termination of Polynomial Programs with Equality Conditions,"  We investigate the termination problem of a family of multi-path polynomial
programs (MPPs), in which all assignments to program variables are polynomials,
and test conditions of loops and conditional statements are polynomial
equalities. We show that the set of non-terminating inputs (NTI) of such a
program is algorithmically computable, thus leading to the decidability of its
termination. To the best of our knowledge, the considered family of MPPs is
hitherto the largest one for which termination is decidable. We present an
explicit recursive function which is essentially Ackermannian, to compute the
maximal length of ascending chains of polynomial ideals under a control
function, and thereby obtain a complete answer to the questions raised by
Seidenberg. This maximal length facilitates a precise complexity analysis of
our algorithms for computing the NTI and deciding termination of MPPs. We
extend our method to programs with polynomial guarded commands and show how an
incomplete procedure for MPPs with inequality guards can be obtained. An
application of our techniques to invariant generation of polynomial programs is
further presented.
"
822,"Sparse Polynomial Systems with many Positive Solutions from Bipartite
  Simplicial Complexes","  Consider a regular triangulation of the convex-hull $P$ of a set $\mathcal A$
of $n$ points in $\mathbb R^d$, and a real matrix $C$ of size $d \times n$. A
version of Viro's method allows to construct from these data an unmixed
polynomial system with support $\mathcal A$ and coefficient matrix $C$ whose
number of positive solutions is bounded from below by the number of
$d$-simplices which are positively decorated by $C$. We show that all the
$d$-simplices of a triangulation can be positively decorated if and only if the
triangulation is balanced, which in turn is equivalent to the fact that its
dual graph is bipartite. This allows us to identify, among classical families,
monomial supports which admit maximally positive systems, i.e. systems all
toric complex solutions of which are real and positive. These families give
some evidence in favor of a conjecture due to Bihan. We also use this technique
in order to construct fewnomial systems with many positive solutions. This is
done by considering a simplicial complex with bipartite dual graph included in
a regular triangulation of the cyclic polytope.
"
823,Multiple binomial sums,"  Multiple binomial sums form a large class of multi-indexed sequences, closed
under partial summation, which contains most of the sequences obtained by
multiple summation of products of binomial coefficients and also all the
sequences with algebraic generating function. We study the representation of
the generating functions of binomial sums by integrals of rational functions.
The outcome is twofold. Firstly, we show that a univariate sequence is a
multiple binomial sum if and only if its generating function is the diagonal of
a rational function. Secondly, we propose algorithms that decide the equality
of multiple binomial sums and that compute recurrence relations for them. In
conjunction with geometric simplifications of the integral representations,
this approach behaves well in practice. The process avoids the computation of
certificates and the problem of the appearance of spurious singularities that
afflicts discrete creative telescoping, both in theory and in practice.
"
824,"Formal Solutions of Completely Integrable Pfaffian Systems With Normal
  Crossings","  In this paper, we present an algorithm for computing a fundamental matrix of
formal solutions of completely integrable Pfaffian systems with normal
crossings in several variables. This algorithm is a generalization of a method
developed for the bivariate case based on a combination of several reduction
techniques and is implemented in the computer algebra system Maple.
"
825,Improved Polynomial Remainder Sequences for Ore Polynomials,"  Polynomial remainder sequences contain the intermediate results of the
Euclidean algorithm when applied to (non-)commutative polynomials. The running
time of the algorithm is dependent on the size of the coefficients of the
remainders. Different ways have been studied to make these as small as
possible. The subresultant sequence of two polynomials is a polynomial
remainder sequence in which the size of the coefficients is optimal in the
generic case, but when taking the input from applications, the coefficients are
often larger than necessary. We generalize two improvements of the subresultant
sequence to Ore polynomials and derive a new bound for the minimal coefficient
size. Our approach also yields a new proof for the results in the commutative
case, providing a new point of view on the origin of the extraneous factors of
the coefficients.
"
826,A general framework for Noetherian well ordered polynomial reductions,"  Polynomial reduction is one of the main tools in computational algebra with
innumerable applications in many areas, both pure and applied. Since many years
both the theory and an efficient design of the related algorithm have been
solidly established.
  This paper presents a general definition of polynomial reduction structure,
studies its features and highlights the aspects needed in order to grant and to
efficiently test the main properties (noetherianity, confluence, ideal
membership).
  The most significant aspect of this analysis is a negative reappraisal of the
role of the notion of term order which is usually considered a central and
crucial tool in the theory. In fact, as it was already established in the
computer science context in relation with termination of algorithms, most of
the properties can be obtained simply considering a well-founded ordering,
while the classical requirement that it be preserved by multiplication is
irrelevant.
  The last part of the paper shows how the polynomial basis concepts present in
literature are interpreted in our language and their properties are
consequences of the general results established in the first part of the paper.
"
827,"Solving the Forward Position Problem of an In-Parallel Planar
  Manipulator in the Gauss Plane","  We study determining the posture of an in-parallel planar manipulator, which
has three connectors composed of revolute, prismatic and revolute joints, from
specified active joint variables. We construct an ideal in the field of complex
numbers, and we introduce self inversive polynomials. We provide results for an
in-parallel planar manipulator, which has a base and moving platform in right
triangular shape. Using Sage computer algebra system, we compute its Groebner
bases. We illustrate that the single variable polynomials obtained from the
Groebner bases are self reciprocal.
"
828,"On the robust hardness of Gr\""obner basis computation","  The computation of Gr\""obner bases is an established hard problem. By
contrast with many other problems, however, there has been little investigation
of whether this hardness is robust. In this paper, we frame and present results
on the problem of approximate computation of Gr\""obner bases. We show that it
is NP-hard to construct a Gr\""obner basis of the ideal generated by a set of
polynomials, even when the algorithm is allowed to discard a $(1 - \epsilon)$
fraction of the generators, and likewise when the algorithm is allowed to
discard variables (and the generators containing them). Our results shows that
computation of Gr\""obner bases is robustly hard even for simple polynomial
systems (e.g. maximum degree 2, with at most 3 variables per generator). We
conclude by greatly strengthening results for the Strong $c$-Partial Gr\""obner
problem posed by De Loera et al. Our proofs also establish interesting
connections between the robust hardness of Gr\""obner bases and that of SAT
variants and graph-coloring.
"
829,"Visualization in teaching and learning mathematics in elementary,
  secondary and higher education","  In this paper we present our experience in using visualization in mathematics
education. The experience with our university courses: ""Computer tools in
matematics"" and ""Symbolic algebra"" provides the basis for mathematics teacher
education program http://vizuelizacija.etf.rs/. The program is intended for
elementary and high school teachers. The education program deals with modern
techniques of visualization by using technologies such as GeoGegebra, JAVA and
HTML.
"
830,Contraction of Ore Ideals with Applications,"  Ore operators form a common algebraic abstraction of linear ordinary
differential and recurrence equations. Given an Ore operator $L$ with
polynomial coefficients in $x$, it generates a left ideal $I$ in the Ore
algebra over the field $\mathbf{k}(x)$ of rational functions. We present an
algorithm for computing a basis of the contraction ideal of $I$ in the Ore
algebra over the ring $R[x]$ of polynomials, where $R$ may be either
$\mathbf{k}$ or a domain with $\mathbf{k}$ as its fraction field. This
algorithm is based on recent work on desingularization for Ore operators by
Chen, Jaroschek, Kauers and Singer. Using a basis of the contraction ideal, we
compute a completely desingularized operator for $L$ whose leading coefficient
not only has minimal degree in $x$ but also has minimal content. Completely
desingularized operators have interesting applications such as certifying
integer sequences and checking special cases of a conjecture of Krattenthaler.
"
831,Similarity detection of rational space curves,"  We provide an algorithm to check whether two rational space curves are
related by a similarity. The algorithm exploits the relationship between the
curvatures and torsions of two similar curves, which is formulated in a
computer algebra setting. Helical curves, where curvature and torsion are
proportional, need to be distinguished as a special case. The algorithm is easy
to implement, as it involves only standard computer algebra techniques, such as
greatest common divisors and resultants, and Gr\""obner basis for the special
case of helical curves. Details on the implementation and experimentation
carried out using the computer algebra system Maple 18 are provided.
"
832,"An Extension of Moebius--Lie Geometry with Conformal Ensembles of Cycles
  and Its Implementation in a GiNaC Library","  We propose to consider ensembles of cycles (quadrics), which are
interconnected through conformal-invariant geometric relations (e.g. ""to be
orthogonal"", ""to be tangent"", etc.), as new objects in an extended Moebius--Lie
geometry. It was recently demonstrated in several related papers, that such
ensembles of cycles naturally parameterise many other conformally-invariant
objects, e.g. loxodromes or continued fractions. The paper describes a method,
which reduces a collection of conformally invariant geometric relations to a
system of linear equations, which may be accompanied by one fixed quadratic
relation.
  To show its usefulness, the method is implemented as a C++ library. It
operates with numeric and symbolic data of cycles in spaces of arbitrary
dimensionality and metrics with any signatures. Numeric calculations can be
done in exact or approximate arithmetic. In the two- and three-dimensional
cases illustrations and animations can be produced. An interactive Python
wrapper of the library is provided as well.
"
833,Computing minimal interpolation bases,"  We consider the problem of computing univariate polynomial matrices over a
field that represent minimal solution bases for a general interpolation
problem, some forms of which are the vector M-Pad\'e approximation problem in
[Van Barel and Bultheel, Numerical Algorithms 3, 1992] and the rational
interpolation problem in [Beckermann and Labahn, SIAM J. Matrix Anal. Appl. 22,
2000]. Particular instances of this problem include the bivariate interpolation
steps of Guruswami-Sudan hard-decision and K\""otter-Vardy soft-decision
decodings of Reed-Solomon codes, the multivariate interpolation step of
list-decoding of folded Reed-Solomon codes, and Hermite-Pad\'e approximation.
  In the mentioned references, the problem is solved using iterative algorithms
based on recurrence relations. Here, we discuss a fast, divide-and-conquer
version of this recurrence, taking advantage of fast matrix computations over
the scalars and over the polynomials. This new algorithm is deterministic, and
for computing shifted minimal bases of relations between $m$ vectors of size
$\sigma$ it uses $O~( m^{\omega-1} (\sigma + |s|) )$ field operations, where
$\omega$ is the exponent of matrix multiplication, and $|s|$ is the sum of the
entries of the input shift $s$, with $\min(s) = 0$. This complexity bound
improves in particular on earlier algorithms in the case of bivariate
interpolation for soft decoding, while matching fastest existing algorithms for
simultaneous Hermite-Pad\'e approximation.
"
834,"A Probabilistic Algorithm for Computing Data-Discriminants of Likelihood
  Equations","  An algebraic approach to the maximum likelihood estimation problem is to
solve a very structured parameterized polynomial system called likelihood
equations that have finitely many complex (real or non-real) solutions. The
only solutions that are statistically meaningful are the real solutions with
positive coordinates. In order to classify the parameters (data) according to
the number of real/positive solutions, we study how to efficiently compute the
discriminants, say data-discriminants (DD), of the likelihood equations. We
develop a probabilistic algorithm with three different strategies for computing
DDs. Our implemented probabilistic algorithm based on Maple and FGb is more
efficient than our previous version presented in ISSAC2015, and is also more
efficient than the standard elimination for larger benchmarks. By applying
RAGlib to a DD we compute, we give the real root classification of 3 by 3
symmetric matrix model.
"
835,"Fast Operations on Linearized Polynomials and their Applications in
  Coding Theory","  This paper considers fast algorithms for operations on linearized
polynomials. We propose a new multiplication algorithm for skew polynomials (a
generalization of linearized polynomials) which has sub-quadratic complexity in
the polynomial degree $s$, independent of the underlying field extension
degree~$m$. We show that our multiplication algorithm is faster than all known
ones when $s \leq m$. Using a result by Caruso and Le Borgne (2017), this
immediately implies a sub-quadratic division algorithm for linearized
polynomials for arbitrary polynomial degree $s$. Also, we propose algorithms
with sub-quadratic complexity for the $q$-transform, multi-point evaluation,
computing minimal subspace polynomials, and interpolation, whose
implementations were at least quadratic before. Using the new fast algorithm
for the $q$-transform, we show how matrix multiplication over a finite field
can be implemented by multiplying linearized polynomials of degrees at most
$s=m$ if an elliptic normal basis of extension degree $m$ exists, providing a
lower bound on the cost of the latter problem. Finally, it is shown how the new
fast operations on linearized polynomials lead to the first error and erasure
decoding algorithm for Gabidulin codes with sub-quadratic complexity.
"
836,Computing Chebyshev knot diagrams,"  A Chebyshev curve $\mathcal{C}(a,b,c,\phi)$ has a parametrization of the
form$ x(t)=T\_a(t)$; \ $y(t)=T\_b(t)$; $z(t)= T\_c(t + \phi)$, where $a,b,c$are
integers, $T\_n(t)$ is the Chebyshev polynomialof degree $n$ and $\phi \in
\mathbb{R}$. When $\mathcal{C}(a,b,c,\phi)$ is nonsingular,it defines a
polynomial knot. We determine all possible knot diagrams when $\phi$ varies.
Let $a,b,c$ be integers, $a$ is odd, $(a,b)=1$, we show that one can list all
possible knots $\mathcal{C}(a,b,c,\phi)$ in$\tilde{\mathcal{O}}(n^2)$ bit
operations, with $n=abc$.
"
837,The Module Isomorphism Problem for Finite Rings and Related Results,"  Let $R$ be a finite ring and let $M, N$ be two finite left $R$-modules. We
present two distinct deterministic algorithms that decide in polynomial time
whether or not $M$ and $N$ are isomorphic, and if they are, exhibit an
isomorphism. As by-products, we are able to determine the largest isomorphic
common direct summand between two modules and the minimum number of generators
of a module. By not requiring $R$ to contain a field, avoiding computation of
the Jacobson radical and not distinguishing between large and small
characteristic, both algorithms constitute improvements to known results.
"
838,"A Modular Algorithm for Computing Polynomial GCDs over Number Fields
  presented with Multiple Extensions","  We consider the problem of computing the monic gcd of two polynomials over a
number field L = Q(alpha_1,...,alpha_n). Langemyr and McCallum have already
shown how Brown's modular GCD algorithm for polynomials over Q can be modified
to work for Q(alpha) and subsequently, Langemyr extended the algorithm to L[x].
Encarnacion also showed how to use rational number to make the algorithm for
Q(alpha) output sensitive, that is, the number of primes used depends on the
size of the integers in the gcd and not on bounds based on the input
polynomials.
  Our first contribution is an extension of Encarnacion's modular GCD algorithm
to the case n>1, which, like Encarnacion's algorithm, is is output sensitive.
  Our second contribution is a proof that it is not necessary to test if p
divides the discriminant. This simplifies the algorithm; it is correct without
this test.
  Our third contribution is a modification to the algorithm to treat the case
of reducible extensions. Such cases arise when solving systems of polynomial
equations.
  Our fourth contribution is an implementation of the modular GCD algorithm in
Maple and in Magma. Both implementations use a recursive dense polynomial data
structure for representing polynomials over number fields with multiple field
extensions.
  Our fifth contribution is a primitive fraction-free algorithm. This is the
best non-modular approach. We present timing comparisons of the Maple and Magma
implementations demonstrating various optimizations and comparing them with the
monic Euclidan algorithm and our primitive fraction-free algorithm.
"
839,"A Polynomial-time Algorithm to Compute Generalized Hermite Normal Form
  of Matrices over Z[x]","  In this paper, a polynomial-time algorithm is given to compute the
generalized Hermite normal form for a matrix F over Z[x], or equivalently, the
reduced Groebner basis of the Z[x]-module generated by the column vectors of F.
The algorithm is also shown to be practically more efficient than existing
algorithms. The algorithm is based on three key ingredients. First, an F4 style
algorithm to compute the Groebner basis is adopted, where a novel prolongation
is designed such that the coefficient matrices under consideration have
polynomial sizes. Second, fast algorithms to compute Hermite normal forms of
matrices over Z are used. Third, the complexity of the algorithm are guaranteed
by a nice estimation for the degree and height bounds of the polynomials in the
generalized Hermite normal form.
"
840,Encoding and Decoding Algorithms for Arbitrary Dimensional Hilbert Order,"  Hilbert order is widely applied in many areas. However, most of the
algorithms are confined to low dimensional cases. In this paper, algorithms for
encoding and decoding arbitrary dimensional Hilbert order are presented. Eight
algorithms are proposed. Four algorithms are based on arithmetic operations and
the other four algorithms are based on bit operations. For the algorithms
complexities, four of them are linear and the other four are constant for given
inputs. In the end of the paper, algorithms for two dimensional Hilbert order
are presented to demonstrate the usage of the algorithms introduced.
"
841,"Fast Computation of the Rank Profile Matrix and the Generalized Bruhat
  Decomposition","  The row (resp. column) rank profile of a matrix describes the stair-case
shape of its row (resp. column) echelon form. We here propose a new matrix
invariant, the rank profile matrix, summarizing all information on the row and
column rank profiles of all the leading sub-matrices. We show that this normal
form exists and is unique over any ring, provided that the notion of McCoy's
rank is used, in the presence of zero divisors. We then explore the conditions
for a Gaussian elimination algorithm to compute all or part of this invariant,
through the corresponding PLUQ decomposition. This enlarges the set of known
Elimination variants that compute row or column rank profiles. As a consequence
a new Crout base case variant significantly improves the practical efficiency
of previously known implementations over a finite field. With matrices of very
small rank, we also generalize the techniques of Storjohann and Yang to the
computation of the rank profile matrix, achieving an $(r^\omega+mn)^{1+o(1)}$
time complexity for an $m \times n$ matrix of rank $r$, where $\omega$ is the
exponent of matrix multiplication. Finally, by give connections to the Bruhat
decomposition, and several of its variants and generalizations. Thus, our
algorithmic improvements for the PLUQ factorization, and their implementations,
directly apply to these decompositions. In particular, we show how a PLUQ
decomposition revealing the rank profile matrix also reveals both a row and a
column echelon form of the input matrix or of any of its leading sub-matrices,
by a simple post-processing made of row and column permutations.
"
842,"A toolbox to solve coupled systems of differential and difference
  equations","  We present algorithms to solve coupled systems of linear differential
equations, arising in the calculation of massive Feynman diagrams with local
operator insertions at 3-loop order, which do {\it not} request special choices
of bases. Here we assume that the desired solution has a power series
representation and we seek for the coefficients in closed form. In particular,
if the coefficients depend on a small parameter $\ep$ (the dimensional
parameter), we assume that the coefficients themselves can be expanded in
formal Laurent series w.r.t.\ $\ep$ and we try to compute the first terms in
closed form. More precisely, we have a decision algorithm which solves the
following problem: if the terms can be represented by an indefinite nested
hypergeometric sum expression (covering as special cases the harmonic sums,
cyclotomic sums, generalized harmonic sums or nested binomial sums), then we
can calculate them. If the algorithm fails, we obtain a proof that the terms
cannot be represented by the class of indefinite nested hypergeometric sum
expressions. Internally, this problem is reduced by holonomic closure
properties to solving a coupled system of linear difference equations. The
underlying method in this setting relies on decoupling algorithms, difference
ring algorithms and recurrence solving. We demonstrate by a concrete example
how this algorithm can be applied with the new Mathematica package
\texttt{SolveCoupledSystem} which is based on the packages \texttt{Sigma},
\texttt{HarmonicSums} and \texttt{OreSys}. In all applications the
representation in $x$-space is obtained as an iterated integral representation
over general alphabets, generalizing Poincar\'{e} iterated integrals.
"
843,Factorization of C-finite Sequences,"  We discuss how to decide whether a given C-finite sequence can be written
nontrivially as a product of two other C-finite sequences.
"
844,Existence Problem of Telescopers: Beyond the Bivariate Case,"  In this paper, we solve the existence problem of telescopers for rational
functions in three discrete variables. We reduce the problem to that of
deciding the summability of bivariate rational functions, which has been solved
recently. The existence criteria we present is needed for detecting the
termination of Zeilberger's algorithm to the function classes studied in this
paper.
"
845,Descartes' Rule of Signs for Polynomial Systems supported on Circuits,"  We give a multivariate version of Descartes' rule of signs to bound the
number of positive real roots of a system of polynomial equations in n
variables with n+2 monomials, in terms of the sign variation of a sequence
associated both to the exponent vectors and the given coefficients. We show
that our bound is sharp and is related to the signature of the circuit.
"
846,"Computing the decomposition group of a zero-dimensional ideal by
  elimination method","  In this note, we show that the decomposition group $Dec(I)$ of a
zero-dimensional radical ideal $I$ in ${\bf K}[x_1,\ldots,x_n]$ can be
represented as the direct sum of several symmetric groups of polynomials based
upon using Gr\""{o}bner bases. The new method makes a theoretical contribution
to discuss the decomposition group of $I$ by using Computer Algebra without
considering the complexity. As one application, we also present an approach to
yield new triangular sets in computing triangular decomposition of polynomial
sets ${\mathbb P}$ if $Dec(<{\mathbb P}>)$ is known.
"
847,On p-adic differential equations with separation of variables,"  Several algorithms in computer algebra involve the computation of a power
series solution of a given ordinary differential equation. Over finite fields,
the problem is often lifted in an approximate $p$-adic setting to be
well-posed. This raises precision concerns: how much precision do we need on
the input to compute the output accurately? In the case of ordinary
differential equations with separation of variables, we make use of the recent
technique of differential precision to obtain optimal bounds on the stability
of the Newton iteration. The results apply, for example, to algorithms for
manipulating algebraic numbers over finite fields, for computing isogenies
between elliptic curves or for deterministically finding roots of polynomials
in finite fields. The new bounds lead to significant speedups in practice.
"
848,A Factorization Algorithm for G-Algebras and Applications,"  It has been recently discovered by Bell, Heinle and Levandovskyy that a large
class of algebras, including the ubiquitous $G$-algebras, are finite
factorization domains (FFD for short).
  Utilizing this result, we contribute an algorithm to find all distinct
factorizations of a given element $f \in \mathcal{G}$, where $\mathcal{G}$ is
any $G$-algebra, with minor assumptions on the underlying field.
  Moreover, the property of being an FFD, in combination with the factorization
algorithm, enables us to propose an analogous description of the factorized
Gr\""obner basis algorithm for $G$-algebras. This algorithm is useful for
various applications, e.g. in analysis of solution spaces of systems of linear
partial functional equations with polynomial coefficients, coming from
$\mathcal{G}$. Additionally, it is possible to include inequality constraints
for ideals in the input.
"
849,Reduction-Based Creative Telescoping for Algebraic Functions,"  Continuing a series of articles in the past few years on creative telescoping
using reductions, we develop a new algorithm to construct minimal telescopers
for algebraic functions. This algorithm is based on Trager's Hermite reduction
and on polynomial reduction, which was originally designed for hyperexponential
functions and extended to the algebraic case in this paper.
"
850,Solving rank-constrained semidefinite programs in exact arithmetic,"  We consider the problem of minimizing a linear function over an affine
section of the cone of positive semidefinite matrices, with the additional
constraint that the feasible matrix has prescribed rank. When the rank
constraint is active, this is a non-convex optimization problem, otherwise it
is a semidefinite program. Both find numerous applications especially in
systems control theory and combinatorial optimization, but even in more general
contexts such as polynomial optimization or real algebra. While numerical
algorithms exist for solving this problem, such as interior-point or
Newton-like algorithms, in this paper we propose an approach based on symbolic
computation. We design an exact algorithm for solving rank-constrained
semidefinite programs, whose complexity is essentially quadratic on natural
degree bounds associated to the given optimization problem: for subfamilies of
the problem where the size of the feasible matrix is fixed, the complexity is
polynomial in the number of variables. The algorithm works under assumptions on
the input data: we prove that these assumptions are generically satisfied. We
also implement it in Maple and discuss practical experiments.
"
851,Holonomic Tools for Basic Hypergeometric Functions,"  With the exception of q-hypergeometric summation, the use of computer algebra
packages implementing Zeilberger's ""holonomic systems approach"" in a broader
mathematical sense is less common in the field of q-series and basic
hypergeometric functions. A major objective of this article is to popularize
the usage of such tools also in these domains. Concrete case studies showing
software in action introduce to the basic techniques. An application highlight
is a new computer-assisted proof of the celebrated Ismail-Zhang formula, an
important q-analog of a classical expansion formula of plane waves in terms of
Gegenbauer polynomials.
"
852,"Fast Computation of the Nth Term of an Algebraic Series over a Finite
  Prime Field","  We address the question of computing one selected term of an algebraic power
series. In characteristic zero, the best algorithm currently known for
computing the $N$th coefficient of an algebraic series uses differential
equations and has arithmetic complexity quasi-linear in $\sqrt{N}$. We show
that over a prime field of positive characteristic $p$, the complexity can be
lowered to $O(\log N)$. The mathematical basis for this dramatic improvement is
a classical theorem stating that a formal power series with coefficients in a
finite field is algebraic if and only if the sequence of its coefficients can
be generated by an automaton. We revisit and enhance two constructive proofs of
this result for finite prime fields. The first proof uses Mahler equations,
whose sizes appear to be prohibitively large. The second proof relies on
diagonals of rational functions; we turn it into an efficient algorithm, of
complexity linear in $\log N$ and quasi-linear in $p$.
"
853,"Fast Computation of Minimal Interpolation Bases in Popov Form for
  Arbitrary Shifts","  We compute minimal bases of solutions for a general interpolation problem,
which encompasses Hermite-Pad\'e approximation and constrained multivariate
interpolation, and has applications in coding theory and security.
  This problem asks to find univariate polynomial relations between $m$ vectors
of size $\sigma$; these relations should have small degree with respect to an
input degree shift. For an arbitrary shift, we propose an algorithm for the
computation of an interpolation basis in shifted Popov normal form with a cost
of $\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ field operations, where $\omega$
is the exponent of matrix multiplication and the notation
$\mathcal{O}\tilde{~}(\cdot)$ indicates that logarithmic terms are omitted.
  Earlier works, in the case of Hermite-Pad\'e approximation and in the general
interpolation case, compute non-normalized bases. Since for arbitrary shifts
such bases may have size $\Theta(m^2 \sigma)$, the cost bound
$\mathcal{O}\tilde{~}(m^{\omega-1} \sigma)$ was feasible only with restrictive
assumptions on the shift that ensure small output sizes. The question of
handling arbitrary shifts with the same complexity bound was left open.
  To obtain the target cost for any shift, we strengthen the properties of the
output bases, and of those obtained during the course of the algorithm: all the
bases are computed in shifted Popov form, whose size is always $\mathcal{O}(m
\sigma)$. Then, we design a divide-and-conquer scheme. We recursively reduce
the initial interpolation problem to sub-problems with more convenient shifts
by first computing information on the degrees of the intermediate bases.
"
854,"Numerically validating the completeness of the real solution set of a
  system of polynomial equations","  Computing the real solutions to a system of polynomial equations is a
challenging problem, particularly verifying that all solutions have been
computed. We describe an approach that combines numerical algebraic geometry
and sums of squares programming to test whether a given set is ""complete"" with
respect to the real solution set. Specifically, we test whether the Zariski
closure of that given set is indeed equal to the solution set of the real
radical of the ideal generated by the given polynomials. Examples with finitely
and infinitely many real solutions are provided, along with an example having
polynomial inequalities.
"
855,"Fast Computation of Shifted Popov Forms of Polynomial Matrices via
  Systems of Modular Polynomial Equations","  We give a Las Vegas algorithm which computes the shifted Popov form of an $m
\times m$ nonsingular polynomial matrix of degree $d$ in expected
$\widetilde{\mathcal{O}}(m^\omega d)$ field operations, where $\omega$ is the
exponent of matrix multiplication and $\widetilde{\mathcal{O}}(\cdot)$
indicates that logarithmic factors are omitted. This is the first algorithm in
$\widetilde{\mathcal{O}}(m^\omega d)$ for shifted row reduction with arbitrary
shifts.
  Using partial linearization, we reduce the problem to the case $d \le \lceil
\sigma/m \rceil$ where $\sigma$ is the generic determinant bound, with $\sigma
/ m$ bounded from above by both the average row degree and the average column
degree of the matrix. The cost above becomes $\widetilde{\mathcal{O}}(m^\omega
\lceil \sigma/m \rceil)$, improving upon the cost of the fastest previously
known algorithm for row reduction, which is deterministic.
  Our algorithm first builds a system of modular equations whose solution set
is the row space of the input matrix, and then finds the basis in shifted Popov
form of this set. We give a deterministic algorithm for this second step
supporting arbitrary moduli in $\widetilde{\mathcal{O}}(m^{\omega-1} \sigma)$
field operations, where $m$ is the number of unknowns and $\sigma$ is the sum
of the degrees of the moduli. This extends previous results with the same cost
bound in the specific cases of order basis computation and M-Pad\'e
approximation, in which the moduli are products of known linear factors.
"
856,"Linear Time Interactive Certificates for the Minimal Polynomial and the
  Determinant of a Sparse Matrix","  Computational problem certificates are additional data structures for each
output, which can be used by a-possibly randomized-verification algorithm that
proves the correctness of each output. In this paper, we give an algorithm that
computes a certificate for the minimal polynomial of sparse or structured nxn
matrices over an abstract field, of sufficiently large cardinality, whose Monte
Carlo verification complexity requires a single matrix-vector multiplication
and a linear number of extra field operations. We also propose a novel
preconditioner that ensures irreducibility of the characteristic polynomial of
the generically preconditioned matrix. This preconditioner takes linear time to
be applied and uses only two random entries. We then combine these two
techniques to give algorithms that compute certificates for the determinant,
and thus for the characteristic polynomial, whose Monte Carlo verification
complexity is therefore also linear.
"
857,Algorithms for Simultaneous Pad\'e Approximations,"  We describe how to solve simultaneous Pad\'e approximations over a power
series ring $K[[x]]$ for a field $K$ using $O~(n^{\omega - 1} d)$ operations in
$K$, where $d$ is the sought precision and $n$ is the number of power series to
approximate. We develop two algorithms using different approaches. Both
algorithms return a reduced sub-bases that generates the complete set of
solutions to the input approximations problem that satisfy the given degree
constraints. Our results are made possible by recent breakthroughs in fast
computations of minimal approximant bases and Hermite Pad\'e approximations.
"
858,On the p-adic stability of the FGLM algorithm,"  Nowadays, many strategies to solve polynomial systems use the computation of
a Gr{\""o}bner basis for the graded reverse lexicographical ordering, followed
by a change of ordering algorithm to obtain a Gr{\""o}bner basis for the
lexicographical ordering. The change of ordering algorithm is crucial for these
strategies. We study the p-adic stability of the main change of ordering
algorithm, FGLM. We show that FGLM is stable and give explicit upper bound on
the loss of precision occuring in its execution. The variant of FGLM designed
to pass from the grevlex ordering to a Gr{\""o}bner basis in shape position is
also stable. Our study relies on the application of Smith Normal Form
computations for linear algebra.
"
859,"HYPERgeometric functions DIfferential REduction: Mathematica-based
  packages for the differential reduction of generalizedhypergeometric
  functions: Fc hypergeometric function of three variables","  We present a further extension of the HYPERDIRE project, which is devoted to
the creation of a set of Mathematica-based program packages for manipulations
with Horn-type hypergeometric functions on the basis of differential equations.
Specifically, we present the implementation of the differential reduction for
the Lauricella function $F_C$ of three variables.
"
860,Computing with quasiseparable matrices,"  The class of quasiseparable matrices is defined by a pair of bounds, called
the quasiseparable orders, on the ranks of the maximal sub-matrices entirely
located in their strictly lower and upper triangular parts. These arise
naturally in applications, as e.g. the inverse of band matrices, and are widely
used for they admit structured representations allowing to compute with them in
time linear in the dimension and quadratic with the quasiseparable order. We
show, in this paper, the connection between the notion of quasisepa-rability
and the rank profile matrix invariant, presented in [Dumas \& al. ISSAC'15].
This allows us to propose an algorithm computing the quasiseparable orders (rL,
rU) in time O(n^2 s^($\omega$--2)) where s = max(rL, rU) and $\omega$ the
exponent of matrix multiplication. We then present two new structured
representations, a binary tree of PLUQ decompositions, and the Bruhat
generator, using respectively O(ns log n/s) and O(ns) field elements instead of
O(ns^2) for the previously known generators. We present algorithms computing
these representations in time O(n^2 s^($\omega$--2)). These representations
allow a matrix-vector product in time linear in the size of their
representation. Lastly we show how to multiply two such structured matrices in
time O(n^2 s^($\omega$--2)).
"
861,Division and Slope Factorization of p-Adic Polynomials,"  We study two important operations on polynomials defined over complete
discrete valuation fields: Euclidean division and factorization. In particular,
we design a simple and efficient algorithm for computing slope factorizations,
based on Newton iteration. One of its main features is that we avoid working
with fractional exponents. We pay particular attention to stability, and
analyze the behavior of the algorithm using several precision models.
"
862,Inverse Inequality Estimates with Symbolic Computation,"  In the convergence analysis of numerical methods for solving partial
differential equations (such as finite element methods) one arrives at certain
generalized eigenvalue problems, whose maximal eigenvalues need to be estimated
as accurately as possible. We apply symbolic computation methods to the
situation of square elements and are able to improve the previously known upper
bound, given in ""p- and hp-finite element methods"" (Schwab, 1998), by a factor
of 8. More precisely, we try to evaluate the corresponding determinant using
the holonomic ansatz, which is a powerful tool for dealing with determinants,
proposed by Zeilberger in 2007. However, it turns out that this method does not
succeed on the problem at hand. As a solution we present a variation of the
original holonomic ansatz that is applicable to a larger class of determinants,
including the one we are dealing with here. We obtain an explicit closed form
for the determinant, whose special form enables us to derive new and tight
upper resp. lower bounds on the maximal eigenvalue, as well as its asymptotic
behaviour.
"
863,"A fast, deterministic algorithm for computing a Hermite Normal Form of a
  polynomial matrix","  Given a square, nonsingular matrix of univariate polynomials $\mathbf{F} \in
\mathbb{K}[x]^{n \times n}$ over a field $\mathbb{K}$, we give a fast,
deterministic algorithm for finding the Hermite normal form of $\mathbf{F}$
with complexity $O^{\sim}\left(n^{\omega}d\right)$ where $d$ is the degree of
$\mathbf{F}$. Here soft-$O$ notation is Big-$O$ with log factors removed and
$\omega$ is the exponent of matrix multiplication. The method relies of a fast
algorithm for determining the diagonal entries of its Hermite normal form,
having as cost $O^{\sim}\left(n^{\omega}s\right)$ operations with $s$ the
average of the column degrees of $\mathbf{F}$.
"
864,The algebra of Kleene stars of the plane and polylogarithms,"  We extend the definition and study the algebraic properties of the
polylogarithm Li(T), where T is rational series over the alphabet X = {x 0, x
1} belonging to suitable subalgebras of rational series.
"
865,"On Gr\""obner Bases and Krull Dimension of Residue Class Rings of
  Polynomial Rings over Integral Domains","  Given an ideal $\mathfrak{a}$ in $A[x_1, \ldots, x_n]$, where $A$ is a
Noetherian integral domain, we propose an approach to compute the Krull
dimension of $A[x_1,\ldots,x_n]/\mathfrak{a}$, when the residue class
polynomial ring is a free $A$-module. When $A$ is a field, the Krull dimension
of $A[x_1,\ldots,x_n]/\mathfrak{a}$ has several equivalent algorithmic
definitions by which it can be computed. But this is not true in the case of
arbitrary Noetherian rings. For a Noetherian integral domain, $A$ we introduce
the notion of combinatorial dimension of $A[x_1, \ldots,x_n]/\mathfrak{a}$ and
give a Gr\""obner basis method to compute it for residue class polynomial rings
that have a free $A$-module representation w.r.t. a lexicographic ordering. For
such $A$-algebras, we derive a relation between Krull dimension and
combinatorial dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$. An immediate
application of this relation is that it gives a uniform method, the first of
its kind, to compute the dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$
without having to consider individual properties of the ideal. For $A$-algebras
that have a free $A$-module representation w.r.t. degree compatible monomial
orderings, we introduce the concepts of Hilbert function, Hilbert series and
Hilbert polynomials and show that Gr\""obner basis methods can be used to
compute these quantities. We then proceed to show that the combinatorial
dimension of such $A$-algebras is equal to the degree of the Hilbert
polynomial. This enables us to extend the relation between Krull dimension and
combinatorial dimension to $A$-algebras with a free $A$-module representation
w.r.t. a degree compatible ordering as well.
"
866,Mathematical Theory Exploration in Theorema: Reduction Rings,"  In this paper we present the first-ever computer formalization of the theory
of Gr\""obner bases in reduction rings, which is an important theory in
computational commutative algebra, in Theorema. Not only the formalization, but
also the formal verification of all results has already been fully completed by
now; this, in particular, includes the generic implementation and correctness
proof of Buchberger's algorithm in reduction rings. Thanks to the seamless
integration of proving and computing in Theorema, this implementation can now
be used to compute Gr\""obner bases in various different domains directly within
the system. Moreover, a substantial part of our formalization is made up solely
by ""elementary theories"" such as sets, numbers and tuples that are themselves
independent of reduction rings and may therefore be used as the foundations of
future theory explorations in Theorema.
  In addition, we also report on two general-purpose Theorema tools we
developed for an efficient and convenient exploration of mathematical theories:
an interactive proving strategy and a ""theory analyzer"" that already proved
extremely useful when creating large structured knowledge bases.
"
867,An Illustrated Introduction to the Truncated Fourier Transform,"  The Truncated Fourier Transform (TFT) is a variation of the Discrete Fourier
Transform (DFT/FFT) that allows for input vectors that do NOT have length $2^n$
for $n$ a positive integer. We present the univariate version of the TFT,
originally due to Joris van der Hoeven, heavily illustrating the presentation
in order to make these methods accessible to a broader audience.
"
868,"GBLA -- Gr\""obner Basis Linear Algebra Package","  This is a system paper about a new GPLv2 open source C library GBLA
implementing and improving the idea of Faug\`ere and Lachartre (GB reduction).
We further exploit underlying structures in matrices generated during Gr\""obner
basis computations in algorithms like F4 or F5 taking advantage of block
patterns by using a special data structure called multilines. Moreover, we
discuss a new order of operations for the reduction process. In various
different experimental results we show that GBLA performs better than GB
reduction or Magma in sequential computations (up to 40% faster) and scales
much better than GB reduction for a higher number of cores: On 32 cores we
reach a scaling of up to 26. GBLA is up to 7 times faster than GB reduction.
Further, we compare different parallel schedulers GBLA can be used with. We
also developed a new advanced storage format that exploits the fact that our
matrices are coming from Gr\""obner basis computations, shrinking storage by a
factor of up to 4. A huge database of our matrices is freely available with
GBLA.
"
869,Finding best possible constant for a polynomial inequality,"  Given a multi-variant polynomial inequality with a parameter, how to find the
best possible value of this parameter that satisfies the inequality? For
instance, find the greatest number $k$ that satisfies $ a^3+b^3+c^3+
k(a^2b+b^2c+c^2a)-(k+1)(ab^2+bc^2+ca^2)\geq 0 $ for all nonnegative real
numbers $ a,b,c $. Analogues problems often appeared in studies of inequalities
and were dealt with by various methods. In this paper, a general algorithm is
proposed for finding the required best possible constant. The algorithm can be
easily implemented by computer algebra tools such as Maple.
"
870,On the ring of local unitary invariants for mixed X-states of two qubits,"  Entangling properties of a mixed 2-qubit system can be described by the local
homogeneous unitary invariant polynomials in elements of the density matrix.
The structure of the corresponding invariant polynomial ring for the special
subclass of states, the so-called mixed X-states, is established. It is shown
that for the X-states there is an injective ring homomorphism of the quotient
ring of SU(2)xSU(2) invariant polynomials modulo its syzygy ideal and the
SO(2)xSO(2)-invariant ring freely generated by five homogeneous polynomials of
degrees 1,1,1,2,2.
"
871,Matrix factoring by fraction-free reduction,"  We consider exact matrix decomposition by Gauss-Bareiss reduction. We
investigate two aspects of the process: common row and column factors and the
influence of pivoting strategies. We identify two types of common factors:
systematic and statistical. Systematic factors depend on the process, while
statistical factors depend on the specific data. We show that existing
fraction-free QR (Gram-Schmidt) algorithms create a common factor in the last
column of Q. We relate the existence of row factors in LU decomposition to
factors appearing in the Smith normal form of the matrix. For statistical
factors, we identify mechanisms and give estimates of the frequency. Our
conclusions are tested by experimental data. For pivoting strategies, we
compare the sizes of output factors obtained by different strategies. We also
comment on timing differences.
"
872,On the computation of the straight lines contained in a rational surface,"  In this paper we present an algorithm to compute the (real and complex)
straight lines contained in a rational surface, defined by a rational
parameterization. The algorithm relies on the well-known theorem of
Differential Geometry that characterizes real straight lines contained in a
surface as curves that are simultaneously asymptotic lines, and geodesics. We
also report on an implementation carried out in Maple 18, and we compare the
behavior of our algorithm with two brute-force approaches.
"
873,Binomial Difference Ideals,"  In this paper, binomial difference ideals are studied. Three canonical
representations for Laurent binomial difference ideals are given in terms of
the reduced Groebner basis of Z[x]-lattices, regular and coherent difference
ascending chains, and partial characters over Z[x]-lattices, respectively.
Criteria for a Laurent binomial difference ideal to be reflexive, prime,
well-mixed, and perfect are given in terms of their support lattices. The
reflexive, well-mixed, and perfect closures of a Laurent binomial difference
ideal are shown to be binomial. Most of the properties of Laurent binomial
difference ideals are extended to the case of difference binomial ideals.
Finally, algorithms are given to check whether a given Laurent binomial
difference ideal I is reflexive, prime, well-mixed, or perfect, and in the
negative case, to compute the reflexive, well-mixed, and perfect closures of I.
An algorithm is given to decompose a finitely generated perfect binomial
difference ideal as the intersection of reflexive prime binomial difference
ideals.
"
874,"Summation Theory II: Characterizations of
  $\boldsymbol{R\Pi\Sigma^*}$-extensions and algorithmic aspects","  Recently, $R\Pi\Sigma^*$-extensions have been introduced which extend Karr's
$\Pi\Sigma^*$-fields substantially: one can represent expressions not only in
terms of transcendental sums and products, but one can work also with products
over primitive roots of unity. Since one can solve the parameterized
telescoping problem in such rings, covering as special cases the summation
paradigms of telescoping and creative telescoping, one obtains a rather
flexible toolbox for symbolic summation. This article is the continuation of
this work. Inspired by Singer's Galois theory of difference equations we will
work out several alternative characterizations of $R\Pi\Sigma^*$-extensions:
adjoining naively sums and products leads to an $R\Pi\Sigma^*$-extension iff
the obtained difference ring is simple iff the ring can be embedded into the
ring of sequences iff the ring can be given by the interlacing of
$\Pi\Sigma^*$-extensions. From the viewpoint of applications this leads to a
fully automatic machinery to represent indefinite nested sums and products in
such $R\Pi\Sigma^*$-rings. In addition, we work out how the parameterized
telescoping paradigm can be used to prove algebraic independence of indefinite
nested sums. Furthermore, one obtains an alternative reduction tactic to solve
the parameterized telescoping problem in basic $R\Pi\Sigma^*$-extensions
exploiting the interlacing property.
"
875,Extraction of cylinders and cones from minimal point sets,"  We propose new algebraic methods for extracting cylinders and cones from
minimal point sets, including oriented points. More precisely, we are
interested in computing efficiently cylinders through a set of three points,
one of them being oriented, or through a set of five simple points. We are also
interested in computing efficiently cones through a set of two oriented points,
through a set of four points, one of them being oriented, or through a set of
six points. For these different interpolation problems, we give optimal bounds
on the number of solutions. Moreover, we describe algebraic methods targeted to
solve these problems efficiently.
"
876,Algorithm for computing $\mu$-bases of univariate polynomials,"  We present a new algorithm for computing a $\mu$-basis of the syzygy module
of $n$ polynomials in one variable over an arbitrary field $\mathbb{K}$. The
algorithm is conceptually different from the previously-developed algorithms by
Cox, Sederberg, Chen, Zheng, and Wang for $n=3$, and by Song and Goldman for an
arbitrary $n$. It involves computing a ""partial"" reduced row-echelon form of a
$ (2d+1)\times n(d+1)$ matrix over $\mathbb{K}$, where $d$ is the maximum
degree of the input polynomials. The proof of the algorithm is based on
standard linear algebra and is completely self-contained. It includes a proof
of the existence of the $\mu$-basis and as a consequence provides an
alternative proof of the freeness of the syzygy module. The theoretical (worst
case asymptotic) computational complexity of the algorithm is
$O(d^2n+d^3+n^2)$. We have implemented this algorithm (HHK) and the one
developed by Song and Goldman (SG). Experiments on random inputs indicate that
SG gets faster than HHK when $d$ gets sufficiently large for a fixed $n$, and
that HHK gets faster than SG when $n$ gets sufficiently large for a fixed $d$.
"
877,Symbolic Tensor Calculus -- Functional and Dynamic Approach,"  In this paper, we briefly discuss the dynamic and functional approach to
computer symbolic tensor analysis. The ccgrg package for Wolfram
Language/Mathematica is used to illustrate this approach. Some examples of
applications are attached.
"
878,Nearest Points on Toric Varieties,"  We determine the Euclidean distance degree of a projective toric variety.
This extends the formula of Matsui and Takeuchi for the degree of the
$A$-discriminant in terms of Euler obstructions. Our primary goal is the
development of reliable algorithmic tools for computing the points on a real
toric variety that are closest to a given data point.
"
879,"Secure cloud computations: Description of (fully)homomorphic ciphers
  within the P-adic model of encryption","  In this paper we consider the description of homomorphic and fully
homomorphic ciphers in the $p$-adic model of encryption. This model describes a
wide class of ciphers, but certainly not all. Homomorphic and fully homomorphic
ciphers are used to ensure the credibility of remote computing, including cloud
technology. The model describes all homomorphic ciphers with respect to
arithmetic and coordinate-wise logical operations in the ring of $p$-adic
integers $Z_p$. We show that there are no fully homomorphic ciphers for each
pair of the considered set of arithmetic and coordinate-wise logical operations
on $Z_p$. We formulate the problem of constructing a fully homomorphic cipher
as follows. We consider a homomorphic cipher with respect to operation ""$*$"" on
$Z_p$. Then, we describe the complete set of operations ""$G$"", for which the
cipher is homomorphic. As a result, we construct a fully homomorphic cipher
with respect to the operations ""$*$"" and ""$G$"". We give a description of all
operations ""$G$"", for which we obtain fully homomorphic ciphers with respect to
the operations ""$+$"" and ""$G$"" from the homomorphic cipher constructed with
respect to the operation ""$+$"". We also present examples of such ""new""
operations.
"
880,"Solution of Interpolation Problems via the Hankel Polynomial
  Construction","  We treat the interpolation problem $ \{f(x_j)=y_j\}_{j=1}^N $ for polynomial
and rational functions. Developing the approach by C.Jacobi, we represent the
interpolants by virtue of the Hankel polynomials generated by the sequences $
\{\sum_{j=1}^N x_j^ky_j/W^{\prime}(x_j) \}_{k\in \mathbb N} $ and $
\{\sum_{j=1}^N x_j^k/(y_jW^{\prime}(x_j)) \}_{k\in \mathbb N} $; here $
W(x)=\prod_{j=1}^N(x-x_j) $. The obtained results are applied for the error
correction problem, i.e. the problem of reconstructing the polynomial from a
redundant set of its values some of which are probably erroneous. The problem
of evaluation of the resultant of polynomials $ p(x) $ and $ q(x) $ from the
set of values $ \{p(x_j)/q(x_j) \}_{j=1}^N $ is also tackled within the
framework of this approach.
"
881,Toric Difference Variety,"  In this paper, the concept of toric difference varieties is defined and four
equivalent descriptions for toric difference varieties are presented in terms
of difference rational parametrization, difference coordinate rings, toric
difference ideals, and group actions by difference tori. Connections between
toric difference varieties and affine N[x]-semimodules are established by
proving the correspondence between the irreducible invariant difference
subvarieties and the faces of the N[x]-submodules and the orbit-face
correspondence. Finally, an algorithm is given to decide whether a binomial
difference ideal represented by a Z[x]-lattice defines a toric difference
variety.
"
882,A modified block Lanczos algorithm with fewer vectors,"  The block Lanczos algorithm proposed by Peter Montgomery is an efficient
means to tackle the sparse linear algebra problem which arises in the context
of the number field sieve factoring algorithm and its predecessors. We present
here a modified version of the algorithm, which incorporates several
improvements: we discuss how to efficiently handle homogeneous systems and how
to reduce the number of vectors stored in the course of the computation. We
also provide heuristic justification for the success probability of our
modified algorithm. While the overall complexity and expected number of steps
of the block Lanczos is not changed by the modifications presented in this
article, we expect these to be useful for implementations of the block Lanczos
algorithm where the storage of auxiliary vectors sometimes has a non-negligible
cost. 1 Linear systems for integer factoring For factoring a composite integer
N, algorithms based on the technique of combination of congruences look for
several pairs of integers (x, y) such that x 2 $\not\equiv$ y 2 mod N. This
equality is hoped to be non trivial for at least one of the obtained pairs,
letting gcd(x -- y, N) unveil a factor of the integer N. Several algorithms use
this strategy: the CFRAC algorithm, the quadratic sieve and its variants, and
the number field sieve. Pairs (x, y) as above are obtained by combining
relations which have been collected as a step of these algorithms. Relations
are written multiplicatively as a set of valuations. All the algorithms
considered seek a multiplicative combination of these relations which can be
rewritten as an equality of squares. This is achieved by solving a system of
linear equations defined over F 2, where equations are parity constraints on
"
883,Chordal networks of polynomial ideals,"  We introduce a novel representation of structured polynomial ideals, which we
refer to as chordal networks. The sparsity structure of a polynomial system is
often described by a graph that captures the interactions among the variables.
Chordal networks provide a computationally convenient decomposition into
simpler (triangular) polynomial sets, while preserving the underlying graphical
structure. We show that many interesting families of polynomial ideals admit
compact chordal network representations (of size linear in the number of
variables), even though the number of components is exponentially large.
Chordal networks can be computed for arbitrary polynomial systems using a
refinement of the chordal elimination algorithm from [Cifuentes-Parrilo-2016].
Furthermore, they can be effectively used to obtain several properties of the
variety, such as its dimension, cardinality, and equidimensional components, as
well as an efficient probabilistic test for radical ideal membership. We apply
our methods to examples from algebraic statistics and vector addition systems;
for these instances, algorithms based on chordal networks outperform existing
techniques by orders of magnitude.
"
884,Algorithmic computation of polynomial amoebas,"  We present algorithms for computation and visualization of amoebas, their
contours, compactified amoebas and sections of three-dimensional amoebas by
two-dimensional planes. We also provide method and an algorithm for the
computation of~polynomials whose amoebas exhibit the most complicated topology
among all polynomials with a fixed Newton polytope. The presented algorithms
are implemented in computer algebra systems Matlab 8 and Mathematica 9.
"
885,Decoding Interleaved Gabidulin Codes using Alekhnovich's Algorithm,"  We prove that Alekhnovich's algorithm can be used for row reduction of skew
polynomial matrices. This yields an $O(\ell^3 n^{(\omega+1)/2} \log(n))$
decoding algorithm for $\ell$-Interleaved Gabidulin codes of length $n$, where
$\omega$ is the matrix multiplication exponent, improving in the exponent of
$n$ compared to previous results.
"
886,Sparse Representations of Clifford and Tensor algebras in Maxima,"  Clifford algebras have broad applications in science and engineering. The use
of Clifford algebras can be further promoted in these fields by availability of
computational tools that automate tedious routine calculations. We offer an
extensive demonstration of the applications of Clifford algebras in
electromagnetism using the geometric algebra G3 = Cl(3,0) as a computational
model in the Maxima computer algebra system. We compare the geometric
algebra-based approach with conventional symbolic tensor calculations supported
by Maxima, based on the itensor package. The Clifford algebra functionality of
Maxima is distributed as two new packages called clifford - for basic
simplification of Clifford products, outer products, scalar products and
inverses; and cliffordan - for applications of geometric calculus.
"
887,"Generalized Homogeneous Polynomials for Efficient Template-Based
  Nonlinear Invariant Synthesis","  The template-based method is one of the most successful approaches to
algebraic invariant synthesis. In this method, an algorithm designates a
template polynomial p over program variables, generates constraints for p=0 to
be an invariant, and solves the generated constraints. However, this approach
often suffers from an increasing template size if the degree of a template
polynomial is too high.
  We propose a technique to make template-based methods more efficient. Our
technique is based on the following finding: If an algebraic invariant exists,
then there is a specific algebraic invariant that we call a generalized
homogeneous algebraic invariant that is often smaller. This finding justifies
using only a smaller template that corresponds to a generalized homogeneous
algebraic invariant.
  Concretely, we state our finding above formally based on the abstract
semantics of an imperative program proposed by Cachera et al. Then, we modify
their template-based invariant synthesis so that it generates only generalized
homogeneous algebraic invariants. This modification is proved to be sound.
Furthermore, we also empirically demonstrate the merit of the restriction to
generalized homogeneous algebraic invariants. Our implementation outperforms
that of Cachera et al. for programs that require a higher-degree template.
"
888,New Bounds for Hypergeometric Creative Telescoping,"  Based on a modified version of Abramov-Petkov\v{s}ek reduction, a new
algorithm to compute minimal telescopers for bivariate hypergeometric terms was
developed last year. We investigate further in this paper and present a new
argument for the termination of this algorithm, which provides an independent
proof of the existence of telescopers and even enables us to derive lower as
well as upper bounds for the order of telescopers for hypergeometric terms.
Compared to the known bounds in the literature, our bounds are sometimes
better, and never worse than the known ones.
"
889,Verifying Buchberger's Algorithm in Reduction Rings,"  In this paper we present the formal, computer-supported verification of a
functional implementation of Buchberger's critical-pair/completion algorithm
for computing Gr\""obner bases in reduction rings. We describe how the algorithm
can be implemented and verified within one single software system, which in our
case is the Theorema system.
  In contrast to existing formal correctness proofs of Buchberger's algorithm
in other systems, e. g. Coq and ACL2, our work is not confined to the classical
setting of polynomial rings over fields, but considers the much more general
setting of reduction rings; this, naturally, makes the algorithm more
complicated and the verification more difficult.
  The correctness proof is essentially based on some non-trivial results from
the theory of reduction rings, which we formalized and formally proved as well.
This formalization already consists of more than 800 interactively proved
lemmas and theorems, making the elaboration an extensive example of
higher-order theory exploration in Theorema.
"
890,"On the Complexity of Solving Zero-Dimensional Polynomial Systems via
  Projection","  Given a zero-dimensional polynomial system consisting of n integer
polynomials in n variables, we propose a certified and complete method to
compute all complex solutions of the system as well as a corresponding
separating linear form l with coefficients of small bit size. For computing l,
we need to project the solutions into one dimension along O(n) distinct
directions but no further algebraic manipulations. The solutions are then
directly reconstructed from the considered projections. The first step is
deterministic, whereas the second step uses randomization, thus being
Las-Vegas.
  The theoretical analysis of our approach shows that the overall cost for the
two problems considered above is dominated by the cost of carrying out the
projections. We also give bounds on the bit complexity of our algorithms that
are exclusively stated in terms of the number of variables, the total degree
and the bitsize of the input polynomials.
"
891,Symbolic-Numeric Tools for Analytic Combinatorics in Several Variables,"  Analytic combinatorics studies the asymptotic behaviour of sequences through
the analytic properties of their generating functions. This article provides
effective algorithms required for the study of analytic combinatorics in
several variables, together with their complexity analyses. Given a
multivariate rational function we show how to compute its smooth isolated
critical points, with respect to a polynomial map encoding asymptotic
behaviour, in complexity singly exponential in the degree of its denominator.
We introduce a numerical Kronecker representation for solutions of polynomial
systems with rational coefficients and show that it can be used to decide
several properties (0 coordinate, equal coordinates, sign conditions for real
solutions, and vanishing of a polynomial) in good bit complexity. Among the
critical points, those that are minimal---a property governed by inequalities
on the moduli of the coordinates---typically determine the dominant asymptotics
of the diagonal coefficient sequence. When the Taylor expansion at the origin
has all non-negative coefficients (known as the `combinatorial case') and under
regularity conditions, we utilize this Kronecker representation to determine
probabilistically the minimal critical points in complexity singly exponential
in the degree of the denominator, with good control over the exponent in the
bit complexity estimate. Generically in the combinatorial case, this allows one
to automatically and rigorously determine asymptotics for the diagonal
coefficient sequence. Examples obtained with a preliminary implementation show
the wide applicability of this approach.
"
892,Computing Real Roots of Real Polynomials ... and now For Real!,"  Very recent work introduces an asymptotically fast subdivision algorithm,
denoted ANewDsc, for isolating the real roots of a univariate real polynomial.
The method combines Descartes' Rule of Signs to test intervals for the
existence of roots, Newton iteration to speed up convergence against clusters
of roots, and approximate computation to decrease the required precision. It
achieves record bounds on the worst-case complexity for the considered problem,
matching the complexity of Pan's method for computing all complex roots and
improving upon the complexity of other subdivision methods by several
magnitudes.
  In the article at hand, we report on an implementation of ANewDsc on top of
the RS root isolator. RS is a highly efficient realization of the classical
Descartes method and currently serves as the default real root solver in Maple.
We describe crucial design changes within ANewDsc and RS that led to a
high-performance implementation without harming the theoretical complexity of
the underlying algorithm.
  With an excerpt of our extensive collection of benchmarks, available online
at http://anewdsc.mpi-inf.mpg.de/, we illustrate that the theoretical gain in
performance of ANewDsc over other subdivision methods also transfers into
practice. These experiments also show that our new implementation outperforms
both RS and mature competitors by magnitudes for notoriously hard instances
with clustered roots. For all other instances, we avoid almost any overhead by
integrating additional optimizations and heuristics.
"
893,Munchausen Iteration,"  We present a method for solving polynomial equations over idempotent
omega-continuous semirings. The idea is to iterate over the semiring of
functions rather than the semiring of interest, and only evaluate when needed.
The key operation is substitution. In the initial step, we compute a linear
completion of the system of equations that exhaustively inserts the equations
into one another. With functions as approximants, the following steps insert
the current approximant into itself. Since the iteration improves its precision
by substitution rather than computation we named it Munchausen, after the
fictional baron that pulled himself out of a swamp by his own hair. The first
result shows that an evaluation of the n-th Munchausen approximant coincides
with the 2^n-th Newton approximant. Second, we show how to compute linear
completions with standard techniques from automata theory. In particular, we
are not bound to (but can use) the notion of differentials prominent in Newton
iteration.
"
894,"Using Two Types of Computer Algebra Systems to Solve Maxwell Optics
  Problems","  To synthesize Maxwell optics systems, the mathematical apparatus of tensor
and vector analysis is generally employed. This mathematical apparatus implies
executing a great number of simple stereotyped operations, which are adequately
supported by computer algebra systems. In this paper, we distinguish between
two stages of working with a mathematical model: model development and model
usage. Each of these stages implies its own computer algebra system. As a model
problem, we consider the problem of geometrization of Maxwell's equations. Two
computer algebra systems---Cadabra and FORM---are selected for use at different
stages of investigation.
"
895,"Determinantal sets, singularities and application to optimal control in
  medical imagery","  Control theory has recently been involved in the field of nuclear magnetic
resonance imagery. The goal is to control the magnetic field optimally in order
to improve the contrast between two biological matters on the pictures.
Geometric optimal control leads us here to analyze mero-morphic vector fields
depending upon physical parameters , and having their singularities defined by
a deter-minantal variety. The involved matrix has polynomial entries with
respect to both the state variables and the parameters. Taking into account the
physical constraints of the problem, one needs to classify, with respect to the
parameters, the number of real singularities lying in some prescribed
semi-algebraic set. We develop a dedicated algorithm for real root
classification of the singularities of the rank defects of a polynomial matrix,
cut with a given semi-algebraic set. The algorithm works under some genericity
assumptions which are easy to check. These assumptions are not so restrictive
and are satisfied in the aforementioned application. As more general strategies
for real root classification do, our algorithm needs to compute the critical
loci of some maps, intersections with the boundary of the semi-algebraic
domain, etc. In order to compute these objects, the determinantal structure is
exploited through a stratifi-cation by the rank of the polynomial matrix. This
speeds up the computations by a factor 100. Furthermore, our implementation is
able to solve the application in medical imagery, which was out of reach of
more general algorithms for real root classification. For instance,
computational results show that the contrast problem where one of the matters
is water is partitioned into three distinct classes.
"
896,"The complexity of cylindrical algebraic decomposition with respect to
  polynomial degree","  Cylindrical algebraic decomposition (CAD) is an important tool for working
with polynomial systems, particularly quantifier elimination. However, it has
complexity doubly exponential in the number of variables. The base algorithm
can be improved by adapting to take advantage of any equational constraints
(ECs): equations logically implied by the input. Intuitively, we expect the
double exponent in the complexity to decrease by one for each EC. In ISSAC 2015
the present authors proved this for the factor in the complexity bound
dependent on the number of polynomials in the input. However, the other term,
that dependent on the degree of the input polynomials, remained unchanged.
  In the present paper the authors investigate how CAD in the presence of ECs
could be further refined using the technology of Groebner Bases to move towards
the intuitive bound for polynomial degree.
"
897,"Critical Point Computations on Smooth Varieties: Degree and Complexity
  bounds","  Let V $\subset$ C n be an equidimensional algebraic set and g be an n-variate
polynomial with rational coefficients. Computing the critical points of the map
that evaluates g at the points of V is a cornerstone of several algorithms in
real algebraic geometry and optimization. Under the assumption that the
critical locus is finite and that the projective closure of V is smooth, we
provide sharp upper bounds on the degree of the critical locus which depend
only on deg(g) and the degrees of the generic polar varieties associated to V.
Hence, in some special cases where the degrees of the generic polar varieties
do not reach the worst-case bounds, this implies that the number of critical
points of the evaluation map of g is less than the currently known degree
bounds. We show that, given a lifting fiber of V , a slight variant of an
algorithm due to Bank, Giusti, Heintz, Lecerf, Matera and Solern{\'o} computes
these critical points in time which is quadratic in this bound up to
logarithmic factors, linear in the complexity of evaluating the input system
and polynomial in the number of variables and the maximum degree of the input
polynomials.
"
898,HLinear: Exact Dense Linear Algebra in Haskell,"  We present an implementation in the functional programming language Haskell
of the PLE decomposition of matrices over division rings. Our benchmarks
indicate that it is competitive with the C-based implementation provided in
Flint. Describing the guiding principles of our work, we introduce the reader
to basic ideas from high-performance functional programming.
"
899,"Theano: A Python framework for fast computation of mathematical
  expressions","  Theano is a Python library that allows to define, optimize, and evaluate
mathematical expressions involving multi-dimensional arrays efficiently. Since
its introduction, it has been one of the most used CPU and GPU mathematical
compilers - especially in the machine learning community - and has shown steady
performance improvements. Theano is being actively and continuously developed
since 2008, multiple frameworks have been built on top of it and it has been
used to produce many state-of-the-art machine learning models.
  The present article is structured as follows. Section I provides an overview
of the Theano software and its community. Section II presents the principal
features of Theano and how to use them, and compares them with other similar
projects. Section III focuses on recently-introduced functionalities and
improvements. Section IV compares the performance of Theano against Torch7 and
TensorFlow on several machine learning models. Section V discusses current
limitations of Theano and potential ways of improving it.
"
900,Synthesizing Probabilistic Invariants via Doob's Decomposition,"  When analyzing probabilistic computations, a powerful approach is to first
find a martingale---an expression on the program variables whose expectation
remains invariant---and then apply the optional stopping theorem in order to
infer properties at termination time. One of the main challenges, then, is to
systematically find martingales.
  We propose a novel procedure to synthesize martingale expressions from an
arbitrary initial expression. Contrary to state-of-the-art approaches, we do
not rely on constraint solving. Instead, we use a symbolic construction based
on Doob's decomposition. This procedure can produce very complex martingales,
expressed in terms of conditional expectations.
  We show how to automatically generate and simplify these martingales, as well
as how to apply the optional stopping theorem to infer properties at
termination time. This last step typically involves some simplification steps,
and is usually done manually in current approaches. We implement our techniques
in a prototype tool and demonstrate our process on several classical examples.
Some of them go beyond the capability of current semi-automatic approaches.
"
901,Need Polynomial Systems be Doubly-exponential?,"  Polynomial Systems, or at least their algorithms, have the reputation of
being doubly-exponential in the number of variables [Mayr and Mayer, 1982],
[Davenport and Heintz, 1988]. Nevertheless, the Bezout bound tells us that that
number of zeros of a zero-dimensional system is singly-exponential in the
number of variables. How should this contradiction be reconciled?
  We first note that [Mayr and Ritscher, 2013] shows that the doubly
exponential nature of Gr\""{o}bner bases is with respect to the dimension of the
ideal, not the number of variables. This inspires us to consider what can be
done for Cylindrical Algebraic Decomposition which produces a
doubly-exponential number of polynomials of doubly-exponential degree.
  We review work from ISSAC 2015 which showed the number of polynomials could
be restricted to doubly-exponential in the (complex) dimension using McCallum's
theory of reduced projection in the presence of equational constraints. We then
discuss preliminary results showing the same for the degree of those
polynomials. The results are under primitivity assumptions whose importance we
illustrate.
"
902,"Extended Hardness Results for Approximate Gr\""obner Basis Computation","  Two models were recently proposed to explore the robust hardness of Gr\""obner
basis computation. Given a polynomial system, both models allow an algorithm to
selectively ignore some of the polynomials: the algorithm is only responsible
for returning a Gr\""obner basis for the ideal generated by the remaining
polynomials. For the $q$-Fractional Gr\""obner Basis Problem the algorithm is
allowed to ignore a constant $(1-q)$-fraction of the polynomials (subject to
one natural structural constraint). Here we prove a new strongest-parameter
result: even if the algorithm is allowed to choose a $(3/10-\epsilon)$-fraction
of the polynomials to ignore, and need only compute a Gr\""obner basis with
respect to some lexicographic order for the remaining polynomials, this cannot
be accomplished in polynomial time (unless $P=NP$). This statement holds even
if every polynomial has maximum degree 3. Next, we prove the first robust
hardness result for polynomial systems of maximum degree 2: for the
$q$-Fractional model a $(1/5-\epsilon)$ fraction of the polynomials may be
ignored without losing provable NP-Hardness. Both theorems hold even if every
polynomial contains at most three distinct variables. Finally, for the Strong
$c$-partial Gr\""obner Basis Problem of De Loera et al. we give conditional
results that depend on famous (unresolved) conjectures of Khot and Dinur, et
al.
"
903,Efficient Algorithms for Mixed Creative Telescoping,"  Creative telescoping is a powerful computer algebra paradigm -initiated by
Doron Zeilberger in the 90's- for dealing with definite integrals and sums with
parameters. We address the mixed continuous-discrete case, and focus on the
integration of bivariate hypergeometric-hyperexponential terms. We design a new
creative telescoping algorithm operating on this class of inputs, based on a
Hermite-like reduction procedure. The new algorithm has two nice features: it
is efficient and it delivers, for a suitable representation of the input, a
minimal-order telescoper. Its analysis reveals tight bounds on the sizes of the
telescoper it produces.
"
904,"Computing Small Certificates of Inconsistency of Quadratic Fewnomial
  Systems","  B{\'e}zout 's theorem states that dense generic systems of n multivariate
quadratic equations in n variables have 2 n solutions over algebraically closed
fields. When only a small subset M of monomials appear in the equations
(fewnomial systems), the number of solutions may decrease dramatically. We
focus in this work on subsets of quadratic monomials M such that generic
systems with support M do not admit any solution at all. For these systems,
Hilbert's Nullstellensatz ensures the existence of algebraic certificates of
inconsistency. However, up to our knowledge all known bounds on the sizes of
such certificates -including those which take into account the Newton polytopes
of the polynomials- are exponential in n. Our main results show that if the
inequality 2|M| -- 2n $\le$ $\sqrt$ 1 + 8{\nu} -- 1 holds for a quadratic
fewnomial system -- where {\nu} is the matching number of a graph associated
with M, and |M| is the cardinality of M -- then there exists generically a
certificate of inconsistency of linear size (measured as the number of
coefficients in the ground field K). Moreover this certificate can be computed
within a polynomial number of arithmetic operations. Next, we evaluate how
often this inequality holds, and we give evidence that the probability that the
inequality is satisfied depends strongly on the number of squares. More
precisely, we show that if M is picked uniformly at random among the subsets of
n + k + 1 quadratic monomials containing at least $\Omega$(n 1/2+$\epsilon$)
squares, then the probability that the inequality holds tends to 1 as n grows.
Interestingly, this phenomenon is related with the matching number of random
graphs in the Erd{\""o}s-Renyi model. Finally, we provide experimental results
showing that certificates in inconsistency can be computed for systems with
more than 10000 variables and equations.
"
905,Computation of the Similarity Class of the p-Curvature,"  The $p$-curvature of a system of linear differential equations in positive
characteristic $p$ is a matrix that measures how far the system is from having
a basis of polynomial solutions. We show that the similarity class of the
$p$-curvature can be determined without computing the $p$-curvature itself.
More precisely, we design an algorithm that computes the invariant factors of
the $p$-curvature in time quasi-linear in $\sqrt p$. This is much less than the
size of the $p$-curvature, which is generally linear in $p$. The new algorithm
allows to answer a question originating from the study of the Ising model in
statistical physics.
"
906,Segmentation of real algebraic plane curves,"  In this article we give an implementation of the standard algorithm to
segment a real algebraic plane curve defined implicitly. Our implementation is
efficient and simpler than previous. We use global information to count the
number of half-branches at a critical point.
"
907,"Bit complexity for multi-homogeneous polynomial system solving
  Application to polynomial minimization","  Multi-homogeneous polynomial systems arise in many applications. We provide
bit complexity estimates for solving them which, up to a few extra other
factors, are quadratic in the number of solutions and linear in the height of
the input system under some genericity assumptions. The assumptions essentially
imply that the Jacobian matrix of the system under study has maximal rank at
the solution set and that this solution set if finite. The algorithm is
probabilistic and a probability analysis is provided. Next, we apply these
results to the problem of optimizing a linear map on the real trace of an
algebraic set. Under some genericity assumptions, we provide bit complexity
estimates for solving this polynomial minimization problem.
"
908,The Symbolic Interior Point Method,"  A recent trend in probabilistic inference emphasizes the codification of
models in a formal syntax, with suitable high-level features such as
individuals, relations, and connectives, enabling descriptive clarity,
succinctness and circumventing the need for the modeler to engineer a custom
solver. Unfortunately, bringing these linguistic and pragmatic benefits to
numerical optimization has proven surprisingly challenging. In this paper, we
turn to these challenges: we introduce a rich modeling language, for which an
interior-point method computes approximate solutions in a generic way. While
logical features easily complicates the underlying model, often yielding
intricate dependencies, we exploit and cache local structure using algebraic
decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently
realizable in ADDs, but we argue and show that well-known optimization methods
are not ideal for ADDs. Our engine, therefore, invokes a sophisticated
matrix-free approach. We demonstrate the flexibility of the resulting
symbolic-numeric optimizer on decision making and compressed sensing tasks with
millions of non-zero entries.
"
909,"Factoring Polynomials over Finite Fields using Drinfeld Modules with
  Complex Multiplication","  We present novel algorithms to factor polynomials over a finite field $\F_q$
of odd characteristic using rank $2$ Drinfeld modules with complex
multiplication. The main idea is to compute a lift of the Hasse invariant
(modulo the polynomial $f(x) \in \F_q[x]$ to be factored) with respect to a
Drinfeld module $\phi$ with complex multiplication. Factors of $f(x)$ supported
on prime ideals with supersingular reduction at $\phi$ have vanishing Hasse
invariant and can be separated from the rest. A Drinfeld module analogue of
Deligne's congruence plays a key role in computing the Hasse invariant lift. We
present two algorithms based on this idea. The first algorithm chooses Drinfeld
modules with complex multiplication at random and has a quadratic expected run
time. The second is a deterministic algorithm with $O(\sqrt{p})$ run time
dependence on the characteristic $p$ of $\F_q$.
"
910,Splitting quaternion algebras over quadratic number fields,"  We propose an algorithm for finding zero divisors in quaternion algebras over
quadratic number fields, or equivalently, solving homogeneous quadratic
equations in three variables over $\mathbb{Q}(\sqrt{d})$ where $d$ is a
square-free integer. The algorithm is randomized and runs in polynomial time if
one is allowed to call oracles for factoring integers.
"
911,The Complexity of Computing all Subfields of an Algebraic Number Field,"  For a finite separable field extension K/k, all subfields can be obtained by
intersecting so-called principal subfields of K/k. In this work we present a
way to quickly compute these intersections. If the number of subfields is high,
then this leads to faster run times and an improved complexity.
"
912,"Computing Hypergeometric Solutions of Second Order Linear Differential
  Equations using Quotients of Formal Solutions and Integral Bases","  We present two algorithms for computing hypergeometric solutions of second
order linear differential operators with rational function coefficients. Our
first algorithm searches for solutions of the form \[ \exp(\int r \,
dx)\cdot{_{2}F_1}(a_1,a_2;b_1;f) \] where $r,f \in \overline{\mathbb{Q}(x)}$,
and $a_1,a_2,b_1 \in \mathbb{Q}$. It uses modular reduction and Hensel lifting.
Our second algorithm tries to find solutions in the form \[ \exp(\int r \,
dx)\cdot \left( r_0 \cdot{_{2}F_1}(a_1,a_2;b_1;f) + r_1
\cdot{_{2}F_1}'(a_1,a_2;b_1;f) \right) \] where $r_0, r_1 \in
\overline{\mathbb{Q}(x)}$, as follows: It tries to transform the input equation
to another equation with solutions of the first type, and then uses the first
algorithm.
"
913,Inverse Mellin Transform of Holonomic Sequences,"  We describe a method to compute the inverse Mellin transform of holonomic
sequences, that is based on a method to compute the Mellin transform of
holonomic functions. Both methods are implemented in the computer algebra
package HarmonicSums.
"
914,"Hypergeometric Expressions for Generating Functions of Walks with Small
  Steps in the Quarter Plane","  We study nearest-neighbors walks on the two-dimensional square lattice, that
is, models of walks on $\mathbb{Z}^2$ defined by a fixed step set that is a
subset of the non-zero vectors with coordinates 0, 1 or $-1$. We concern
ourselves with the enumeration of such walks starting at the origin and
constrained to remain in the quarter plane $\mathbb{N}^2$, counted by their
length and by the position of their ending point. Bousquet-M\'elou and Mishna
[Contemp. Math., pp. 1--39, Amer. Math. Soc., 2010] identified 19 models of
walks that possess a D-finite generating function; linear differential
equations have then been guessed in these cases by Bostan and Kauers [FPSAC
2009, Discrete Math. Theor. Comput. Sci. Proc., pp. 201--215, 2009]. We give
here the first proof that these equations are indeed satisfied by the
corresponding generating functions. As a first corollary, we prove that all
these 19 generating functions can be expressed in terms of Gauss'
hypergeometric functions that are intimately related to elliptic integrals. As
a second corollary, we show that all the 19 generating functions are
transcendental, and that among their $19 \times 4$ combinatorially meaningful
specializations only four are algebraic functions.
"
915,"Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial
  Factorization over Finite Fields","  The fastest known algorithm for factoring univariate polynomials over finite
fields is the Kedlaya-Umans (fast modular composition) implementation of the
Kaltofen-Shoup algorithm. It is randomized and takes $\widetilde{O}(n^{3/2}\log
q + n \log^2 q)$ time to factor polynomials of degree $n$ over the finite field
$\mathbb{F}_q$ with $q$ elements. A significant open problem is if the $3/2$
exponent can be improved. We study a collection of algebraic problems and
establish a web of reductions between them. A consequence is that an algorithm
for any one of these problems with exponent better than $3/2$ would yield an
algorithm for polynomial factorization with exponent better than $3/2$.
"
916,"Computing all Space Curve Solutions of Polynomial Systems by Polyhedral
  Methods","  A polyhedral method to solve a system of polynomial equations exploits its
sparse structure via the Newton polytopes of the polynomials. We propose a
hybrid symbolic-numeric method to compute a Puiseux series expansion for every
space curve that is a solution of a polynomial system. The focus of this paper
concerns the difficult case when the leading powers of the Puiseux series of
the space curve are contained in the relative interior of a higher dimensional
cone of the tropical prevariety. We show that this difficult case does not
occur for polynomials with generic coefficients. To resolve this case, we
propose to apply polyhedral end games to recover tropisms hidden in the
tropical prevariety.
"
917,Computing hypergeometric functions rigorously,"  We present an efficient implementation of hypergeometric functions in
arbitrary-precision interval arithmetic. The functions ${}_0F_1$, ${}_1F_1$,
${}_2F_1$ and ${}_2F_0$ (or the Kummer $U$-function) are supported for
unrestricted complex parameters and argument, and by extension, we cover
exponential and trigonometric integrals, error functions, Fresnel integrals,
incomplete gamma and beta functions, Bessel functions, Airy functions, Legendre
functions, Jacobi polynomials, complete elliptic integrals, and other special
functions. The output can be used directly for interval computations or to
generate provably correct floating-point approximations in any format.
Performance is competitive with earlier arbitrary-precision software, and
sometimes orders of magnitude faster. We also partially cover the generalized
hypergeometric function ${}_pF_q$ and computation of high-order parameter
derivatives.
"
918,"Asymptotic and exact results on the complexity of the
  Novelli-Pak-Stoyanovskii algorithm","  The Novelli-Pak-Stoyanovskii algorithm is a sorting algorithm for Young
tableaux of a fixed shape that was originally devised to give a bijective proof
of the hook-length formula. We obtain new asymptotic results on the average
case and worst case complexity of this algorithm as the underlying shape tends
to a fixed limit curve. Furthermore, using the summation package Sigma we prove
an exact formula for the average case complexity when the underlying shape
consists of only two rows. We thereby answer questions posed by Krattenthaler
and M\""uller.
"
919,Rigorous Multiple-Precision Evaluation of D-Finite Functions in SageMath,"  We present a new open source implementation in the SageMath computer algebra
system of algorithms for the numerical solution of linear ODEs with polynomial
coefficients. Our code supports regular singular connection problems and
provides rigorous error bounds.
"
920,"Numeric Deduction in Symbolic Computation. Application to Normalizing
  Transformations","  Algorithms of numeric (in exact arithmetic) deduction of analytical
expressions, proposed and described by Shevchenko and Vasiliev (1993), are
developed and implemented in a computer algebra code. This code is built as a
superstructure for the computer algebra package by Shevchenko and Sokolsky
(1993a) for normalization of Hamiltonian systems of ordinary differential
equations, in order that high complexity problems of normalization could be
solved. As an example, a resonant normal form of a Hamiltonian describing the
hyperboloidal precession of a dynamically symmetric satellite is derived by
means of the numeric deduction technique. The technique provides a considerable
economy, about 30 times in this particular application, in computer memory
consumption. It is naturally parallelizable. Thus the economy of memory
consumption is convertible into a gain in computation speed.
"
921,Finding binomials in polynomial ideals,"  We describe an algorithm which finds binomials in a given ideal
$I\subset\mathbb{Q}[x_1,\dots,x_n]$ and in particular decides whether binomials
exist in $I$ at all. Binomials in polynomial ideals can be well hidden. For
example, the lowest degree of a binomial cannot be bounded as a function of the
number of indeterminates, the degree of the generators, or the
Castelnuovo--Mumford regularity. We approach the detection problem by reduction
to the Artinian case using tropical geometry. The Artinian case is solved with
algorithms from computational number theory.
"
922,Private Multi-party Matrix Multiplication and Trust Computations,"  This paper deals with distributed matrix multiplication. Each player owns
only one row of both matrices and wishes to learn about one distinct row of the
product matrix, without revealing its input to the other players. We first
improve on a weighted average protocol, in order to securely compute a
dot-product with a quadratic volume of communications and linear number of
rounds. We also propose a protocol with five communication rounds, using a
Paillier-like underlying homomorphic public key cryptosystem, which is secure
in the semi-honest model or secure with high probability in the malicious
adversary model. Using ProVerif, a cryptographic protocol verification tool, we
are able to check the security of the protocol and provide a countermeasure for
each attack found by the tool. We also give a randomization method to avoid
collusion attacks. As an application, we show that this protocol enables a
distributed and secure evaluation of trust relationships in a network, for a
large class of trust evaluation schemes.
"
923,"Fast, deterministic computation of the Hermite normal form and
  determinant of a polynomial matrix","  Given a nonsingular $n \times n$ matrix of univariate polynomials over a
field $\mathbb{K}$, we give fast and deterministic algorithms to compute its
determinant and its Hermite normal form. Our algorithms use
$\widetilde{\mathcal{O}}(n^\omega \lceil s \rceil)$ operations in $\mathbb{K}$,
where $s$ is bounded from above by both the average of the degrees of the rows
and that of the columns of the matrix and $\omega$ is the exponent of matrix
multiplication. The soft-$O$ notation indicates that logarithmic factors in the
big-$O$ are omitted while the ceiling function indicates that the cost is
$\widetilde{\mathcal{O}}(n^\omega)$ when $s = o(1)$. Our algorithms are based
on a fast and deterministic triangularization method for computing the diagonal
entries of the Hermite form of a nonsingular matrix.
"
924,Evaluation of binomial double sums involving absolute values,"  We show that double sums of the form $$ \sum_{i,j=-n} ^{n}
|i^sj^t(i^k-j^k)^\beta| \binom {2n} {n+i} \binom {2n} {n+j} $$ can always be
expressed in terms of a linear combination of just four functions, namely
$\binom {4n}{2n}$, ${\binom {2n}n}^2$, $4^n\binom {2n}n$, and $16^n$, with
coefficients that are rational in $n$. We provide two different proofs: one is
algorithmic and uses the second author's computer algebra package Sigma; the
second is based on complex contour integrals. In many instances, these results
are extended to double sums of the above form where $\binom {2n}{n+j}$ is
replaced by $\binom {2m}{m+j}$ with independent parameter $m$.
"
925,Reconstruction Algorithms for Sums of Affine Powers,"  In this paper we study sums of powers of affine functions in (mostly) one
variable. Although quite simple, this model is a generalization of two
well-studied models: Waring decomposition and sparsest shift. For these three
models there are natural extensions to several variables, but this paper is
mostly focused on univariate polynomials. We present structural results which
compare the expressive power of the three models; and we propose algorithms
that find the smallest decomposition of f in the first model (sums of affine
powers) for an input polynomial f given in dense representation. We also begin
a study of the multivariate case. This work could be extended in several
directions. In particular, just as for Sparsest Shift and Waring decomposition,
one could consider extensions to ""supersparse"" polynomials and attempt a fuller
study of the multi-variate case. We also point out that the basic univariate
problem studied in the present paper is far from completely solved: our
algorithms all rely on some assumptions for the exponents in an optimal
decomposition, and some algorithms also rely on a distinctness assumption for
the shifts. It would be very interesting to weaken these assumptions, or even
to remove them entirely. Another related and poorly understood issue is that of
the bit size of the constants appearing in an optimal decomposition: is it
always polynomially related to the bit size of the input polynomial given in
dense representation?
"
926,Satisfiability Checking and Symbolic Computation,"  Symbolic Computation and Satisfiability Checking are viewed as individual
research areas, but they share common interests in the development,
implementation and application of decision procedures for arithmetic theories.
Despite these commonalities, the two communities are currently only weakly
connected. We introduce a new project SC-square to build a joint community in
this area, supported by a newly accepted EU (H2020-FETOPEN-CSA) project of the
same name. We aim to strengthen the connection between these communities by
creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap. This abstract and
accompanying poster describes the motivation and aims for the project, and
reports on the first activities.
"
927,Satisfiability Checking meets Symbolic Computation (Project Paper),"  Symbolic Computation and Satisfiability Checking are two research areas, both
having their individual scientific focus but sharing also common interests in
the development, implementation and application of decision procedures for
arithmetic theories. Despite their commonalities, the two communities are
rather weakly connected. The aim of our newly accepted SC-square project
(H2020-FETOPEN-CSA) is to strengthen the connection between these communities
by creating common platforms, initiating interaction and exchange, identifying
common challenges, and developing a common roadmap from theory along the way to
tools and (industrial) applications. In this paper we report on the aims and on
the first activities of this project, and formalise some relevant challenges
for the unified SC-square community.
"
928,Rational Solutions of Underdetermined Polynomial Equations,"  In this paper we report on an application of computer algebra in which
mathematical puzzles are generated of a type that had been widely used in
mathematics contests by a large number of participants worldwide.
  The algorithmic aspect of our work provides a method to compute rational
solutions of single polynomial equations that are typically large with $10^2
\ldots 10^5$ terms and that are heavily underdetermined. This functionality was
obtained by adding modules for a new type of splitting of equations to the
existing package CRACK that is normally used to solve polynomial algebraic and
differential systems.
"
929,"Using Machine Learning to Decide When to Precondition Cylindrical
  Algebraic Decomposition With Groebner Bases","  Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, particularly for quantifier elimination over real-closed
fields. However, it can be expensive, with worst case complexity doubly
exponential in the size of the input. Hence it is important to formulate the
problem in the best manner for the CAD algorithm. One possibility is to
precondition the input polynomials using Groebner Basis (GB) theory. Previous
experiments have shown that while this can often be very beneficial to the CAD
algorithm, for some problems it can significantly worsen the CAD performance.
  In the present paper we investigate whether machine learning, specifically a
support vector machine (SVM), may be used to identify those CAD problems which
benefit from GB preconditioning. We run experiments with over 1000 problems
(many times larger than previous studies) and find that the machine learned
choice does better than the human-made heuristic.
"
930,"Algorithms to solve coupled systems of differential equations in terms
  of power series","  Using integration by parts relations, Feynman integrals can be represented in
terms of coupled systems of differential equations. In the following we suppose
that the unknown Feynman integrals can be given in power series
representations, and that sufficiently many initial values of the integrals are
given. Then there exist algorithms that decide constructively if the
coefficients of their power series representations can be given within the
class of nested sums over hypergeometric products. In this article we will work
out the calculation steps that solve this problem. First, we will present a
successful tactic that has been applied recently to challenging problems coming
from massive 3-loop Feynman integrals. Here our main tool is to solve scalar
linear recurrences within the class of nested sums over hypergeometric
products. Second, we will present a new variation of this tactic which relies
on more involved summation technologies but succeeds in reducing the problem to
solve scalar recurrences with lower recurrence orders. The article will work
out the different challenges of this new tactic and demonstrates how they can
be treated efficiently with our existing summation technologies.
"
931,Congruences and Concurrent Lines in Multi-View Geometry,"  We present a new framework for multi-view geometry in computer vision. A
camera is a mapping between $\mathbb{P}^3$ and a line congruence. This model,
which ignores image planes and measurements, is a natural abstraction of
traditional pinhole cameras. It includes two-slit cameras, pushbroom cameras,
catadioptric cameras, and many more. We study the concurrent lines variety,
which consists of $n$-tuples of lines in $\mathbb{P}^3$ that intersect at a
point. Combining its equations with those of various congruences, we derive
constraints for corresponding images in multiple views. We also study
photographic cameras which use image measurements and are modeled as rational
maps from $\mathbb{P}^3$ to $\mathbb{P}^2$ or $\mathbb{P}^1\times
\mathbb{P}^1$.
"
932,"Conversion Methods for Improving Structural Analysis of
  Differential-Algebraic Equation Systems","  Differential-algebraic equation systems (DAEs) are generated routinely by
simulation and modeling environments. Before a simulation starts and a
numerical method is applied, some kind of structural analysis (SA) is used to
determine which equations to be differentiated, and how many times. Both
Pantelides's algorithm and Pryce's $\Sigma$-method are equivalent: if one of
them finds correct structural information, the other does also. Nonsingularity
of the Jacobian produced by SA indicates a success, which occurs on many
problems of interest. However, these methods can fail on simple, solvable DAEs
and give incorrect structural information including the index. This article
investigates $\Sigma$-method's failures and presents two conversion methods for
fixing them. Both methods convert a DAE on which the $\Sigma$-method fails to
an equivalent problem on which this SA is more likely to succeed.
"
933,"Conversion Methods, Block Triangularization, and Structural Analysis of
  Differential-Algebraic Equation Systems","  In a previous article, the authors developed two conversion methods to
improve the $\Sigma$-method for structural analysis (SA) of
differential-algebraic equations (DAEs). These methods reformulate a DAE on
which the $\Sigma$-method fails into an equivalent problem on which this SA is
more likely to succeed with a generically nonsingular Jacobian. The basic
version of these methods processes the DAE as a whole. This article presents
the block version that exploits block triangularization of a DAE. Using a block
triangular form of a Jacobian sparsity pattern, we identify which diagonal
blocks of the Jacobian are identically singular and then perform a conversion
on each such block. This approach improves the efficiency of finding a suitable
conversion for fixing SA's failures. All of our conversion methods can be
implemented in a computer algebra system so that every conversion can be
automated.
"
934,"Efficient algorithms for computing the Euler-Poincar\'e characteristic
  of symmetric semi-algebraic sets","  Let $\mathrm{R}$ be a real closed field and $\mathrm{D} \subset \mathrm{R}$
an ordered domain. We consider the algorithmic problem of computing the
generalized Euler-Poincar\'e characteristic of real algebraic as well as
semi-algebraic subsets of $\mathrm{R}^k$, which are defined by symmetric
polynomials with coefficients in $\mathrm{D}$. We give algorithms for computing
the generalized Euler-Poincar\'e characteristic of such sets, whose
complexities measured by the number the number of arithmetic operations in
$\mathrm{D}$, are polynomially bounded in terms of $k$ and the number of
polynomials in the input, assuming that the degrees of the input polynomials
are bounded by a constant. This is in contrast to the best complexity of the
known algorithms for the same problems in the non-symmetric situation, which
are singly exponential. This singly exponential complexity for the latter
problem is unlikely to be improved because of hardness result
($\#\mathbf{P}$-hardness) coming from discrete complexity theory.
"
935,Lopsided Approximation of Amoebas,"  The amoeba of a Laurent polynomial is the image of the corresponding
hypersurface under the coordinatewise log absolute value map. In this article,
we demonstrate that a theoretical amoeba approximation method due to Purbhoo
can be used efficiently in practice. To do this, we resolve the main bottleneck
in Purbhoo's method by exploiting relations between cyclic resultants. We use
the same approach to give an approximation of the Log preimage of the amoeba of
a Laurent polynomial using semi-algebraic sets. We also provide a SINGULAR/SAGE
implementation of these algorithms, which shows a significant speedup when our
specialized cyclic resultant computation is used, versus a general purpose
resultant algorithm.
"
936,Automatic Library Generation for Modular Polynomial Multiplication,"  Polynomial multiplication is a key algorithm underlying computer algebra
systems (CAS) and its efficient implementation is crucial for the performance
of CAS. In this paper we design and implement algorithms for polynomial
multiplication using approaches based the fast Fourier transform (FFT) and the
truncated Fourier transform (TFT). We improve on the state-of-the-art in both
theoretical and practical performance. The {\SPIRAL} library generation system
is extended and used to automatically generate and tune the performance of a
polynomial multiplication library that is optimized for memory hierarchy,
vectorization and multi-threading, using new and existing algorithms. The
performance tuning has been aided by the use of automation where many code
choices are generated and intelligent search is utilized to find the ""best""
implementation on a given architecture. The performance of autotuned
implementations is comparable to, and in some cases better than, the best
hand-tuned code.
"
937,"OpenSBLI: A framework for the automated derivation and parallel
  execution of finite difference solvers on a range of computer architectures","  Exascale computing will feature novel and potentially disruptive hardware
architectures. Exploiting these to their full potential is non-trivial.
Numerical modelling frameworks involving finite difference methods are
currently limited by the 'static' nature of the hand-coded discretisation
schemes and repeatedly may have to be re-written to run efficiently on new
hardware. In contrast, OpenSBLI uses code generation to derive the model's code
from a high-level specification. Users focus on the equations to solve, whilst
not concerning themselves with the detailed implementation. Source-to-source
translation is used to tailor the code and enable its execution on a variety of
hardware.
"
938,"A modular architecture for transparent computation in Recurrent Neural
  Networks","  Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.
"
939,"Proceedings 9th International Workshop on Computing with Terms and
  Graphs","  This volume contains the proceedings of TERMGRAPH 2016, the Ninth
International Workshop on Computing with Terms and Graphs which was held on
April 8, 2016 in Eindhoven, The Netherlands, as a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS 2016).
"
940,Some Open Problems related to Creative Telescoping,"  Creative telescoping is the method of choice for obtaining information about
definite sums or integrals. It has been intensively studied since the early
1990s, and can now be considered as a classical technique in computer algebra.
At the same time, it is still subject of ongoing research. In this paper, we
present a selection of open problems in this context. We would be curious to
hear about any substantial progress on any of these problems.
"
941,A Fast Algorithm for Computing the Truncated Resultant,"  Let P and Q be two polynomials in K[x, y] with degree at most d, where K is a
field. Denoting by R $\in$ K[x] the resultant of P and Q with respect to y, we
present an algorithm to compute R mod x^k in O~(kd) arithmetic operations in K,
where the O~ notation indicates that we omit polylogarithmic factors. This is
an improvement over state-of-the-art algorithms that require to compute R in
O~(d^3) operations before computing its first k coefficients.
"
942,"Some results on counting roots of polynomials and the Sylvester
  resultant","  We present two results, the first on the distribution of the roots of a
polynomial over the ring of integers modulo $n$ and the second on the
distribution of the roots of the Sylvester resultant of two multivariate
polynomials. The second result has application to polynomial GCD computation
and solving polynomial diophantine equations.
"
943,"Experience with Heuristics, Benchmarks & Standards for Cylindrical
  Algebraic Decomposition","  In the paper which inspired the SC-Square project, [E. Abraham, Building
Bridges between Symbolic Computation and Satisfiability Checking, Proc. ISSAC
'15, pp. 1-6, ACM, 2015] the author identified the use of sophisticated
heuristics as a technique that the Satisfiability Checking community excels in
and from which it is likely the Symbolic Computation community could learn and
prosper. To start this learning process we summarise our experience with
heuristic development for the computer algebra algorithm Cylindrical Algebraic
Decomposition. We also propose and discuss standards and benchmarks as another
area where Symbolic Computation could prosper from Satisfiability Checking
expertise, noting that these have been identified as initial actions for the
new SC-Square community in the CSA project, as described in [E.~Abraham et al.,
SC$^2$: Satisfiability Checking meets Symbolic Computation (Project Paper)},
Intelligent Computer Mathematics (LNCS 9761), pp. 28--43, Springer, 2015].
"
944,Algebraic and algorithmic aspects of radical parametrizations,"  In this article algebraic constructions are introduced in order to study the
variety defined by a radical parametrization (a tuple of functions involving
complex numbers, $n$ variables, the four field operations and radical
extractions). We provide algorithms to implicitize radical parametrizations and
to check whether a radical parametrization can be reparametrized into a
rational parametrization.
"
945,"Bounds for elimination of unknowns in systems of differential-algebraic
  equations","  Elimination of unknowns in systems of equations, starting with Gaussian
elimination, is a problem of general interest. The problem of finding an a
priori upper bound for the number of differentiations in elimination of
unknowns in a system of differential-algebraic equations (DAEs) is an important
challenge, going back to Ritt (1932). The first characterization of this via an
asymptotic analysis is due to Grigoriev's result (1989) on quantifier
elimination in differential fields, but the challenge still remained.
  In this paper, we present a new bound, which is a major improvement over the
previously known results. We also present a new lower bound, which shows
asymptotic tightness of our upper bound in low dimensions, which are frequently
occurring in applications. Finally, we discuss applications of our results to
designing new algorithms for elimination of unknowns in systems of DAEs.
"
946,Explicit equivalence of quadratic forms over $\mathbb{F}_q(t)$,"  We propose a randomized polynomial time algorithm for computing nontrivial
zeros of quadratic forms in 4 or more variables over $\mathbb{F}_q(t)$, where
$\mathbb{F}_q$ is a finite field of odd characteristic. The algorithm is based
on a suitable splitting of the form into two forms and finding a common value
they both represent. We make use of an effective formula for the number of
fixed degree irreducible polynomials in a given residue class. We apply our
algorithms for computing a Witt decomposition of a quadratic form, for
computing an explicit isometry between quadratic forms and finding zero
divisors in quaternion algebras over quadratic extensions of $\mathbb{F}_q(t)$.
"
947,"SPECTRA -- a Maple library for solving linear matrix inequalities in
  exact arithmetic","  This document describes our freely distributed Maple library {\sc spectra},
for Semidefinite Programming solved Exactly with Computational Tools of Real
Algebra. It solves linear matrix inequalities with symbolic computation in
exact arithmetic and it is targeted to small-size, possibly degenerate problems
for which symbolic infeasibility or feasibility certificates are required.
"
948,"Sparse multivariate factorization by mean of a few bivariate
  factorizations","  We describe an algorithm to factor sparse multivariate polynomials using O(d)
bivariate factorizations where d is the number of variables. This algorithm is
implemented in the Giac/Xcas computer algebra system.
"
949,Arb: Efficient Arbitrary-Precision Midpoint-Radius Interval Arithmetic,"  Arb is a C library for arbitrary-precision interval arithmetic using the
midpoint-radius representation, also known as ball arithmetic. It supports real
and complex numbers, polynomials, power series, matrices, and evaluation of
many special functions. The core number types are designed for versatility and
speed in a range of scenarios, allowing performance that is competitive with
non-interval arbitrary-precision types such as MPFR and MPC floating-point
numbers. We discuss the low-level number representation, strategies for
precision and error bounds, and the implementation of efficient polynomial
arithmetic with interval coefficients.
"
950,Efficient Parallel Verification of Galois Field Multipliers,"  Galois field (GF) arithmetic is used to implement critical arithmetic
components in communication and security-related hardware, and verification of
such components is of prime importance. Current techniques for formally
verifying such components are based on computer algebra methods that proved
successful in verification of integer arithmetic circuits. However, these
methods are sequential in nature and do not offer any parallelism. This paper
presents an algebraic functional verification technique of gate-level GF (2m )
multipliers, in which verification is performed in bit-parallel fashion. The
method is based on extracting a unique polynomial in Galois field of each
output bit independently. We demonstrate that this method is able to verify an
n-bit GF multiplier in n threads. Experiments performed on pre- and
post-synthesized Mastrovito and Montgomery multipliers show high efficiency up
to 571 bits.
"
951,D-finite Numbers,"  D-finite functions and P-recursive sequences are defined in terms of linear
differential and recurrence equations with polynomial coefficients. In this
paper, we introduce a class of numbers closely related to D-finite functions
and P-recursive sequences. It consists of the limits of convergent P-recursive
sequences. Typically, this class contains many well-known mathematical
constants in addition to the algebraic numbers. Our definition of the class of
D-finite numbers depends on two subrings of the field of complex numbers. We
investigate how different choices of these two subrings affect the class.
Moreover, we show that D-finite numbers are essentially limits of D-finite
functions at the point one, and evaluating D-finite functions at non-singular
algebraic points typically yields D-finite numbers. This result makes it easier
to recognize certain numbers to be D-finite.
"
952,"A note about ""Faster algorithms for computing Hong's bound on absolute
  positiveness"" by K. Mehlhorn and S. Ray","  We show that a linear-time algorithm for computing Hong's bound for positive
roots of a univariate polynomial, described by K. Mehlhorn and S. Ray in an
article ""Faster algorithms for computing Hong's bound on absolute
positiveness"", is incorrect. We present a corrected version.
"
953,Symmetries of Canal Surfaces and Dupin Cyclides,"  We develop a characterization for the existence of symmetries of canal
surfaces defined by a rational spine curve and rational radius function. In
turn, this characterization inspires an algorithm for computing the symmetries
of such canal surfaces. For Dupin cyclides in canonical form, we apply the
characterization to derive an intrinsic description of their symmetries and
symmetry groups, which gives rise to a method for computing the symmetries of a
Dupin cyclide not necessarily in canonical form. As a final application, we
discuss the construction of patches and blends of rational canal surfaces with
a prescribed symmetry.
"
954,Faster integer multiplication using plain vanilla FFT primes,"  Assuming a conjectural upper bound for the least prime in an arithmetic
progression, we show that n-bit integers may be multiplied in O(n log n
4^(log^* n)) bit operations.
"
955,Groebner Bases for Everyone with CoCoA-5 and CoCoALib,"  We present a survey on the developments related to Groebner bases, and show
explicit examples in CoCoA. The CoCoA project dates back to 1987: its aim was
to create a ""mathematician""-friendly computational laboratory for studying
Commutative Algebra, most especially Groebner bases. Always maintaining this
""friendly"" tradition, the project has grown and evolved, and the software has
been completely rewritten. CoCoA offers Groebner bases for all levels of
interest: from the basic, explicit call in the interactive system CoCoA-5, to
problem-specific optimized implementations, to the computer--computer
communication with the open source C++ software library, CoCoALib, or the
prototype OpenMath-based server. The openness and clean design of CoCoALib and
CoCoA-5 are intended to offer different levels of usage, and to encourage
external contributions.
"
956,Reduction-Based Creative Telescoping for Fuchsian D-finite Functions,"  Continuing a series of articles in the past few years on creative telescoping
using reductions, we adapt Trager's Hermite reduction for algebraic functions
to fuchsian D-finite functions and develop a reduction-based creative
telescoping algorithm for this class of functions, thereby generalizing our
recent reduction-based algorithm for algebraic functions, presented at ISSAC
2016.
"
957,"Symbolic Representation for Analog Realization of A Family of Fractional
  Order Controller Structures via Continued Fraction Expansion","  This paper uses the Continued Fraction Expansion (CFE) method for analog
realization of fractional order differ-integrator and few special classes of
fractional order (FO) controllers viz. Fractional Order
Proportional-Integral-Derivative (FOPID) controller, FO[PD] controller and FO
lead-lag compensator. Contemporary researchers have given several formulations
for rational approximation of fractional order elements. However, approximation
of the controllers studied in this paper, due to having fractional power of a
rational transfer function, is not available in analog domain; although its
digital realization already exists. This motivates us for applying CFE based
analog realization technique for complicated FO controller structures to get
equivalent rational transfer functions in terms of the controller tuning
parameters. The symbolic expressions for rationalized transfer function in
terms of the controller tuning parameters are especially important as ready
references, without the need of running CFE algorithm every time and also helps
in the synthesis of analog circuits for such FO controllers.
"
958,Computer Algebra and Material Design,"  This article is intended to an introductory lecture in material physics, in
which the modern computational group theory and the electronic structure
calculation are in collaboration. The effort of mathematicians in field of the
group theory, have ripened as a new trend, called ""computer algebra"", outcomes
of which now can be available as handy computational packages, and would also
be useful to physicists with practical purposes. This article, in the former
part, explains how to use the computer algebra for the applications in the
solid-state simulation, by means of one of the computer algebra package, the
GAP system. The computer algebra enables us to obtain various group theoretical
properties with ease, such as the representations, the character tables, the
subgroups, etc. Furthermore it would grant us a new perspective of material
design, which could be executed in mathematically rigorous and systematic way.
Some technical details and some computations which require the knowledge of a
little higher mathematics (but computable easily by the computer algebra) are
also given. The selected topics will provide the reader with some insights
toward the dominating role of the symmetry in crystal, or, the ""mathematical
first principles"" in it. In the latter part of the article, we analyze the
relation between the structural symmetry and the electronic structure in
C$_{60}$ (as an example to the sysmem without periodicity). The principal
object of the study is to illustrate the hierarchical change of the
quantum-physical properties of the molecule, in accordance with the reduction
of the symmetry (as it descends down in the ladder of subgroups). In order to
serve the common interest of the researchers, the details of the computations
(the required initial data and the small programs developed for the purpose)
are explained as minutely as possible.
"
959,"Automatic Differentiation: a look through Tensor and Operational
  Calculus","  In this paper we take a look at Automatic Differentiation through the eyes of
Tensor and Operational Calculus. This work is best consumed as supplementary
material for learning tensor and operational calculus by those already familiar
with automatic differentiation. To that purpose, we provide a simple
implementation of automatic differentiation, where the steps taken are
explained in the language tensor and operational calculus.
"
960,Baby-Step Giant-Step Algorithms for the Symmetric Group,"  We study discrete logarithms in the setting of group actions. Suppose that
$G$ is a group that acts on a set $S$. When $r,s \in S$, a solution $g \in G$
to $r^g = s$ can be thought of as a kind of logarithm. In this paper, we study
the case where $G = S_n$, and develop analogs to the Shanks baby-step /
giant-step procedure for ordinary discrete logarithms. Specifically, we compute
two sets $A, B \subseteq S_n$ such that every permutation of $S_n$ can be
written as a product $ab$ of elements $a \in A$ and $b \in B$. Our
deterministic procedure is optimal up to constant factors, in the sense that
$A$ and $B$ can be computed in optimal asymptotic complexity, and $|A|$ and
$|B|$ are a small constant from $\sqrt{n!}$ in size. We also analyze randomized
""collision"" algorithms for the same problem.
"
961,"A-Discriminants for Complex Exponents, and Counting Real Isotopy Types","  We extend the definition of $\mathcal{A}$-discriminant varieties, and
Kapranov's parametrization of $\mathcal{A}$-discriminant varieties, to complex
exponents. As an application, we study the special case where $\mathcal{A}$ is
a fixed real $n\times (n+3)$ matrix whose columns form the spectrum of an
$n$-variate exponential sum $g$ with fixed sign vector for its coefficients: We
prove that the number of possible isotopy types for the real zero set of $g$ is
$O(n^2)$. The best previous upper bound was $2^{O(n^4)}$. Along the way, we
also show that the singular loci of our generalized $\mathcal{A}$-discriminants
are images of low-degree algebraic sets under certain analytic maps.
"
962,Reverse Engineering of Irreducible Polynomials in GF(2^m) Arithmetic,"  Current techniques for formally verifying circuits implemented in Galois
field (GF) arithmetic are limited to those with a known irreducible polynomial
P(x). This paper presents a computer algebra based technique that extracts the
irreducible polynomial P(x) used in the implementation of a multiplier in
GF(2^m). The method is based on first extracting a unique polynomial in Galois
field of each output bit independently. P(x) is then obtained by analyzing the
algebraic expression in GF(2^m) of each output bit. We demonstrate that this
method is able to reverse engineer the irreducible polynomial of an n-bit GF
multiplier in n threads. Experiments were performed on Mastrovito and
Montgomery multipliers with different P (x), including NIST-recommended
polynomials and optimal polynomials for different microprocessor architectures.
"
963,"The Method of Gauss-Newton to Compute Power Series Solutions of
  Polynomial Homotopies","  We consider the extension of the method of Gauss-Newton from complex
floating-point arithmetic to the field of truncated power series with complex
floating-point coefficients. With linearization we formulate a linear system
where the coefficient matrix is a series with matrix coefficients, and provide
a characterization for when the matrix series is regular based on the algebraic
variety of an augmented system. The structure of the linear system leads to a
block triangular system. In the regular case, solving the linear system is
equivalent to solving a Hermite interpolation problem. We show that this
solution has cost cubic in the problem size. In general, at singular points, we
rely on methods of tropical algebraic geometry to compute Puiseux series. With
a few illustrative examples, we demonstrate the application to polynomial
homotopy continuation.
"
964,Efficient sparse polynomial factoring using the Funnel heap,"  This work is a comprehensive extension of Abu-Salem et al. (2015) that
investigates the prowess of the Funnel Heap for implementing sums of products
in the polytope method for factoring polynomials, when the polynomials are in
sparse distributed representation. We exploit that the work and cache
complexity of an Insert operation using Funnel Heap can be refined to de- pend
on the rank of the inserted monomial product, where rank corresponds to its
lifetime in Funnel Heap. By optimising on the pattern by which insertions and
extractions occur during the Hensel lifting phase of the polytope method, we
are able to obtain an adaptive Funnel Heap that minimises all of the work,
cache, and space complexity of this phase. Additionally, we conduct a detailed
empirical study confirming the superiority of Funnel Heap over the generic
Binary Heap once swaps to external memory begin to take place. We demonstrate
that Funnel Heap is a more efficient merger than the cache oblivious k-merger,
which fails to achieve its optimal (and amortised) cache complexity when used
for performing sums of products. This provides an empirical proof of concept
that the overlapping approach for perform- ing sums of products using one
global Funnel Heap is more suited than the serialised approach, even when the
latter uses the best merging structures available.
"
965,Computing solutions of linear Mahler equations,"  Mahler equations relate evaluations of the same function $f$ at iterated
$b$th powers of the variable. They arise in particular in the study of
automatic sequences and in the complexity analysis of divide-and-conquer
algorithms. Recently, the problem of solving Mahler equations in closed form
has occurred in connection with number-theoretic questions. A difficulty in the
manipulation of Mahler equations is the exponential blow-up of degrees when
applying a Mahler operator to a polynomial. In this work, we present algorithms
for solving linear Mahler equations for series, polynomials, and rational
functions, and get polynomial-time complexity under a mild assumption.
Incidentally, we develop an algorithm for computing the gcrd of a family of
linear Mahler operators.
"
966,Fast Matrix Multiplication and Symbolic Computation,"  The complexity of matrix multiplication (hereafter MM) has been intensively
studied since 1969, when Strassen surprisingly decreased the exponent 3 in the
cubic cost of the straightforward classical MM to log 2 (7) $\approx$ 2.8074.
Applications to some fundamental problems of Linear Algebra and Computer
Science have been immediately recognized, but the researchers in Computer
Algebra keep discovering more and more applications even today, with no sign of
slowdown. We survey the unfinished history of decreasing the exponent towards
its information lower bound 2, recall some important techniques discovered in
this process and linked to other fields of computing, reveal sample surprising
applications to fast computation of the inner products of two vectors and
summation of integers, and discuss the curse of recursion, which separates the
progress in fast MM into its most acclaimed and purely theoretical part and
into valuable acceleration of MM of feasible sizes. Then, in the second part of
our paper, we cover fast MM in realistic symbolic computations and discuss
applications and implementation of fast exact matrix multiplication. We first
review how most of exact linear algebra can be reduced to matrix multiplication
over small finite fields. Then we highlight the differences in the design of
approximate and exact implementations of fast MM, taking into account nowadays
processor and memory hierarchies. In the concluding section we comment on
current perspectives of the study of fast MM.
"
967,Parallel Integer Polynomial Multiplication,"  We propose a new algorithm for multiplying dense polynomials with integer
coefficients in a parallel fashion, targeting multi-core processor
architectures. Complexity estimates and experimental comparisons demonstrate
the advantages of this new approach.
"
968,"Comparative study of space filling curves for cache oblivious TU
  Decomposition","  We examine several matrix layouts based on space-filling curves that allow
for a cache-oblivious adaptation of parallel TU decomposition for rectangular
matrices over finite fields. The TU algorithm of \cite{Dumas} requires index
conversion routines for which the cost to encode and decode the chosen curve is
significant. Using a detailed analysis of the number of bit operations required
for the encoding and decoding procedures, and filtering the cost of lookup
tables that represent the recursive decomposition of the Hilbert curve, we show
that the Morton-hybrid order incurs the least cost for index conversion
routines that are required throughout the matrix decomposition as compared to
the Hilbert, Peano, or Morton orders. The motivation lies in that cache
efficient parallel adaptations for which the natural sequential evaluation
order demonstrates lower cache miss rate result in overall faster performance
on parallel machines with private or shared caches, on GPU's, or even cloud
computing platforms. We report on preliminary experiments that demonstrate how
the TURBO algorithm in Morton-hybrid layout attains orders of magnitude
improvement in performance as the input matrices increase in size. For example,
when $N = 2^{13}$, the row major TURBO algorithm concludes within about 38.6
hours, whilst the Morton-hybrid algorithm with truncation size equal to $64$
concludes within 10.6 hours.
"
969,Computing in quotients of rings of integers,"  We develop algorithms to turn quotients of rings of rings of integers into
effective Euclidean rings by giving polynomial algorithms for all fundamental
ring operations. In addition, we study normal forms for modules over such rings
and their behavior under certain quotients. We illustrate the power of our
ideas in a new modular normal form algorithm for modules over rings of
integers, vastly outperforming classical algorithms.
"
970,"On the computation of the HNF of a module over the ring of integers of a
  number field","  We present a variation of the modular algorithm for computing the Hermite
normal form of an $\mathcal O_K$-module presented by Cohen, where $\mathcal
O_K$ is the ring of integers of a number field $K$. An approach presented in
(Cohen 1996) based on reductions modulo ideals was conjectured to run in
polynomial time by Cohen, but so far, no such proof was available in the
literature. In this paper, we present a modification of the approach of Cohen
to prevent the coefficient swell and we rigorously assess its complexity with
respect to the size of the input and the invariants of the field $K$.
"
971,Time and space efficient generators for quasiseparable matrices,"  The class of quasiseparable matrices is defined by the property that any
submatrix entirely below or above the main diagonal has small rank, namely
below a bound called the order of quasiseparability. These matrices arise
naturally in solving PDE's for particle interaction with the Fast Multi-pole
Method (FMM), or computing generalized eigenvalues. From these application
fields, structured representations and algorithms have been designed in
numerical linear algebra to compute with these matrices in time linear in the
matrix dimension and either quadratic or cubic in the quasiseparability order.
Motivated by the design of the general purpose exact linear algebra library
LinBox, and by algorithmic applications in algebraic computing, we adapt
existing techniques introduce novel ones to use quasiseparable matrices in
exact linear algebra, where sub-cubic matrix arithmetic is available. In
particular, we will show, the connection between the notion of
quasiseparability and the rank profile matrix invariant, that we have
introduced in 2015. It results in two new structured representations, one being
a simpler variation on the hierarchically semiseparable storage, and the second
one exploiting the generalized Bruhat decomposition. As a consequence, most
basic operations, such as computing the quasiseparability orders, applying a
vector, a block vector, multiplying two quasiseparable matrices together,
inverting a quasiseparable matrix, can be at least as fast and often faster
than previous existing algorithms.
"
972,"Computing Approximate Greatest Common Right Divisors of Differential
  Polynomials","  Differential (Ore) type polynomials with ""approximate"" polynomial
coefficients are introduced. These provide an effective notion of approximate
differential operators, with a strong algebraic structure. We introduce the
approximate Greatest Common Right Divisor Problem (GCRD) of differential
polynomials, as a non-commutative generalization of the well-studied
approximate GCD problem.
  Given two differential polynomials, we present an algorithm to find nearby
differential polynomials with a non-trivial GCRD, where nearby is defined with
respect to a suitable coefficient norm. Intuitively, given two linear
differential polynomials as input, the (approximate) GCRD problem corresponds
to finding the (approximate) differential polynomial whose solution space is
the intersection of the solution spaces of the two inputs.
  The approximate GCRD problem is proven to be locally well-posed. A method
based on the singular value decomposition of a differential Sylvester matrix is
developed to produce an initial approximation of the GCRD. With a sufficiently
good initial approximation, Newton iteration is shown to converge quadratically
to an optimal solution. Finally, sufficient conditions for existence of a
solution to the global problem are presented along with examples demonstrating
that no solution exists when these conditions are not satisfied.
"
973,Semialgebraic Invariant Synthesis for the Kannan-Lipton Orbit Problem,"  The \emph{Orbit Problem} consists of determining, given a linear
transformation $A$ on $\mathbb{Q}^d$, together with vectors $x$ and $y$,
whether the orbit of $x$ under repeated applications of $A$ can ever reach $y$.
This problem was famously shown to be decidable by Kannan and Lipton in the
1980s.
  In this paper, we are concerned with the problem of synthesising suitable
\emph{invariants} $\mathcal{P} \subseteq \mathbb{R}^d$, \emph{i.e.}, sets that
are stable under $A$ and contain $x$ and not $y$, thereby providing compact and
versatile certificates of non-reachability. We show that whether a given
instance of the Orbit Problem admits a semialgebraic invariant is decidable,
and moreover in positive instances we provide an algorithm to synthesise
suitable invariants of polynomial size.
  It is worth noting that the existence of \emph{semilinear} invariants, on the
other hand, is (to the best of our knowledge) not known to be decidable.
"
974,Functional Decomposition using Principal Subfields,"  Let $f\in K(t)$ be a univariate rational function. It is well known that any
non-trivial decomposition $g \circ h$, with $g,h\in K(t)$, corresponds to a
non-trivial subfield $K(f(t))\subsetneq L \subsetneq K(t)$ and vice-versa. In
this paper we use the idea of principal subfields and fast
subfield-intersection techniques to compute the subfield lattice of
$K(t)/K(f(t))$. This yields a Las Vegas type algorithm with improved complexity
and better run times for finding all non-equivalent complete decompositions of
$f$.
"
975,On Bezout Inequalities for non-homogeneous Polynomial Ideals,"  We introduce a ""workable"" notion of degree for non-homogeneous polynomial
ideals and formulate and prove ideal theoretic B\'ezout Inequalities for the
sum of two ideals in terms of this notion of degree and the degree of
generators. We compute probabilistically the degree of an equidimensional
ideal.
"
976,"The Method of Arbitrarily Large Moments to Calculate Single Scale
  Processes in Quantum Field Theory","  We device a new method to calculate a large number of Mellin moments of
single scale quantities using the systems of differential and/or difference
equations obtained by integration-by-parts identities between the corresponding
Feynman integrals of loop corrections to physical quantities. These scalar
quantities have a much simpler mathematical structure than the complete
quantity. A sufficiently large set of moments may even allow the analytic
reconstruction of the whole quantity considered, holding in case of first order
factorizing systems. In any case, one may derive highly precise numerical
representations in general using this method, which is otherwise completely
analytic.
"
977,The number of realizations of a Laman graph,"  Laman graphs model planar frameworks that are rigid for a general choice of
distances between the vertices. There are finitely many ways, up to isometries,
to realize a Laman graph in the plane. Such realizations can be seen as
solutions of systems of quadratic equations prescribing the distances between
pairs of points. Using ideas from algebraic and tropical geometry, we provide a
recursive formula for the number of complex solutions of such systems.
"
978,"Criteria for Finite Difference Groebner Bases of Normal Binomial
  Difference Ideals","  In this paper, we give decision criteria for normal binomial difference
polynomial ideals in the univariate difference polynomial ring F{y} to have
finite difference Groebner bases and an algorithm to compute the finite
difference Groebner bases if these criteria are satisfied. The novelty of these
criteria lies in the fact that complicated properties about difference
polynomial ideals are reduced to elementary properties of univariate
polynomials in Z[x].
"
979,Computations with p-adic numbers,"  This document contains the notes of a lecture I gave at the ""Journ\'ees
Nationales du Calcul Formel"" (JNCF) on January 2017. The aim of the lecture was
to discuss low-level algorithmics for p-adic numbers. It is divided into two
main parts: first, we present various implementations of p-adic numbers and
compare them and second, we introduce a general framework for studying
precision issues and apply it in several concrete situations.
"
980,Bounds for Substituting Algebraic Functions into D-finite Functions,"  It is well known that the composition of a D-finite function with an
algebraic function is again D-finite. We give the first estimates for the
orders and the degrees of annihilating operators for the compositions. We find
that the analysis of removable singularities leads to an order-degree curve
which is much more accurate than the order-degree curve obtained from the usual
linear algebra reasoning.
"
981,"A proof of Hilbert's theorem on ternary quartic forms with the ladder
  technique","  This paper proposes a totally constructive approach for the proof of
Hilbert's theorem on ternary quartic forms. The main contribution is the ladder
technique, with which the Hilbert's theorem is proved vividly.
"
982,Riemann Tensor Polynomial Canonicalization by Graph Algebra Extension,"  Tensor expression simplification is an ""ancient"" topic in computer algebra, a
representative of which is the canonicalization of Riemann tensor polynomials.
Practically fast algorithms exist for monoterm canonicalization, but not for
multiterm canonicalization. Targeting the multiterm difficulty, in this paper
we establish the extension theory of graph algebra, and propose a
canonicalization algorithm for Riemann tensor polynomials based on this theory.
"
983,Characteristic polynomials of p-adic matrices,"  We analyze the precision of the characteristic polynomial of an $n\times n$
p-adic matrix A using differential precision methods developed previously. When
A is integral with precision O(p^N), we give a criterion (checkable in time
O~(n^omega)) for $\chi$(A) to have precision exactly O(p^N). We also give a
O~(n^3) algorithm for determining the optimal precision when the criterion is
not satisfied, and give examples when the precision is larger than O(p^N).
"
984,Fast multiplication for skew polynomials,"  We describe an algorithm for fast multiplication of skew polynomials. It is
based on fast modular multiplication of such skew polynomials, for which we
give an algorithm relying on evaluation and interpolation on normal bases. Our
algorithms improve the best known complexity for these problems, and reach the
optimal asymptotic complexity bound for large degree. We also give an
adaptation of our algorithm for polynomials of small degree. Finally, we use
our methods to improve on the best known complexities for various arithmetics
problems.
"
985,Discriminants of complete intersection space curves,"  In this paper, we develop a new approach to the discrimi-nant of a complete
intersection curve in the 3-dimensional projective space. By relying on the
resultant theory, we first prove a new formula that allows us to define this
discrimi-nant without ambiguity and over any commutative ring, in particular in
any characteristic. This formula also provides a new method for evaluating and
computing this discrimi-nant efficiently, without the need to introduce new
variables as with the well-known Cayley trick. Then, we obtain new properties
and computational rules such as the covariance and the invariance formulas.
Finally, we show that our definition of the discriminant satisfies to the
expected geometric property and hence yields an effective smoothness criterion
for complete intersection space curves. Actually, we show that in the generic
setting, it is the defining equation of the discriminant scheme if the ground
ring is assumed to be a unique factorization domain.
"
986,Certificates for triangular equivalence and rank profiles,"  In this paper, we give novel certificates for triangular equivalence and rank
profiles. These certificates enable to verify the row or column rank profiles
or the whole rank profile matrix faster than recomputing them, with a
negligible overall overhead. We first provide quadratic time and space
non-interactive certificates saving the logarithmic factors of previously known
ones. Then we propose interactive certificates for the same problems whose
Monte Carlo verification complexity requires a small constant number of
matrix-vector multiplications, a linear space, and a linear number of extra
field operations. As an application we also give an interactive protocol,
certifying the determinant of dense matrices, faster than the best previously
known one.
"
987,"Algorithmic Verification of Linearizability for Ordinary Differential
  Equations","  For a nonlinear ordinary differential equation solved with respect to the
highest order derivative and rational in the other derivatives and in the
independent variable, we devise two algorithms to check if the equation can be
reduced to a linear one by a point transformation of the dependent and
independent variables. The first algorithm is based on a construction of the
Lie point symmetry algebra and on the computation of its derived algebra. The
second algorithm exploits the differential Thomas decomposition and allows not
only to test the linearizability, but also to generate a system of nonlinear
partial differential equations that determines the point transformation and the
coefficients of the linearized equation. Both algorithms have been implemented
in Maple and their application is illustrated using several examples.
"
988,Hybrid System Modelling and Simulation with Dirac Deltas,"  For a wide variety of problems, creating detailed continuous models of
(continuous) physical systems is, at the very least, impractical. Hybrid models
can abstract away short transient behaviour (thus introducing discontinuities)
in order to simplify the study of such systems. For example, when modelling a
bouncing ball, the bounce can be abstracted as a discontinuous change of the
velocity, instead of resorting to the physics of the ball (de-)compression to
keep the velocity signal continuous. Impulsive differential equations can be
used to model and simulate hybrid systems such as the bouncing ball. In this
approach, the force acted on the ball by the floor is abstracted as an
infinitely large function in an infinitely small interval of time, that is, an
impulse. Current simulators cannot handle such approximations well due to the
limitations of machine precision.
  In this paper, we explore the simulation of impulsive differential equations,
where impulses are first class citizens. We present two approaches for the
simulation of impulses: symbolic and numerical. Our contribution is a
theoretically founded description of the implementation of both approaches in a
Causal Block Diagram modelling and simulation tool. Furthermore, we investigate
the conditions for which one approach is better than the other.
"
989,Bad Primes in Computational Algebraic Geometry,"  Computations over the rational numbers often suffer from intermediate
coefficient swell. One solution to this problem is to apply the given algorithm
modulo a number of primes and then lift the modular results to the rationals.
This method is guaranteed to work if we use a sufficiently large set of good
primes. In many applications, however, there is no efficient way of excluding
bad primes. In this note, we describe a technique for rational reconstruction
which will nevertheless return the correct result, provided the number of good
primes in the selected set of primes is large enough. We give a number of
illustrating examples which are implemented using the computer algebra system
Singular and the programming language Julia. We discuss applications of our
technique in computational algebraic geometry.
"
990,"Algorithm for computing semi-Fourier sequences of expressions involving
  exponentiations and integrations","  We provide an algorithm for computing semi-Fourier sequences for expressions
constructed from arithmetic operations, exponentiations and integrations. The
semi-Fourier sequence is a relaxed version of Fourier sequence for polynomials
(expressions made of additions and multiplications).
"
991,Fast generalized Bruhat decomposition,"  The deterministic recursive pivot-free algorithms for the computation of
generalized Bruhat decomposition of the matrix in the field and for the
computation of the inverse matrix are presented. This method has the same
complexity as algorithm of matrix multiplication and it is suitable for the
parallel computer systems.
"
992,Triangular Decomposition of Matrices in a Domain,"  Deterministic recursive algorithms for the computation of matrix triangular
decompositions with permutations like LU and Bruhat decomposition are presented
for the case of commutative domains. This decomposition can be considered as a
generalization of LU and Bruhat decompositions, because they both may be easily
obtained from this triangular decomposition. Algorithms have the same
complexity as the algorithm of matrix multiplication.
"
993,Generalized Bruhat decomposition in commutative domains,"  Deterministic recursive algorithms for the computation of generalized Bruhat
decomposition of the matrix in commutative domain are presented. This method
has the same complexity as the algorithm of matrix multiplication.
"
994,Computing and Using Minimal Polynomials,"  Given a zero-dimensional ideal I in a polynomial ring, many computations
start by finding univariate polynomials in I. Searching for a univariate
polynomial in I is a particular case of considering the minimal polynomial of
an element in P/I. It is well known that minimal polynomials may be computed
via elimination, therefore this is considered to be a ""resolved problem"". But
being the key of so many computations, it is worth investigating its meaning,
its optimization, its applications (e.g. testing if a zero-dimensional ideal is
radical, primary or maximal). We present efficient algorithms for computing the
minimal polynomial of an element of P/I. For the specific case where the
coefficients are in Q, we show how to use modular methods to obtain a
guaranteed result. We also present some applications of minimal polynomials,
namely algorithms for computing radicals and primary decompositions of
zero-dimensional ideals, and also for testing radicality and maximality.
"
995,"The natural algorithmic approach of mixed trigonometric-polynomial
  problems","  The aim of this paper is to present a new algorithm for proving mixed
trigonometric-polynomial inequalities by reducing to polynomial inequalities.
Finally, we show the great applicability of this algorithm and as examples, we
use it to analyze some new rational (Pade) approximations of the function
$\cos^2(x)$, and to improve a class of inequalities by Z.-H. Yang. The results
of our analysis could be implemented by means of an automated proof assistant,
so our work is a contribution to the library of automatic support tools for
proving various analytic inequalities.
"
996,Faster Tensor Canonicalization,"  The Butler-Portugal algorithm for obtaining the canonical form of a tensor
expression with respect to slot symmetries and dummy-index renaming suffers, in
certain cases with a high degree of symmetry, from $O(n!)$ explosion in both
computation time and memory. We present a modified algorithm which alleviates
this problem in the most common cases---tensor expressions with subsets of
indices which are totally symmetric or totally antisymmetric---in polynomial
time. We also present an implementation of the label-renaming mechanism which
improves upon that of the original Butler-Portugal algorithm, thus providing a
significant speed increase for the average case as well as the highly-symmetric
special case. The worst-case behavior remains $O(n!)$, although it occurs in
more limited situations unlikely to appear in actual computations. We comment
on possible strategies to take if the nature of a computation should make these
situations more likely.
"
997,Decomposition of polynomial sets into characteristic pairs,"  A characteristic pair is a pair (G,C) of polynomial sets in which G is a
reduced lexicographic Groebner basis, C is the minimal triangular set contained
in G, and C is normal. In this paper, we show that any finite polynomial set P
can be decomposed algorithmically into finitely many characteristic pairs with
associated zero relations, which provide representations for the zero set of P
in terms of those of Groebner bases and those of triangular sets. The algorithm
we propose for the decomposition makes use of the inherent connection between
Ritt characteristic sets and lexicographic Groebner bases and is based
essentially on the structural properties and the computation of lexicographic
Groebner bases. Several nice properties about the decomposition and the
resulting characteristic pairs, in particular relationships between the
Groebner basis and the triangular set in each pair, are established. Examples
are given to illustrate the algorithm and some of the properties.
"
998,Faster truncated integer multiplication,"  We present new algorithms for computing the low n bits or the high n bits of
the product of two n-bit integers. We show that these problems may be solved in
asymptotically 75% of the time required to compute the full 2n-bit product,
assuming that the underlying integer multiplication algorithm relies on
computing cyclic convolutions of real sequences.
"
999,Symbolic Solutions of Simultaneous First-order PDEs in One Unknown,"  We propose and implement an algorithm for solving an overdetermined system of
partial differential equations in one unknown. Our approach relies on
Bour-Mayer method to determine compatibility conditions via Jacobi-Mayer
brackets. We solve compatible systems recursively by imitating what one would
do with pen and paper: Solve one equation, substitute the solution into the
remaining equations and iterate the process until the equations of the system
are exhausted. The method we employ for assessing the consistency of the
underlying system differs from the traditional use of differential Gr\""obner
bases yet seems more efficient and straightforward to implement. We are not
aware of a computer algebra system that adopts the procedure we advocate in
this work.
"
1000,A lattice formulation of the F4 completion procedure,"  We write a procedure for constructing noncommutative Groebner bases.
Reductions are done by particular linear projectors, called reduction
operators. The operators enable us to use a lattice construction to reduce
simultaneously each S-polynomial into a unique normal form. We write an
implementation as well as an example to illustrate our procedure. Moreover, the
lattice construction is done by Gaussian elimination, which relates our
procedure to the F4 algorithm for constructing commutative Groebner bases.
"
1001,"On matrices with displacement structure: generalized operators and
  faster algorithms","  For matrices with displacement structure, basic operations like
multiplication, inversion, and linear system solving can all be expressed in
terms of the following task: evaluate the product $\mathsf{A}\mathsf{B}$, where
$\mathsf{A}$ is a structured $n \times n$ matrix of displacement rank $\alpha$,
and $\mathsf{B}$ is an arbitrary $n\times\alpha$ matrix. Given $\mathsf{B}$ and
a so-called ""generator"" of $\mathsf{A}$, this product is classically computed
with a cost ranging from $O(\alpha^2 \mathscr{M}(n))$ to $O(\alpha^2
\mathscr{M}(n)\log(n))$ arithmetic operations, depending on the type of
structure of $\mathsf{A}$; here, $\mathscr{M}$ is a cost function for
polynomial multiplication. In this paper, we first generalize classical
displacement operators, based on block diagonal matrices with companion
diagonal blocks, and then design fast algorithms to perform the task above for
this extended class of structured matrices. The cost of these algorithms ranges
from $O(\alpha^{\omega-1} \mathscr{M}(n))$ to $O(\alpha^{\omega-1}
\mathscr{M}(n)\log(n))$, with $\omega$ such that two $n \times n$ matrices over
a field can be multiplied using $O(n^\omega)$ field operations. By combining
this result with classical randomized regularization techniques, we obtain
faster Las Vegas algorithms for structured inversion and linear system solving.
"
1002,Orbital Graphs,"  We introduce orbital graphs and discuss some of their basic properties. Then
we focus on their usefulness for search algorithms for permutation groups,
including finding the intersection of groups and the stabilizer of sets in a
group.
"
1003,A clever elimination strategy for efficient minimal solvers,"  We present a new insight into the systematic generation of minimal solvers in
computer vision, which leads to smaller and faster solvers. Many minimal
problem formulations are coupled sets of linear and polynomial equations where
image measurements enter the linear equations only. We show that it is useful
to solve such systems by first eliminating all the unknowns that do not appear
in the linear equations and then extending solutions to the rest of unknowns.
This can be generalized to fully non-linear systems by linearization via
lifting. We demonstrate that this approach leads to more efficient solvers in
three problems of partially calibrated relative camera pose computation with
unknown focal length and/or radial distortion. Our approach also generates new
interesting constraints on the fundamental matrices of partially calibrated
cameras, which were not known before.
"
1004,Ultimate Positivity of Diagonals of Quasi-rational Functions,"  The problem to decide whether a given multivariate (quasi-)rational function
has only positive coefficients in its power series expansion has a long
history. It dates back to Szego in 1933 who showed certain quasi-rational
function to be positive, in the sense that all the series coefficients are
positive, using an involved theory of special functions. In contrast to the
simplicity of the statement, the method was surprisingly difficult. This
dependency motivated further research for positivity of (quasi-)rational
functions. More and more (quasi-)rational functions have been proven to be
positive, and some of the proofs are even quite simple. However, there are also
others whose positivity are still open conjectures. In this talk, we focus on a
less difficult but also interesting question to decide whether the diagonal of
a given quasi-rational function is ultimately positive, especially for the one
conjectured to be positive by Kauers in 2007. To solve this question, it
suffices to compute the asymptotics of the diagonal coefficients, which can be
done by the multivariate singularity analysis developed by Baryshnikov,
Pemantle and Wilson. Note that the ultimate positivity is a necessary condition
for the positivity, and therefore can be used to either exclude the nonpositive
cases or further support the conjectural positivity.
"
1005,Roots multiplicity without companion matrices,"  We show a method for constructing a polynomial interpolating roots'
multiplicities of another polynomial, that does not use companion matrices.
This leads to a modification to Guersenzvaig--Szechtman square-free
decomposition algorithm that is more efficient both in theory and in practice.
"
1006,"Laderman matrix multiplication algorithm can be constructed using
  Strassen algorithm and related tensor's isotropies","  In 1969, V. Strassen improves the classical~2x2 matrix multiplication
algorithm. The current upper bound for 3x3 matrix multiplication was reached by
J.B. Laderman in 1976. This note presents a geometric relationship between
Strassen and Laderman algorithms. By doing so, we retrieve a geometric
formulation of results very similar to those presented by O. Sykora in 1977.
"
1007,Recursive Method for the Solution of Systems of Linear Equations,"  New solution method for the systems of linear equations in commutative
integral domains is proposed. Its complexity is the same that the complexity of
the matrix multiplication.
"
1008,"Prover efficient public verification of dense or sparse/structured
  matrix-vector multiplication","  With the emergence of cloud computing services, computationally weak devices
(Clients) can delegate expensive tasks to more powerful entities (Servers).
This raises the question of verifying a result at a lower cost than that of
recomputing it. This verification can be private, between the Client and the
Server, or public, when the result can be verified by any third party. We here
present protocols for the verification of matrix-vector multiplications, that
are secure against malicious Servers. The obtained algorithms are essentially
optimal in the amortized model: the overhead for the Server is limited to a
very small constant factor, even in the sparse or structured matrix case; and
the computational time for the public Verifier is linear in the dimension. Our
protocols combine probabilistic checks and cryptographic operations, but
minimize the latter to preserve practical efficiency. Therefore our protocols
are overall more than two orders of magnitude faster than existing ones.
"
1009,"Modular Techniques For Noncommutative Gr\""obner Bases","  In this note, we extend modular techniques for computing Gr\""obner bases from
the commutative setting to the vast class of noncommutative $G$-algebras. As in
the commutative case, an effective verification test is only known to us in the
graded case. In the general case, our algorithm is probabilistic in the sense
that the resulting Gr\""obner basis can only be expected to generate the given
ideal, with high probability. We have implemented our algorithm in the computer
algebra system {\sc{Singular}} and give timings to compare its performance with
that of other instances of Buchberger's algorithm, testing examples from
$D$-module theory as well as classical benchmark examples. A particular feature
of the modular algorithm is that it allows parallel runs.
"
1010,"Sparse Polynomial Interpolation with Finitely Many Values for the
  Coefficients","  In this paper, we give new sparse interpolation algorithms for black box
polynomial f whose coefficients are from a finite set. In the univariate case,
we recover f from one evaluation of f(a) for a sufficiently large number a. In
the multivariate case, we introduce the modified Kronecker substitution to
reduce the interpolation of a multivariate polynomial to the univariate case.
Both algorithms have polynomial bit-size complexity.
"
1011,CAD Adjacency Computation Using Validated Numerics,"  We present an algorithm for computation of cell adjacencies for well-based
cylindrical algebraic decomposition. Cell adjacency information can be used to
compute topological operations e.g. closure, boundary, connected components,
and topological properties e.g. homology groups. Other applications include
visualization and path planning. Our algorithm determines cell adjacency
information using validated numerical methods similar to those used in CAD
construction, thus computing CAD with adjacency information in time comparable
to that of computing CAD without adjacency information. We report on
implementation of the algorithm and present empirical data.
"
1012,Efficiently Computing Real Roots of Sparse Polynomials,"  We propose an efficient algorithm to compute the real roots of a sparse
polynomial $f\in\mathbb{R}[x]$ having $k$ non-zero real-valued coefficients. It
is assumed that arbitrarily good approximations of the non-zero coefficients
are given by means of a coefficient oracle. For a given positive integer $L$,
our algorithm returns disjoint disks
$\Delta_{1},\ldots,\Delta_{s}\subset\mathbb{C}$, with $s<2k$, centered at the
real axis and of radius less than $2^{-L}$ together with positive integers
$\mu_{1},\ldots,\mu_{s}$ such that each disk $\Delta_{i}$ contains exactly
$\mu_{i}$ roots of $f$ counted with multiplicity. In addition, it is ensured
that each real root of $f$ is contained in one of the disks. If $f$ has only
simple real roots, our algorithm can also be used to isolate all real roots.
  The bit complexity of our algorithm is polynomial in $k$ and $\log n$, and
near-linear in $L$ and $\tau$, where $2^{-\tau}$ and $2^{\tau}$ constitute
lower and upper bounds on the absolute values of the non-zero coefficients of
$f$, and $n$ is the degree of $f$. For root isolation, the bit complexity is
polynomial in $k$ and $\log n$, and near-linear in $\tau$ and
$\log\sigma^{-1}$, where $\sigma$ denotes the separation of the real roots.
"
1013,A Special Homotopy Continuation Method For A Class of Polynomial Systems,"  A special homotopy continuation method, as a combination of the polyhedral
homotopy and the linear product homotopy, is proposed for computing all the
isolated solutions to a special class of polynomial systems. The root number
bound of this method is between the total degree bound and the mixed volume
bound and can be easily computed. The new algorithm has been implemented as a
program called LPH using C++. Our experiments show its efficiency compared to
the polyhedral or other homotopies on such systems. As an application, the
algorithm can be used to find witness points on each connected component of a
real variety.
"
1014,"Computing representation matrices for the action of Frobenius to
  cohomology groups","  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
"
1015,A Case Study on the Parametric Occurrence of Multiple Steady States,"  We consider the problem of determining multiple steady states for positive
real values in models of biological networks. Investigating the potential for
these in models of the mitogen-activated protein kinases (MAPK) network has
consumed considerable effort using special insights into the structure of
corresponding models. Here we apply combinations of symbolic computation
methods for mixed equality/inequality systems, specifically virtual
substitution, lazy real triangularization and cylindrical algebraic
decomposition. We determine multistationarity of an 11-dimensional MAPK network
when numeric values are known for all but potentially one parameter. More
precisely, our considered model has 11 equations in 11 variables and 19
parameters, 3 of which are of interest for symbolic treatment, and furthermore
positivity conditions on all variables and parameters.
"
1016,"Denominator Bounds for Systems of Recurrence Equations using
  $\Pi\Sigma$-Extensions","  We consider linear systems of recurrence equations whose coefficients are
given in terms of indefinite nested sums and products covering, e.g., the
harmonic numbers, hypergeometric products, $q$-hypergeometric products or their
mixed versions. These linear systems are formulated in the setting of
$\Pi\Sigma$-extensions and our goal is to find a denominator bound (also known
as universal denominator) for the solutions; i.e., a non-zero polynomial $d$
such that the denominator of every solution of the system divides $d$. This is
the first step in computing all rational solutions of such a rather general
recurrence system. Once the denominator bound is known, the problem of solving
for rational solutions is reduced to the problem of solving for polynomial
solutions.
"
1017,Apparent Singularities of D-finite Systems,"  We generalize the notions of singularities and ordinary points from linear
ordinary differential equations to D-finite systems. Ordinary points of a
D-finite system are characterized in terms of its formal power series
solutions. We also show that apparent singularities can be removed like in the
univariate case by adding suitable additional solutions to the system at hand.
Several algorithms are presented for removing and detecting apparent
singularities. In addition, an algorithm is given for computing formal power
series solutions of a D-finite system at apparent singularities.
"
1018,"Non-linear Associative-Commutative Many-to-One Pattern Matching with
  Sequence Variables","  Pattern matching is a powerful tool which is part of many functional
programming languages as well as computer algebra systems such as Mathematica.
Among the existing systems, Mathematica offers the most expressive pattern
matching. Unfortunately, no open source alternative has comparable pattern
matching capabilities. Notably, these features include support for associative
and/or commutative function symbols and sequence variables. While those
features have individually been subject of previous research, their
comprehensive combination has not yet been investigated. Furthermore, in many
applications, a fixed set of patterns is matched repeatedly against different
subjects. This many-to-one matching can be sped up by exploiting similarities
between patterns. Discrimination nets are the state-of-the-art solution for
many-to-one matching. In this thesis, a generalized discrimination net which
supports the full feature set is presented. All algorithms have been
implemented as an open-source library for Python. In experiments on real world
examples, significant speedups of many-to-one over one-to-one matching have
been observed.
"
1019,Computing isomorphisms and embeddings of finite fields,"  Let $\mathbb{F}_q$ be a finite field. Given two irreducible polynomials $f,g$
over $\mathbb{F}_q$, with $\mathrm{deg} f$ dividing $\mathrm{deg} g$, the
finite field embedding problem asks to compute an explicit description of a
field embedding of $\mathbb{F}_q[X]/f(X)$ into $\mathbb{F}_q[Y]/g(Y)$. When
$\mathrm{deg} f = \mathrm{deg} g$, this is also known as the isomorphism
problem.
  This problem, a special instance of polynomial factorization, plays a central
role in computer algebra software. We review previous algorithms, due to
Lenstra, Allombert, Rains, and Narayanan, and propose improvements and
generalizations. Our detailed complexity analysis shows that our newly proposed
variants are at least as efficient as previously known algorithms, and in many
cases significantly better.
  We also implement most of the presented algorithms, compare them with the
state of the art computer algebra software, and make the code available as open
source. Our experiments show that our new variants consistently outperform
available software.
"
1020,"Representing ($q$--)hypergeometric products and mixed versions in
  difference rings","  In recent years, Karr's difference field theory has been extended to the
so-called $R\Pi\Sigma$-extensions in which one can represent not only
indefinite nested sums and products that can be expressed by transcendental
ring extensions, but one can also handle algebraic products of the form
$\alpha^n$ where $\alpha$ is a root of unity. In this article we supplement
this summation theory substantially by the following building block. We provide
new algorithms that represent a finite number of hypergeometric or mixed
$(q_1,...,q_e)$-multibasic hypergeometric products in such a difference ring.
This new insight provides a complete summation machinery that enables one to
formulate such products and indefinite nested sums defined over such products
in $R\Pi\Sigma$-extensions fully automatically. As a side-product, one obtains
compactified expressions where the products are algebraically independent among
each other, and one can solve the zero-recognition problem for such products.
"
1021,Compile-Time Symbolic Differentiation Using C++ Expression Templates,"  Template metaprogramming is a popular technique for implementing compile time
mechanisms for numerical computing. We demonstrate how expression templates can
be used for compile time symbolic differentiation of algebraic expressions in
C++ computer programs. Given a positive integer $N$ and an algebraic function
of multiple variables, the compiler generates executable code for the $N$th
partial derivatives of the function. Compile-time simplification of the
derivative expressions is achieved using recursive templates. A detailed
analysis indicates that current C++ compiler technology is already sufficient
for practical use of our results, and highlights a number of issues where
further improvements may be desirable.
"
1022,On Drinfel'd associators,"  In 1986, in order to study the linear representations of the braid group
$B\_n$coming from the monodromy of the Knizhnik-Zamolodchikov differential
equations,Drinfel'd introduced a class of formal power series $\Phi$on
noncommutative variables. These formal series can be considered as a class of
associators. We here give an interpretation of them as well as some new tools
over Noncommutative Evolution Equations. Asymptotic phenomena are also
discussed.
"
1023,Dimension-Dependent Upper Bounds for Grobner Bases,"  We improve certain degree bounds for Grobner bases of polynomial ideals in
generic position. We work exclusively in deterministically verifiable and
achievable generic positions of a combinatorial nature, namely either strongly
stable position or quasi stable position. Furthermore, we exhibit new
dimension- (and depth-)dependent upper bounds for the Castelnuovo-Mumford
regularity and the degrees of the elements of the reduced Grobner basis (w.r.t.
the degree reverse lexicographical ordering) of a homogeneous ideal in these
positions.
"
1024,Deterministic Genericity for Polynomial Ideals,"  We consider several notions of genericity appearing in algebraic geometry and
commutative algebra. Special emphasis is put on various stability notions which
are defined in a combinatorial manner and for which a number of equivalent
algebraic characterisations are provided. It is shown that in characteristic
zero the corresponding generic positions can be obtained with a simple
deterministic algorithm. In positive characteristic, only adapted stable
positions are reachable except for quasi-stability which is obtainable in any
characteristic.
"
1025,"Automated Generation of Non-Linear Loop Invariants Utilizing
  Hypergeometric Sequences","  Analyzing and reasoning about safety properties of software systems becomes
an especially challenging task for programs with complex flow and, in
particular, with loops or recursion. For such programs one needs additional
information, for example in the form of loop invariants, expressing properties
to hold at intermediate program points. In this paper we study program loops
with non-trivial arithmetic, implementing addition and multiplication among
numeric program variables. We present a new approach for automatically
generating all polynomial invariants of a class of such programs. Our approach
turns programs into linear ordinary recurrence equations and computes closed
form solutions of these equations. These closed forms express the most precise
inductive property, and hence invariant. We apply Gr\""obner basis computation
to obtain a basis of the polynomial invariant ideal, yielding thus a finite
representation of all polynomial invariants. Our work significantly extends the
class of so-called P-solvable loops by handling multiplication with the loop
counter variable. We implemented our method in the Mathematica package Aligator
and showcase the practical use of our approach.
"
1026,Improved Computation of Involutive Bases,"  In this paper, we describe improved algorithms to compute Janet and Pommaret
bases. To this end, based on the method proposed by Moller et al., we present a
more efficient variant of Gerdt's algorithm (than the algorithm presented by
Gerdt-Hashemi-M.Alizadeh) to compute minimal involutive bases. Further, by
using the involutive version of Hilbert driven technique, along with the new
variant of Gerdt's algorithm, we modify the algorithm, given by Seiler, to
compute a linear change of coordinates for a given homogeneous ideal so that
the new ideal (after performing this change) possesses a finite Pommaret basis.
All the proposed algorithms have been implemented in Maple and their efficiency
is discussed via a set of benchmark polynomials.
"
1027,"Denominator Bounds and Polynomial Solutions for Systems of q-Recurrences
  over K(t) for Constant K","  We consider systems A_\ell(t) y(q^\ell t) + ... + A_0(t) y(t) = b(t) of
higher order q-recurrence equations with rational coefficients. We extend a
method for finding a bound on the maximal power of t in the denominator of
arbitrary rational solutions y(t) as well as a method for bounding the degree
of polynomial solutions from the scalar case to the systems case. The approach
is direct and does not rely on uncoupling or reduction to a first order system.
Unlike in the scalar case this usually requires an initial transformation of
the system.
"
1028,A Tropical F5 algorithm,"  Let K be a field equipped with a valuation. Tropical varieties over K can be
defined with a theory of Gr{\""o}bner bases taking into account the valuation of
K. While generalizing the classical theory of Gr{\""o}bner bases, it is not
clear how modern algorithms for computing Gr{\""o}bner bases can be adapted to
the tropical case. Among them, one of the most efficient is the celebrated F5
Algorithm of Faug{\`e}re. In this article, we prove that, for homogeneous
ideals, it can be adapted to the tropical case. We prove termination and
correctness. Because of the use of the valuation, the theory of tropical
Gr{\""o}b-ner bases is promising for stable computations over polynomial rings
over a p-adic field. We provide numerical examples to illustrate
time-complexity and p-adic stability of this tropical F5 algorithm.
"
1029,"Nemo/Hecke: Computer Algebra and Number Theory Packages for the Julia
  Programming Language","  We introduce two new packages, Nemo and Hecke, written in the Julia
programming language for computer algebra and number theory. We demonstrate
that high performance generic algorithms can be implemented in Julia, without
the need to resort to a low-level C implementation. For specialised algorithms,
we use Julia's efficient native C interface to wrap existing C/C++ libraries
such as Flint, Arb, Antic and Singular. We give examples of how to use Hecke
and Nemo and discuss some algorithms that we have implemented to provide high
performance basic arithmetic.
"
1030,A lower bound on the positive semidefinite rank of convex bodies,"  The positive semidefinite rank of a convex body $C$ is the size of its
smallest positive semidefinite formulation. We show that the positive
semidefinite rank of any convex body $C$ is at least $\sqrt{\log d}$ where $d$
is the smallest degree of a polynomial that vanishes on the boundary of the
polar of $C$. This improves on the existing bound which relies on results from
quantifier elimination. The proof relies on the B\'ezout bound applied to the
Karush-Kuhn-Tucker conditions of optimality. We discuss the connection with the
algebraic degree of semidefinite programming and show that the bound is tight
(up to constant factor) for random spectrahedra of suitable dimension.
"
1031,"Improved method for finding optimal formulae for bilinear maps in a
  finite field","  In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework
to exhaustively search for optimal formulae for evaluating bilinear maps, such
as Strassen or Karatsuba formulae. The main contribution of this work is a new
criterion to aggressively prune useless branches in the exhaustive search, thus
leading to the computation of new optimal formulae, in particular for the short
product modulo X 5 and the circulant product modulo (X 5 -- 1). Moreover , we
are able to prove that there is essentially only one optimal decomposition of
the product of 3 x 2 by 2 x 3 matrices up to the action of some group of
automorphisms.
"
1032,Power series expansions for the planar monomer-dimer problem,"  We compute the free energy of the planar monomer-dimer model. Unlike the
classical planar dimer model, an exact solution is not known in this case. Even
the computation of the low-density power series expansion requires heavy and
nontrivial computations. Despite of the exponential computational complexity,
we compute almost three times more terms than were previously known. Such an
expansion provides both lower and upper bound for the free energy, and allows
to obtain more accurate numerical values than previously possible. We expect
that our methods can be applied to other similar problems.
"
1033,Computing Canonical Bases of Modules of Univariate Relations,"  We study the computation of canonical bases of sets of univariate relations
$(p_1,\ldots,p_m) \in \mathbb{K}[x]^{m}$ such that $p_1 f_1 + \cdots + p_m f_m
= 0$; here, the input elements $f_1,\ldots,f_m$ are from a quotient
$\mathbb{K}[x]^n/\mathcal{M}$, where $\mathcal{M}$ is a $\mathbb{K}[x]$-module
of rank $n$ given by a basis $\mathbf{M}\in\mathbb{K}[x]^{n\times n}$ in
Hermite form. We exploit the triangular shape of $\mathbf{M}$ to generalize a
divide-and-conquer approach which originates from fast minimal approximant
basis algorithms. Besides recent techniques for this approach, we rely on
high-order lifting to perform fast modular products of polynomial matrices of
the form $\mathbf{P}\mathbf{F} \bmod \mathbf{M}$.
  Our algorithm uses $O\tilde{~}(m^{\omega-1}D + n^{\omega} D/m)$ operations in
$\mathbb{K}$, where $D = \mathrm{deg}(\det(\mathbf{M}))$ is the
$\mathbb{K}$-vector space dimension of $\mathbb{K}[x]^n/\mathcal{M}$,
$O\tilde{~}(\cdot)$ indicates that logarithmic factors are omitted, and
$\omega$ is the exponent of matrix multiplication. This had previously only
been achieved for a diagonal matrix $\mathbf{M}$. Furthermore, our algorithm
can be used to compute the shifted Popov form of a nonsingular matrix within
the same cost bound, up to logarithmic factors, as the previously fastest known
algorithm, which is randomized.
"
1034,"Fast Computation of the Roots of Polynomials Over the Ring of Power
  Series","  We give an algorithm for computing all roots of polynomials over a univariate
power series ring over an exact field $\mathbb{K}$. More precisely, given a
precision $d$, and a polynomial $Q$ whose coefficients are power series in $x$,
the algorithm computes a representation of all power series $f(x)$ such that
$Q(f(x)) = 0 \bmod x^d$. The algorithm works unconditionally, in particular
also with multiple roots, where Newton iteration fails. Our main motivation
comes from coding theory where instances of this problem arise and multiple
roots must be handled.
  The cost bound for our algorithm matches the worst-case input and output size
$d \deg(Q)$, up to logarithmic factors. This improves upon previous algorithms
which were quadratic in at least one of $d$ and $\deg(Q)$. Our algorithm is a
refinement of a divide \& conquer algorithm by Alekhnovich (2005), where the
cost of recursive steps is better controlled via the computation of a factor of
$Q$ which has a smaller degree while preserving the roots.
"
1035,Automatic Differentiation using Constraint Handling Rules in Prolog,"  Automatic differentiation is a technique which allows a programmer to define
a numerical computation via compositions of a broad range of numeric and
computational primitives and have the underlying system support the computation
of partial derivatives of the result with respect to any of its inputs, without
making any finite difference approximations, and without manipulating large
symbolic expressions representing the computation. This note describes a novel
approach to reverse mode automatic differentiation using constraint logic
programmming, specifically, the constraint handling rules (CHR) library of SWI
Prolog, resulting in a very small (50 lines of code) implementation. When
applied to a differentiation-based implementation of the inside-outside
algorithm for parameter learning in probabilistic grammars, the CHR based
implementations outperformed two well-known frameworks for optimising
differentiable functions, Theano and TensorFlow, by a large margin.
"
1036,"Sparse Rational Function Interpolation with Finitely Many Values for the
  Coefficients","  In this paper, we give new sparse interpolation algorithms for black box
univariate and multivariate rational functions h=f/g whose coefficients are
integers with an upper bound. The main idea is as follows: choose a proper
integer beta and let h(beta) = a/b with gcd(a,b)=1. Then f and g can be
computed by solving the polynomial interpolation problems f(beta)=ka and
g(beta)=ka for some integer k. It is shown that the univariate interpolation
algorithm is almost optimal and multivariate interpolation algorithm has low
complexity in T but the data size is exponential in n.
"
1037,Iterated Elliptic and Hypergeometric Integrals for Feynman Diagrams,"  We calculate 3-loop master integrals for heavy quark correlators and the
3-loop QCD corrections to the $\rho$-parameter. They obey non-factorizing
differential equations of second order with more than three singularities,
which cannot be factorized in Mellin-$N$ space either. The solution of the
homogeneous equations is possible in terms of convergent close integer power
series as $_2F_1$ Gau\ss{} hypergeometric functions at rational argument. In
some cases, integrals of this type can be mapped to complete elliptic integrals
at rational argument. This class of functions appears to be the next one
arising in the calculation of more complicated Feynman integrals following the
harmonic polylogarithms, generalized polylogarithms, cyclotomic harmonic
polylogarithms, square-root valued iterated integrals, and combinations
thereof, which appear in simpler cases. The inhomogeneous solution of the
corresponding differential equations can be given in terms of iterative
integrals, where the new innermost letter itself is not an iterative integral.
A new class of iterative integrals is introduced containing letters in which
(multiple) definite integrals appear as factors. For the elliptic case, we also
derive the solution in terms of integrals over modular functions and also
modular forms, using $q$-product and series representations implied by Jacobi's
$\vartheta_i$ functions and Dedekind's $\eta$-function. The corresponding
representations can be traced back to polynomials out of Lambert--Eisenstein
series, having representations also as elliptic polylogarithms, a $q$-factorial
$1/\eta^k(\tau)$, logarithms and polylogarithms of $q$ and their $q$-integrals.
Due to the specific form of the physical variable $x(q)$ for different
processes, different representations do usually appear. Numerical results are
also presented.
"
1038,Symbolic Multibody Methods for Real-Time Simulation of Railway Vehicles,"  In this work, recently developed state-of-the-art symbolic multibody methods
are tested to accurately model a complex railway vehicle. The model is
generated using a symbolic implementation of the principle of the virtual
power. Creep forces are modeled using a direct symbolic implementation of the
standard linear Kalker model. No simplifications, as base parameter reduction,
partial-linearization or look-up tables for contact kinematics, are used. An
Implicit-Explicit integration scheme is proposed to efficiently deal with the
stiff creep dynamics. Hard real-time performance is achieved: the CPU time
required for a very stable 1 ms integration time step is 256 {\mu}s.
"
1039,Beyond Polyhedral Homotopies,"  We present a new algorithmic framework which utilizes tropical geometry and
homotopy continuation for solving systems of polynomial equations where some of
the polynomials are generic elements in linear subspaces of the polynomial
ring. This approach generalizes the polyhedral homotopies by Huber and
Sturmfels.
"
1040,"Automatic differentiation of hybrid models Illustrated by Diffedge
  Graphic Methodology. (Survey)","  We investigate the automatic differentiation of hybrid models, viz. models
that may contain delays, logical tests and discontinuities or loops. We
consider differentiation with respect to parameters, initial conditions or the
time. We emphasize the case of a small number of derivations and iterated
differentiations are mostly treated with a foccus on high order iterations of
the same derivation. The models we consider may involve arithmetic operations,
elementary functions, logical tests but also more elaborate components such as
delays, integrators, equations and differential equations solvers. This survey
has no pretention to exhaustivity but tries to fil a gap in the litterature
where each kind of of component may be documented, but seldom their common use.
  The general approach is illustrated by computer algebra experiments,
stressing the interest of performing differentiation, whenever possible, on
high level objects, before any translation in Fortran or C code. We include
ordinary differential systems with discontinuity, with a special interest for
those comming from discontinuous Lagrangians.
  We conclude with an overview of the graphic methodology developped in the
Diffedge software for Simulink hybrid models. Not all possibilities are
covered, but the methodology can be adapted. The result of automatic
differentiation is a new block diagram and so it can be easily translated to
produce real time embedded programs.
  We welcome any comments or suggestions of references that we may have missed.
"
1041,When is a polynomial ideal binomial after an ambient automorphism?,"  Can an ideal I in a polynomial ring k[x] over a field be moved by a change of
coordinates into a position where it is generated by binomials $x^a - cx^b$
with c in k, or by unital binomials (i.e., with c = 0 or 1)? Can a variety be
moved into a position where it is toric? By fibering the G-translates of I over
an algebraic group G acting on affine space, these problems are special cases
of questions about a family F of ideals over an arbitrary base B. The main
results in this general setting are algorithms to find the locus of points in B
over which the fiber of F
  - is contained in the fiber of a second family F' of ideals over B;
  - defines a variety of dimension at least d;
  - is generated by binomials; or
  - is generated by unital binomials.
  A faster containment algorithm is also presented when the fibers of F are
prime. The big-fiber algorithm is probabilistic but likely faster than known
deterministic ones. Applications include the setting where a second group T
acts on affine space, in addition to G, in which case algorithms compute the
set of G-translates of I
  - whose stabilizer subgroups in T have maximal dimension; or
  - that admit a faithful multigrading by $Z^r$ of maximal rank r.
  Even with no ambient group action given, the final application is an
algorithm to
  - decide whether a normal projective variety is abstractly toric.
  All of these loci in B and subsets of G are constructible; in some cases they
are closed.
"
1042,Refined Holonomic Summation Algorithms in Particle Physics,"  An improved multi-summation approach is introduced and discussed that enables
one to simultaneously handle indefinite nested sums and products in the setting
of difference rings and holonomic sequences. Relevant mathematics is reviewed
and the underlying advanced difference ring machinery is elaborated upon. The
flexibility of this new toolbox contributed substantially to evaluating
complicated multi-sums coming from particle physics. Illustrative examples of
the functionality of the new software package RhoSum are given.
"
1043,"Algorithms for Weighted Sums of Squares Decomposition of Non-negative
  Univariate Polynomials","  It is well-known that every non-negative univariate real polynomial can be
written as the sum of two polynomial squares with real coefficients. When one
allows a weighted sum of finitely many squares instead of a sum of two squares,
then one can choose all coefficients in the representation to lie in the field
generated by the coefficients of the polynomial.
  In this article, we describe, analyze and compare both from the theoretical
and practical points of view, two algorithms computing such a weighted sums of
squares decomposition for univariate polynomials with rational coefficients.
  The first algorithm, due to the third author relies on real root isolation,
quadratic approximations of positive polynomials and square-free decomposition
but its complexity was not analyzed. We provide bit complexity estimates, both
on runtime and output size of this algorithm. They are exponential in the
degree of the input univariate polynomial and linear in the maximum bitsize of
its complexity. This analysis is obtained using quantifier elimination and root
isolation bounds.
  The second algorithm, due to Chevillard, Harrison, Joldes and Lauter, relies
on complex root isolation and square-free decomposition and has been introduced
for certifying positiveness of polynomials in the context of computer
arithmetics. Again, its complexity was not analyzed. We provide bit complexity
estimates, both on runtime and output size of this algorithm, which are
polynomial in the degree of the input polynomial and linear in the maximum
bitsize of its complexity. This analysis is obtained using Vieta's formula and
root isolation bounds.
  Finally, we report on our implementations of both algorithms. While the
second algorithm is, as expected from the complexity result, more efficient on
most of examples, we exhibit families of non-negative polynomials for which the
first algorithm is better.
"
1044,"On the Complexity of Exact Counting of Dynamically Irreducible
  Polynomials","  We give an efficient algorithm to enumerate all sets of $r\ge 1$ quadratic
polynomials over a finite field, which remain irreducible under iterations and
compositions.
"
1045,"Symbolic Versus Numerical Computation and Visualization of Parameter
  Regions for Multistationarity of Biological Networks","  We investigate models of the mitogenactivated protein kinases (MAPK) network,
with the aim of determining where in parameter space there exist multiple
positive steady states. We build on recent progress which combines various
symbolic computation methods for mixed systems of equalities and inequalities.
We demonstrate that those techniques benefit tremendously from a newly
implemented graph theoretical symbolic preprocessing method. We compare
computation times and quality of results of numerical continuation methods with
our symbolic approach before and after the application of our preprocessing.
"
1046,Bivariate Extensions of Abramov's Algorithm for Rational Summation,"  Abramov's algorithm enables us to decide whether a univariate rational
function can be written as a difference of another rational function, which has
been a fundamental algorithm for rational summation. In 2014, Chen and Singer
generalized Abramov's algorithm to the case of rational functions in two
($q$-)discrete variables. In this paper we solve the remaining three mixed
cases, which completes our recent project on bivariate extensions of Abramov's
algorithm for rational summation.
"
1047,"$\mathcal{P}$-schemes and Deterministic Polynomial Factoring over Finite
  Fields","  We introduce a family of mathematical objects called $\mathcal{P}$-schemes,
where $\mathcal{P}$ is a poset of subgroups of a finite group $G$. A
$\mathcal{P}$-scheme is a collection of partitions of the right coset spaces
$H\backslash G$, indexed by $H\in\mathcal{P}$, that satisfies a list of axioms.
These objects generalize the classical notion of association schemes as well as
the notion of $m$-schemes (Ivanyos et al. 2009).
  Based on $\mathcal{P}$-schemes, we develop a unifying framework for the
problem of deterministic factoring of univariate polynomials over finite fields
under the generalized Riemann hypothesis (GRH).
"
1048,Compiling LATEX to computer algebra-enabled HTML5,"  This document explains how to create or modify an existing LATEX document
with commands enabling computations in the HTML5 output: when the reader opens
the HTML5 output, he can run a computation in his browser, or modify the
command to be executed and run it. This is done by combining different
softwares: hevea for compilation to HTML5, giac.js for the CAS computing kernel
(itself compiled from the C++ Giac library with emscripten), and a modified
version of itex2MML for fast and nice rendering in MathML in browsers that
support MathML.
"
1049,Algorithms for zero-dimensional ideals using linear recurrent sequences,"  Inspired by Faug\`ere and Mou's sparse FGLM algorithm, we show how using
linear recurrent multi-dimensional sequences can allow one to perform
operations such as the primary decomposition of an ideal, by computing the
annihilator of one or several such sequences.
"
1050,Measured Multiseries and Integration,"  A paper by Bruno Salvy and the author introduced measured multiseries and
gave an algorithm to compute these for a large class of elementary functions,
modulo a zero-equivalence method for constants. This gave a theoretical
background for the implementation that Salvy was developing at that time. The
main result of the present article is an algorithm to calculate measured
multiseries for integrals of functions of the form h*sin G, where h and G
belong to a Hardy field. The process can reiterated with the resulting algebra,
and also applied to solutions of a second order differential equation of a
particular form.
"
1051,The PSLQ Algorithm for Empirical Data,"  The celebrated integer relation finding algorithm PSLQ has been successfully
used in many applications. PSLQ was only analyzed theoretically for exact input
data, however, when the input data are irrational numbers, they must be
approximate ones due to the finite precision of the computer. When the
algorithm takes empirical data (inexact data with error bounded) instead of
exact real numbers as its input, how do we theoretically ensure the output of
the algorithm to be an exact integer relation?
  In this paper, we investigate the PSLQ algorithm for empirical data as its
input. Firstly, we give a termination condition for this case. Secondly, we
analyze a perturbation on the hyperplane matrix constructed from the input data
and hence disclose a relationship between the accuracy of the input data and
the output quality (an upper bound on the absolute value of the inner product
of the exact data and the computed integer relation), which naturally leads to
an error control strategy for PSLQ. Further, we analyze the complexity bound of
the PSLQ algorithm for empirical data. Examples on transcendental numbers and
algebraic numbers show the meaningfulness of our error control strategy.
"
1052,FORM version 4.2,"  We introduce FORM 4.2, a new minor release of the symbolic manipulation
toolkit. We demonstrate several new features, such as a new pattern matching
option, new output optimization, and automatic expansion of rational functions.
"
1053,"A non-commutative algorithm for multiplying 5x5 matrices using 99
  multiplications","  We present a non-commutative algorithm for multiplying 5x5 matrices using 99
multiplications. This algorithm is a minor modification of Makarov's algorithm
which exhibit the previous best known bound with 100 multiplications.
"
1054,"Complexity of Model Testing for Dynamical Systems with Toric Steady
  States","  In this paper we investigate the complexity of model selection and model
testing for dynamical systems with toric steady states. Such systems frequently
arise in the study of chemical reaction networks. We do this by formulating
these tasks as a constrained optimization problem in Euclidean space. This
optimization problem is known as a Euclidean distance problem; the complexity
of solving this problem is measured by an invariant called the Euclidean
distance (ED) degree. We determine closed-form expressions for the ED degree of
the steady states of several families of chemical reaction networks with toric
steady states and arbitrarily many reactions. To illustrate the utility of this
work we show how the ED degree can be used as a tool for estimating the
computational cost of solving the model testing and model selection problems.
"
1055,"Dealing with Rational Second Order Ordinary Differential Equations where
  both Darboux and Lie Find It Difficult: The $S$-function Method","  Here we present a new approach to search for first order invariants (first
integrals) of rational second order ordinary differential equations. This
method is an alternative to the Darbouxian and symmetry approaches. Our
procedure can succeed in many cases where these two approaches fail. We also
present here a Maple implementation of the theoretical results and methods,
hereby introduced, in a computational package -- {\it InSyDE}. The package is
designed, apart from materializing the algorithms presented, to provide a set
of tools to allow the user to analyse the intermediary steps of the process.
"
1056,Rational invariants of even ternary forms under the orthogonal group,"  In this article we determine a generating set of rational invariants of
minimal cardinality for the action of the orthogonal group $\mathrm{O}_3$ on
the space $\mathbb{R}[x,y,z]_{2d}$ of ternary forms of even degree $2d$. The
construction relies on two key ingredients: On one hand, the Slice Lemma allows
us to reduce the problem to dermining the invariants for the action on a
subspace of the finite subgroup $\mathrm{B}_3$ of signed permutations. On the
other hand, our construction relies in a fundamental way on specific bases of
harmonic polynomials. These bases provide maps with prescribed
$\mathrm{B}_3$-equivariance properties. Our explicit construction of these
bases should be relevant well beyond the scope of this paper. The expression of
the $\mathrm{B}_3$-invariants can then be given in a compact form as the
composition of two equivariant maps. Instead of providing (cumbersome) explicit
expressions for the $\mathrm{O}_3$-invariants, we provide efficient algorithms
for their evaluation and rewriting. We also use the constructed
$\mathrm{B}_3$-invariants to determine the $\mathrm{O}_3$-orbit locus and
provide an algorithm for the inverse problem of finding an element in
$\mathbb{R}[x,y,z]_{2d}$ with prescribed values for its invariants. These are
the computational issues relevant in brain imaging.
"
1057,"Robust Computer Algebra, Theorem Proving, and Oracle AI","  In the context of superintelligent AI systems, the term ""oracle"" has two
meanings. One refers to modular systems queried for domain-specific tasks.
Another usage, referring to a class of systems which may be useful for
addressing the value alignment and AI control problems, is a superintelligent
AI system that only answers questions. The aim of this manuscript is to survey
contemporary research problems related to oracles which align with long-term
research goals of AI safety. We examine existing question answering systems and
argue that their high degree of architectural heterogeneity makes them poor
candidates for rigorous analysis as oracles. On the other hand, we identify
computer algebra systems (CASs) as being primitive examples of domain-specific
oracles for mathematics and argue that efforts to integrate computer algebra
systems with theorem provers, systems which have largely been developed
independent of one another, provide a concrete set of problems related to the
notion of provable safety that has emerged in the AI safety community. We
review approaches to interfacing CASs with theorem provers, describe
well-defined architectural deficiencies that have been identified with CASs,
and suggest possible lines of research and practical software projects for
scientists interested in AI safety.
"
1058,On Euler's inequality and automated reasoning with dynamic geometry,"  Euler's inequality $R\geq 2r$ can be investigated in a novel way by using
implicit loci in GeoGebra. Some unavoidable side effects of the implicit locus
computation introduce unexpected algebraic curves. By using a mixture of
symbolic and numerical methods a possible approach is sketched up to
investigate the situation. By exploiting fast GPU computations, a web
application written in CindyJS helps in understanding the situation even
better.
"
1059,Counting Roots of Polynomials over $\mathbb{Z}/p^2\mathbb{Z}$,"  Until recently, the only known method of finding the roots of polynomials
over prime power rings, other than fields, was brute force. One reason for this
is the lack of a division algorithm, obstructing the use of greatest common
divisors. Fix a prime $p \in \mathbb{Z}$ and $f \in ( \mathbb{Z}/p^n \mathbb{Z}
) [x]$ any nonzero polynomial of degree $d$ whose coefficients are not all
divisible by $p$. For the case $n=2$, we prove a new efficient algorithm to
count the roots of $f$ in $\mathbb{Z}/p^2\mathbb{Z}$ within time polynomial in
$(d+\operatorname{size}(f)+\log{p})$, and record a concise formula for the
number of roots, formulated by Cheng, Gao, Rojas, and Wan.
"
1060,Strassen's 2x2 matrix multiplication algorithm: A conceptual perspective,"  The main purpose of this paper is pedagogical.
  Despite its importance, all proofs of the correctness of Strassen's famous
1969 algorithm to multiply two 2x2 matrices with only seven multiplications
involve some basis-dependent calculations such as explicitly multiplying
specific 2x2 matrices, expanding expressions to cancel terms with opposing
signs, or expanding tensors over the standard basis. This makes the proof
nontrivial to memorize and many presentations of the proof avoid showing all
the details and leave a significant amount of verifications to the reader.
  In this note we give a short, self-contained, basis-independent proof of the
existence of Strassen's algorithm that avoids these types of calculations. We
achieve this by focusing on symmetries and algebraic properties.
  Our proof can be seen as a coordinate-free version of the construction of
Clausen from 1988, combined with recent work on the geometry of Strassen's
algorithm by Chiantini, Ikenmeyer, Landsberg, and Ottaviani from 2016.
"
1061,Syzygies among reduction operators,"  We introduce the notion of syzygy for a set of reduction operators and relate
it to the notion of syzygy for presentations of algebras. We give a method for
constructing a linear basis of the space of syzygies for a set of reduction
operators. We interpret these syzygies in terms of the confluence property from
rewriting theory. This enables us to optimise the completion procedure for
reduction operators based on a criterion for detecting useless reductions. We
illustrate this criterion with an example of construction of commutative
Gr{\""o}bner basis.
"
1062,Model Checking Regular Language Constraints,"  Even the fastest SMT solvers have performance problems with regular
expressions from real programs. Because these performance issues often arise
from the problem representation (e.g. non-deterministic finite automata get
determinized and regular expressions get unrolled), we revisit Boolean finite
automata, which allow for the direct and natural representation of any Boolean
combination of regular languages. By applying the IC3 model checking algorithm
to Boolean finite automata, not only can we efficiently answer emptiness and
universality problems, but through an extension, we can decide satisfiability
of multiple variable string membership problems. We demonstrate the resulting
system's effectiveness on a number of popular benchmarks and regular
expressions.
"
1063,Designing Strassen's algorithm,"  In 1969, Strassen shocked the world by showing that two n x n matrices could
be multiplied in time asymptotically less than $O(n^3)$. While the recursive
construction in his algorithm is very clear, the key gain was made by showing
that 2 x 2 matrix multiplication could be performed with only 7 multiplications
instead of 8. The latter construction was arrived at by a process of
elimination and appears to come out of thin air. Here, we give the simplest and
most transparent proof of Strassen's algorithm that we are aware of, using only
a simple unitary 2-design and a few easy lines of calculation. Moreover, using
basic facts from the representation theory of finite groups, we use 2-designs
coming from group orbits to generalize our construction to all n (although the
resulting algorithms aren't optimal for n at least 3).
"
1064,Faster Multiplication for Long Binary Polynomials,"  We set new speed records for multiplying long polynomials over finite fields
of characteristic two. Our multiplication algorithm is based on an additive FFT
(Fast Fourier Transform) by Lin, Chung, and Huang in 2014 comparing to
previously best results based on multiplicative FFTs. Both methods have similar
complexity for arithmetic operations on underlying finite field; however, our
implementation shows that the additive FFT has less overhead. For further
optimization, we employ a tower field construction because the multipliers in
the additive FFT naturally fall into small subfields, which leads to speed-ups
using table-lookup instructions in modern CPUs. Benchmarks show that our method
saves about $40 \%$ computing time when multiplying polynomials of $2^{28}$ and
$2^{29}$ bits comparing to previous multiplicative FFT implementations.
"
1065,"Exact Inference for Relational Graphical Models with Interpreted
  Functions: Lifted Probabilistic Inference Modulo Theories","  Probabilistic Inference Modulo Theories (PIMT) is a recent framework that
expands exact inference on graphical models to use richer languages that
include arithmetic, equalities, and inequalities on both integers and real
numbers. In this paper, we expand PIMT to a lifted version that also processes
random functions and relations. This enhancement is achieved by adapting
Inversion, a method from Lifted First-Order Probabilistic Inference literature,
to also be modulo theories. This results in the first algorithm for exact
probabilistic inference that efficiently and simultaneously exploits random
relations and functions, arithmetic, equalities and inequalities.
"
1066,"A Curious Family of Binomial Determinants That Count Rhombus Tilings of
  a Holey Hexagon","  We evaluate a curious determinant, first mentioned by George Andrews in 1980
in the context of descending plane partitions. Our strategy is to combine the
famous Desnanot-Jacobi-Dodgson identity with automated proof techniques. More
precisely, we follow the holonomic ansatz that was proposed by Doron Zeilberger
in 2007. We derive a compact and nice formula for Andrews's determinant, and
use it to solve a challenge problem that we posed in a previous paper. By
noting that Andrews's determinant is a special case of a two-parameter family
of determinants, we find closed forms for several one-parameter subfamilies.
The interest in these determinants arises because they count cyclically
symmetric rhombus tilings of a hexagon with several triangular holes inside.
"
1067,Root Separation for Trinomials,"  We give a separation bound for the complex roots of a trinomial $f \in
\mathbb{Z}[X]$. The logarithm of the inverse of our separation bound is
polynomial in the size of the sparse encoding of $f$; in particular, it is
polynomial in $\log (\deg f)$. It is known that no such bound is possible for
4-nomials (polynomials with 4 monomials). For trinomials, the classical results
(which are based on the degree of $f$ rather than the number of monomials) give
separation bounds that are exponentially worse.As an algorithmic application,
we show that the number of real roots of a trinomial $f$ can be computed in
time polynomial in the size of the sparse encoding of~$f$. The same problem is
open for 4-nomials.
"
1068,"Rational Solutions of High-Order Algebraic Ordinary Differential
  Equations","  We consider algebraic ordinary differential equations (AODEs) and study their
polynomial and rational solutions. A sufficient condition for an AODE to have a
degree bound for its polynomial solutions is presented. An AODE satisfying this
condition is called \emph{noncritical}. We prove that usual low order classes
of AODEs are noncritical. For rational solutions, we determine a class of
AODEs, which are called \emph{maximally comparable}, such that the poles of
their rational solutions are recognizable from their coefficients. This
generalizes a fact from linear AODEs, that the poles of their rational
solutions are the zeros of the corresponding highest coefficient. An algorithm
for determining all rational solutions, if there is any, of certain maximally
comparable AODEs, which covers $78.54\%$ AODEs from a standard differential
equations collection by Kamke, is presented.
"
1069,"Analytic Combinatorics in Several Variables: Effective Asymptotics and
  Lattice Path Enumeration","  The field of analytic combinatorics, which studies the asymptotic behaviour
of sequences through analytic properties of their generating functions, has led
to the development of deep and powerful tools with applications across
mathematics and the natural sciences. In addition to the now classical
univariate theory, recent work in the study of analytic combinatorics in
several variables (ACSV) has shown how to derive asymptotics for the
coefficients of certain D-finite functions represented by diagonals of
multivariate rational functions. We give a pedagogical introduction to the
methods of ACSV from a computer algebra viewpoint, developing rigorous
algorithms and giving the first complexity results in this area under
conditions which are broadly satisfied. Furthermore, we give several new
applications of ACSV to the enumeration of lattice walks restricted to certain
regions. In addition to proving several open conjectures on the asymptotics of
such walks, a detailed study of lattice walk models with weighted steps is
undertaken.
"
1070,"In-depth comparison of the Berlekamp -- Massey -- Sakata and the
  Scalar-FGLM algorithms: the non adaptive variants","  We compare thoroughly the Berlekamp -- Massey -- Sakata algorithm and the
Scalar-FGLM algorithm, which compute both the ideal of relations of a
multi-dimensional linear recurrent sequence. Suprisingly, their behaviors
differ. We detail in which way they do and prove that it is not possible to
tweak one of the algorithms in order to mimic exactly the behavior of the
other.
"
1071,"High Degree Sum of Squares Proofs, Bienstock-Zuckerberg hierarchy and
  Chvatal-Gomory cuts","  Chvatal-Gomory (CG) cuts and the Bienstock-Zuckerberg hierarchy capture
useful linear programs that the standard bounded degree Lasserre/Sum-of-Squares
SOS hierarchy fails to capture.
  In this paper we present a novel polynomial time SOS hierarchy for 0/1
problems with a custom subspace of high degree polynomials (not the standard
subspace of low-degree polynomials). We show that the new SOS hierarchy
recovers the Bienstock-Zuckerberg hierarchy. Our result implies a linear
program that reproduces the Bienstock-Zuckerberg hierarchy as a polynomial
sized, efficiently constructive extended formulation that satisfies all
constant pitch inequalities. The construction is also very simple, and it is
fully defined by giving the supporting polynomials. Moreover, for a class of
polytopes (e.g. set covering and packing problems), the resulting SOS hierarchy
optimizes in polynomial time over the polytope resulting from any constant
rounds of CG-cuts, up to an arbitrarily small error.
  Arguably, this is the first example where different basis functions can be
useful in asymmetric situations to obtain a hierarchy of relaxations.
"
1072,"Faster Interpolation Algorithms for Sparse Multivariate Polynomials
  Given by Straight-Line Programs\","  In this paper, we propose new deterministic and Monte Carlo interpolation
algorithms for sparse multivariate polynomials represented by straight-line
programs. Let $f$ be an $n$-variate polynomial given by a straight-line
program, which has a degree bound $D$ and a term bound $T$. Our deterministic
algorithm is quadratic in $n,T$ and cubic in $\log D$ in the Soft-Oh sense,
which has better complexities than existing deterministic interpolation
algorithms in most cases. Our Monte Carlo interpolation algorithms have better
complexities than existing Monte Carlo interpolation algorithms and are the
first algorithms whose complexities are linear in $nT$ in the Soft-Oh sense.
Since $nT$ is a factor of the size of $f$, our Monte Carlo algorithms are
optimal in $n$ and $T$ in the Soft-Oh sense.
"
1073,"Deterministic Interpolation of Sparse Black-box Multivariate Polynomials
  using Kronecker Type Substitutions","  In this paper, we propose two new deterministic interpolation algorithms for
a sparse multivariate polynomial given as a standard black-box by introducing
new Kronecker type substitutions. Let $f\in \RB[x_1,\dots,x_n]$ be a sparse
black-box polynomial with a degree bound $D$. When $\RB=\C$ or a finite field,
our algorithms either have better bit complexity or better bit complexity in
$D$ than existing deterministic algorithms. In particular, in the case of
deterministic algorithms for standard black-box models, our second algorithm
has the current best complexity in $D$ which is the dominant factor in the
complexity.
"
1074,Improved Complexity Bounds for Counting Points on Hyperelliptic Curves,"  We present a probabilistic Las Vegas algorithm for computing the local zeta
function of a hyperelliptic curve of genus $g$ defined over $\mathbb{F}_q$. It
is based on the approaches by Schoof and Pila combined with a modeling of the
$\ell$-torsion by structured polynomial systems. Our main result improves on
previously known complexity bounds by showing that there exists a constant
$c>0$ such that, for any fixed $g$, this algorithm has expected time and space
complexity $O((\log q)^{cg})$ as $q$ grows and the characteristic is large
enough.
"
1075,Compact Formulae in Sparse Elimination,"  It has by now become a standard approach to use the theory of sparse (or
toric) elimination, based on the Newton polytope of a polynomial, in order to
reveal and exploit the structure of algebraic systems. This talk surveys
compact formulae, including older and recent results, in sparse elimination. We
start with root bounds and juxtapose two recent formulae: a generating function
of the m-B{\'e}zout bound and a closed-form expression for the mixed volume by
means of a matrix permanent. For the sparse resultant, a bevy of results have
established determinantal or rational formulae for a large class of systems,
starting with Macaulay. The discriminant is closely related to the resultant
but admits no compact formula except for very simple cases. We offer a new
determinantal formula for the discriminant of a sparse multilinear system
arising in computing Nash equilibria. We introduce an alternative notion of
compact formula, namely the Newton polytope of the unknown polynomial. It is
possible to compute it efficiently for sparse resultants, discriminants, as
well as the implicit equation of a parameterized variety. This leads us to
consider implicit matrix representations of geometric objects.
"
1076,On the bit-size of non-radical triangular sets,"  We present upper bounds on the bit-size of coefficients of non-radical
lexicographical Groebner bases in purely triangular form (triangular sets) of
dimension zero. This extends a previous work [Dahan-Schost, Issac'2004],
constrained to radical triangular sets; it follows the same technical steps,
based on interpolation. However, key notion of height of varieties is not
available for points with multiplicities; therefore the bounds obtained are
less universal and depend on some input data. We also introduce a related
family of non- monic polynomials that have smaller coefficients, and smaller
bounds. It is not obvious to compute them from the initial triangular set
though.
"
1077,MatchPy: A Pattern Matching Library,"  Pattern matching is a powerful tool for symbolic computations, based on the
well-defined theory of term rewriting systems. Application domains include
algebraic expressions, abstract syntax trees, and XML and JSON data.
Unfortunately, no lightweight implementation of pattern matching as general and
flexible as Mathematica exists for Python Mathics,MacroPy,patterns,PyPatt.
Therefore, we created the open source module MatchPy which offers similar
pattern matching functionality in Python using a novel algorithm which finds
matches for large pattern sets more efficiently by exploiting similarities
between patterns.
"
1078,Univariate Contraction and Multivariate Desingularization of Ore Ideals,"  Ore operators with polynomial coefficients form a common algebraic
abstraction for representing D-finite functions. They form the Ore ring
$K(x)[D_x]$, where $K$ is the constant field. Suppose $K$ is the quotient field
of some principal ideal domain $R$. The ring $R[x][D_x]$ consists of elements
in $K(x)[D_x]$ without ""denominator"".
  Given $L \in K(x)[D_x]$, it generates a left ideal $I$ in $K(x)[D_x]$. We
call $I \cap R[x][D_x]$ the univariate contraction of $I$.
  When $L$ is a linear ordinary differential or difference operator, we design
a contraction algorithm for $L$ by using desingularized operators as proposed
by Chen, Jaroschek, Kauers and Singer. When $L$ is an ordinary differential
operator and $R = K$, our algorithm is more elementary than known algorithms.
In other cases, our results are new.
  We propose the notion of completely desingularized operators, study their
properties, and design an algorithm for computing them. Completely
desingularized operators have interesting applications such as certifying
integer sequences and checking special cases of a conjecture of Krattenthaler.
  A D-finite system is a finite set of linear homogeneous partial differential
equations in several variables, whose solution space is of finite dimension.
For such systems, we give the notion of a singularity in terms of the
polynomials appearing in them. We show that a point is a singularity of the
system unless it admits a basis of power series solutions in which the starting
monomials are as small as possible with respect to some term order. Then a
singularity is apparent if the system admits a full basis of power series
solutions, the starting terms of which are not as small as possible. We prove
that apparent singularities in the multivariate case can be removed like in the
univariate case by adding suitable additional solutions to the original system.
"
1079,On the Annihilator Ideal of an Inverse Form,"  Let $K$ be a field. We simplify and extend work of Althaler \& D\""ur on
finite sequences over $K$ by regarding $K[x^{-1},z^{-1}]$ as a $K[x,z]$ module,
and studying forms in $K[x^{-1},z^{-1}]$ from first principles. Then we apply
our results to finite sequences.
  First we define the annihilator ideal $I_F$ of a non-zero form $F\in
K[x^{-1},z^{-1}]$, a homogeneous ideal. We inductively construct an ordered
pair ($f_1$\,,\,$f_2$) of forms which generate $I_F$\,; our generators are
special in that $z$ does not divide the leading grlex monomial of $f_1$ but $z$
divides $f_2$\,, and the sum of their total degrees is always $2-|F|$, where
$|F|$ is the total degree of $F$. We show that $f_1,f_2$ is a maximal regular
sequence for $I_F$, so that the height of $I_F$ is 2. The corresponding
algorithm is $\sim |F|^2/2$.
  The row vector obtained by accumulating intermediate forms of the
construction gives a minimal grlex Gr\""obner basis for $I_F$ for no extra
computational cost other than storage and apply this to determining $\dim_K
(K[x,z] /I_F)$\,. We show that either the form vector is reduced or a monomial
of $f_1$ can be reduced by $f_2$\,. This enables us to efficiently construct
the unique reduced Gr\""obner basis for $I_F$ from the vector extension of our
algorithm.
  Then we specialise to the inverse form of a finite sequence, obtaining
generator forms for its annihilator ideal and a corresponding algorithm which
does not use the last 'length change' of Massey. We compute the intersection of
two annihilator ideals using syzygies in $K[x,z]^5$. This improves a result of
Althaler \& D\""ur. Finally, dehomogenisation induces a one-to-one
correspondence ($f_1$\,,$f_2$) $\mapsto$ (minimal polynomial, auxiliary
polynomial), the output of the author's variant of the Berlekamp-Massey
algorithm. So we can also solve the LFSR synthesis problem via the
corresponding algorithm for sequences.
"
1080,Symbolic Computations of First Integrals for Polynomial Vector Fields,"  In this article we show how to generalize to the Darbouxian, Liouvillian and
Riccati case the extactic curve introduced by J. Pereira. With this approach,
we get new algorithms for computing, if it exists, a rational, Darbouxian,
Liouvillian or Riccati first integral with bounded degree of a polynomial
planar vector field. We give probabilistic and deterministic algorithms. The
arithmetic complexity of our probabilistic algorithm is in
$\tilde{\mathcal{O}}(N^{\omega+1})$, where $N$ is the bound on the degree of a
representation of the first integral and $\omega \in [2;3]$ is the exponent of
linear algebra. This result improves previous algorithms. Our algorithms have
been implemented in Maple and are available on authors' websites. In the last
section, we give some examples showing the efficiency of these algorithms.
"
1081,Lower bounds on the number of realizations of rigid graphs,"  Computing the number of realizations of a minimally rigid graph is a
notoriously difficult problem. Towards this goal, for graphs that are minimally
rigid in the plane, we take advantage of a recently published algorithm, which
is the fastest available method, although its complexity is still exponential.
Combining computational results with the theory of constructing new rigid
graphs by gluing, we give a new lower bound on the maximal possible number of
(complex) realizations for graphs with a given number of vertices. We extend
these ideas to rigid graphs in three dimensions and we derive similar lower
bounds, by exploiting data from extensive Gr\""obner basis computations.
"
1082,"Definite Sums of Hypergeometric Terms and Limits of P-Recursive
  Sequences","  The ubiquity of the class of D-finite functions and P-recursive sequences in
symbolic computation is widely recognized. In this thesis, the presented work
consists of two parts related to this class.
  In the first part, we generalize the reduction-based creative telescoping
algorithms to the hypergeometric setting, which allows to deal with definite
sums of hypergeometric terms more quickly. We first modify the
Abramov-Petkovsek reduction, and then design a new algorithm to compute minimal
telescopers for bivariate hypergeometric terms based on the modified reduction.
This new algorithm can avoid the costly computation of certificates, and
outperforms the classical Zeilberger algorithm no matter whether certificates
are computed or not according to the computational experiments. Moreover, we
also derive order bounds for minimal telescopers. These bounds are sometimes
better, and never worse than the known ones.
  In the second part of the thesis, we study the class of D-finite numbers. It
consists of the limits of convergent P-recursive sequences. Typically, this
class contains many well-known mathematical constants in addition to the
algebraic numbers. Our definition of the class of D-finite numbers depends on
two subrings of the field of complex numbers. We investigate how different
choices of these two subrings affect the class. Moreover, we show that D-finite
numbers over the Gaussian rational field are essentially the same as the values
of D-finite functions at non-singular algebraic number arguments (so-called the
regular holonomic constants). This result makes it easier to recognize certain
numbers as belonging to this class.
"
1083,"The Potential and Challenges of CAD with Equational Constraints for
  SC-Square","  Cylindrical algebraic decomposition (CAD) is a core algorithm within Symbolic
Computation, particularly for quantifier elimination over the reals and
polynomial systems solving more generally. It is now finding increased
application as a decision procedure for Satisfiability Modulo Theories (SMT)
solvers when working with non-linear real arithmetic. We discuss the potentials
from increased focus on the logical structure of the input brought by the SMT
applications and SC-Square project, particularly the presence of equational
constraints. We also highlight the challenges for exploiting these: primitivity
restrictions, well-orientedness questions, and the prospect of incrementality.
"
1084,Automatic Differentiation for Tensor Algebras,"  Kjolstad et. al. proposed a tensor algebra compiler. It takes expressions
that define a tensor element-wise, such as $f_{ij}(a,b,c,d) =
\exp\left[-\sum_{k=0}^4 \left((a_{ik}+b_{jk})^2\, c_{ii} + d_{i+k}^3 \right)
\right]$, and generates the corresponding compute kernel code.
  For machine learning, especially deep learning, it is often necessary to
compute the gradient of a loss function $l(a,b,c,d)=l(f(a,b,c,d))$ with respect
to parameters $a,b,c,d$. If tensor compilers are to be applied in this field,
it is necessary to derive expressions for the derivatives of element-wise
defined tensors, i.e. expressions for $(da)_{ik}=\partial l/\partial a_{ik}$.
  When the mapping between function indices and argument indices is not 1:1,
special attention is required. For the function $f_{ij} (x) = x_i^2$, the
derivative of the loss is $(dx)_i=\partial l/\partial x_i=\sum_j
(df)_{ij}2x_i$; the sum is necessary because index $j$ does not appear in the
indices of $f$. Another example is $f_{i}(x)=x_{ii}^2$, where $x$ is a matrix;
here we have $(dx)_{ij}=\delta_{ij}(df)_i2x_{ii}$; the Kronecker delta is
necessary because the derivative is zero for off-diagonal elements. Another
indexing scheme is used by $f_{ij}(x)=\exp x_{i+j}$; here the correct
derivative is $(dx)_{k}=\sum_i (df)_{i,k-i} \exp x_{k}$, where the range of the
sum must be chosen appropriately.
  In this publication we present an algorithm that can handle any case in which
the indices of an argument are an arbitrary linear combination of the indices
of the function, thus all the above examples can be handled. Sums (and their
ranges) and Kronecker deltas are automatically inserted into the derivatives as
necessary. Additionally, the indices are transformed, if required (as in the
last example). The algorithm outputs a symbolic expression that can be
subsequently fed into a tensor algebra compiler.
  Source code is provided.
"
1085,Counting Roots of Polynomials Over Prime Power Rings,"  Suppose $p$ is a prime, $t$ is a positive integer, and
$f\!\in\!\mathbb{Z}[x]$ is a univariate polynomial of degree $d$ with
coefficients of absolute value $<\!p^t$. We show that for any fixed $t$, we can
compute the number of roots in $\mathbb{Z}/(p^t)$ of $f$ in deterministic time
$(d+\log p)^{O(1)}$. This fixed parameter tractability appears to be new for
$t\!\geq\!3$. A consequence for arithmetic geometry is that we can efficiently
compute Igusa zeta functions $Z$, for univariate polynomials, assuming the
degree of $Z$ is fixed.
"
1086,"Decoupled molecules with binding polynomials of bidegree (n,2)","  We present a result on the number of decoupled molecules for systems binding
two different types of ligands. In the case of $n$ and $2$ binding sites
respectively, we show that, generically, there are $2(n!)^{2}$ decoupled
molecules with the same binding polynomial. For molecules with more binding
sites for the second ligand, we provide computational results.
"
1087,Computation of the Adjoint Matrix,"  The best method for computing the adjoint matrix of an order $n$ matrix in an
arbitrary commutative ring requires $O(n^{\beta+1/3}\log n \log \log n)$
operations, provided the complexity of the algorithm for multiplying two
matrices is $\gamma n^\beta+o(n^\beta)$. For a commutative domain -- and under
the same assumptions -- the complexity of the best method is ${6\gamma
n^\beta}/{(2^{\beta}-2)}+o(n^\beta)$. In the present work a new method is
presented for the computation of the adjoint matrix in a commutative domain.
Despite the fact that the number of operations required is now 1.5 times more,
than that of the best method, this new method permits a better parallelization
of the computational process and may be successfully employed for computations
in parallel computational systems.
"
1088,Solution of a System of Linear Equations in an Integral Ring,"  A modified Gauss's algorithm for solving a system of linear equations in an
integral ring is proposed, as well as an appropriate algorithm for calculating
the elements of the adjoint matrix.
"
1089,Effective Matrix Methods in Commutative Domains,"  Effective matrix methods for solving standard linear algebra problems in a
commutative domains are discussed. Two of them are new. There are a methods for
computing adjoined matrices and solving system of linear equations in a
commutative domains.
"
1090,"Algorithms for the solution of systems of linear equations in
  commutative ring","  Solution methods for linear equation systems in a commutative ring are
discussed. Four methods are compared, in the setting of several different
rings: Dodgson's method [1], Bareiss's method [2] and two methods of the author
- method by forward and back-up procedures [3] and a one-pass method [4]. We
show that for the number of coefficient operations, or for the number of
operations in the finite rings, or for modular computation in the polynomial
rings the one-pass method [4] is the best. The method of forward and back-up
procedures [3] is the best for the polynomial rings when we make use of
classical algorithms for polynomial operations.
"
1091,Algorithms for the Computing Determinants in Commutative Rings,"  Two known computation methods and one new computation method for matrix
determinant over an integral domain are discussed. For each of the methods we
evaluate the computation times for different rings and show that the new method
is the best.
"
1092,"Drinfeld Modules with Complex Multiplication, Hasse Invariants and
  Factoring Polynomials over Finite Fields","  We present a novel randomized algorithm to factor polynomials over a finite
field $\F_q$ of odd characteristic using rank $2$ Drinfeld modules with complex
multiplication. The main idea is to compute a lift of the Hasse invariant
(modulo the polynomial $f \in \F_q[x]$ to be factored) with respect to a random
Drinfeld module $\phi$ with complex multiplication. Factors of $f$ supported on
prime ideals with supersingular reduction at $\phi$ have vanishing Hasse
invariant and can be separated from the rest. Incorporating a Drinfeld module
analogue of Deligne's congruence, we devise an algorithm to compute the Hasse
invariant lift, which turns out to be the crux of our algorithm. The resulting
expected runtime of $n^{3/2+\varepsilon} (\log q)^{1+o(1)}+n^{1+\varepsilon}
(\log q)^{2+o(1)}$ to factor polynomials of degree $n$ over $\F_q$ matches the
fastest previously known algorithm, the Kedlaya-Umans implementation of the
Kaltofen-Shoup algorithm.
"
1093,Partial Predicate Abstraction and Counter-Example Guided Refinement,"  In this paper we present a counter-example guided abstraction and
approximation refinement (CEGAAR) technique for {\em partial predicate
abstraction}, which combines predicate abstraction and fixpoint approximations
for model checking infinite-state systems. The proposed approach incrementally
considers growing sets of predicates for abstraction refinement. The novelty of
the approach stems from recognizing source of the imprecision: abstraction or
approximation. We use Craig interpolation to deal with imprecision due to
abstraction. In the case of imprecision due to approximation, we delay
application of the approximation. Our experimental results on a variety of
models provide insights into effectiveness of partial predicate abstraction as
well as refinement techniques in this context.
"
1094,Symbolic-Numeric Integration of Rational Functions,"  We consider the problem of symbolic-numeric integration of symbolic
functions, focusing on rational functions. Using a hybrid method allows the
stable yet efficient computation of symbolic antiderivatives while avoiding
issues of ill-conditioning to which numerical methods are susceptible. We
propose two alternative methods for exact input that compute the rational part
of the integral using Hermite reduction and then compute the transcendental
part two different ways using a combination of exact integration and efficient
numerical computation of roots. The symbolic computation is done within BPAS,
or Basic Polynomial Algebra Subprograms, which is a highly optimized
environment for polynomial computation on parallel architectures, while the
numerical computation is done using the highly optimized multiprecision
rootfinding package MPSolve. We show that both methods are forward and backward
stable in a structured sense and away from singularities tolerance
proportionality is achieved by adjusting the precision of the rootfinding
tasks.
"
1095,Constructive Arithmetics in Ore Localizations of Domains,"  For a non-commutative domain $R$ and a multiplicatively closed set $S$ the
(left) Ore localization of $R$ at $S$ exists if and only if $S$ satisfies the
(left) Ore property. Since the concept has been introduced by Ore back in the
1930's, Ore localizations have been widely used in theory and in applications.
We investigate the arithmetics of the localized ring $S^{-1}R$ from both
theoretical and practical points of view. We show that the key component of the
arithmetics is the computation of the intersection of a left ideal with a
submonoid $S$ of $R$. It is not known yet, whether there exists an algorithmic
solution of this problem in general. Still, we provide such solutions for cases
where $S$ is equipped with additional structure by distilling three most
frequently occurring types of Ore sets. We introduce the notion of the (left)
saturation closure and prove that it is a canonical form for (left) Ore sets in
$R$. We provide an implementation of arithmetics over the ubiquitous
$G$-algebras in \textsc{Singular:Plural} and discuss questions arising in this
context. Numerous examples illustrate the effectiveness of the proposed
approach.
"
1096,Rings: an efficient Java/Scala library for polynomial rings,"  In this paper we briefly discuss \Rings --- an efficient lightweight library
for commutative algebra. Polynomial arithmetic, GCDs, polynomial factorization
and Gr\""obner bases are implemented with the use of modern asymptotically fast
algorithms. \Rings can be easily interacted or embedded in applications in
high-energy physics and other research areas via a simple API with fully typed
hierarchy of algebraic structures and algorithms for commutative algebra. The
use of the Scala language brings a quite novel powerful, strongly typed
functional programming model allowing to write short, expressive, and fast code
for applications. At the same time Rings shows one of the best performances
among existing software for algebraic calculations. \Rings is available from
http://github.com/PoslavskySV/rings
"
1097,"Faster integer and polynomial multiplication using cyclotomic
  coefficient rings","  We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).
"
1098,Computing Lower Rank Approximations of Matrix Polynomials,"  Given an input matrix polynomial whose coefficients are floating point
numbers, we consider the problem of finding the nearest matrix polynomial which
has rank at most a specified value. This generalizes the problem of finding a
nearest matrix polynomial that is algebraically singular with a prescribed
lower bound on the dimension given in a previous paper by the authors. In this
paper we prove that such lower rank matrices at minimal distance always exist,
satisfy regularity conditions, and are all isolated and surrounded by a basin
of attraction of non-minimal solutions. In addition, we present an iterative
algorithm which, on given input sufficiently close to a rank-at-most matrix,
produces that matrix. The algorithm is efficient and is proven to converge
quadratically given a sufficiently good starting point. An implementation
demonstrates the effectiveness and numerical robustness of our algorithm in
practice.
"
1099,Block-Krylov techniques in the context of sparse-FGLM algorithms,"  Consider a zero-dimensional ideal $I$ in $\mathbb{K}[X_1,\dots,X_n]$.
Inspired by Faug\`ere and Mou's Sparse FGLM algorithm, we use Krylov sequences
based on multiplication matrices of $I$ in order to compute a description of
its zero set by means of univariate polynomials.
  Steel recently showed how to use Coppersmith's block-Wiedemann algorithm in
this context; he describes an algorithm that can be easily parallelized, but
only computes parts of the output in this manner. Using generating series
expressions going back to work of Bostan, Salvy, and Schost, we show how to
compute the entire output for a small overhead, without making any assumption
on the ideal $I$ other than it having dimension zero. We then propose a
refinement of this idea that partially avoids the introduction of a generic
linear form. We comment on experimental results obtained by an implementation
based on the C++ libraries Eigen, LinBox and NTL.
"
1100,"Revisit Sparse Polynomial Interpolation based on Randomized Kronecker
  Substitution","  In this paper, a new reduction based interpolation algorithm for black-box
multivariate polynomials over finite fields is given. The method is based on
two main ingredients. A new Monte Carlo method is given to reduce black-box
multivariate polynomial interpolation to black-box univariate polynomial
interpolation over any ring. The reduction algorithm leads to multivariate
interpolation algorithms with better or the same complexities most cases when
combining with various univariate interpolation algorithms. We also propose a
modified univariate Ben-or and Tiwarri algorithm over the finite field, which
has better total complexity than the Lagrange interpolation algorithm.
Combining our reduction method and the modified univariate Ben-or and Tiwarri
algorithm, we give a Monte Carlo multivariate interpolation algorithm, which
has better total complexity in most cases for sparse interpolation of black-box
polynomial over finite fields.
"
1101,Counting Solutions of a Polynomial System Locally and Exactly,"  We propose a symbolic-numeric algorithm to count the number of solutions of a
polynomial system within a local region. More specifically, given a
zero-dimensional system $f_1=\cdots=f_n=0$, with
$f_i\in\mathbb{C}[x_1,\ldots,x_n]$, and a polydisc
$\mathbf{\Delta}\subset\mathbb{C}^n$, our method aims to certify the existence
of $k$ solutions (counted with multiplicity) within the polydisc.
  In case of success, it yields the correct result under guarantee. Otherwise,
no information is given. However, we show that our algorithm always succeeds if
$\mathbf{\Delta}$ is sufficiently small and well-isolating for a $k$-fold
solution $\mathbf{z}$ of the system.
  Our analysis of the algorithm further yields a bound on the size of the
polydisc for which our algorithm succeeds under guarantee. This bound depends
on local parameters such as the size and multiplicity of $\mathbf{z}$ as well
as the distances between $\mathbf{z}$ and all other solutions. Efficiency of
our method stems from the fact that we reduce the problem of counting the roots
in $\mathbf{\Delta}$ of the original system to the problem of solving a
truncated system of degree $k$. In particular, if the multiplicity $k$ of
$\mathbf{z}$ is small compared to the total degrees of the polynomials $f_i$,
our method considerably improves upon known complete and certified methods.
  For the special case of a bivariate system, we report on an implementation of
our algorithm, and show experimentally that our algorithm leads to a
significant improvement, when integrated as inclusion predicate into an
elimination method.
"
1102,"Quantum Algorithms for Boolean Equation Solving and Quantum Algebraic
  Attack on Cryptosystems","  Decision of whether a Boolean equation system has a solution is an NPC
problem and finding a solution is NP hard. In this paper, we present a quantum
algorithm to decide whether a Boolean equation system FS has a solution and
compute one if FS does have solutions with any given success probability. The
runtime complexity of the algorithm is polynomial in the size of FS and the
condition number of FS. As a consequence, we give a polynomial-time quantum
algorithm for solving Boolean equation systems if their condition numbers are
small, say polynomial in the size of FS. We apply our quantum algorithm for
solving Boolean equations to the cryptanalysis of several important
cryptosystems: the stream cipher Trivum, the block cipher AES, the hash
function SHA-3/Keccak, and the multivariate public key cryptosystems, and show
that they are secure under quantum algebraic attack only if the condition
numbers of the corresponding equation systems are large. This leads to a new
criterion for designing cryptosystems that can against the attack of quantum
computers: their corresponding equation systems must have large condition
numbers.
"
1103,"Can one design a geometry engine? On the (un)decidability of affine
  Euclidean geometries","  We survey the status of decidabilty of the consequence relation in various
axiomatizations of Euclidean geometry. We draw attention to a widely overlooked
result by Martin Ziegler from 1980, which proves Tarski's conjecture on the
undecidability of finitely axiomatizable theories of fields. We elaborate on
how to use Ziegler's theorem to show that the consequence relations for the
first order theory of the Hilbert plane and the Euclidean plane are
undecidable. As new results we add: (A) The first order consequence relations
for Wu's orthogonal and metric geometries (Wen-Ts\""un Wu, 1984), and for the
axiomatization of Origami geometry (J. Justin 1986, H. Huzita 1991)are
undecidable.
  It was already known that the universal theory of Hilbert planes and Wu's
orthogonal geometry is decidable. We show here using elementary model theoretic
tools that (B) the universal first order consequences of any geometric theory
$T$ of Pappian planes which is consistent with the analytic geometry of the
reals is decidable.
"
1104,"A non-commutative algorithm for multiplying (7 $\times$ 7) matrices
  using 250 multiplications","  We present a non-commutative algorithm for multiplying (7x7) matrices using
250 multiplications and a non-commutative algorithm for multiplying (9x9)
matrices using 520 multiplications. These algorithms are obtained using the
same divide-and-conquer technique.
"
1105,"Automatic Generation of Bounds for Polynomial Systems with Application
  to the Lorenz System","  This study covers an analytical approach to calculate positively invariant
sets of dynamical systems. Using Lyapunov techniques and quantifier elimination
methods, an automatic procedure for determining bounds in the state space as an
enclosure of attractors is proposed. The available software tools permit an
algorithmizable process, which normally requires a good insight into the
systems dynamics and experience. As a result we get an estimation of the
attractor, whose conservatism only results from the initial choice of the
Lyapunov candidate function. The proposed approach is illustrated on the
well-known Lorenz system.
"
1106,Computer Algebra Methods in Control Systems,"  As dynamic and control systems become more complex, relying purely on
numerical computations for systems analysis and design might become extremely
expensive or totally infeasible. Computer algebra can act as an enabler for
analysis and design of such complex systems. It also provides means for
characterization of all solutions and studying them before realizing a
particular solution. This note provides a brief survey on some of the
applications of symbolic computations in control systems analysis and design.
"
1107,An Extensible Ad Hoc Interface between Lean and Mathematica,"  We implement a user-extensible ad hoc connection between the Lean proof
assistant and the computer algebra system Mathematica. By reflecting the syntax
of each system in the other and providing a flexible interface for extending
translation, our connection allows for the exchange of arbitrary information
between the two systems. We show how to make use of the Lean metaprogramming
framework to verify certain Mathematica computations, so that the rigor of the
proof assistant is not compromised.
"
1108,"Computing the Inverse Mellin Transform of Holonomic Sequences using
  Kovacic's Algorithm","  We describe how the extension of a solver for linear differential equations
by Kovacic's algorithm helps to improve a method to compute the inverse Mellin
transform of holonomic sequences. The method is implemented in the computer
algebra package HarmonicSums.
"
1109,"Deciding and Interpolating Algebraic Data Types by Reduction (Technical
  Report)","  Recursive algebraic data types (term algebras, ADTs) are one of the most
well-studied theories in logic, and find application in contexts including
functional programming, modelling languages, proof assistants, and
verification. At this point, several state-of-the-art theorem provers and SMT
solvers include tailor-made decision procedures for ADTs, and version 2.6 of
the SMT-LIB standard includes support for ADTs. We study an extremely simple
approach to decide satisfiability of ADT constraints, the reduction of ADT
constraints to equisatisfiable constraints over uninterpreted functions (EUF)
and linear integer arithmetic (LIA). We show that the reduction approach gives
rise to both decision and Craig interpolation procedures in (extensions of)
ADTs.
"
1110,On Division Polynomial PIT and Supersingularity,"  For an elliptic curve $E$ over a finite field $\F_q$, where $q$ is a prime
power, we propose new algorithms for testing the supersingularity of $E$. Our
algorithms are based on the Polynomial Identity Testing (PIT) problem for the
$p$-th division polynomial of $E$. In particular, an efficient algorithm using
points of high order on $E$ is given.
"
1111,Resolving zero-divisors using Hensel lifting,"  Algorithms which compute modulo triangular sets must respect the presence of
zero-divisors. We present Hensel lifting as a tool for dealing with them. We
give an application: a modular algorithm for computing GCDs of univariate
polynomials with coefficients modulo a radical triangular set over the
rationals. Our modular algorithm naturally generalizes previous work from
algebraic number theory. We have implemented our algorithm using Maple's RECDEN
package. We compare our implementation with the procedure RegularGcd in the
RegularChains package.
"
1112,Invariant Generation for Multi-Path Loops with Polynomial Assignments,"  Program analysis requires the generation of program properties expressing
conditions to hold at intermediate program locations. When it comes to programs
with loops, these properties are typically expressed as loop invariants. In
this paper we study a class of multi-path program loops with numeric variables,
in particular nested loops with conditionals, where assignments to program
variables are polynomial expressions over program variables. We call this class
of loops extended P-solvable and introduce an algorithm for generating all
polynomial invariants of such loops. By an iterative procedure employing
Gr\""obner basis computation, our approach computes the polynomial ideal of the
polynomial invariants of each program path and combines these ideals
sequentially until a fixed point is reached. This fixed point represents the
polynomial ideal of all polynomial invariants of the given extended P-solvable
loop. We prove termination of our method and show that the maximal number of
iterations for reaching the fixed point depends linearly on the number of
program variables and the number of inner loops. In particular, for a loop with
m program variables and r conditional branches we prove an upper bound of m*r
iterations. We implemented our approach in the Aligator software package.
Furthermore, we evaluated it on 18 programs with polynomial arithmetic and
compared it to existing methods in invariant generation. The results show the
efficiency of our approach.
"
1113,Desingularization in the $q$-Weyl algebra,"  In this paper, we study the desingularization problem in the first $q$-Weyl
algebra. We give an order bound for desingularized operators, and thus derive
an algorithm for computing desingularized operators in the first $q$-Weyl
algebra. Moreover, an algorithm is presented for computing a generating set of
the first $q$-Weyl closure of a given $q$-difference operator. As an
application, we certify that several instances of the colored Jones polynomial
are Laurent polynomial sequences by computing the corresponding desingularized
operator.
"
1114,"Comprehensive Optimization of Parametric Kernels for Graphics Processing
  Units","  This work deals with the optimization of computer programs targeting Graphics
Processing Units (GPUs). The goal is to lift, from programmers to optimizing
compilers, the heavy burden of determining program details that are dependent
on the hardware characteristics. The expected benefit is to improve robustness,
portability and efficiency of the generated computer programs. We address these
requirements by: (1) treating machine and program parameters as unknown symbols
during code generation, and (2) generating optimized programs in the form of a
case discussion, based on the possible values of the machine and program
parameters. By taking advantage of recent advances in the area of computer
algebra, preliminary experimentation yield promising results.
"
1115,Fast computation of approximant bases in canonical form,"  In this article, we design fast algorithms for the computation of approximant
bases in shifted Popov normal form. We first recall the algorithm known as
PM-Basis, which will be our second fundamental engine after polynomial matrix
multiplication: most other fast approximant basis algorithms basically aim at
efficiently reducing the input instance to instances for which PM-Basis is
fast. Such reductions usually involve partial linearization techniques due to
Storjohann, which have the effect of balancing the degrees and dimensions in
the manipulated matrices.
  Following these ideas, Zhou and Labahn gave two algorithms which are faster
than PM-Basis for important cases including Hermite-Pade approximation, yet
only for shifts whose values are concentrated around the minimum or the maximum
value. The three mentioned algorithms were designed for balanced orders and
compute approximant bases that are generally not normalized. Here, we show how
they can be modified to return the shifted Popov basis without impact on their
cost bound; besides, we extend Zhou and Labahn's algorithms to arbitrary
orders.
  Furthermore, we give an algorithm which handles arbitrary shifts with one
extra logarithmic factor in the cost bound compared to the above algorithms. To
the best of our knowledge, this improves upon previously known algorithms for
arbitrary shifts, including for particular cases such as Hermite-Pade
approximation. This algorithm is based on a recent divide and conquer approach
which reduces the general case to the case where information on the output
degree is available. As outlined above, we solve the latter case via partial
linearizations and PM-Basis.
"
1116,"Computing effectively stabilizing controllers for a class of $n$D
  systems","  In this paper, we study the internal stabilizability and internal
stabilization problems for multidimensional (nD) systems. Within the fractional
representation approach, a multidimen-sional system can be studied by means of
matrices with entries in the integral domain of structurally stable rational
fractions, namely the ring of rational functions which have no poles in the
closed unit polydisc U n = {z = (z 1 ,. .. , z n) $\in$ C n | |z 1 | 1,. .. ,
|z n | 1}. It is known that the internal stabilizability of a multidimensional
system can be investigated by studying a certain polynomial ideal I = p 1 ,. ..
, p r that can be explicitly described in terms of the transfer matrix of the
plant. More precisely the system is stabilizable if and only if V (I) = {z
$\in$ C n | p 1 (z) = $\times$ $\times$ $\times$ = p r (z) = 0} $\cap$ U n =
$\emptyset$. In the present article, we consider the specific class of linear
nD systems (which includes the class of 2D systems) for which the ideal I is
zero-dimensional, i.e., the p i 's have only a finite number of common complex
zeros. We propose effective symbolic-numeric algorithms for testing if V (I)
$\cap$ U n = $\emptyset$, as well as for computing, if it exists, a stable
polynomial p $\in$ I which allows the effective computation of a stabilizing
controller. We illustrate our algorithms through an example and finally provide
running times of prototype implementations for 2D and 3D systems.
"
1117,A random walk through experimental mathematics,"  We describe our adventures in creating a new first-year course in
Experimental Mathematics that uses active learning. We used a state-of-the-art
facility, called The Western Active Learning Space, and got the students to
""drive the spaceship"" (at least a little bit). This paper describes some of our
techniques for pedagogy, some of the vignettes of experimental mathematics that
we used, and some of the outcomes. EYSC was a student in the
simultaneously-taught senior sister course ""Open Problems in Experimental
Mathematics"" the first time it was taught and an unofficial co-instructor the
second time. Jon Borwein attended the Project Presentation Day (the second
time) and gave thoughtful feedback to each student. This paper is dedicated to
his memory.
"
1118,The Complexity of Subdivision for Diameter-Distance Tests,"  We present a general framework for analyzing the complexity of
subdivision-based algorithms whose tests are based on the sizes of regions and
their distance to certain sets (often varieties) intrinsic to the problem under
study. We call such tests diameter-distance tests. We illustrate that
diameter-distance tests are common in the literature by proving that many
interval arithmetic-based tests are, in fact, diameter-distance tests. For this
class of algorithms, we provide both non-adaptive bounds for the complexity,
based on separation bounds, as well as adaptive bounds, by applying the
framework of continuous amortization.
  Using this structure, we provide the first complexity analysis for the
algorithm by Plantinga and Vegeter for approximating real implicit curves and
surfaces. We present both adaptive and non-adaptive a priori worst-case bounds
on the complexity of this algorithm both in terms of the number of subregions
constructed and in terms of the bit complexity for the construction. Finally,
we construct families of hypersurfaces to prove that our bounds are tight.
"
1119,Block SOS Decomposition,"  A widely used method for solving SOS (Sum Of Squares) decomposition problem
is to reduce it to the problem of semi-definite programs (SDPs) which can be
efficiently solved in theory. In practice, although many SDP solvers can work
out some problems of big scale, the efficiency and reliability of such method
decrease greatly while the input size increases. Recently, by exploiting the
sparsity of the input SOS decomposition problem, some preprocessing algorithms
were proposed [5,17], which first divide the input problem satisfying special
definitions or properties into smaller SDP problems and then pass the smaller
ones to SDP solvers to obtain reliable results efficiently. A natural question
is that to what extent the above mentioned preprocessing algorithms work. That
is, how many polynomials satisfying those definitions or properties are there
in the SOS polynomials? In this paper, we define a concept of block SOS
decomposable polynomials which is a generalization of those special classes in
[5] and [17]. Roughly speaking, it is a class of polynomials whose SOS
decomposition problem can be transformed into smaller ones (in other words, the
corresponding SDP matrices can be block-diagnolized) by considering their
supports only (coefficients are not considered). Then we prove that the set of
block SOS decomposable polynomials has measure zero in the set of SOS
polynomials. That means if we only consider supports (not with coefficients) of
polynomials, such algorithms decreasing the size of SDPs for those SDP-based
SOS solvers can only work on very few polynomials. As a result, this shows that
the SOS decomposition problems that can be optimized by the above mentioned
preprocessing algorithms are very few.
"
1120,"Fast Algorithm for Calculating the Minimal Annihilating Polynomials of
  Matrices via Pseudo Annihilating Polynomials","  Minimal annihilating polynomials are very useful in a wide variety of
algorithms in exact linear algebra. A new efficient method is proposed for
calculating the minimal annihilating polynomials for all the unit vectors, for
a square matrix over a field of characteristic zero. Key ideas of the proposed
method are the concept of pseudo annihilating polynomial and the use of binary
splitting technique. Efficiency of the resulting algorithms is shown by
arithmetic time complexity analysis.
"
1121,Algorithmic Linearly Constrained Gaussian Processes,"  We algorithmically construct multi-output Gaussian process priors which
satisfy linear differential equations. Our approach attempts to parametrize all
solutions of the equations using Gr\""obner bases. If successful, a push forward
Gaussian process along the paramerization is the desired prior. We consider
several examples from physics, geomathematics and control, among them the full
inhomogeneous system of Maxwell's equations. By bringing together stochastic
learning and computer algebra in a novel way, we combine noisy observations
with precise algebraic computations.
"
1122,"An Algorithm to Decompose Permutation Representations of Finite Groups:
  Polynomial Algebra Approach","  We describe an algorithm for splitting permutation representations of finite
group over fields of characteristic zero into irreducible components. The
algorithm is based on the fact that the components of the invariant inner
product in invariant subspaces are operators of projection into these
subspaces. An important element of the algorithm is the calculation of
Gr\""obner bases of polynomial ideals. A preliminary implementation of the
algorithm splits representations up to dimensions of tens of thousands. Some
examples of computations are given in appendix.
"
1123,"Symmetries and similarities of planar algebraic curves using harmonic
  polynomials","  We present novel, deterministic, efficient algorithms to compute the
symmetries of a planar algebraic curve, implicitly defined, and to check
whether or not two given implicit planar algebraic curves are similar, i.e.
equal up to a similarity transformation. Both algorithms are based on the fact,
well-known in Harmonic Analysis, that the Laplacian operator commutes with
orthogonal transformations, and on efficient algorithms to find the
symmetriessimilarities of a harmonic algebraic curvetwo given harmonic
algebraic curves. In fact, we show that in general the problem can be reduced
to the harmonic case, except for some special cases, easy to treat.
"
1124,"An efficient algorithm for global interval solution of nonlinear
  algebraic equations and its GPGPU implementation","  Solving nonlinear algebraic equations is a classic mathematics problem, and
common in scientific researches and engineering applications. There are many
numeric, symbolic and numeric-symbolic methods of solving (real) solutions.
Unlucky, these methods are constrained by some factors, e.g., high complexity,
slow serial calculation, and the notorious intermediate expression expansion.
Especially when the count of variables is larger than six, the efficiency is
decreasing drastically. In this paper, according to the property of physical
world, we pay attention to nonlinear algebraic equations whose variables are in
fixed constraints, and get meaningful real solutions. Combining with
parallelism of GPGPU, we present an efficient algorithm, by searching the
solution space globally and solving the nonlinear algebraic equations with real
interval solutions. Furthermore, we realize the Hansen-Sengupta method on
GPGPU. The experiments show that our method can solve many nonlinear algebraic
equations, and the results are accurate and more efficient compared to
traditional serial methods.
"
1125,"Desingularization of First Order Linear Difference Systems with Rational
  Function Coefficients","  It is well known that for a first order system of linear difference equations
with rational function coefficients, a solution that is holomorphic in some
left half plane can be analytically continued to a meromorphic solution in the
whole complex plane. The poles stem from the singularities of the rational
function coefficients of the system. Just as for differential equations, not
all of these singularities necessarily lead to poles in solutions, as they
might be what is called removable. In our work, we show how to detect and
remove these singularities and further study the connection between poles of
solutions and removable singularities. We describe two algorithms to
(partially) desingularize a given difference system and present a
characterization of removable singularities in terms of shifts of the original
system.
"
1126,"A Signature-based Algorithm for computing Computing Gr\""obner Bases over
  Principal Ideal Domains","  Signature-based algorithms have become a standard approach for Gr\""obner
basis computations for polynomial systems over fields, but how to extend these
techniques to coefficients in general rings is not yet as well understood.
  In this paper, we present a proof-of-concept signature-based algorithm for
computing Gr\""obner bases over commutative integral domains. It is adapted from
a general version of M\""oller's algorithm (1988) which considers reductions by
multiple polynomials at each step. This algorithm performs reductions with
non-decreasing signatures, and in particular, signature drops do not occur.
When the coefficients are from a principal ideal domain (e.g. the ring of
integers or the ring of univariate polynomials over a field), we prove
correctness and termination of the algorithm, and we show how to use signature
properties to implement classic signature-based criteria to eliminate some
redundant reductions. In particular, if the input is a regular sequence, the
algorithm operates without any reduction to 0.
  We have written a toy implementation of the algorithm in Magma. Early
experimental results suggest that the algorithm might even be correct and
terminate in a more general setting, for polynomials over a unique
factorization domain (e.g. the ring of multivariate polynomials over a field or
a PID).
"
1127,"On the chordality of polynomial sets in triangular decomposition in
  top-down style","  In this paper the chordal graph structures of polynomial sets appearing in
triangular decomposition in top-down style are studied when the input
polynomial set to decompose has a chordal associated graph. In particular, we
prove that the associated graph of one specific triangular set computed in any
algorithm for triangular decomposition in top-down style is a subgraph of the
chordal graph of the input polynomial set and that all the polynomial sets
including all the computed triangular sets appearing in one specific
simply-structured algorithm for triangular decomposition in top-down style
(Wang's method) have associated graphs which are subgraphs of the the chordal
graph of the input polynomial set. These subgraph structures in triangular
decomposition in top-down style are multivariate generalization of existing
results for Gaussian elimination and may lead to specialized efficient
algorithms and refined complexity analyses for triangular decomposition of
chordal polynomial sets.
"
1128,Certification of minimal approximant bases,"  For a given computational problem, a certificate is a piece of data that one
(the prover) attaches to the output with the aim of allowing efficient
verification (by the verifier) that this output is correct. Here, we consider
the minimal approximant basis problem, for which the fastest known algorithms
output a polynomial matrix of dimensions $m \times m$ and average degree $D/m$
using $O\tilde{~}(m^\omega \frac{D}{m})$ field operations. We propose a
certificate which, for typical instances of the problem, is computed by the
prover using $O(m^\omega \frac{D}{m})$ additional field operations and allows
verification of the approximant basis by a Monte Carlo algorithm with cost
bound $O(m^\omega + m D)$.
  Besides theoretical interest, our motivation also comes from the fact that
approximant bases arise in most of the fastest known algorithms for linear
algebra over the univariate polynomials; thus, this work may help in designing
certificates for other polynomial matrix computations. Furthermore,
cryptographic challenges such as breaking records for discrete logarithm
computations or for integer factorization rely in particular on computing
minimal approximant bases for large instances: certificates can then be used to
provide reliable computation on outsourced and error-prone clusters.
"
1129,Computing Popov and Hermite forms of rectangular polynomial matrices,"  We consider the computation of two normal forms for matrices over the
univariate polynomials: the Popov form and the Hermite form. For matrices which
are square and nonsingular, deterministic algorithms with satisfactory cost
bounds are known. Here, we present deterministic, fast algorithms for
rectangular input matrices. The obtained cost bound for the Popov form matches
the previous best known randomized algorithm, while the cost bound for the
Hermite form improves on the previous best known ones by a factor which is at
least the largest dimension of the input matrix.
"
1130,Error correction in fast matrix multiplication and inverse,"  We present new algorithms to detect and correct errors in the product of two
matrices, or the inverse of a matrix, over an arbitrary field. Our algorithms
do not require any additional information or encoding other than the original
inputs and the erroneous output. Their running time is softly linear in the
number of nonzero entries in these matrices when the number of errors is
sufficiently small, and they also incorporate fast matrix multiplication so
that the cost scales well when the number of errors is large. These algorithms
build on the recent result of Gasieniec et al (2017) on correcting matrix
products, as well as existing work on verification algorithms, sparse low-rank
linear algebra, and sparse polynomial interpolation.
"
1131,Additive Decompositions in Primitive Extensions,"  This paper extends the classical Ostrogradsky-Hermite reduction for rational
functions to more general functions in primitive extensions of certain types.
For an element $f$ in such an extension $K$, the extended reduction decomposes
$f$ as the sum of a derivative in $K$ and another element $r$ such that $f$ has
an antiderivative in $K$ if and only if $r=0$; and $f$ has an elementary
antiderivative over $K$ if and only if $r$ is a linear combination of
logarithmic derivatives over the constants when $K$ is a logarithmic extension.
Moreover, $r$ is minimal in some sense. Additive decompositions may lead to
reduction-based creative-telescoping methods for nested logarithmic functions,
which are not necessarily $D$-finite.
"
1132,Exact algorithms for semidefinite programs with degenerate feasible set,"  Given symmetric matrices $A_0, A_1, \ldots, A_n$ of size $m$ with rational
entries, the set of real vectors $x = (x_1, \ldots, x_n)$ such that the matrix
$A_0 + x_1 A_1 + \cdots + x_n A_n$ has non-negative eigenvalues is called a
spectrahedron. Minimization of linear functions over spectrahedra is called
semidefinite programming. Such problems appear frequently in control theory and
real algebra, especially in the context of nonnegativity certificates for
multivariate polynomials based on sums of squares. Numerical software for
semidefinite programming are mostly based on interior point methods, assuming
non-degeneracy properties such as the existence of an interior point in the
spectrahedron. In this paper, we design an exact algorithm based on symbolic
homotopy for solving semidefinite programs without assumptions on the feasible
set, and we analyze its complexity. Because of the exactness of the output, it
cannot compete with numerical routines in practice. However, we prove that
solving such problems can be done in polynomial time if either $n$ or $m$ is
fixed.
"
1133,"Quantum Algorithm for Optimization and Polynomial System Solving over
  Finite Field and Application to Cryptanalysis","  In this paper, we give quantum algorithms for two fundamental computation
problems: solving polynomial systems over finite fields and optimization where
the arguments of the objective function and constraints take values from a
finite field or a bounded interval of integers. The quantum algorithms can
solve these problems with any given success probability and have polynomial
runtime complexities in the size of the input, the degree of the inequality
constraints, and the condition number of certain matrices derived from the
problem. So, we achieved exponential speedup for these problems when their
condition numbers are small. As applications, quantum algorithms are given to
three basic computational problems in cryptography: the polynomial system with
noise problem, the short integer solution problem, the shortest vector problem,
as well as the cryptanalysis for the lattice based NTRU cryptosystem. It is
shown that these problems and NTRU can against quantum computer attacks only if
their condition numbers are large, so the condition number could be used as a
new criterion for the lattice based post-quantum cryptosystems.
"
1134,Frobenius Additive Fast Fourier Transform,"  In ISSAC 2017, van der Hoeven and Larrieu showed that evaluating a polynomial
P in GF(q)[x] of degree <n at all n-th roots of unity in GF($q^d$) can
essentially be computed d-time faster than evaluating Q in GF($q^d$)[x] at all
these roots, assuming GF($q^d$) contains a primitive n-th root of unity. Termed
the Frobenius FFT, this discovery has a profound impact on polynomial
multiplication, especially for multiplying binary polynomials, which finds
ample application in coding theory and cryptography. In this paper, we show
that the theory of Frobenius FFT beautifully generalizes to a class of additive
FFT developed by Cantor and Gao-Mateer. Furthermore, we demonstrate the power
of Frobenius additive FFT for q=2: to multiply two binary polynomials whose
product is of degree <256, the new technique requires only 29,005 bit
operations, while the best result previously reported was 33,397. To the best
of our knowledge, this is the first time that FFT-based multiplication
outperforms Karatsuba and the like at such a low degree in terms of
bit-operation count.
"
1135,Symmetries of Quantified Boolean Formulas,"  While symmetries are well understood for Boolean formulas and successfully
exploited in practical SAT solving, less is known about symmetries in
quantified Boolean formulas (QBF). There are some works introducing adaptions
of propositional symmetry breaking techniques, with a theory covering only very
specific parts of QBF symmetries. We present a general framework that gives a
concise characterization of symmetries of QBF. Our framework naturally
incorporates the duality of universal and existential symmetries resulting in a
general basis for QBF symmetry breaking.
"
1136,Unbounded Software Model Checking with Incremental SAT-Solving,"  This paper describes a novel unbounded software model checking approach to
find errors in programs written in the C language based on incremental
SAT-solving. Instead of using the traditional assumption based API to
incremental SAT solvers we use the DimSpec format that is used in SAT based
automated planning. A DimSpec formula consists of four CNF formulas
representing the initial, goal and intermediate states and the relations
between each pair of neighboring states of a transition system. We present a
new tool called LLUMC which encodes the presence of certain errors in a C
program into a DimSpec formula, which can be solved by either an incremental
SAT-based DimSpec solver or the IC3 algorithm for invariant checking. We
evaluate the approach in the context of SAT-based model checking for both the
incremental SAT-solving and the IC3 algorithm. We show that our encoding
expands the functionality of bounded model checkers by also covering large and
infinite loops, while still maintaining a feasible time performance.
Furthermore, we demonstrate that our approach offers the opportunity to
generate runtime-optimizations by utilizing parallel SAT-solving.
"
1137,"Integration in terms of exponential integrals and incomplete gamma
  functions I","  This paper provides a Liouville principle for integration in terms of
exponential integrals and incomplete gamma functions.
"
1138,"Formal Analysis of Galois Field Arithmetics - Parallel Verification and
  Reverse Engineering","  Galois field (GF) arithmetic circuits find numerous applications in
communications, signal processing, and security engineering. Formal
verification techniques of GF circuits are scarce and limited to circuits with
known bit positions of the primary inputs and outputs. They also require
knowledge of the irreducible polynomial $P(x)$, which affects final hardware
implementation. This paper presents a computer algebra technique that performs
verification and reverse engineering of GF($2^m$) multipliers directly from the
gate-level implementation. The approach is based on extracting a unique
irreducible polynomial in a parallel fashion and proceeds in three steps: 1)
determine the bit position of the output bits; 2) determine the bit position of
the input bits; and 3) extract the irreducible polynomial used in the design.
We demonstrate that this method is able to reverse engineer GF($2^m$)
multipliers in \textit{m} threads. Experiments performed on synthesized
\textit{Mastrovito} and \textit{Montgomery} multipliers with different $P(x)$,
including NIST-recommended polynomials, demonstrate high efficiency of the
proposed method.
"
1139,Faster integer multiplication using short lattice vectors,"  We prove that $n$-bit integers may be multiplied in $O(n \log n \, 4^{\log^*
n})$ bit operations. This complexity bound had been achieved previously by
several authors, assuming various unproved number-theoretic hypotheses. Our
proof is unconditional, and depends in an essential way on Minkowski's theorem
concerning lattice vectors in symmetric convex sets.
"
1140,ZpL: a p-adic precision package,"  We present a new package ZpL for the mathematical software system SM. It
implements a sharp tracking of precision on p-adic numbers, following the
theory of ultrametric precision introduced in [4]. The underlying algorithms
are mostly based on automatic dierentiation techniques. We introduce them,
study their complexity and discuss our design choices. We illustrate the
bene-ts of our package (in comparison with previous implementations) with a
large sample of examples coming from linear algebra, com-mutative algebra and
dierential equations.
"
1141,On Probabilistic Term Rewriting,"  We study the termination problem for probabilistic term rewrite systems. We
prove that the interpretation method is sound and complete for a strengthening
of positive almost sure termination, when abstract reduction systems and term
rewrite systems are considered. Two instances of the interpretation method -
polynomial and matrix interpretations - are analyzed and shown to capture
interesting and nontrivial examples when automated. We capture probabilistic
computation in a novel way by way of multidistribution reduction sequences,
this way accounting for both the nondeterminism in the choice of the redex and
the probabilism intrinsic in firing each rule.
"
1142,How to generate all possible rational Wilf-Zeilberger pairs?,"  A Wilf--Zeilberger pair $(F, G)$ in the discrete case satisfies the equation
$ F(n+1, k) - F(n, k) = G(n, k+1) - G(n, k)$. We present a structural
description of all possible rational Wilf--Zeilberger pairs and their
continuous and mixed analogues.
"
1143,On Exact Polya and Putinar's Representations,"  We consider the problem of finding exact sums of squares (SOS) decompositions
for certain classes of non-negative multivariate polynomials, relying on
semidefinite programming (SDP) solvers.
  We start by providing a hybrid numeric-symbolic algorithm computing exact
rational SOS decompositions for polynomials lying in the interior of the SOS
cone. It computes an approximate SOS decomposition for a perturbation of the
input polynomial with an arbitrary-precision SDP solver. An exact SOS
decomposition is obtained thanks to the perturbation terms. We prove that bit
complexity estimates on output size and runtime are both polynomial in the
degree of the input polynomial and simply exponential in the number of
variables. Next, we apply this algorithm to compute exact Polya and Putinar's
representations respectively for positive definite forms and positive
polynomials over basic compact semi-algebraic sets. We also compare the
implementation of our algorithms with existing methods in computer algebra
including cylindrical algebraic decomposition and critical point method.
"
1144,Solving determinantal systems using homotopy techniques,"  Let $\K$ be a field of characteristic zero and $\Kbar$ be an algebraic
closure of $\K$. Consider a sequence of polynomials$G=(g\_1,\dots,g\_s)$ in
$\K[X\_1,\dots,X\_n]$, a polynomial matrix $\F=[f\_{i,j}] \in
\K[X\_1,\dots,X\_n]^{p \times q}$, with $p \leq q$,and the algebraic set
$V\_p(F, G)$ of points in $\KKbar$ at which all polynomials in $\G$ and all
$p$-minors of $\F$vanish. Such polynomial systems appear naturally in e.g.
polynomial optimization, computational geometry.We provide bounds on the number
of isolated points in $V\_p(F, G)$ depending on the maxima of the degrees in
rows (resp. columns) of $\F$. Next, we design homotopy algorithms for computing
those points. These algorithms take advantage of the determinantal structure of
the system defining $V\_p(F, G)$. In particular, the algorithms run in time
that is polynomial in the bound on the number of isolated points.
"
1145,OpenMath and SMT-LIB,"  OpenMath and SMT-LIB are languages with very different origins, but both
""represent mathematics"". We describe SMT-LIB for the OpenMath community and
consider adaptations for both languages to support the growing SC-Square
initiative.
"
1146,Probabilistic Analysis of Block Wiedemann for Leading Invariant Factors,"  We determine the probability, structure dependent, that the block Wiedemann
algorithm correctly computes leading invariant factors. This leads to a tight
lower bound for the probability, structure independent. We show, using block
size slightly larger than $r$, that the leading $r$ invariant factors are
computed correctly with high probability over any field. Moreover, an algorithm
is provided to compute the probability bound for a given matrix size and thus
to select the block size needed to obtain the desired probability. The worst
case probability bound is improved, post hoc, by incorporating the partial
information about the invariant factors.
"
1147,Regular cylindrical algebraic decomposition,"  We show that a strong well-based cylindrical algebraic decomposition P of a
bounded semi-algebraic set is a regular cell decomposition, in any dimension
and independently of the method by which P is constructed. Being well-based is
a global condition on P that holds for the output of many widely used
algorithms. We also show the same for S of dimension at most 3 and P a strong
cylindrical algebraic decomposition that is locally boundary simply connected:
this is a purely local extra condition.
"
1148,"Solving First Order Autonomous Algebraic Ordinary Differential Equations
  by Places","  Given a first-order autonomous algebraic ordinary differential equation, we
present a method for computing formal power series solutions by means of
places. We provide an algorithm for computing a full characterization of
possible initial values, classified in terms of the number of distinct formal
power series solutions extending them. In addition, if a particular initial
value is given, we present a second algorithm that computes all the formal
power series solutions, up to a suitable degree, corresponding to it.
Furthermore, when the ground field is the field of the complex numbers, we
prove that the computed formal power series solutions are all convergent in
suitable neighborhoods.
"
1149,"A New Bound on Hrushovski's Algorithm for Computing the Galois Group of
  a Linear Differential Equation","  The complexity of computing the Galois group of a linear differential
equation is of general interest. In a recent work, Feng gave the first degree
bound on Hrushovski's algorithm for computing the Galois group of a linear
differential equation. This bound is the degree bound of the polynomials used
in the first step of the algorithm for finding a proto-Galois group and is
sextuply exponential in the order of the differential equation. In this paper,
we use Szanto's algorithm of triangular representation for algebraic sets to
analyze the complexity of computing the Galois group of a linear differential
equation and we give a new bound which is triple exponential in the order of
the given differential equation.
"
1150,"Truncated Normal Forms for Solving Polynomial Systems: Generalized and
  Efficient Algorithms","  We consider the problem of finding the isolated common roots of a set of
polynomial functions defining a zero-dimensional ideal I in a ring R of
polynomials over C. Normal form algorithms provide an algebraic approach to
solve this problem. The framework presented in Telen et al. (2018) uses
truncated normal forms (TNFs) to compute the algebra structure of R/I and the
solutions of I. This framework allows for the use of much more general bases
than the standard monomials for R/I. This is exploited in this paper to
introduce the use of two special (nonmonomial) types of basis functions with
nice properties. This allows, for instance, to adapt the basis functions to the
expected location of the roots of I. We also propose algorithms for efficient
computation of TNFs and a generalization of the construction of TNFs in the
case of non-generic zero-dimensional systems. The potential of the TNF method
and usefulness of the new results are exposed by many experiments.
"
1151,Computing Periods of Hypersurfaces,"  We give an algorithm to compute the periods of smooth projective
hypersurfaces of any dimension. This is an improvement over existing algorithms
which could only compute the periods of plane curves. Our algorithm reduces the
evaluation of period integrals to an initial value problem for ordinary
differential equations of Picard-Fuchs type. In this way, the periods can be
computed to extreme-precision in order to study their arithmetic properties.
The initial conditions are obtained by an exact determination of the cohomology
pairing on Fermat hypersurfaces with respect to a natural basis.
"
1152,Convolutions of Liouvillian Sequences,"  While Liouvillian sequences are closed under many operations, simple examples
show that they are not closed under convolution, and the same goes for
d'Alembertian sequences. Nevertheless, we show that d'Alembertian sequences are
closed under convolution with rationally d'Alembertian sequences, and that
Liouvillian sequences are closed under convolution with rationally Liouvillian
sequences.
"
1153,"On Formal Power Series Solutions of Algebraic Ordinary Differential
  Equations","  We propose a computational method to determine when a solution modulo a
certain power of the independent variable of a given algebraic differential
equation (AODE) can be extended to a formal power series solution. The
existence and the uniqueness conditions for the initial value problems for
AODEs at singular points are included. Moreover, when the existence is
confirmed, we present the algebraic structure of the set of all formal power
series solutions satisfying the initial value conditions.
"
1154,"Multiplying boolean Polynomials with Frobenius Partitions in Additive
  Fast Fourier Transform","  We show a new algorithm and its implementation for multiplying
bit-polynomials of large degrees. The algorithm is based on evaluating
polynomials at a specific set comprising a natural set for evaluation with
additive FFT and a high order element under Frobenius map of $\mathbb{F}_{2}$.
With the high order element, we can derive more values of the polynomials under
Frobenius map. Besides, we also adapt the additive FFT to efficiently evaluate
polynomials at the set with an encoding process.
  For the implementation, we reorder the computations in the additive FFT for
reducing the number of memory writes and hiding the latency for reads. The
algebraic operations, including field multiplication, bit-matrix transpose, and
bit-matrix multiplication, are implemented with efficient SIMD instructions. As
a result, we effect a software of best known efficiency, shown in our
experiments.
"
1155,"Normal and Triangular Determinantal Representations of Multivariate
  Polynomials","  In this paper we give a new and simple algorithm to put any multivariate
polynomial into a normal determinant form in which each entry has the form ,
and in each column the same variable appears. We also apply the algorithm to
obtain a triangular determinant representation, a reduced determinant
representation, and a uniform determinant representation of any multivariable
polynomial. The algorithm could be useful for obtaining representations of
dimensions smaller than those available up to now to solve numerical problems.
"
1156,"Applying Computer Algebra Systems with SAT Solvers to the Williamson
  Conjecture","  We employ tools from the fields of symbolic computation and satisfiability
checking---namely, computer algebra systems and SAT solvers---to study the
Williamson conjecture from combinatorial design theory and increase the bounds
to which Williamson matrices have been enumerated. In particular, we completely
enumerate all Williamson matrices of even order up to and including 70 which
gives us deeper insight into the behaviour and distribution of Williamson
matrices. We find that, in contrast to the case when the order is odd,
Williamson matrices of even order are quite plentiful and exist in every even
order up to and including 70. As a consequence of this and a new construction
for 8-Williamson matrices we construct 8-Williamson matrices in all odd orders
up to and including 35. We additionally enumerate all Williamson matrices whose
orders are divisible by 3 and less than 70, finding one previously unknown set
of Williamson matrices of order 63.
"
1157,Groebner bases of reaction networks with intermediate species,"  In this work we consider the computation of Groebner bases of the steady
state ideal of reaction networks equipped with mass-action kinetics.
Specifically, we focus on the role of intermediate species and the relation
between the extended network (with intermediate species) and the core network
(without intermediate species).
  We show that a Groebner basis of the steady state ideal of the core network
always lifts to a Groebner basis of the steady state ideal of the extended
network by means of linear algebra, with a suitable choice of monomial order.
As illustrated with examples, this contributes to a substantial reduction of
the computation time, due mainly to the reduction in the number of variables
and polynomials. We further show that if the steady state ideal of the core
network is binomial, then so is the case for the extended network, as long as
an extra condition is fulfilled. For standard networks, this extra condition
can be visually explored from the network structure alone.
"
1158,The Geometry of SDP-Exactness in Quadratic Optimization,"  Consider the problem of minimizing a quadratic objective subject to quadratic
equations. We study the semialgebraic region of objective functions for which
this problem is solved by its semidefinite relaxation. For the Euclidean
distance problem, this is a bundle of spectrahedral shadows surrounding the
given variety. We characterize the algebraic boundary of this region and we
derive a formula for its degree.
"
1159,"Definite Sums as Solutions of Linear Recurrences With Polynomial
  Coefficients","  We present an algorithm which, given a linear recurrence operator $L$ with
polynomial coefficients, $m \in \mathbb{N}\setminus\{0\}$, $a_1,a_2,\ldots,a_m
\in \mathbb{N}\setminus\{0\}$ and $b_1,b_2,\ldots,b_m \in \mathbb{K}$, returns
a linear recurrence operator $L'$ with rational coefficients such that for
every sequence $h$, \[ L\left(\sum_{k=0}^\infty \prod_{i=1}^m \binom{a_i n +
b_i}{k} h_k\right) = 0 \] if and only if $L' h = 0$.
"
1160,"Simulation-Based Reachability Analysis for High-Index Large Linear
  Differential Algebraic Equations","  Reachability analysis is a fundamental problem for safety verification and
falsification of Cyber-Physical Systems (CPS) whose dynamics follow physical
laws usually represented as differential equations. In the last two decades,
numerous reachability analysis methods and tools have been proposed for a
common class of dynamics in CPS known as ordinary differential equations (ODE).
However, there is lack of methods dealing with differential algebraic equations
(DAE) which is a more general class of dynamics that is widely used to describe
a variety of problems from engineering and science such as multibody mechanics,
electrical cicuit design, incompressible fluids, molecular dynamics and
chemcial process control. Reachability analysis for DAE systems is more complex
than ODE systems, especially for high-index DAEs because they contain both a
differential part (i.e., ODE) and algebraic constraints (AC). In this paper, we
extend the recent scalable simulation-based reachability analysis in
combination with decoupling techniques for a class of high-index large linear
DAEs. In particular, a high-index linear DAE is first decoupled into one ODE
and one or several AC subsystems based on the well-known Marz decoupling method
ultilizing admissible projectors. Then, the discrete reachable set of the DAE,
represented as a list of star-sets, is computed using simulation. Unlike ODE
reachability analysis where the initial condition is freely defined by a user,
in DAE cases, the consistency of the inititial condition is an essential
requirement to guarantee a feasible solution. Therefore, a thorough check for
the consistency is invoked before computing the discrete reachable set. Our
approach sucessfully verifies (or falsifies) a wide range of practical,
high-index linear DAE systems in which the number of state variables varies
from several to thousands.
"
1161,A Blackbox Polynomial System Solver on Parallel Shared Memory Computers,"  A numerical irreducible decomposition for a polynomial system provides
representations for the irreducible factors of all positive dimensional
solution sets of the system, separated from its isolated solutions. Homotopy
continuation methods are applied to compute a numerical irreducible
decomposition. Load balancing and pipelining are techniques in a parallel
implementation on a computer with multicore processors. The application of the
parallel algorithms is illustrated on solving the cyclic $n$-roots problems, in
particular for $n = 8, 9$, and~12.
"
1162,Freeness and invariants of rational plane curves,"  Given a parameterization $\phi$ of a rational plane curve C, we study some
invariants of C via $\phi$. We first focus on the characterization of rational
cuspidal curves, in particular we establish a relation between the discriminant
of the pull-back of a line via $\phi$, the dual curve of C and its singular
points. Then, by analyzing the pull-backs of the global differential forms via
$\phi$, we prove that the (nearly) freeness of a rational curve can be tested
by inspecting the Hilbert function of the kernel of a canonical map. As a by
product, we also show that the global Tjurina number of a rational curve can be
computed directly from one of its parameterization, without relying on the
computation of an equation of C.
"
1163,"Summer Research Report: Towards Incremental Lazard Cylindrical Algebraic
  Decomposition","  Cylindrical Algebraic Decomposition (CAD) is an important tool within
computational real algebraic geometry, capable of solving many problems to do
with polynomial systems over the reals, but known to have worst-case
computational complexity doubly exponential in the number of variables. It has
long been studied by the Symbolic Computation community and is implemented in a
variety of computer algebra systems, however, it has also found recent interest
in the Satisfiability Checking community for use with SMT-solvers. The SCSC
Project seeks to build bridges between these communities.
  The present report describes progress made during a Research Internship in
Summer 2017 funded by the EU H2020 SCSC CSA. We describe a proof of concept
implementation of an Incremental CAD algorithm in Maple, where CADs are built
and refined incrementally by polynomial constraint, in contrast to the usual
approach of a single computation from a single input. This advance would make
CAD of use to SMT-solvers who search for solutions by constantly reformulating
logical formula and querying solvers like CAD for whether a logical solution is
admissible. We describe experiments for the proof of concept, which clearly
display the computational advantages when compared to iterated re-computation.
In addition, the project implemented this work under the recently verified
Lazard projection scheme (with corresponding Lazard evaluation). That is the
minimal complete CAD method in theory, and this is the first documented
implementation.
"
1164,"Positive Solutions of Systems of Signed Parametric Polynomial
  Inequalities","  We consider systems of strict multivariate polynomial inequalities over the
reals. All polynomial coefficients are parameters ranging over the reals, where
for each coefficient we prescribe its sign. We are interested in the existence
of positive real solutions of our system for all choices of coefficients
subject to our sign conditions. We give a decision procedure for the existence
of such solutions. In the positive case our procedure yields a parametric
positive solution as a rational function in the coefficients. Our framework
allows to reformulate heuristic subtropical approaches for non-parametric
systems of polynomial inequalities that have been recently used in qualitative
biological network analysis and, independently, in satisfiability modulo theory
solving. We apply our results to characterize the incompleteness of those
methods.
"
1165,Quantifier Elimination for Reasoning in Economics,"  We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. QE dates back to Tarski's work in the 1940s with
software to perform it dating to the 1970s. There is a great body of work
considering its application in science and engineering but we show here how it
can also find application in the social sciences. We explain how many suggested
theorems in economics could either be proven, or even have their hypotheses
shown to be inconsistent, automatically; and describe the application of this
in both economics education and research. We describe a bank of QE examples
gathered from economics literature and note the structure of these are, on
average, quite different to those occurring in the computer algebra literature.
This leads us to suggest a new incremental QE approach based on result
memorization of commonly occurring generic QE results.
"
1166,Using Machine Learning to Improve Cylindrical Algebraic Decomposition,"  Cylindrical Algebraic Decomposition (CAD) is a key tool in computational
algebraic geometry, best known as a procedure to enable Quantifier Elimination
over real-closed fields. However, it has a worst case complexity doubly
exponential in the size of the input, which is often encountered in practice.
It has been observed that for many problems a change in algorithm settings or
problem formulation can cause huge differences in runtime costs, changing
problem instances from intractable to easy. A number of heuristics have been
developed to help with such choices, but the complicated nature of the
geometric relationships involved means these are imperfect and can sometimes
make poor choices. We investigate the use of machine learning (specifically
support vector machines) to make such choices instead.
  Machine learning is the process of fitting a computer model to a complex
function based on properties learned from measured data. In this paper we apply
it in two case studies: the first to select between heuristics for choosing a
CAD variable ordering; the second to identify when a CAD problem instance would
benefit from Groebner Basis preconditioning. These appear to be the first such
applications of machine learning to Symbolic Computation. We demonstrate in
both cases that the machine learned choice outperforms human developed
heuristics.
"
1167,Diagonal asymptotics for symmetric rational functions via ACSV,"  We consider asymptotics of power series coefficients of rational functions of
the form $1/Q$ where $Q$ is a symmetric multilinear polynomial. We review a
number of such cases from the literature, chiefly concerned either with
positivity of coefficients or diagonal asymptotics. We then analyze coefficient
asymptotics using ACSV (Analytic Combinatorics in Several Variables) methods.
While ACSV sometimes requires considerable overhead and geometric computation,
in the case of symmetric multilinear rational functions there are some
reductions that streamline the analysis. Our results include diagonal
asymptotics across entire classes of functions, for example the general
3-variable case and the Gillis-Reznick-Zeilberger (GRZ) case, where the
denominator in terms of elementary symmetric functions is $1 - e_1 + c e_d$ in
any number $d$ of variables. The ACSV analysis also explains a discontinuous
drop in exponential growth rate for the GRZ class at the parameter value $c =
(d-1)^{d-1}$, previously observed for $d=4$ only by separately computing
diagonal recurrences for critical and noncritical values of $c$.
"
1168,"Gr\""obner Bases of Modules and Faug\`ere's $F_4$ Algorithm in
  Isabelle/HOL","  We present an elegant, generic and extensive formalization of Gr\""obner bases
in Isabelle/HOL. The formalization covers all of the essentials of the theory
(polynomial reduction, S-polynomials, Buchberger's algorithm, Buchberger's
criteria for avoiding useless pairs), but also includes more advanced features
like reduced Gr\""obner bases. Particular highlights are the first-time
formalization of Faug\`ere's matrix-based $F_4$ algorithm and the fact that the
entire theory is formulated for modules and submodules rather than rings and
ideals. All formalized algorithms can be translated into executable code
operating on concrete data structures, enabling the certified computation of
(reduced) Gr\""obner bases and syzygy modules.
"
1169,RealCertify: a Maple package for certifying non-negativity,"  Let $\mathbb{Q}$ (resp. $\mathbb{R}$) be the field of rational (resp. real)
numbers and $X = (X_1, \ldots, X_n)$ be variables. Deciding the non-negativity
of polynomials in $\mathbb{Q}[X]$ over $\mathbb{R}^n$ or over semi-algebraic
domains defined by polynomial constraints in $\mathbb{Q}[X]$ is a classical
algorithmic problem for symbolic computation.
  The Maple package \textsc{RealCertify} tackles this decision problem by
computing sum of squares certificates of non-negativity for inputs where such
certificates hold over the rational numbers. It can be applied to numerous
problems coming from engineering sciences, program verification and
cyber-physical systems. It is based on hybrid symbolic-numeric algorithms based
on semi-definite programming.
"
1170,"Implementing a Method for Stochastization of One-Step Processes in a
  Computer Algebra System","  When modeling such phenomena as population dynamics, controllable ows, etc.,
a problem arises of adapting the existing models to a phenomenon under study.
For this purpose, we propose to derive new models from the rst principles by
stochastization of one-step processes. Research can be represented as an
iterative process that consists in obtaining a model and its further re nement.
The number of such iterations can be extremely large. This work is aimed at
software implementation (by means of computer algebra) of a method for
stochastization of one-step processes. As a basis of the software
implementation, we use the SymPy computer algebra system. Based on a developed
algorithm, we derive stochastic di erential equations and their interaction
schemes. The operation of the program is demonstrated on the Verhulst and
Lotka-Volterra models.
"
1171,Computing an LLL-reduced basis of the orthogonal lattice,"  As a typical application, the Lenstra-Lenstra-Lovasz lattice basis reduction
algorithm (LLL) is used to compute a reduced basis of the orthogonal lattice
for a given integer matrix, via reducing a special kind of lattice bases. With
such bases in input, we propose a new technique for bounding from above the
number of iterations required by the LLL algorithm. The main technical
ingredient is a variant of the classical LLL potential, which could prove
useful to understand the behavior of LLL for other families of input bases.
"
1172,"Generalized Hermite Reduction, Creative Telescoping and Definite
  Integration of D-Finite Functions","  Hermite reduction is a classical algorithmic tool in symbolic integration. It
is used to decompose a given rational function as a sum of a function with
simple poles and the derivative of another rational function. We extend Hermite
reduction to arbitrary linear differential operators instead of the pure
derivative, and develop efficient algorithms for this reduction. We then apply
the generalized Hermite reduction to the computation of linear operators
satisfied by single definite integrals of D-finite functions of several
continuous or discrete parameters. The resulting algorithm is a generalization
of reduction-based methods for creative telescoping.
"
1173,Computing basepoints of linear series in the plane,"  We present an algorithm for detecting basepoints of linear series of curves
in the plane. Moreover, we give an algorithm for constructing a linear series
of curves in the plane for given basepoints. The underlying method of these
algorithms is the classical procedure of blowing up points in the plane. We
motivate the algorithmic version of this procedure with several applications.
"
1174,"Towards Mixed Gr{\""o}bner Basis Algorithms: the Multihomogeneous and
  Sparse Case","  One of the biggest open problems in computational algebra is the design of
efficient algorithms for Gr{\""o}bner basis computations that take into account
the sparsity of the input polynomials. We can perform such computations in the
case of unmixed polynomial systems, that is systems with polynomials having the
same support, using the approach of Faug{\`e}re, Spaenlehauer, and Svartz
[ISSAC'14]. We present two algorithms for sparse Gr{\""o}bner bases computations
for mixed systems. The first one computes with mixed sparse systems and
exploits the supports of the polynomials. Under regularity assumptions, it
performs no reductions to zero. For mixed, square, and 0-dimensional
multihomogeneous polynomial systems, we present a dedicated, and potentially
more efficient, algorithm that exploits different algebraic properties that
performs no reduction to zero. We give an explicit bound for the maximal degree
appearing in the computations.
"
1175,Computing curves on real rational surfaces,"  We present an algorithm for computing curves and families of curves of
prescribed degree and geometric genus on real rational surfaces.
"
1176,On the Annihilator Ideal of an Inverse Form. A Simplification,"  We simplify an earlier paper of the same title by not using syzygy
polynomials and by not using a trichotomy of inverse forms. Let $\K$ be a field
and $\M=\K[x^{-1},z^{-1}]$ denote Macaulay's $\K[x,z]$ module of inverse
polynomials; here $z$ and $z^{-1}$ are homogenising variables. An inverse form
$F\in\M$ has a homogeneous annihilator ideal, $\I_F$\,. In an earlier paper we
inductively constructed an ordered pair ($f_1$\,,\,$f_2$) of forms in $\K[x,z]$
which generate $\I_F$. We used syzygy polynomials to show that the intermediate
forms give a minimal grlex Groebner basis, which can be efficiently reduced.
  We give a significantly shorter proof that the intermediate forms are a
minimal grlex Groebner basis for $\I_F$\,. We also simplify our proof that
either $ F$ is already reduced or a monomial of $f_1$ can be reduced by
$f_2$\,. The algorithm that computes $f_1\,,f_2$ yields a variant of the
Berlekamp-Massey algorithm which does not use the last 'length change' approach
of Massey.
  These new proofs avoid the three separate cases, 'triples' and the technical
factorisation of intermediate 'essential' forms. We also show that $f_1,f_2$ is
a maximal $\R$ regular sequence for $\I_F$\,, so that $\I_F$ is a complete
intersection.
"
1177,"Bilinear systems with two supports: Koszul resultant matrices,
  eigenvalues, and eigenvectors","  A fundamental problem in computational algebraic geometry is the computation
of the resultant. A central question is when and how to compute it as the
determinant of a matrix. whose elements are the coefficients of the input
polynomials up-to sign. This problem is well understood for unmixed
multihomogeneous systems, that is for systems consisting of multihomogeneous
polynomials with the * 1 same support. However, little is known for mixed
systems, that is for systems consisting of polynomials with different supports.
We consider the computation of the multihomogeneous resultant of bilinear
systems involving two different supports. We present a constructive approach
that expresses the resultant as the exact determinant of a Koszul resultant
matrix, that is a matrix constructed from maps in the Koszul complex. We
exploit the resultant matrix to propose an algorithm to solve such systems. In
the process we extend the classical eigenvalues and eigenvectors criterion to a
more general setting. Our extension of the eigenvalues criterion applies to a
general class of matrices, including the Sylvester-type and the Koszul-type
ones.
"
1178,Enumeration of Complex Golay Pairs via Programmatic SAT,"  We provide a complete enumeration of all complex Golay pairs of length up to
25, verifying that complex Golay pairs do not exist in lengths 23 and 25 but do
exist in length 24. This independently verifies work done by F. Fiedler in 2013
that confirms the 2002 conjecture of Craigen, Holzmann, and Kharaghani that
complex Golay pairs of length 23 don't exist. Our enumeration method relies on
the recently proposed SAT+CAS paradigm of combining computer algebra systems
with SAT solvers to take advantage of the advances made in the fields of
symbolic computation and satisfiability checking. The enumeration proceeds in
two stages: First, we use a fine-tuned computer program and functionality from
computer algebra systems to construct a list containing all sequences which
could appear as the first sequence in a complex Golay pair (up to equivalence).
Second, we use a programmatic SAT solver to construct all sequences (if any)
that pair off with the sequences constructed in the first stage to form a
complex Golay pair.
"
1179,On Affine Tropical F5 Algorithms,"  Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr{\""o}bner bases taking into account the valuation
of $K$.Because of the use of the valuation, the theory of tropical Gr{\""o}bner
bases has proved to provide settings for computations over polynomial rings
over a $p$-adic field that are more stable than that of classical Gr{\""o}bner
bases.Beforehand, these strategies were only available for homogeneous
polynomials. In this article, we extend the F5 strategy to a new definition of
tropical Gr{\""o}bner bases in an affine setting.We provide numerical examples
to illustrate time-complexity and $p$-adic stability of this tropical F5
algorithm.We also illustrate its merits as a first step before an FGLM
algorithm to compute (classical) lex bases over $p$-adics.
"
1180,"A Simple Re-Derivation of Onsager's Solution of the 2D Ising Model using
  Experimental Mathematics","  In this case study, we illustrate the great potential of experimental
mathematics and symbolic computation, by rederiving, ab initio, Onsager's
celebrated solution of the twodimensional Ising model in zero magnetic field.
Onsager's derivation is extremely complicated and ad hoc, as are all the
subsequent proofs. Unlike Onsager's, our derivation is not rigorous, yet it is
absolutely certain (even if Onsager did not do it before), and should have been
acceptable to physicists who do not share mathematicians' fanatical (and often
misplaced) insistence on rigor.
"
1181,Towards Incremental Cylindrical Algebraic Decomposition in Maple,"  Cylindrical Algebraic Decomposition (CAD) is an important tool within
computational real algebraic geometry, capable of solving many problems for
polynomial systems over the reals. It has long been studied by the Symbolic
Computation community and has found recent interest in the Satisfiability
Checking community. The present report describes a proof of concept
implementation of an Incremental CAD algorithm in Maple, where CADs are built
and then refined as additional polynomial constraints are added. The aim is to
make CAD suitable for use as a theory solver for SMT tools who search for
solutions by continually reformulating logical formula and querying whether a
logical solution is admissible. We describe experiments for the proof of
concept, which clearly display the computational advantages compared to
iterated re-computation. In addition, the project implemented this work under
the recently verified Lazard projection scheme (with corresponding Lazard
valuation).
"
1182,Tritangents and Their Space Sextics,"  Two classical results in algebraic geometry are that the branch curve of a
del Pezzo surface of degree 1 can be embedded as a space sextic curve and that
every space sextic curve has exactly 120 tritangents corresponding to its odd
theta characteristics. In this paper we revisit both results from the
computational perspective. Specifically, we give an algorithm to construct
space sextic curves that arise from blowing up projective plane at eight points
and provide algorithms to compute the 120 tritangents and their Steiner system
of any space sextic. Furthermore, we develop efficient inverses to the
aforementioned methods. We present an algorithm to either reconstruct the
original eight points in the projective plane from a space sextic or certify
that this is not possible. Moreover, we extend a construction of Lehavi which
recovers a space sextic from its tritangents and Steiner system. All algorithms
in this paper have been implemented in magma.
"
1183,Monodromy Solver: Sequential and Parallel,"  We describe, study, and experiment with an algorithm for finding all
solutions of systems of polynomial equations using homotopy continuation and
monodromy. This algorithm follows a framework developed in previous work and
can operate in the presence of a large number of failures of the homotopy
continuation subroutine. We give special attention to parallelization and
probabilistic analysis of a model adapted to parallelization and failures.
Apart from theoretical results, we developed a simulator that allows us to run
a large number of experiments without recomputing the outcomes of the
continuation subroutine.
"
1184,"In-depth comparison of the Berlekamp--Massey--Sakata and the Scalar-FGLM
  algorithms: the adaptive variants","  The Berlekamp--Massey--Sakata algorithm and the Scalar-FGLM algorithm both
compute the ideal of relations of a multidimensional linear recurrent
sequence.Whenever quering a single sequence element is prohibitive, the
bottleneck of these algorithms becomes the computation of all the needed
sequence terms. As such, having adaptive variants of these algorithms, reducing
the number of sequence queries, becomes mandatory.A native adaptive variant of
the Scalar-FGLM algorithm was presented by its authors, the so-called Adaptive
Scalar-FGLM algorithm.In this paper, our first contribution is to make the
Berlekamp--Massey--Sakata algorithm more efficient by making it adaptive to
avoid some useless relation test-ings. This variant allows us to divide by four
in dimension 2 and by seven in dimension 3 the number of basic operations
performed on some sequence family.Then, we compare the two adaptive algorithms.
We show that their behaviors differ in a way that it is not possible to tweak
one of the algorithms in order to mimic exactly the behavior of the other. We
detail precisely the differences and the similarities of both algorithms and
conclude that in general the Adaptive Scalar-FGLM algorithm needs fewer queries
and performs fewer basic operations than the Adaptive Berlekamp--Massey--Sakata
algorithm.We also show that these variants are always more efficient than the
original algorithms.
"
1185,"Efficient Differentiable Programming in a Functional Array-Processing
  Language","  We present a system for the automatic differentiation of a higher-order
functional array-processing language. The core functional language underlying
this system simultaneously supports both source-to-source automatic
differentiation and global optimizations such as loop transformations. Thanks
to this feature, we demonstrate how for some real-world machine learning and
computer vision benchmarks, the system outperforms the state-of-the-art
automatic differentiation tools.
"
1186,A fast algorithm for solving linearly recurrent sequences,"  We present an algorithm which computes the $D^{th}$ term of a sequence
satisfying a linear recurrence relation of order $d$ over a field $K$ in $O(
\mathsf{M}(\bar d)\log(D) + \mathsf{M}(d)\log(d))$ operations in $K$, where
$\bar d \leq d$ is the degree of the squarefree part of the annihilating
polynomial of the recurrence and $\mathsf{M}$ is the cost of polynomial
multiplication in $K$. This is a refinement of the previously optimal result of
$O( \mathsf{M}(d)\log(D) )$ operations, due to Fiduccia.
"
1187,"Counting points on genus-3 hyperelliptic curves with explicit real
  multiplication","  We propose a Las Vegas probabilistic algorithm to compute the zeta function
of a genus-3 hyperelliptic curve defined over a finite field $\mathbb F_q$,
with explicit real multiplication by an order $\mathbb Z[\eta]$ in a totally
real cubic field. Our main result states that this algorithm requires an
expected number of $\widetilde O((\log q)^6)$ bit-operations, where the
constant in the $\widetilde O()$ depends on the ring $\mathbb Z[\eta]$ and on
the degrees of polynomials representing the endomorphism $\eta$. As a
proof-of-concept, we compute the zeta function of a curve defined over a 64-bit
prime field, with explicit real multiplication by $\mathbb Z[2\cos(2\pi/7)]$.
"
1188,"Fast Coefficient Computation for Algebraic Power Series in Positive
  Characteristic","  We revisit Christol's theorem on algebraic power series in positive
characteristic and propose yet another proof for it. This new proof combines
several ingredients and advantages of existing proofs, which make it very
well-suited for algorithmic purposes. We apply the construction used in the new
proof to the design of a new efficient algorithm for computing the $N$th
coefficient of a given algebraic power series over a perfect field of
characteristic~$p$. It has several nice features: it is more general, more
natural and more efficient than previous algorithms. Not only the arithmetic
complexity of the new algorithm is linear in $\log N$ and quasi-linear in~$p$,
but its dependency with respect to the degree of the input is much smaller than
in the previously best algorithm. {Moreover, when the ground field is finite,
the new approach yields an even faster algorithm, whose bit complexity is
linear in $\log N$ and quasi-linear in~$\sqrt{p}$}.
"
1189,Segre Class Computation and Practical Applications,"  Let $X \subset Y$ be closed (possibly singular) subschemes of a smooth
projective toric variety $T$. We show how to compute the Segre class $s(X,Y)$
as a class in the Chow group of $T$. Building on this, we give effective
methods to compute intersection products in projective varieties, to determine
algebraic multiplicity without working in local rings, and to test pairwise
containment of subvarieties of $T$. Our methods may be implemented without
using Groebner bases; in particular any algorithm to compute the number of
solutions of a zero-dimensional polynomial system may be used.
"
1190,Real root finding for equivariant semi-algebraic systems,"  Let $R$ be a real closed field. We consider basic semi-algebraic sets defined
by $n$-variate equations/inequalities of $s$ symmetric polynomials and an
equivariant family of polynomials, all of them of degree bounded by $2d < n$.
Such a semi-algebraic set is invariant by the action of the symmetric group. We
show that such a set is either empty or it contains a point with at most $2d-1$
distinct coordinates. Combining this geometric result with efficient algorithms
for real root finding (based on the critical point method), one can decide the
emptiness of basic semi-algebraic sets defined by $s$ polynomials of degree $d$
in time $(sn)^{O(d)}$. This improves the state-of-the-art which is exponential
in $n$. When the variables $x_1, \ldots, x_n$ are quantified and the
coefficients of the input system depend on parameters $y_1, \ldots, y_t$, one
also demonstrates that the corresponding one-block quantifier elimination
problem can be solved in time $(sn)^{O(dt)}$.
"
1191,Implementation of a Near-Optimal Complex Root Clustering Algorithm,"  We describe Ccluster, a software for computing natural $\epsilon$-clusters of
complex roots in a given box of the complex plane. This algorithm from Becker
et al.~(2016) is near-optimal when applied to the benchmark problem of
isolating all complex roots of an integer polynomial. It is one of the first
implementations of a near-optimal algorithm for complex roots. We describe some
low level techniques for speeding up the algorithm. Its performance is compared
with the well-known MPSolve library and Maple.
"
1192,Machine Learning for Mathematical Software,"  While there has been some discussion on how Symbolic Computation could be
used for AI there is little literature on applications in the other direction.
However, recent results for quantifier elimination suggest that, given enough
example problems, there is scope for machine learning tools like Support Vector
Machines to improve the performance of Computer Algebra Systems. We survey the
authors own work and similar applications for other mathematical software.
  It may seem that the inherently probabilistic nature of machine learning
tools would invalidate the exact results prized by mathematical software.
However, algorithms and implementations often come with a range of choices
which have no effect on the mathematical correctness of the end result but a
great effect on the resources required to find it, and thus here, machine
learning can have a significant impact.
"
1193,TheoryGuru: A Mathematica Package to apply Quantifier Elimination,"  We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. There is a great body of work considering QE
applications in science and engineering but we demonstrate here that it also
has use in the social sciences. We explain how many suggested theorems in
economics could either be proven, or even have their hypotheses shown to be
inconsistent, automatically via QE.
  However, economists who this technology could benefit are usually unfamiliar
with QE, and the use of mathematical software generally. This motivated the
development of a Mathematica Package TheoryGuru, whose purpose is to lower the
costs of applying QE to economics. We describe the package's functionality and
give examples of its use.
"
1194,Proof-of-work certificates that can be efficiently computed in the cloud,"  In an emerging computing paradigm, computational capabilities, from
processing power to storage capacities, are offered to users over communication
networks as a cloud-based service. There, demanding computations are outsourced
in order to limit infrastructure costs. The idea of verifiable computing is to
associate a data structure, a proof-of-work certificate, to the result of the
outsourced computation. This allows a verification algorithm to prove the
validity of the result, faster than by recomputing it. We talk about a Prover
(the server performing the computations) and a Verifier. Goldwasser, Kalai and
Rothblum gave in 2008 a generic method to verify any parallelizable
computation, in almost linear time in the size of the, potentially structured,
inputs and the result. However, the extra cost of the computations for the
Prover (and therefore the extra cost to the customer), although only almost a
constant factor of the overall work, is nonetheless prohibitive in practice.
Differently, we will here present problem-specific procedures in computer
algebra, e.g. for exact linear algebra computations, that are Prover-optimal,
that is that have much less financial overhead.
"
1195,"Non-linear Real Arithmetic Benchmarks derived from Automated Reasoning
  in Economics","  We consider problems originating in economics that may be solved
automatically using mathematical software. We present and make freely available
a new benchmark set of such problems. The problems have been shown to fall
within the framework of non-linear real arithmetic, and so are in theory
soluble via Quantifier Elimination (QE) technology as usually implemented in
computer algebra systems. Further, they all can be phrased in prenex normal
form with only existential quantifiers and so are also admissible to those
Satisfiability Module Theory (SMT) solvers that support the QF_NRA. There is a
great body of work considering QE and SMT application in science and
engineering, but we demonstrate here that there is potential for this
technology also in the social sciences.
"
1196,"A Strongly Consistent Finite Difference Scheme for Steady Stokes Flow
  and its Modified Equations","  We construct and analyze a strongly consistent second-order finite difference
scheme for the steady two-dimensional Stokes flow. The pressure Poisson
equation is explicitly incorporated into the scheme. Our approach suggested by
the first two authors is based on a combination of the finite volume method,
difference elimination, and numerical integration. We make use of the
techniques of the differential and difference Janet/Groebner bases. In order to
prove strong consistency of the generated scheme we correlate the differential
ideal generated by the polynomials in the Stokes equations with the difference
ideal generated by the polynomials in the constructed difference scheme.
Additionally, we compute the modified differential system of the obtained
scheme and analyze the scheme's accuracy and strong consistency by considering
this system. An evaluation of our scheme against the established
marker-and-cell method is carried out.
"
1197,"Fast Hermite interpolation and evaluation over finite fields of
  characteristic two","  This paper presents new fast algorithms for Hermite interpolation and
evaluation over finite fields of characteristic two. The algorithms reduce the
Hermite problems to instances of the standard multipoint interpolation and
evaluation problems, which are then solved by existing fast algorithms. The
reductions are simple to implement and free of multiplications, allowing low
overall multiplicative complexities to be obtained. The algorithms are suitable
for use in encoding and decoding algorithms for multiplicity codes.
"
1198,"Verification Protocols with Sub-Linear Communication for Polynomial
  Matrix Operations","  We design and analyze new protocols to verify the correctness of various
computations on matrices over the ring F[x] of univariate polynomials over a
field F. For the sake of efficiency, and because many of the properties we
verify are specific to matrices over a principal ideal domain, we cannot simply
rely on previously-developed linear algebra protocols for matrices over a
field. Our protocols are interactive, often randomized, and feature a constant
number of rounds of communication between the Prover and Verifier. We seek to
minimize the communication cost so that the amount of data sent during the
protocol is significantly smaller than the size of the result being verified,
which can be useful when combining protocols or in some multi-party settings.
The main tools we use are reductions to existing linear algebra verification
protocols and a new protocol to verify that a given vector is in the F[x]-row
space of a given matrix.
"
1199,A Purely Functional Computer Algebra System Embedded in Haskell,"  We demonstrate how methods in Functional Programming can be used to implement
a computer algebra system. As a proof-of-concept, we present the
computational-algebra package. It is a computer algebra system implemented as
an embedded domain-specific language in Haskell, a purely functional
programming language. Utilising methods in functional programming and prominent
features of Haskell, this library achieves safety, composability, and
correctness at the same time. To demonstrate the advantages of our approach, we
have implemented advanced Gr\""{o}bner basis algorithms, such as Faug\`{e}re's
$F_4$ and $F_5$, in a composable way.
"
1200,"Multistationarity and Bistability for Fewnomial Chemical Reaction
  Networks","  Bistability and multistationarity are properties of reaction networks linked
to switch-like responses and connected to cell memory and cell decision making.
Determining whether and when a network exhibits bistability is a hard and open
mathematical problem. One successful strategy consists of analyzing small
networks and deducing that some of the properties are preserved upon passage to
the full network. Motivated by this we study chemical reaction networks with
few chemical complexes. Under mass-action kinetics the steady states of these
networks are described by fewnomial systems, that is polynomial systems having
few distinct monomials. Such systems of polynomials are often studied in real
algebraic geometry by the use of Gale dual systems. Using this Gale duality we
give precise conditions in terms of the reaction rate constants for the number
and stability of the steady states of families of reaction networks with one
non-flow reaction.
"
1201,Orbits of monomials and factorization into products of linear forms,"  This paper is devoted to the factorization of multivariate polynomials into
products of linear forms, a problem which has applications to differential
algebra, to the resolution of systems of polynomial equations and to Waring
decomposition (i.e., decomposition in sums of d-th powers of linear forms; this
problem is also known as symmetric tensor decomposition). We provide three
black box algorithms for this problem. Our main contribution is an algorithm
motivated by the application to Waring decomposition. This algorithm reduces
the corresponding factorization problem to simultaenous matrix diagonalization,
a standard task in linear algebra. The algorithm relies on ideas from invariant
theory, and more specifically on Lie algebras. Our second algorithm
reconstructs a factorization from several bi-variate projections. Our third
algorithm reconstructs it from the determination of the zero set of the input
polynomial, which is a union of hyperplanes.
"
1202,Fast transforms over finite fields of characteristic two,"  An additive fast Fourier transform over a finite field of characteristic two
efficiently evaluates polynomials at every element of an $\mathbb{F}_2$-linear
subspace of the field. We view these transforms as performing a change of basis
from the monomial basis to the associated Lagrange basis, and consider the
problem of performing the various conversions between these two bases, the
associated Newton basis, and the '' novel '' basis of Lin, Chung and Han (FOCS
2014). Existing algorithms are divided between two families, those designed for
arbitrary subspaces and more efficient algorithms designed for specially
constructed subspaces of fields with degree equal to a power of two. We
generalise techniques from both families to provide new conversion algorithms
that may be applied to arbitrary subspaces, but which benefit equally from the
specially constructed subspaces. We then construct subspaces of fields with
smooth degree for which our algorithms provide better performance than existing
algorithms.
"
1203,"Uma an\'alise comparativa de ferramentas de an\'alise est\'atica para
  dete\c{c}\~ao de erros de mem\'oria","  --- Portuguese version
  As falhas de software est\~ao com frequ\^encia associadas a acidentes com
graves consequ\^encias econ\'omicas e/ou humanas, pelo que se torna imperioso
investir na valida\c{c}\~ao do software, nomeadamente daquele que \'e
cr\'itico. Este artigo endere\c{c}a a tem\'atica da qualidade do software
atrav\'es de uma an\'alise comparativa da usabilidade e efic\'acia de quatro
ferramentas de an\'alise est\'atica de programas em C/C++. Este estudo permitiu
compreender o grande potencial e o elevado impacto que as ferramentas de
an\'alise est\'atica podem ter na valida\c{c}\~ao e verifica\c{c}\~ao de
software. Como resultado complementar, foram identificados novos erros em
programas de c\'odigo aberto e com elevada popularidade, que foram reportados.
  --- English version
  Software bugs are frequently associated with accidents with serious
economical and/or human consequences, being thus imperative the investment in
the validation of software, namely of the critical one. This article addresses
the topic of software quality by making a comparative analysis of the usability
and efficiency of four static analysis tools for C/C++ programs. This study
allow to understand the big potential and high impact that these tools may have
in the validation and verification of software. As a complementary result, we
identified new errors in very popular open source projects, which have been
reported.
"
1204,What Can (and Can't) we Do with Sparse Polynomials?,"  Simply put, a sparse polynomial is one whose zero coefficients are not
explicitly stored. Such objects are ubiquitous in exact computing, and so
naturally we would like to have efficient algorithms to handle them. However,
with this compact storage comes new algorithmic challenges, as fast algorithms
for dense polynomials may no longer be efficient. In this tutorial we examine
the state of the art for sparse polynomial algorithms in three areas:
arithmetic, interpolation, and factorization. The aim is to highlight recent
progress both in theory and in practice, as well as opportunities for future
work.
"
1205,"Toward an Optimal Quantum Algorithm for Polynomial Factorization over
  Finite Fields","  We present a randomized quantum algorithm for polynomial factorization over
finite fields. For polynomials of degree $n$ over a finite field $\F_q$, the
average-case complexity of our algorithm is an expected $O(n^{1 + o(1)} \log^{2
+ o(1)}q)$ bit operations. Only for a negligible subset of polynomials of
degree $n$ our algorithm has a higher complexity of $O(n^{4 / 3 + o(1)} \log^{2
+ o(1)}q)$ bit operations. This breaks the classical $3/2$-exponent barrier for
polynomial factorization over finite fields \cite{guo2016alg}.
"
1206,Tropical recurrent sequences,"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
"
1207,Minimal solutions of the rational interpolation problem,"  We explore connections between the approach of solving the rational
interpolation problem via resolutions of ideals and syzygies with the standard
method provided by the Extended Euclidean Algorithm. As a consequence, we
obtain explicit descriptions for solutions of ""minimal"" degrees in terms of the
degrees of elements appearing in the EEA. This allows us to describe the
minimal degree in a $\mu$-basis of a polynomial planar parametrization in terms
of a ""critical"" degree arising in the EEA.
"
1208,"An Effective Framework for Constructing Exponent Lattice Basis of
  Nonzero Algebraic Numbers","  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
"
1209,Computing Unit Groups of Curves,"  The group of units modulo constants of an affine variety over an
algebraically closed field is free abelian of finite rank. Computing this group
is difficult but of fundamental importance in tropical geometry, where it is
desirable to realize intrinsic tropicalizations. We present practical
algorithms for computing unit groups of smooth curves of low genus. Our
approach is rooted in divisor theory, based on interpolation in the case of
rational curves and on methods from algebraic number theory in the case of
elliptic curves.
"
1210,"Bringing Together Dynamic Geometry Software and the Graphics Processing
  Unit","  We equip dynamic geometry software (DGS) with a user-friendly method that
enables massively parallel calculations on the graphics processing unit (GPU).
This interplay of DGS and GPU opens up various applications in education and
mathematical research. The GPU-aided discovery of mathematical properties,
interactive visualizations of algebraic surfaces (raycasting), the mathematical
deformation of images and footage in real-time, and computationally demanding
numerical simulations of PDEs are examples from the long and versatile list of
new domains that our approach makes accessible within a DGS. We ease the
development of complex (mathematical) visualizations and provide a
rapid-prototyping scheme for general-purpose computations (GPGPU).
  The possibility to program both CPU and GPU with the use of only one
high-level (scripting) programming language is a crucial aspect of our concept.
We embed shader programming seamlessly within a high-level (scripting)
programming environment. The aforementioned requires the symbolic process of
the transcompilation of a high-level programming language into shader
programming language for GPU and, in this article, we address the challenge of
the automatic translation of a high-level programming language to a shader
language of the GPU. To maintain platform independence and the possibility to
use our technology on modern devices, we focus on a realization through WebGL.
"
1211,Aligator.jl - A Julia Package for Loop Invariant Generation,"  We describe the Aligator.jl software package for automatically generating all
polynomial invariants of the rich class of extended P-solvable loops with
nested conditionals. Aligator.jl is written in the programming language Julia
and is open-source. Aligator.jl transforms program loops into a system of
algebraic recurrences and implements techniques from symbolic computation to
solve recurrences, derive closed form solutions of loop variables and infer the
ideal of polynomial invariants by variable elimination based on Gr\""obner basis
computation.
"
1212,Randomized Polynomial-Time Root Counting in Prime Power Rings,"  Suppose $k,p\!\in\!\mathbb{N}$ with $p$ prime and $f\!\in\!\mathbb{Z}[x]$ is
a univariate polynomial with degree $d$ and all coefficients having absolute
value less than $p^k$. We give a Las Vegas randomized algorithm that computes
the number of roots of $f$ in $\mathbb{Z}/\!\left(p^k\right)$ within time
$d^3(k\log p)^{2+o(1)}$. (We in fact prove a more intricate complexity bound
that is slightly better.) The best previous general algorithm had
(deterministic) complexity exponential in $k$. We also present some
experimental data evincing the potential practicality of our algorithm.
"
1213,Detecting tropical defects of polynomial equations,"  We introduce the notion of tropical defects, certificates that a system of
polynomial equations is not a tropical basis, and provide two algorithms for
finding them in affine spaces of complementary dimension to the zero set. We
use these techniques to solve open problems regarding del Pezzo surfaces of
degree 3 and realizability of valuated gaussoids on 4 elements.
"
1214,"Probabilistic Condition Number Estimates for Real Polynomial Systems II:
  Structure and Smoothed Analysis","  We consider the sensitivity of real zeros of structured polynomial systems to
perturbations of their coefficients. In particular, we provide explicit
estimates for condition numbers of structured random real polynomial systems,
and extend these estimates to smoothed analysis setting.
"
1215,Computer algebra tools for Feynman integrals and related multi-sums,"  In perturbative calculations, e.g., in the setting of Quantum Chromodynamics
(QCD) one aims at the evaluation of Feynman integrals. Here one is often faced
with the problem to simplify multiple nested integrals or sums to expressions
in terms of indefinite nested integrals or sums. Furthermore, one seeks for
solutions of coupled systems of linear differential equations, that can be
represented in terms of indefinite nested sums (or integrals). In this article
we elaborate the main tools and the corresponding packages, that we have
developed and intensively used within the last 10 years in the course of our
QCD-calculations.
"
1216,Degree bound for toric envelope of a linear algebraic group,"  Algorithms working with linear algebraic groups often represent them via
defining polynomial equations. One can always choose defining equations for an
algebraic group to be of the degree at most the degree of the group as an
algebraic variety. However, the degree of a linear algebraic group $G \subset
\mathrm{GL}_n(C)$ can be arbitrarily large even for $n = 1$. One of the key
ingredients of Hrushovski's algorithm for computing the Galois group of a
linear differential equation was an idea to `approximate' every algebraic
subgroup of $\mathrm{GL}_n(C)$ by a `similar' group so that the degree of the
latter is bounded uniformly in $n$. Making this uniform bound computationally
feasible is crucial for making the algorithm practical.
  In this paper, we derive a single-exponential degree bound for such an
approximation (we call it toric envelope), which is qualitatively optimal. As
an application, we improve the quintuply exponential bound for the first step
of the Hrushovski's algorithm due to Feng to a single-exponential bound. For
the cases $n = 2, 3$ often arising in practice, we further refine our general
bound.
"
1217,Towards a symbolic summation theory for unspecified sequences,"  The article addresses the problem whether indefinite double sums involving a
generic sequence can be simplified in terms of indefinite single sums.
Depending on the structure of the double sum, the proposed summation machinery
may provide such a simplification without exceptions. If it fails, it may
suggest a more advanced simplification introducing in addition a single nested
sum where the summand has to satisfy a particular constraint. More precisely,
an explicitly given parameterized telescoping equation must hold. Restricting
to the case that the arising unspecified sequences are specialized to the class
of indefinite nested sums defined over hypergeometric, multi-basic or mixed
hypergeometric products, it can be shown that this constraint is not only
sufficient but also necessary.
"
1218,Machine-Assisted Proofs (ICM 2018 Panel),"  This submission to arXiv is the report of a panel session at the 2018
International Congress of Mathematicians (Rio de Janeiro, August). It is
intended that, while v1 is that report, this stays a living document containing
the panelists', and others', reflections on the topic.
"
1219,Bohemian Upper Hessenberg Matrices,"  We look at Bohemian matrices, specifically those with entries from $\{-1, 0,
{+1}\}$. More, we specialize the matrices to be upper Hessenberg, with
subdiagonal entries $\pm1$. Many properties remain after these specializations,
some of which surprised us. We find two recursive formulae for the
characteristic polynomials of upper Hessenberg matrices. Focusing on only those
matrices whose characteristic polynomials have maximal height allows us to
explicitly identify these polynomials and give a lower bound on their height.
This bound is exponential in the order of the matrix. We count stable matrices,
normal matrices, and neutral matrices, and tabulate the results of our
experiments. We prove a theorem about the only possible kinds of normal
matrices amongst a specific family of Bohemian upper Hessenberg matrices.
"
1220,Bohemian Upper Hessenberg Toeplitz Matrices,"  We look at Bohemian matrices, specifically those with entries from $\{-1, 0,
{+1}\}$. More, we specialize the matrices to be upper Hessenberg, with
subdiagonal entries $1$. Even more, we consider Toeplitz matrices of this kind.
Many properties remain after these specializations, some of which surprised us.
Focusing on only those matrices whose characteristic polynomials have maximal
height allows us to explicitly identify these polynomials and give a lower
bound on their height. This bound is exponential in the order of the matrix.
"
1221,A Pommaret Bases Approach to the Degree of a Polynomial Ideal,"  In this paper, we study first the relationship between Pommaret bases and
Hilbert series. Given a finite Pommaret basis, we derive new explicit formulas
for the Hilbert series and for the degree of the ideal generated by it which
exhibit more clearly the influence of each generator. Then we establish a new
dimension depending Bezout bound for the degree and use it to obtain a
dimension depending bound for the ideal membership problem.
"
1222,Computation of Pommaret Bases Using Syzygies,"  We investigate the application of syzygies for efficiently computing (finite)
Pommaret bases. For this purpose, we first describe a non-trivial variant of
Gerdt's algorithm to construct an involutive basis for the input ideal as well
as an involutive basis for the syzygy module of the output basis. Then we apply
this new algorithm in the context of Seiler's method to transform a given ideal
into quasi stable position to ensure the existence of a finite Pommaret basis.
This new approach allows us to avoid superfluous reductions in the iterative
computation of Janet bases required by this method. We conclude the paper by
proposing an involutive variant of the signature based algorithm of Gao et al.
to compute simultaneously a Grobner basis for a given ideal and for the syzygy
module of the input basis. All the presented algorithms have been implemented
in Maple and their performance is evaluated via a set of benchmark ideals.
"
1223,New bounds and efficient algorithm for sparse difference resultant,"  Let $\mathbb{P}=\{\mathbb{P}_0,\mathbb{P}_1,\dots,\mathbb{P}_n\}$ be a
generic Laurent transformally essential system and
$\mathbb{P}_{\mathbb{T}}=\{\mathbb{P}_0,\mathbb{P}_1,\dots,\mathbb{P}_m\}
(m\leq n)$ be its super essential system. We show that the sparse difference
resultant of a simplified system of $\mathbb{P}_{\mathbb{T}}$ by setting the
selected $n-m$ variables to one is the same to the one of $\mathbb{P}$.
Moreover, new order bounds of sparse difference resultant are obtained. Then we
propose an efficient algorithm to compute sparse difference resultant which is
the quotient of two determinants whose elements are the coefficients of the
polynomials in the strong essential system. We analyze complexity of the
algorithm. Experimental results show the efficiency of the algorithm.
"
1224,"A Novel Algebraic Geometry Compiling Framework for Adiabatic Quantum
  Computations","  Adiabatic Quantum Computing (AQC) is an attractive paradigm for solving hard
integer polynomial optimization problems. Available hardware restricts the
Hamiltonians to be of a structure that allows only pairwise interactions. This
requires that the original optimization problem to be first converted -- from
its polynomial form -- to a quadratic unconstrained binary optimization (QUBO)
problem, which we frame as a problem in algebraic geometry. Additionally, the
hardware graph where such a QUBO-Hamiltonian needs to be embedded -- assigning
variables of the problem to the qubits of the physical optimizer -- is not a
complete graph, but rather one with limited connectivity. This ""problem graph
to hardware graph"" embedding can also be framed as a problem of computing a
Groebner basis of a certain specially constructed polynomial ideal. We develop
a systematic computational approach to prepare a given polynomial optimization
problem for AQC in three steps. The first step reduces an input polynomial
optimization problem into a QUBO through the computation of the Groebner basis
of a toric ideal generated from the monomials of the input objective function.
The second step computes feasible embeddings. The third step computes the
spectral gap of the adiabatic Hamiltonian associated to a given embedding.
These steps are applicable well beyond the integer polynomial optimization
problem. Our paper provides the first general purpose computational procedure
that can be used directly as a $translator$ to solve polynomial integer
optimization. Alternatively, it can be used as a test-bed (with small size
problems) to help design efficient heuristic quantum compilers by studying
various choices of reductions and embeddings in a systematic and comprehensive
manner. An added benefit of our framework is in designing Ising architectures
through the study of $\mathcal Y-$minor universal graphs.
"
1225,Algebraic number fields and the LLL algorithm,"  In this paper we analyze the computational costs of various operations and
algorithms in algebraic number fields using exact arithmetic. Let $K$ be an
algebraic number field. In the first half of the paper, we calculate the
running time and the size of the output of many operations in $K$ in terms of
the size of the input and the parameters of $K$. We include some earlier
results about these, but we go further than them, e.g. we also analyze some
$\mathbb{R}$-specific operations in $K$ like less-than comparison. In the
second half of the paper, we analyze two algorithms: the Bareiss algorithm,
which is an integer-preserving version of the Gaussian elimination, and the LLL
algorithm, which is for lattice basis reduction. In both cases, we extend the
algorithm from $\mathbb{Z}^n$ to $K^n$, and give a polynomial upper bound on
the running time when the computations in $K$ are performed exactly (as opposed
to floating-point approximations).
"
1226,"Proceedings of the 15th International Workshop on the ACL2 Theorem
  Prover and Its Applications","  This volume contains the proceedings of the Fifteenth International Workshop
on the ACL2 Theorem Prover and Its Applications (ACL2-2018), a two-day workshop
held in Austin, Texas, USA, on November 5-6, 2018, immediately after FMCAD'18.
The proceedings of ACL2-2018 include eleven long papers and two extended
abstracts.
"
1227,"Reconstruction of surfaces with ordinary singularities from their
  silhouettes","  We present algorithms for reconstructing, up to unavoidable projective
automorphisms, surfaces with ordinary singularities in three dimensional space
starting from their silhouette, or ""apparent contour"" - namely the branching
locus of a projection on the plane - and the projection of their singular
locus.
"
1228,Computing Elimination Ideals and Discriminants of Likelihood Equations,"  We develop a probabilistic algorithm for computing elimination ideals of
likelihood equations, which is for larger models by far more efficient than
directly computing Groebner bases or the interpolation method proposed in the
first author's previous work. The efficiency is improved by a theoretical
result showing that the sum of data variables appears in most coefficients of
the generator polynomial of elimination ideal. Furthermore, applying the known
structures of Newton polytopes of discriminants, we can also efficiently deduce
discriminants of the elimination ideals. For instance, the discriminants of 3
by 3 matrix model and one Jukes-Cantor model in phylogenetics (with sizes over
30 GB and 8 GB text files, respectively) can be computed by our methods.
"
1229,Integration in terms of polylogarithm,"  This paper provides a Liouville principle for integration in terms of
dilogarithm and partial result for polylogarithm.
"
1230,Computation of gcd chain over the power of an irreducible polynomial,"  A notion of gcd chain has been introduced by the author at ISSAC 2017 for two
univariate monic polynomials with coefficients in a ring R = k[x_1, ..., x_n
]/(T) where T is a primary triangular set of dimension zero. A complete
algorithm to compute such a gcd chain remains challenging. This work treats
completely the case of a triangular set T = (T_1 (x)) in one variable, namely a
power of an irreducible polynomial. This seemingly ""easy"" case reveals the main
steps necessary for treating the general case, and it allows to isolate the
particular one step that does not directly extend and requires more care.
"
1231,"Counting points on hyperelliptic curves with explicit real
  multiplication in arbitrary genus","  We present a probabilistic Las Vegas algorithm for computing the local zeta
function of a genus-$g$ hyperelliptic curve defined over $\mathbb F_q$ with
explicit real multiplication (RM) by an order $\Z[\eta]$ in a degree-$g$
totally real number field.
  It is based on the approaches by Schoof and Pila in a more favorable case
where we can split the $\ell$-torsion into $g$ kernels of endomorphisms, as
introduced by Gaudry, Kohel, and Smith in genus 2. To deal with these kernels
in any genus, we adapt a technique that the author, Gaudry, and Spaenlehauer
introduced to model the $\ell$-torsion by structured polynomial systems.
Applying this technique to the kernels, the systems we obtain are much smaller
and so is the complexity of solving them.
  Our main result is that there exists a constant $c>0$ such that, for any
fixed $g$, this algorithm has expected time and space complexity $O((\log
q)^{c})$ as $q$ grows and the characteristic is large enough. We prove that
$c\le 9$ and we also conjecture that the result still holds for $c=7$.
"
1232,"On the complexity of class group computations for large degree number
  fields","  In this paper, we examine the general algorithm for class group computations,
when we do not have a small defining polynomial for the number field. Based on
a result of Biasse and Fieker, we simplify their algorithm, improve the
complexity analysis and identify the optimal parameters to reduce the runtime.
We make use of the classes $\mathcal D$ defined in [GJ16] for classifying the
fields according to the size of the extension degree and prove that they enable
to describe all the number fields.
"
1233,"Reducing the complexity for class group computations using small
  defining polynomials","  In this paper, we describe an algorithm that efficiently collect relations in
class groups of number fields defined by a small defining polynomial. This
conditional improvement consists in testing directly the smoothness of
principal ideals generated by small algebraic integers. This strategy leads to
an algorithm for computing the class group whose complexity is possibly as low
as $L_{|\Delta_{\mathbf K}|}\left(\frac{1}{3}\right)$.
"
1234,A Simple Recurrent Unit with Reduced Tensor Product Representations,"  idely used recurrent units, including Long-short Term Memory (LSTM) and the
Gated Recurrent Unit (GRU), perform well on natural language tasks, but their
ability to learn structured representations is still questionable. Exploiting
reduced Tensor Product Representations (TPRs) --- distributed representations
of symbolic structure in which vector-embedded symbols are bound to
vector-embedded structural positions --- we propose the TPRU, a simple
recurrent unit that, at each time step, explicitly executes structural-role
binding and unbinding operations to incorporate structural information into
learning. A gradient analysis of our proposed TPRU is conducted to support our
model design, and its performance on multiple datasets shows the effectiveness
of our design choices. Furthermore, observations on a linguistically grounded
study demonstrate the interpretability of our TPRU.
"
1235,A nearly optimal algorithm to decompose binary forms,"  Symmetric tensor decomposition is an important problem with applications in
several areas for example signal processing, statistics, data analysis and
computational neuroscience. It is equivalent to Waring's problem for
homogeneous polynomials, that is to write a homogeneous polynomial in n
variables of degree D as a sum of D-th powers of linear forms, using the
minimal number of summands. This minimal number is called the rank of the
polynomial/tensor. We focus on decomposing binary forms, a problem that
corresponds to the decomposition of symmetric tensors of dimension 2 and order
D. Under this formulation, the problem finds its roots in invariant theory
where the decompositions are known as canonical forms. In this context many
different algorithms were proposed. We introduce a superfast algorithm that
improves the previous approaches with results from structured linear algebra.
It achieves a softly linear arithmetic complexity bound. To the best of our
knowledge, the previously known algorithms have at least quadratic complexity
bounds. Our algorithm computes a symbolic decomposition in $O(M(D) log(D))$
arithmetic operations, where $M(D)$ is the complexity of multiplying two
polynomials of degree D. It is deterministic when the decomposition is unique.
When the decomposition is not unique, our algorithm is randomized. We present a
Monte Carlo version of it and we show how to modify it to a Las Vegas one,
within the same complexity. From the symbolic decomposition, we approximate the
terms of the decomposition with an error of $2^{--$\epsilon$}$ , in $O(D
log^2(D) (log^2(D) + log($\epsilon$)))$ arithmetic operations. We use results
from Kaltofen and Yagati (1989) to bound the size of the representation of the
coefficients involved in the decomposition and we bound the algebraic degree of
the problem by min(rank, D -- rank + 1). We show that this bound can be tight.
When the input polynomial has integer coefficients, our algorithm performs, up
to poly-logarithmic factors, $O\_{bit} (D{\ell} + D^4 + D^3 $\tau$)$ bit
operations, where $$\tau$$ is the maximum bitsize of the coefficients and
$2^{--{\ell}}$ is the relative error of the terms in the decomposition.
"
1236,"Putting F\""urer Algorithm into Practice with the BPAS Library","  Fast algorithms for integer and polynomial multiplication play an important
role in scientific computing as well as in other disciplines. In 1971,
Sch{\""o}nhage and Strassen designed an algorithm that improved the
multiplication time for two integers of at most $n$ bits to $\mathcal{O}(\log n
\log \log n)$. In 2007, Martin F\""urer presented a new algorithm that runs in
$O \left(n \log n\ \cdot 2^{O(\log^* n)} \right)$, where $\log^* n$ is the
iterated logarithm of $n$.
  We explain how we can put F\""urer's ideas into practice for multiplying
polynomials over a prime field $\mathbb{Z} / p \mathbb{Z}$, for which $p$ is a
Generalized Fermat prime of the form $p = r^k + 1$ where $k$ is a power of $2$
and $r$ is of machine word size. When $k$ is at least 8, we show that
multiplication inside such a prime field can be efficiently implemented via
Fast Fourier Transform (FFT). Taking advantage of Cooley-Tukey tensor formula
and the fact that $r$ is a $2k$-th primitive root of unity in $\mathbb{Z} / p
\mathbb{Z}$, we obtain an efficient implementation of FFT over $\mathbb{Z} / p
\mathbb{Z}$. This implementation outperforms comparable implementations either
using other encodings of $\mathbb{Z} / p \mathbb{Z}$ or other ways to perform
multiplication in $\mathbb{Z} / p \mathbb{Z}$.
"
1237,Complexity Estimates for Fourier-Motzkin Elimination,"  In this paper, we propose a new method for removing all the redundant
inequalities generated by Fourier-Motzkin elimination. This method is based on
an improved version of Balas' work and can also be used to remove all the
redundant inequalities in the input system. Moreover, our method only uses
arithmetic operations on matrices and avoids resorting to linear programming
techniques. Algebraic complexity estimates and experimental results show that
our method outperforms alternative approaches, in particular those based on
linear programming and simplex algorithm.
"
1238,"An Application of Rubi: Series Expansion of the Quark Mass
  Renormalization Group Equation","  We highlight how Rule-based Integration (Rubi) is an enhanced method of
symbolic integration which allows for the integration of many difficult
integrals not accomplished by other computer algebra systems. Using Rubi, many
integration techniques become tractable. Integrals are approached using
step-wise simplification, hence distilling an integral (if the solution is
unknown) into composite integrals which highlight yet undiscovered integration
rules. The motivating example we use is the derivation of the updated series
expansion of the quark mass renormalization group equation (RGE) to five-loop
order. This series provides the relation between a light quark mass in the
modified minimal subtraction ($\overline{\text{MS}}$) scheme defined at some
given scale, e.g. at the tau-lepton mass scale, and another chosen energy
scale, $s$. This relation explicitly depicts the renormalization scheme
dependence of the running quark mass on the scale parameter, $s$, and is
important in accurately determining a light quark mass at a chosen scale. The
five-loop QCD $\beta(a_s)$ and $\gamma(a_s)$ functions are used in this
determination.
"
1239,"A SAT+CAS Approach to Finding Good Matrices: New Examples and
  Counterexamples","  We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
"
1240,ATENSOR - REDUCE program for tensor simplification,"  The paper presents a REDUCE program for the simplification of tensor
expressions that are considered as formal indexed objects. The proposed
algorithm is based on the consideration of tensor expressions as vectors in
some linear space. This linear space is formed by all the elements of the group
algebra of the corresponding tensor expression. Such approach permits us to
simplify the tensor expressions possessing symmetry properties, summation
(dummy) indices and multiterm identities by unify manner. The canonical element
for the tensor expression is defined in terms of the basic vectors of this
linear space. The main restriction of the algorithm is the dimension of the
linear space that is equal to N!, where N is a number of indices of the tensor
expression. The program uses REDUCE as user interface.
"
1241,"Staging Human-computer Dialogs: An Application of the Futamura
  Projections","  We demonstrate an application of the Futamura Projections to human-computer
interaction, and particularly to staging human-computer dialogs. Specifically,
by providing staging analogs to the classical Futamura Projections, we
demonstrate that the Futamura Projections can be applied to the staging of
human-computer dialogs in addition to the execution of programs.
"
1242,"Temporal viability regulation for control affine systems with
  applications to mobile vehicle coordination under time-varying motion
  constraints","  Controlled invariant set and viability regulation of dynamical control
systems have played important roles in many control and coordination
applications. In this paper we develop a temporal viability regulation theory
for general dynamical control systems, and in particular for control affine
systems. The time-varying viable set is parameterized by time-varying
constraint functions, with the aim to regulate a dynamical control system to be
invariant in the time-varying viable set so that temporal state-dependent
constraints are enforced. We consider both time-varying equality and inequality
constraints in defining a temporal viable set. We also present sufficient
conditions for the existence of feasible control input for the control affine
systems. The developed temporal viability regulation theory is applied to
mobile vehicle coordination.
"
1243,Simplification of tensor expressions in computer algebra,"  Computer algebra is widely used in various fields of mathematics, physics and
other sciences. The simplification of tensor expressions is an important
special case of computer algebra. In this paper, we consider the reduction of
tensor polynomials to canonical form, taking into account the properties of
symmetry under permutations of indices, the symmetries associated with the
renaming of summation indices, and also linear relations between tensors of a
general form. We give a definition of the canonical representation for
polynomial (multiplicative) expressions of variables with abstract indices,
which is the result of averaging of the original expression by the action of
some finite group (the signature stabilizer). In practice, the proposed
algorithms demonstrate high efficiency for expressions made of Riemann
curvature tensors.
"
1244,A Fast Randomized Geometric Algorithm for Computing Riemann-Roch Spaces,"  We propose a probabilistic variant of Brill-Noether's algorithm for computing
a basis of the Riemann-Roch space $L(D)$ associated to a divisor $D$ on a
projective nodal plane curve $\mathcal C$ over a sufficiently large perfect
field $k$. Our main result shows that this algorithm requires at most
$O(\max(\mathrm{deg}(\mathcal C)^{2\omega}, \mathrm{deg}(D_+)^\omega))$
arithmetic operations in $k$, where $\omega$ is a feasible exponent for matrix
multiplication and $D_+$ is the smallest effective divisor such that $D_+\geq
D$. This improves the best known upper bounds on the complexity of computing
Riemann-Roch spaces. Our algorithm may fail, but we show that provided that a
few mild assumptions are satisfied, the failure probability is bounded by
$O(\max(\mathrm{deg}(\mathcal C)^4, \mathrm{deg}(D_+)^2)/\lvert \mathcal
E\rvert)$, where $\mathcal E$ is a finite subset of $k$ in which we pick
elements uniformly at random. We provide a freely available C++/NTL
implementation of the proposed algorithm and we present experimental data. In
particular, our implementation enjoys a speedup larger than 6 on many examples
(and larger than 200 on some instances over large finite fields) compared to
the reference implementation in the Magma computer algebra system. As a
by-product, our algorithm also yields a method for computing the group law on
the Jacobian of a smooth plane curve of genus $g$ within $O(g^\omega)$
operations in $k$, which equals the best known complexity for this problem.
"
1245,Linear Differential Equations as a Data-Structure,"  A lot of information concerning solutions of linear differential equations
can be computed directly from the equation. It is therefore natural to consider
these equations as a data-structure, from which mathematical properties can be
computed. A variety of algorithms has thus been designed in recent years that
do not aim at ""solving"", but at computing with this representation. Many of
these results are surveyed here.
"
1246,"Kleene stars of the plane, polylogarithms and symmetries","  We extend the definition and construct several bases for polylogarithms Li T
, where T are some series, recognizable by a finite state (multiplicity)
automaton of alphabet 4 X = {x 0 , x 1 }. The kernel of this new
""polylogarithmic map"" Li $\bullet$ is also characterized and provides a
rewriting process which terminates to a normal form. We concentrate on
algebraic and analytic aspects of this extension allowing index polylogarithms
at non positive multi-indices, by rational series and regularize polyzetas at
non positive multi-indices.
"
1247,"Fast Algorithms for Computing Eigenvectors of Matrices via Pseudo
  Annihilating Polynomials","  An efficient algorithm for computing eigenvectors of a matrix of integers by
exact computation is proposed. The components of calculated eigenvectors are
expressed as polynomials in the eigenvalue to which the eigenvector is
associated, as a variable. The algorithm, in principle, utilizes the minimal
annihilating polynomials for eliminating redundant calculations. Furthermore,
in the actual computation, the algorithm computes candidates of eigenvectors by
utilizing pseudo annihilating polynomials and verifies their correctness. The
experimental results show that our algorithms have better performance compared
to conventional methods.
"
1248,"On Exact Reznick, Hilbert-Artin and Putinar's Representations","  We consider the problem of computing exact sums of squares (SOS)
decompositions for certain classes of non-negative multivariate polynomials,
relying on semidefinite programming (SDP) solvers.
  We provide a hybrid numeric-symbolic algorithm computing exact rational SOS
decompositions with rational coefficients for polynomials lying in the interior
of the SOS cone. The first step of this algorithm computes an approximate SOS
decomposition for a perturbation of the input polynomial with an
arbitrary-precision SDP solver. Next, an exact SOS decomposition is obtained
thanks to the perturbation terms and a compensation phenomenon. We prove that
bit complexity estimates on output size and runtime are both polynomial in the
degree of the input polynomial and singly exponential in the number of
variables. Next, we apply this algorithm to compute exact Reznick,
Hilbert-Artin's representation and Putinar's representations respectively for
positive definite forms and positive polynomials over basic compact
semi-algebraic sets. We also report on practical experiments done with the
implementation of these algorithms and existing alternatives such as the
critical point method and cylindrical algebraic decomposition.
"
1249,Chordal Graphs in Triangular Decomposition in Top-Down Style,"  In this paper, we first prove that when the associated graph of a polynomial
set is chordal, a particular triangular set computed by a general algorithm in
top-down style for computing the triangular decomposition of this polynomial
set has an associated graph as a subgraph of this chordal graph. Then for
Wang's method and a subresultant-based algorithm for triangular decomposition
in top-down style and for a subresultant-based algorithm for regular
decomposition in top-down style, we prove that all the polynomial sets
appearing in the process of triangular decomposition with any of these
algorithms have associated graphs as subgraphs of this chordal graph. These
theoretical results can be viewed as non-trivial polynomial generalization of
existing ones for sparse Gaussian elimination, inspired by which we further
propose an algorithm for sparse triangular decomposition in top-down style by
making use of the chordal structure of the polynomial set. The effectiveness of
the proposed algorithm for triangular decomposition, when the polynomial set is
chordal and sparse with respect to the variables, is demonstrated by
preliminary experimental results.
"
1250,Deep learning for pedestrians: backpropagation in CNNs,"  The goal of this document is to provide a pedagogical introduction to the
main concepts underpinning the training of deep neural networks using gradient
descent; a process known as backpropagation. Although we focus on a very
influential class of architectures called ""convolutional neural networks""
(CNNs) the approach is generic and useful to the machine learning community as
a whole. Motivated by the observation that derivations of backpropagation are
often obscured by clumsy index-heavy narratives that appear somewhat
mathemagical, we aim to offer a conceptually clear, vectorized description that
articulates well the higher level logic. Following the principle of ""writing is
nature's way of letting you know how sloppy your thinking is"", we try to make
the calculations meticulous, self-contained and yet as intuitive as possible.
Taking nothing for granted, ample illustrations serve as visual guides and an
extensive bibliography is provided for further explorations.
  (For the sake of clarity, long mathematical derivations and visualizations
have been broken up into short ""summarized views"" and longer ""detailed views""
encoded into the PDF as optional content groups. Some figures contain
animations designed to illustrate important concepts in a more engaging style.
For these reasons, we advise to download the document locally and open it using
Adobe Acrobat Reader. Other viewers were not tested and may not render the
detailed views, animations correctly.)
"
1251,"An efficient reduction strategy for signature-based algorithms to
  compute Groebner basis","  This paper introduces a strategy for signature-based algorithms to compute
Groebner basis. The signature-based algorithms generate S-pairs instead of
S-polynomials, and use s-reduction instead of the usual reduction used in the
Buchberger algorithm. There are two strategies for s-reduction: one is the
only-top reduction strategy which is the way that only leading monomials are
s-reduced. The other is the full reduction strategy which is the way that all
monomials are s-reduced. A new strategy, which we call selective-full strategy,
for s-reduction of S-pairs is introduced in this paper. In the experiment, this
strategy is efficient for computing the reduced Groebner basis. For computing a
signature Groebner basis, it is the most efficient or not the worst of the
three strategies.
"
1252,BOSPHORUS: Bridging ANF and CNF Solvers,"  Algebraic Normal Form (ANF) and Conjunctive Normal Form (CNF) are commonly
used to encode problems in Boolean algebra. ANFs are typically solved via
Gr""obner basis algorithms, often using more memory than is feasible; while CNFs
are solved using SAT solvers, which cannot exploit the algebra of polynomials
naturally. We propose a paradigm that bridges between ANF and CNF solving
techniques: the techniques are applied in an iterative manner to emph{learn
facts} to augment the original problems. Experiments on over 1,100 benchmarks
arising from four different applications domains demonstrate that learnt facts
can significantly improve runtime and enable more benchmarks to be solved.
"
1253,Computing Nearby Non-trivial Smith Forms,"  We consider the problem of computing the nearest matrix polynomial with a
non-trivial Smith Normal Form. We show that computing the Smith form of a
matrix polynomial is amenable to numeric computation as an optimization
problem. Furthermore, we describe an effective optimization technique to find a
nearby matrix polynomial with a non-trivial Smith form. The results are then
generalized to include the computation of a matrix polynomial having a maximum
specified number of ones in the Smith Form (i.e., with a maximum specified
McCoy rank). We discuss the geometry and existence of solutions and how our
results can be used for an error analysis. We develop an optimization-based
approach and demonstrate an iterative numerical method for computing a nearby
matrix polynomial with the desired spectral properties. We also describe an
implementation of our algorithms and demonstrate the robustness with examples
in Maple.
"
1254,In Praise of Sequence (Co-)Algebra and its implementation in Haskell,"  What is Sequence Algebra? This is a question that any teacher or student of
mathematics or computer science can engage with. Sequences are in Calculus,
Combinatorics, Statistics and Computation. They are foundational, a step up
from number arithmetic. Sequence operations are easy to implement from scratch
(in Haskell) and afford a wide variety of testing and experimentation. When
bits and pieces of sequence algebra are pulled together from the literature,
there emerges a claim for status as a substantial pre-analysis topic. Here we
set the stage by bringing together a variety of sequence algebra concepts for
the first time in one paper. This provides a novel economical overview,
intended to invite a broad mathematical audience to cast an eye over the
subject. A complete, yet succinct, basic implementation of sequence operations
is presented, ready to play with. The implementation also serves as a benchmark
for introducing Haskell by mathematical example.
"
1255,The Complexity of Factors of Multivariate Polynomials,"  The existence of string functions, which are not polynomial time computable,
but whose graph is checkable in polynomial time, is a basic assumption in
cryptography. We prove that in the framework of algebraic complexity, there are
no such families of polynomial functions of polynomially bounded degree over
fields of characteristic zero. The proof relies on a polynomial upper bound on
the approximative complexity of a factor g of a polynomial f in terms of the
(approximative) complexity of f and the degree of the factor g. This extends a
result by Kaltofen (STOC 1986). The concept of approximative complexity allows
to cope with the case that a factor has an exponential multiplicity, by using a
perturbation argument. Our result extends to randomized (two-sided error)
decision complexity.
"
1256,SIAN: software for structural identifiability analysis of ODE models,"  Biological processes are often modeled by ordinary differential equations
with unknown parameters. The unknown parameters are usually estimated from
experimental data. In some cases, due to the structure of the model, this
estimation problem does not have a unique solution even in the case of
continuous noise-free data. It is therefore desirable to check the uniqueness a
priori before carrying out actual experiments. We present a new software SIAN
(Structural Identifiability ANalyser) that does this. Our software can tackle
problems that could not be tackled by previously developed packages.
"
1257,Computing the topology of a planar or space hyperelliptic curve,"  We present algorithms to compute the topology of 2D and 3D hyperelliptic
curves. The algorithms are based on the fact that 2D and 3D hyperelliptic
curves can be seen as the image of a planar curve (the Weierstrass form of the
curve), whose topology is easy to compute, under a birational mapping of the
plane or the space. We report on a {\tt Maple} implementation of these
algorithms, and present several examples. Complexity and certification issues
are also discussed.
"
1258,"A New Deflation Method For Verifying the Isolated Singular Zeros of
  Polynomial Systems","  In this paper, we develop a new deflation technique for refining or verifying
the isolated singular zeros of polynomial systems. Starting from a polynomial
system with an isolated singular zero, by computing the derivatives of the
input polynomials directly or the linear combinations of the related
polynomials, we construct a new system, which can be used to refine or verify
the isolated singular zero of the input system. In order to preserve the
accuracy in numerical computation as much as possible, new variables are
introduced to represent the coefficients of the linear combinations of the
related polynomials. To our knowledge, it is the first time that considering
the deflation problem of polynomial systems from the perspective of the linear
combination. Some acceleration strategies are proposed to reduce the scale of
the final system. We also give some further analysis of the tolerances we use,
which can help us have a better understanding of our method.The experiments
show that our method is effective and efficient. Especially, it works well for
zeros with high multiplicities of large systems. It also works for isolated
singular zeros of non-polynomial systems.
"
1259,"Subresultants of $(x-\alpha)^m$ and $(x-\beta)^n$, Jacobi polynomials
  and complexity","  In an earlier article together with Carlos D'Andrea [BDKSV2017], we described
explicit expressions for the coefficients of the order-$d$ polynomial
subresultant of $(x-\alpha)^m$ and $(x-\beta)^n $ with respect to Bernstein's
set of polynomials $\{(x-\alpha)^j(x-\beta)^{d-j}, \, 0\le j\le d\}$, for $0\le
d<\min\{m, n\}$. The current paper further develops the study of these
structured polynomials and shows that the coefficients of the subresultants of
$(x-\alpha)^m$ and $(x-\beta)^n$ with respect to the monomial basis can be
computed in linear arithmetic complexity, which is faster than for arbitrary
polynomials. The result is obtained as a consequence of the amazing though
seemingly unnoticed fact that these subresultants are scalar multiples of
Jacobi polynomials up to an affine change of variables.
"
1260,On Fast Matrix Inversion via Fast Matrix Multiplication,"  Volker Strassen first suggested an algorithm to multiply matrices with worst
case running time less than the conventional $\mathcal{O}(n^3)$ operations in
1969. He also presented a recursive algorithm with which to invert matrices,
and calculate determinants using matrix multiplication. James R. Bunch & John
E. Hopcroft improved upon this in 1974 by providing modifications to the
inversion algorithm in the case where principal submatrices were singular,
amongst other improvements. We cover the case of multivariate polynomial matrix
inversion, where it is noted that conventional methods that assume a field will
experience major setbacks. Initially, the author and others published a
presentation of a fraction free formulation of inversion via matrix
multiplication along with motivations, however analysis of this presentation
was rudimentary. We hence provide a discussion of the true complexities of this
fraction free method arising from matrix multiplication, and arrive at its
limitations.
"
1261,Sheaves: A Topological Approach to Big Data,"  This document develops general concepts useful for extracting knowledge
embedded in large graphs or datasets that have pair-wise relationships, such as
cause-effect-type relations. Almost no underlying assumptions are made, other
than that the data can be presented in terms of pair-wise relationships between
objects/events. This assumption is used to mine for patterns in the dataset,
defining a reduced graph or dataset that boils-down or concentrates information
into a more compact form. The resulting extracted structure or set of patterns
are manifestly symbolic in nature, as they capture and encode the graph
structure of the dataset in terms of a (generative) grammar. This structure is
identified as having the formal mathematical structure of a sheaf. In essence,
this paper introduces the basic concepts of sheaf theory into the domain of
graphical datasets.
"
1262,Spectral Approach to Verifying Non-linear Arithmetic Circuits,"  This paper presents a fast and effective computer algebraic method for
analyzing and verifying non-linear integer arithmetic circuits using a novel
algebraic spectral model. It introduces a concept of algebraic spectrum, a
numerical form of polynomial expression; it uses the distribution of
coefficients of the monomials to determine the type of arithmetic function
under verification. In contrast to previous works, the proof of functional
correctness is achieved by computing an algebraic spectrum combined with a
local rewriting of word-level polynomials. The speedup is achieved by
propagating coefficients through the circuit using And-Inverter Graph (AIG)
datastructure. The effectiveness of the method is demonstrated with experiments
including standard and Booth multipliers, and other synthesized non-linear
arithmetic circuits up to 1024 bits containing over 12 million gates.
"
1263,"Automated Synthesis of Safe Digital Controllers for Sampled-Data
  Stochastic Nonlinear Systems","  We present a new method for the automated synthesis of digital controllers
with formal safety guarantees for systems with nonlinear dynamics, noisy output
measurements, and stochastic disturbances. Our method derives digital
controllers such that the corresponding closed-loop system, modeled as a
sampled-data stochastic control system, satisfies a safety specification with
probability above a given threshold. The proposed synthesis method alternates
between two steps: generation of a candidate controller pc, and verification of
the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
probability, and by using a non-validated ODE solver for simulating the system.
Such a candidate is therefore sub-optimal but can be generated very rapidly. To
rule out unstable candidate controllers, we prove and utilize Lyapunov's
indirect method for instability of sampled-data nonlinear systems. In the
subsequent verification step, we use a validated solver based on SMT
(Satisfiability Modulo Theories) to compute a numerically and statistically
valid confidence interval for the safety probability of pc. If the probability
so obtained is not above the threshold, we expand the search space for
candidates by increasing the controller degree. We evaluate our technique on
three case studies: an artificial pancreas model, a powertrain control model,
and a quadruple-tank process.
"
1264,"A new algorithm for irreducible decomposition of representations of
  finite groups","  An algorithm for irreducible decomposition of representations of finite
groups over fields of characteristic zero is described. The algorithm uses the
fact that the decomposition induces a partition of the invariant inner product
into a complete set of mutually orthogonal projectors. By expressing the
projectors through the basis elements of the centralizer ring of the
representation, the problem is reduced to solving systems of quadratic
equations. The current implementation of the algorithm is able to split
representations of dimensions up to hundreds of thousands. Examples of
calculations are given.
"
1265,Efficiently factoring polynomials modulo $p^4$,"  Polynomial factoring has famous practical algorithms over fields-- finite,
rational \& $p$-adic. However, modulo prime powers it gets hard as there is
non-unique factorization and a combinatorial blowup ensues. For example, $x^2+p
\bmod p^2$ is irreducible, but $x^2+px \bmod p^2$ has exponentially many
factors! We present the first randomized poly(deg $f, \log p$) time algorithm
to factor a given univariate integral $f(x)$ modulo $p^k$, for a prime $p$ and
$k \leq 4$. Thus, we solve the open question of factoring modulo $p^3$ posed in
(Sircana, ISSAC'17).
  Our method reduces the general problem of factoring $f(x) \bmod p^k$ to that
of {\em root finding} in a related polynomial $E(y) \bmod\langle p^k,
\varphi(x)^\ell \rangle$ for some irreducible $\varphi \bmod p$. We could
efficiently solve the latter for $k\le4$, by incrementally transforming $E(y)$.
Moreover, we discover an efficient and strong generalization of Hensel lifting
to lift factors of $f(x) \bmod p$ to those $\bmod\ p^4$ (if possible). This was
previously unknown, as the case of repeated factors of $f(x) \bmod p$ forbids
classical Hensel lifting.
"
1266,Symbolic integration of hyperexponential 1-forms,"  Let $H$ be a hyperexponential function in $n$ variables $x=(x_1,\dots,x_n)$
with coefficients in a field $\mathbb{K}$, $[\mathbb{K}:\mathbb{Q}] <\infty$,
and $\omega$ a rational differential $1$-form. Assume that $H\omega$ is closed
and $H$ transcendental. We prove using Schanuel conjecture that there exist a
univariate function $f$ and multivariate rational functions $F,R$ such that
$\int H\omega= f(F(x))+H(x)R(x)$. We present an algorithm to compute this
decomposition. This allows us to present an algorithm to construct a basis of
the cohomology of differential $1$-forms with coefficients in
$H\mathbb{K}[x,1/(SD)]$ for a given $H$, $D$ being the denominator of $dH/H$
and $S\in\mathbb{K}[x]$ a square free polynomial. As an application, we
generalize a result of Singer on differential equations on the plane: whenever
it admits a Liouvillian first integral $I$ but no Darbouxian first integral,
our algorithm gives a rational variable change linearising the system.
"
1267,Nearly Optimal Sparse Polynomial Multiplication,"  In the sparse polynomial multiplication problem, one is asked to multiply two
sparse polynomials f and g in time that is proportional to the size of the
input plus the size of the output. The polynomials are given via lists of their
coefficients F and G, respectively. Cole and Hariharan (STOC 02) have given a
nearly optimal algorithm when the coefficients are positive, and Arnold and
Roche (ISSAC 15) devised an algorithm running in time proportional to the
""structural sparsity"" of the product, i.e. the set supp(F)+supp(G). The latter
algorithm is particularly efficient when there not ""too many cancellations"" of
coefficients in the product. In this work we give a clean, nearly optimal
algorithm for the sparse polynomial multiplication problem.
"
1268,"On the Existence of Telescopers for Rational Functions in Three
  Variables","  Zeilberger's method of creative telescoping is crucial for the
computer-generated proofs of combinatorial and special-function identities.
Telescopers are linear differential or ($q$-)recurrence operators computed by
algorithms for creative telescoping. For a given class of inputs, when
telescopers exist and how to construct telescopers efficiently if they exist
are two fundamental problems related to creative telescoping. In this paper, we
solve the existence problem of telescopers for rational functions in three
variables including 18 cases. We reduce the existence problem from the
trivariate case to the bivariate case and some related problems. The existence
criteria given in this paper enable us to determine the termination of
algorithms for creative telescoping with trivariate rational inputs.
"
1269,"Gr{\""o}bner bases over Tate algebras","  Tate algebras are fundamental objects in the context of analytic geometry
over the p-adics. Roughly speaking, they play the same role as polynomial
algebras play in classical algebraic geometry. In the present article, we
develop the formalism of Gr{\""o}bner bases for Tate algebras. We prove an
analogue of the Buchberger criterion in our framework and design a
Buchberger-like and a F4-like algorithm for computing Gr{\""o}bner bases over
Tate algebras. An implementation in SM is also discussed.
"
1270,"Signature-based M\""oller's algorithm for strong Gr\""obner bases over
  PIDs","  Signature-based algorithms are the latest and most efficient approach as of
today to compute Gr\""obner bases for polynomial systems over fields. Recently,
possible extensions of these techniques to general rings have attracted the
attention of several authors.
  In this paper, we present a signature-based version of M\""oller's classical
variant of Buchberger's algorithm for computing strong Gr\""obner bases over
Principal Ideal Domains (or PIDs). It ensures that the signatures do not
decrease during the algorithm, which makes it possible to apply classical
signature criteria for further optimization. In particular, with the F5
criterion, the signature version of M\""oller's algorithm computes a Gr\""obner
basis without reductions to zero for a polynomial system given by a regular
sequence. We also show how Buchberger's chain criterion can be implemented so
as to be compatible with the signatures.
  We prove correctness and termination of the algorithm. Furthermore, we have
written a toy implementation in Magma, allowing us to quantitatively compare
the efficiency of the various criteria for eliminating S-pairs.
"
1271,A Faster Solution to Smale's 17th Problem I: Real Binomial Systems,"  Suppose $F:=(f_1,\ldots,f_n)$ is a system of random $n$-variate polynomials
with $f_i$ having degree $\leq\!d_i$ and the coefficient of $x^{a_1}_1\cdots
x^{a_n}_n$ in $f_i$ being an independent complex Gaussian of mean $0$ and
variance $\frac{d_i!}{a_1!\cdots a_n!\left(d_i-\sum^n_{j=1}a_j \right)!}$.
Recent progress on Smale's 17th Problem by Lairez --- building upon seminal
work of Shub, Beltran, Pardo, B\""{u}rgisser, and Cucker --- has resulted in a
deterministic algorithm that finds a single (complex) approximate root of $F$
using just $N^{O(1)}$ arithmetic operations on average, where
$N\!:=\!\sum^n_{i=1}\frac{(n+d_i)!}{n!d_i!}$ ($=n(n+\max_i
d_i)^{O(\min\{n,\max_i d_i)\}}$) is the maximum possible total number of
monomial terms for such an $F$. However, can one go faster when the number of
terms is smaller, and we restrict to real coefficient and real roots? And can
one still maintain average-case polynomial-time with more general probability
measures?
  We show the answer is yes when $F$ is instead a binomial system --- a case
whose numerical solution is a key step in polyhedral homotopy algorithms for
solving arbitrary polynomial systems. We give a deterministic algorithm that
finds a real approximate root (or correctly decides there are none) using just
$O(n^2(\log(n)+\log\max_i d_i))$ arithmetic operations on average. Furthermore,
our approach allows Gaussians with arbitrary variance. We also discuss briefly
the obstructions to maintaining average-case time polynomial in $n\log \max_i
d_i$ when $F$ has more terms.
"
1272,An Optimization-Based Sum-of-Squares Approach to Vizing's Conjecture,"  Vizing's conjecture (open since 1968) relates the sizes of dominating sets in
two graphs to the size of a dominating set in their Cartesian product graph. In
this paper, we formulate Vizing's conjecture itself as a Positivstellensatz
existence question. In particular, we encode the conjecture as an
ideal/polynomial pair such that the polynomial is nonnegative if and only if
the conjecture is true. We demonstrate how to use semidefinite optimization
techniques to computationally obtain numeric sum-of-squares certificates, and
then show how to transform these numeric certificates into symbolic
certificates approving nonnegativity of our polynomial.
  After outlining the theoretical structure of this computer-based proof of
Vizing's conjecture, we present computational and theoretical results. In
particular, we present exact low-degree sparse sum-of-squares certificates for
particular families of graphs.
"
1273,"On the Complexity of Computing the Topology of Real Algebraic Space
  Curves","  In this paper, we present a deterministic algorithm to find a strong generic
position for an algebraic space curve. We modify our existing algorithm for
computing the topology of an algebraic space curve and analyze the bit
complexity of the algorithm. It is $\tilde{\mathcal {O}} (N^{20})$, where
$N=\max\{d,\tau\}$, $d, \tau$ are the degree bound and the bit size bound of
the coefficients of the defining polynomials of the algebraic space curve. To
our knowledge, this is the best bound among the existing work. It gains the
existing results at least $N^2$.
"
1274,"Effective certification of approximate solutions to systems of equations
  involving analytic functions","  We develop algorithms for certifying an approximation to a nonsingular
solution of a square system of equations built from univariate analytic
functions. These algorithms are based on the existence of oracles for
evaluating basic data about the input analytic functions. One approach for
certification is based on alpha-theory while the other is based on the Krawczyk
generalization of Newton's iteration. We show that the necessary oracles exist
for D-finite functions and compare the two algorithmic approaches for this case
using our software implementation in SageMath.
"
1275,LU factorization with errors *,"  We present new algorithms to detect and correct errors in the lower-upper
factorization of a matrix, or the triangular linear system solution, over an
arbitrary field. Our main algorithms do not require any additional information
or encoding other than the original inputs and the erroneous output. Their
running time is softly linear in the dimension times the number of errors when
there are few errors, smoothly growing to the cost of fast matrix
multiplication as the number of errors increases. We also present applications
to general linear system solving.
"
1276,"Rational Solutions of First-Order Algebraic Ordinary Difference
  Equations","  We propose an algebraic geometric approach for studying rational solutions of
first-order algebraic ordinary difference equations. For an autonomous
first-order algebraic ordinary difference equations, we give an upper bound for
the degrees of its rational solutions, and thus derive a complete algorithm for
computing corresponding rational solutions.
"
1277,Algorithmic counting of nonequivalent compact Huffman codes,"  It is known that the following five counting problems lead to the same
integer sequence~$f_t(n)$: the number of nonequivalent compact Huffman codes of
length~$n$ over an alphabet of $t$ letters, the number of `nonequivalent'
canonical rooted $t$-ary trees (level-greedy trees) with $n$~leaves, the number
of `proper' words, the number of bounded degree sequences, and the number of
ways of writing $1= \frac{1}{t^{x_1}}+ \dots + \frac{1}{t^{x_n}}$ with integers
$0 \leq x_1 \leq x_2 \leq \dots \leq x_n$. In this work, we show that one can
compute this sequence for \textbf{all} $n<N$ with essentially one power series
division. In total we need at most $N^{1+\varepsilon}$ additions and
multiplications of integers of $cN$ bits, $c<1$, or $N^{2+\varepsilon}$ bit
operations, respectively. This improves an earlier bound by Even and Lempel who
needed $O(N^3)$ operations in the integer ring or $O(N^4)$ bit operations,
respectively.
"
1278,"Gr{\""o}bner Basis over Semigroup Algebras: Algorithms and Applications
  for Sparse Polynomial Systems","  Gr{\""o}bner bases is one the most powerful tools in algorithmic non-linear
algebra. Their computation is an intrinsically hard problem with a complexity
at least single exponential in the number of variables. However, in most of the
cases, the polynomial systems coming from applications have some kind of
structure. For example , several problems in computer-aided design, robotics,
vision, biology , kinematics, cryptography, and optimization involve sparse
systems where the input polynomials have a few non-zero terms. Our approach to
exploit sparsity is to embed the systems in a semigroup algebra and to compute
Gr{\""o}bner bases over this algebra. Up to now, the algorithms that follow this
approach benefit from the sparsity only in the case where all the polynomials
have the same sparsity structure, that is the same Newton polytope. We
introduce the first algorithm that overcomes this restriction. Under regularity
assumptions, it performs no redundant computations. Further, we extend this
algorithm to compute Gr{\""o}bner basis in the standard algebra and solve sparse
polynomials systems over the torus $(C*)^n$. The complexity of the algorithm
depends on the Newton polytopes.
"
1279,On the Complexity of Toric Ideals,"  We investigate the computational complexity of problems on toric ideals such
as normal forms, Gr\""obner bases, and Graver bases. We show that all these
problems are strongly NP-hard in the general case. Nonetheless, we can derive
efficient algorithms by taking advantage of the sparsity pattern of the matrix.
We describe this sparsity pattern with a graph, and study the parameterized
complexity of toric ideals in terms of graph parameters such as treewidth and
treedepth. In particular, we show that the normal form problem can be solved in
parameter-tractable time in terms of the treedepth. An important application of
this result is in multiway ideals arising in algebraic statistics. We also give
a parameter-tractable membership test to the reduced Gr\""obner basis. This test
leads to an efficient procedure for computing the reduced Gr\""obner basis.
Similar results hold for Graver bases computation.
"
1280,Modeling Terms by Graphs with Structure Constraints (Two Illustrations),"  In the talk at the workshop my aim was to demonstrate the usefulness of graph
techniques for tackling problems that have been studied predominantly as
problems on the term level: increasing sharing in functional programs, and
addressing questions about Milner's process semantics for regular expressions.
For both situations an approach that is based on modeling terms by graphs with
structure constraints has turned out to be fruitful. In this extended abstract
I describe the underlying problems, give references, provide examples, indicate
the chosen approaches, and compare the initial situations as well as the
results that have been obtained, and some results that are being developed at
present.
"
1281,"Exact Optimization via Sums of Nonnegative Circuits and Sums of AM/GM
  Exponentials","  We provide two hybrid numeric-symbolic optimization algorithms, computing
exact sums of nonnegative circuits (SONC) and sums of
arithmetic-geometric-exponentials (SAGE) decompositions. Moreover, we provide a
hybrid numeric-symbolic decision algorithm for polynomials lying in the
interior of the SAGE cone. Each framework, inspired by previous contributions
of Parrilo and Peyrl, is a rounding-projection procedure.
  For a polynomial lying in the interior of the SAGE cone, we prove that the
decision algorithm terminates within a number of arithmetic operations, which
is polynomial in the degree and number of terms of the input, and singly
exponential in the number of variables. We also provide experimental
comparisons regarding the implementation of the two optimization algorithms.
"
1282,Generic reductions for in-place polynomial multiplication,"  The polynomial multiplication problem has attracted considerable attention
since the early days of computer algebra, and several algorithms have been
designed to achieve the best possible time complexity. More recently, efforts
have been made to improve the space complexity, developing modified versions of
a few specific algorithms to use no extra space while keeping the same
asymptotic running time. In this work, we broaden the scope in two regards.
First, we ask whether an arbitrary multiplication algorithm can be performed
in-place generically. Second, we consider two important variants which produce
only part of the result (and hence have less space to work with), the so-called
middle and short products, and ask whether these operations can also be
performed in-place. To answer both questions in (mostly) the affirmative, we
provide a series of reductions starting with any linear-space multiplication
algorithm. For full and short product algorithms these reductions yield
in-place versions with the same asymptotic time complexity as the out-of-place
version. For the middle product, the reduction incurs an extra logarithmic
factor in the time complexity only when the algorithm is quasi-linear.
"
1283,"When Causal Intervention Meets Adversarial Examples and Image Masking
  for Deep Neural Networks","  Discovering and exploiting the causality in deep neural networks (DNNs) are
crucial challenges for understanding and reasoning causal effects (CE) on an
explainable visual model. ""Intervention"" has been widely used for recognizing a
causal relation ontologically. In this paper, we propose a causal inference
framework for visual reasoning via do-calculus. To study the intervention
effects on pixel-level features for causal reasoning, we introduce pixel-wise
masking and adversarial perturbation. In our framework, CE is calculated using
features in a latent space and perturbed prediction from a DNN-based model. We
further provide the first look into the characteristics of discovered CE of
adversarially perturbed images generated by gradient-based methods
\footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}.
Experimental results show that CE is a competitive and robust index for
understanding DNNs when compared with conventional methods such as
class-activation mappings (CAMs) on the Chest X-Ray-14 dataset for
human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds
promises for detecting adversarial examples as it possesses distinct
characteristics in the presence of adversarial perturbations.
"
1284,"Identifying the Parametric Occurrence of Multiple Steady States for some
  Biological Networks","  We consider a problem from biological network analysis of determining regions
in a parameter space over which there are multiple steady states for positive
real values of variables and parameters. We describe multiple approaches to
address the problem using tools from Symbolic Computation. We describe how
progress was made to achieve semi-algebraic descriptions of the
multistationarity regions of parameter space, and compare symbolic results to
numerical methods. The biological networks studied are models of the
mitogen-activated protein kinases (MAPK) network which has already consumed
considerable effort using special insights into its structure of corresponding
models. Our main example is a model with 11 equations in 11 variables and 19
parameters, 3 of which are of interest for symbolic treatment. The model also
imposes positivity conditions on all variables and parameters.
  We apply combinations of symbolic computation methods designed for mixed
equality/inequality systems, specifically virtual substitution, lazy real
triangularization and cylindrical algebraic decomposition, as well as a
simplification technique adapted from Gaussian elimination and graph theory. We
are able to determine multistationarity of our main example over a
2-dimensional parameter space. We also study a second MAPK model and a symbolic
grid sampling technique which can locate such regions in 3-dimensional
parameter space.
"
1285,"Computing Minimal Presentations and Bigraded Betti Numbers of
  2-Parameter Persistent Homology","  Motivated by applications to topological data analysis, we give an efficient
algorithm for computing a (minimal) presentation of a bigraded $K[x,y]$-module
$M$, where $K$ is a field. The algorithm takes as input a short chain complex
of free modules \[ F^2 \xrightarrow{\partial^2} F^1 \xrightarrow{\partial^1}
F^0 \] such that $M\cong \ker{\partial^1}/\mathrm{im}{\partial^2}$. It runs in
time $O(\sum_i |F^i|^3)$ and requires $O(\sum_i |F^i|^2)$ memory, where $|F^i|$
denotes the size of a basis of $F^i$. We observe that, given the presentation
computed by our algorithm, the bigraded Betti numbers of $M$ are readily
computed. We also introduce a different but related algorithm, based on Koszul
homology, which computes the bigraded Betti numbers without computing a
presentation, with these same complexity bounds. These algorithms have been
implemented in RIVET, a software tool for the visualization and analysis of
two-parameter persistent homology. In experiments on topological data analysis
problems, our approach outperforms the standard computational commutative
algebra packages Singular and Macaulay2 by a wide margin.
"
1286,"Counting basic-irreducible factors mod $p^k$ in deterministic poly-time
  and $p$-adic applications","  Finding an irreducible factor, of a polynomial $f(x)$ modulo a prime $p$, is
not known to be in deterministic polynomial time. Though there is such a
classical algorithm that {\em counts} the number of irreducible factors of
$f\bmod p$. We can ask the same question modulo prime-powers $p^k$. The
irreducible factors of $f\bmod p^k$ blow up exponentially in number; making it
hard to describe them. Can we count those irreducible factors $\bmod~p^k$ that
remain irreducible mod $p$? These are called {\em basic-irreducible}. A simple
example is in $f=x^2+px \bmod p^2$; it has $p$ many basic-irreducible factors.
Also note that, $x^2+p \bmod p^2$ is irreducible but not basic-irreducible!
  We give an algorithm to count the number of basic-irreducible factors of
$f\bmod p^k$ in deterministic poly(deg$(f),k\log p$)-time. This solves the open
questions posed in (Cheng et al, ANTS'18 \& Kopp et al, Math.Comp.'19). In
particular, we are counting roots $\bmod\ p^k$; which gives the first
deterministic poly-time algorithm to compute Igusa zeta function of $f$. Also,
our algorithm efficiently partitions the set of all basic-irreducible factors
(possibly exponential) into merely deg$(f)$-many disjoint sets, using a compact
tree data structure and {\em split} ideals.
"
1287,Discovering and Proving Infinite Pochhammer Sum Identities,"  We consider nested sums involving the Pochhammer symbol at infinity and
rewrite them in terms of a small set of constants, such as powers of $\pi,$
$\log(2)$ or zeta values. In order to perform these simplifications, we view
the series as specializations of generating series. For these generating
series, we derive integral representations in terms of root-valued iterated
integrals or directly in terms of cyclotomic harmonic polylogarithms. Using
substitutions, we express the root-valued iterated integrals as cyclotomic
harmonic polylogarithms. Finally, by applying known relations among the
cyclotomic harmonic polylogarithms, we derive expressions in terms of several
constants. The methods are implemented in the computer algebra package
HarmonicSums.
"
1288,Quadratic Probabilistic Algorithms for Normal Bases,"  It is well known that for any finite Galois extension field $K/F$, with
Galois group $G = \mathrm{Gal}(K/F)$, there exists an element $\alpha \in K$
whose orbit $G\cdot\alpha$ forms an $F$-basis of $K$. Such an element $\alpha$
is called \emph{normal} and $G\cdot\alpha$ is called a normal basis. In this
paper we introduce a probabilistic algorithm for finding a normal element when
$G$ is either a finite abelian or a metacyclic group. The algorithm is based on
the fact that deciding whether a random element $\alpha \in K$ is normal can be
reduced to deciding whether $\sum_{\sigma \in G} \sigma(\alpha)\sigma \in K[G]$
is invertible. In an algebraic model, the cost of our algorithm is quadratic in
the size of $G$ for metacyclic $G$ and slightly subquadratic for abelian $G$.
"
1289,"Recursive Matrix Algorithms in Commutative Domain for Cluster with
  Distributed Memory","  We give an overview of the theoretical results for matrix block-recursive
algorithms in commutative domains and present the results of experiments that
we conducted with new parallel programs based on these algorithms on a
supercomputer MVS-10P at the Joint Supercomputer Center of the Russian Academy
of Science. To demonstrate a scalability of these programs we measure the
running time of the program for a different number of processors and plot the
graphs of efficiency factor. Also we present the main application areas in
which such parallel algorithms are used. It is concluded that this class of
algorithms allows to obtain efficient parallel programs on clusters with
distributed memory.
"
1290,Minimizing polynomial functions on quantum computers,"  This expository paper reviews some of the recent uses of computational
algebraic geometry in classical and quantum optimization. The paper assumes an
elementary background in algebraic geometry and adiabatic quantum computing
(AQC), and concentrates on presenting concrete examples (with Python codes
tested on a quantum computer) of applying algebraic geometry constructs:
solving binary optimization, factoring, and compiling. Reversing the direction,
we also briefly describe a novel use of quantum computers to compute Groebner
bases for toric ideals. We also show how Groebner bases play a role in studying
AQC at a fundamental level within a Morse theory framework. We close by placing
our work in perspective, by situating this leg of the journey, as part of a
marvelous intellectual expedition that began with our ancients over 4000 years
ago.
"
1291,On some classes of irreducible polynomials,"  The aim of the paper is to produce new families of irreducible polynomials,
generalizing previous results in the area. One example of our general result is
that for a near-separated polynomial, i.e., polynomials of the form
$F(x,y)=f_1(x)f_2(y)-f_2(x)f_1(y)$, then $F(x,y)+r$ is always irreducible for
any constant $r$ different from zero. We also provide the biggest known family
of HIP polynomials in several variables. These are polynomials
$p(x_1,\ldots,x_n) \in K[x_1,\ldots,x_n]$ over a zero characteristic field $K$
such that $p(h_1(x_1),\ldots,h_n(x_n))$ is irreducible over $K$ for every
$n$-tuple $h_1(x_1),\ldots,h_n(x_n)$ of non constant one variable polynomials
over $K$. The results can also be applied to fields of positive characteristic,
with some modifications.
"
1292,Cylindrical Algebraic Decomposition with Equational Constraints,"  Cylindrical Algebraic Decomposition (CAD) has long been one of the most
important algorithms within Symbolic Computation, as a tool to perform
quantifier elimination in first order logic over the reals. More recently it is
finding prominence in the Satisfiability Checking community as a tool to
identify satisfying solutions of problems in nonlinear real arithmetic.
  The original algorithm produces decompositions according to the signs of
polynomials, when what is usually required is a decomposition according to the
truth of a formula containing those polynomials. One approach to achieve that
coarser (but hopefully cheaper) decomposition is to reduce the polynomials
identified in the CAD to reflect a logical structure which reduces the solution
space dimension: the presence of Equational Constraints (ECs).
  This paper may act as a tutorial for the use of CAD with ECs: we describe all
necessary background and the current state of the art. In particular, we
present recent work on how McCallum's theory of reduced projection may be
leveraged to make further savings in the lifting phase: both to the polynomials
we lift with and the cells lifted over. We give a new complexity analysis to
demonstrate that the double exponent in the worst case complexity bound for CAD
reduces in line with the number of ECs. We show that the reduction can apply to
both the number of polynomials produced and their degree.
"
1293,"Computation of the Expected Euler Characteristic for the Largest
  Eigenvalue of a Real Non-central Wishart Matrix","  We give an approximate formula for the distribution of the largest eigenvalue
of real Wishart matrices by the expected Euler characteristic method for the
general dimension. The formula is expressed in terms of a definite integral
with parameters. We derive a differential equation satisfied by the integral
for the $2 \times 2$ matrix case and perform a numerical analysis of it.
"
1294,Local Search for Fast Matrix Multiplication,"  Laderman discovered a scheme for computing the product of two 3x3 matrices
using only 23 multiplications in 1976. Since then, some more such schemes were
proposed, but it remains open how many there are and whether there exist
schemes with fewer than 23 multiplications. In this paper we present two
independent SAT-based methods for finding new schemes. Both methods allow
computing a few hundred new schemes individually, and many thousands when
combined. Local search SAT solvers outperform CDCL solvers consistently in this
application.
"
1295,Testing zero-dimensionality of varieties at a point,"  Effective methods are introduced for testing zero-dimensionality of varieties
at a point. The motivation of this paper is to compute and analyze deformations
of isolated hypersurface singularities. As an application, methods for
computing local dimensions are also described. For the case where a given ideal
contains parameters, the proposed algorithms can output in particular a
decomposition of a parameter space into strata according to the local dimension
at a point of the associated varieties. The key of the proposed algorithms is
the use of the notion of comprehensive Gr\""obner systems.
"
1296,Computing huge Groebner basis like cyclic10 over $\Q$ with Giac,"  We present a short description on how to fine-tune the modular algorithm
implemented in the Giac computer algebra system to reconstruct huge Groebner
basis over $\Q$.The classical cyclic10 benchmark will serve as example.
"
1297,Reconstructing Rational Functions with $\texttt{FireFly}$,"  We present the open-source $\texttt{C++}$ library $\texttt{FireFly}$ for the
reconstruction of multivariate rational functions over finite fields. We
discuss the involved algorithms and their implementation. As an application, we
use $\texttt{FireFly}$ in the context of integration-by-parts reductions and
compare runtime and memory consumption to a fully algebraic approach with the
program $\texttt{Kira}$.
"
1298,Exact Lower Bounds for Monochromatic Schur Triples and Generalizations,"  We derive exact and sharp lower bounds for the number of monochromatic
generalized Schur triples $(x,y,x+ay)$ whose entries are from the set
$\{1,\dots,n\}$, subject to a coloring with two different colors. Previously,
only asymptotic formulas for such bounds were known, and only for
$a\in\mathbb{N}$. Using symbolic computation techniques, these results are
extended here to arbitrary $a\in\mathbb{R}$. Furthermore, we give exact
formulas for the minimum number of monochromatic Schur triples for $a=1,2,3,4$,
and briefly discuss the case $0<a<1$.
"
1299,Tropical Differential Groebner Basis,"  In this paper, the tropical differential Gr\""obner basis is studied, which is
a natural generalization of the tropical Gr\""obner basis to the recently
introduced tropical differential algebra. Like the differential Gr\""obner
basis, the tropical differential Gr\""obner basis generally contains an infinite
number of elements. We give a Buchberger style criterion for the tropical
differential Gr\""obner basis. For ideals generated by homogeneous linear
differential polynomials with constant coefficients, we give a complete
algorithm to compute the tropical differential Gr\""obner basis.
"
1300,"On the Equivalence of Forward Mode Automatic Differentiation and
  Symbolic Differentiation","  We show that forward mode automatic differentiation and symbolic
differentiation are equivalent in the sense that they both perform the same
operations when computing derivatives. This is in stark contrast to the common
claim that they are substantially different. The difference is often
illustrated by claiming that symbolic differentiation suffers from ""expression
swell"" whereas automatic differentiation does not. Here, we show that this
statement is not true. ""Expression swell"" refers to the phenomenon of a much
larger representation of the derivative as opposed to the representation of the
original function.
"
1301,"Proceedings Joint International Workshop on Linearity & Trends in Linear
  Logic and Applications","  This volume contains a selection of papers presented at Linearity/TLLA 2018:
Joint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018
in Oxford. Linearity has been a key feature in several lines of research in
both theoretical and practical approaches to computer science. On the
theoretical side there is much work stemming from linear logic dealing with
proof technology, complexity classes and more recently quantum computation. On
the practical side there is work on program analysis, expressive operational
semantics for programming languages, linear programming languages, program
transformation, update analysis and efficient implementation techniques. Linear
logic is not only a theoretical tool to analyse the use of resources in logic
and computation. It is also a corpus of tools, approaches, and methodologies
(proof nets, exponential decomposition, geometry of interaction, coherent
spaces, relational models, etc.) that were originally developed for the study
of linear logic's syntax and semantics and are nowadays applied in several
other fields.
"
1302,Asymptotic Solutions of Polynomial Equations with Exp-Log Coefficients,"  We present an algorithm for computing asymptotic approximations of roots of
polynomials with exp-log function coefficients. The real and imaginary parts of
the approximations are given as explicit exp-log expressions. We provide a
method for deciding which approximations correspond to real roots. We report on
implementation of the algorithm and present empirical data.
"
1303,"Unification and combination of iterative insertion strategies with
  one-step traversals","  Motivated by an ongoing project on the computer aided derivation of
multiscale partial differential equation models, we introduce a class of term
transformations that consists in navigation strategies and insertion of
contexts. We define a unification and combination operations on this class
which enjoy nice algebraic properties like associativity, congruence, and the
existence of a neutral and an absorbing element. The main part of this paper is
devoted to proving that the unification and combination operations are correct.
"
1304,Improved algorithms for left factorial residues,"  We present improved algorithms for computing the left factorial residues
$!p=0!+1!+...+(p-1)! \!\mod p$. We use these algorithms for the calculation of
the residues $!p\!\mod p$, for all primes $p$ up to $2^{40}$. Our results
confirm that Kurepa's left factorial conjecture is still an open problem, as
they show that there are no odd primes $p<2^{40}$ such that $p$ divides $!p$.
Additionally, we confirm that there are no socialist primes $p$ with
$5<p<2^{40}$.
"
1305,"$\mathtt{bimEX}$: A Mathematica package for exact computations in $3+1$
  bimetric relativity","  We present $\mathtt{bimEX}$, a Mathematica package for exact computations in
3$+$1 bimetric relativity. It is based on the $\mathtt{xAct}$ bundle, which can
handle computations involving both abstract tensors and their components. In
this communication, we refer to the latter case as concrete computations. The
package consists of two main parts. The first part involves the abstract
tensors, and focuses on how to deal with multiple metrics in $\mathtt{xAct}$.
The second part takes an ansatz for the primary variables in a chart as the
input, and returns the covariant BSSN bimetric equations in components in that
chart. Several functions are implemented to make this process as fast and
user-friendly as possible. The package has been used and tested extensively in
spherical symmetry and was the workhorse in obtaining the bimetric covariant
BSSN equations and reproducing the bimetric 3$+$1 equations in the spherical
polar chart.
"
1306,"Unification and combination of iterative insertion strategies with
  rudimentary traversals and failure","  We introduce a new class of extensions of terms that consists in navigation
strategies and insertion of contexts. We introduce an operation of combination
on this class which is associative, admits a neutral element and so that each
extension is idempotent. The class of extension is also shown to be closed by
combination, with a constructive proof. This new framework is general and
independent of any application semantics. However it has been introduced for
the kernel of a software tool which aims at aiding derivation of multiscale
partial differential equation models.
"
1307,"Comparing machine learning models to choose the variable ordering for
  cylindrical algebraic decomposition","  There has been recent interest in the use of machine learning (ML) approaches
within mathematical software to make choices that impact on the computing
performance without affecting the mathematical correctness of the result. We
address the problem of selecting the variable ordering for cylindrical
algebraic decomposition (CAD), an important algorithm in Symbolic Computation.
Prior work to apply ML on this problem implemented a Support Vector Machine
(SVM) to select between three existing human-made heuristics, which did better
than anyone heuristic alone. The present work extends to have ML select the
variable ordering directly, and to try a wider variety of ML techniques.
  We experimented with the NLSAT dataset and the Regular Chains Library CAD
function for Maple 2018. For each problem, the variable ordering leading to the
shortest computing time was selected as the target class for ML. Features were
generated from the polynomial input and used to train the following ML models:
k-nearest neighbours (KNN) classifier, multi-layer perceptron (MLP), decision
tree (DT) and SVM, as implemented in the Python scikit-learn package. We also
compared these with the two leading human constructed heuristics for the
problem: Brown's heuristic and sotd. On this dataset all of the ML approaches
outperformed the human made heuristics, some by a large margin.
"
1308,"Constructing minimal telescopers for rational functions in three
  discrete variables","  We present a new algorithm for constructing minimal telescopers for rational
functions in three discrete variables. This is the first discrete
reduction-based algorithm that goes beyond the bivariate case. The termination
of the algorithm is guaranteed by a known existence criterion of telescopers.
Our approach has the important feature that it avoids the potentially costly
computation of certificates. Computational experiments are also provided so as
to illustrate the efficiency of our approach.
"
1309,Computing the volume of compact semi-algebraic sets,"  Let $S\subset R^n$ be a compact basic semi-algebraic set defined as the real
solution set of multivariate polynomial inequalities with rational
coefficients. We design an algorithm which takes as input a polynomial system
defining $S$ and an integer $p\geq 0$ and returns the $n$-dimensional volume of
$S$ at absolute precision $2^{-p}$.Our algorithm relies on the relationship
between volumes of semi-algebraic sets and periods of rational integrals. It
makes use of algorithms computing the Picard-Fuchs differential equation of
appropriate periods, properties of critical points, and high-precision
numerical integration of differential equations.The algorithm runs in
essentially linear time with respect to~$p$. This improves upon the previous
exponential bounds obtained by Monte-Carlo or moment-based methods. Assuming a
conjecture of Dimca, the arithmetic cost of the algebraic subroutines for
computing Picard-Fuchs equations and critical points is singly exponential in
$n$ and polynomial in the maximum degree of the input.
"
1310,"Algorithmic approach to strong consistency analysis of finite difference
  approximations to PDE systems","  For a wide class of polynomially nonlinear systems of partial differential
equations we suggest an algorithmic approach to the s(trong)-consistency
analysis of their finite difference approximations on Cartesian grids. First we
apply the differential Thomas decomposition to the input system, resulting in a
partition of the solution set. We consider the output simple subsystem that
contains a solution of interest. Then, for this subsystem, we suggest an
algorithm for verification of s-consistency for its finite difference
approximation. For this purpose we develop a difference analogue of the
differential Thomas decomposition, both of which jointly allow to verify the
s-consistency of the approximation. As an application of our approach, we show
how to produce s-consistent difference approximations to the incompressible
Navier-Stokes equations including the pressure Poisson equation.
"
1311,Derandomization from Algebraic Hardness,"  A hitting-set generator (HSG) is a polynomial map $G:\mathbb{F}^k \to
\mathbb{F}^n$ such that for all $n$-variate polynomials $C$ of small enough
circuit size and degree, if $C$ is nonzero, then $C\circ G$ is nonzero. In this
paper, we give a new construction of such an HSG assuming that we have an
explicit polynomial of sufficient hardness. Formally, we prove the following
over any field of characteristic zero:
  Let $k\in \mathbb{N}$ and $\delta > 0$ be arbitrary constants. Suppose
$\{P_d\}_{d\in \mathbb{N}}$ is an explicit family of $k$-variate polynomials
such that $\operatorname{deg} P_d = d$ and $P_d$ requires algebraic circuits of
size $d^\delta$. Then, there are explicit hitting sets of polynomial size for
$\mathsf{VP}$.
  This is the first HSG in the algebraic setting that yields a complete
derandomization of polynomial identity testing (PIT) for general circuits from
a suitable algebraic hardness assumption. As a direct consequence, we show that
even saving a single point from the ""trivial"" explicit, exponential sized
hitting sets for constant-variate polynomials of low individual degree which
are computable by small circuits, implies a deterministic polynomial time
algorithm for PIT. More precisely, we show the following:
  Let $k\in \mathbb{N}$ and $\delta > 0$ be arbitrary constants. Suppose for
every $s$ large enough, there is an explicit hitting set of size at most
$((s+1)^k - 1)$ for the class of $k$-variate polynomials of individual degree
$s$ that are computable by size $s^\delta$ circuits. Then there is an explicit
hitting set of size $\operatorname{poly}(s)$ for the class of $s$-variate
polynomials, of degree $s$, that are computable by size $s$ circuits.
  As a consequence, we give a deterministic polynomial time construction of
hitting sets for algebraic circuits, if a strengthening of the
$\tau$-Conjecture of Shub and Smale is true.
"
1312,The complexity of MinRank,"  In this note, we leverage some of our results from arXiv:1706.06319 to
produce a concise and rigorous proof for the complexity of the generalized
MinRank Problem in the under-defined and well-defined case. Our main theorem
recovers and extends previous results by Faug\`ere, Safey El Din, Spaenlehauer
(arXiv:1112.4411).
"
1313,The strong approximation theorem and computing with linear groups,"  We obtain a computational realization of the strong approximation theorem.
That is, we develop algorithms to compute all congruence quotients modulo
rational primes of a finitely generated Zariski dense group $H \leq
\mathrm{SL}(n, \mathbb{Z})$ for $n \geq 2$. More generally, we are able to
compute all congruence quotients of a finitely generated Zariski dense subgroup
of $\mathrm{SL}(n, \mathbb{Q})$ for $n > 2$.
"
1314,Automatic Generation of Moment-Based Invariants for Prob-Solvable Loops,"  One of the main challenges in the analysis of probabilistic programs is to
compute invariant properties that summarise loop behaviours. Automation of
invariant generation is still at its infancy and most of the times targets only
expected values of the program variables, which is insufficient to recover the
full probabilistic program behaviour. We present a method to automatically
generate moment-based invariants of a subclass of probabilistic programs,
called Prob-Solvable loops, with polynomial assignments over random variables
and parametrised distributions. We combine methods from symbolic summation and
statistics to derive invariants as valid properties over higher-order moments,
such as expected values or variances, of program variables. We successfully
evaluated our work on several examples where full automation for computing
higher-order moments and invariants over program variables was not yet
possible.
"
1315,"An Algorithmic Approach to Limit Cycles of Nonlinear Differential
  Systems: the Averaging Method Revisited","  This paper introduces an algorithmic approach to the analysis of bifurcation
of limit cycles from the centers of nonlinear continuous differential systems
via the averaging method. We develop three algorithms to implement the
averaging method. The first algorithm allows to transform the considered
differential systems to the normal formal of averaging. Here, we restricted the
unperturbed term of the normal form of averaging to be identically zero. The
second algorithm is used to derive the computational formulae of the averaged
functions at any order. The third algorithm is based on the first two
algorithms that determines the exact expressions of the averaged functions for
the considered differential systems. The proposed approach is implemented in
Maple and its effectiveness is shown by several examples. Moreover, we report
some incorrect results in published papers on the averaging method.
"
1316,Asymptotics of multivariate sequences in the presence of a lacuna,"  We explain a discontinuous drop in the exponential growth rate for certain
multivariate generating functions at a critical parameter value, in even
dimensions $d \geq 4$. This result depends on computations in the homology of
the algebraic variety where the generating function has a pole. These
computations are similar to, and inspired by, a thread of research in
applications of complex algebraic geometry to hyperbolic PDEs, going back to
Leray, Petrowski, Atiyah, Bott and G\""arding. As a consequence, we give a
topological explanation for certain asymptotic phenomenon appearing in the
combinatorics and number theory literature. Furthermore, we show how to combine
topological methods with symbolic algebraic computation to determine explicitly
the dominant asymptotics for such multivariate generating functions. This in
turn enables the rigorous determination of integer coefficients in the
Morse-Smale complex, which are difficult to determine using direct geometric
methods.
"
1317,"Effective Coefficient Asymptotics of Multivariate Rational Functions via
  Semi-Numerical Algorithms for Polynomial Systems","  The coefficient sequences of multivariate rational functions appear in many
areas of combinatorics. Their diagonal coefficient sequences enjoy nice
arithmetic and asymptotic properties, and the field of analytic combinatorics
in several variables (ACSV) makes it possible to compute asymptotic expansions.
We consider these methods from the point of view of effectivity. In particular,
given a rational function, ACSV requires one to determine a (generically)
finite collection of points that are called critical and minimal. Criticality
is an algebraic condition, meaning it is well treated by classical methods in
computer algebra, while minimality is a semi-algebraic condition describing
points on the boundary of the domain of convergence of a multivariate power
series. We show how to obtain dominant asymptotics for the diagonal coefficient
sequence of multivariate rational functions under some genericity assumptions
using symbolic-numeric techniques. To our knowledge, this is the first
completely automatic treatment and complexity analysis for the asymptotic
enumeration of rational functions in an arbitrary number of variables.
"
1318,Integrality and arithmeticity of solvable linear groups,"  We develop a practical algorithm to decide whether a finitely generated
subgroup of a solvable algebraic group $G$ is arithmetic. This incorporates a
procedure to compute a generating set of an arithmetic subgroup of $G$. We also
provide a simple new algorithm for integrality testing of finitely generated
solvable-by-finite linear groups over the rational field. The algorithms have
been implemented in {\sc Magma}.
"
1319,"Implementations of efficient univariate polynomial matrix algorithms and
  application to bivariate resultants","  Complexity bounds for many problems on matrices with univariate polynomial
entries have been improved in the last few years. Still, for most related
algorithms, efficient implementations are not available, which leaves open the
question of the practical impact of these algorithms, e.g. on applications such
as decoding some error-correcting codes and solving polynomial systems or
structured linear systems. In this paper, we discuss implementation aspects for
most fundamental operations: multiplication, truncated inversion, approximants,
interpolants, kernels, linear system solving, determinant, and basis reduction.
We focus on prime fields with a word-size modulus, relying on Shoup's C++
library NTL. Combining these new tools to implement variants of Villard's
algorithm for the resultant of generic bivariate polynomials (ISSAC 2018), we
get better performance than the state of the art for large parameters.
"
1320,Change of basis for m-primary ideals in one and two variables,"  Following recent work by van der Hoeven and Lecerf (ISSAC 2017), we discuss
the complexity of linear mappings, called untangling and tangling by those
authors, that arise in the context of computations with univariate polynomials.
We give a slightly faster tangling algorithm and discuss new applications of
these techniques. We show how to extend these ideas to bivariate settings, and
use them to give bounds on the arithmetic complexity of certain algebras.
"
1321,Stationary points at infinity for analytic combinatorics,"  On complex algebraic varieties, height functions arising in combinatorial
applications fail to be proper. This complicates the description and
computation via Morse theory of key topological invariants. Here we establish
checkable conditions under which the behavior at infinity may be ignored, and
the usual theorems of classical and stratified Morse theory may be applied.
This allows for simplified arguments in the field of analytic combinatorics in
several variables, and forms the basis for new methods applying to problems
beyond the reach of previous techniques.
"
1322,Effects Without Monads: Non-determinism -- Back to the Meta Language,"  We reflect on programming with complicated effects, recalling an
undeservingly forgotten alternative to monadic programming and checking to see
how well it can actually work in modern functional languages. We adopt and
argue the position of factoring an effectful program into a first-order
effectful DSL with a rich, higher-order 'macro' system. Not all programs can be
thus factored. Although the approach is not general-purpose, it does admit
interesting programs. The effectful DSL is likewise rather problem-specific and
lacks general-purpose monadic composition, or even functions. On the upside, it
expresses the problem elegantly, is simple to implement and reason about, and
lends itself to non-standard interpretations such as code generation
(compilation) and abstract interpretation. A specialized DSL is liable to be
frequently extended; the experience with the tagless-final style of DSL
embedding shown that the DSL evolution can be made painless, with the maximum
code reuse. We illustrate the argument on a simple but representative example
of a rather complicated effect -- non-determinism, including committed choice.
Unexpectedly, it turns out we can write interesting non-deterministic programs
in an ML-like language just as naturally and elegantly as in the
functional-logic language Curry -- and not only run them but also statically
analyze, optimize and compile. The richness of the Meta Language does, in
reality, compensate for the simplicity of the effectful DSL. The key idea goes
back to the origins of ML as the Meta Language for the Edinburgh LCF theorem
prover. Instead of using ML to build theorems, we now build (DSL) programs.
"
1323,Fairness in Machine Learning with Tractable Models,"  Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.
"
1324,"A Correctness Result for Synthesizing Plans With Loops in Stochastic
  Domains","  Finite-state controllers (FSCs), such as plans with loops, are powerful and
compact representations of action selection widely used in robotics, video
games and logistics. There has been steady progress on synthesizing FSCs in
deterministic environments, but the algorithmic machinery needed for lifting
such techniques to stochastic environments is not yet fully understood. While
the derivation of FSCs has received some attention in the context of discounted
expected reward measures, they are often solved approximately and/or without
correctness guarantees. In essence, that makes it difficult to analyze
fundamental concerns such as: do all paths terminate, and do the majority of
paths reach a goal state?
  In this paper, we present new theoretical results on a generic technique for
synthesizing FSCs in stochastic environments, allowing for highly granular
specifications on termination and goal satisfaction.
"
1325,Computing symmetric determinantal representations,"  We introduce the DeterminantalRepresentations package for Macaulay2, which
computes definite symmetric determinantal representations of real polynomials.
We focus on quadrics and plane curves of low degree (i.e. cubics and quartics).
Our algorithms are geared towards speed and robustness, employing linear
algebra and numerical algebraic geometry, without genericity assumptions on the
polynomials.
"
1326,"FiniteFlow: multivariate functional reconstruction using finite fields
  and dataflow graphs","  Complex algebraic calculations can be performed by reconstructing analytic
results from numerical evaluations over finite fields. We describe FiniteFlow,
a framework for defining and executing numerical algorithms over finite fields
and reconstructing multivariate rational functions. The framework employs
computational graphs, known as dataflow graphs, to combine basic building
blocks into complex algorithms. This allows to easily implement a wide range of
methods over finite fields in high-level languages and computer algebra
systems, without being concerned with the low-level details of the numerical
implementation. This approach sidesteps the appearance of large intermediate
expressions and can be massively parallelized. We present applications to the
calculation of multi-loop scattering amplitudes, including the reduction via
integration-by-parts identities to master integrals or special functions, the
computation of differential equations for Feynman integrals, multi-loop
integrand reduction, the decomposition of amplitudes into form factors, and the
derivation of integrable symbols from a known alphabet. We also release a
proof-of-concept C++ implementation of this framework, with a high-level
interface in Mathematica.
"
1327,A polynomial approach to the Collatz conjecture,"  The Collatz conjecture is explored using polynomials based on a binary
numeral system. It is shown that the degree of the polynomials, on average,
decreases after a finite number of steps of the Collatz operation, which
provides a weak proof of the conjecture by using induction with respect to the
degree of the polynomials.
"
1328,Lonely Points in Simplices,"  Given a lattice L in Z^m and a subset A of R^m, we say that a point in A is
lonely if it is not equivalent modulo L to another point of A. We are
interested in identifying lonely points for specific choices of L when A is a
dilated standard simplex, and in conditions on L which ensure that the number
of lonely points is unbounded as the simplex dilation goes to infinity.
"
1329,New ways to multiply 3 x 3-matrices,"  It is known since the 1970s that no more than 23 multiplications are required
for computing the product of two 3 x 3-matrices. It is not known whether this
can also be done with fewer multiplications. However, there are several
mutually inequivalent ways of doing the job with 23 multiplications. In this
article, we extend this list considerably by providing more than 13 000 new and
mutually inequivalent schemes for multiplying 3 x 3-matrices using 23
multiplications. Moreover, we show that the set of all these schemes is a
manifold of dimension at least 17.
"
1330,"A closed-form formula for the Kullback-Leibler divergence between Cauchy
  distributions","  We report a closed-form expression for the Kullback-Leibler divergence
between Cauchy distributions which involves the calculation of a novel definite
integral. The formula shows that the Kullback-Leibler divergence between Cauchy
densities is always finite and symmetric.
"
1331,Confluence by Critical Pair Analysis Revisited (Extended Version),"  We present two methods for proving confluence of left-linear term rewrite
systems. One is hot-decreasingness, combining the parallel/development
closedness theorems with rule labelling based on a terminating subsystem. The
other is critical-pair-closing system, allowing to boil down the confluence
problem to confluence of a special subsystem whose duplicating rules are
relatively terminating.
"
1332,Reconstruction of rational ruled surfaces from their silhouettes,"  We provide algorithms to reconstruct rational ruled surfaces in
three-dimensional projective space from the `apparent contour' of a single
projection to the projective plane. We deal with the case of tangent
developables and of general projections to $\mathbb{p}^3$ of rational normal
scrolls. In the first case, we use the fact that every such surface is the
projection of the tangent developable of a rational normal curve, while in the
second we start by reconstructing the rational normal scroll. In both instances
we then reconstruct the correct projection to $\mathbb{p}^3$ of these surfaces
by exploiting the information contained in the singularities of the apparent
contour.
"
1333,Factorizations for a Class of Multivariate Polynomial Matrices,"  Following the works by Lin et al. (Circuits Syst. Signal Process. 20(6):
601-618, 2001) and Liu et al. (Circuits Syst. Signal Process. 30(3): 553-566,
2011), we investigate how to factorize a class of multivariate polynomial
matrices. The main theorem in this paper shows that an $l\times m$ polynomial
matrix admits a factorization with respect to a polynomial if the polynomial
and all the $(l-1)\times (l-1)$ reduced minors of the matrix generate the unit
ideal. This result is a further generalization of previous works, and based on
this, we give an algorithm which can be used to factorize more polonomial
matrices. In addition, an illustrate example is given to show that our main
theorem is non-trivial and valuable.
"
1334,On the Parallelization of Triangular Decomposition of Polynomial Systems,"  We discuss the parallelization of algorithms for solving polynomial systems
symbolically by way of triangular decomposition. Algorithms for solving
polynomial systems combine low-level routines for performing arithmetic
operations on polynomials and high-level procedures which produce the different
components (points, curves, surfaces) of the solution set. The latter
""component-level"" parallelization of triangular decompositions, our focus here,
belongs to the class of dynamic irregular parallel applications. Possible
speedup factors depend on geometrical properties of the solution set (number of
components, their dimensions and degrees); these algorithms do not scale with
the number of processors. In this paper we combine two different concurrency
schemes, the fork-join model and producer-consumer patterns, to better capture
opportunities for component-level parallelization. We report on our
implementation with the publicly available BPAS library. Our experimentation
with 340 systems yields promising results.
"
1335,"Abstract Predicate Entailment over Points-To Heaplets is Syntax
  Recognition","  Abstract predicates are considered in this paper as abstraction technique for
heap-separated configurations, and as genuine Prolog predicates which are
translated straight into a corresponding formal language grammar used as
validation scheme for intermediate heap states. The approach presented is
rule-based because the abstract predicates are rule-based, the parsing
technique can be interpreted as an automated fold/unfold of the corresponding
heap graph.
"
1336,"An Algorithm for Computing Invariant Projectors in Representations of
  Wreath Products","  We describe an algorithm for computing the complete set of primitive
orthogonal idempotents in the centralizer ring of the permutation
representation of a wreath product. This set of idempotents determines the
decomposition of the representation into irreducible components. In the
formalism of quantum mechanics, these idempotents are projection operators into
irreducible invariant subspaces of the Hilbert space of a multipartite quantum
system. The C implementation of the algorithm constructs irreducible
decompositions of high-dimensional representations of wreath products. Examples
of computations are given.
"
1337,Standard Lattices of Compatibly Embedded Finite Fields,"  Lattices of compatibly embedded finite fields are useful in computer algebra
systems for managing many extensions of a finite field $\mathbb{F}_p$ at once.
They can also be used to represent the algebraic closure $\bar{\mathbb{F}}_p$,
and to represent all finite fields in a standard manner. The most well known
constructions are Conway polynomials, and the Bosma-Cannon-Steel framework used
in Magma. In this work, leveraging the theory of the Lenstra-Allombert
isomorphism algorithm, we generalize both at the same time. Compared to Conway
polynomials, our construction defines a much larger set of field extensions
from a small pre-computed table; however it is provably as inefficient as
Conway polynomials if one wants to represent all field extensions, and thus
yields no asymptotic improvement for representing $\bar{\mathbb{F}}_p$.
Compared to Bosma-Cannon-Steel lattices, it is considerably more efficient both
in computation time and storage: all algorithms have at worst quadratic
complexity, and storage is linear in the number of represented field extensions
and their degrees. Our implementation written in C/Flint/Julia/Nemo shows that
our construction in indeed practical.
"
1338,"Algorithmically generating new algebraic features of polynomial systems
  for machine learning","  There are a variety of choices to be made in both computer algebra systems
(CASs) and satisfiability modulo theory (SMT) solvers which can impact
performance without affecting mathematical correctness. Such choices are
candidates for machine learning (ML) approaches, however, there are
difficulties in applying standard ML techniques, such as the efficient
identification of ML features from input data which is typically a polynomial
system. Our focus is selecting the variable ordering for cylindrical algebraic
decomposition (CAD), an important algorithm implemented in several CASs, and
now also SMT-solvers. We created a framework to describe all the previously
identified ML features for the problem and then enumerated all options in this
framework to automatically generation many more features. We validate the
usefulness of these with an experiment which shows that an ML choice for CAD
variable ordering is superior to those made by human created heuristics, and
further improved with these additional features. We expect that this technique
of feature generation could be useful for other choices related to CAD, or even
choices for other algorithms with polynomial systems for input.
"
1339,"New Features in the Second Version of the Cadabra Computer Algebra
  System","  In certain scientific domains, there is a need for tensor operations. To
facilitate tensor computations,computer algebra systems are employed. In our
research, we have been using Cadabra as the main computer algebra system for
several years. Recently, an operable second version of this software was
released. In this version, a number of improvements were made that can be
regarded as revolutionary ones. The most significant improvements are the
implementation of component computations and the change in the ideology of the
Cadabra's software mechanism as compared to the first version. This paper
provides a brief overview of the key improvements in the Cadabra system.
"
1340,Polynomial root clustering and explicit deflation,"  We seek complex roots of a univariate polynomial $P$ with real or complex
coefficients. We address this problem based on recent algorithms that use
subdivision and have a nearly optimal complexity. They are particularly
efficient when only roots in a given Region Of Interest (ROI) are sought. We
propose two improvements for root finders. The first one is applied to
polynomials having only real coefficients; their roots are either real or
appear in complex conjugate pairs. We show how to adapt the subdivision scheme
to focus the computational effort on the imaginary positive part of the ROI. In
our second improvement we deflate $P$ to decrease its degree and the arithmetic
cost of the subdivision.
"
1341,"Neural Variational Inference For Estimating Uncertainty in Knowledge
  Graph Embeddings","  Recent advances in Neural Variational Inference allowed for a renaissance in
latent variable models in a variety of domains involving high-dimensional data.
While traditional variational methods derive an analytical approximation for
the intractable distribution over the latent variables, here we construct an
inference network conditioned on the symbolic representation of entities and
relation types in the Knowledge Graph, to provide the variational
distributions. The new framework results in a highly-scalable method. Under a
Bernoulli sampling framework, we provide an alternative justification for
commonly used techniques in large-scale stochastic variational inference, which
drastically reduce training time at a cost of an additional approximation to
the variational lower bound. We introduce two models from this highly scalable
probabilistic framework, namely the Latent Information and Latent Fact models,
for reasoning over knowledge graph-based representations. Our Latent
Information and Latent Fact models improve upon baseline performance under
certain conditions. We use the learnt embedding variance to estimate predictive
uncertainty during link prediction, and discuss the quality of these learnt
uncertainty estimates. Our source code and datasets are publicly available
online at
https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs.
"
1342,Efficient Graph Rewriting,"  Graph transformation is the rule-based modification of graphs, and is a
discipline dating back to the 1970s. The declarative nature of graph rewriting
rules comes at a cost. In general, to match the left-hand graph of a fixed rule
within a host graph requires polynomial time. To improve matching performance,
D\""orr proposed to equip rules and host graphs with distinguished root nodes.
This model was implemented by Plump and Bak, but unfortunately, is not
invertible. We address this problem by defining rootedness using a partial
function onto a two-point set rather than pointing graphs with root nodes. We
show a new result that the graph class of trees can be recognised by a rooted
GT system in linear time, given an input graph of bounded degree. Finally, we
define a new notion of confluence modulo garbage and non-garbage critical
pairs, showing it is sufficient to require strong joinability of only the
non-garbage critical pairs to establish confluence modulo garbage.
"
1343,Effective problem solving using SAT solvers,"  In this article we demonstrate how to solve a variety of problems and puzzles
using the built-in SAT solver of the computer algebra system Maple. Once the
problems have been encoded into Boolean logic, solutions can be found (or shown
to not exist) automatically, without the need to implement any search
algorithm. In particular, we describe how to solve the $n$-queens problem, how
to generate and solve Sudoku puzzles, how to solve logic puzzles like the
Einstein riddle, how to solve the 15-puzzle, how to solve the maximum clique
problem, and finding Graeco-Latin squares.
"
1344,Neurally-Guided Structure Inference,"  Most structure inference methods either rely on exhaustive search or are
purely data-driven. Exhaustive search robustly infers the structure of
arbitrarily complex data, but it is slow. Data-driven methods allow efficient
inference, but do not generalize when test data have more complex structures
than training data. In this paper, we propose a hybrid inference algorithm, the
Neurally-Guided Structure Inference (NG-SI), keeping the advantages of both
search-based and data-driven methods. The key idea of NG-SI is to use a neural
network to guide the hierarchical, layer-wise search over the compositional
space of structures. We evaluate our algorithm on two representative structure
inference tasks: probabilistic matrix decomposition and symbolic program
parsing. It outperforms data-driven and search-based alternatives on both
tasks.
"
1345,"Semantic Preserving Bijective Mappings for Expressions involving Special
  Functions in Computer Algebra Systems and Document Preparation Systems","  Purpose: Modern mathematicians and scientists of math-related disciplines
often use Document Preparation Systems (DPS) to write and Computer Algebra
Systems (CAS) to calculate mathematical expressions. Usually, they translate
the expressions manually between DPS and CAS. This process is time-consuming
and error-prone. Our goal is to automate this translation. This paper uses
Maple and Mathematica as the CAS, and LaTeX as our DPS.
  Design/methodology/approach: Bruce Miller at the National Institute of
Standards and Technology (NIST) developed a collection of special LaTeX macros
that create links from mathematical symbols to their definitions in the NIST
Digital Library of Mathematical Functions (DLMF). We are using these macros to
perform rule-based translations between the formulae in the DLMF and CAS.
Moreover, we develop software to ease the creation of new rules and to discover
inconsistencies.
  Findings: We created 396 mappings and translated 58.8% of DLMF formulae
(2,405 expressions) successfully between Maple and DLMF. For a significant
percentage, the special function definitions in Maple and the DLMF were
different. Therefore, an atomic symbol in one system maps to a composite
expression in the other system. The translator was also successfully used for
automatic verification of mathematical online compendia and CAS. Our evaluation
techniques discovered two errors in the DLMF and one defect in Maple.
  Originality: This paper introduces the first translation tool for special
functions between LaTeX and CAS. The approach improves error-prone manual
translations and can be used to verify mathematical online compendia and CAS.
"
1346,Solving Polynomial Systems with phcpy,"  The solutions of a system of polynomials in several variables are often
needed, e.g.: in the design of mechanical systems, and in phase-space analyses
of nonlinear biological dynamics. Reliable, accurate, and comprehensive
numerical solutions are available through PHCpack, a FOSS package for solving
polynomial systems with homotopy continuation. This paper explores new
developments in phcpy, a scripting interface for PHCpack, over the past five
years. For instance, phcpy is now available online through a JupyterHub server
featuring Python2, Python3, and SageMath kernels. As small systems are solved
in real-time by phcpy, they are suitable for interactive exploration through
the notebook interface. Meanwhile, phcpy supports GPU parallelization,
improving the speed and quality of solutions to much larger polynomial systems.
From various model design and analysis problems in STEM, certain classes of
polynomial system frequently arise, to which phcpy is well-suited.
"
1347,Absolute root separation,"  The absolute separation of a polynomial is the minimum nonzero difference
between the absolute values of its roots. In the case of polynomials with
integer coefficients, it can be bounded from below in terms of the degree and
the height (the maximum absolute value of the coefficients) of the polynomial.
We improve the known bounds for this problem and related ones. Then we report
on extensive experiments in low degrees, suggesting that the current bounds are
still very pessimistic.
"
1348,Automatic Differentiation for Adjoint Stencil Loops,"  Stencil loops are a common motif in computations including convolutional
neural networks, structured-mesh solvers for partial differential equations,
and image processing. Stencil loops are easy to parallelise, and their fast
execution is aided by compilers, libraries, and domain-specific languages.
Reverse-mode automatic differentiation, also known as algorithmic
differentiation, autodiff, adjoint differentiation, or back-propagation, is
sometimes used to obtain gradients of programs that contain stencil loops.
Unfortunately, conventional automatic differentiation results in a memory
access pattern that is not stencil-like and not easily parallelisable.
  In this paper we present a novel combination of automatic differentiation and
loop transformations that preserves the structure and memory access pattern of
stencil loops, while computing fully consistent derivatives. The generated
loops can be parallelised and optimised for performance in the same way and
using the same tools as the original computation. We have implemented this new
technique in the Python tool PerforAD, which we release with this paper along
with test cases derived from seismic imaging and computational fluid dynamics
applications.
"
1349,Solving p-adic polynomial systems via iterative eigenvector algorithms,"  In this article, we describe an implementation of a polynomial system solver
to compute the approximate solutions of a 0-dimensional polynomial system with
finite precision p-adic arithmetic. We also describe an improvement to an
algorithm of Caruso, Roe, and Vaccon for calculating the eigenvalues and
eigenvectors of a p-adic matrix.
"
1350,"Annotary: A Concolic Execution System for Developing Secure Smart
  Contracts","  Ethereum smart contracts are executable programs, deployed on a peer-to-peer
network and executed in a consensus-based fashion. Their bytecode is public,
immutable and once deployed to the blockchain, cannot be patched anymore. As
smart contracts may hold Ether worth of several million dollars, they are
attractive targets for attackers and indeed some contracts have successfully
been exploited in the recent past, resulting in tremendous financial losses.
The correctness of smart contracts is thus of utmost importance. While first
approaches on formal verification exist, they demand users to be well-versed in
formal methods which are alien to many developers and are only able to analyze
individual contracts, without considering their execution environment, i.e.,
calls to external contracts, sequences of transaction, and values from the
actual blockchain storage. In this paper, we present Annotary, a concolic
execution framework to analyze smart contracts for vulnerabilities, supported
by annotations which developers write directly in the Solidity source code. In
contrast to existing work, Annotary supports analysis of inter-transactional,
inter-contract control flows and combines symbolic execution of EVM bytecode
with a resolution of concrete values from the public Ethereum blockchain. While
the analysis of Annotary tends to weight precision higher than soundness, we
analyze inter-transactional call chains to eliminate false positives from
unreachable states that traditional symbolic execution would not be able to
handle. We present the annotation and analysis concepts of Annotary, explain
its implementation on top of the Laser symbolic virtual machine, and
demonstrate its usage as a plugin for the Sublime Text editor.
"
1351,"Proving Properties of Sorting Programs: A Case Study in Horn Clause
  Verification","  The proof of a program property can be reduced to the proof of satisfiability
of a set of constrained Horn clauses (CHCs) which can be automatically
generated from the program and the property. In this paper we have conducted a
case study in Horn clause verification by considering several sorting programs
with the aim of exploring the effectiveness of a transformation technique which
allows us to eliminate inductive data structures such as lists or trees. If
this technique is successful, we derive a set of CHCs with constraints over the
integers and booleans only, and the satisfiability check can often be performed
in an effective way by using state-of-the-art CHC solvers, such as Eldarica or
Z3. In this case study we have also illustrated the usefulness of a companion
technique based on the introduction of the so-called difference predicates,
whose definitions correspond to lemmata required during the verification. We
have considered functional programs which implement the following kinds of
sorting algorithms acting on lists of integers: (i) linearly recursive sorting
algorithms, such as insertion sort and selection sort, and (ii) non-linearly
recursive sorting algorithms, such as quicksort and mergesort, and we have
considered the following properties: (i) the partial correctness properties,
that is, the orderedness of the output lists, and the equality of the input and
output lists when viewed as multisets, and (ii) some arithmetic properties,
such as the equality of the sum of the elements before and after sorting.
"
1352,"SAT Solvers and Computer Algebra Systems: A Powerful Combination for
  Mathematics","  Over the last few decades, many distinct lines of research aimed at
automating mathematics have been developed, including computer algebra systems
(CASs) for mathematical modelling, automated theorem provers for first-order
logic, SAT/SMT solvers aimed at program verification, and higher-order proof
assistants for checking mathematical proofs. More recently, some of these lines
of research have started to converge in complementary ways. One success story
is the combination of SAT solvers and CASs (SAT+CAS) aimed at resolving
mathematical conjectures.
  Many conjectures in pure and applied mathematics are not amenable to
traditional proof methods. Instead, they are best addressed via computational
methods that involve very large combinatorial search spaces. SAT solvers are
powerful methods to search through such large combinatorial
spaces---consequently, many problems from a variety of mathematical domains
have been reduced to SAT in an attempt to resolve them. However, solvers
traditionally lack deep repositories of mathematical domain knowledge that can
be crucial to pruning such large search spaces. By contrast, CASs are deep
repositories of mathematical knowledge but lack efficient general search
capabilities. By combining the search power of SAT with the deep mathematical
knowledge in CASs we can solve many problems in mathematics that no other known
methods seem capable of solving.
  We demonstrate the success of the SAT+CAS paradigm by highlighting many
conjectures that have been disproven, verified, or partially verified using our
tool MathCheck. These successes indicate that the paradigm is positioned to
become a standard method for solving problems requiring both a significant
amount of search and deep mathematical reasoning. For example, the SAT+CAS
paradigm has recently been used by Heule, Kauers, and Seidl to find many new
algorithms for $3\times3$ matrix multiplication.
"
1353,"Improved Structural Methods for Nonlinear Differential-Algebraic
  Equations via Combinatorial Relaxation","  Differential-algebraic equations (DAEs) are widely used for modeling of
dynamical systems. In numerical analysis of DAEs, consistent initialization and
index reduction are important preprocessing prior to numerical integration.
Existing DAE solvers commonly adopt structural preprocessing methods based on
combinatorial optimization. Unfortunately, the structural methods fail if the
DAE has numerical or symbolic cancellations. For such DAEs, methods have been
proposed to modify them to other DAEs to which the structural methods are
applicable, based on the combinatorial relaxation technique. Existing
modification methods, however, work only for a class of DAEs that are linear or
close to linear.
  This paper presents two new modification methods for nonlinear DAEs: the
substitution method and the augmentation method. Both methods are based on the
combinatorial relaxation approach and are applicable to a large class of
nonlinear DAEs. The substitution method symbolically solves equations for some
derivatives based on the implicit function theorem and substitutes the solution
back into the system. Instead of solving equations, the augmentation method
modifies DAEs by appending new variables and equations. The augmentation method
has advantages that the equation solving is not needed and the sparsity of DAEs
is retained. It is shown in numerical experiments that both methods, especially
the augmentation method, successfully modify high-index DAEs that the DAE
solver in MATLAB cannot handle.
"
1354,Computing the Maximum Degree of Minors in Skew Polynomial Matrices,"  Skew polynomials, which have a noncommutative multiplication rule between
coefficients and an indeterminate, are the most general polynomial concept that
admits the degree function with desirable properties. This paper presents the
first algorithms to compute the maximum degree of the Dieudonn\'e determinant
of a $k \times k$ submatrix in a matrix $A$ whose entries are skew polynomials
over a skew field $F$. Our algorithms make use of the discrete Legendre
conjugacy between the sequences of the maximum degrees and the ranks of block
matrices over $F$ obtained from coefficient matrices of $A$. Three applications
of our algorithms are provided: (i) computing the dimension of the solution
spaces of linear differential and difference equations, (ii) determining the
Smith-McMillan form of transfer function matrices of linear time-varying
systems and (iii) solving the ""weighted"" version of noncommutative Edmonds'
problem with polynomial bit complexity. We also show that the deg-det
computation for matrices over sparse polynomials is at least as hard as solving
commutative Edmonds' problem.
"
1355,"The SAT+CAS Method for Combinatorial Search with Applications to Best
  Matrices","  In this paper, we provide an overview of the SAT+CAS method that combines
satisfiability checkers (SAT solvers) and computer algebra systems (CAS) to
resolve combinatorial conjectures, and present new results vis-\`a-vis best
matrices. The SAT+CAS method is a variant of the
Davis$\unicode{8211}$Putnam$\unicode{8211}$Logemann$\unicode{8211}$Loveland
$\operatorname{DPLL}(T)$ architecture, where the $T$ solver is replaced by a
CAS. We describe how the SAT+CAS method has been previously used to resolve
many open problems from graph theory, combinatorial design theory, and number
theory, showing that the method has broad applications across a variety of
fields. Additionally, we apply the method to construct the largest best
matrices yet known and present new skew Hadamard matrices constructed from best
matrices. We show the best matrix conjecture (that best matrices exist in all
orders of the form $r^2+r+1$) which was previously known to hold for $r\leq6$
also holds for $r=7$. We also confirmed the results of the exhaustive searches
that have been previously completed for $r\leq6$.
"
1356,"Topological rewriting systems applied to standard bases and syntactic
  algebras","  We propose a functional description of rewriting systems on topological
vector spaces. We introduce the topological confluence property as an
approximation of the confluence property. Using a representation of linear
topological rewriting systems with continuous reduction operators, we show that
the topological confluence is characterised by lattice operations. We relate
these operations to standard bases and show that the latter induce
topologically confluent rewriting systems on formal power series. Finally, we
investigate duality for reduction operators that we relate to series
representations and syntactic algebras. In particular, we use duality for
proving that an algebra is syntactic or not.
"
1357,"Logic Conditionals, Supervenience, and Selection Tasks","  Principles of cognitive economy would require that concepts about objects,
properties and relations should be introduced only if they simplify the
conceptualisation of a domain. Unexpectedly, classic logic conditionals,
specifying structures holding within elements of a formal conceptualisation, do
not always satisfy this crucial principle. The paper argues that this
requirement is captured by supervenience, hereby further identified as a
property necessary for compression. The resulting theory suggests an
alternative explanation of the empirical experiences observable in Wason's
selection tasks, associating human performance with conditionals on the ability
of dealing with compression, rather than with logic necessity.
"
1358,Formal verification of trading in financial markets,"  We introduce a formal framework for analyzing trades in financial markets. An
exchange is where multiple buyers and sellers participate to trade. These days,
all big exchanges use computer algorithms that implement double sided auctions
to match buy and sell requests and these algorithms must abide by certain
regulatory guidelines. For example, market regulators enforce that a matching
produced by exchanges should be \emph{fair}, \emph{uniform} and
\emph{individual rational}. To verify these properties of trades, we first
formally define these notions in a theorem prover and then give formal proofs
of relevant results on matchings. Finally, we use this framework to verify
properties of two important classes of double sided auctions. All the
definitions and results presented in this paper are completely formalised in
the Coq proof assistant without adding any additional axioms to it.
"
1359,A Unified Algebraic Framework for Non-Monotonicity,"  Tremendous research effort has been dedicated over the years to thoroughly
investigate non-monotonic reasoning. With the abundance of non-monotonic
logical formalisms, a unified theory that enables comparing the different
approaches is much called for. In this paper, we present an algebraic graded
logic we refer to as LogAG capable of encompassing a wide variety of
non-monotonic formalisms. We build on Lin and Shoham's argument systems first
developed to formalize non-monotonic commonsense reasoning. We show how to
encode argument systems as LogAG theories, and prove that LogAG captures the
notion of belief spaces in argument systems. Since argument systems capture
default logic, autoepistemic logic, the principle of negation as failure, and
circumscription, our results show that LogAG captures the before-mentioned
non-monotonic logical formalisms as well. Previous results show that LogAG
subsumes possibilistic logic and any non-monotonic inference relation
satisfying Makinson's rationality postulates. In this way, LogAG provides a
powerful unified framework for non-monotonicity.
"
1360,Polynomial Reduction and Super Congruences,"  Based on a reduction processing, we rewrite a hypergeometric term as the sum
of the difference of a hypergeometric term and a reduced hypergeometric term
(the reduced part, in short). We show that when the initial hypergeometric term
has a certain kind of symmetry, the reduced part contains only odd or even
powers. As applications, we derived two infinite families of super-congruences.
"
1361,"ART: Abstraction Refinement-Guided Training for Provably Correct Neural
  Networks","  Artificial Neural Networks (ANNs) have demonstrated remarkable utility in
various challenging machine learning applications. While formally verified
properties of their behaviors are highly desired, they have proven notoriously
difficult to derive and enforce. Existing approaches typically formulate this
problem as a post facto analysis process. In this paper, we present a novel
learning framework that ensures such formal guarantees are enforced by
construction. Our technique enables training provably correct networks with
respect to a broad class of safety properties, a capability that goes
well-beyond existing approaches, without compromising much accuracy. Our key
insight is that we can integrate an optimization-based abstraction refinement
loop into the learning process and operate over dynamically constructed
partitions of the input space that considers accuracy and safety objectives
synergistically. The refinement procedure iteratively splits the input space
from which training data is drawn, guided by the efficacy with which such
partitions enable safety verification. We have implemented our approach in a
tool (ART) and applied it to enforce general safety properties on unmanned
aviator collision avoidance system ACAS Xu dataset and the Collision Detection
dataset. Importantly, we empirically demonstrate that realizing safety does not
come at the price of much accuracy. Our methodology demonstrates that an
abstraction refinement methodology provides a meaningful pathway for building
both accurate and correct machine learning networks.
"
1362,Upper Hessenberg and Toeplitz Bohemians,"  We look at Bohemians, specifically those with population $\{-1, 0, {+1}\}$
and sometimes $\{0,1,i,-1,-i\}$. More, we specialize the matrices to be upper
Hessenberg Bohemian. From there, focusing on only those matrices whose
characteristic polynomials have maximal height allows us to explicitly identify
these polynomials and give useful bounds on their height, and conjecture an
accurate asymptotic formula. The lower bound for the maximal characteristic
height is exponential in the order of the matrix; in contrast, the height of
the matrices remains constant. We give theorems about the numbers of normal
matrices and the numbers of stable matrices in these families.
"
1363,Extensional Higher-Order Paramodulation in Leo-III,"  Leo-III is an automated theorem prover for extensional type theory with
Henkin semantics and choice. Reasoning with primitive equality is enabled by
adapting paramodulation-based proof search to higher-order logic. The prover
may cooperate with multiple external specialist reasoning systems such as
first-order provers and SMT solvers. Leo-III is compatible with the TPTP/TSTP
framework for input formats, reporting results and proofs, and standardized
communication between reasoning systems, enabling e.g. proof reconstruction
from within proof assistants such as Isabelle/HOL. Leo-III supports reasoning
in polymorphic first-order and higher-order logic, in all normal quantified
modal logics, as well as in different deontic logics. Its development had
initiated the ongoing extension of the TPTP infrastructure to reasoning within
non-classical logics.
"
1364,"Complex Golay Pairs up to Length 28: A Search via Computer Algebra and
  Programmatic SAT","  We use techniques from the fields of computer algebra and satisfiability
checking to develop a new algorithm to search for complex Golay pairs. We
implement this algorithm and use it to perform a complete search for complex
Golay pairs of lengths up to 28. In doing so, we find that complex Golay pairs
exist in the lengths 24 and 26 but do not exist in the lengths 23, 25, 27, and
28. This independently verifies work done by F. Fiedler in 2013 and confirms
the 2002 conjecture of Craigen, Holzmann, and Kharaghani that complex Golay
pairs of length 23 don't exist. Our algorithm is based on the recently proposed
SAT+CAS paradigm of combining SAT solvers with computer algebra systems to
efficiently search large spaces specified by both algebraic and logical
constraints. The algorithm has two stages: first, a fine-tuned computer program
uses functionality from computer algebra systems and numerical libraries to
construct a list containing every sequence which could appear as the first
sequence in a complex Golay pair up to equivalence. Second, a programmatic SAT
solver constructs every sequence (if any) that pair off with the sequences
constructed in the first stage to form a complex Golay pair. This extends work
originally presented at the International Symposium on Symbolic and Algebraic
Computation (ISSAC) in 2018; we discuss and implement several improvements to
our algorithm that enabled us to improve the efficiency of the search and
increase the maximum length we search from length 25 to 28.
"
1365,"Computing the Characteristic Polynomial of a Finite Rank Two Drinfeld
  Module","  Motivated by finding analogues of elliptic curve point counting techniques,
we introduce one deterministic and two new Monte Carlo randomized algorithms to
compute the characteristic polynomial of a finite rank-two Drinfeld module. We
compare their asymptotic complexity to that of previous algorithms given by
Gekeler, Narayanan and Garai-Papikian and discuss their practical behavior. In
particular, we find that all three approaches represent either an improvement
in complexity or an expansion of the parameter space over which the algorithm
may be applied. Some experimental results are also presented.
"
1366,Computing strong regular characteristic pairs with Groebner bases,"  The W-characteristic set of a polynomial ideal is the minimal triangular set
contained in the reduced lexicographical Groebner basis of the ideal. A pair
(G,C) of polynomial sets is a strong regular characteristic pair if G is a
reduced lexicographical Groebner basis, C is the W-characteristic set of the
ideal <G>, the saturated ideal sat(C) of C is equal to <G>, and C is regular.
In this paper, we show that for any polynomial ideal I with given generators
one can either detect that I is unit, or construct a strong regular
characteristic pair (G,C) by computing Groebner bases such that
I$\subseteq$sat(C)=<G> and sat(C) divides I, so the ideal I can be split into
the saturated ideal sat(C) and the quotient ideal I:sat(C). Based on this
strategy of splitting by means of quotient and with Groebner basis and ideal
computations, we devise a simple algorithm to decompose an arbitrary polynomial
set F into finitely many strong regular characteristic pairs, from which two
representations for the zeros of F are obtained: one in terms of strong regular
Groebner bases and the other in terms of regular triangular sets. We present
some properties about strong regular characteristic pairs and characteristic
decomposition and illustrate the proposed algorithm and its performance by
examples and experimental results.
"
1367,CREST: Hardware Formal Verification with ANSI-C Reference Specifications,"  This paper presents CREST, a prototype front-end tool intended as an add-on
to commercial EDA formal verifcation environments. CREST is an adaptation of
the CBMC bounded model checker for C, an academic tool widely used in industry
for software analysis and property verification. It leverages the capabilities
of CBMC to process hardware datapath specifications written in arbitrary
ANSI-C, without limiting restrictions to a synthesizable subset. We briefly
sketch the architecture of our tool and show its use in a range of verification
case studies.
"
1368,On Symbolic Approaches for Computing the Matrix Permanent,"  Counting the number of perfect matchings in bipartite graphs, or equivalently
computing the permanent of 0-1 matrices, is an important combinatorial problem
that has been extensively studied by theoreticians and practitioners alike. The
permanent is #P-Complete; hence it is unlikely that a polynomial-time algorithm
exists for the problem. Researchers have therefore focused on finding tractable
subclasses of matrices for permanent computation. One such subclass that has
received much attention is that of sparse matrices i.e. matrices with few
entries set to 1, the rest being 0. For this subclass, improved theoretical
upper bounds and practically efficient algorithms have been developed. In this
paper, we ask whether it is possible to go beyond sparse matrices in our quest
for developing scalable techniques for the permanent, and answer this question
affirmatively. Our key insight is to represent permanent computation
symbolically using Algebraic Decision Diagrams (ADDs). ADD-based techniques
naturally use dynamic programming, and hence avoid redundant computation
through memoization. This permits exploiting the hidden structure in a large
class of matrices that have so far remained beyond the reach of permanent
computation techniques. The availability of sophisticated libraries
implementing ADDs also makes the task of engineering practical solutions
relatively straightforward. While a complete characterization of matrices
admitting a compact ADD representation remains open, we provide strong
experimental evidence of the effectiveness of our approach for computing the
permanent, not just for sparse matrices, but also for dense matrices and for
matrices with ""similar"" rows.
"
1369,A localized version of the basic triangle theorem,"  In this short note, we give a localized version of the basic triangle
theorem, first published in 2011 (see [4]) in order to prove the independence
of hyperlogarithms over various function fields. This version provides direct
access to rings of scalars and avoids the recourse to fraction fields as that
of meromorphic functions for instance.
"
1370,Computing zero-dimensional tropical varieties via projections,"  We present an algorithm for computing zero-dimensional tropical varieties
using projections. Our main tools are fast unimodular transforms of
lexicographical Gr\""obner bases. We prove that our algorithm requires only a
polynomial number of arithmetic operations if given a Gr\""obner basis, and we
demonstrate that our implementation compares favourably to other existing
implementations. Applying it to the computation of general positive-dimensional
tropical varieties, we argue that the complexity for calculating tropical links
is dominated by the complexity of the Gr\""obner walk.
"
1371,Probabilistic Saturations and Alt's Problem,"  Alt's problem, formulated in 1923, is to count the number of four-bar
linkages whose coupler curve interpolates nine general points in the plane.
This problem can be phrased as counting the number of solutions to a system of
polynomial equations which was first solved numerically using homotopy
continuation by Wampler, Morgan, and Sommese in 1992. Since there is still not
a proof that all solutions were obtained, we consider upper bounds for Alt's
problem by counting the number of solutions outside of the base locus to a
system arising as the general linear combination of polynomials. In particular,
we derive effective symbolic and numeric methods for studying such systems
using probabilistic saturations that can be employed using both finite fields
and floating-point computations. We give bounds on the size of finite field
required to achieve a desired level of certainty. These methods can also be
applied to many other problems where similar systems arise such as computing
the volumes of Newton-Okounkov bodies and computing intersection theoretic
invariants including Euler characteristics, Chern classes, and Segre classes.
"
1372,"Proving two conjectural series for $\zeta(7)$ and discovering more
  series for $\zeta(7)$","  We give a proof of two identities involving binomial sums at infinity
conjectured by Z-W Sun. In order to prove these identities, we use a recently
presented method i.e. we view the series as specializations of generating
series and derive integral representations. Using substitutions, we express
these integral representations in terms of cyclotomic harmonic polylogarithms.
Finally, by applying known relations among the cyclotomic harmonic
polylogarithms, we derive the results. These methods are implemented in the
computer algebra package HarmonicSums.
"
1373,"PPT: New Low Complexity Deterministic Primality Tests Leveraging
  Explicit and Implicit Non-Residues. A Set of Three Companion Manuscripts","  In this set of three companion manuscripts/articles, we unveil our new
results on primality testing and reveal new primality testing algorithms
enabled by those results. The results have been classified (and referred to) as
lemmas/corollaries/claims whenever we have complete analytic proof(s);
otherwise the results are introduced as conjectures.
  In Part/Article 1, we start with the Baseline Primality Conjecture~(PBPC)
which enables deterministic primality detection with a low complexity = O((log
N)^2) ; when an explicit value of a Quadratic Non Residue (QNR) modulo-N is
available (which happens to be the case for an overwhelming majority = 11/12 =
91.67% of all odd integers). We then demonstrate Primality Lemma PL-1, which
reveals close connections between the state-of-the-art Miller-Rabin method and
the renowned Euler-Criterion. This Lemma, together with the Baseline Primality
Conjecture enables a synergistic fusion of Miller-Rabin iterations and our
method(s), resulting in hybrid algorithms that are substantially better than
their components. Next, we illustrate how the requirement of an explicit value
of a QNR can be circumvented by using relations of the form: Polynomial(x) mod
N = 0 ; whose solutions implicitly specify Non Residues modulo-N. We then
develop a method to derive low-degree canonical polynomials that together
guarantee implicit Non Residues modulo-N ; which along with the Generalized
Primality Conjectures enable algorithms that achieve a worst case deterministic
polynomial complexity = O( (log N)^3 polylog(log N)) ; unconditionally ; for
any/all values of N.
  In Part/Article 2 , we present substantial experimental data that corroborate
all the conjectures. No counter example has been found.
  Finally in Part/Article 3, we present analytic proof(s) of the Baseline
Primality Conjecture that we have been able to complete for some special cases.
"
1374,Lemma Generation for Horn Clause Satisfiability: A Preliminary Study,"  It is known that the verification of imperative, functional, and logic
programs can be reduced to the satisfiability of constrained Horn clauses
(CHCs), and this satisfiability check can be performed by using CHC solvers,
such as Eldarica and Z3. These solvers perform well when they act on simple
constraint theories, such as Linear Integer Arithmetic and the theory of
Booleans, but their efficacy is very much reduced when the clauses refer to
constraints on inductively defined structures, such as lists or trees.
Recently, we have presented a transformation technique for eliminating those
inductively defined data structures, and hence avoiding the need for
incorporating induction principles into CHC solvers. However, this technique
may fail when the transformation requires the use of lemmata whose generation
needs ingenuity. In this paper we show, through an example, how during the
process of transforming CHCs for eliminating inductively defined structures one
can introduce suitable predicates, called difference predicates, whose
definitions correspond to the lemmata to be introduced. Through a second
example, we show that, whenever difference predicates cannot be introduced, we
can introduce, instead, auxiliary queries which also correspond to lemmata, and
the proof of these lemmata can be done by showing the satisfiability of those
queries.
"
1375,Residues of skew rational functions and linearized Goppa codes,"  This paper constitutes a first attempt to do analysis with skew polynomials.
Precisely, our main objective is to develop a theory of residues for skew
rational functions (which are, by definition, the quotients of two skew
polynomials). We prove in particular a skew analogue of the residue formula and
a skew analogue of the classical formula of change of variables for residues.
We then use our theory to define and study a linearized version of Goppa codes.
We show that these codes meet the Singleton bound (for the sum-rank metric) and
are the duals of the linearized Reed-Solomon codes defined recently by
Mart{\'i}nez-Pe{\~n}as. We also design efficient encoding and decoding
algorithms for them.
"
1376,Converting ALC Connection Proofs into ALC Sequents,"  The connection method has earned good reputation in the area of automated
theorem proving, due to its simplicity, efficiency and rational use of memory.
This method has been applied recently in automatic provers that reason over
ontologies written in the description logic ALC. However, proofs generated by
connection calculi are difficult to understand. Proof readability is largely
lost by the transformations to disjunctive normal form applied over the
formulae to be proven. Such a proof model, albeit efficient, prevents inference
systems based on it from effectively providing justifications and/or
descriptions of the steps used in inferences. To address this problem, in this
paper we propose a method for converting matricial proofs generated by the ALC
connection method to ALC sequent proofs, which are much easier to understand,
and whose translation to natural language is more straightforward. We also
describe a calculus that accepts the input formula in a non-clausal ALC format,
what simplifies the translation.
"
1377,Proceedings Third Symposium on Working Formal Methods,"  This volume contains the proceedings of FROM 2019: the Third Symposium on
Working Formal Methods, held on September 3-5, 2019 in Timi\c{s}oara (Romania).
FROM aims to bring together researchers and practitioners who work on formal
methods by contributing new theoretical results, methods, techniques, and
frameworks, and/or make the formal methods to work by creating or using
software tools that apply theoretical contributions.
"
1378,On the k-synchronizability of systems,"  In this paper, we work on the notion of k-synchronizability: a system is
k-synchronizable if any of its executions, up to reordering causally
independent actions, can be divided into a succession of k-bounded interaction
phases. We show two results (both for mailbox and peer-to-peer automata):
first, the reachability problem is decidable for k-synchronizable systems;
second, the membership problem (whether a given system is k-synchronizable) is
decidable as well. Our proofs fix several important issues in previous attempts
to prove these two results for mailbox automata.
"
1379,"Defeating Opaque Predicates Statically through Machine Learning and
  Binary Analysis","  We present a new approach that bridges binary analysis techniques with
machine learning classification for the purpose of providing a static and
generic evaluation technique for opaque predicates, regardless of their
constructions. We use this technique as a static automated deobfuscation tool
to remove the opaque predicates introduced by obfuscation mechanisms. According
to our experimental results, our models have up to 98% accuracy at detecting
and deob-fuscating state-of-the-art opaque predicates patterns. By contrast,
the leading edge deobfuscation methods based on symbolic execution show less
accuracy mostly due to the SMT solvers constraints and the lack of scalability
of dynamic symbolic analyses. Our approach underlines the efficiency of hybrid
symbolic analysis and machine learning techniques for a static and generic
deobfuscation methodology.
"
1380,Verifying the DPLL Algorithm in Dafny,"  Modern high-performance SAT solvers quickly solve large satisfiability
instances that occur in practice. If the instance is satisfiable, then the SAT
solver can provide a witness which can be checked independently in the form of
a satisfying truth assignment. However, if the instance is unsatisfiable, the
certificates could be exponentially large or the SAT solver might not be able
to output certificates. The implementation of the SAT solver should then be
trusted not to contain bugs. However, the data structures and algorithms
implemented by a typical high-performance SAT solver are complex enough to
allow for subtle programming errors. To counter this issue, we build a verified
SAT solver using the Dafny system. We discuss its implementation in the present
article.
"
1381,"Gr\""obner Bases with Reduction Machines","  In this paper, we make a contribution to the computation of Gr\""obner bases.
For polynomial reduction, instead of choosing the leading monomial of a
polynomial as the monomial with respect to which the reduction process is
carried out, we investigate what happens if we make that choice arbitrarily. It
turns out not only this is possible (the fact that this produces a normal form
being already known in the literature), but, for a fixed choice of reductors,
the obtained normal form is the same no matter the order in which we reduce the
monomials. To prove this, we introduce reduction machines, which work by
reducing each monomial independently and then collecting the result. We show
that such a machine can simulate any such reduction. We then discuss different
implementations of these machines. Some of these implementations address
inherent inefficiencies in reduction machines (repeating the same
computations). We describe a first implementation and look at some experimental
results.
"
1382,SATURN -- Software Deobfuscation Framework Based on LLVM,"  The strength of obfuscated software has increased over the recent years.
Compiler based obfuscation has become the de facto standard in the industry and
recent papers also show that injection of obfuscation techniques is done at the
compiler level. In this paper we discuss a generic approach for deobfuscation
and recompilation of obfuscated code based on the compiler framework LLVM. We
show how binary code can be lifted back into the compiler intermediate language
LLVM-IR and explain how we recover the control flow graph of an obfuscated
binary function with an iterative control flow graph construction algorithm
based on compiler optimizations and SMT solving. Our approach does not make any
assumptions about the obfuscated code, but instead uses strong compiler
optimizations available in LLVM and Souper Optimizer to simplify away the
obfuscation. Our experimental results show that this approach can be effective
to weaken or even remove the applied obfuscation techniques like constant
unfolding, certain arithmetic-based opaque expressions, dead code insertions,
bogus control flow or integer encoding found in public and commercial
obfuscators. The recovered LLVM-IR can be further processed by custom
deobfuscation passes that are now applied at the same level as the injected
obfuscation techniques or recompiled with one of the available LLVM backends.
The presented work is implemented in a deobfuscation tool called SATURN.
"
1383,"Elimination-based certificates for triangular equivalence and rank
  profiles","  In this paper, we give novel certificates for triangular equivalence and rank
profiles. These certificates enable somebody to verify the row or column rank
profiles or the whole rank profile matrix faster than recomputing them, with a
negligible overall overhead. We first provide quadratic time and space
non-interactive certificates saving the logarithmic factors of previously known
ones. Then we propose interactive certificates for the same problems whose
Monte Carlo verification complexity requires a small constant number of
matrix-vector multiplications, a linear space, and a linear number of extra
field operations , with a linear number of interactions. As an application we
also give an interactive protocol, certifying the determinant or the signature
of dense matrices, faster for the Prover than the best previously known one.
Finally we give linear space and constant round certificates for the row or
column rank profiles.
"
1384,Efficient Rational Creative Telescoping,"  We present a new algorithm to compute minimal telescopers for rational
functions in two discrete variables. As with recent reduction-based approach,
our algorithm has the nice feature that the computation of a telescoper is
independent of its certificate. Moreover, our algorithm uses a sparse
representation of the certificate, which allows it to be easily manipulated and
analyzed without knowing the precise expanded form. This representation hides
potential expression swell until the final (and optional) expansion, which can
be accomplished in time polynomial in the size of the expanded certificate. A
complexity analysis, along with a Maple implementation, suggests that our
algorithm has better theoretical and practical performance than the
reduction-based approach in the rational case.
"
1385,Imperative Program Synthesis from Answer Set Programs,"  Our research concerns generating imperative programs from Answer Set
Programming Specifications. ASP is highly declarative and is ideal for writing
specifications. Further with negation-as-failure it is easy to succinctly
represent combinatorial search problems. We are currently working on
synthesizing imperative programs from ASP programs by turning the negation into
useful computations. This opens up a novel way to synthesize programs from
executable specifications.
"
1386,Graph Neural Reasoning May Fail in Certifying Boolean Unsatisfiability,"  It is feasible and practically-valuable to bridge the characteristics between
graph neural networks (GNNs) and logical reasoning. Despite considerable
efforts and successes witnessed to solve Boolean satisfiability (SAT), it
remains a mystery of GNN-based solvers for more complex predicate logic
formulae. In this work, we conjectures with some evidences, that
generally-defined GNNs present several limitations to certify the
unsatisfiability (UNSAT) in Boolean formulae. It implies that GNNs may probably
fail in learning the logical reasoning tasks if they contain proving UNSAT as
the sub-problem included by most predicate logic formulae.
"
1387,Equivariant Hilbert series for hierarchical models,"  Toric ideals to hierarchical models are invariant under the action of a
product of symmetric groups. Taking the number of factors, say m, into account,
we introduce and study invariant filtrations and their equivariant Hilbert
series. We present a condition that guarantees that the equivariant Hilbert
series is a rational function in m+1 variables with rational coefficients.
Furthermore we give explicit formulas for the rational functions with
coefficients in a number field and an algorithm for determining the rational
functions with rational coefficients. A key is to construct finite automata
that recognize languages corresponding to invariant filtrations.
"
1388,The Multivariate Schwartz-Zippel Lemma,"  Motivated by applications in combinatorial geometry, we consider the
following question: Let $\lambda=(\lambda_1,\lambda_2,\ldots,\lambda_m)$ be an
$m$-partition of a positive integer $n$, $S_i \subseteq \mathbb{C}^{\lambda_i}$
be finite sets, and let $S:=S_1 \times S_2 \times \ldots \times S_m \subset
\mathbb{C}^n$ be the multi-grid defined by $S_i$. Suppose $p$ is an $n$-variate
degree $d$ polynomial. How many zeros does $p$ have on $S$?
  We first develop a multivariate generalization of Combinatorial
Nullstellensatz that certifies existence of a point $t \in S$ so that $p(t)
\neq 0$. Then we show that a natural multivariate generalization of the
DeMillo-Lipton-Schwartz-Zippel lemma holds, except for a special family of
polynomials that we call $\lambda$-reducible. This yields a simultaneous
generalization of Szemer\'edi-Trotter theorem and Schwartz-Zippel lemma into
higher dimensions, and has applications in incidence geometry. Finally, we
develop a symbolic algorithm that identifies certain $\lambda$-reducible
polynomials. More precisely, our symbolic algorithm detects polynomials that
include a cartesian product of hypersurfaces in their zero set. It is likely
that using Chow forms the algorithm can be generalized to handle arbitrary
$\lambda$-reducible polynomials, which we leave as an open problem.
"
1389,D-Modules and Holonomic Functions,"  In algebraic geometry, one studies the solutions to polynomial equations, or,
equivalently, to linear partial differential equations with constant
coefficients. These lecture notes address the more general case when the
coefficients are polynomials. The letter D stands for the Weyl algebra, and a
D-module is a left module over D. We focus on left ideals, or D-ideals. We
represent holonomic functions in several variables by the linear differential
equations they satisfy. This encoding by a D-ideal is useful for many problems,
e.g., in geometry, physics and statistics. We explain how to work with
holonomic functions. Applications include volume computations and likelihood
inference.
"
1390,"Note on the construction of Picard-Vessiot rings for linear differential
  equations","  In this note, we describe a method to construct the Picard-Vessiot ring of a
given linear differential equation.
"
1391,Approximate GCD in a Bernstein basis,"  We adapt Victor Y. Pan's root-based algorithm for finding approximate GCD to
the case where the polynomials are expressed in Bernstein bases. We use the
numerically stable companion pencil of Gudbj\""orn F. J\'onsson to compute the
roots, and the Hopcroft-Karp bipartite matching method to find the degree of
the approximate GCD. We offer some refinements to improve the process.
"
1392,Input-output equations and identifiability of linear ODE models,"  Structural identifiability is a property of a differential model with
parameters that allows for the parameters to be determined from the model
equations in the absence of noise. The method of input-output equations is one
method for verifying structural identifiability. This method stands out in its
importance because the additional insights it provides can be used to analyze
and improve models. However, its complete theoretical grounds and applicability
are still to be established. A subtlety and key for this method to work is
knowing if the coefficients of these equations are identifiable.
  In this paper, to address this, we prove identifiability of the coefficients
of input-output equations for types of differential models that often appear in
practice, such as linear models with one output and linear compartment models
in which, from each compartment, one can reach either a leak or an input. This
shows that checking identifiability via input-output equations for these models
is legitimate and, as we prove, that the field of identifiable functions is
generated by the coefficients of the input-output equations. Finally, we show
that, for a linear compartment model with an input and strongly connected
graph, the field of all identifiable functions is generated by the coefficients
of the equations obtained from the model just using Cramer's rule.
"
1393,"Efficiently and Effectively Recognizing Toricity of Steady State
  Varieties","  We consider the problem of testing whether the points in a complex or real
variety with non-zero coordinates form a multiplicative group or, more
generally, a coset of a multiplicative group. For the coset case, we study the
notion of shifted toric varieties which generalizes the notion of toric
varieties. This requires a geometric view on the varieties rather than an
algebraic view on the ideals. We present algorithms and computations on 129
models from the BioModels repository testing for group and coset structures
over both the complex numbers and the real numbers. Our methods over the
complex numbers are based on Gr\""obner basis techniques and binomiality tests.
Over the real numbers we use first-order characterizations and employ real
quantifier elimination. In combination with suitable prime decompositions and
restrictions to subspaces it turns out that almost all models show coset
structure. Beyond our practical computations, we give upper bounds on the
asymptotic worst-case complexity of the corresponding problems by proposing
single exponential algorithms that test complex or real varieties for toricity
or shifted toricity. In the positive case, these algorithms produce generating
binomials. In addition, we propose an asymptotically fast algorithm for testing
membership in a binomial variety over the algebraic closure of the rational
numbers.
"
1394,Programming and Symbolic Computation in Maude,"  Rewriting logic is both a flexible semantic framework within which widely
different concurrent systems can be naturally specified and a logical framework
in which widely different logics can be specified. Maude programs are exactly
rewrite theories. Maude has also a formal environment of verification tools.
Symbolic computation is a powerful technique for reasoning about the
correctness of concurrent systems and for increasing the power of formal tools.
We present several new symbolic features of Maude that enhance formal reasoning
about Maude programs and the effectiveness of formal tools. They include: (i)
very general unification modulo user-definable equational theories, and (ii)
symbolic reachability analysis of concurrent systems using narrowing. The paper
does not focus just on symbolic features: it also describes several other new
Maude features, including: (iii) Maude's strategy language for controlling
rewriting, and (iv) external objects that allow flexible interaction of Maude
object-based concurrent systems with the external world. In particular,
meta-interpreters are external objects encapsulating Maude interpreters that
can interact with many other objects. To make the paper self-contained and give
a reasonably complete language overview, we also review the basic Maude
features for equational rewriting and rewriting with rules, Maude programming
of concurrent object systems, and reflection. Furthermore, we include many
examples illustrating all the Maude notions and features described in the
paper.
"
1395,Review of Recent Techniques on Heap Specification and Verification,"  This review article provides an overview of recent approaches and techniques
in specifying and verifying dynamic memory with class objects. Dynamic memory
verification may be used in order to show, for instance, the absence of memory
leaks or the validity of memory access.
"
1396,"qFunctions -- A Mathematica package for $q$-series and partition theory
  applications","  We describe the qFunctions Mathematica package for $q$-series and partition
theory applications. This package includes both experimental and symbolic
tools. The experimental set of elements includes guessers for $q$-shift
equations and recurrences for given $q$-series and fitting/finding explicit
expressions for sequences of polynomials. This package can symbolically handle
formal manipulations on $q$-differential, $q$-shift equations and recurrences,
such as switching between these forms, finding the greatest common divisor of
recurrences, and formal substitutions. Here, we also extend the classical
method of the weighted words approach. Moreover, qFunctions has implementations
that automate the recurrence system creation of the weighted words approach as
well as a scheme on cylindric partitions.
"
1397,"RationalizeRoots: Software Package for the Rationalization of Square
  Roots","  The computation of Feynman integrals often involves square roots. One way to
obtain a solution in terms of multiple polylogarithms is to rationalize these
square roots by a suitable variable change. We present a program that can be
used to find such transformations. After an introduction to the theoretical
background, we explain in detail how to use the program in practice.
"
1398,A sufficient condition for local nonnegativity,"  A real polynomial $f$ is called local nonnegative at a point $p$, if it is
nonnegative in a neighbourhood of $p$. In this paper, a sufficient condition
for determining this property is constructed. Newton's principal part of $f$
(denoted as $f_N$) plays a key role in this process. We proved that if every
$F$-face, $(f_N)_F$, of $f_N$ is strictly positive over $(\mathbb{R}\setminus
0)^n$, then $f$ is local nonnegative at the origin $O$.
"
1399,Multi-Agent Safety Verification using Symmetry Transformations,"  We show that symmetry transformations and caching can enable scalable, and
possibly unbounded, verification of multi-agent systems. Symmetry
transformations map solutions and to other solutions. We show that this
property can be used to transform cached reachsets to compute new reachsets,
for hybrid and multi-agent models. We develop a notion of virtual system which
define symmetry transformations for a broad class of agent models that visit
waypoint sequences. Using this notion of virtual system, we present a prototype
tool CacheReach that builds a cache of reachtubes for this system, in a way
that is agnostic of the representation of the reachsets and the reachability
analysis subroutine used. Our experimental evaluation of CacheReach shows up to
66% savings in safety verification computation time on multi-agent systems with
3-dimensional linear and 4-dimensional nonlinear fixed-wing aircraft models
following sequences of waypoints. These savings and our theoretical results
illustrate the potential benefits of using symmetry-based caching in the safety
verification of multi-agent systems.
"
1400,Fast Derivatives for Multilinear Polynomials,"  The article considers linear functions of many (n) variables - multilinear
polynomials (MP). The three-steps evaluation is presented that uses the minimal
possible number of floating point operations for non-sparse MP at each step.
The minimal number of additions is achieved in the algorithm for fast MP
derivatives (FMPD) calculation. The cost of evaluating all first derivatives
approaches to only 1/8 of MP evaluation with a growing number of variables. The
FMPD algorithm structure exhibits similarity to the Fast Fourier Transformation
(FFT) algorithm.
"
1401,"Entropy supplementary conservation law for non-linear systems of PDEs
  with non-conservative terms: application to the modelling and analysis of
  complex fluid flows using computer algebra","  In the present contribution, we investigate first-order nonlinear systems of
partial differential equations which are constituted of two parts: a system of
conservation laws and non-conservative first order terms. Whereas the theory of
first-order systems of conservation laws is well established and the conditions
for the existence of supplementary conservation laws, and more specifically of
an entropy supplementary conservation law for smooth solutions, well known,
there exists so far no general extension to obtain such supplementary
conservation laws when non-conservative terms are present. We propose a
framework in order to extend the existing theory and show that the presence of
non-conservative terms somewhat complexifies the problem since numerous
combinations of the conservative and non-conservative terms can lead to a
supplementary conservation law. We then identify a restricted framework in
order to design and analyze physical models of complex fluid flows by means of
computer algebra and thus obtain the entire ensemble of possible combination of
conservative and non-conservative terms with the objective of obtaining
specifically an entropy supplementary conservation law. The theory as well as
developed computer algebra tool are then applied to a Baer-Nunziato two-phase
flow model and to a multicomponent plasma fluid model. The first one is a
first-order fluid model, with non-conservative terms impacting on the linearly
degenerate field and requires a closure since there is no way to derive
interfacial quantities from averaging principles and we need guidance in order
to close the pressure and velocity of the interface and the thermodynamics of
the mixture. The second one involves first order terms for the heavy species
coupled to second order terms for the electrons, the non-conservative terms
impact the genuinely nonlinear fields and the model can be rigorously derived
from kinetic theory. We show how the theory allows to recover the whole
spectrum of closures obtained so far in the literature for the two-phase flow
system as well as conditions when one aims at extending the thermodynamics and
also applies to the plasma case, where we recover the usual entropy
supplementary equation, thus assessing the effectiveness and scope of the
proposed theory.
"
1402,"A Nonexistence Certificate for Projective Planes of Order Ten with
  Weight 15 Codewords","  Using techniques from the fields of symbolic computation and satisfiability
checking we verify one of the cases used in the landmark result that projective
planes of order ten do not exist. In particular, we show that there exist no
projective planes of order ten that generate codewords of weight fifteen, a
result first shown in 1973 via an exhaustive computer search. We provide a
simple satisfiability (SAT) instance and a certificate of unsatisfiability that
can be used to automatically verify this result for the first time. All
previous demonstrations of this result have relied on search programs that are
difficult or impossible to verify---in fact, our search found partial
projective planes that were missed by previous searches due to previously
undiscovered bugs. Furthermore, we show how the performance of the SAT solver
can be dramatically increased by employing functionality from a computer
algebra system (CAS). Our SAT+CAS search runs significantly faster than all
other published searches verifying this result.
"
1403,"Minimal representations and algebraic relations for single nested
  products","  Recently, it has been shown constructively how a finite set of hypergeometric
products, multibasic hypergeometric products or their mixed versions can be
modeled properly in the setting of formal difference rings. Here special
emphasis is put on robust constructions: whenever further products have to be
considered, one can reuse --up to some mild modifications-- the already
existing difference ring. In this article we relax this robustness criteria and
seek for another form of optimality. We will elaborate a general framework to
represent a finite set of products in a formal difference ring where the number
of transcendental product generators is minimal. As a bonus we are able to
describe explicitly all relations among the given input products.
"
1404,A Root-Free Splitting-Lemma for Systems of Linear Differential Equations,"  We consider the formal reduction of a system of linear differential equations
and show that, if the system can be block-diagonalised through transformation
with a ramified Shearing-transformation and following application of the
Splitting Lemma, and if the spectra of the leading block matrices of the
ramified system satisfy a symmetry condition, this block-diagonalisation can
also be achieved through an unramified transformation. Combined with classical
results by Turritin and Wasow as well as work by Balser, this yields a
constructive and simple proof of the existence of an unramified block-diagonal
form from which formal invariants such as the Newton polygon can be read
directly. Our result is particularly useful for designing efficient algorithms
for the formal reduction of the system.
"
1405,New practical advances in polynomial root clustering,"  We report an ongoing work on clustering algorithms for complex roots of a
univariate polynomial $p$ of degree $d$ with real or complex coefficients. As
in their previous best subdivision algorithms our root-finders are robust even
for multiple roots of a polynomial given by a black box for the approximation
of its coefficients, and their complexity decreases at least proportionally to
the number of roots in a region of interest (ROI) on the complex plane, such as
a disc or a square, but we greatly strengthen the main ingredient of the
previous algorithms. Namely our new counting test essentially amounts to the
evaluation of a polynomial $p$ and its derivative $p'$, which is a major
benefit, e.g., for sparse polynomials $p$. Moreover with evaluation at about
$\log(d)$ points (versus the previous record of order $d$) we output correct
number of roots in a disc whose contour has no roots of $p$ nearby. Moreover we
greatly soften the latter requirement versus the known subdivision algorithms.
Our second and less significant contribution concerns subdivision algorithms
for polynomials with real coefficients. Our tests demonstrate the power of the
proposed algorithms.
"
1406,"Improving Graph Neural Network Representations of Logical Formulae with
  Subgraph Pooling","  Recent advances in the integration of deep learning with automated theorem
proving have centered around the representation of logical formulae as inputs
to deep learning systems. In particular, there has been a growing interest in
adapting structure-aware neural methods to work with the underlying graph
representations of logical expressions. While more effective than character and
token-level approaches, graph-based methods have often made representational
trade-offs that limited their ability to capture key structural properties of
their inputs. In this work we propose a novel approach for embedding logical
formulae that is designed to overcome the representational limitations of prior
approaches. Our architecture works for logics of different expressivity; e.g.,
first-order and higher-order logic. We evaluate our approach on two standard
datasets and show that the proposed architecture achieves state-of-the-art
performance on both premise selection and proof step classification.
"
1407,"On sequences associated to the invariant theory of rank two simple Lie
  algebras","  We study two families of sequences, listed in the On-Line Encyclopedia of
Integer Sequences (OEIS), which are associated to invariant theory of Lie
algebras. For the first family, we prove combinatorially that the sequences
A059710 and A108307 are related by a binomial transform. Based on this, we
present two independent proofs of a recurrence equation for A059710, which was
conjectured by Mihailovs. Besides, we also give a direct proof of Mihailovs'
conjecture by the method of algebraic residues. As a consequence, closed
formulae for the generating function of sequence A059710 are obtained in terms
of classical Gaussian hypergeometric functions. Moreover, we show that
sequences in the second family are also related by binomial transforms.
"
1408,Schur Polynomials do not have small formulas if the Determinant doesn't!,"  Schur Polynomials are families of symmetric polynomials that have been
classically studied in Combinatorics and Algebra alike. They play a central
role in the study of Symmetric functions, in Representation theory [Sta99], in
Schubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,
Sta99]. In recent years, they have also shown up in various incarnations in
Computer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric
complexity theory [IP17].
  However, unlike some other families of symmetric polynomials like the
Elementary Symmetric polynomials, the Power Symmetric polynomials and the
Complete Homogeneous Symmetric polynomials, the computational complexity of
syntactically computing Schur polynomials has not been studied much. In
particular, it is not known whether Schur polynomials can be computed
efficiently by algebraic formulas. In this work, we address this question, and
show that unless \emph{every} polynomial with a small algebraic branching
program (ABP) has a small algebraic formula, there are Schur polynomials that
cannot be computed by algebraic formula of polynomial size. In other words,
unless the algebraic complexity class $\mathrm{VBP}$ is equal to the complexity
class $\mathrm{VF}$, there exist Schur polynomials which do not have polynomial
size algebraic formulas.
  As a consequence of our proof, we also show that computing the determinant of
certain \emph{generalized} Vandermonde matrices is essentially as hard as
computing the general symbolic determinant. To the best of our knowledge, these
are one of the first hardness results of this kind for families of polynomials
which are not \emph{multilinear}. A key ingredient of our proof is the study of
composition of \emph{well behaved} algebraically independent polynomials with a
homogeneous polynomial, and might be of independent interest.
"
1409,"Improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness","  Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.
"
1410,Efficient Recognition of Graph Languages,"  Graph transformation is the rule-based modification of graphs, and is a
discipline dating back to the 1970s. In general, to match the left-hand graph
of a fixed rule within a host graph requires polynomial time, but to improve
matching performance, D\""orr proposed to equip rules and host graphs with
distinguished root nodes. This model was implemented by Plump and Bak, but
unfortunately, such rules are not invertible. We address this problem by
defining rootedness using a partial function into a two-point set rather than
pointing graphs with root nodes, meaning derivations are natural double
pushouts. Moreover, we give a sufficient condition on rules to give constant
time rule application on graphs of bounded degree, and that, the graph class of
trees can be recognised in linear time, given an input graph of bounded degree.
Finally, we define a new notion of confluence up to garbage and non-garbage
critical pairs, showing it is sufficient to require strong joinability of only
the non-garbage critical pairs to establish confluence up to garbage. Finally,
this new result, presented for conventional graph transformation systems, can
be lifted to our rooted setting by encoding node labels and rootedness as
looped edges.
"
1411,"Chain Rules for Hessian and Higher Derivatives Made Easy by Tensor
  Calculus","  Computing multivariate derivatives of matrix-like expressions in the compact,
coordinate free fashion is very important for both theory and applied
computations (e.g. optimization and machine learning).
  The critical components of such computations are \emph{chain and product
rules} for derivatives. Although they are taught early in simple scenarios,
practical applications involve high-dimensional arrays; in this context it is
very hard to find easy accessible and compact explanation.
  This paper discusses how to relatively simply carry such derivations based on
the (simplified as adapted in applied computer science) concept of tensors.
Numerical examples in modern Python libraries are provided. This discussion
simplifies and illustrates an earlier exposition by Manton (2012).
"
1412,Counting invariant subspaces and decompositions of additive polynomials,"  The functional (de)composition of polynomials is a topic in pure and computer
algebra with many applications. The structure of decompositions of (suitably
normalized) polynomials f(x) = g(h(x)) in F[x] over a field F is well
understood in many cases, but less well when the degree of f is divisible by
the positive characteristic p of F. This work investigates the decompositions
of r-additive polynomials, where every exponent and also the field size is a
power of r, which itself is a power of p.
  The decompositions of an r-additive polynomial f are intimately linked to the
Frobenius-invariant subspaces of its root space V in the algebraic closure of
F. We present an efficient algorithm to compute the rational Jordan form of the
Frobenius automorphism on V. A formula of Fripertinger (2011) then counts the
number of Frobenius-invariant subspaces of a given dimension and we derive the
number of decompositions with prescribed degrees.
"
1413,Algebraic Analysis of Rotation Data,"  We develop algebraic tools for statistical inference from samples of rotation
matrices. This rests on the theory of D-modules in algebraic analysis.
Noncommutative Gr\""obner bases are used to design numerical algorithms for
maximum likelihood estimation, building on the holonomic gradient method of
Sei, Shibata, Takemura, Ohara, and Takayama. We study the Fisher model for
sampling from rotation matrices, and we apply our algorithms for data from the
applied sciences. On the theoretical side, we generalize the underlying
equivariant D-modules from SO(3) to arbitrary Lie groups. For compact groups,
our D-ideals encode the normalizing constant of the Fisher model.
"
1414,Deep Learning for Symbolic Mathematics,"  Neural networks have a reputation for being better at solving statistical or
approximate problems than at performing calculations or working with symbolic
data. In this paper, we show that they can be surprisingly good at more
elaborated tasks in mathematics, such as symbolic integration and solving
differential equations. We propose a syntax for representing mathematical
problems, and methods for generating large datasets that can be used to train
sequence-to-sequence models. We achieve results that outperform commercial
Computer Algebra Systems such as Matlab or Mathematica.
"
1415,On Computational Poisson Geometry I: Symbolic Foundations,"  We present a computational toolkit for (local) Poisson-Nijenhuis calculus on
manifolds. Our python module $\textsf{PoissonGeometry}$ implements our
algorithms, and accompanies this paper. We include two examples of how our
methods can be used, one for gauge transformations of Poisson bivectors in
dimension 3, and a second one that determines parametric Poisson bivector
fields in dimension 4.
"
1416,Computing syzygies in finite dimension using fast linear algebra,"  We consider the computation of syzygies of multivariate polynomials in a
finite-dimensional setting: for a $\mathbb{K}[X_1,\dots,X_r]$-module
$\mathcal{M}$ of finite dimension $D$ as a $\mathbb{K}$-vector space, and given
elements $f_1,\dots,f_m$ in $\mathcal{M}$, the problem is to compute syzygies
between the $f_i$'s, that is, polynomials $(p_1,\dots,p_m)$ in
$\mathbb{K}[X_1,\dots,X_r]^m$ such that $p_1 f_1 + \dots + p_m f_m = 0$ in
$\mathcal{M}$. Assuming that the multiplication matrices of the $r$ variables
with respect to some basis of $\mathcal{M}$ are known, we give an algorithm
which computes the reduced Gr\""obner basis of the module of these syzygies, for
any monomial order, using $O(m D^{\omega-1} + r D^\omega \log(D))$ operations
in the base field $\mathbb{K}$, where $\omega$ is the exponent of matrix
multiplication. Furthermore, assuming that $\mathcal{M}$ is itself given as
$\mathcal{M} = \mathbb{K}[X_1,\dots,X_r]^n/\mathcal{N}$, under some assumptions
on $\mathcal{N}$ we show that these multiplication matrices can be computed
from a Gr\""obner basis of $\mathcal{N}$ within the same complexity bound. In
particular, taking $n=1$, $m=1$ and $f_1=1$ in $\mathcal{M}$, this yields a
change of monomial order algorithm along the lines of the FGLM algorithm with a
complexity bound which is sub-cubic in $D$.
"
1417,Complexity of a Root Clustering Algorithm,"  Approximating the roots of a holomorphic function in an input box is a
fundamental problem in many domains. Most algorithms in the literature for
solving this problem are conditional, i.e., they make some simplifying
assumptions, such as, all the roots are simple or there are no roots on the
boundary of the input box, or the underlying machine model is Real RAM. Root
clustering is a generalization of the root approximation problem that allows
for errors in the computation and makes no assumption on the multiplicity of
the roots. An unconditional algorithm for computing a root clustering of a
holomorphic function was given by Yap, Sagraloff and Sharma in 2013. They
proposed a subdivision based algorithm using effective predicates based on
Pellet's test while avoiding any comparison with zeros (using soft zero
comparisons instead). In this paper, we analyze the running time of their
algorithm. We use the continuous amortization framework to derive an upper
bound on the size of the subdivision tree. We specialize this bound to the case
of polynomials and some simple transcendental functions such as exponential and
trigonometric sine. We show that the algorithm takes exponential time even for
these simple functions, unlike the case of polynomials. We also derive a bound
on the bit-precision used by the algorithm. To the best of our knowledge, this
is the first such result for holomorphic functions. We introduce new geometric
parameters, such as the relative growth of the function on the input box, for
analyzing the algorithm. Thus, our estimates naturally generalize the known
results, i.e., for the case of polynomials.
"
1418,"Modular Termination for Second-Order Computation Rules and Application
  to Algebraic Effect Handlers","  We present a new modular proof method of termination for second-order
computation, and report its implementation SOL. The proof method is useful for
proving termination of higher-order foundational calculi. To establish the
method, we use a variation of semantic labelling translation and Blanqui's
General Schema: a syntactic criterion of strong normalisation. As an
application, we apply this method to show termination of a variant of
call-by-push-value calculus with algebraic effects and effect handlers. We also
show that our tool SOLl is effective to solve higher-order termination
problems.
"
1419,"Building Executable Secure Design Models for Smart Contracts with Formal
  Methods","  Smart contracts are appealing because they are self-executing business
agreements between parties with the predefined and immutable obligations and
rights. However, as with all software, smart contracts may contain
vulnerabilities because of design flaws, which may be exploited by one of the
parties to defraud the others. In this paper, we demonstrate a systematic
approach to building secure design models for smart contracts using formal
methods. To build the secure models, we first model the behaviors of
participating parties as state machines, and then, we model the predefined
obligations and rights of contracts, which specify the interactions among state
machines for achieving the business goal. After that, we illustrate executable
secure model design patterns in TLA+ (Temporal Logic of Actions) to against
well-known smart contract vulnerabilities in terms of state machines and
obligations and rights at the design level. These vulnerabilities are found in
Ethereum contracts, including Call to the unknown, Gasless send, Reentrancy,
Lost in the transfer, and Unpredictable state. The resultant TLA+
specifications are called secure models. We illustrate our approach to detect
the vulnerabilities using a real-estate contract example at the design level.
"
1420,"A refined machinery to calculate large moments from coupled systems of
  linear differential equations","  The large moment method can be used to compute a large number of moments of
physical quantities that are described by coupled systems of linear
differential equations. Besides these systems the algorithm requires a certain
number of initial values as input, that are often hard to derive in a
preprocessing step.Thus a major challenge is to keep the number of initial
values as small as possible. We present the basic ideas of the underlying large
moment method and present refined versions that reduce significantly the number
of required initial values.
"
1421,A Fast Self-correcting $\pi$ Algorithm,"  We have rediscovered a simple algorithm to compute the mathematical constant
\[ \pi=3.14159265\cdots. \] The algorithm had been known for a long time but it
might not be recognized as a fast, practical algorithm. The time complexity of
it can be proved to be \[ O(M(n)\log^2 n) \] bit operations for computing $\pi$
with error $O(2^{-n})$, where $M(n)$ is the time complexity to multiply two
$n$-bit integers. We conjecture that the algorithm actually runs in \[
O(M(n)\log n). \] The algorithm is \emph{self-correcting} in the sense that,
given an approximated value of $\pi$ as an input, it can compute a more
accurate approximation of $\pi$ with cubic convergence.
"
1422,"Sparse Interpolation With Errors in Chebyshev Basis Beyond
  Redundant-Block Decoding","  We present sparse interpolation algorithms for recovering a polynomial with
$\le B$ terms from $N$ evaluations at distinct values for the variable when
$\le E$ of the evaluations can be erroneous. Our algorithms perform exact
arithmetic in the field of scalars $\mathsf{K}$ and the terms can be standard
powers of the variable or Chebyshev polynomials, in which case the
characteristic of $\mathsf{K}$ is $\ne 2$. Our algorithms return a list of
valid sparse interpolants for the $N$ support points and run in
polynomial-time. For standard power basis our algorithms sample at $N = \lfloor
\frac{4}{3} E + 2 \rfloor B$ points, which are fewer points than $N = 2(E+1)B -
1$ given by Kaltofen and Pernet in 2014. For Chebyshev basis our algorithms
sample at $N = \lfloor \frac{3}{2} E + 2 \rfloor B$ points, which are also
fewer than the number of points required by the algorithm given by Arnold and
Kaltofen in 2015, which has $N = 74 \lfloor \frac{E}{13} + 1 \rfloor$ for $B =
3$ and $E \ge 222$. Our method shows how to correct $2$ errors in a block of
$4B$ points for standard basis and how to correct $1$ error in a block of $3B$
points for Chebyshev Basis.
"
1423,"Evaluation of Chebyshev polynomials on intervals and application to root
  finding","  In approximation theory, it is standard to approximate functions by
polynomials expressed in the Chebyshev basis. Evaluating a polynomial $f$ of
degree n given in the Chebyshev basis can be done in $O(n)$ arithmetic
operations using the Clenshaw algorithm. Unfortunately, the evaluation of $f$
on an interval $I$ using the Clenshaw algorithm with interval arithmetic
returns an interval of width exponential in $n$. We describe a variant of the
Clenshaw algorithm based on ball arithmetic that returns an interval of width
quadratic in $n$ for an interval of small enough width. As an application, our
variant of the Clenshaw algorithm can be used to design an efficient root
finding algorithm.
"
1424,"Analysis of the Conradi-Kahle Algorithm for Detecting Binomiality on
  Biological Models","  We analyze the Conradi-Kahle Algorithm for detecting binomiality. We present
experiments using two implementations of the algorithm in Macaulay2 and Maple
on biological models and assess the performance of the algorithm on these
models. We compare the two implementations with each other and with Gr\""obner
bases computations up to their performance on these biological models.
"
1425,"Visualizing Planar and Space Implicit Real Algebraic Curves with
  Singularities","  We present a new method for visualizing implicit real algebraic curves inside
a bounding box in the $2$-D or $3$-D ambient space based on numerical
continuation and critical point methods. The underlying techniques work also
for tracing space curve in higher-dimensional space. Since the topology of a
curve near a singular point of it is not numerically stable, we trace only the
curve outside neighborhoods of singular points and replace each neighborhood
simply by a point, which produces a polygonal approximation that is
$\epsilon$-close to the curve. Such an approximation is more stable for
defining the numerical connectedness of the complement of the projection of the
curve in $\mathbb{R}^2$, which is important for applications such as solving
bi-parametric polynomial systems. The algorithm starts by computing three types
of key points of the curve, namely the intersection of the curve with small
spheres centered at singular points, regular critical points of every connected
components of the curve, as well as intersection points of the curve with the
given bounding box. It then traces the curve starting with and in the order of
the above three types of points. This basic scheme is further enhanced by
several optimizations, such as grouping singular points in natural clusters,
tracing the curve by a try-and-resume strategy and handling ""pseudo singular
points"". The effectiveness of the algorithm is illustrated by numerous
examples. This manuscript extends our preliminary results that appeared in CASC
2018.
"
1426,A sparse resultant based method for efficient minimal solvers,"  Many computer vision applications require robust and efficient estimation of
camera geometry. The robust estimation is usually based on solving camera
geometry problems from a minimal number of input data measurements, i.e.
solving minimal problems in a RANSAC framework. Minimal problems often result
in complex systems of polynomial equations. Many state-of-the-art efficient
polynomial solvers to these problems are based on Gr\""obner bases and the
action-matrix method that has been automatized and highly optimized in recent
years. In this paper we study an alternative algebraic method for solving
systems of polynomial equations, i.e., the sparse resultant-based method and
propose a novel approach to convert the resultant constraint to an eigenvalue
problem. This technique can significantly improve the efficiency and stability
of existing resultant-based solvers. We applied our new resultant-based method
to a large variety of computer vision problems and show that for most of the
considered problems, the new method leads to solvers that are the same size as
the the best available Gr\""obner basis solvers and of similar accuracy. For
some problems the new sparse-resultant based method leads to even smaller and
more stable solvers than the state-of-the-art Gr\""obner basis solvers. Our new
method can be fully automatized and incorporated into existing tools for
automatic generation of efficient polynomial solvers and as such it represents
a competitive alternative to popular Gr\""obner basis methods for minimal
problems in computer vision.
"
1427,Deeply Integrating C11 Code Support into Isabelle/PIDE,"  We present a framework for C code in C11 syntax deeply integrated into the
Isabelle/PIDE development environment. Our framework provides an abstract
interface for verification back-ends to be plugged-in independently. Thus,
various techniques such as deductive program verification or white-box testing
can be applied to the same source, which is part of an integrated PIDE document
model. Semantic back-ends are free to choose the supported C fragment and its
semantics. In particular, they can differ on the chosen memory model or the
specification mechanism for framing conditions.
  Our framework supports semantic annotations of C sources in the form of
comments. Annotations serve to locally control back-end settings, and can
express the term focus to which an annotation refers. Both the logical and the
syntactic context are available when semantic annotations are evaluated. As a
consequence, a formula in an annotation can refer both to HOL or C variables.
  Our approach demonstrates the degree of maturity and expressive power the
Isabelle/PIDE subsystem has achieved in recent years. Our integration technique
employs Lex and Yacc style grammars to ensure efficient deterministic parsing.
We present two case studies for the integration of (known) semantic back-ends
in order to validate the design decisions for our back-end interface.
"
1428,Towards Symbolic Factual Change in DEL,"  We extend symbolic model checking for Dynamic Epistemic Logic (DEL) with
factual change. Our transformers provide a compact representation of action
models with pre- and postconditions, for both S5 and the general case. The
method can be implemented using binary decision diagrams and we expect it to
improve model checking performance. As an example we give a symbolic
representation of the Sally-Anne false belief task.
"
1429,"Efficient Algorithm for the Linear Complexity of Sequences and Some
  Related Consequences","  The linear complexity of a sequence $s$ is one of the measures of its
predictability. It represents the smallest degree of a linear recursion which
the sequence satisfies. There are several algorithms to find the linear
complexity of a periodic sequence $s$ of length $N$ (where $N$ is of some given
form) over a finite field $F_q$ in $O(N)$ symbol field operations. The first
such algorithm is The Games-Chan Algorithm which considers binary sequences of
period $2^n$, and is known for its extreme simplicity. We generalize this
algorithm and apply it efficiently for several families of binary sequences.
Our algorithm is very simple, it requires $\beta N$ bit operations for a small
constant $\beta$, where $N$ is the period of the sequence. We make an analysis
on the number of bit operations required by the algorithm and compare it with
previous algorithms. In the process, the algorithm also finds the recursion for
the shortest linear feedback shift-register which generates the sequence. Some
other interesting properties related to shift-register sequences, which might
not be too surprising but generally unnoted, are also consequences of our
exposition.
"
1430,"Towards identification of explicit solutions to overdetermined systems
  of differential equations","  The authors proposed a general way to find particular solutions for
overdetermined systems of PDEs previously, where the number of equations is
greater than the number of unknown functions. In this paper, we propose an
algorithm for finding solutions for overdetermined PDE systems, where we use a
method for finding an explicit solution for overdetermined algebraic
(polynomial) equations. Using this algorithm, the solution of some
overdetermined PDE systems can be obtained in explicit form. The main
difficulty of this algorithm is the huge number of polynomial equations that
arise, which need to be investigated and solved numerically or explicitly. For
example, the overdetermined hydrodynamic equations obtained earlier by the
authors give a minimum of 10 million such equations. However, if they are
solved explicitly, then we can write out the solution of the hydrodynamic
equations in a general form, which is of great scientific interest.
"
1431,Differentiable Set Operations for Algebraic Expressions,"  Basic principles of set theory have been applied in the context of
probability and binary computation. Applying the same principles on
inequalities is less common but can be extremely beneficial in a variety of
fields. This paper formulates a novel approach to directly apply set operations
on inequalities to produce resultant inequalities with differentiable
boundaries. The suggested approach uses inequalities of the form Ei:
fi(x1,x2,..,xn) and an expression of set operations in terms of Ei like, (E1
and E2) or E3, or can be in any standard form like the Conjunctive Normal Form
(CNF) to produce an inequality F(x1,x2,..,xn)<=1 which represents the resulting
bounded region from the expressions and has a differentiable boundary. To
ensure differentiability of the solution, a trade-off between representation
accuracy and curvature at borders (especially corners) is made. A set of
parameters is introduced which can be fine-tuned to improve the accuracy of
this approach. The various applications of the suggested approach have also
been discussed which range from computer graphics to modern machine learning
systems to fascinating demonstrations for educational purposes (current use). A
python script to parse such expressions is also provided.
"
1432,Linear Programming using Limited-Precision Oracles,"  Since the elimination algorithm of Fourier and Motzkin, many different
methods have been developed for solving linear programs. When analyzing the
time complexity of LP algorithms, it is typically either assumed that
calculations are performed exactly and bounds are derived on the number of
elementary arithmetic operations necessary, or the cost of all arithmetic
operations is considered through a bit-complexity analysis. Yet in practice,
implementations typically use limited-precision arithmetic. In this paper we
introduce the idea of a limited-precision LP oracle and study how such an
oracle could be used within a larger framework to compute exact precision
solutions to LPs. Under mild assumptions, it is shown that a polynomial number
of calls to such an oracle and a polynomial number of bit operations, is
sufficient to compute an exact solution to an LP. This work provides a
foundation for understanding and analyzing the behavior of the methods that are
currently most effective in practice for solving LPs exactly.
"
1433,Intuitionistic Linear Temporal Logics,"  We consider intuitionistic variants of linear temporal logic with `next',
`until' and `release' based on expanding posets: partial orders equipped with
an order-preserving transition function. This class of structures gives rise to
a logic which we denote $\iltl$, and by imposing additional constraints we
obtain the logics $\itlb$ of persistent posets and $\itlht$ of here-and-there
temporal logic, both of which have been considered in the literature. We prove
that $\iltl$ has the effective finite model property and hence is decidable,
while $\itlb$ does not have the finite model property. We also introduce
notions of bounded bisimulations for these logics and use them to show that the
`until' and `release' operators are not definable in terms of each other, even
over the class of persistent posets.
"
1434,"Proof of the tree module property for exceptional representations of the
  quiver $\widetilde{\mathbb{E}}_6$","  This document (together with the ancillary file e6_proof.pdf) is an appendix
to the paper [12]. The ancillary file contains the computer generated part of
the proof of the main result in [12], giving a complete and general list of
tree representations corresponding to exceptional modules over the path algebra
of the canonically oriented Euclidean quiver $\widetilde{\mathbb{E}}_6$. The
proof (involving induction and symbolic computation with block matrices) was
partially generated by a purposefully developed computer software, outputting
in a detailed step-by-step fashion as if written ""by hand"".
  Tree representations are exhibited using matrices involving only the elements
0 and 1, and all representations enlisted in the ancillary document remain
valid over any base field.
"
1435,Stieltjes moment sequences for pattern-avoiding permutations,"  A small set of combinatorial sequences have coefficients that can be
represented as moments of a nonnegative measure on $[0, \infty)$. Such
sequences are known as Stieltjes moment sequences. This article focuses on some
classical sequences in enumerative combinatorics, denoted $Av(\mathcal{P})$,
and counting permutations of $\{1, 2, \ldots, n \}$ that avoid some given
pattern $\mathcal{P}$. For increasing patterns $\mathcal{P}=(12\ldots k)$, we
recall that the corresponding sequences, $Av(123\ldots k)$, are Stieltjes
moment sequences, and we explicitly find the underlying density function,
either exactly or numerically, by using the Stieltjes inversion formula as a
fundamental tool. We show that the generating functions of the sequences $\,
Av(1234)$ and $\, Av(12345)$ correspond, up to simple rational functions, to an
order-one linear differential operator acting on a classical modular form given
as a pullback of a Gaussian $\, _2F_1$ hypergeometric function, respectively to
an order-two linear differential operator acting on the square of a classical
modular form given as a pullback of a $\, _2F_1$ hypergeometric function. We
demonstrate that the density function for the Stieltjes moment sequence
$Av(123\ldots k)$ is closely, but non-trivially, related to the density
attached to the distance traveled by a walk in the plane with $k-1$ unit steps
in random directions. Finally, we study the challenging case of the $Av(1324)$
sequence and give compelling numerical evidence that this too is a Stieltjes
moment sequence. Accepting this, we show how rigorous lower bounds on the
growth constant of this sequence can be constructed, which are stronger than
existing bounds. A further unproven assumption leads to even better bounds,
which can be extrapolated to give an estimate of the (unknown) growth constant.
"
1436,A Condition for Multiplicity Structure of Univariate Polynomials,"  We consider the problem of finding a condition for a univariate polynomial
having a given multiplicity structure when the number of distinct roots is
given. It is well known that such conditions can be written as conjunctions of
several polynomial equations and one inequation in the coefficients, by using
repeated parametric gcd's. In this paper, we give a novel condition which is
not based on repeated gcd's. Furthermore, it is shown that the number of
polynomials in the condition is optimal and the degree of polynomials is
smaller than that in the previous condition based on repeated gcd's.
"
1437,On fast multiplication of a matrix by its transpose,"  We present a non-commutative algorithm for the multiplication of a
2x2-block-matrix by its transpose using 5 block products (3 recursive calls and
2 general products) over C or any finite field.We use geometric considerations
on the space of bilinear forms describing 2x2 matrix products to obtain this
algorithm and we show how to reduce the number of involved additions.The
resulting algorithm for arbitrary dimensions is a reduction of multiplication
of a matrix by its transpose to general matrix product, improving by a constant
factor previously known reductions.Finally we propose schedules with low memory
footprint that support a fast and memory efficient practical implementation
over a finite field.To conclude, we show how to use our result in LDLT
factorization.
"
1438,Bisimilar Conversion of Multi-valued Networks to Boolean Networks,"  Discrete modelling frameworks of Biological networks can be divided in two
distinct categories: Boolean and Multi-valued. Although Multi-valued networks
are more expressive for qualifying the regulatory behaviours modelled by more
than two values, the ability to automatically convert them to Boolean network
with an equivalent behaviour breaks down the fundamental borders between the
two approaches. Theoretically investigating the conversion process provides
relevant insights into bridging the gap between them. Basically, the conversion
aims at finding a Boolean network bisimulating a Multi-valued one. In this
article, we investigate the bisimilar conversion where the Boolean integer
coding is a parameter that can be freely modified. Based on this analysis, we
define a computational method automatically inferring a bisimilar Boolean
network from a given Multi-valued one.
"
1439,On mu-Symmetric Polynomials,"  In this paper, we study functions of the roots of a univariate polynomial in
which the roots have a given multiplicity structure $\mu$. Traditionally, root
functions are studied via the theory of symmetric polynomials; we extend this
theory to $\mu$-symmetric polynomials. We were motivated by a conjecture from
Becker et al.~(ISSAC 2016) about the $\mu$-symmetry of a particular root
function $D^+(\mu)$, called D-plus. To investigate this conjecture, it was
desirable to have fast algorithms for checking if a given root function is
$\mu$-symmetric. We designed three such algorithms: one based on Gr\""{o}bner
bases, another based on preprocessing and reduction, and the third based on
solving linear equations. We implemented them in Maple and experiments show
that the latter two algorithms are significantly faster than the first.
"
1440,Sparse Interpolation in Terms of Multivariate Chebyshev Polynomials,"  Sparse interpolation} refers to the exact recovery of a function as a short
linear combination of basis functions from a limited number of evaluations. For
multivariate functions, the case of the monomial basis is well studied, as is
now the basis of exponential functions. Beyond the multivariate Chebyshev
polynomial obtained as tensor products of univariate Chebyshev polynomials, the
theory of root systems allows to define a variety of generalized multivariate
Chebyshev polynomials that have connections to topics such as Fourier analysis
and representations of Lie algebras. We present a deterministic algorithm to
recover a function that is the linear combination of at most r such polynomials
from the knowledge of r and an explicitly bounded number of evaluations of this
function.
"
1441,On the Uniqueness Problem for Quadrature Domains,"  We study questions of existence and uniqueness of quadrature domains using
computational tools from real algebraic geometry. These problems are
transformed into questions about the number of solutions to an associated real
semi-algebraic system, which is analyzed using the method of real comprehensive
triangular decomposition.
"
1442,Smart Induction for Isabelle/HOL (System Description),"  Proof assistants offer tactics to facilitate inductive proofs. However, it
still requires human ingenuity to decide what arguments to pass to those
induction tactics. To automate this process, we present smart_induct for
Isabelle/HOL. Given an inductive problem in any problem domain, smart_induct
lists promising arguments for the induct tactic without relying on a search.
Our evaluation demonstrated smart_induct produces valuable recommendations
across problem domains.
"
1443,Essentially Optimal Sparse Polynomial Multiplication,"  We present a probabilistic algorithm to compute the product of two univariate
sparse polynomials over a field with a number of bit operations that is
quasi-linear in the size of the input and the output. Our algorithm works for
any field of characteristic zero or larger than the degree. We mainly rely on
sparse interpolation and on a new algorithm for verifying a sparse product that
has also a quasi-linear time complexity. Using Kronecker substitution
techniques we extend our result to the multivariate case.
"
1444,Unsatisfiability Proofs for Weight 16 Codewords in Lam's Problem,"  In the 1970s and 1980s, searches performed by L. Carter, C. Lam, L. Thiel,
and S. Swiercz showed that projective planes of order ten with weight 16
codewords do not exist. These searches required highly specialized and
optimized computer programs and required about 2,000 hours of computing time on
mainframe and supermini computers. In 2011, these searches were verified by D.
Roy using an optimized C program and 16,000 hours on a cluster of desktop
machines. We performed a verification of these searches by reducing the problem
to the Boolean satisfiability problem (SAT). Our verification uses the
cube-and-conquer SAT solving paradigm, symmetry breaking techniques using the
computer algebra system Maple, and a result of Carter that there are ten
nonisomorphic cases to check. Our searches completed in about 30 hours on a
desktop machine and produced nonexistence proofs of about 1 terabyte in the
DRAT (deletion resolution asymmetric tautology) format.
"
1445,Nonexistence Certificates for Ovals in a Projective Plane of Order Ten,"  In 1983, a computer search was performed for ovals in a projective plane of
order ten. The search was exhaustive and negative, implying that such ovals do
not exist. However, no nonexistence certificates were produced by this search,
and to the best of our knowledge the search has never been independently
verified. In this paper, we rerun the search for ovals in a projective plane of
order ten and produce a collection of nonexistence certificates that, when
taken together, imply that such ovals do not exist. Our search program uses the
cube-and-conquer paradigm from the field of satisfiability (SAT) checking,
coupled with a programmatic SAT solver and the nauty symbolic computation
library for removing symmetries from the search.
"
1446,Efficient q-Integer Linear Decomposition of Multivariate Polynomials,"  We present two new algorithms for the computation of the q-integer linear
decomposition of a multivariate polynomial. Such a decomposition is essential
in the q-analogous world of symbolic summation, for example, describing the
q-counterpart of Ore-Sato theory or determining the applicability of the
q-analogue of Zeilberger's algorithm to a q-hypergeometric term. Both of our
algorithms require only basic integer and polynomial arithmetic and work for
any unique factorization domain containing the ring of integers. Complete
complexity analyses are conducted for both our algorithms and two previous
algorithms in the case of multivariate integer polynomials, showing that our
algorithms have better theoretical performances. A Maple implementation is also
included which suggests that our algorithms are also much faster in practice
than previous algorithms.
"
1447,Linearly Constrained Gaussian Processes with Boundary Conditions,"  One goal in Bayesian machine learning is to encode prior knowledge into prior
distributions, to model data efficiently. We consider prior knowledge from
systems of linear (partial and ordinary) differential equations together with
their boundary conditions. We construct multi-output Gaussian process priors
with realizations dense in the solution set of such systems, in particular any
solution (and only such solutions) can be represented to arbitrary precision by
Gaussian process regression. The construction is fully algorithmic via
Gr\""obner bases and it does not employ any approximation. It builds these
priors combining two parametrizations via a pullback: the first parametrizes
the solutions for the system of differential equations and the second
parametrizes all functions adhering to the boundary conditions.
"
1448,Separating Variables in Bivariate Polynomial Ideals,"  We present an algorithm which for any given ideal $I\subseteq\mathbb{K}
[x,y]$ finds all elements of $I$ that have the form $f(x) - g(y)$, i.e., all
elements in which no monomial is a multiple of $xy$.
"
1449,"Convergence analysis of particle swarm optimization using stochastic
  Lyapunov functions and quantifier elimination","  This paper adds to the discussion about theoretical aspects of particle swarm
stability by proposing to employ stochastic Lyapunov functions and to determine
the convergence set by quantifier elimination. We present a computational
procedure and show that this approach leads to reevaluation and extension of
previously know stability regions for PSO using a Lyapunov approach under
stagnation assumptions.
"
1450,An Additive Decomposition in S-Primitive Towers,"  We consider the additive decomposition problem in primitive towers and
present an algorithm to decompose a function in an S-primitive tower as a sum
of a derivative in the tower and a remainder which is minimal in some sense.
Special instances of S-primitive towers include differential fields generated
by finitely many logarithmic functions and logarithmic integrals. A function in
an S-primitive tower is integrable in the tower if and only if the remainder is
equal to zero. The additive decomposition is achieved by viewing our towers not
as a traditional chain of extension fields, but rather as a direct sum of
certain subrings. Furthermore, we can determine whether or not a function in an
S-primitive tower has an elementary integral without solving any differential
equations. We also show that a kind of S-primitive towers, known as logarithmic
towers, can be embedded into a particular extension where we can obtain a finer
remainder.
"
1451,Integral P-Recursive Sequences,"  In an earlier paper, the notion of integrality known from algebraic number
fields and fields of algebraic functions has been extended to D-finite
functions. The aim of the present paper is to extend the notion to the case of
P-recursive sequences. In order to do so, we formulate a general algorithm for
finding all integral elements for valued vector spaces and then show that this
algorithm includes not only the algebraic and the D-finite cases but also
covers the case of P-recursive sequences.
"
1452,"The Fundamental Theorem of Tropical Partial Differential Algebraic
  Geometry","  Tropical Differential Algebraic Geometry considers difficult or even
intractable problems in Differential Equations and tries to extract information
on their solutions from a restricted structure of the input. The Fundamental
Theorem of Tropical Differential Algebraic Geometry states that the support of
solutions of systems of ordinary differential equations with formal power
series coefficients over an uncountable algebraically closed field of
characteristic zero can be obtained by solving a so-called tropicalized
differential system. Tropicalized differential equations work on a completely
different algebraic structure which may help in theoretical and computational
questions. We show that the Fundamental Theorem can be extended to the case of
systems of partial differential equations by introducing vertex sets of Newton
polygons.
"
1453,First-Order Tests for Toricity,"  Motivated by problems arising with the symbolic analysis of steady state
ideals in Chemical Reaction Network Theory, we consider the problem of testing
whether the points in a complex or real variety with non-zero coordinates form
a coset of a multiplicative group. That property corresponds to Shifted
Toricity, a recent generalization of toricity of the corresponding polynomial
ideal. The key idea is to take a geometric view on varieties rather than an
algebraic view on ideals. Recently, corresponding coset tests have been
proposed for complex and for real varieties. The former combine numerous
techniques from commutative algorithmic algebra with Gr\""obner bases as the
central algorithmic tool. The latter are based on interpreted first-order logic
in real closed fields with real quantifier elimination techniques on the
algorithmic side. Here we take a new logic approach to both theories, complex
and real, and beyond. Besides alternative algorithms, our approach provides a
unified view on theories of fields and helps to understand the relevance and
interconnection of the rich existing literature in the area, which has been
focusing on complex numbers, while from a scientific point of view the
(positive) real numbers are clearly the relevant domain in chemical reaction
network theory. We apply prototypical implementations of our new approach to a
set of 129 models from the BioModels repository.
"
1454,"Compatible rewriting of noncommutative polynomials for proving operator
  identities","  The goal of this paper is to prove operator identities using equalities
between noncommutative polynomials. In general, a polynomial expression is not
valid in terms of operators, since it may not be compatible with domains and
codomains of the corresponding operators. Recently, some of the authors
introduced a framework based on labelled quivers to rigorously translate
polynomial identities to operator identities. In the present paper, we extend
and adapt the framework to the context of rewriting and polynomial reduction.
We give a sufficient condition on the polynomials used for rewriting to ensure
that standard polynomial reduction automatically respects domains and codomains
of operators. Finally, we adapt the noncommutative Buchberger procedure to
compute additional compatible polynomials for rewriting. In the package
OperatorGB, we also provide an implementation of the concepts developed.
"
1455,Sparse Polynomial Interpolation Based on Diversification,"  We consider the problem of interpolating a sparse multivariate polynomial
over a finite field, represented with a black box. Building on the algorithm of
Ben-Or and Tiwari for interpolating polynomials over rings with characteristic
zero, we develop a new Monte Carlo algorithm over the finite field by doing
additional probes. To interpolate a polynomial $f\in F_q[x_1,\dots,x_n]$ with a
partial degree bound $D$ and a term bound $T$, our new algorithm costs
$O^\thicksim(nT\log ^2q+nT\sqrt{D}\log q)$ bit operations and uses $2(n+1)T$
probes to the black box. If $q\geq O(nT^2D)$, it has constant success rate to
return the correct polynomial. Compared with previous algorithms over general
finite field, our algorithm has better complexity in the parameters $n,T,D$ and
is the first one to achieve the complexity of fractional power about $D$, while
keeping linear in $n,T$. A key technique is a randomization which makes all
coefficients of the unknown polynomial distinguishable, producing a diverse
polynomial. This approach, called diversification, was proposed by Giesbrecht
and Roche in 2011. Our algorithm interpolates each variable independently using
$O(T)$ probes, and then uses the diversification to correlate terms in
different images. At last, we get the exponents by solving the discrete
logarithms and obtain coefficients by solving a linear system. We have
implemented our algorithm in Maple. Experimental results shows that our
algorithm can applied to sparse polynomials with large degree. We also analyze
the success rate of the algorithm.
"
1456,Sparse Polynomial Interpolation Based on Derivative,"  In this paper, we propose two new interpolation algorithms for sparse
multivariate polynomials represented by a straight-line program(SLP). Both of
our algorithms work over any finite fields $F_q$ with large characteristic. The
first one is a Monte Carlo randomized algorithm. Its arithmetic complexity is
linear in the number $T$ of non-zero terms of $f$, in the number $n$ of
variables. If $q$ is $O((nTD)^{(1)})$, where $D$ is the partial degree bound,
then our algorithm has better complexity than other existing algorithms. The
second one is a deterministic algorithm. It has better complexity than existing
deterministic algorithms over a field with large characteristic. Its arithmetic
complexity is quadratic in $n,T,\log D$, i.e., quadratic in the size of the
sparse representation. And we also show that the complexity of our
deterministic algorithm is the same as the one of deterministic zero-testing of
Bl\""{a}ser et al. for the polynomial given by an SLP over finite field (for
large characteristic).
"
1457,"Signature-based algorithms for Gr{\""o}bner bases over Tate algebras","  Introduced by Tate in [Ta71], Tate algebras play a major role in the context
of analytic geometry over the-adics, where they act as a counterpart to the use
of polynomial algebras in classical algebraic geometry. In [CVV19] the
formalism of Gr{\""o}bner bases over Tate algebras has been introduced and
effectively implemented. One of the bottleneck in the algorithms was the time
spent on reduction , which are significantly costlier than over polynomials. In
the present article, we introduce two signature-based Gr{\""o}bner bases
algorithms for Tate algebras, in order to avoid many reductions. They have been
implemented in SageMath. We discuss their superiority based on numerical
evidences.
"
1458,Smooth Points on Semi-algebraic Sets,"  Many algorithms for determining properties of real algebraic or
semi-algebraic sets rely upon the ability to compute smooth points. Existing
methods to compute smooth points on semi-algebraic sets use symbolic quantifier
elimination tools. In this paper, we present a simple algorithm based on
computing the critical points of some well-chosen function that guarantees the
computation of smooth points in each connected compact component of a real
(semi)-algebraic set. Our technique is intuitive in principal, performs well on
previously difficult examples, and is straightforward to implement using
existing numerical algebraic geometry software. The practical efficiency of our
approach is demonstrated by solving a conjecture on the number of equilibria of
the Kuramoto model for the $n=4$ case. We also apply our method to design an
efficient algorithm to compute the real dimension of (semi)-algebraic sets, the
original motivation for this research.
"
1459,"ENIGMA Anonymous: Symbol-Independent Inference Guiding Machine (system
  description)","  We describe an implementation of gradient boosting and neural guidance of
saturation-style automated theorem provers that does not depend on consistent
symbol names across problems. For the gradient-boosting guidance, we manually
create abstracted features by considering arity-based encodings of formulas.
For the neural guidance, we use symbol-independent graph neural networks (GNNs)
and their embedding of the terms and clauses. The two methods are efficiently
implemented in the E prover and its ENIGMA learning-guided framework.
  To provide competitive real-time performance of the GNNs, we have developed a
new context-based approach to evaluation of generated clauses in E. Clauses are
evaluated jointly in larger batches and with respect to a large number of
already selected clauses (context) by the GNN that estimates their collectively
most useful subset in several rounds of message passing. This means that
approximative inference rounds done by the GNN are efficiently interleaved with
precise symbolic inference rounds done inside E. The methods are evaluated on
the MPTP large-theory benchmark and shown to achieve comparable real-time
performance to state-of-the-art symbol-based methods. The methods also show
high complementarity, solving a large number of hard Mizar problems.
"
1460,"A divide-and-conquer algorithm for computing Gr\""obner bases of syzygies
  in finite dimension","  Let $f_1,\ldots,f_m$ be elements in a quotient $R^n / N$ which has finite
dimension as a $K$-vector space, where $R = K[X_1,\ldots,X_r]$ and $N$ is an
$R$-submodule of $R^n$. We address the problem of computing a Gr\""obner basis
of the module of syzygies of $(f_1,\ldots,f_m)$, that is, of vectors
$(p_1,\ldots,p_m) \in R^m$ such that $p_1 f_1 + \cdots + p_m f_m = 0$.
  An iterative algorithm for this problem was given by Marinari, M\""oller, and
Mora (1993) using a dual representation of $R^n / N$ as the kernel of a
collection of linear functionals. Following this viewpoint, we design a
divide-and-conquer algorithm, which can be interpreted as a generalization to
several variables of Beckermann and Labahn's recursive approach for matrix
Pad\'e and rational interpolation problems. To highlight the interest of this
method, we focus on the specific case of bivariate Pad\'e approximation and
show that it improves upon the best known complexity bounds.
"
1461,On the Uniqueness of Simultaneous Rational Function Reconstruction,"  This paper focuses on the problem of reconstructing a vector of rational
functions given some evaluations, or more generally given their remainders
modulo different polynomials. The special case of rational functions sharing
the same denominator, a.k.a.Simultaneous Rational Function Reconstruction
(SRFR), has many applications from linear system solving to coding theory,
provided that SRFR has a unique solution. The number of unknowns in SRFR is
smaller than for a general vector of rational function. This allows to reduce
the number of evaluation points needed to guarantee the existence of a
solution, but we may lose its uniqueness. In this work, we prove that
uniqueness is guaranteed for a generic instance.
"
1462,"Robust Numerical Tracking of One Path of a Polynomial Homotopy on
  Parallel Shared Memory Computers","  We consider the problem of tracking one solution path defined by a polynomial
homotopy on a parallel shared memory computer. Our robust path tracker applies
Newton's method on power series to locate the closest singular parameter value.
On top of that, it computes singular values of the Hessians of the polynomials
in the homotopy to estimate the distance to the nearest different path.
Together, these estimates are used to compute an appropriate adaptive stepsize.
For n-dimensional problems, the cost overhead of our robust path tracker is
O(n), compared to the commonly used predictor-corrector methods. This cost
overhead can be reduced by a multithreaded program on a parallel shared memory
computer.
"
1463,"Fast In-place Algorithms for Polynomial Operations: Division,
  Evaluation, Interpolation","  We consider space-saving versions of several important operations on
univariate polynomials, namely power series inversion and division, division
with remainder, multi-point evaluation, and interpolation. Now-classical
results show that such problems can be solved in (nearly) the same asymptotic
time as fast polynomial multiplication. However, these reductions, even when
applied to an in-place variant of fast polynomial multiplication, yield
algorithms which require at least a linear amount of extra space for
intermediate results. We demonstrate new in-place algorithms for the
aforementioned polynomial computations which require only constant extra space
and achieve the same asymptotic running time as their out-of-place
counterparts. We also provide a precise complexity analysis so that all
constants are made explicit, parameterized by the space usage of the underlying
multiplication algorithms.
"
1464,Space Efficient Representations of Finite Groups,"  The Cayley table representation of a group uses $\mathcal{O}(n^2)$ words for
a group of order $n$ and answers multiplication queries in time
$\mathcal{O}(1)$. It is interesting to ask if there is a $o(n^2)$ space
representation of groups that still has $\mathcal{O}(1)$ query-time. We show
that for any $\delta$, $\frac{1}{\log n} \le \delta \le 1$, there is an
$\mathcal{O}(\frac{n^{1 +\delta}}{\delta})$ space representation for groups of
order $n$ with $\mathcal{O}(\frac{1}{\delta})$ query-time.
  We also show that for Z-groups, simple groups and several group classes
defined in terms of semidirect product, there are linear space representations
with at most logarithmic query-time.
  Farzan and Munro (ISSAC'06) defined a model for group representation and gave
a succinct data structure for abelian groups with constant query-time. They
asked if their result can be extended to categorically larger group classes. We
construct data structures in their model for Hamiltonian groups and some other
classes of groups with constant query-time.
"
1465,Criteria for the numerical constant recognition,"  The need for recognition of numerical (decimal, floating-point) constants in
terms of elementary functions emerges in many areas of experimental
mathematics, numerical analysis, computer algebra systems, model building,
approximation and data compression. However, existing solutions are plagued by
lack of any criteria distinguishing between random formula, matching literally
decimal expansion (i.e. approximation) and probable ""exact"" (or at least
probable) expression match in the sense of Occam's razor. In particular,
convincing STOP criteria for search were never developed. In article, such a
criteria, working in statistical sense, are provided. Recognition process can
be viewed as (1) enumeration of all formulas in order of increasing Kolmogorov
complexity (2) random process with appropriate statistical distribution (3)
compression of a decimal string. All three approaches are remarkably
consistent, and provide essentially the same limit for practical depth of
search. Tested unique formulas count must not exceed 1/sigma, where sigma is
relative numerical error of the target constant. Beyond that, further search is
pointless, because, in the view of approach (1), number of equivalent
expressions within error bounds grows exponentially; in view of (2),
probability of random match approaches 1; in view of (3) compression ratio much
smaller than 1.
"
1466,"A Linear Algebra Approach for Detecting Binomiality of Steady State
  Ideals of Reversible Chemical Reaction Networks","  Motivated by problems from Chemical Reaction Network Theory, we investigate
whether steady state ideals of reversible reaction networks are generated by
binomials. We take an algebraic approach considering, besides concentrations of
species, also rate constants as indeterminates. This leads us to the concept of
unconditional binomiality, meaning binomiality for all values of the rate
constants. This concept is different from conditional binomiality that applies
when rate constant values or relations among rate constants are given. We start
by representing the generators of a steady state ideal as sums of binomials,
which yields a corresponding coefficient matrix. On these grounds we propose an
efficient algorithm for detecting unconditional binomiality. That algorithm
uses exclusively elementary column and row operations on the coefficient
matrix. We prove asymptotic worst case upper bounds on the time complexity of
our algorithm. Furthermore, we experimentally compare its performance with
other existing methods.
"
1467,"Effective Localization Using Double Ideal Quotient and Its
  Implementation","  In this paper, we propose a new method for localization of polynomial ideal,
which we call ""Local Primary Algorithm"". For an ideal $I$ and a prime ideal
$P$, our method computes a $P$-primary component of $I$ after checking if $P$
is associated with $I$ by using ""double ideal quotient"" $(I:(I:P))$ and its
variants which give us a lot of information about localization of $I$.
"
1468,"A complexity chasm for solving sparse polynomial equations over $p$-adic
  fields","  We reveal a complexity chasm, separating the trinomial and tetranomial cases,
for solving univariate sparse polynomial equations over certain local fields.
First, for any fixed field
$K\in\{\mathbb{Q}_2,\mathbb{Q}_3,\mathbb{Q}_5,\ldots\}$, we prove that any
polynomial $f\in\mathbb{Z}[x_1]$ with exactly $3$ monomial terms, degree $d$,
and all coefficients having absolute value at most $H$, can be solved over $K$
in deterministic time $\log^{O(1)}(dH)$ in the classical Turing model. (The
best previous algorithms were of complexity exponential in $\log d$, even for
just counting roots in $\mathbb{Q}_p$.) In particular, our algorithm generates
approximations in $\mathbb{Q}$ with bit-length $\log^{O(1)}(dH)$ to all the
roots of $f$ in $K$, and these approximations converge quadratically under
Newton iteration. On the other hand, we give a unified family of {\em
tetra}nomials requiring $\Omega(d\log H)$ bits to distinguish the base-$b$
expansions of their roots in $K$.
"
1469,Solving Satisfiability of Polynomial Formulas By Sample-Cell Projection,"  A new algorithm for deciding the satisfiability of polynomial formulas over
the reals is proposed. The key point of the algorithm is a new projection
operator, called sample-cell projection operator, custom-made for
Conflict-Driven Clause Learning (CDCL)-style search. Although the new operator
is also a CAD (Cylindrical Algebraic Decomposition)-like projection operator
which computes the cell (not necessarily cylindrical) containing a given sample
such that each polynomial from the problem is sign-invariant on the cell, it is
of singly exponential time complexity. The sample-cell projection operator can
efficiently guide CDCL-style search away from conflicting states. Experiments
show the effectiveness of the new algorithm.
"
1470,Maximum Absolute Determinants of Upper Hessenberg Bohemian Matrices,"  A matrix is called Bohemian if its entries are sampled from a finite set of
integers. We determine the maximum absolute determinant of upper Hessenberg
Bohemian Matrices for which the subdiagonal entries are fixed to be $1$ and
upper triangular entries are sampled from $\{0,1,\cdots,n\}$, extending
previous results for $n=1$ and $n=2$ and proving a recent conjecture of Fasi &
Negri Porzio [8]. Furthermore, we generalize the problem to non-integer-valued
entries.
"
1471,Modular Techniques for Effective Localization and Double Ideal Quotient,"  By double ideal quotient, we mean $(I:(I:J))$ where ideals $I$ and $J$. In
our previous work [11], double ideal quotient and its variants are shown to be
very useful for checking prime divisor and generating primary component.
Combining those properties, we can compute ""direct localization"" effectively,
comparing with full primary decomposition. In this paper, we apply modular
techniques effectively to computation of such double ideal quotient and its
variants, where first we compute them modulo several prime numbers and then
lift them up over rational numbers by Chinese Remainder Theorem and rational
reconstruction. As a new modular technique for double ideal quotient and its
variants, we devise criteria for output from modular computations. Also, we
apply modular techniques to intermediate primary decomposition. We examine the
effectiveness of our modular techniques for several examples by preliminary
computational experiences on Singular.
"
1472,"Enhancing simultaneous rational function recovery: adaptive error
  correction capability and new bounds for applications","  In this work we present some results that allow to improve the decoding
radius in solving polynomial linear systems with errors in the scenario where
errors are additive and randomly distributed over a finite field. The decoding
radius depends on some bounds on the solution that we want to recover, so their
overestimation could significantly decrease our error correction capability.
For this reason, we introduce an algorithm that can bridge this gap,
introducing some ad hoc parameters that reduce the discrepancy between the
estimate decoding radius and the effective error correction capability.
"
1473,"The Absent-Minded Passengers Problem: A Motivating Challenge Solved by
  Computer Algebra","  In (S.B. Ekhad and D. Zeilberger, 2020) an exciting case study has been
initiated in which experimental mathematics and symbolic computation are
utilized to discover new properties concerning the so-called Absent-Minded
Passengers Problem. Based on these results, Doron Zeilberger raised some
challenging tasks to gain further probabilistic insight. In this note we report
on this enterprise. In particular, we demonstrate how the computer algebra
packages of RISC can be used to carry out the underlying heavy calculations.
"
1474,Algorithm to enumerate superspecial Howe curves of genus $4$,"  A Howe curve is a curve of genus $4$ obtained as the fiber product over
$\mathbf{P}^1$ of two elliptic curves. Any Howe curve is canonical. This paper
provides an efficient algorithm to find superspecial Howe curves and that to
enumerate their isomorphism classes. We discuss not only an algorithm to test
the superspeciality but also an algorithm to test isomorphisms for Howe curves.
Our algorithms are much more efficient than conventional ones proposed by the
authors so far for general canonical curves. We show the existence of a
superspecial Howe curve in characteristic $7<p\le 331$ and enumerate the
isomorphism classes of superspecial Howe curves in characteristic $p\le 53$, by
executing our algorithms over the computer algebra system Magma.
"
1475,Neuro-symbolic Architectures for Context Understanding,"  Computational context understanding refers to an agent's ability to fuse
disparate sources of information for decision-making and is, therefore,
generally regarded as a prerequisite for sophisticated machine reasoning
capabilities, such as in artificial intelligence (AI). Data-driven and
knowledge-driven methods are two classical techniques in the pursuit of such
machine sense-making capability. However, while data-driven methods seek to
model the statistical regularities of events by making observations in the
real-world, they remain difficult to interpret and they lack mechanisms for
naturally incorporating external knowledge. Conversely, knowledge-driven
methods, combine structured knowledge bases, perform symbolic reasoning based
on axiomatic principles, and are more interpretable in their inferential
processing; however, they often lack the ability to estimate the statistical
salience of an inference. To combat these issues, we propose the use of hybrid
AI methodology as a general framework for combining the strengths of both
approaches. Specifically, we inherit the concept of neuro-symbolism as a way of
using knowledge-bases to guide the learning progress of deep neural networks.
We further ground our discussion in two applications of neuro-symbolism and, in
both cases, show that our systems maintain interpretability while achieving
comparable performance, relative to the state-of-the-art.
"
1476,Entropy of tropical holonomic sequences,"  We introduce tropical holonomic sequences of a given order and calculate
their entropy in case of the second order.
"
1477,"Deciding the Consistency of Non-Linear Real Arithmetic Constraints with
  a Conflict Driven Search Using Cylindrical Algebraic Coverings","  We present a new algorithm for determining the satisfiability of conjunctions
of non-linear polynomial constraints over the reals, which can be used as a
theory solver for satisfiability modulo theory (SMT) solving for non-linear
real arithmetic. The algorithm is a variant of Cylindrical Algebraic
Decomposition (CAD) adapted for satisfiability, where solution candidates
(sample points) are constructed incrementally, either until a satisfying sample
is found or sufficient samples have been sampled to conclude unsatisfiability.
The choice of samples is guided by the input constraints and previous
conflicts.
  The key idea behind our new approach is to start with a partial sample;
demonstrate that it cannot be extended to a full sample; and from the reasons
for that rule out a larger space around the partial sample, which build up
incrementally into a cylindrical algebraic covering of the space. There are
similarities with the incremental variant of CAD, the NLSAT method of
Jovanovi\'{c} and de~Moura, and the NuCAD algorithm of Brown; but we present
worked examples and experimental results on a preliminary implementation to
demonstrate the differences to these, and the benefits of the new approach.
"
1478,FunGrim: a symbolic library for special functions,"  We present the Mathematical Functions Grimoire (FunGrim), a website and
database of formulas and theorems for special functions. We also discuss the
symbolic computation library used as the backend and main development tool for
FunGrim, and the Grim formula language used in these projects to represent
mathematical content semantically.
"
1479,Experimental Evaluation of a Method to Simplify Expressions,"  We present a method to simplify expressions in the context of an equational
theory. The basic ideas and concepts of the method have been presented
previously elsewhere but here we tackle the difficult task of making it
efficient in practice, in spite of its great generality. We first recall the
notion of a collection of structures, which allows us to manipulate very large
(possibly infinite) sets of terms as a whole, i.e., without enumerating their
elements. Then we use this tool to construct algorithms to simplify
expressions. We give various reasons why it is difficult to make these
algorithms precise and efficient. We then propose a number of approches to
solve the raised issues. Finally, and importantly, we provide a detailed
experimental evaluation of the method and a comparison of several variants of
it. Although the method is completely generic, we use (arbitrary, not only
two-level) boolean expressions as the application field for these experiments
because impressive simplifications can be obtained in spite of the hardness of
the problem.
"
1480,"Transforming ODEs and PDEs with radical coefficients into rational
  coefficients","  We present an algorithm that transforms, if possible, a given ODE or PDE with
radical function coefficients into one with rational coefficients by means of a
rational change of variables. It also applies to systems of linear ODEs. It is
based on previous work on reparametrization of radical algebraic varieties.
"
1481,"An Algorithm for Computing a Minimal Comprehensive Gr\""obner\, Basis of
  a Parametric Polynomial System","  An algorithm to generate a minimal comprehensive Gr\""obner\, basis of a
parametric polynomial system from an arbitrary faithful comprehensive
Gr\""obner\, system is presented. A basis of a parametric polynomial ideal is a
comprehensive Gr\""obner\, basis if and only if for every specialization of
parameters in a given field, the specialization of the basis is a Gr\""obner\,
basis of the associated specialized polynomial ideal. The key idea used in
ensuring minimality is that of a polynomial being essential with respect to a
comprehensive Gr\""obner\, basis. The essentiality check is performed by
determining whether a polynomial can be covered for various specializations by
other polynomials in the associated branches in a comprehensive Gr\""obner\,
system. The algorithm has been implemented and successfully tried on many
examples from the literature.
"
1482,"Moment State Dynamical Systems for Nonlinear Chance-Constrained Motion
  Planning","  Chance-constrained motion planning requires uncertainty in dynamics to be
propagated into uncertainty in state. When nonlinear models are used, Gaussian
assumptions on the state distribution do not necessarily apply since almost all
random variables propagated through nonlinear dynamics results in non-Gaussian
state distributions. To address this, recent works have developed moment-based
approaches for enforcing chance-constraints on non-Gaussian state
distributions. However, there still lacks fast and accurate moment propagation
methods to determine the necessary statistical moments of these state
distributions. To address this gap, we present a framework that, given a
stochastic dynamical system, can algorithmically search for a new dynamical
system in terms of moment state that can be used to propagate moments of
disturbance random variables into moments of the state distribution. The key
algorithm, TreeRing, can be applied to a large class of nonlinear systems which
we refer to as trigonometric polynomial systems. As an example application, we
present a distributionally robust RRT (DR-RRT) algorithm that propagates
uncertainty through the nonlinear Dubin's car model without linearization.
"
1483,"Generic bivariate multi-point evaluation, interpolation and modular
  composition with precomputation","  Suppose $\mathbb{K}$ is a large enough field and $\mathcal{P} \subset
\mathbb{K}^2$ is a fixed, generic set of points which is available for
precomputation. We introduce a technique called \emph{reshaping} which allows
us to design quasi-linear algorithms for both: computing the evaluations of an
input polynomial $f \in \mathbb{K}[x,y]$ at all points of $\mathcal{P}$; and
computing an interpolant $f \in \mathbb{K}[x,y]$ which takes prescribed values
on $\mathcal{P}$ and satisfies an input $y$-degree bound. Our genericity
assumption is explicit and we prove that it holds for most point sets over a
large enough field. If $\mathcal{P}$ violates the assumption, our algorithms
still work and the performance degrades smoothly according to a distance from
being generic. To show that the reshaping technique may have an impact on other
related problems, we apply it to modular composition: suppose generic
polynomials $M \in \mathbb{K}[x]$ and $A \in \mathbb{K}[x]$ are available for
precomputation, then given an input $f \in \mathbb{K}[x,y]$ we show how to
compute $f(x, A(x)) \operatorname{rem} M(x)$ in quasi-linear time.
"
1484,Fast Encoding of AG Codes over $C_{ab}$ Curves,"  We investigate algorithms for encoding of one-point algebraic geometry (AG)
codes over certain plane curves called $C_{ab}$ curves, as well as algorithms
for inverting the encoding map, which we call ""unencoding"". Some $C_{ab}$
curves have many points or are even maximal, e.g. the Hermitian curve. Our
encoding resp. unencoding algorithms have complexity $\tilde{O}(n^{3/2})$ resp.
$\tilde{O}(qn)$ for AG codes over any $C_{ab}$ curve satisfying very mild
assumptions, where $n$ is the code length and $q$ the base field size, and
$\tilde{O}$ ignores constants and logarithmic factors in the estimate. For
codes over curves whose evaluation points lie on a grid-like structure, notably
the Hermitian curve and norm-trace curves, we show that our algorithms have
quasi-linear time complexity $\tilde{O}(n)$ for both operations. For infinite
families of curves whose number of points is a constant factor away from the
Hasse--Weil bound, our encoding algorithm has complexity $\tilde{O}(n^{5/4})$
while unencoding has $\tilde{O}(n^{3/2})$.
"
1485,"Parallel Computation of tropical varieties, their positive part, and
  tropical Grassmannians","  In this article, we present a massively parallel framework for computing
tropicalizations of algebraic varieties which can make use of finite
symmetries. We compute the tropical Grassmannian TGr$_0(3,8)$, and show that it
refines the $15$-dimensional skeleton of the Dressian Dr$(3,8)$ with the
exception of $23$ special cones for which we construct explicit obstructions to
the realizability of their tropical linear spaces. Moreover, we propose
algorithms for identifying maximal-dimensional tropical cones which belong to
the positive tropicalization. These algorithms exploit symmetries of the
tropical variety even though the positive tropicalization need not be
symmetric. We compute the maximal-dimensional cones of the positive
Grassmannian TGr$^+(3,8)$ and compare them to the cluster complex of the
classical Grassmannian Gr$(3,8)$.
"
1486,"Stream/block ciphers, difference equations and algebraic attacks","  In this paper we introduce a general class of stream and block ciphers that
are defined by means of systems of (ordinary) explicit difference equations
over a finite field. We call this class ""difference ciphers"". Many important
ciphers such as systems of LFSRs, Trivium/Bivium and Keeloq are difference
ciphers. To the purpose of studying their underlying explicit difference
systems, we introduce key notions as state transition endomorphisms and show
conditions for their invertibility. Reducible and periodic systems are also
considered. We then propose general algebraic attacks to difference ciphers
which are experimented by means of Bivium and Keeloq.
"
1487,Explosive Proofs of Mathematical Truths,"  Mathematical proofs are both paradigms of certainty and some of the most
explicitly-justified arguments that we have in the cultural record. Their very
explicitness, however, leads to a paradox, because their probability of error
grows exponentially as the argument expands. Here we show that under a
cognitively-plausible belief formation mechanism that combines deductive and
abductive reasoning, mathematical arguments can undergo what we call an
epistemic phase transition: a dramatic and rapidly-propagating jump from
uncertainty to near-complete confidence at reasonable levels of claim-to-claim
error rates. To show this, we analyze an unusual dataset of forty-eight
machine-aided proofs from the formalized reasoning system Coq, including major
theorems ranging from ancient to 21st Century mathematics, along with four
hand-constructed cases from Euclid, Apollonius, Spinoza, and Andrew Wiles. Our
results bear both on recent work in the history and philosophy of mathematics,
and on a question, basic to cognitive science, of how we form beliefs, and
justify them to others.
"
1488,"Interpolation of Dense and Sparse Rational Functions and other
  Improvements in $\texttt{FireFly}$","  We present the main improvements and new features in version $\texttt{2.0}$
of the open-source $\texttt{C++}$ library $\texttt{FireFly}$ for the
interpolation of rational functions. This includes algorithmic improvements,
e.g. a hybrid algorithm for dense and sparse rational functions and an
algorithm to identify and remove univariate factors. The new version is applied
to a Feynman-integral reduction to showcase the runtime improvements achieved.
Moreover, $\texttt{FireFly}$ now supports parallelization with $\texttt{MPI}$
and offers new tools like a parser for expressions or an executable for the
insertion of replacement tables.
"
1489,Resultants over principal Artinian rings,"  The resultant of two univariate polynomials is an invariant of great
importance in commutative algebra and vastly used in computer algebra systems.
Here we present an algorithm to compute it over Artinian principal rings with a
modified version of the Euclidean algorithm. Using the same strategy, we show
how the reduced resultant and a pair of B\'ezout coefficient can be computed.
Particular attention is devoted to the special case of
$\mathbf{Z}/n\mathbf{Z}$, where we perform a detailed analysis of the
asymptotic cost of the algorithm. Finally, we illustrate how the algorithms can
be exploited to improve ideal arithmetic in number fields and polynomial
arithmetic over $p$-adic fields.
"
1490,Neural Analogical Matching,"  Analogy is core to human cognition. It allows us to solve problems based on
prior experience, it governs the way we conceptualize new information, and it
even influences our visual perception. The importance of analogy to humans has
made it an active area of research in the broader field of artificial
intelligence, resulting in data-efficient models that learn and reason in
human-like ways. While analogy and deep learning have generally been studied
independently of one another, the integration of the two lines of research
seems like a promising step towards more robust and efficient learning
techniques. As part of the first steps towards such an integration, we
introduce the Analogical Matching Network: a neural architecture that learns to
produce analogies between structured, symbolic representations that are largely
consistent with the principles of Structure-Mapping Theory.
"
1491,New Opportunities for the Formal Proof of Computational Real Geometry?,"  The purpose of this paper is to explore the question ""to what extent could we
produce formal, machine-verifiable, proofs in real algebraic geometry?"" The
question has been asked before but as yet the leading algorithms for answering
such questions have not been formalised. We present a thesis that a new
algorithm for ascertaining satisfiability of formulae over the reals via
Cylindrical Algebraic Coverings [\'{A}brah\'{a}m, Davenport, England, Kremer,
\emph{Deciding the Consistency of Non-Linear Real Arithmetic Constraints with a
Conflict Driver Search Using Cylindrical Algebraic Coverings}, 2020] might
provide trace and outputs that allow the results to be more susceptible to
machine verification than those of competing algorithms.
"
1492,"A Simple Method for Computing Some Pseudo-Elliptic Integrals in Terms of
  Elementary Functions","  We introduce a method for computing some pseudo-elliptic integrals in terms
of elementary functions. The method is simple and fast in comparison to the
algebraic case of the Risch-Trager-Bronstein algorithm. This method can quickly
solve many pseudo-elliptic integrals, which other well-known computer algebra
systems either fail, return an answer in terms of special functions, or require
more than 20 seconds of computing time. Randomised tests showed our method
solved 73.4% of the integrals that could be solved with the best implementation
of the Risch-Trager-Bronstein algorithm. Unlike the symbolic integration
algorithms of Risch, Davenport, Trager, Bronstein and Miller; our method is not
a decision process. The implementation of this method is less than 200 lines of
Mathematica code and can be easily ported to other CAS that can solve systems
of polynomial equations.
"
1493,Computing all identifiable functions for ODE models,"  Parameter identifiability is a structural property of an ODE model for
recovering the values of parameters from the data (i.e., from the input and
output variables). This property is a prerequisite for meaningful parameter
identification in practice. In the presence of nonidentifiability, it is
important to find all functions of the parameters that are identifiable. The
existing algorithms check whether a given function of parameters is
identifiable or, under the solvability condition, find all identifiable
functions. Our first main result is an algorithm that computes all identifiable
functions without any additional assumptions. Our second main result concerns
the identifiability from multiple experiments. For this problem, we show that
the set of functions identifiable from multiple experiments is what would
actually be computed by input-output equation-based algorithms if the
solvability condition is not fulfilled. We give an algorithm that not only
finds these functions but also provides an upper bound for the number of
experiments to be performed to identify these functions.
"
1494,A case study for $\zeta(4)$,"  Using symbolic summation tools in the setting of difference rings, we prove a
two-parametric identity that relates rational approximations to $\zeta(4)$.
"
1495,"A practical approach to testing random number generators in computer
  algebra systems","  This paper has a practical aim. For a long time, implementations of
pseudorandom number generators in standard libraries of programming languages
had poor quality. The situation started to improve only recently. Up to now, a
large number of libraries and weakly supported mathematical packages use
outdated algorithms for random number generation. Four modern sets of
statistical tests that can be used for verifying random number generators are
described. It is proposed to use command line utilities, which makes it
possible to avoid low-level programming in such languages as C or C++. Only
free open source systems are considered.
"
1496,"An Efficient Method for Computing Liouvillian First Integrals of Planar
  Polynomial Vector Fields","  Here we present an efficient method to compute Darboux polynomials for
polynomial vector fields in the plane. This approach is restricetd to
polynomial vector fields presenting a Liouvillian first integral (or,
equivalently, to rational first order differential equations (rational 1ODEs)
presenting a Liouvillian general solution). The key to obtaining this method
was to separate the procedure of solving the (nonlinear) algebraic systems
resulting from the equation that translates the condition of existence of a
Darboux polynomial into feasible steos (procedures that requires less memory
consumption). We also present a brief performance analysis of the algorithms
developed.
"
1497,The Imandra Automated Reasoning System (system description),"  We describe Imandra, a modern computational logic theorem prover designed to
bridge the gap between decision procedures such as SMT, semi-automatic
inductive provers of the Boyer-Moore family like ACL2, and interactive proof
assistants for typed higher-order logics. Imandra's logic is computational,
based on a pure subset of OCaml in which all functions are terminating, with
restrictions on types and higher-order functions that allow conjectures to be
translated into multi-sorted first-order logic with theories, including
arithmetic and datatypes. Imandra has novel features supporting large-scale
industrial applications, including a seamless integration of bounded and
unbounded verification, first-class computable counterexamples, efficiently
executable models and a cloud-native architecture supporting live multiuser
collaboration.
  The core reasoning mechanisms of Imandra are (i) a semi-complete procedure
for finding models of formulas in the logic mentioned above, centered around
the lazy expansion of recursive functions, and (ii) an inductive waterfall and
simplifier which ""lifts"" many Boyer-Moore ideas to our typed higher-order
setting.
  These mechanisms are tightly integrated and subject to many forms of user
control. Imandra's user interfaces include an interactive toplevel, Jupyter
notebooks and asynchronous document-based verification (in the spirit of
Isabelle's Prover IDE) with VS Code.
"
1498,High performance SIMD modular arithmetic for polynomial evaluation,"  Two essential problems in Computer Algebra, namely polynomial factorization
and polynomial greatest common divisor computation, can be efficiently solved
thanks to multiple polynomial evaluations in two variables using modular
arithmetic. In this article, we focus on the efficient computation of such
polynomial evaluations on one single CPU core. We first show how to leverage
SIMD computing for modular arithmetic on AVX2 and AVX-512 units, using both
intrinsics and OpenMP compiler directives. Then we manage to increase the
operational intensity and to exploit instruction-level parallelism in order to
increase the compute efficiency of these polynomial evaluations. All this
results in the end to performance gains up to about 5x on AVX2 and 10x on
AVX-512.
"
1499,GAPS: Generator for Automatic Polynomial Solvers,"  Minimal problems in computer vision raise the demand of generating efficient
automatic solvers for polynomial equation systems. Given a polynomial system
repeated with different coefficient instances, the traditional Gr\""obner basis
or normal form based solution is very inefficient. Fortunately the Gr\""obner
basis of a same polynomial system with different coefficients is found to share
consistent inner structure. By precomputing such structures offline, Gr\""obner
basis as well as the polynomial system solutions can be solved automatically
and efficiently online. In the past decade, several tools have been released to
generate automatic solvers for a general minimal problems. The most recent tool
autogen from Larsson et al. is a representative of these tools with
state-of-the-art performance in solver efficiency. GAPS wraps and improves
autogen with more user-friendly interface, more functionality and better
stability. We demonstrate in this report the main approach and enhancement
features of GAPS. A short tutorial of the software is also included.
"
1500,"An Abstraction-guided Approach to Scalable and Rigorous Floating-Point
  Error Analysis","  Automated techniques for rigorous floating-point round-off error analysis are
important in areas including formal verification of correctness and precision
tuning. Existing tools and techniques, while providing tight bounds, fail to
analyze expressions with more than a few hundred operators, thus unable to
cover important practical problems. In this work, we present Satire, a new tool
that sheds light on how scalability and bound-tightness can be attained through
a combination of incremental analysis, abstraction, and judicious use of
concrete and symbolic evaluation. Satire has handled problems exceeding 200K
operators. We present Satire's underlying error analysis approach,
information-theoretic abstraction heuristics, and a wide range of case studies,
with evaluation covering FFT, Lorenz system of equations, and various PDE
stencil types. Our results demonstrate the tightness of Satire's bounds, its
acceptable runtime, and valuable insights provided.
"
1501,"CLUE: Exact maximal reduction of kinetic models by constrained lumping
  of differential equations","  Detailed mechanistic models of biological processes can pose significant
challenges for analysis and parameter estimations due to the large number of
equations used to track the dynamics of all distinct configurations in which
each involved biochemical species can be found. Model reduction can help tame
such complexity by providing a lower-dimensional model in which each
macro-variable can be directly related to the original variables.
  We present CLUE, an algorithm for exact model reduction of systems of
polynomial differential equations by constrained linear lumping. It computes
the smallest dimensional reduction as a linear mapping of the state space such
that the reduced model preserves the dynamics of user-specified linear
combinations of the original variables. Even though CLUE works with nonlinear
differential equations, it is based on linear algebra tools, which makes it
applicable to high-dimensional models. Using case studies from the literature,
we show how CLUE can substantially lower model dimensionality and help extract
biologically intelligible insights from the reduction.
  An implementation of the algorithm and relevant resources to replicate the
experiments herein reported are freely available for download at
https://github.com/pogudingleb/CLUE.
"
1502,Iterative Variable Reordering: Taming Huge System Families,"  For the verification of systems using model-checking techniques, symbolic
representations based on binary decision diagrams (BDDs) often help to tackle
the well-known state-space explosion problem. Symbolic BDD-based
representations have been also shown to be successful for the analysis of
families of systems that arise, e.g., through configurable parameters or
following the feature-oriented modeling approach. The state space of such
system families face an additional exponential blowup in the number of
parameters or features. It is well known that the order of variables in ordered
BDDs is crucial for the size of the model representation. Especially for
automatically generated models from real-world systems, family models might
even be not constructible due to bad variable orders. In this paper we describe
a technique, called iterative variable reordering, that can enable the
construction of large-scale family models. We exemplify feasibility of our
approach by means of an aircraft velocity control system with redundancy
mechanisms modeled in the input language of the probabilistic model checker
PRISM. We show that standard reordering and dynamic reordering techniques fail
to construct the family model due to memory and time constraints, respectively,
while the new iterative approach succeeds to generate a symbolic family model.
"
1503,It is Time for New Perspectives on How to Fight Bloat in GP,"  The present and future of evolutionary algorithms depends on the proper use
of modern parallel and distributed computing infrastructures. Although still
sequential approaches dominate the landscape, available multi-core, many-core
and distributed systems will make users and researchers to more frequently
deploy parallel version of the algorithms. In such a scenario, new
possibilities arise regarding the time saved when parallel evaluation of
individuals are performed. And this time saving is particularly relevant in
Genetic Programming. This paper studies how evaluation time influences not only
time to solution in parallel/distributed systems, but may also affect size
evolution of individuals in the population, and eventually will reduce the
bloat phenomenon GP features. This paper considers time and space as two sides
of a single coin when devising a more natural method for fighting bloat. This
new perspective allows us to understand that new methods for bloat control can
be derived, and the first of such a method is described and tested.
Experimental data confirms the strength of the approach: using computing time
as a measure of individuals' complexity allows to control the growth in size of
genetic programming individuals.
"
1504,"Rational Solutions of First Order Algebraic Ordinary Differential
  Equations","  Let $f(t, y,y')=\sum_{i=0}^d a_i(t, y)y'^i=0$ be a first order ordinary
differential equation with polynomial coefficients. Eremenko in 1999 proved
that there exists a constant $C$ such that every rational solution of $f(t,
y,y')=0$ is of degree not greater than $C$. Examples show that this degree
bound $C$ depends not only on the degrees of $f$ in $t,y,y'$ but also on the
coefficients of $f$ viewed as polynomial in $t,y,y'$. In this paper, we show
that if $$\max_{i=0}^d \{{\rm deg}(a_i,y)-2(d-i)\}>0 $$ then the degree bound
$C$ only depends on the degrees of $f$, and furthermore we present an explicit
expression for $C$ in terms of the degrees of $f$.
"
1505,Algorithms yield upper bounds in differential algebra,"  Consider an algorithm computing in a differential field with several
commuting derivations such that the only operations it performs with the
elements of the field are arithmetic operations, differentiation, and zero
testing. We show that, if the algorithm is guaranteed to terminate on every
input, then there is a computable upper bound for the size of the output of the
algorithm in terms of the input. We also generalize this to algorithms working
with models of good enough theories (including for example, difference fields).
  We then apply this to differential algebraic geometry to show that there
exists a computable uniform upper bound for the number of components of any
variety defined by a system of polynomial PDEs. We then use this bound to show
the existence of a computable uniform upper bound for the elimination problem
in systems of polynomial PDEs with delays.
"
1506,Learning selection strategies in Buchberger's algorithm,"  Studying the set of exact solutions of a system of polynomial equations
largely depends on a single iterative algorithm, known as Buchberger's
algorithm. Optimized versions of this algorithm are crucial for many computer
algebra systems (e.g., Mathematica, Maple, Sage). We introduce a new approach
to Buchberger's algorithm that uses reinforcement learning agents to perform
S-pair selection, a key step in the algorithm. We then study how the difficulty
of the problem depends on the choices of domain and distribution of
polynomials, about which little is known. Finally, we train a policy model
using proximal policy optimization (PPO) to learn S-pair selection strategies
for random systems of binomial equations. In certain domains, the trained model
outperforms state-of-the-art selection heuristics in total number of polynomial
additions performed, which provides a proof-of-concept that recent developments
in machine learning have the potential to improve performance of algorithms in
symbolic computation.
"
1507,"Characterizing Triviality of the Exponent Lattice of A Polynomial
  through Galois and Galois-Like Groups","  The problem of computing \emph{the exponent lattice} which consists of all
the multiplicative relations between the roots of a univariate polynomial has
drawn much attention in the field of computer algebra. As is known, almost all
irreducible polynomials with integer coefficients have only trivial exponent
lattices. However, the algorithms in the literature have difficulty in proving
such triviality for a generic polynomial. In this paper, the relations between
the Galois group (respectively, \emph{the Galois-like groups}) and the
triviality of the exponent lattice of a polynomial are investigated. The
$\bbbq$\emph{-trivial} pairs, which are at the heart of the relations between
the Galois group and the triviality of the exponent lattice of a polynomial,
are characterized. An effective algorithm is developed to recognize these
pairs. Based on this, a new algorithm is designed to prove the triviality of
the exponent lattice of a generic irreducible polynomial, which considerably
improves a state-of-the-art algorithm of the same type when the polynomial
degree becomes larger. In addition, the concept of the Galois-like groups of a
polynomial is introduced. Some properties of the Galois-like groups are proved
and, more importantly, a sufficient and necessary condition is given for a
polynomial (which is not necessarily irreducible) to have trivial exponent
lattice.
"
1508,"Probing the Natural Language Inference Task with Automated Reasoning
  Tools","  The Natural Language Inference (NLI) task is an important task in modern NLP,
as it asks a broad question to which many other tasks may be reducible: Given a
pair of sentences, does the first entail the second? Although the
state-of-the-art on current benchmark datasets for NLI are deep learning-based,
it is worthwhile to use other techniques to examine the logical structure of
the NLI task. We do so by testing how well a machine-oriented controlled
natural language (Attempto Controlled English) can be used to parse NLI
sentences, and how well automated theorem provers can reason over the resulting
formulae. To improve performance, we develop a set of syntactic and semantic
transformation rules. We report their performance, and discuss implications for
NLI and logic-based NLP.
"
1509,"Towards Concise, Machine-discovered Proofs of G\""odel's Two
  Incompleteness Theorems","  There is an increasing interest in applying recent advances in AI to
automated reasoning, as it may provide useful heuristics in reasoning over
formalisms in first-order, second-order, or even meta-logics. To facilitate
this research, we present MATR, a new framework for automated theorem proving
explicitly designed to easily adapt to unusual logics or integrate new
reasoning processes. MATR is formalism-agnostic, highly modular, and
programmer-friendly. We explain the high-level design of MATR as well as some
details of its implementation. To demonstrate MATR's utility, we then describe
a formalized metalogic suitable for proofs of G\""odel's Incompleteness
Theorems, and report on our progress using our metalogic in MATR to
semi-autonomously generate proofs of both the First and Second Incompleteness
Theorems.
"
1510,"Algorithmic Averaging for Studying Periodic Orbits of Planar
  Differential Systems","  One of the main open problems in the qualitative theory of real planar
differential systems is the study of limit cycles. In this article, we present
an algorithmic approach for detecting how many limit cycles can bifurcate from
the periodic orbits of a given polynomial differential center when it is
perturbed inside a class of polynomial differential systems via the averaging
method. We propose four symbolic algorithms to implement the averaging method.
The first algorithm is based on the change of polar coordinates that allows one
to transform a considered differential system to the normal form of averaging.
The second algorithm is used to derive the solutions of certain differential
systems associated to the unperturbed term of the normal of averaging. The
third algorithm exploits the partial Bell polynomials and allows one to compute
the integral formula of the averaged functions at any order. The last algorithm
is based on the aforementioned algorithms and determines the exact expressions
of the averaged functions for the considered differential systems. The
implementation of our algorithms is discussed and evaluated using several
examples. The experimental results have extended the existing relevant results
for certain classes of differential systems.
"
1511,Subquadratic-Time Algorithms for Normal Bases,"  For any finite Galois field extension $\mathsf{K}/\mathsf{F}$, with Galois
group $G = \mathrm{Gal}(\mathsf{K}/\mathsf{F})$, there exists an element
$\alpha \in \mathsf{K}$ whose orbit $G\cdot\alpha$ forms an $\mathsf{F}$-basis
of $\mathsf{K}$. Such an $\alpha$ is called a normal element and $G\cdot\alpha$
is a normal basis. We introduce a probabilistic algorithm for testing whether a
given $\alpha \in \mathsf{K}$ is normal, when $G$ is either a finite abelian or
a metacyclic group. The algorithm is based on the fact that deciding whether
$\alpha$ is normal can be reduced to deciding whether $\sum_{g \in G}
g(\alpha)g \in \mathsf{K}[G]$ is invertible; it requires a slightly
subquadratic number of operations. Once we know that $\alpha$ is normal, we
show how to perform conversions between the working basis of
$\mathsf{K}/\mathsf{F}$ and the normal basis with the same asymptotic cost.
"
1512,Efficient Exact Verification of Binarized Neural Networks,"  Concerned with the reliability of neural networks, researchers have developed
verification techniques to prove their robustness. Most verifiers work with
real-valued networks. Unfortunately, the exact (complete and sound) verifiers
face scalability challenges and provide no correctness guarantees due to
floating point errors. We argue that Binarized Neural Networks (BNNs) provide
comparable robustness and allow exact and significantly more efficient
verification. We present a new system, EEV, for efficient and exact
verification of BNNs. EEV consists of two parts: (i) a novel SAT solver that
speeds up BNN verification by natively handling the reified cardinality
constraints arising in BNN encodings; and (ii) strategies to train
solver-friendly robust BNNs by inducing balanced layer-wise sparsity and low
cardinality bounds, and adaptively cancelling the gradients. We demonstrate the
effectiveness of EEV by presenting the first exact verification results for
L-inf-bounded adversarial robustness of nontrivial convolutional BNNs on the
MNIST and CIFAR10 datasets. Compared to exact verification of real-valued
networks of the same architectures on the same tasks, EEV verifies BNNs
hundreds to thousands of times faster, while delivering comparable verifiable
accuracy in most cases.
"
1513,On the complexity of computing integral bases of function fields,"  Let $\mathcal{C}$ be a plane curve given by an equation $f(x,y)=0$ with $f\in
K[x][y]$ a monic squarefree polynomial. We study the problem of computing an
integral basis of the algebraic function field $K(\mathcal{C})$ and give new
complexity bounds for three known algorithms dealing with this problem. For
each algorithm, we study its subroutines and, when it is possible, we modify or
replace them so as to take advantage of faster primitives. Then, we combine
complexity results to derive an overall complexity estimate for each algorithm.
In particular, we modify an algorithm due to B\""ohm et al. and achieve a
quasi-optimal runtime.
"
1514,"On Rational and Hypergeometric Solutions of Linear Ordinary Difference
  Equations in $\Pi\mathbf\Sigma^*$-field extensions","  We present a complete algorithm that computes all hypergeometric solutions of
homogeneous linear difference equations and rational solutions of parameterized
linear difference equations in the setting of $\Pi\Sigma^*$-fields. More
generally, we provide a flexible framework for a big class of difference fields
that is built by a tower of $\Pi\Sigma^*$-field extensions over a difference
field that satisfies certain algorithmic properties. As a consequence one can
compute all solutions in terms of indefinite nested sums and products that
arise within the components of a parameterized linear difference equation, and
one can find all hypergeometric solutions that are defined over the arising
sums and products of a homogeneous linear difference equation.
"
1515,Towards Efficient Normalizers of Primitive Groups,"  We present the ideas behind an algorithm to compute normalizers of primitive
groups with non-regular socle in polynomial time. We highlight a concept we
developed called permutation morphisms and present timings for a partial
implementation of our algorithm. This article is a collection of results from
the author's PhD thesis.
"
1516,Positional Games and QBF: The Corrective Encoding,"  Positional games are a mathematical class of two-player games comprising
Tic-tac-toe and its generalizations. We propose a novel encoding of these games
into Quantified Boolean Formulas (QBF) such that a game instance admits a
winning strategy for first player if and only if the corresponding formula is
true. Our approach improves over previous QBF encodings of games in multiple
ways. First, it is generic and lets us encode other positional games, such as
Hex. Second, structural properties of positional games together with a careful
treatment of illegal moves let us generate more compact instances that can be
solved faster by state-of-the-art QBF solvers. We establish the latter fact
through extensive experiments. Finally, the compactness of our new encoding
makes it feasible to translate realistic game problems. We identify a few such
problems of historical significance and put them forward to the QBF community
as milestones of increasing difficulty.
"
1517,A modular extension for a computer algebra system,"  Computer algebra systems are complex software systems that cover a wide range
of scientific and practical problems. However, the absolute coverage cannot be
achieved. Often, it is required to create a user extension for an existing
computer algebra system. In this case, the extensibility of the system should
be taken into account. In this paper, we consider a technology for extending
the SymPy computer algebra system with a low-level module that implements a
random number generator.
"
1518,The Extended Theory of Trees and Algebraic (Co)datatypes,"  The first-order theory of finite and infinite trees has been studied since
the eighties, especially by the logic programming community. Following
Djelloul, Dao and Fr\""uhwirth, we consider an extension of this theory with an
additional predicate for finiteness of trees, which is useful for expressing
properties about (not just datatypes but also) codatatypes. Based on their
work, we present a simplification procedure that determines whether any given
(not necessarily closed) formula is satisfiable, returning a simplified formula
which enables one to read off all possible models. Our extension makes the
algorithm usable for algebraic (co)datatypes, which was impossible in their
original work due to restrictive assumptions. We also provide a prototype
implementation of our simplification procedure and evaluate it on instances
from the SMT-LIB.
"
1519,Generalizing The Davenport-Mahler-Mignotte Bound -- The Weighted Case,"  Root separation bounds play an important role as a complexity measure in
understanding the behaviour of various algorithms in computational algebra,
e.g., root isolation algorithms. A classic result in the univariate setting is
the Davenport-Mahler-Mignotte (DMM) bound. One way to state the bound is to
consider a directed acyclic graph $(V,E)$ on a subset of roots of a degree $d$
polynomial $f(z) \in \mathbb{C}[z]$, where the edges point from a root of
smaller absolute value to one of larger absolute, and the in-degrees of all
vertices is at most one. Then the DMM bound is an amortized lower bound on the
following product: $\prod_{(\alpha,\beta) \in E}|\alpha-\beta|$. However, the
lower bound involves the discriminant of the polynomial $f$, and becomes
trivial if the polynomial is not square-free. This was resolved by Eigenwillig,
(2008), by using a suitable subdiscriminant instead of the discriminant.
Escorcielo-Perrucci, 2016, further dropped the in-degree constraint on the
graph by using the theory of finite differences. Emiris et al., 2019, have
generalized their result to handle the case where the exponent of the term
$|\alpha-\beta|$ in the product is at most the multiplicity of either of the
roots. In this paper, we generalize these results by allowing arbitrary
positive integer weights on the edges of the graph, i.e., for a weight function
$w: E \rightarrow \mathbb{Z}_{>0}$, we derive an amortized lower bound on
$\prod_{(\alpha,\beta) \in E}|\alpha-\beta|^{w(\alpha,\beta)}$. Such a product
occurs in the complexity estimates of some recent algorithms for root
clustering (e.g., Becker et al., 2016), where the weights are usually some
function of the multiplicity of the roots. Because of its amortized nature, our
bound is arguably better than the bounds obtained by manipulating existing
results to accommodate the weights.
"
1520,Neural Collaborative Reasoning,"  Collaborative Filtering (CF) has been an important approach to recommender
systems. However, existing CF methods are mostly designed based on the idea of
matching, i.e., by learning user and item embeddings from data using shallow or
deep models, they try to capture the relevance patterns in data, so that a user
embedding can be matched with appropriate item embeddings using designed or
learned similarity functions. We argue that as a cognition rather than a
perception intelligent task, recommendation requires not only the ability of
pattern recognition and matching from data, but also the ability of logical
reasoning in the data.
  Inspired by recent progress on neural-symbolic machine learning, we propose a
neural collaborative reasoning framework to integrate the power of embedding
learning and logical reasoning, where the embeddings capture similarity
patterns in data from perceptual perspectives, and the logic facilitates
cognitive reasoning for informed decision making. An important challenge,
however, is to bridge differentiable neural networks and symbolic reasoning in
a shared architecture for optimization and inference. To solve the problem, we
propose a Modularized Logical Neural Network architecture, which learns basic
logical operations such as AND, OR, and NOT as neural modules based on logical
regularizer, and learns logic variables as vector embeddings. In this way, each
logic expression can be equivalently organized as a neural network, so that
logical reasoning and prediction can be conducted in a continuous space.
Experiments on several real-world datasets verified the advantages of our
framework compared with both traditional shallow and deep models.
"
1521,An Algebraic Model For Quorum Systems,"  Quorum systems are a key mathematical abstraction in distributed
fault-tolerant computing for capturing trust assumptions. A quorum system is a
collection of subsets of all processes, called quorums, with the property that
each pair of quorums have a non-empty intersection. They can be found at the
core of many reliable distributed systems, such as cloud computing platforms,
distributed storage systems and blockchains. In this paper we give a new
interpretation of quorum systems, starting with classical majority-based quorum
systems and extending this to Byzantine quorum systems. We propose an algebraic
representation of the theory underlying quorum systems making use of
multivariate polynomial ideals, incorporating properties of these systems, and
studying their algebraic varieties. To achieve this goal we will exploit
properties of Boolean Groebner bases. The nice nature of Boolean Groebner bases
allows us to avoid part of the combinatorial computations required to check
consistency and availability of quorum systems. Our results provide a novel
approach to test quorum systems properties from both algebraic and algorithmic
perspectives.
"
1522,Pegasus: Sound Continuous Invariant Generation,"  Continuous invariants are an important component in deductive verification of
hybrid and continuous systems. Just like discrete invariants are used to reason
about correctness in discrete systems without having to unroll their loops,
continuous invariants are used to reason about differential equations without
having to solve them. Automatic generation of continuous invariants remains one
of the biggest practical challenges to the automation of formal proofs of
safety for hybrid systems. There are at present many disparate methods
available for generating continuous invariants; however, this wealth of diverse
techniques presents a number of challenges, with different methods having
different strengths and weaknesses. To address some of these challenges, we
develop Pegasus: an automatic continuous invariant generator which allows for
combinations of various methods, and integrate it with the KeYmaera X theorem
prover for hybrid systems. We describe some of the architectural aspects of
this integration, comment on its methods and challenges, and present an
experimental evaluation on a suite of benchmarks.
"
1523,"Applying Genetic Programming to Improve Interpretability in Machine
  Learning Models","  Explainable Artificial Intelligence (or xAI) has become an important research
topic in the fields of Machine Learning and Deep Learning. In this paper, we
propose a Genetic Programming (GP) based approach, named Genetic Programming
Explainer (GPX), to the problem of explaining decisions computed by AI systems.
The method generates a noise set located in the neighborhood of the point of
interest, whose prediction should be explained, and fits a local explanation
model for the analyzed sample. The tree structure generated by GPX provides a
comprehensible analytical, possibly non-linear, symbolic expression which
reflects the local behavior of the complex model. We considered three machine
learning techniques that can be recognized as complex black-box models: Random
Forest, Deep Neural Network and Support Vector Machine in twenty data sets for
regression and classifications problems. Our results indicate that the GPX is
able to produce more accurate understanding of complex models than the state of
the art. The results validate the proposed approach as a novel way to deploy GP
to improve interpretability.
"
1524,"Fast Decoding of Codes in the Rank, Subspace, and Sum-Rank Metric","  We speed up existing decoding algorithms for three code classes in different
metrics: interleaved Gabidulin codes in the rank metric, lifted interleaved
Gabidulin codes in the subspace metric, and linearized Reed-Solomon codes in
the sum-rank metric. The speed-ups are achieved by reducing the core of the
underlying computational problems of the decoders to one common tool: computing
left and right approximant bases of matrices over skew polynomial rings. To
accomplish this, we describe a skew-analogue of the existing PM-Basis algorithm
for matrices over usual polynomials. This captures the bulk of the work in
multiplication of skew polynomials, and the complexity benefit comes from
existing algorithms performing this faster than in classical quadratic
complexity. The new faster algorithms for the various decoding-related
computational problems are interesting in their own and have further
applications, in particular parts of decoders of several other codes and
foundational problems related to the remainder-evaluation of skew polynomials.
"
1525,"A machine learning based software pipeline to pick the variable ordering
  for algorithms with polynomial inputs","  We are interested in the application of Machine Learning (ML) technology to
improve mathematical software. It may seem that the probabilistic nature of ML
tools would invalidate the exact results prized by such software, however, the
algorithms which underpin the software often come with a range of choices which
are good candidates for ML application. We refer to choices which have no
effect on the mathematical correctness of the software, but do impact its
performance.
  In the past we experimented with one such choice: the variable ordering to
use when building a Cylindrical Algebraic Decomposition (CAD). We used the
Python library Scikit-Learn (sklearn) to experiment with different ML models,
and developed new techniques for feature generation and hyper-parameter
selection.
  These techniques could easily be adapted for making decisions other than our
immediate application of CAD variable ordering. Hence in this paper we present
a software pipeline to use sklearn to pick the variable ordering for an
algorithm that acts on a polynomial system. The code described is freely
available online.
"
1526,miniKanren as a Tool for Symbolic Computation in Python,"  In this article, we give a brief overview of the current state and future
potential of symbolic computation within the Python statistical modeling and
machine learning community. We detail the use of miniKanren as an underlying
framework for term rewriting and symbolic mathematics, as well as its ability
to orchestrate the use of existing Python libraries. We also discuss the
relevance and potential of relational programming for implementing more robust,
portable, domain-specific ""math-level"" optimizations--with a slight focus on
Bayesian modeling. Finally, we describe the work going forward and raise some
questions regarding potential cross-overs between statistical modeling and
programming language theory.
"
1527,First Neural Conjecturing Datasets and Experiments,"  We describe several datasets and first experiments with creating conjectures
by neural methods. The datasets are based on the Mizar Mathematical Library
processed in several forms and the problems extracted from it by the MPTP
system and proved by the E prover using the ENIGMA guidance. The conjecturing
experiments use the Transformer architecture and in particular its GPT-2
implementation.
"
1528,"Nonlinear observability algorithms with known and unknown inputs:
  analysis and implementation","  The observability of a dynamical system is affected by the presence of
external inputs, either known (such as control actions) or unknown
(disturbances). Inputs of unknown magnitude are especially detrimental for
observability, and they also complicate its analysis. Hence the availability of
computational tools capable of analysing the observability of nonlinear systems
with unknown inputs has been limited until lately. Two symbolic algorithms
based on differential geometry, ORC-DF and FISPO, have been recently proposed
for this task, but their critical analysis and comparison is still lacking.
Here we perform an analytical comparison of both algorithms and evaluate their
performance on a set of problems, discussing their strengths and limitations.
Additionally, we use these analyses to provide insights about certain aspects
of the relationship between inputs and observability. We find that, while
ORC-DF and FISPO follow a similar approach, they differ in key aspects that can
have a substantial influence on their applicability and computational cost. The
FISPO algorithm is more generally applicable, since it can analyse any
nonlinear ODE model. The ORC-DF algorithm analyses models that are affine in
the inputs, and if those models have known inputs it is sometimes more
efficient. Thus, the optimal choice of a method depends on the characteristics
of the problem under consideration. To facilitate the use of both algorithms we
implement the ORC-DF algorithm in a new version of STRIKE-GOLDD, a MATLAB
toolbox for structural identifiability and observability analysis. Since this
software tool already had an implementation of the FISPO algorithm, the new
release allows modellers and model users the convenience of choosing between
different algorithms in a single tool, without changing the coding of their
model.
"
1529,Good pivots for small sparse matrices,"  For sparse matrices up to size $8 \times 8$, we determine optimal choices for
pivot selection in Gaussian elimination. It turns out that they are slightly
better than the pivots chosen by a popular pivot selection strategy, so there
is some room for improvement. We then create a pivot selection strategy using
machine learning and find that it indeed leads to a small improvement compared
to the classical strategy.
"
1530,"Characterizing an Analogical Concept Memory for Architectures
  Implementing the Common Model of Cognition","  Architectures that implement the Common Model of Cognition - Soar, ACT-R, and
Sigma - have a prominent place in research on cognitive modeling as well as on
designing complex intelligent agents. In this paper, we explore how
computational models of analogical processing can be brought into these
architectures to enable concept acquisition from examples obtained
interactively. We propose a new analogical concept memory for Soar that
augments its current system of declarative long-term memories. We frame the
problem of concept learning as embedded within the larger context of
interactive task learning (ITL) and embodied language processing (ELP). We
demonstrate that the analogical learning methods implemented in the proposed
memory can quickly learn a diverse types of novel concepts that are useful not
only in recognition of a concept in the environment but also in action
selection. Our approach has been instantiated in an implemented cognitive
system \textsc{Aileen} and evaluated on a simulated robotic domain.
"
1531,Analogical Proportions,"  Analogy-making is at the core of human intelligence and creativity with
applications to such diverse tasks as commonsense reasoning, learning, language
acquisition, and story telling. This paper contributes to the foundations of
artificial general intelligence by introducing an abstract algebraic framework
of analogical proportions of the form `$a$ is to $b$ what $c$ is to $d$' in the
general setting of universal algebra. This enables us to compare mathematical
objects possibly across different domains in a uniform way which is crucial for
AI-systems. The main idea is to define solutions to analogical equations in
terms of generalizations and to derive abstract terms of concrete elements from
a `known' source domain which can then be instantiated in an `unknown' target
domain to obtain analogous elements. We extensively compare our framework with
two prominent and recently introduced frameworks of analogical proportions from
the literature in the concrete domains of sets, numbers, and words and show
that our framework yields strictly more reasonable solutions in all of these
cases which provides evidence for the applicability of our framework. In a
broader sense, this paper is a first step towards an algebraic theory of
analogical reasoning and learning systems with potential applications to
fundamental AI-problems like commonsense reasoning and computational learning
and creativity.
"
1532,Decomposable sparse polynomial systems,"  The Macaulay2 package DecomposableSparseSystems implements methods for
studying and numerically solving decomposable sparse polynomial systems. We
describe the structure of decomposable sparse systems and explain how the
methods in this package may be used to exploit this structure, with examples.
"
1533,"Walsh functions, scrambled $(0,m,s)$-nets, and negative covariance:
  applying symbolic computation to quasi-Monte Carlo integration","  We investigate base $b$ Walsh functions for which the variance of the
integral estimator based on a scrambled $(0,m,s)$-net in base $b$ is less than
or equal to that of the Monte-Carlo estimator based on the same number of
points. First we compute the Walsh decomposition for the joint probability
density function of two distinct points randomly chosen from a scrambled
$(t,m,s)$-net in base $b$ in terms of certain counting numbers and simplify it
in the special case $t$ is zero. Using this, we obtain an expression for the
covariance of the integral estimator in terms of the Walsh coefficients of the
function. Finally, we prove that the covariance of the integral estimator is
negative when the Walsh coefficients of the function satisfy a certain decay
condition. To do this, we use creative telescoping and recurrence solving
algorithms from symbolic computation to find a sign equivalent closed form
expression for the covariance term.
"
1534,"Pointer Data Structure Synthesis from Answer Set Programming
  Specifications","  We develop an inductive proof-technique to generate imperative programs for
pointer data structures from behavioural specifications expressed in the Answer
Set Programming (ASP) formalism. ASP is a non-monotonic logic based formalism
that employs negation-as-failure which helps emulate the human thought process,
allowing domain experts to model desired system behaviour succinctly. We argue
in this paper that ASP's reliance on negation-as-failure makes it a better
formalism than those based on first-order logic for writing formal
specifications. We assume the a domain expert provides the representation of
inductively defined data structures along with a specification of its
operations. Our procedures combined with our novel proof-technique reason over
the specifications and automatically generate an imperative program. Our
proof-technique leverages the idea of partial deduction to simplify logical
specifications. By algebraically simplifying logical specifications we arrive
at a residual specification which can be interpreted as an appropriate
imperative program. This work is in the realm of constructing programs that are
correct according to a given specification.
"
1535,"Computing Igusa's local zeta function of univariates in deterministic
  polynomial-time","  Igusa's local zeta function $Z_{f,p}(s)$ is the generating function that
counts the number of integral roots, $N_{k}(f)$, of $f(\mathbf x) \bmod p^k$,
for all $k$. It is a famous result, in analytic number theory, that $Z_{f,p}$
is a rational function in $\mathbb{Q}(p^s)$. We give an elementary proof of
this fact for a univariate polynomial $f$. Our proof is constructive as it
gives a closed-form expression for the number of roots $N_{k}(f)$.
  Our proof, when combined with the recent root-counting algorithm of (Dwivedi,
Mittal, Saxena, CCC, 2019), yields the first deterministic poly($|f|, \log p$)
time algorithm to compute $Z_{f,p}(s)$. Previously, an algorithm was known only
in the case when $f$ completely splits over $\mathbb{Q}_p$; it required the
rational roots to use the concept of generating function of a tree
(Z\'u\~niga-Galindo, J.Int.Seq., 2003).
"
1536,On the Complexity of Solving Generic Over-determined Bilinear Systems,"  In this paper, we study the complexity of solving generic over-determined
bilinear systems over a finite field $\mathbb{F}$. Given a generic bilinear
sequence $B \in \mathbb{F}[\mathbf{x},\mathbf{y}]$, with respect to a partition
of variables $\mathbf{x}$, $\mathbf{y}$, we show that, the solutions of the
system $B= \mathbf{0}$ can be efficiently found on the
$\mathbb{F}[\mathbf{y}]$-module generated by $B$. Following this observation,
we propose three variations of Gr\""obner basis algorithms, that only involve
multiplication by monomials in they-variables, namely, $\mathbf{y}$-XL, based
on the XL algorithm, $\mathbf{y}$-MLX, based on the mutant XL algorithm, and
$\mathbf{y}$-HXL, basedon a hybrid approach. We define notions of regularity
for over-determined bilinear systems,that capture the idea of genericity, and
we develop the necessary theoretical tools to estimate the complexity of the
algorithms for such sequences. We also present extensive experimental results,
testing our conjecture, verifying our results, and comparing the complexity of
the various methods.
"
1537,"Logic, Probability and Action: A Situation Calculus Perspective","  The unification of logic and probability is a long-standing concern in AI,
and more generally, in the philosophy of science. In essence, logic provides an
easy way to specify properties that must hold in every possible world, and
probability allows us to further quantify the weight and ratio of the worlds
that must satisfy a property. To that end, numerous developments have been
undertaken, culminating in proposals such as probabilistic relational models.
While this progress has been notable, a general-purpose first-order knowledge
representation language to reason about probabilities and dynamics, including
in continuous settings, is still to emerge. In this paper, we survey recent
results pertaining to the integration of logic, probability and actions in the
situation calculus, which is arguably one of the oldest and most well-known
formalisms. We then explore reduction theorems and programming interfaces for
the language. These results are motivated in the context of cognitive robotics
(as envisioned by Reiter and his colleagues) for the sake of concreteness.
Overall, the advantage of proving results for such a general language is that
it becomes possible to adapt them to any special-purpose fragment, including
but not limited to popular probabilistic relational models.
"
1538,Toric Eigenvalue Methods for Solving Sparse Polynomial Systems,"  We consider the problem of computing homogeneous coordinates of points in a
zero-dimensional subscheme of a compact toric variety $X$. Our starting point
is a homogeneous ideal $I$ in the Cox ring of $X$, which gives a global
description of this subscheme. It was recently shown that eigenvalue methods
for solving this problem lead to robust numerical algorithms for solving
(nearly) degenerate sparse polynomial systems. In this work, we give a first
description of this strategy for non-reduced, zero-dimensional subschemes of
$X$. That is, we allow isolated points with arbitrary multiplicities.
Additionally, we investigate the regularity of $I$ to provide the first
universal complexity bounds for the approach, as well as sharper bounds for
weighted homogeneous, multihomogeneous and unmixed sparse systems, among
others. We disprove a recent conjecture regarding the regularity and prove an
alternative version. Our contributions are illustrated by several examples.
"
1539,A unified framework for equivalences in social networks,"  A key concern in network analysis is the study of social positions and roles
of actors in a network. The notion of ""position"" refers to an equivalence class
of nodes that have similar ties to other nodes, whereas a ""role"" is an
equivalence class of compound relations that connect the same pairs of nodes.
An open question in network science is whether it is possible to simultaneously
perform role and positional analysis. Motivated by the principle of
functoriality in category theory we propose a new method that allows to tie
role and positional analysis together. We illustrate our methods on two
well-studied data sets in network science.
"
1540,"Neuro-Symbolic Visual Reasoning: Disentangling ""Visual"" from ""Reasoning""","  Visual reasoning tasks such as visual question answering (VQA) require an
interplay of visual perception with reasoning about the question semantics
grounded in perception. However, recent advances in this area are still
primarily driven by perception improvements (e.g. scene graph generation)
rather than reasoning. Neuro-symbolic models such as Neural Module Networks
bring the benefits of compositional reasoning to VQA, but they are still
entangled with visual representation learning, and thus neural reasoning is
hard to improve and assess on its own. To address this, we propose (1) a
framework to isolate and evaluate the reasoning aspect of VQA separately from
its perception, and (2) a novel top-down calibration technique that allows the
model to answer reasoning questions even with imperfect perception. To this
end, we introduce a differentiable first-order logic formalism for VQA that
explicitly decouples question answering from visual perception. On the
challenging GQA dataset, this framework is used to perform in-depth,
disentangled comparisons between well-known VQA models leading to informative
insights regarding the participating models as well as the task.
"
1541,"Quantum Runge-Lenz Vector and the Hydrogen Atom, the hidden SO(4)
  symmetry using Computer Algebra","  Pauli first noticed the hidden SO(4) symmetry for the Hydrogen atom in the
early stages of quantum mechanics [1]. Departing from that symmetry, one can
recover the spectrum of a spinless hydrogen atom and the degeneracy of its
states without explicitly solving Schr\""odinger's equation [2]. In this paper,
we derive that SO(4) symmetry and spectrum using a computer algebra system
(CAS). While this problem is well known [3, 4], its solution involves several
steps of manipulating expressions with tensorial quantum operators, simplifying
them by taking into account a combination of commutator rules and Einstein's
sum rule for repeated indices. Therefore, it is an excellent model to test the
current status of CAS concerning this kind of quantum-and-tensor-algebra
computations. Generally speaking, when capable, CAS can significantly help with
manipulations that, like non-commutative tensor calculus subject to algebra
rules, are tedious, time-consuming and error-prone. The presentation also shows
a pattern of computer algebra operations that can be useful for systematically
tackling more complicated symbolic problems of this kind.
"
1542,Noetherian operators and primary decomposition,"  Noetherian operators are differential operators that encode primary
components of a polynomial ideal. We develop a framework, as well as
algorithms, for computing Noetherian operators with local dual spaces, both
symbolically and numerically. For a primary ideal, such operators provide an
alternative representation to one given by a set of generators. This
description fits well with numerical algebraic geometry, taking a step toward
the goal of numerical primary decomposition.
"
1543,Machine learning the real discriminant locus,"  Parameterized systems of polynomial equations arise in many applications in
science and engineering with the real solutions describing, for example,
equilibria of a dynamical system, linkages satisfying design constraints, and
scene reconstruction in computer vision. Since different parameter values can
have a different number of real solutions, the parameter space is decomposed
into regions whose boundary forms the real discriminant locus. This article
views locating the real discriminant locus as a supervised classification
problem in machine learning where the goal is to determine classification
boundaries over the parameter space, with the classes being the number of real
solutions. For multidimensional parameter spaces, this article presents a novel
sampling method which carefully samples the parameter space. At each sample
point, homotopy continuation is used to obtain the number of real solutions to
the corresponding polynomial system. Machine learning techniques including
nearest neighbor and deep learning are used to efficiently approximate the real
discriminant locus. One application of having learned the real discriminant
locus is to develop a real homotopy method that only tracks the real solution
paths unlike traditional methods which track all~complex~solution~paths.
Examples show that the proposed approach can efficiently approximate
complicated solution boundaries such as those arising from the equilibria of
the Kuramoto model.
"
1544,Ideal Membership Problem for Boolean Minority,"  The Ideal Membership Problem (IMP) tests if an input polynomial $f\in
\mathbb{F}[x_1,\dots,x_n]$ with coefficients from a field $\mathbb{F}$ belongs
to a given ideal $I \subseteq \mathbb{F}[x_1,\dots,x_n]$. It is a well-known
fundamental problem with many important applications, though notoriously
intractable in the general case. In this paper we consider the IMP for
polynomial ideals encoding combinatorial problems and where the input
polynomial $f$ has degree at most $d=O(1)$ (we call this problem IMP$_d$). A
dichotomy result between ``hard'' (NP-hard) and ``easy'' (polynomial time) IMPs
was recently achieved for Constraint Satisfaction Problems over finite domains
[Bulatov FOCS'17, Zhuk FOCS'17] (this is equivalent to IMP$_0$) and IMP$_d$ for
the Boolean domain [Mastrolilli SODA'19], both based on the classification of
the IMP through functions called polymorphisms. The complexity of the IMP$_d$
for five polymorphisms has been solved in [Mastrolilli SODA'19] whereas for the
ternary minority polymorphism it was incorrectly declared to have been resolved
by a previous result. As a matter of fact the complexity of the IMP$_d$ for the
ternary minority polymorphism is open. In this paper we provide the missing
link by proving that the IMP$_d$ for Boolean combinatorial ideals whose
constraints are closed under the minority polymorphism can be solved in
polynomial time. This result, along with the results in [Mastrolilli SODA'19],
completes the identification of the precise borderline of tractability for the
IMP$_d$ for constrained problems over the Boolean domain. This paper is
motivated by the pursuit of understanding the issue of bit complexity of
Sum-of-Squares proofs raised by O'Donnell [ITCS'17]. Raghavendra and Weitz
[ICALP'17] show how the IMP$_d$ tractability for combinatorial ideals implies
bounded coefficients in Sum-of-Squares proofs.
"
1545,Learning an arbitrary mixture of two multinomial logits,"  In this paper, we consider mixtures of multinomial logistic models (MNL),
which are known to $\epsilon$-approximate any random utility model. Despite its
long history and broad use, rigorous results are only available for learning a
uniform mixture of two MNLs. Continuing this line of research, we study the
problem of learning an arbitrary mixture of two MNLs. We show that the
identifiability of the mixture models may only fail on an algebraic variety of
a negligible measure. This is done by reducing the problem of learning a
mixture of two MNLs to the problem of solving a system of univariate quartic
equations. We also devise an algorithm to learn any mixture of two MNLs using a
polynomial number of samples and a linear number of queries, provided that a
mixture of two MNLs over some finite universe is identifiable. Several
numerical experiments and conjectures are also presented.
"
1546,"Error Correcting Codes, finding polynomials of bounded degree agreeing
  on a dense fraction of a set of points","  Here we present some revised arguments to a randomized algorithm proposed by
Sudan to find the polynomials of bounded degree agreeing on a dense fraction of
a set of points in $\mathbb{F}^{2}$ for some field $\mathbb{F}$.
"
1547,"Improvement on Extrapolation of Species Abundance Distribution Across
  Scales from Moments Across Scales","  Raw moments are used as a way to estimate species abundance distribution. The
almost linear pattern of the log transformation of raw moments across scales
allow us to extrapolate species abundance distribution for larger areas.
However, results may produce errors. Some of these errors are due to
computational complexity, fittings of patterns, binning methods, and so on. We
provide some methods to reduce some of the errors. The main result is
introducing new techniques for evaluating a more accurate species abundance
distributions across scales through moments across scales.
"
1548,A Family of Denominator Bounds for First Order Linear Recurrence Systems,"  For linear recurrence systems, the problem of finding rational solutions is
reduced to the problem of computing polynomial solutions by computing a content
bound or a denominator bound. There are several bounds in the literature. The
sharpest bound leads to polynomial solutions of lower degrees, but this
advantage need not compensate for the time spent on computing that bound.
  To strike the best balance between sharpness of the bound versus CPU time
spent obtaining it, we will give a family of bounds. The $J$'th member of this
family is similar to (Abramov, Barkatou, 1998) when $J=1$, similar to (van
Hoeij, 1998) when $J$ is large, and novel for intermediate values of $J$, which
give the best balance between sharpness and CPU time.
  The setting for our content bounds are systems $\tau(Y) = MY$ where $\tau$ is
an automorphism of a UFD, and $M$ is an invertible matrix with entries in its
field of fractions. This setting includes the shift case, the $q$-shift case,
the multi-basic case and others. We give two versions, a global version, and a
version that bounds each entry separately.
"
1549,ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians,"  The computation of first and second-order derivatives is a staple in many
computing applications, ranging from machine learning to scientific computing.
We propose an algorithm to automatically differentiate algorithms written in a
subset of C99 code and its efficient implementation as a Python script. We
demonstrate that our algorithm enables automatic, reliable, and efficient
differentiation of common algorithms used in physical simulation and geometry
processing.
"
1550,Learning Reasoning Strategies in End-to-End Differentiable Proving,"  Attempts to render deep learning models interpretable, data-efficient, and
robust have seen some success through hybridisation with rule-based systems,
for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can
induce interpretable rules and learn representations from data via
back-propagation, while providing logical explanations for their predictions.
However, they are restricted by their computational complexity, as they need to
consider all possible proof paths for explaining a goal, thus rendering them
unfit for large-scale applications. We present Conditional Theorem Provers
(CTPs), an extension to NTPs that learns an optimal rule selection strategy via
gradient-based optimisation. We show that CTPs are scalable and yield
state-of-the-art results on the CLUTRR dataset, which tests systematic
generalisation of neural models by learning to reason over smaller graphs and
evaluating on larger ones. Finally, CTPs show better link prediction results on
standard benchmarks in comparison with other neural-symbolic models, while
being explainable. All source code and datasets are available online, at
https://github.com/uclnlp/ctp.
"
1551,Algorithmic applications of the corestriction of central simple algebras,"  Let $L$ be a separable quadratic extension of either $\mathbb{Q}$ or
$\mathbb{F}_q(t)$. We propose efficient algorithms for finding isomorphisms
between quaternion algebras over $L$. Our techniques are based on computing
maximal one-sided ideals of the corestriction of a central simple $L$-algebra.
In order to obtain efficient algorithms in the characteristic 2 case, we
propose an algorithm for finding nontrivial zeros of a regular quadratic form
in four variables over $\mathbb{F}_{2^k}(t)$
"
1552,"On the Complexity of Quadratization for Polynomial Differential
  Equations","  Chemical reaction networks (CRNs) are a standard formalism used in chemistry
and biology to reason about the dynamics of molecular interaction networks. In
their interpretation by ordinary differential equations, CRNs provide a
Turing-complete model of analog computattion, in the sense that any computable
function over the reals can be computed by a finite number of molecular species
with a continuous CRN which approximates the result of that function in one of
its components in arbitrary precision. The proof of that result is based on a
previous result of Bournez et al. on the Turing-completeness of polyno-mial
ordinary differential equations with polynomial initial conditions (PIVP). It
uses an encoding of real variables by two non-negative variables for
concentrations, and a transformation to an equivalent quadratic PIVP (i.e. with
degrees at most 2) for restricting ourselves to at most bimolecular reactions.
In this paper, we study the theoretical and practical complexities of the
quadratic transformation. We show that both problems of minimizing either the
number of variables (i.e., molecular species) or the number of monomials (i.e.
elementary reactions) in a quadratic transformation of a PIVP are NP-hard. We
present an encoding of those problems in MAX-SAT and show the practical
complexity of this algorithm on a benchmark of quadratization problems inspired
from CRN design problems.
"
1553,"Bit-Slicing the Hilbert Space: Scaling Up Accurate Quantum Circuit
  Simulation to a New Level","  Quantum computing is greatly advanced in recent years and is expected to
transform the computation paradigm in the near future. Quantum circuit
simulation plays a key role in the toolchain for the development of quantum
hardware and software systems. However, due to the enormous Hilbert space of
quantum states, simulating quantum circuits with classical computers is
extremely challenging despite notable efforts have been made. In this paper, we
enhance quantum circuit simulation in two dimensions: accuracy and scalability.
The former is achieved by using an algebraic representation of complex numbers;
the latter is achieved by bit-slicing the number representation and replacing
matrix-vector multiplication with symbolic Boolean function manipulation.
Experimental results demonstrate that our method can be superior to the
state-of-the-art for various quantum circuits and can simulate certain
benchmark families with up to tens of thousands of qubits.
"
1554,"On Algorithmic Estimation of Analytic Complexity for Polynomial
  Solutions to Hypergeometric Systems","  The paper deals with the analytic complexity of solutions to bivariate
holonomic hypergeometric systems of the Horn type. We obtain estimates on the
analytic complexity of Puiseux polynomial solutions to the hypergeometric
systems defined by zonotopes. We also propose algorithms of the analytic
complexity estimation for polynomials.
"
1555,"Computing regular meromorphic differential forms via Saito's logarithmic
  residues","  Logarithmic differential forms and logarithmic residues associated to a
hypersurface with an isolated singularity are considered in the context of
computational complex analysis. An effective method is introduced for computing
logarithmic residues. A relation between logarithmic differential forms and the
Brieskorn formula on Gauss-Manin connection are discussed. Some examples are
also given for illustration.
"
1556,Interpretable Control by Reinforcement Learning,"  In this paper, three recently introduced reinforcement learning (RL) methods
are used to generate human-interpretable policies for the cart-pole balancing
benchmark. The novel RL methods learn human-interpretable policies in the form
of compact fuzzy controllers and simple algebraic equations. The
representations as well as the achieved control performances are compared with
two classical controller design methods and three non-interpretable RL methods.
All eight methods utilize the same previously generated data batch and produce
their controller offline - without interaction with the real benchmark
dynamics. The experiments show that the novel RL methods are able to
automatically generate well-performing policies which are at the same time
human-interpretable. Furthermore, one of the methods is applied to
automatically learn an equation-based policy for a hardware cart-pole
demonstrator by using only human-player-generated batch data. The solution
generated in the first attempt already represents a successful balancing
policy, which demonstrates the methods applicability to real-world problems.
"
1557,Computing stable resultant-based minimal solvers by hiding a variable,"  Many computer vision applications require robust and efficient estimation of
camera geometry. The robust estimation is usually based on solving camera
geometry problems from a minimal number of input data measurements, i.e.,
solving minimal problems, in a RANSAC-style framework. Minimal problems often
result in complex systems of polynomial equations. The existing
state-of-the-art methods for solving such systems are either based on Gr\""obner
bases and the action matrix method, which have been extensively studied and
optimized in the recent years or recently proposed approach based on a sparse
resultant computation using an extra variable.
  In this paper, we study an interesting alternative sparse resultant-based
method for solving sparse systems of polynomial equations by hiding one
variable. This approach results in a larger eigenvalue problem than the action
matrix and extra variable sparse resultant-based methods; however, it does not
need to compute an inverse or elimination of large matrices that may be
numerically unstable. The proposed approach includes several improvements to
the standard sparse resultant algorithms, which significantly improves the
efficiency and stability of the hidden variable resultant-based solvers as we
demonstrate on several interesting computer vision problems. We show that for
the studied problems, our sparse resultant based approach leads to more stable
solvers than the state-of-the-art Gr\""obner bases-based solvers as well as
existing sparse resultant-based solvers, especially in close to critical
configurations. Our new method can be fully automated and incorporated into
existing tools for the automatic generation of efficient minimal solvers.
"
1558,Groebner basis structure of ideal interpolation,"  We study the relationship between certain Groebner bases for zero dimensional
ideals, and the interpolation condition functionals of ideal interpolation.
Ideal interpolation is defined by a linear idempotent projector whose kernel is
a polynomial ideal. In this paper, we propose the notion of ""reverse"" complete
reduced basis. Based on the notion, we present a fast algorithm to compute the
reduced Groebner basis for the kernel of ideal projector under an arbitrary
compatible ordering. As an application, we show that knowing the affine variety
makes available information concerning the reduced Groebner basis.
"
1559,"Globally Optimal Solution to Inverse Kinematics of 7DOF Serial
  Manipulator","  The Inverse Kinematics (IK) problem is to nd robot control parameters to
bring it into the desired position under the kinematics and collision
constraints. We present a global solution to the optimal IK problem for a
general serial 7DOF manipulator with revolute joints and a quadratic polynomial
objective function. We show that the kinematic constraints due to rotations can
all be generated by second-degree polynomials. This is important since it
signicantly simplies further step where we nd the optimal solution by Lasserre
relaxations of non-convex polynomial systems. We demonstrate that the second
relaxation is sucient to solve the 7DOF IK problem. Our approach is certiably
globally optimal. We demonstrate the method on the 7DOF KUKA LBR IIWA
manipulator and show that we are able to compute the optimal IK or certify
in-feasibility in 99 % tested poses.
"
1560,Cyclotomic Identity Testing and Applications,"  We consider the cyclotomic identity testing problem: given a polynomial
$f(x_1,\ldots,x_k)$, decide whether $f(\zeta_n^{e_1},\ldots,\zeta_n^{e_k})$ is
zero, for $\zeta_n = e^{2\pi i/n}$ a primitive complex $n$-th root of unity and
integers $e_1,\ldots,e_k$. We assume that $n$ and $e_1,\ldots,e_k$ are
represented in binary and consider several versions of the problem, according
to the representation of $f$. For the case that $f$ is given by an algebraic
circuit we give a randomized polynomial-time algorithm with two-sided errors,
showing that the problem lies in BPP. In case $f$ is given by a circuit of
polynomially bounded syntactic degree, we give a randomized algorithm with
two-sided errors that runs in poly-logarithmic parallel time, showing that the
problem lies in BPNC. In case $f$ is given by a depth-2 $\Sigma\Pi$ circuit
(or, equivalently, as a list of monomials), we show that the cyclotomic
identity testing problem lies in NC. Under the generalised Riemann hypothesis,
we are able to extend this approach to obtain a polynomial-time algorithm also
for a very simple subclass of depth-3 $\Sigma\Pi\Sigma$ circuits. We complement
this last result by showing that for a more general class of depth-3
$\Sigma\Pi\Sigma$ circuits, a polynomial-time algorithm for the cyclotomic
identity testing problem would yield a sub-exponential-time algorithm for
polynomial identity testing. Finally, we use cyclotomic identity testing to
give a new proof that equality of compressed strings, i.e., strings presented
using context-free grammars, can be decided in coRNC: randomized NC with
one-sided errors.
"
1561,On Bergman's Diamond Lemma for Ring Theory,"  This expository paper deals with the Diamond Lemma for ring theory, which is
proved in the first section of G.M. Bergman, The Diamond Lemma for Ring Theory,
Advances in Mathematics, 29 (1978), pp. 178--218. No originality of the present
note is claimed on the part of the author, except for some suggestions and
figures. Throughout this paper, I shall mostly use Bergman's expressions in his
paper.
"
1562,Formal Power Series on Algebraic Cryptanalysis,"  In cryptography, attacks that utilize a Gr\""{o}bner basis have broken several
cryptosystems. The complexity of computing a Gr\""{o}bner basis dominates the
overall computing and its estimation is important for such cryptanalysis. The
complexity is given by using the solving degree, but it is hard to decide this
value of a large scale system arisen from cryptography. Thus the degree of
regularity and the first fall degree are used as proxies for the solving degree
based on a wealth of experiments. If a given system is semi-regular, the
complexity is estimated by using the degree of regularity derived from a
certain power series, otherwise, by using the first fall degree derived from a
construction of a syzygy. The degree of regularity is also defined on a
non-semi-regular system and is experimentally larger than the first fall
degree, but those relation is not clear theoretically. Moreover, in contrast to
the degree of regularity, the first fall degree has been investigated
specifically for each cryptosystem and its discussion on generic systems is not
given. In this paper, we show an upper bound for the first fall degree of a
polynomial system over a sufficiently large field. In detail, we prove that
this upper bound for a non-semi-regular system is the degree of regularity.
Moreover, we prove that the upper bound for a multi-graded polynomial system is
a certain value only decided by its multi-degree. Furthermore, we show that the
condition for the order of a field in our results is satisfied in attacks
against actual multivariate cryptosystems. Consequently, under a reasonable
condition for the order of a field, we clear a relation between the first fall
degree and the degree of regularity and provide a theoretical method using a
multivariate power series for cryptanalysis.
"
1563,Parameter identifiability and input-output equations,"  Structural parameter identifiability is a property of a differential model
with parameters that allows for the parameters to be determined from the model
equations in the absence of noise. One of the standard approaches to assessing
this problem is via input-output equations and, in particular, characteristic
sets of differential ideals. The precise relation between identifiability and
input-output identifiability is subtle. The goal of this note is to clarify
this relation. The main results are:
  1) identifiability implies input-output identifiability;
  2) these notions coincide if the model does not have rational first
integrals;
  3) the field of input-output identifiable functions is generated by the
coefficients of a ""minimal"" characteristic set of the corresponding
differential ideal.
  We expect that some of these facts may be known to the experts in the area,
but we are not aware of any articles in which these facts are stated precisely
and rigorously proved.
"
1564,Graphical Conditions for Rate Independence in Chemical Reaction Networks,"  Chemical Reaction Networks (CRNs) provide a useful abstraction of molecular
interaction networks in which molecular structures as well as mass conservation
principles are abstracted away to focus on the main dynamical properties of the
network structure. In their interpretation by ordinary differential equations,
we say that a CRN with distinguished input and output species computes a
positive real function $f : R+ $\rightarrow$ R+$, if for any initial
concentration x of the input species, the concentration of the output molecular
species stabilizes at concentration f (x). The Turing-completeness of that
notion of chemical analog computation has been established by proving that any
computable real function can be computed by a CRN over a finite set of
molecular species. Rate-independent CRNs form a restricted class of CRNs of
high practical value since they enjoy a form of absolute robustness in the
sense that the result is completely independent of the reaction rates and
depends solely on the input concentrations. The functions computed by
rate-independent CRNs have been characterized mathematically as the set of
piecewise linear functions from input species. However, this does not provide a
mean to decide whether a given CRN is rate-independent. In this paper, we
provide graphical conditions on the Petri Net structure of a CRN which entail
the rate-independence property either for all species or for some output
species. We show that in the curated part of the Biomodels repository, among
the 590 reaction models tested, 2 reaction graphs were found to satisfy our
rate-independence conditions for all species, 94 for some output species, among
which 29 for some non-trivial output species. Our graphical conditions are
based on a non-standard use of the Petri net notions of place-invariants and
siphons which are computed by constraint programming techniques for efficiency
reasons.
"
1565,Trace Logic for Inductive Loop Reasoning,"  We propose trace logic, an instance of many-sorted first-order logic, to
automate the partial correctness verification of programs containing loops.
Trace logic generalizes semantics of program locations and captures loop
semantics by encoding properties at arbitrary timepoints and loop iterations.
We guide and automate inductive loop reasoning in trace logic by using generic
trace lemmas capturing inductive loop invariants. Our work is implemented in
the RAPID framework, by extending and integrating superposition-based
first-order reasoning within RAPID. We successfully used RAPID to prove
correctness of many programs whose functional behavior are best summarized in
the first-order theories of linear integer arithmetic, arrays and inductive
data types.
"
1566,"Proceedings 8th International Workshop on Verification and Program
  Transformation and 7th Workshop on Horn Clauses for Verification and
  Synthesis","  The proceedings consist of a keynote paper by Alberto followed by 6 invited
papers written by Lorenzo Clemente (U. Warsaw), Alain Finkel (U. Paris-Saclay),
John Gallagher (Roskilde U. and IMDEA Software Institute) et al., Neil Jones
(U. Copenhagen) et al., Michael Leuschel (Heinrich-Heine U.) and Maurizio
Proietti (IASI-CNR) et al.. These invited papers are followed by 4 regular
papers accepted at VPT 2020 and the papers of HCVS 2020 which consist of three
contributed papers and an invited paper on the third competition of solvers for
Constrained Horn Clauses.
  In addition, the abstracts (in HTML format) of 3 invited talks at VPT 2020 by
Andrzej Skowron (U. Warsaw), Sophie Renault (EPO) and Moa Johansson (Chalmers
U.), are included.
"
1567,"From Big-Step to Small-Step Semantics and Back with Interpreter
  Specialisation","  We investigate representations of imperative programs as constrained Horn
clauses. Starting from operational semantics transition rules, we proceed by
writing interpreters as constrained Horn clause programs directly encoding the
rules. We then specialise an interpreter with respect to a given source program
to achieve a compilation of the source language to Horn clauses (an instance of
the first Futamura projection). The process is described in detail for an
interpreter for a subset of C, directly encoding the rules of big-step
operational semantics for C. A similar translation based on small-step
semantics could be carried out, but we show an approach to obtaining a
small-step representation using a linear interpreter for big-step Horn clauses.
This interpreter is again specialised to achieve the translation from big-step
to small-step style. The linear small-step program can be transformed back to a
big-step non-linear program using a third interpreter. A regular path
expression is computed for the linear program using Tarjan's algorithm, and
this regular expression then guides an interpreter to compute a program path.
The transformation is realised by specialisation of the path interpreter. In
all of the transformation phases, we use an established partial evaluator and
exploit standard logic program transformation to remove redundant data
structures and arguments in predicates and rename predicates to make clear
their link to statements in the original source program.
"
1568,An Experiment Combining Specialization with Abstract Interpretation,"  It was previously shown that control-flow refinement can be achieved by a
program specializer incorporating property-based abstraction, to improve
termination and complexity analysis tools. We now show that this purpose-built
specializer can be reconstructed in a more modular way, and that the previous
results can be achieved using an off-the-shelf partial evaluation tool, applied
to an abstract interpreter. The key feature of the abstract interpreter is the
abstract domain, which is the product of the property-based abstract domain
with the concrete domain. This language-independent framework provides a
practical approach to implementing a variety of powerful specializers, and
contributes to a stream of research on using interpreters and specialization to
achieve program transformations.
"
1569,Competition Report: CHC-COMP-20,"  CHC-COMP-20 is the third competition of solvers for Constrained Horn Clauses.
In this year, 9 solvers participated at the competition, and were evaluated in
four separate tracks on problems in linear integer arithmetic, linear real
arithmetic, and arrays. The competition was run in the first week of May 2020
using the StarExec computing cluster. This report gives an overview of the
competition design, explains the organisation of the competition, and presents
the competition results.
"
1570,"A Simple and Fast Algorithm for Computing the $N$-th Term of a Linearly
  Recurrent Sequence","  We present a simple and fast algorithm for computing the $N$-th term of a
given linearly recurrent sequence. Our new algorithm uses $O(\mathsf{M}(d) \log
N)$ arithmetic operations, where $d$ is the order of the recurrence, and
$\mathsf{M}(d)$ denotes the number of arithmetic operations for computing the
product of two polynomials of degree $d$. The state-of-the-art algorithm, due
to Charles Fiduccia (1985), has the same arithmetic complexity up to a constant
factor. Our algorithm is simpler, faster and obtained by a totally different
method. We also discuss several algorithmic applications, notably to polynomial
modular exponentiation, powering of matrices and high-order lifting.
"
1571,Computing the Real Isolated Points of an Algebraic Hypersurface,"  Let $\mathbb{R}$ be the field of real numbers. We consider the problem of
computing the real isolated points of a real algebraic set in $\mathbb{R}^n$
given as the vanishing set of a polynomial system. This problem plays an
important role for studying rigidity properties of mechanism in material
designs. In this paper, we design an algorithm which solves this problem. It is
based on the computations of critical points as well as roadmaps for answering
connectivity queries in real algebraic sets. This leads to a probabilistic
algorithm of complexity $(nd)^{O(n\log(n))}$ for computing the real isolated
points of real algebraic hypersurfaces of degree $d$. It allows us to solve in
practice instances which are out of reach of the state-of-the-art.
"
1572,Computing singular elements modulo squares,"  The group of singular elements was first introduced by Helmut Hasse and later
it has been studied by numerous authors including such well known
mathematicians as: Cassels, Furtw\""{a}ngler, Hecke, Knebusch, Takagi and of
course Hasse himself; to name just a few. The aim of the present paper is to
present algorithms that explicitly construct groups of singular and
$S$-singular elements (modulo squares) in a global function field.
"
1573,Exact $p$-adic computation in Magma,"  We describe a new arithmetic system for the Magma computer algebra system for
working with $p$-adic numbers exactly, in the sense that numbers are
represented lazily to infinite $p$-adic precision. This is the first highly
featured such implementation. This has the benefits of increasing
user-friendliness and speeding up some computations, as well as forcibly
producing provable results. We give theoretical and practical justification for
its design and describe some use cases. The intention is that this article will
be of benefit to anyone wanting to implement similar functionality in other
languages.
"
1574,"Robots, computer algebra and eight connected components","  Answering connectivity queries in semi-algebraic sets is a long-standing and
challenging computational issue with applications in robotics, in particular
for the analysis of kinematic singularities. One task there is to compute the
number of connected components of the complementary of the singularities of the
kinematic map. Another task is to design a continuous path joining two given
points lying in the same connected component of such a set. In this paper, we
push forward the current capabilities of computer algebra to obtain
computer-aided proofs of the analysis of the kinematic singularities of various
robots used in industry. We first show how to combine mathematical reasoning
with easy symbolic computations to study the kinematic singularities of an
infinite family (depending on paramaters) modelled by the UR-series produced by
the company ``Universal Robots''. Next, we compute roadmaps (which are curves
used to answer connectivity queries) for this family of robots. We design an
algorithm for ``solving'' positive dimensional polynomial system depending on
parameters. The meaning of solving here means partitioning the parameter's
space into semi-algebraic components over which the number of connected
components of the semi-algebraic set defined by the input system is invariant.
Practical experiments confirm our computer-aided proof and show that such an
algorithm can already be used to analyze the kinematic singularities of the
UR-series family. The number of connected components of the complementary of
the kinematic singularities of generic robots in this family is $8$.
"
1575,"Homotopy techniques for solving sparse column support determinantal
  polynomial systems","  Let $\mathbf{K}$ be a field of characteristic zero with
$\overline{\mathbf{K}}$ its algebraic closure. Given a sequence of polynomials
$\mathbf{g} = (g_1, \ldots, g_s) \in \mathbf{K}[x_1, \ldots , x_n]^s$ and a
polynomial matrix $\mathbf{F} = [f_{i,j}] \in \mathbf{K}[x_1, \ldots, x_n]^{p
\times q}$, with $p \leq q$, we are interested in determining the isolated
points of $V_p(\mathbf{F},\mathbf{g})$, the algebraic set of points in
$\overline{\mathbf{K}}$ at which all polynomials in $\mathbf{g}$ and all
$p$-minors of $\mathbf{F}$ vanish, under the assumption $n = q - p + s + 1$.
Such polynomial systems arise in a variety of applications including for
example polynomial optimization and computational geometry. We design a
randomized sparse homotopy algorithm for computing the isolated points in
$V_p(\mathbf{F},\mathbf{g})$ which takes advantage of the determinantal
structure of the system defining $V_p(\mathbf{F}, \mathbf{g})$. Its complexity
is polynomial in the maximum number of isolated solutions to such systems
sharing the same sparsity pattern and in some combinatorial quantities attached
to the structure of such systems. It is the first algorithm which takes
advantage both on the determinantal structure and sparsity of input
polynomials. We also derive complexity bounds for the particular but important
case where $\mathbf{g}$ and the columns of $\mathbf{F}$ satisfy weighted degree
constraints. Such systems arise naturally in the computation of critical points
of maps restricted to algebraic sets when both are invariant by the action of
the symmetric group.
"
1576,Computing critical points for invariant algebraic systems,"  Let $\mathbf{K}$ be a field and $\phi$, $\mathbf{f} = (f_1, \ldots, f_s)$ in
$\mathbf{K}[x_1, \dots, x_n]$ be multivariate polynomials (with $s < n$)
invariant under the action of $\mathcal{S}_n$, the group of permutations of
$\{1, \dots, n\}$. We consider the problem of computing the points at which
$\mathbf{f}$ vanish and the Jacobian matrix associated to $\mathbf{f}, \phi$ is
rank deficient provided that this set is finite. We exploit the invariance
properties of the input to split the solution space according to the orbits of
$\mathcal{S}_n$. This allows us to design an algorithm which gives a triangular
description of the solution space and which runs in time polynomial in $d^s$,
${{n+d}\choose{d}}$ and $\binom{n}{s+1}$ where $d$ is the maximum degree of the
input polynomials. When $d,s$ are fixed, this is polynomial in $n$ while when
$s$ is fixed and $d \simeq n$ this yields an exponential speed-up with respect
to the usual polynomial system solving algorithms.
"
1577,On a non-archimedean broyden method,"  Newton's method is an ubiquitous tool to solve equations, both in the
archimedean and non-archimedean settings -- for which it does not really
differ. Broyden was the instigator of what is called ""quasi-Newton methods"".
These methods use an iteration step where one does not need to compute a
complete Jacobian matrix nor its inverse. We provide an adaptation of Broyden's
method in a general non-archimedean setting, compatible with the lack of inner
product, and study its Q and R convergence. We prove that our adapted method
converges at least Q-linearly and R-superlinearly with R-order
$2^{\frac{1}{2m}}$ in dimension m. Numerical data are provided.
"
1578,"Strong Consistency and Thomas Decomposition of Finite Difference
  Approximations to Systems of Partial Differential Equations","  For a wide class of polynomially nonlinear systems of partial differential
equations we suggest an algorithmic approach that combines differential and
difference algebra to analyze s(trong)-consistency of finite difference
approximations. Our approach is applicable to regular solution grids. For the
grids of this type we give a new definition of s-consistency for finite
difference approximations which generalizes our definition given earlier for
Cartesian grids. The algorithmic verification of s-consistency presented in the
paper is based on the use of both differential and difference Thomas
decomposition. First, we apply the differential decomposition to the input
system, resulting in a partition of its solution space. Then, to the output
subsystem that contains a solution of interest we apply a difference analogue
of the differential Thomas decomposition which allows to check the
s-consistency. For linear and some quasi-linear differential systems one can
also apply difference \Gr bases for the s-consistency analysis. We illustrate
our methods and algorithms by a number of examples, which include Navier-Stokes
equations for viscous incompressible flow.
"
1579,"On FGLM Algorithms with Tropical Gr\""obner bases","  Let K be a field equipped with a valuation. Tropical varieties over K can be
defined with a theory of Gr{\""o}bner bases taking into account the valuation of
K. Because of the use of the valuation, the theory of tropical Gr{\""o}bner
bases has proved to provide settings for computations over polynomial rings
over a p-adic field that are more stable than that of classical Gr{\""o}bner
bases. In this article, we investigate how the FGLM change of ordering
algorithm can be adapted to the tropical setting. As the valuations of the
polynomial coefficients are taken into account, the classical FGLM algorithm's
incremental way, monomo-mial by monomial, to compute the multiplication
matrices and the change of basis matrix can not be transposed at all to the
tropical setting. We mitigate this issue by developing new linear algebra
algorithms and apply them to our new tropical FGLM algorithms. Motivations are
twofold. Firstly, to compute tropical varieties, one usually goes through the
computation of many tropical Gr{\""o}bner bases defined for varying weights (and
then varying term orders). For an ideal of dimension 0, the tropical FGLM
algorithm provides an efficient way to go from a tropical Gr{\""o}bner basis
from one weight to one for another weight. Secondly, the FGLM strategy can be
applied to go from a tropical Gr{\""o}bner basis to a classical Gr{\""o}bner
basis. We provide tools to chain the stable computation of a tropical
Gr{\""o}bner basis (for weight [0,. .. , 0]) with the p-adic stabilized variants
of FGLM of [RV16] to compute a lexicographical or shape position basis. All our
algorithms have been implemented into SageMath. We provide numerical examples
to illustrate time-complexity. We then illustrate the superiority of our
strategy regarding to the stability of p-adic numerical computations.
"
1580,PolyAdd: Polynomial Formal Verification of Adder Circuits,"  Only by formal verification approaches functional correctness can be ensured.
While for many circuits fast verification is possible, in other cases the
approaches fail. In general no efficient algorithms can be given, since the
underlying verification problem is NP-complete.
  In this paper we prove that for different types of adder circuits polynomial
verification can be ensured based on BDDs. While it is known that the output
functions for addition are polynomially bounded, we show in the following that
the entire construction process can be carried out in polynomial time. This is
shown for the simple Carry Ripple Adder, but also for fast adders like the
Conditional Sum Adder and the Carry Look Ahead Adder. Properties about the
adder function are proven and the core principle of polynomial verification is
described that can also be extended to other classes of functions and circuit
realizations.
"
1581,"Guessing Gr{\""o}bner Bases of Structured Ideals of Relations of
  Sequences","  Assuming sufficiently many terms of a n-dimensional table defined over a
field are given, we aim at guessing the linear recurrence relations with either
constant or polynomial coefficients they satisfy. In many applications, the
table terms come along with a structure: for instance, they may be zero outside
of a cone, they may be built from a Gr{\""o}bner basis of an ideal invariant
under the action of a finite group. Thus, we show how to take advantage of this
structure to both reduce the number of table queries and the number of
operations in the base field to recover the ideal of relations of the table. In
applications like in combinatorics, where all these zero terms make us guess
many fake relations, this allows us to drastically reduce these wrong guesses.
These algorithms have been implemented and, experimentally, they let us handle
examples that we could not manage otherwise. Furthermore, we show which kind of
cone and lattice structures are preserved by skew polynomial multiplication.
This allows us to speed up the guessing of linear recurrence relations with
polynomial coefficients by computing sparse Gr{\""o}bner bases or Gr{\""o}bner
bases of an ideal invariant under the action of a finite group in a ring of
skew polynomials.
"
1582,"Characterizing Positively Invariant Sets: Inductive and Topological
  Methods","  Set positive invariance is an important concept in the theory of dynamical
systems and one which also has practical applications in areas of computer
science, such as formal verification, as well as in control theory. Great
progress has been made in understanding positively invariant sets in continuous
dynamical systems and powerful computational tools have been developed for
reasoning about them; however, many of the insights from recent developments in
this area have largely remained folklore and are not elaborated in existing
literature. This article contributes an explicit development of modern methods
for checking positively invariant sets of ordinary differential equations and
describes two possible characterizations of positive invariants: one based on
the real induction principle, and a novel alternative based on topological
notions. The two characterizations, while in a certain sense equivalent, lead
to two different decision procedures for checking whether a given
semi-algebraic set is positively invariant under the flow of a system of
polynomial ordinary differential equations.
"
1583,Modeling Hierarchical System with Operads,"  This paper applies operads and functorial semantics to address the problem of
failure diagnosis in complex systems. We start with a concrete example,
developing a hierarchical interaction model for the Length Scale
Interferometer, a high-precision measurement system operated by the US National
Institute of Standards and Technology. The model is expressed in terms of
combinatorial/diagrammatic structures called port-graphs, and we explain how to
extract an operad LSI from a collection of these diagrams. Next we show how
functors to the operad of probabilities organize and constrain the relative
probabilities of component failure in the system. Finally, we show how to
extend the analysis from general component failure to specific failure modes.
"
1584,"Deriving Theorems in Implicational Linear Logic, Declaratively","  The problem we want to solve is how to generate all theorems of a given size
in the implicational fragment of propositional intuitionistic linear logic. We
start by filtering for linearity the proof terms associated by our Prolog-based
theorem prover for Implicational Intuitionistic Logic. This works, but using
for each formula a PSPACE-complete algorithm limits it to very small formulas.
We take a few walks back and forth over the bridge between proof terms and
theorems, provided by the Curry-Howard isomorphism, and derive step-by-step an
efficient algorithm requiring a low polynomial effort per generated theorem.
The resulting Prolog program runs in O(N) space for terms of size N and
generates in a few hours 7,566,084,686 theorems in the implicational fragment
of Linear Intuitionistic Logic together with their proof terms in normal form.
As applications, we generate datasets for correctness and scalability testing
of linear logic theorem provers and training data for neural networks working
on theorem proving challenges. The results in the paper, organized as a
literate Prolog program, are fully replicable.
  Keywords: combinatorial generation of provable formulas of a given size,
intuitionistic and linear logic theorem provers, theorems of the implicational
fragment of propositional linear intuitionistic logic, Curry-Howard
isomorphism, efficient generation of linear lambda terms in normal form, Prolog
programs for lambda term generation and theorem proving.
"
1585,"Enhancing Linear Algebraic Computation of Logic Programs Using Sparse
  Representation","  Algebraic characterization of logic programs has received increasing
attention in recent years. Researchers attempt to exploit connections between
linear algebraic computation and symbolic computation in order to perform
logical inference in large scale knowledge bases. This paper proposes further
improvement by using sparse matrices to embed logic programs in vector spaces.
We show its great power of computation in reaching the fixpoint of the
immediate consequence operator from the initial vector. In particular,
performance for computing the least models of definite programs is dramatically
improved in this way. We also apply the method to the computation of stable
models of normal programs, in which the guesses are associated with initial
matrices, and verify its effect when there are small numbers of negation. These
results show good enhancement in terms of performance for computing
consequences of programs and depict the potential power of tensorized logic
programs.
"
1586,A Low-Level Index for Distributed Logic Programming,"  A distributed logic programming language with support for meta-programming
and stream processing offers a variety of interesting research problems, such
as: How can a versatile and stable data structure for the indexing of a large
number of expressions be implemented with simple low-level data structures? Can
low-level programming help to reduce the number of occur checks in Robinson's
unification algorithm? This article gives the answers.
"
1587,"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense
  Reasoning","  Generative commonsense reasoning which aims to empower machines to generate
sentences with the capacity of reasoning over a set of concepts is a critical
bottleneck for text generation. Even the state-of-the-art pre-trained language
generation models struggle at this task and often produce implausible and
anomalous sentences. One reason is that they rarely consider incorporating the
knowledge graph which can provide rich relational information among the
commonsense concepts. To promote the ability of commonsense reasoning for text
generation, we propose a novel knowledge graphaugmented pre-trained language
generation model KG-BART, which encompasses the complex relations of concepts
through the knowledge graph and produces more logical and natural sentences as
output. Moreover, KG-BART can leverage the graph attention to aggregate the
rich concept semantics that enhances the model generalization on unseen concept
sets. Experiments on benchmark CommonGen dataset verify the effectiveness of
our proposed approach by comparing with several strong pre-trained language
generation models, particularly KG-BART outperforms BART by 15.98%, 17.49%, in
terms of BLEU-3, 4. Moreover, we also show that the generated context by our
model can work as background scenarios to benefit downstream commonsense QA
tasks.
"
1588,"Formal Verification of Arithmetic RTL: Translating Verilog to C++ to
  ACL2","  We present a methodology for formal verification of arithmetic RTL designs
that combines sequential logic equivalence checking with interactive theorem
proving. An intermediate model of a Verilog module is hand-coded in Restricted
Algorithmic C (RAC), a primitive subset of C augmented by the integer and
fixed-point register class templates of Algorithmic C. The model is designed to
be as abstract and compact as possible, but sufficiently faithful to the RTL to
allow efficient equivalence checking with a commercial tool. It is then
automatically translated to the logic of ACL2, enabling a mechanically checked
proof of correctness with respect to a formal architectural specification. In
this paper, we describe the RAC language, the translation process, and some
techniques that facilitate formal analysis of the resulting ACL2 code.
"
1589,Iteration in ACL2,"  Iterative algorithms are traditionally expressed in ACL2 using recursion. On
the other hand, Common Lisp provides a construct, loop, which -- like most
programming languages -- provides direct support for iteration. We describe an
ACL2 analogue loop$ of loop that supports efficient ACL2 programming and
reasoning with iteration.
"
1590,"On Differentially Algebraic Generating Series for Walks in the Quarter
  Plane","  We refine necessary and sufficient conditions for the generating series of a
weighted model of a quarter plane walk to be differentially algebraic. In
addition, we give algorithms based on the theory of Mordell-Weil lattices,
that, for each weighted model, yield polynomial conditions on the weights
determining this property of the associated generating series.
"
1591,Factorization of Dual Quaternion Polynomials Without Study's Condition,"  In this paper we investigate factorizations of polynomials over the ring of
dual quaternions into linear factors. While earlier results assume that the
norm polynomial is real (""motion polynomials""), we only require the absence of
real polynomial factors in the primal part and factorizability of the norm
polynomial over the dual numbers into monic quadratic factors. This obviously
necessary condition is also sufficient for existence of factorizations. We
present an algorithm to compute factorizations of these polynomials and use it
for new constructions of mechanisms which cannot be obtained by existing
factorization algorithms for motion polynomials. While they produce mechanisms
with rotational or translational joints, our approach yields mechanisms
consisting of ""vertical Darboux joints"". They exhibit mechanical deficiencies
so that we explore ways to replace them by cylindrical joints while keeping the
overall mechanism sufficiently constrained.
"
1592,A Simple and Efficient Tensor Calculus for Machine Learning,"  Computing derivatives of tensor expressions, also known as tensor calculus,
is a fundamental task in machine learning. A key concern is the efficiency of
evaluating the expressions and their derivatives that hinges on the
representation of these expressions. Recently, an algorithm for computing
higher order derivatives of tensor expressions like Jacobians or Hessians has
been introduced that is a few orders of magnitude faster than previous
state-of-the-art approaches. Unfortunately, the approach is based on Ricci
notation and hence cannot be incorporated into automatic differentiation
frameworks from deep learning like TensorFlow, PyTorch, autograd, or JAX that
use the simpler Einstein notation. This leaves two options, to either change
the underlying tensor representation in these frameworks or to develop a new,
provably correct algorithm based on Einstein notation. Obviously, the first
option is impractical. Hence, we pursue the second option. Here, we show that
using Ricci notation is not necessary for an efficient tensor calculus and
develop an equally efficient method for the simpler Einstein notation. It turns
out that turning to Einstein notation enables further improvements that lead to
even better efficiency.
  The methods that are described in this paper have been implemented in the
online tool www.MatrixCalculus.org for computing derivatives of matrix and
tensor expressions.
  An extended abstract of this paper appeared as ""A Simple and Efficient Tensor
Calculus"", AAAI 2020.
"
1593,"Exact Symbolic Inference in Probabilistic Programs via Sum-Product
  Representations","  We present the Sum-Product Probabilistic Language (SPPL), a new system that
automatically delivers exact solutions to a broad range of probabilistic
inference queries. SPPL symbolically represents the full distribution on
execution traces specified by a probabilistic program using a generalization of
sum-product networks. SPPL handles continuous and discrete distributions,
many-to-one numerical transformations, and a query language that includes
general predicates on random variables. We formalize SPPL in terms of a novel
translation strategy from probabilistic programs to a semantic domain of
sum-product representations, present new algorithms for exactly conditioning on
and computing probabilities of queries, and prove their soundness under the
semantics. We present techniques for improving the scalability of translation
and inference by automatically exploiting conditional independences and
repeated structure in SPPL programs. We implement a prototype of SPPL with a
modular architecture and evaluate it on a suite of common benchmarks, which
establish that our system is up to 3500x faster than state-of-the-art systems
for fairness verification; up to 1000x faster than state-of-the-art symbolic
algebra techniques; and can compute exact probabilities of rare events in
milliseconds.
"
1594,"Deterministic computation of the characteristic polynomial in the time
  of matrix multiplication","  This paper describes an algorithm which computes the characteristic
polynomial of a matrix over a field within the same asymptotic complexity, up
to constant factors, as the multiplication of two square matrices. Previously,
to our knowledge, this was only achieved by resorting to genericity assumptions
or randomization techniques, while the best known complexity bound with a
general deterministic algorithm was obtained by Keller-Gehrig in 1985 and
involves logarithmic factors. Our algorithm computes more generally the
determinant of a univariate polynomial matrix in reduced form, and relies on
new subroutines for transforming shifted reduced matrices into shifted weak
Popov matrices, and shifted weak Popov matrices into shifted Popov matrices.
"
1595,A Categorical Programming Language,"  A theory of data types based on category theory is presented. We organize
data types under a new categorical notion of F,G-dialgebras which is an
extension of the notion of adjunctions as well as that of T-algebras.
T-algebras are also used in domain theory, but while domain theory needs some
primitive data types, like products, to start with, we do not need any.
Products, coproducts and exponentiations (i.e. function spaces) are defined
exactly like in category theory using adjunctions. F,G-dialgebras also enable
us to define the natural number object, the object for finite lists and other
familiar data types in programming. Furthermore, their symmetry allows us to
have the dual of the natural number object and the object for infinite lists
(or lazy lists). We also introduce a programming language in a categorical
style using F,G-dialgebras as its data type declaration mechanism. We define
the meaning of the language operationally and prove that any program terminates
using Tait's computability method.
"
1596,Exploiting Knowledge Graphs for Facilitating Product/Service Discovery,"  Most of the existing techniques to product discovery rely on syntactic
approaches, thus ignoring valuable and specific semantic information of the
underlying standards during the process. The product data comes from different
heterogeneous sources and formats giving rise to the problem of
interoperability. Above all, due to the continuously increasing influx of data,
the manual labeling is getting costlier. Integrating the descriptions of
different products into a single representation requires organizing all the
products across vendors in a single taxonomy. Practically relevant and quality
product categorization standards are still limited in number; and that too in
academic research projects where we can majorly see only prototypes as compared
to industry. This work presents a cost-effective solution for e-commerce on the
Data Web by employing an unsupervised approach for data classification and
exploiting the knowledge graphs for matching. The proposed architecture
describes available products in web ontology language OWL and stores them in a
triple store. User input specifications for certain products are matched
against the available product categories to generate a knowledge graph. This
mullti-phased top-down approach to develop and improve existing, if any,
tailored product recommendations will be able to connect users with the exact
product/service of their choice.
"
1597,An Algorithm for the Factorization of Split Quaternion Polynomials,"  We present an algorithm to compute all factorizations into linear factors of
univariate polynomials over the split quaternions, provided such a
factorization exists. Failure of the algorithm is equivalent to
non-factorizability for which we present also geometric interpretations in
terms of rulings on the quadric of non-invertible split quaternions. However,
suitable real polynomial multiples of split quaternion polynomials can still be
factorized and we describe how to find these real polynomials. Split quaternion
polynomials describe rational motions in the hyperbolic plane. Factorization
with linear factors corresponds to the decomposition of the rational motion
into hyperbolic rotations. Since multiplication with a real polynomial does not
change the motion, this decomposition is always possible. Some of our ideas can
be transferred to the factorization theory of motion polynomials. These are
polynomials over the dual quaternions with real norm polynomial and they
describe rational motions in Euclidean kinematics. We transfer techniques
developed for split quaternions to compute new factorizations of certain dual
quaternion polynomials.
"
1598,On lattice point counting in $\Delta$-modular polyhedra,"  Let a polyhedron $P$ be defined by one of the following ways:
  (i) $P = \{x \in R^n \colon A x \leq b\}$, where $A \in Z^{(n+k) \times n}$,
$b \in Z^{(n+k)}$ and $rank\, A = n$;
  (ii) $P = \{x \in R_+^n \colon A x = b\}$, where $A \in Z^{k \times n}$, $b
\in Z^{k}$ and $rank\, A = k$.
  And let all rank order minors of $A$ be bounded by $\Delta$ in absolute
values. We show that the short rational generating function for the power
series $$ \sum\limits_{m \in P \cap Z^n} x^m $$ can be computed with the
arithmetic complexity $ O\left(T_{SNF}(d) \cdot d^{k} \cdot d^{\log_2
\Delta}\right), $ where $k$ and $\Delta$ are fixed, $d = \dim P$, and
$T_{SNF}(m)$ is the complexity to compute the Smith Normal Form for $m \times
m$ integer matrix. In particular, $d = n$ for the case (i) and $d = n-k$ for
the case (ii).
  The simplest examples of polyhedra that meet conditions (i) or (ii) are the
simplicies, the subset sum polytope and the knapsack or multidimensional
knapsack polytopes.
  We apply these results to parametric polytopes, and show that the step
polynomial representation of the function $c_P(y) = |P_{y} \cap Z^n|$, where
$P_{y}$ is parametric polytope, can be computed by a polynomial time even in
varying dimension if $P_{y}$ has a close structure to the cases (i) or (ii). As
another consequence, we show that the coefficients $e_i(P,m)$ of the Ehrhart
quasi-polynomial $$ \left| mP \cap Z^n\right| = \sum\limits_{j = 0}^n
e_i(P,m)m^j $$ can be computed by a polynomial time algorithm for fixed $k$ and
$\Delta$.
"
1599,"A variational autoencoder for music generation controlled by tonal
  tension","  Many of the music generation systems based on neural networks are fully
autonomous and do not offer control over the generation process. In this
research, we present a controllable music generation system in terms of tonal
tension. We incorporate two tonal tension measures based on the Spiral Array
Tension theory into a variational autoencoder model. This allows us to control
the direction of the tonal tension throughout the generated piece, as well as
the overall level of tonal tension. Given a seed musical fragment, stemming
from either the user input or from directly sampling from the latent space, the
model can generate variations of this original seed fragment with altered tonal
tension. This altered music still resembles the seed music rhythmically, but
the pitch of the notes are changed to match the desired tonal tension as
conditioned by the user.
"
1600,"On Minor Left Prime Factorization Problem for Multivariate Polynomial
  Matrices","  A new necessary and sufficient condition for the existence of minor left
prime factorizations of multivariate polynomial matrices without full row rank
is presented. The key idea is to establish a relationship between a matrix and
its full row rank submatrix. Based on the new result, we propose an algorithm
for factorizing matrices and have implemented it on the computer algebra system
Maple. Two examples are given to illustrate the effectiveness of the algorithm,
and experimental data shows that the algorithm is efficient.
"
1601,"On Factor Left Prime Factorization Problems for Multivariate Polynomial
  Matrices","  This paper is concerned with factor left prime factorization problems for
multivariate polynomial matrices without full row rank. We propose a necessary
and sufficient condition for the existence of factor left prime factorizations
of a class of multivariate polynomial matrices, and then design an algorithm to
compute all factor left prime factorizations if they exist. We implement the
algorithm on the computer algebra system Maple, and two examples are given to
illustrate the effectiveness of the algorithm. The results presented in this
paper are also true for the existence of factor right prime factorizations of
multivariate polynomial matrices without full column rank.
"
1602,"New Remarks on the Factorization and Equivalence Problems for a Class of
  Multivariate Polynomial Matrices","  This paper is concerned with the factorization and equivalence problems of
multivariate polynomial matrices. We present some new criteria for the
existence of matrix factorizations for a class of multivariate polynomial
matrices, and obtain a necessary and sufficient condition for the equivalence
of a square polynomial matrix and a diagonal matrix. Based on the constructive
proof of the new criteria, we give a factorization algorithm and prove the
uniqueness of the factorization. We implement the algorithm on Maple, and two
illustrative examples are given to show the effectiveness of the algorithm.
"
1603,Projective isomorphisms between rational surfaces,"  We present a method for computing projective isomorphisms between rational
surfaces that are given in terms of their parametrizations. The main idea is to
reduce the computation of such projective isomorphisms to five base cases by
modifying the parametric maps such that the components of the resulting maps
have lower degree. Our method can be used to compute affine, Euclidean and
M\""obius isomorphisms between surfaces.
"
1604,Creative Telescoping on Multiple Sums,"  We discuss the strategies and difficulties of determining a recurrence which
a certain polynomial (in the form of a symbolic multiple sum) satisfies. The
polynomial comes from an analysis of integral estimators derived via
quasi-Monte Carlo methods.
"
1605,Optimal Descartes' Rule of Signs for Circuits,"  We present an optimal version of Descartes' rule of signs to bound the number
of positive real roots of a sparse system of polynomial equations in n
variables with n+2 monomials. This sharp upper bound is given in terms of the
sign variation of a sequence associated to the exponents and the coefficients
of the system.
"
1606,Algorithmic Reduction of Biological Networks With Multiple Time Scales,"  We present a symbolic algorithmic approach that allows to compute invariant
manifolds and corresponding reduced systems for differential equations modeling
biological networks which comprise chemical reaction networks for cellular
biochemistry, and compartmental models for pharmacology, epidemiology and
ecology. Multiple time scales of a given network are obtained by scaling, based
on tropical geometry. Our reduction is mathematically justified within a
singular perturbation setting using a recent result by Cardin and Teixeira. The
existence of invariant manifolds is subject to hyperbolicity conditions, which
we test algorithmically using Hurwitz criteria. We finally obtain a sequence of
nested invariant manifolds and respective reduced systems on those manifolds.
Our theoretical results are generally accompanied by rigorous algorithmic
descriptions suitable for direct implementation based on existing off-the-shelf
software systems, specifically symbolic computation libraries and
Satisfiability Modulo Theories solvers. We present computational examples taken
from the well-known BioModels database using our own prototypical
implementations.
"
1607,"Evaluation of Logic Programs with Built-Ins and Aggregation: A Calculus
  for Bag Relations","  We present a scheme for translating logic programs, which may use aggregation
and arithmetic, into algebraic expressions that denote bag relations over
ground terms of the Herbrand universe. To evaluate queries against these
relations, we develop an operational semantics based on term rewriting of the
algebraic expressions. This approach can exploit arithmetic identities and
recovers a range of useful strategies, including lazy strategies that defer
work until it becomes possible or necessary.
"
1608,Logic Guided Genetic Algorithms,"  We present a novel Auxiliary Truth enhanced Genetic Algorithm (GA) that uses
logical or mathematical constraints as a means of data augmentation as well as
to compute loss (in conjunction with the traditional MSE), with the aim of
increasing both data efficiency and accuracy of symbolic regression (SR)
algorithms. Our method, logic-guided genetic algorithm (LGGA), takes as input a
set of labelled data points and auxiliary truths (ATs) (mathematical facts
known a priori about the unknown function the regressor aims to learn) and
outputs a specially generated and curated dataset that can be used with any SR
method. Three key insights underpin our method: first, SR users often know
simple ATs about the function they are trying to learn. Second, whenever an SR
system produces a candidate equation inconsistent with these ATs, we can
compute a counterexample to prove the inconsistency, and further, this
counterexample may be used to augment the dataset and fed back to the SR system
in a corrective feedback loop. Third, the value addition of these ATs is that
their use in both the loss function and the data augmentation process leads to
better rates of convergence, accuracy, and data efficiency. We evaluate LGGA
against state-of-the-art SR tools, namely, Eureqa and TuringBot on 16 physics
equations from ""The Feynman Lectures on Physics"" book. We find that using these
SR tools in conjunction with LGGA results in them solving up to 30.0% more
equations, needing only a fraction of the amount of data compared to the same
tool without LGGA, i.e., resulting in up to a 61.9% improvement in data
efficiency.
"
1609,Optimized Multivariate Polynomial Determinant on GPU,"  We present an optimized algorithm calculating determinant for multivariate
polynomial matrix on GPU. The novel algorithm provides precise determinant for
input multivariate polynomial matrix in controllable time. Our approach is
based on modular methods and split into Fast Fourier Transformation,
Condensation method and Chinese Remainder Theorem where each algorithm is
paralleled on GPU. The experiment results show that our parallel method owns
substantial speedups compared to Maple, allowing memory overhead and time
expedition in steady increment. We are also able to deal with complex matrix
which is over the threshold on Maple and constrained on CPU. In addition,
calculation during the process could be recovered without losing accuracy at
any point regardless of disruptions. Furthermore, we propose a time prediction
for calculation of polynomial determinant according to some basic matrix
attributes and we solve an open problem relating to harmonic elimination
equations on the basis of our GPU implementation.
"
1610,"A Graph Theoretical Approach for Testing Binomiality of Reversible
  Chemical Reaction Networks","  We study binomiality of the steady state ideals of chemical reaction
networks. Considering rate constants as indeterminates, the concept of
unconditional binomiality has been introduced and an algorithm based on linear
algebra has been proposed in a recent work for reversible chemical reaction
networks, which has a polynomial time complexity upper bound on the number of
species and reactions. In this article, using a modified version of
species--reaction graphs, we present an algorithm based on graph theory which
performs by adding and deleting edges and changing the labels of the edges in
order to test unconditional binomiality. We have implemented our graph
theoretical algorithm as well as the linear algebra one in Maple and made
experiments on biochemical models. Our experiments show that the performance of
the graph theoretical approach is similar to or better than the linear algebra
approach, while it is drastically faster than Groebner basis and quantifier
elimination methods.
"
1611,"On computation of the inverse of a polynomial map over finite fields
  using the reduced Koopman dual linear map","  This paper proposes a symbolic representation of non-linear maps $F$ in
$\ff^n$ in terms of linear combination of basis functions of a subspace of
$(\ff^n)^0$, the dual space of $\ff^n$. Using this representation, it is shown
that the inverse of $F$ whenever it exists can also be represented in a similar
symbolic form using the same basis functions (using different coefficients).
This form of representation should be of importance to solving many problems of
iterations or compositions of non-linear maps using linear algebraic methods
which would otherwise require solving hard computational problems due to
non-linear nature of $F$.
"
1612,"Lexicographic Groebner bases of bivariate polynomials modulo a
  univariate one","  Let T(x) in k[x] be a monic non-constant polynomial and write R=k[x] / (T)
the quotient ring. Consider two bivariate polynomials a(x, y), b(x, y) in R[y].
In a first part, T = p^e is assumed to be the power of an irreducible
polynomial p. A new algorithm that computes a minimal lexicographic Groebner
basis of the ideal ( a, b, p^e), is introduced. A second part extends this
algorithm when T is general through the ""local/global"" principle realized by a
generalization of ""dynamic evaluation"", restricted so far to a polynomial T
that is squarefree. The algorithm produces splittings according to the case
distinction ""invertible/nilpotent"", extending the usual ""invertible/zero"" in
classic dynamic evaluation. This algorithm belongs to the Euclidean family, the
core being a subresultant sequence of a and b modulo T. In particular no
factorization or Groebner basis computations are necessary. The theoretical
background relies on Lazard's structural theorem for lexicographic Groebner
bases in two variables. An implementation is realized in Magma. Benchmarks show
clearly the benefit, sometimes important, of this approach compared to the
Groebner bases approach.
"
1613,Fast Minimal Presentations of Bi-graded Persistence Modules,"  Multi-parameter persistent homology is a recent branch of topological data
analysis. In this area, data sets are investigated through the lens of homology
with respect to two or more scale parameters. The high computational cost of
many algorithms calls for a preprocessing step to reduce the input size. In
general, a minimal presentation is the smallest possible representation of a
persistence module. Lesnick and Wright proposed recently an algorithm (the
LW-algorithm) for computing minimal presentations based on matrix reduction. In
this work, we propose, implement and benchmark several improvements over the
LW-algorithm. Most notably, we propose the use of priority queues to avoid
extensive scanning of the matrix columns, which constitutes the computational
bottleneck in the LW-algorithm, and we combine their algorithm with ideas from
the multi-parameter chunk algorithm by Fugacci and Kerber. Our extensive
experiments show that our algorithm outperforms the LW-algorithm and computes
the minimal presentation for data sets with millions of simplices within a few
seconds. Our software is publicly available.
"
1614,The New Rewriting Engine of Dedukti,"  Dedukti is a type-checker for the $\lambda$$\Pi$-calculus modulo rewriting,
an extension of Edinburgh's logicalframework LF where functions and type
symbols can be defined by rewrite rules. It thereforecontains an engine for
rewriting LF terms and types according to the rewrite rules given by the user.A
key component of this engine is the matching algorithm to find which rules can
be fired. In thispaper, we describe the class of rewrite rules supported by
Dedukti and the new implementation ofthe matching algorithm. Dedukti supports
non-linear rewrite rules on terms with binders usinghigher-order
pattern-matching as in Combinatory Reduction Systems (CRS). The new
matchingalgorithm extends the technique of decision trees introduced by Luc
Maranget in the OCamlcompiler to this more general context.
"
1615,Rounding Error Analysis of Linear Recurrences Using Generating Series,"  We develop a toolbox for the error analysis of linear recurrences with
constant or polynomial coefficients, based on generating series, Cauchy's
method of majorants, and simple results from analytic combinatorics. We
illustrate the power of the approach by several nontrivial application
examples. Among these examples are a new worst-case analysis of an algorithm
for computing Bernoulli numbers, and a new algorithm for evaluating
differentially finite functions in interval arithmetic while avoiding interval
blow-up.
"
1616,Calcium: computing in exact real and complex fields,"  Calcium is a C library for real and complex numbers in a form suitable for
exact algebraic and symbolic computation. Numbers are represented as elements
of fields $\mathbb{Q}(a_1,\ldots,a_n)$ where the extensions numbers $a_k$ may
be algebraic or transcendental. The system combines efficient field operations
with automatic discovery and certification of algebraic relations, resulting in
a practical computational model of $\mathbb{R}$ and $\mathbb{C}$ in which
equality is rigorously decidable for a large class of numbers.
"
1617,Connectivity in Semi-Algebraic Sets I,"  A semi-algebraic set is a subset of the real space defined by polynomial
equations and inequalities having real coefficients and is a union of finitely
many maximally connected components. We consider the problem of deciding
whether two given points in a semi-algebraic set are connected; that is,
whether the two points lie in the same connected component. In particular, we
consider the semi-algebraic set defined by f <> 0 where f is a given polynomial
with integer coefficients. The motivation comes from the observation that many
important or non-trivial problems in science and engineering can be often
reduced to that of connectivity. Due to its importance, there has been intense
research effort on the problem. We will describe a symbolic-numeric method
based on gradient ascent. The method will be described in two papers. The first
paper (the present one) will describe the symbolic part and the forthcoming
second paper will describe the numeric part. In the present paper, we give
proofs of correctness and termination for the symbolic part and illustrate the
efficacy of the method using several non-trivial examples.
"
1618,"A Neuro-Symbolic Method for Solving Differential and Functional
  Equations","  When neural networks are used to solve differential equations, they usually
produce solutions in the form of black-box functions that are not directly
mathematically interpretable. We introduce a method for generating symbolic
expressions to solve differential equations while leveraging deep learning
training methods. Unlike existing methods, our system does not require learning
a language model over symbolic mathematics, making it scalable, compact, and
easily adaptable for a variety of tasks and configurations. As part of the
method, we propose a novel neural architecture for learning mathematical
expressions to optimize a customizable objective. The system is designed to
always return a valid symbolic formula, generating a useful approximation when
an exact analytic solution to a differential equation is not or cannot be
found. We demonstrate through examples how our method can be applied on a
number of differential equations, often obtaining symbolic approximations that
are useful or insightful. Furthermore, we show how the system can be
effortlessly generalized to find symbolic solutions to other mathematical
tasks, including integration and functional equations.
"
1619,Quadratization of ODEs: Monomial vs. Non-Monomial,"  Quadratization is a transform of a system of ODEs with polynomial right-hand
side into a system of ODEs with at most quadratic right-hand side via the
introduction of new variables. It has been recently used as a pre-processing
step for new model order reduction methods, so it is important to keep the
number of new variables small. Several algorithms have been designed to search
for a quadratization with the new variables being monomials in the original
variables. To understand the limitations and potential ways of improving such
algorithms, we study the following question: can quadratizations with not
necessarily monomial new variables produce a model of substantially smaller
dimension than quadratization with only monomial new variables?
  To do this, we restrict our attention to scalar polynomial ODEs. Our first
result is that a scalar polynomial ODE
$\dot{x}=p(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots + a_0$ with $n\geqslant 5$ and
$a_n\neq0$ can be quadratized using exactly one new variable if and only if
$p(x-\frac{a_{n-1}}{n\cdot a_n})=a_nx^n+ax^2+bx$ for some $a, b \in
\mathbb{C}$. In fact, the new variable can be taken
$z:=(x-\frac{a_{n-1}}{n\cdot a_n})^{n-1}$. Our second result is that two
non-monomial new variables are enough to quadratize all degree $6$ scalar
polynomial ODEs. Based on these results, we observe that a quadratization with
not necessarily monomial new variables can be much smaller than a monomial
quadratization even for scalar ODEs.
  The main results of the paper have been discovered using computational
methods of applied nonlinear algebra (Gr\""obner bases), and we describe these
computations.
"
1620,LDU factorization,"  LU-factorization of matrices is one of the fundamental algorithms of linear
algebra. The widespread use of supercomputers with distributed memory requires
a review of traditional algorithms, which were based on the common memory of a
computer. Matrix block recursive algorithms are a class of algorithms that
provide coarse-grained parallelization. The block recursive LU factorization
algorithm was obtained in 2010. This algorithm is called LEU-factorization. It,
like the traditional LU-algorithm, is designed for matrices over number fields.
However, it does not solve the problem of numerical instability. We propose a
generalization of the LEU algorithm to the case of a commutative domain and its
field of quotients. This LDU factorization algorithm decomposes the matrix over
the commutative domain into a product of three matrices, in which the matrices
L and U belong to the commutative domain, and the elements of the weighted
truncated permutation matrix D are the elements inverse to the product of some
pair of minors. All elements are calculated without errors, so the problem of
instability does not arise.
"
1621,"Invariants of Self-Intersected and Inversive N-Periodics in the Elliptic
  Billiard","  We study more invariants in the elliptic billiard, including those manifested
by self-intersected orbits and inversive polygons. We also derive expressions
for some entries in ""Eighty New Invariants of N-Periodics in the Elliptic
Billiard"" (2020), arXiv:2004.12497.
"
1622,Symbolically Solving Partial Differential Equations using Deep Learning,"  We describe a neural-based method for generating exact or approximate
solutions to differential equations in the form of mathematical expressions.
Unlike other neural methods, our system returns symbolic expressions that can
be interpreted directly. Our method uses a neural architecture for learning
mathematical expressions to optimize a customizable objective, and is scalable,
compact, and easily adaptable for a variety of tasks and configurations. The
system has been shown to effectively find exact or approximate symbolic
solutions to various differential equations with applications in natural
sciences. In this work, we highlight how our method applies to partial
differential equations over multiple variables and more complex boundary and
initial value conditions.
"
1623,"Sequence Positivity Through Numeric Analytic Continuation: Uniqueness of
  the Canham Model for Biomembranes","  We prove solution uniqueness for the genus one Canham variational problem
arising in the shape prediction of biomembranes. The proof builds on a result
of Yu and Chen that reduces the variational problem to proving non-negativity
of a sequence defined by a linear recurrence relation with polynomial
coefficients. We combine rigorous numeric analytic continuation of D-finite
functions with classic bounds from singularity analysis to derive an effective
index where the asymptotic behaviour of the sequence, which is positive,
dominates the sequence behaviour. Positivity of the finite number of remaining
terms is then checked computationally.
"
1624,"Representation of hypergeometric products of higher nesting depths in
  difference rings","  A non-trivial symbolic machinery is presented that can rephrase
algorithmically a finite set of nested hypergeometric products in appropriately
designed difference rings. As a consequence, one obtains an alternative
representation in terms of one single product defined over a root of unity and
nested hypergeometric products which are algebraically independent among each
other. In particular, one can solve the zero-recognition problem: the input
expression of nested hypergeometric products evaluates to zero if and only if
the output expression is the zero expression. Combined with available symbolic
summation algorithms in the setting of difference rings, one obtains a general
machinery that can represent (and simplify) nested sums defined over nested
products.
"
1625,An effective method for computing Grothendieck point residue mappings,"  Grothendieck point residue is considered in the context of computational
complex analysis. A new effective method is proposed for computing Grothendieck
point residues mappings and residues. Basic ideas of our approach are the use
of Grothendieck local duality and a transformation law for local cohomology
classes. A new tool is devised for efficiency to solve the extended ideal
membership problems in local rings. The resulting algorithms are described with
an example to illustrate them. An extension of the proposed method to
parametric cases is also discussed as an application.
"
1626,"Introduction to the GiNaC Framework for Symbolic Computation within the
  C++ Programming Language","  The traditional split-up into a low level language and a high level language
in the design of computer algebra systems may become obsolete with the advent
of more versatile computer languages. We describe GiNaC, a special-purpose
system that deliberately denies the need for such a distinction. It is entirely
written in C++ and the user can interact with it directly in that language. It
was designed to provide efficient handling of multivariate polynomials,
algebras and special functions that are needed for loop calculations in
theoretical quantum field theory. It also bears some potential to become a more
general purpose symbolic package.
"
1627,"A Lambda-Calculus with letrec, case, constructors and non-determinism","  A non-deterministic call-by-need lambda-calculus \calc with case,
constructors, letrec and a (non-deterministic) erratic choice, based on
rewriting rules is investigated. A standard reduction is defined as a variant
of left-most outermost reduction. The semantics is defined by contextual
equivalence of expressions instead of using $\alpha\beta(\eta)$-equivalence. It
is shown that several program transformations are correct, for example all
(deterministic) rules of the calculus, and in addition the rules for garbage
collection, removing indirections and unique copy.
  This shows that the combination of a context lemma and a meta-rewriting on
reductions using complete sets of commuting (forking, resp.) diagrams is a
useful and successful method for providing a semantics of a functional
programming language and proving correctness of program transformations.
"
1628,Rewriting Calculus: Foundations and Applications,"  This thesis is devoted to the study of a calculus that describes the
application of conditional rewriting rules and the obtained results at the same
level of representation. We introduce the rewriting calculus, also called the
rho-calculus, which generalizes the first order term rewriting and
lambda-calculus, and makes possible the representation of the non-determinism.
In our approach the abstraction operator as well as the application operator
are objects of calculus. The result of a reduction in the rewriting calculus is
either an empty set representing the application failure, or a singleton
representing a deterministic result, or a set having several elements
representing a not-deterministic choice of results.
  In this thesis we concentrate on the properties of the rewriting calculus
where a syntactic matching is used in order to bind the variables to their
current values. We define evaluation strategies ensuring the confluence of the
calculus and we show that these strategies become trivial for restrictions of
the general rewriting calculus to simpler calculi like the lambda-calculus. The
rewriting calculus is not terminating in the untyped case but the strong
normalization is obtained for the simply typed calculus.
  In the rewriting calculus extended with an operator allowing to test the
application failure we define terms representing innermost and outermost
normalizations with respect to a set of rewriting rules. By using these terms,
we obtain a natural and concise description of the conditional rewriting.
Finally, starting from the representation of the conditional rewriting rules,
we show how the rewriting calculus can be used to give a semantics to ELAN, a
language based on the application of rewriting rules controlled by strategies.
"
1629,Lectures on Reduce and Maple at UAM I - Mexico,"  These lectures give a brief introduction to the Computer Algebra systems
Reduce and Maple. The aim is to provide a systematic survey of most important
commands and concepts. In particular, this includes a discussion of
simplification schemes and the handling of simplification and substitution
rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification
rules).
  Another emphasis is on the different implementations of tensor calculi and
the exterior calculus by Reduce and Maple and their application in Gravitation
theory and Differential Geometry.
  I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa,
Departamento de Fisica, Mexico, in November 1999.
"
1630,The Set of Equations to Evaluate Objects,"  The notion of an equational shell is studied to involve the objects and their
environment. Appropriate methods are studied as valid embeddings of refined
objects. The refinement process determines the linkages between the variety of
possible representations giving rise to variants of computations. The case
study is equipped with the adjusted equational systems that validate the
initial applicative framework.
"
1631,MACE 2.0 Reference Manual and Guide,"  MACE is a program that searches for finite models of first-order statements.
The statement to be modeled is first translated to clauses, then to relational
clauses; finally for the given domain size, the ground instances are
constructed. A Davis-Putnam-Loveland-Logeman procedure decides the
propositional problem, and any models found are translated to first-order
models. MACE is a useful complement to the theorem prover Otter, with Otter
searching for proofs and MACE looking for countermodels.
"
1632,"TeXmacs interfaces to Maxima, MuPAD and REDUCE","  GNU TeXmacs is a free wysiwyg word processor providing an excellent
typesetting quality of texts and formulae. It can also be used as an interface
to Computer Algebra Systems (CASs). In the present work, interfaces to three
general-purpose CASs have been implemented.
"
1633,Set Unification,"  The unification problem in algebras capable of describing sets has been
tackled, directly or indirectly, by many researchers and it finds important
applications in various research areas--e.g., deductive databases, theorem
proving, static analysis, rapid software prototyping. The various solutions
proposed are spread across a large literature. In this paper we provide a
uniform presentation of unification of sets, formalizing it at the level of set
theory. We address the problem of deciding existence of solutions at an
abstract level. This provides also the ability to classify different types of
set unification problems. Unification algorithms are uniformly proposed to
solve the unification problem in each of such classes.
  The algorithms presented are partly drawn from the literature--and properly
revisited and analyzed--and partly novel proposals. In particular, we present a
new goal-driven algorithm for general ACI1 unification and a new simpler
algorithm for general (Ab)(Cl) unification.
"
1634,Performance Comparison of Function Evaluation Methods,"  We perform a comparison of the performance and efficiency of four different
function evaluation methods: black-box functions, binary trees, $n$-ary trees
and string parsing. The test consists in evaluating 8 different functions of
two variables $x,y$ over 5000 floating point values of the pair $(x,y)$. The
outcome of the test indicates that the $n$-ary tree representation of algebraic
expressions is the fastest method, closely followed by black-box function
method, then by binary trees and lastly by string parsing.
"
1635,A correct proof of the heuristic GCD algorithm,"  In this note, we fill a gap in the proof of the heuristic GCD in the
multivariate case made by Char, Geddes and Gonnet (JSC 1989) and give some
additionnal information on this method.
"
1636,"Orthonormal RBF wavelet and ridgelet-like series and transforms for
  high-dimensional problems","  This paper developed a systematic strategy establishing RBF on the wavelet
analysis, which includes continuous and discrete RBF orthonormal wavelet
transforms respectively in terms of singular fundamental solutions and
nonsingular general solutions of differential operators. In particular, the
harmonic Bessel RBF transforms were presented for high-dimensional data
processing. It was also found that the kernel functions of convection-diffusion
operator are feasible to construct some stable ridgelet-like RBF transforms. We
presented time-space RBF transforms based on non-singular solution and
fundamental solution of time-dependent differential operators. The present
methodology was further extended to analysis of some known RBFs such as the MQ,
Gaussian and pre-wavelet kernel RBFs.
"
1637,A Note on the DQ Analysis of Anisotropic Plates,"  Recently, Bert, Wang and Striz [1, 2] applied the differential quadrature
(DQ) and harmonic differential quadrature (HDQ) methods to analyze static and
dynamic behaviors of anisotropic plates. Their studies showed that the methods
were conceptually simple and computationally efficient in comparison to other
numerical techniques. Based on some recent work by the present author [3, 4],
the purpose of this note is to further simplify the formulation effort and
improve computing efficiency in applying the DQ and HDQ methods for these
cases.
"
1638,Parameterized Type Definitions in Mathematica: Methods and Advantages,"  The theme of symbolic computation in algebraic categories has become of
utmost importance in the last decade since it enables the automatic modeling of
modern algebra theories. On this theoretical background, the present paper
reveals the utility of the parameterized categorical approach by deriving a
multivariate polynomial category (over various coefficient domains), which is
used by our Mathematica implementation of Buchberger's algorithms for
determining the Groebner basis. These implementations are designed according to
domain and category parameterization principles underlining their advantages:
operation protection, inheritance, generality, easy extendibility. In
particular, such an extension of Mathematica, a widely used symbolic
computation system, with a new type system has a certain practical importance.
The approach we propose for Mathematica is inspired from D. Gruntz and M.
Monagan's work in Gauss, for Maple.
"
1639,A comparison of four approaches to the calculation of conservation laws,"  The paper compares computational aspects of four approaches to compute
conservation laws of single differential equations (DEs) or systems of them,
ODEs and PDEs. The only restriction, required by two of the four corresponding
computer algebra programs, is that each DE has to be solvable for a leading
derivative. Extra constraints for the conservation laws can be specified.
Examples include new conservation laws that are non-polynomial in the
functions, that have an explicit variable dependence and families of
conservation laws involving arbitrary functions. The following equations are
investigated in examples: Ito, Liouville, Burgers, Kadomtsev-Petviashvili,
Karney-Sen-Chu-Verheest, Boussinesq, Tzetzeica, Benney.
"
1640,"The integration of systems of linear PDEs using conservation laws of
  syzygies","  A new integration technique is presented for systems of linear partial
differential equations (PDEs) for which syzygies can be formulated that obey
conservation laws. These syzygies come for free as a by-product of the
differential Groebner Basis computation. Compared with the more obvious way of
integrating a single equation and substituting the result in other equations
the new technique integrates more than one equation at once and therefore
introduces temporarily fewer new functions of integration that in addition
depend on fewer variables. Especially for high order PDE systems in many
variables the conventional integration technique may lead to an explosion of
the number of functions of integration which is avoided with the new method. A
further benefit is that redundant free functions in the solution are either
prevented or that their number is at least reduced.
"
1641,Size reduction and partial decoupling of systems of equations,"  A method is presented that reduces the number of terms of systems of linear
equations (algebraic, ordinary and partial differential equations). As a
byproduct these systems have a tendency to become partially decoupled and are
more likely to be factorizable or integrable. A variation of this method is
applicable to non-linear systems. Modifications to improve efficiency are given
and examples are shown. This procedure can be used in connection with the
computation of the radical of a differential ideal (differential Groebner
basis).
"
1642,Hidden Polynomial(s) Cryptosystems,"  We propose variations of the class of hidden monomial cryptosystems in order
to make it resistant to all known attacks. We use identities built upon a
single bivariate polynomial equation with coefficients in a finite field.
Indeed, it can be replaced by a ``small'' ideal, as well. Throughout, we set up
probabilistic encryption protocols, too. The same ideas extend to digital
signature algorithms, as well. Our schemes work as well on differential fields
of positive characteristic, and elsewhere.
"
1643,"Numerical Coverage Estimation for the Symbolic Simulation of Real-Time
  Systems","  Three numerical coverage metrics for the symbolic simulation of dense-time
systems and their estimation methods are presented. Special techniques to
derive numerical estimations of dense-time state-spaces have also been
developed. Properties of the metrics are also discussed with respect to four
criteria. Implementation and experiments are then reported.
"
1644,TCTL Inevitability Analysis of Dense-time Systems,"  Inevitability properties in branching temporal logics are of the syntax
forall eventually \phi, where \phi is an arbitrary (timed) CTL formula. In the
sense that ""good things will happen"", they are parallel to the ""liveness""
properties in linear temporal logics. Such inevitability properties in
dense-time logics can be analyzed with greatest fixpoint calculation. We
present algorithms to model-check inevitability properties both with and
without requirement of non-Zeno computations. We discuss a technique for early
decision on greatest fixpoints in the temporal logics, and experiment with the
effect of non-Zeno computations on the evaluation of greatest fixpoints. We
also discuss the TCTL subclass with only universal path quantifiers which
allows for the safe abstraction analysis of inevitability properties. Finally,
we report our implementation and experiments to show the plausibility of our
ideas.
"
1645,Quasi-Optimal Arithmetic for Quaternion Polynomials,"  Fast algorithms for arithmetic on real or complex polynomials are well-known
and have proven to be not only asymptotically efficient but also very
practical. Based on Fast Fourier Transform (FFT), they for instance multiply
two polynomials of degree up to N or multi-evaluate one at N points
simultaneously within quasi-linear time O(N.polylog N). An extension to (and in
fact the mere definition of) polynomials over the skew-field H of quaternions
is promising but still missing. The present work proposes three such
definitions which in the commutative case coincide but for H turn out to
differ, each one satisfying some desirable properties while lacking others. For
each notion we devise algorithms for according arithmetic; these are
quasi-optimal in that their running times match lower complexity bounds up to
polylogarithmic factors.
"
1646,Hidden Polynomial(s) Cryptosystems,"  We propose public-key cryptosystems with public key a system of polynomial
equations, algebraic or differential, and private key a single polynomial or a
small-size ideal. We set up probabilistic encryption, signature, and
signcryption protocols.
"
1647,gTybalt - a free computer algebra system,"  This article documents the free computer algebra system ""gTybalt"". The
program is build on top of other packages, among others GiNaC, TeXmacs and
Root. It offers the possibility of interactive symbolic calculations within the
C++ programming language. Mathematical formulae are visualized using TeX fonts.
"
1648,Cryptanalysis of HFE,"  I transform the trapdoor problem of HFE into a linear algebra problem.
"
1649,Model Checking Linear Logic Specifications,"  The overall goal of this paper is to investigate the theoretical foundations
of algorithmic verification techniques for first order linear logic
specifications. The fragment of linear logic we consider in this paper is based
on the linear logic programming language called LO enriched with universally
quantified goal formulas. Although LO was originally introduced as a
theoretical foundation for extensions of logic programming languages, it can
also be viewed as a very general language to specify a wide range of
infinite-state concurrent systems.
  Our approach is based on the relation between backward reachability and
provability highlighted in our previous work on propositional LO programs.
Following this line of research, we define here a general framework for the
bottom-up evaluation of first order linear logic specifications. The evaluation
procedure is based on an effective fixpoint operator working on a symbolic
representation of infinite collections of first order linear logic formulas.
The theory of well quasi-orderings can be used to provide sufficient conditions
for the termination of the evaluation of non trivial fragments of first order
linear logic.
"
1650,"Digital Version of Green`s Theorem and its Application to The Coverage
  Problem in Formal Verification","  We present a novel scheme to the coverage problem, introducing a quantitative
way to estimate the interaction between a block and its enviroment.This is
achieved by setting a discrete version of Green`s theorem, specially adapted
for Model Checking based verification of integrated circuits.This method is
best suited for the coverage problem since it enables one to quantify the
incompleteness or, on the other hand, the redundancy of a set of rules,
describing the model under verification.Moreover this can be done continuously
throughout the verification process, thus enabling the user to pinpoint the
stages at which incompleteness/redundancy occurs. Although the method is
presented locally on a small hardware example, we additionally show its
possibility to provide precise coverage estimation also for large scale
systems. We compare this method to others by checking it on the same
test-cases.
"
1651,"A uniform approach to constraint-solving for lists, multisets, compact
  lists, and sets","  Lists, multisets, and sets are well-known data structures whose usefulness is
widely recognized in various areas of Computer Science. These data structures
have been analyzed from an axiomatic point of view with a parametric approach
in (*) where the relevant unification algorithms have been developed. In this
paper we extend these results considering more general constraints including
not only equality but also membership constraints as well as their negative
counterparts.
  (*) A. Dovier, A. Policriti, and G. Rossi. A uniform axiomatic view of lists,
multisets, and sets, and the relevant unification algorithms. Fundamenta
Informaticae, 36(2/3):201--234, 1998.
"
1652,"Computing Igusa's Local Zeta Functions of Univariate Polynomials, and
  Linear Feedback Shift Registers","  We give a polynomial time algorithm for computing the Igusa local zeta
function $Z(s,f)$ attached to a polynomial $f(x)\in \QTR{Bbb}{Z}[x]$, in one
variable, with splitting field $\QTR{Bbb}{Q}$, and a prime number $p$. We also
propose a new class of Linear Feedback Shift Registers based on the computation
of Igusa's local zeta function.
"
1653,Mace4 Reference Manual and Guide,"  Mace4 is a program that searches for finite models of first-order formulas.
For a given domain size, all instances of the formulas over the domain are
constructed. The result is a set of ground clauses with equality. Then, a
decision procedure based on ground equational rewriting is applied. If
satisfiability is detected, one or more models are printed. Mace4 is a useful
complement to first-order theorem provers, with the prover searching for proofs
and Mace4 looking for countermodels, and it is useful for work on finite
algebras. Mace4 performs better on equational problems than did our previous
model-searching program Mace2.
"
1654,OTTER 3.3 Reference Manual,"  OTTER is a resolution-style theorem-proving program for first-order logic
with equality. OTTER includes the inference rules binary resolution,
hyperresolution, UR-resolution, and binary paramodulation. Some of its other
abilities and features are conversion from first-order formulas to clauses,
forward and back subsumption, factoring, weighting, answer literals, term
ordering, forward and back demodulation, evaluable functions and predicates,
Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is
free, and is portable to many different kinds of computer.
"
1655,Weak Bezout inequality for D-modules,"  Let $\{w_{i,j}\}_{1\leq i\leq n, 1\leq j\leq s} \subset
L_m=F(X_1,...,X_m)[{\partial \over \partial X_1},..., {\partial \over \partial
X_m}]$ be linear partial differential operators of orders with respect to
${\partial \over \partial X_1},..., {\partial \over \partial X_m}$ at most $d$.
We prove an upper bound n(4m^2d\min\{n,s\})^{4^{m-t-1}(2(m-t))} on the leading
coefficient of the Hilbert-Kolchin polynomial of the left $L_m$-module
$<\{w_{1,j}, ..., w_{n,j}\}_{1\leq j \leq s} > \subset L_m^n$ having the
differential type $t$ (also being equal to the degree of the Hilbert-Kolchin
polynomial). The main technical tool is the complexity bound on solving systems
of linear equations over {\it algebras of fractions} of the form
$$L_m(F[X_1,..., X_m, {\partial \over \partial X_1},..., {\partial \over
\partial X_k}])^{-1}.$$
"
1656,Algebraic Elimination of epsilon-transitions,"  We present here algebraic formulas associating a k-automaton to a
k-epsilon-automaton. The existence depends on the definition of the star of
matrices and of elements in the semiring k. For this reason, we present the
theorem which allows the transformation of k-epsilon-automata into k-automata.
The two automata have the same behaviour.
"
1657,"Polynomial-time computing over quadratic maps I: sampling in real
  algebraic sets","  Given a quadratic map Q : K^n -> K^k defined over a computable subring D of a
real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider
the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a
procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of
(real univariate representations of) sampling points in K^n that intersects
nontrivially each connected component of Z. As soon as k=o(n), this is faster
than the standard methods that all have exponential dependence on n in the
complexity. In particular, our procedure is polynomial-time for constant k. In
contrast, the best previously known procedure (due to A.Barvinok) is only
capable of deciding in n^O(k^2) operations the nonemptiness (rather than
constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and
homogeneous Q.
  A by-product of our procedure is a bound (dn)^O(k) on the number of connected
components of Z.
  The procedure consists of exact symbolic computations in D and outputs
vectors of algebraic numbers. It involves extending K by infinitesimals and
subsequent limit computation by a novel procedure that utilizes knowledge of an
explicit isomorphism between real algebraic sets.
"
1658,Efficient dot product over word-size finite fields,"  We want to achieve efficiency for the exact computation of the dot product of
two vectors over word-size finite fields. We therefore compare the practical
behaviors of a wide range of implementation techniques using different
representations. The techniques used include oating point representations,
discrete logarithms, tabulations, Montgomery reduction, delayed modulus.
"
1659,Computing Multi-Homogeneous Bezout Numbers is Hard,"  The multi-homogeneous Bezout number is a bound for the number of solutions of
a system of multi-homogeneous polynomial equations, in a suitable product of
projective spaces.
  Given an arbitrary, not necessarily multi-homogeneous system, one can ask for
the optimal multi-homogenization that would minimize the Bezout number.
  In this paper, it is proved that the problem of computing, or even estimating
the optimal multi-homogeneous Bezout number is actually NP-hard.
  In terms of approximation theory for combinatorial optimization, the problem
of computing the best multi-homogeneous structure does not belong to APX,
unless P = NP.
  Moreover, polynomial time algorithms for estimating the minimal
multi-homogeneous Bezout number up to a fixed factor cannot exist even in a
randomized setting, unless BPP contains NP.
"
1660,"An unexpected application of minimization theory to module
  decompositions","  The aim of this work is to show how we can decompose a module (if
decomposable) into an indecomposable module with the help of the minimization
process.
"
1661,"A Framework for Combining Defeasible Argumentation with Labeled
  Deduction","  In the last years, there has been an increasing demand of a variety of
logical systems, prompted mostly by applications of logic in AI and other
related areas. Labeled Deductive Systems (LDS) were developed as a flexible
methodology to formalize such a kind of complex logical systems. Defeasible
argumentation has proven to be a successful approach to formalizing commonsense
reasoning, encompassing many other alternative formalisms for defeasible
reasoning. Argument-based frameworks share some common notions (such as the
concept of argument, defeater, etc.) along with a number of particular features
which make it difficult to compare them with each other from a logical
viewpoint. This paper introduces LDSar, a LDS for defeasible argumentation in
which many important issues concerning defeasible argumentation are captured
within a unified logical framework. We also discuss some logical properties and
extensions that emerge from the proposed framework.
"
1662,A novel approach to symbolic algebra,"  A prototype for an extensible interactive graphical term manipulation system
is presented that combines pattern matching and nondeterministic evaluation to
provide a convenient framework for doing tedious algebraic manipulations that
so far had to be done manually in a semi-automatic fashion.
"
1663,Abstract Canonical Inference,"  An abstract framework of canonical inference is used to explore how different
proof orderings induce different variants of saturation and completeness.
Notions like completion, paramodulation, saturation, redundancy elimination,
and rewrite-system reduction are connected to proof orderings. Fairness of
deductive mechanisms is defined in terms of proof orderings, distinguishing
between (ordinary) ""fairness,"" which yields completeness, and ""uniform
fairness,"" which yields saturation.
"
1664,On rational definite summation,"  We present a partial proof of van Hoeij-Abramov conjecture about the
algorithmic possibility of computation of finite sums of rational functions.
The theoretical results proved in this paper provide an algorithm for
computation of a large class of sums $ S(n) = \sum_{k=0}^{n-1}R(k,n)$.
"
1665,ParFORM: Parallel Version of the Symbolic Manipulation Program FORM,"  After an introduction to the sequential version of FORM and the mechanisms
behind, we report on the status of our project of parallelization. We have now
a parallel version of FORM running on Cluster- and SMP-architectures. This
version can be used to run arbitrary FORM programs in parallel.
"
1666,Maple+GrTensorII libraries for cosmology,"  The article mainly presents some results in using MAPLE platform for computer
algebra and GrTensorII package in doing calculations for theoretical and
numerical cosmology
"
1667,"Efficient polynomial time algorithms computing industrial-strength
  primitive roots","  E. Bach, following an idea of T. Itoh, has shown how to build a small set of
numbers modulo a prime p such that at least one element of this set is a
generator of $\pF{p}$\cite{Bach:1997:sppr,Itoh:2001:PPR}. E. Bach suggests also
that at least half of his set should be generators. We show here that a slight
variant of this set can indeed be made to contain a ratio of primitive roots as
close to 1 as necessary. We thus derive several algorithms computing primitive
roots correct with very high probability in polynomial time. In particular we
present an asymptotically $O^{\sim}(\sqrt{\frac{1}{\epsilon}}log^1.5(p) +
\log^2(p))$ algorithm providing primitive roots of $p$ with probability of
correctness greater than $1-\epsilon$ and several $O(log^\alpha(p))$, $\alpha
\leq 5.23$ algorithms computing ""Industrial-strength"" primitive roots with
probabilities e.g. greater than the probability of ""hardware malfunctions"".
"
1668,FORM Matters: Fast Symbolic Computation under UNIX,"  We give a brief introduction to FORM, a symbolic programming language for
massive batch operations, designed by J.A.M. Vermaseren. In particular, we
stress various methods to efficiently use FORM under the UNIX operating system.
Several scripts and examples are given, and suggestions on how to use the vim
editor as development platform.
"
1669,An Example of Clifford Algebras Calculations with GiNaC,"  This example of Clifford algebras calculations uses GiNaC
(http://www.ginac.de/) library, which includes a support for generic Clifford
algebra starting from version~1.3.0. Both symbolic and numeric calculation are
possible and can be blended with other functions of GiNaC. This calculations
was made for the paper math.CV/0410399.
  Described features of GiNaC are already available at PyGiNaC
(http://sourceforge.net/projects/pyginac/) and due to course should propagate
into other software like GNU Octave (http://www.octave.org/), gTybalt
(http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as
their back-end.
"
1670,"From Tensor Equations to Numerical Code -- Computer Algebra Tools for
  Numerical Relativity","  In this paper we present our recent work in developing a computer-algebra
tool for systems of partial differential equations (PDEs), termed ""Kranc"". Our
work is motivated by the problem of finding solutions of the Einstein equations
through numerical simulations. Kranc consists of Mathematica based
computer-algebra packages, that facilitate the task of dealing with symbolic
tensorial calculations and realize the conversion of systems of partial
differential evolution equations into parallelized C or Fortran code.
"
1671,Jordan Normal and Rational Normal Form Algorithms,"  In this paper, we present a determinist Jordan normal form algorithms based
on the Fadeev formula: \[(\lambda \cdot I-A) \cdot B(\lambda)=P(\lambda) \cdot
I\] where $B(\lambda)$ is $(\lambda \cdot I-A)$'s comatrix and $P(\lambda)$ is
$A$'s characteristic polynomial. This rational Jordan normal form algorithm
differs from usual algorithms since it is not based on the Frobenius/Smith
normal form but rather on the idea already remarked in Gantmacher that the
non-zero column vectors of $B(\lambda_0)$ are eigenvectors of $A$ associated to
$\lambda_0$ for any root $\lambda_0$ of the characteristical polynomial. The
complexity of the algorithm is $O(n^4)$ field operations if we know the
factorization of the characteristic polynomial (or $O(n^5 \ln(n))$ operations
for a matrix of integers of fixed size). This algorithm has been implemented
using the Maple and Giac/Xcas computer algebra systems.
"
1672,"Free quasi-symmetric functions, product actions and quantum field theory
  of partitions","  We examine two associative products over the ring of symmetric functions
related to the intransitive and Cartesian products of permutation groups. As an
application, we give an enumeration of some Feynman type diagrams arising in
Bender's QFT of partitions. We end by exploring possibilities to construct
noncommutative analogues.
"
1673,Implementation of Motzkin-Burger algorithm in Maple,"  Subject of this paper is an implementation of a well-known Motzkin-Burger
algorithm, which solves the problem of finding the full set of solutions of a
system of linear homogeneous inequalities. There exist a number of
implementations of this algorithm, but there was no one in Maple, to the best
of the author's knowledge.
"
1674,"Generalized Laplace transformations and integration of hyperbolic
  systems of linear partial differential equations","  We give a new procedure for generalized factorization and construction of the
complete solution of strictly hyperbolic linear partial differential equations
or strictly hyperbolic systems of such equations in the plane. This procedure
generalizes the classical theory of Laplace transformations of second-order
equations in the plane.
"
1675,Efficient Computation of the Characteristic Polynomial,"  This article deals with the computation of the characteristic polynomial of
dense matrices over small finite fields and over the integers. We first present
two algorithms for the finite fields: one is based on Krylov iterates and
Gaussian elimination. We compare it to an improvement of the second algorithm
of Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's third
algorithm could improve both complexity and computational time. We use these
results as a basis for the computation of the characteristic polynomial of
integer matrices. We first use early termination and Chinese remaindering for
dense matrices. Then a probabilistic approach, based on integer minimal
polynomial and Hensel factorization, is particularly well suited to sparse
and/or structured matrices.
"
1676,Can Computer Algebra be Liberated from its Algebraic Yoke ?,"  So far, the scope of computer algebra has been needlessly restricted to exact
algebraic methods. Its possible extension to approximate analytical methods is
discussed. The entangled roles of functional analysis and symbolic programming,
especially the functional and transformational paradigms, are put forward. In
the future, algebraic algorithms could constitute the core of extended symbolic
manipulation systems including primitives for symbolic approximations.
"
1677,"The complexity of computing the Hilbert polynomial of smooth
  equidimensional complex projective varieties","  We continue the study of counting complexity begun in [Buergisser, Cucker 04]
and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the
complexity of computing the Hilbert polynomial of a homogeneous ideal. We show
that the problem of computing the Hilbert polynomial of a smooth
equidimensional complex projective variety can be reduced in polynomial time to
the problem of counting the number of complex common zeros of a finite set of
multivariate polynomials. Moreover, we prove that the more general problem of
computing the Hilbert polynomial of a homogeneous ideal is polynomial space
hard. This implies polynomial space lower bounds for both the problems of
computing the rank and the Euler characteristic of cohomology groups of
coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC
1999).
"
1678,"Approximation of dynamical systems using S-systems theory : application
  to biological systems","  In this paper we propose a new symbolic-numeric algorithm to find positive
equilibria of a n-dimensional dynamical system. This algorithm implies a
symbolic manipulation of ODE in order to give a local approximation of
differential equations with power-law dynamics (S-systems). A numerical
calculus is then needed to converge towards an equilibrium, giving at the same
time a S-system approximating the initial system around this equilibrium. This
algorithm is applied to a real biological example in 14 dimensions which is a
subsystem of a metabolic pathway in Arabidopsis Thaliana.
"
1679,Tensor manipulation in GPL Maxima,"  GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA.
GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but
these were in various states of disrepair. One of the two packages, CTENSOR,
implemented component-based tensor manipulation; the other, ITENSOR, treated
tensor symbols as opaque, manipulating them based on their index properties.
The present paper describes the state in which these packages were found, the
steps that were needed to make the packages fully functional again, and the new
functionality that was implemented to make them more versatile. A third
package, ATENSOR, was also implemented; fully compatible with the identically
named package in the commercial version of MACSYMA, ATENSOR implements abstract
tensor algebras.
"
1680,TeXmacs-maxima interface,"  This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later).
"
1681,Computing the Rank and a Small Nullspace Basis of a Polynomial Matrix,"  We reduce the problem of computing the rank and a nullspace basis of a
univariate polynomial matrix to polynomial matrix multiplication. For an input
n x n matrix of degree d over a field K we give a rank and nullspace algorithm
using about the same number of operations as for multiplying two matrices of
dimension n and degree d. If the latter multiplication is done in
MM(n,d)=softO(n^omega d) operations, with omega the exponent of matrix
multiplication over K, then the algorithm uses softO(MM(n,d)) operations in K.
The softO notation indicates some missing logarithmic factors. The method is
randomized with Las Vegas certification. We achieve our results in part through
a combination of matrix Hensel high-order lifting and matrix minimal fraction
reconstruction, and through the computation of minimal or small degree vectors
in the nullspace seen as a K[x]-module
"
1682,"Asymptotically fast polynomial matrix algorithms for multivariable
  systems","  We present the asymptotically fastest known algorithms for some basic
problems on univariate polynomial matrices: rank, nullspace, determinant,
generic inverse, reduced form. We show that they essentially can be reduced to
two computer algebra techniques, minimal basis computations and matrix fraction
expansion/reconstruction, and to polynomial matrix multiplication. Such
reductions eventually imply that all these problems can be solved in about the
same amount of time as polynomial matrix multiplication.
"
1683,A formally verified proof of the prime number theorem,"  The prime number theorem, established by Hadamard and de la Vall'ee Poussin
independently in 1896, asserts that the density of primes in the positive
integers is asymptotic to 1 / ln x. Whereas their proofs made serious use of
the methods of complex analysis, elementary proofs were provided by Selberg and
Erd""os in 1948. We describe a formally verified version of Selberg's proof,
obtained using the Isabelle proof assistant.
"
1684,"A Maple Package for Computing Groebner Bases for Linear Recurrence
  Relations","  A Maple package for computing Groebner bases of linear difference ideals is
described. The underlying algorithm is based on Janet and Janet-like monomial
divisions associated with finite difference operators. The package can be used,
for example, for automatic generation of difference schemes for linear partial
differential equations and for reduction of multiloop Feynman integrals. These
two possible applications are illustrated by simple examples of the Laplace
equation and a one-loop scalar integral of propagator type
"
1685,Computing the Kalman form,"  We present two algorithms for the computation of the Kalman form of a linear
control system. The first one is based on the technique developed by
Keller-Gehrig for the computation of the characteristic polynomial. The cost is
a logarithmic number of matrix multiplications. To our knowledge, this improves
the best previously known algebraic complexity by an order of magnitude. Then
we also present a cubic algorithm proven to more efficient in practice.
"
1686,Feynman graphs and related Hopf algebras,"  In a recent series of communications we have shown that the reordering
problem of bosons leads to certain combinatorial structures. These structures
may be associated with a certain graphical description. In this paper, we show
that there is a Hopf Algebra structure associated with this problem which is,
in a certain sense, unique.
"
1687,Towards a diagrammatic modeling of the LinBox C++ linear algebra library,"  We propose a new diagrammatic modeling language, DML. The paradigm used is
that of the category theory and in particular of the pushout tool. We show that
most of the object-oriented structures can be described with this tool and have
many examples in C++, ranging from virtual inheritance and polymorphism to
template genericity. With this powerful tool, we propose a quite simple
description of the C++ LinBox library. This library has been designed for
efficiency and genericity and therefore makes heavy usage of complex template
and polymorphic mecanism. Be reverse engineering, we are able to describe in a
simple manner the complex structure of archetypes in LinBox.
"
1688,ParFORM: recent development,"  We report on the status of our project of parallelization of the symbolic
manipulation program FORM. We have now parallel versions of FORM running on
Cluster- or SMP-architectures. These versions can be used to run arbitrary FORM
programs in parallel.
"
1689,"Fast (Multi-)Evaluation of Linearly Recurrent Sequences: Improvements
  and Applications","  For a linearly recurrent vector sequence P[n+1] = A(n) * P[n], consider the
problem of calculating either the n-th term P[n] or L<=n arbitrary terms
P[n_1],...,P[n_L], both for the case of constant coefficients A(n)=A and for a
matrix A(n) with entries polynomial in n. We improve and extend known
algorithms for this problem and present new applications for it. Specifically
it turns out that for instance * any family (p_n) of classical orthogonal
polynomials admits evaluation at given x within O(n^{1/2} log n) operations
INDEPENDENT of the family (p_n) under consideration. * For any L indices
n_1,...,n_L <= n, the values p_{n_i}(x) can be calculated simultaneously using
O(n^{1/2} log n + L log(n/L)) arithmetic operations; again this running time
bound holds uniformly. * Every hypergeometric (or, more generally, holonomic)
function admits approximate evaluation up to absolute error e>0 within
O((log(1/e)^{1/2} loglog(1/e)) -- as opposed to O(log(1/e)) -- arithmetic
steps. * Given m and a polynomial p of degree d over a field of characteristic
zero, the coefficient of p^m to term X^n can be computed within O(d^2
M(n^{1/2})) steps where M(n) denotes the cost of multiplying two degree-n
polynomials. * The same time bound holds for the joint calculation of any
L<=n^{1/2} desired coefficients of p^m to terms X^{n_i}, n_1,...,n_L <= n.
"
1690,An introspective algorithm for the integer determinant,"  We present an algorithm computing the determinant of an integer matrix A. The
algorithm is introspective in the sense that it uses several distinct
algorithms that run in a concurrent manner. During the course of the algorithm
partial results coming from distinct methods can be combined. Then, depending
on the current running time of each method, the algorithm can emphasize a
particular variant. With the use of very fast modular routines for linear
algebra, our implementation is an order of magnitude faster than other existing
implementations. Moreover, we prove that the expected complexity of our
algorithm is only O(n^3 log^{2.5}(n ||A||)) bit operations in the dense case
and O(Omega n^{1.5} log^2(n ||A||) + n^{2.5}log^3(n||A||)) in the sparse case,
where ||A|| is the largest entry in absolute value of the matrix and Omega is
the cost of matrix-vector multiplication in the case of a sparse matrix.
"
1691,Solving Partial Order Constraints for LPO Termination,"  This paper introduces a new kind of propositional encoding for reasoning
about partial orders. The symbols in an unspecified partial order are viewed as
variables which take integer values and are interpreted as indices in the
order. For a partial order statement on n symbols each index is represented in
log2 n propositional variables and partial order constraints between symbols
are modeled on the bit representations. We illustrate the application of our
approach to determine LPO termination for term rewrite systems. Experimental
results are unequivocal, indicating orders of magnitude speedups in comparison
with current implementations for LPO termination. The proposed encoding is
general and relevant to other applications which involve propositional
reasoning about partial orders.
"
1692,Computations with one and two real algebraic numbers,"  We present algorithmic and complexity results concerning computations with
one and two real algebraic numbers, as well as real solving of univariate
polynomials and bivariate polynomial systems with integer coefficients using
Sturm-Habicht sequences.
  Our main results, in the univariate case, concern the problems of real root
isolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate,
the problems of system real solving (Th.42), sign evaluation (Th. 37) and
simultaneous inequalities (Cor. 43).
"
1693,Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC,"  This paper presents an implementation of the
Schwerdtfeger-Fillmore-Springer-Cnops construction (SFSCc) along with
illustrations of its usage. SFSCc linearises the linear-fraction action of the
Moebius group in R^n. This has clear advantages in several theoretical and
applied fields including engineering. Our implementation is based on the
Clifford algebra capacities of the GiNaC computer algebra system
(http://www.ginac.de/), which were described in cs.MS/0410044.
  The core of this realisation of SFSCc is done for an arbitrary dimension of
R^n with a metric given by an arbitrary bilinear form. We also present a
subclass for two dimensional cycles (i.e. circles, parabolas and hyperbolas),
which add some 2D specific routines including a visualisation to PostScript
files through the MetaPost (http://www.tug.org/metapost.html) or Asymptote
(http://asymptote.sourceforge.net/) packages.
  This software is the backbone of many results published in math.CV/0512416
and we use its applications their for demonstration. The library can be ported
(with various level of required changes) to other CAS with Clifford algebras
capabilities similar to GiNaC.
  There is an ISO image of a Live Debian DVD attached to this paper as an
auxiliary file, a copy is stored on Google Drive as well.
"
1694,A Constructive Semantic Characterization of Aggregates in ASP,"  This technical note describes a monotone and continuous fixpoint operator to
compute the answer sets of programs with aggregates. The fixpoint operator
relies on the notion of aggregate solution. Under certain conditions, this
operator behaves identically to the three-valued immediate consequence operator
$\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al.
This operator allows us to closely tie the computational complexity of the
answer set checking and answer sets existence problems to the cost of checking
a solution of the aggregates in the program. Finally, we relate the semantics
described by the operator to other proposals for logic programming with
aggregates.
  To appear in Theory and Practice of Logic Programming (TPLP).
"
1695,"The complexity of class polynomial computation via floating point
  approximations","  We analyse the complexity of computing class polynomials, that are an
important ingredient for CM constructions of elliptic curves, via complex
floating point approximations of their roots. The heart of the algorithm is the
evaluation of modular functions in several arguments. The fastest one of the
presented approaches uses a technique devised by Dupont to evaluate modular
functions by Newton iterations on an expression involving the
arithmetic-geometric mean. It runs in time $O (|D| \log^5 |D| \log \log |D|) =
O (|D|^{1 + \epsilon}) = O (h^{2 + \epsilon})$ for any $\epsilon > 0$, where
$D$ is the CM discriminant and $h$ is the degree of the class polynomial.
Another fast algorithm uses multipoint evaluation techniques known from
symbolic computation; its asymptotic complexity is worse by a factor of $\log
|D|$. Up to logarithmic factors, this running time matches the size of the
constructed polynomials. The estimate also relies on a new result concerning
the complexity of enumerating the class group of an imaginary-quadratic order
and on a rigorously proven upper bound for the height of class polynomials.
"
1696,Dense Linear Algebra over Finite Fields: the FFLAS and FFPACK packages,"  In the past two decades, some major efforts have been made to reduce exact
(e.g. integer, rational, polynomial) linear algebra problems to matrix
multiplication in order to provide algorithms with optimal asymptotic
complexity. To provide efficient implementations of such algorithms one need to
be careful with the underlying arithmetic. It is well known that modular
techniques such as the Chinese remainder algorithm or the p-adic lifting allow
very good practical performance, especially when word size arithmetic are used.
Therefore, finite field arithmetic becomes an important core for efficient
exact linear algebra libraries. In this paper, we study high performance
implementations of basic linear algebra routines over word size prime fields:
specially the matrix multiplication; our goal being to provide an exact
alternate to the numerical BLAS library. We show that this is made possible by
a carefull combination of numerical computations and asymptotically faster
algorithms. Our kernel has several symbolic linear algebra applications enabled
by diverse matrix multiplication reductions: symbolic triangularization, system
solving, determinant and matrix inverse implementations are thus studied.
"
1697,Demand Analysis with Partial Predicates,"  In order to alleviate the inefficiencies caused by the interaction of the
logic and functional sides, integrated languages may take advantage of
\emph{demand} information -- i.e. knowing in advance which computations are
needed and, to which extent, in a particular context. This work studies
\emph{demand analysis} -- which is closely related to \emph{backwards
strictness analysis} -- in a semantic framework of \emph{partial predicates},
which in turn are constructive realizations of ideals in a domain. This will
allow us to give a concise, unified presentation of demand analysis, to relate
it to other analyses based on abstract interpretation or strictness logics,
some hints for the implementation, and, more important, to prove the soundness
of our analysis based on \emph{demand equations}. There are also some
innovative results. One of them is that a set constraint-based analysis has
been derived in a stepwise manner using ideas taken from the area of program
transformation. The other one is the possibility of using program
transformation itself to perform the analysis, specially in those domains of
properties where algorithms based on constraint solving are too weak.
"
1698,Computing spectral sequences,"  In this paper, a set of programs enhancing the Kenzo system is presented.
Kenzo is a Common Lisp program designed for computing in Algebraic Topology, in
particular it allows the user to calculate homology and homotopy groups of
complicated spaces. The new programs presented here entirely compute Serre and
Eilenberg-Moore spectral sequences, in particular the groups and differential
maps for arbitrary r. They also determine when the spectral sequence has
converged and describe the filtration of the target homology groups induced by
the spectral sequence.
"
1699,"Parallel Symbolic Computation of Curvature Invariants in General
  Relativity","  We present a practical application of parallel symbolic computation in
General Relativity: the calculation of curvature invariants for large
dimension. We discuss the structure of the calculations, an implementation of
the technique and scaling of the computation with spacetime dimension for
various invariants.
"
1700,Unary Primitive Recursive Functions,"  In this article, we study some new characterizations of primitive recursive
functions based on restricted forms of primitive recursion, improving the
pioneering work of R. M. Robinson and M. D. Gladstone in this area. We reduce
certain recursion schemes (mixed/pure iteration without parameters) and we
characterize one-argument primitive recursive functions as the closure under
substitution and iteration of certain optimal sets.
"
1701,An Explicit Solution to Post's Problem over the Reals,"  In the BCSS model of real number computations we prove a concrete and
explicit semi-decidable language to be undecidable yet not reducible from (and
thus strictly easier than) the real Halting Language. This solution to Post's
Problem over the reals significantly differs from its classical, discrete
variant where advanced diagonalization techniques are only known to yield the
existence of such intermediate Turing degrees. Strengthening the above result,
we construct (that is, obtain again explicitly) as well an uncountable number
of incomparable semi-decidable Turing degrees below the real Halting problem in
the BCSS model. Finally we show the same to hold for the linear BCSS model,
that is over (R,+,-,<) rather than (R,+,-,*,/,<).
"
1702,Solving Sparse Integer Linear Systems,"  We propose a new algorithm to solve sparse linear systems of equations over
the integers. This algorithm is based on a $p$-adic lifting technique combined
with the use of block matrices with structured blocks. It achieves a sub-cubic
complexity in terms of machine operations subject to a conjecture on the
effectiveness of certain sparse projections. A LinBox-based implementation of
this algorithm is demonstrated, and emphasizes the practical benefits of this
new method over the previous state of the art.
"
1703,Benchmark Problems for Constraint Solving,"  Constraint Programming is roughly a new software technology introduced by
Jaffar and Lassez in 1987 for description and effective solving of large,
particularly combinatorial, problems especially in areas of planning and
scheduling. In the following we define three problems for constraint solving
from the domain of electrical networks; based on them we define 43 related
problems. For the defined set of problems we benchmarked five systems: ILOG
OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems
performed very well for some problems while others performed very well on
others.
"
1704,"Real Computational Universality: The Word Problem for a class of groups
  with infinite presentation","  The word problem for discrete groups is well-known to be undecidable by a
Turing Machine; more precisely, it is reducible both to and from and thus
equivalent to the discrete Halting Problem.
  The present work introduces and studies a real extension of the word problem
for a certain class of groups which are presented as quotient groups of a free
group and a normal subgroup. Most important, the free group will be generated
by an uncountable set of generators with index running over certain sets of
real numbers. This allows to include many mathematically important groups which
are not captured in the framework of the classical word problem.
  Our contribution extends computational group theory from the discrete to the
Blum-Shub-Smale (BSS) model of real number computation. We believe this to be
an interesting step towards applying BSS theory, in addition to semi-algebraic
geometry, also to further areas of mathematics.
  The main result establishes the word problem for such groups to be not only
semi-decidable (and thus reducible FROM) but also reducible TO the Halting
Problem for such machines. It thus provides the first non-trivial example of a
problem COMPLETE, that is, computationally universal for this model.
"
1705,"Extension of the functionality of the symbolic program FORM by external
  software","  We describe the implementation of facilities for the communication with
external resources in the Symbolic Manipulation System FORM. This is done
according to the POSIX standards defined for the UNIX operating system. We
present a number of examples that illustrate the increased power due to these
new capabilities.
"
1706,"Polynomial Time Nondimensionalisation of Ordinary Differential Equations
  via their Lie Point Symmetries","  Lie group theory states that knowledge of a $m$-parameters solvable group of
symmetries of a system of ordinary differential equations allows to reduce by
$m$ the number of equation. We apply this principle by finding dilatations and
translations that are Lie point symmetries of considered ordinary differential
system. By rewriting original problem in an invariant coordinates set for these
symmetries, one can reduce the involved number of parameters. This process is
classically call nondimensionalisation in dimensional analysis. We present an
algorithm based on this standpoint and show that its arithmetic complexity is
polynomial in input's size.
"
1707,Univariate polynomial real root isolation: Continued Fractions revisited,"  We present algorithmic, complexity and implementation results concerning real
root isolation of integer univariate polynomials using the continued fraction
expansion of real algebraic numbers. One motivation is to explain the method's
good performance in practice. We improve the previously known bound by a factor
of $d \tau$, where $d$ is the polynomial degree and $\tau$ bounds the
coefficient bitsize, thus matching the current record complexity for real root
isolation by exact methods. Namely, the complexity bound is $\sOB(d^4 \tau^2)$
using the standard bound on the expected bitsize of the integers in the
continued fraction expansion. We show how to compute the multiplicities within
the same complexity and extend the algorithm to non square-free polynomials.
Finally, we present an efficient open-source \texttt{C++} implementation in the
algebraic library \synaps, and illustrate its efficiency as compared to other
available software. We use polynomials with coefficient bitsize up to 8000 and
degree up to 1000.
"
1708,"A Recursive Method for Determining the One-Dimensional Submodules of
  Laurent-Ore Modules","  We present a method for determining the one-dimensional submodules of a
Laurent-Ore module. The method is based on a correspondence between
hyperexponential solutions of associated systems and one-dimensional
submodules. The hyperexponential solutions are computed recursively by solving
a sequence of first-order ordinary matrix equations. As the recursion proceeds,
the matrix equations will have constant coefficients with respect to the
operators that have been considered.
"
1709,"Fast computation of power series solutions of systems of differential
  equations","  We propose new algorithms for the computation of the first N terms of a
vector (resp. a basis) of power series solutions of a linear system of
differential equations at an ordinary point, using a number of arithmetic
operations which is quasi-linear with respect to N. Similar results are also
given in the non-linear case. This extends previous results obtained by Brent
and Kung for scalar differential equations of order one and two.
"
1710,SAT Techniques for Lexicographic Path Orders,"  This seminar report is concerned with expressing LPO-termination of term
rewrite systems as a satisfiability problem in propositional logic. After
relevant algorithms are explained, experimental results are reported.
"
1711,Low Complexity Algorithms for Linear Recurrences,"  We consider two kinds of problems: the computation of polynomial and rational
solutions of linear recurrences with coefficients that are polynomials with
integer coefficients; indefinite and definite summation of sequences that are
hypergeometric over the rational numbers. The algorithms for these tasks all
involve as an intermediate quantity an integer $N$ (dispersion or root of an
indicial polynomial) that is potentially exponential in the bit size of their
input. Previous algorithms have a bit complexity that is at least quadratic in
$N$. We revisit them and propose variants that exploit the structure of
solutions and avoid expanding polynomials of degree $N$. We give two
algorithms: a probabilistic one that detects the existence or absence of
nonzero polynomial and rational solutions in $O(\sqrt{N}\log^{2}N)$ bit
operations; a deterministic one that computes a compact representation of the
solution in $O(N\log^{3}N)$ bit operations. Similar speed-ups are obtained in
indefinite and definite hypergeometric summation. We describe the results of an
implementation.
"
1712,"Efficient algorithm for computing the Euler-Poincar\'e characteristic of
  a semi-algebraic set defined by few quadratic inequalities","  We present an algorithm which takes as input a closed semi-algebraic set, $S
\subset \R^k$, defined by \[ P_1 \leq 0, ..., P_\ell \leq 0, P_i \in
\R[X_1,...,X_k], \deg(P_i) \leq 2, \] and computes the Euler-Poincar\'e
characteristic of $S$. The complexity of the algorithm is $k^{O(\ell)}$.
"
1713,"Complexity of Resolution of Parametric Systems of Polynomial Equations
  and Inequations","  Consider a system of n polynomial equations and r polynomial inequations in n
indeterminates of degree bounded by d with coefficients in a polynomial ring of
s parameters with rational coefficients of bit-size at most $\sigma$. From the
real viewpoint, solving such a system often means describing some
semi-algebraic sets in the parameter space over which the number of real
solutions of the considered parametric system is constant. Following the works
of Lazard and Rouillier, this can be done by the computation of a discriminant
variety. In this report we focus on the case where for a generic specialization
of the parameters the system of equations generates a radical zero-dimensional
ideal, which is usual in the applications. In this case, we provide a
deterministic method computing the minimal discriminant variety reducing the
problem to a problem of elimination. Moreover, we prove that the degree of the
computed minimal discriminant variety is bounded by $D:=(n+r)d^{(n+1)}$ and
that the complexity of our method is $\sigma^{\mathcal{O}(1)}
D^{\mathcal{O}(n+s)}$ bit-operations on a deterministic Turing machine.
"
1714,"On computing fixpoints in well-structured regular model checking, with
  applications to lossy channel systems","  We prove a general finite convergence theorem for ""upward-guarded"" fixpoint
expressions over a well-quasi-ordered set. This has immediate applications in
regular model checking of well-structured systems, where a main issue is the
eventual convergence of fixpoint computations. In particular, we are able to
directly obtain several new decidability results on lossy channel systems.
"
1715,A field-theory motivated approach to symbolic computer algebra,"  Field theory is an area in physics with a deceptively compact notation.
Although general purpose computer algebra systems, built around generic
list-based data structures, can be used to represent and manipulate
field-theory expressions, this often leads to cumbersome input formats,
unexpected side-effects, or the need for a lot of special-purpose code. This
makes a direct translation of problems from paper to computer and back
needlessly time-consuming and error-prone. A prototype computer algebra system
is presented which features TeX-like input, graph data structures, lists with
Young-tableaux symmetries and a multiple-inheritance property system. The
usefulness of this approach is illustrated with a number of explicit
field-theory problems.
"
1716,ACD Term Rewriting,"  We introduce Associative Commutative Distributive Term Rewriting (ACDTR), a
rewriting language for rewriting logical formulae. ACDTR extends AC term
rewriting by adding distribution of conjunction over other operators.
Conjunction is vital for expressive term rewriting systems since it allows us
to require that multiple conditions hold for a term rewriting rule to be used.
ACDTR uses the notion of a ""conjunctive context"", which is the conjunction of
constraints that must hold in the context of a term, to enable the programmer
to write very expressive and targeted rewriting rules. ACDTR can be seen as a
general logic programming language that extends Constraint Handling Rules and
AC term rewriting. In this paper we define the semantics of ACDTR and describe
our prototype implementation.
"
1717,Satisfying KBO Constraints,"  This paper presents two new approaches to prove termination of rewrite
systems with the Knuth-Bendix order efficiently. The constraints for the weight
function and for the precedence are encoded in (pseudo-)propositional logic and
the resulting formula is tested for satisfiability. Any satisfying assignment
represents a weight function and a precedence such that the induced
Knuth-Bendix order orients the rules of the encoded rewrite system from left to
right.
"
1718,"A linear algebra approach to the differentiation index of generic DAE
  systems","  The notion of differentiation index for DAE systems of arbitrary order with
generic second members is discussed by means of the study of the behavior of
the ranks of certain Jacobian associated sub-matrices. As a by-product, we
obtain upper bounds for the regularity of the Hilbert-Kolchin function and the
order of the ideal associated to the DAE systems under consideration, not
depending on characteristic sets. Some quantitative and algorithmic results
concerning differential transcendence bases and induced equivalent explicit ODE
systems are also established.
"
1719,Fast algorithms for computing isogenies between elliptic curves,"  We survey algorithms for computing isogenies between elliptic curves defined
over a field of characteristic either 0 or a large prime. We introduce a new
algorithm that computes an isogeny of degree $\ell$ ($\ell$ different from the
characteristic) in time quasi-linear with respect to $\ell$. This is based in
particular on fast algorithms for power series expansion of the Weierstrass
$\wp$-function and related functions.
"
1720,"On factorization and solution of multidimensional linear partial
  differential equations","  We describe a method of obtaining closed-form complete solutions of certain
second-order linear partial differential equations with more than two
independent variables. This method generalizes the classical method of Laplace
transformations of second-order hyperbolic equations in the plane and is based
on an idea given by Ulisse Dini in 1902.
"
1721,"Strong bi-homogeneous B\'{e}zout theorem and its use in effective real
  algebraic geometry","  Let f1, ..., fs be a polynomial family in Q[X1,..., Xn] (with s less than n)
of degree bounded by D. Suppose that f1, ..., fs generates a radical ideal, and
defines a smooth algebraic variety V. Consider a projection P. We prove that
the degree of the critical locus of P restricted to V is bounded by
D^s(D-1)^(n-s) times binomial of n and n-s. This result is obtained in two
steps. First the critical points of P restricted to V are characterized as
projections of the solutions of Lagrange's system for which a bi-homogeneous
structure is exhibited. Secondly we prove a bi-homogeneous B\'ezout Theorem,
which bounds the sum of the degrees of the equidimensional components of the
radical of an ideal generated by a bi-homogeneous polynomial family. This
result is improved when f1,..., fs is a regular sequence. Moreover, we use
Lagrange's system to design an algorithm computing at least one point in each
connected component of a smooth real algebraic set. This algorithm generalizes,
to the non equidimensional case, the one of Safey El Din and Schost. The
evaluation of the output size of this algorithm gives new upper bounds on the
first Betti number of a smooth real algebraic set. Finally, we estimate its
arithmetic complexity and prove that in the worst cases it is polynomial in n,
s, D^s(D-1)^(n-s) and the binomial of n and n-s, and the complexity of
evaluation of f1,..., fs.
"
1722,List Decoding of Hermitian Codes using Groebner Bases,"  List decoding of Hermitian codes is reformulated to allow an efficient and
simple algorithm for the interpolation step. The algorithm is developed using
the theory of Groebner bases of modules. The computational complexity of the
algorithm seems comparable to previously known algorithms achieving the same
task, and the algorithm is better suited for hardware implementation.
"
1723,Bounds on the coefficients of the characteristic and minimal polynomials,"  This note presents absolute bounds on the size of the coefficients of the
characteristic and minimal polynomials depending on the size of the
coefficients of the associated matrix. Moreover, we present algorithms to
compute more precise input-dependant bounds on these coefficients. Such bounds
are e.g. useful to perform deterministic chinese remaindering of the
characteristic or minimal polynomial of an integer matrix.
"
1724,Groebner Bases Applied to Systems of Linear Difference Equations,"  In this paper we consider systems of partial (multidimensional) linear
difference equations. Specifically, such systems arise in scientific computing
under discretization of linear partial differential equations and in
computational high energy physics as recurrence relations for multiloop Feynman
integrals. The most universal algorithmic tool for investigation of linear
difference systems is based on their transformation into an equivalent Groebner
basis form. We present an algorithm for this transformation implemented in
Maple. The algorithm and its implementation can be applied to automatic
generation of difference schemes for linear partial differential equations and
to reduction of Feynman integrals. Some illustrative examples are given.
"
1725,Predicate Abstraction via Symbolic Decision Procedures,"  We present a new approach for performing predicate abstraction based on
symbolic decision procedures. Intuitively, a symbolic decision procedure for a
theory takes a set of predicates in the theory and symbolically executes a
decision procedure on all the subsets over the set of predicates. The result of
the symbolic decision procedure is a shared expression (represented by a
directed acyclic graph) that implicitly represents the answer to a predicate
abstraction query.
  We present symbolic decision procedures for the logic of Equality and
Uninterpreted Functions (EUF) and Difference logic (DIFF) and show that these
procedures run in pseudo-polynomial (rather than exponential) time. We then
provide a method to construct symbolic decision procedures for simple mixed
theories (including the two theories mentioned above) using an extension of the
Nelson-Oppen combination method. We present preliminary evaluation of our
Procedure on predicate abstraction benchmarks from device driver verification
in SLAM.
"
1726,"Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted
  Finite-State Machines","  The automatic extraction of acronyms and their meaning from corpora is an
important sub-task of text mining. It can be seen as a special case of string
alignment, where a text chunk is aligned with an acronym. Alternative
alignments have different cost, and ideally the least costly one should give
the correct meaning of the acronym. We show how this approach can be
implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which
reads a text chunk on tape 1 and an acronym on tape 2, and generates all
alternative alignments on tape 3. The 3-WFSM can be automatically generated
from a simple regular expression. No additional algorithms are required at any
stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the
best analysis of an acronym in a few milliseconds.
"
1727,Viterbi Algorithm Generalized for n-Tape Best-Path Search,"  We present a generalization of the Viterbi algorithm for identifying the path
with minimal (resp. maximal) weight in a n-tape weighted finite-state machine
(n-WFSM), that accepts a given n-tuple of input strings (s_1,... s_n). It also
allows us to compile the best transduction of a given input n-tuple by a
weighted (n+m)-WFSM (transducer) with n input and m output tapes. Our algorithm
has a worst-case time complexity of O(|s|^n |E| log (|s|^n |Q|)), where n and
|s| are the number and average length of the strings in the n-tuple, and |Q|
and |E| the number of states and transitions in the n-WFSM, respectively. A
straight forward alternative, consisting in intersection followed by classical
shortest-distance search, operates in O(|s|^n (|E|+|Q|) log (|s|^n |Q|)) time.
"
1728,Explicit factors of some iterated resultants and discriminants,"  In this paper, the result of applying iterative univariate resultant
constructions to multivariate polynomials is analyzed. We consider the input
polynomials as generic polynomials of a given degree and exhibit explicit
decompositions into irreducible factors of several constructions involving two
times iterated univariate resultants and discriminants over the integer
universal ring of coefficients of the entry polynomials. Cases involving from
two to four generic polynomials and resultants or discriminants in one of their
variables are treated. The decompositions into irreducible factors we get are
obtained by exploiting fundamental properties of the univariate resultants and
discriminants and induction on the degree of the polynomials. As a consequence,
each irreducible factor can be separately and explicitly computed in terms of a
certain multivariate resultant. With this approach, we also obtain as direct
corollaries some results conjectured by Collins and McCallum which correspond
to the case of polynomials whose coefficients are themselves generic
polynomials in other variables. Finally, a geometric interpretation of the
algebraic factorization of the iterated discriminant of a single polynomial is
detailled.
"
1729,"Reduction of Algebraic Parametric Systems by Rectification of their
  Affine Expanded Lie Symmetries","  Lie group theory states that knowledge of a $m$-parameters solvable group of
symmetries of a system of ordinary differential equations allows to reduce by
$m$ the number of equations. We apply this principle by finding some
\emph{affine derivations} that induces \emph{expanded} Lie point symmetries of
considered system. By rewriting original problem in an invariant coordinates
set for these symmetries, we \emph{reduce} the number of involved parameters.
We present an algorithm based on this standpoint whose arithmetic complexity is
\emph{quasi-polynomial} in input's size.
"
1730,Symmetric Subresultants and Applications,"  Schur's transforms of a polynomial are used to count its roots in the unit
disk. These are generalized them by introducing the sequence of symmetric
sub-resultants of two polynomials. Although they do have a determinantal
definition, we show that they satisfy a structure theorem which allows us to
compute them with a type of Euclidean division. As a consequence, a fast
algorithm based on a dichotomic process and FFT is designed. We prove also that
these symmetric sub-resultants have a deep link with Toeplitz matrices.
Finally, we propose a new algorithm of inversion for such matrices. It has the
same cost as those already known, however it is fraction-free and consequently
well adapted to computer algebra.
"
1731,Maximum Entropy in the framework of Algebraic Statistics: A First Step,"  Algebraic statistics is a recently evolving field, where one would treat
statistical models as algebraic objects and thereby use tools from
computational commutative algebra and algebraic geometry in the analysis and
computation of statistical models. In this approach, calculation of parameters
of statistical models amounts to solving set of polynomial equations in several
variables, for which one can use celebrated Grobner bases theory. Owing to the
important role of information theory in statistics, this paper as a first step,
explores the possibility of describing maximum and minimum entropy (ME) models
in the framework of algebraic statistics. We show that ME-models are toric
models (a class of algebraic statistical models) when the constraint functions
(that provide the information about the underlying random variable) are integer
valued functions, and the set of statistical models that results from
ME-methods are indeed an affine variety.
"
1732,"Signature Sequence of Intersection Curve of Two Quadrics for Exact
  Morphological Classification","  We present an efficient method for classifying the morphology of the
intersection curve of two quadrics (QSIC) in PR3, 3D real projective space;
here, the term morphology is used in a broad sense to mean the shape,
topological, and algebraic properties of a QSIC, including singularity,
reducibility, the number of connected components, and the degree of each
irreducible component, etc. There are in total 35 different QSIC morphologies
with non-degenerate quadric pencils. For each of these 35 QSIC morphologies,
through a detailed study of the eigenvalue curve and the index function jump we
establish a characterizing algebraic condition expressed in terms of the Segre
characteristics and the signature sequence of a quadric pencil. We show how to
compute a signature sequence with rational arithmetic so as to determine the
morphology of the intersection curve of any two given quadrics. Two immediate
applications of our results are the robust topological classification of QSIC
in computing B-rep surface representation in solid modeling and the derivation
of algebraic conditions for collision detection of quadric primitives.
"
1733,Time- and Space-Efficient Evaluation of Some Hypergeometric Constants,"  The currently best known algorithms for the numerical evaluation of
hypergeometric constants such as $\zeta(3)$ to $d$ decimal digits have time
complexity $O(M(d) \log^2 d)$ and space complexity of $O(d \log d)$ or $O(d)$.
Following work from Cheng, Gergel, Kim and Zima, we present a new algorithm
with the same asymptotic complexity, but more efficient in practice. Our
implementation of this algorithm improves slightly over existing programs for
the computation of $\pi$, and we announce a new record of 2 billion digits for
$\zeta(3)$.
"
1734,"A Prototype for Educational Planning Using Course Constraints to
  Simulate Student Populations","  Distance learning universities usually afford their students the flexibility
to advance their studies at their own pace. This can lead to a considerable
fluctuation of student populations within a program's courses, possibly
affecting the academic viability of a program as well as the related required
resources. Providing a method that estimates this population could be of
substantial help to university management and academic personnel. We describe
how to use course precedence constraints to calculate alternative tuition paths
and then use Markov models to estimate future populations. In doing so, we
identify key issues of a large scale potential deployment.
"
1735,"Certification of the QR factor R, and of lattice basis reducedness","  Given a lattice basis of n vectors in Z^n, we propose an algorithm using
12n^3+O(n^2) floating point operations for checking whether the basis is
LLL-reduced. If the basis is reduced then the algorithm will hopefully answer
''yes''. If the basis is not reduced, or if the precision used is not
sufficient with respect to n, and to the numerical properties of the basis, the
algorithm will answer ''failed''. Hence a positive answer is a rigorous
certificate. For implementing the certificate itself, we propose a floating
point algorithm for computing (certified) error bounds for the entries of the R
factor of the QR matrix factorization. This algorithm takes into account all
possible approximation and rounding errors. The cost 12n^3+O(n^2) of the
certificate is only six times more than the cost of numerical algorithms for
computing the QR factorization itself, and the certificate may be implemented
using matrix library routines only. We report experiments that show that for a
reduced basis of adequate dimension and quality the certificate succeeds, and
establish the effectiveness of the certificate. This effectiveness is applied
for certifying the output of fastest existing floating point heuristics of LLL
reduction, without slowing down the whole process.
"
1736,"Faster Inversion and Other Black Box Matrix Computations Using Efficient
  Block Projections","  Block projections have been used, in [Eberly et al. 2006], to obtain an
efficient algorithm to find solutions for sparse systems of linear equations. A
bound of softO(n^(2.5)) machine operations is obtained assuming that the input
matrix can be multiplied by a vector with constant-sized entries in softO(n)
machine operations. Unfortunately, the correctness of this algorithm depends on
the existence of efficient block projections, and this has been conjectured. In
this paper we establish the correctness of the algorithm from [Eberly et al.
2006] by proving the existence of efficient block projections over sufficiently
large fields. We demonstrate the usefulness of these projections by deriving
improved bounds for the cost of several matrix problems, considering, in
particular, ``sparse'' matrices that can be be multiplied by a vector using
softO(n) field operations. We show how to compute the inverse of a sparse
matrix over a field F using an expected number of softO(n^(2.27)) operations in
F. A basis for the null space of a sparse matrix, and a certification of its
rank, are obtained at the same cost. An application to Kaltofen and Villard's
Baby-Steps/Giant-Steps algorithms for the determinant and Smith Form of an
integer matrix yields algorithms requiring softO(n^(2.66)) machine operations.
The derived algorithms are all probabilistic of the Las Vegas type.
"
1737,A canonical form for some piecewise defined functions,"  We define a canonical form for piecewise defined functions. We show that this
has a wider range of application as well as better complexity properties than
previous work.
"
1738,Towards a New ODE Solver Based on Cartan's Equivalence Method,"  The aim of the present paper is to propose an algorithm for a new ODE--solver
which should improve the abilities of current solvers to handle second order
differential equations. The paper provides also a theoretical result revealing
the relationship between the change of coordinates, that maps the generic
equation to a given target equation, and the symmetry $\D$-groupoid of this
target.
"
1739,"Formal proof for delayed finite field arithmetic using floating point
  operators","  Formal proof checkers such as Coq are capable of validating proofs of
correction of algorithms for finite field arithmetics but they require
extensive training from potential users. The delayed solution of a triangular
system over a finite field mixes operations on integers and operations on
floating point numbers. We focus in this report on verifying proof obligations
that state that no round off error occurred on any of the floating point
operations. We use a tool named Gappa that can be learned in a matter of
minutes to generate proofs related to floating point arithmetic and hide
technicalities of formal proof checkers. We found that three facilities are
missing from existing tools. The first one is the ability to use in Gappa new
lemmas that cannot be easily expressed as rewriting rules. We coined the second
one ``variable interchange'' as it would be required to validate loop
interchanges. The third facility handles massive loop unrolling and argument
instantiation by generating traces of execution for a large number of cases. We
hope that these facilities may sometime in the future be integrated into
mainstream code validation.
"
1740,Differential Equations for Algebraic Functions,"  It is classical that univariate algebraic functions satisfy linear
differential equations with polynomial coefficients. Linear recurrences follow
for the coefficients of their power series expansions. We show that the linear
differential equation of minimal order has coefficients whose degree is cubic
in the degree of the function. We also show that there exists a linear
differential equation of order linear in the degree whose coefficients are only
of quadratic degree. Furthermore, we prove the existence of recurrences of
order and degree close to optimal. We study the complexity of computing these
differential equations and recurrences. We deduce a fast algorithm for the
expansion of algebraic series.
"
1741,"Factorization of linear partial differential operators and Darboux
  integrability of nonlinear PDEs","  Using a new definition of generalized divisors we prove that the lattice of
such divisors for a given linear partial differential operator is modular and
obtain analogues of the well-known theorems of the Loewy-Ore theory of
factorization of linear ordinary differential operators. Possible applications
to factorized Groebner bases computations in the commutative and
non-commutative cases are discussed, an application to finding criterions of
Darboux integrability of nonlinear PDEs is given.
"
1742,Formulas as Programs,"  We provide here a computational interpretation of first-order logic based on
a constructive interpretation of satisfiability w.r.t. a fixed but arbitrary
interpretation. In this approach the formulas themselves are programs. This
contrasts with the so-called formulas as types approach in which the proofs of
the formulas are typed terms that can be taken as programs. This view of
computing is inspired by logic programming and constraint logic programming but
differs from them in a number of crucial aspects.
  Formulas as programs is argued to yield a realistic approach to programming
that has been realized in the implemented programming language ALMA-0 (Apt et
al.) that combines the advantages of imperative and logic programming. The work
here reported can also be used to reason about the correctness of non-recursive
ALMA-0 programs that do not include destructive assignment.
"
1743,Object Oriented and Functional Programming for Symbolic Manipulation,"  The advantages of mixed approach with using different kinds of programming
techniques for symbolic manipulation are discussed. The main purpose of
approach offered is merge the methods of object oriented programming that
convenient for presentation data and algorithms for user with advantages of
functional languages for data manipulation, internal presentation, and
portability of software.
"
1744,Events in Property Patterns,"  A pattern-based approach to the presentation, codification and reuse of
property specifications for finite-state verification was proposed by Dwyer and
his collegues. The patterns enable non-experts to read and write formal
specifications for realistic systems and facilitate easy conversion of
specifications between formalisms, such as LTL, CTL, QRE. In this paper, we
extend the pattern system with events - changes of values of variables in the
context of LTL.
"
1745,"Gravity, torsion, Dirac field and computer algebra using MAPLE and
  REDUCE","  The article presents computer algebra procedures and routines applied to the
study of the Dirac field on curved spacetimes. The main part of the procedures
is devoted to the construction of Pauli and Dirac matrices algebra on an
anholonomic orthonormal reference frame. Then these procedures are used to
compute the Dirac equation on curved spacetimes in a sequence of special
dedicated routines. A comparative review of such procedures obtained for two
computer algebra platforms (REDUCE + EXCALC and MAPLE + GRTensorII) is carried
out. Applications for the calculus of Dirac equation on specific examples of
spacetimes with or without torsion are pointed out.
"
1746,The Multithreaded version of FORM,"  We present TFORM, the version of the symbolic manipulation system FORM that
can make simultaneous use of several processors in a shared memory
architecture. The implementation uses Posix threads, also called pthreads, and
is therefore easily portable between various operating systems. Most existing
FORM programs will be able to take advantage of the increased processing power,
without the need for modifications. In some cases some minor additions may be
needed. For a computer with two processors a typical improvement factor in the
running time is 1.7 when compared to the traditional version of FORM. In the
case of computers with 4 processors a typical improvement factor in the
execution time is slightly above 3.
"
1747,"Mapping the vacuum structure of gauged maximal supergravities: an
  application of high-performance symbolic algebra","  The analysis of the extremal structure of the scalar potentials of gauged
maximally extended supergravity models in five, four, and three dimensions, and
hence the determination of possible vacuum states of these models is a
computationally challenging task due to the occurrence of the exceptional Lie
groups $E_6$, $E_7$, $E_8$ in the definition of these potentials. At present,
the most promising approach to gain information about nontrivial vacua of these
models is to perform a truncation of the potential to submanifolds of the $G/H$
coset manifold of scalars which are invariant under a subgroup of the gauge
group and of sufficiently low dimension to make an analytic treatment possible.
  New tools are presented which allow a systematic and highly effective study
of these potentials up to a previously unreached level of complexity. Explicit
forms of new truncations of the potentials of four- and three-dimensional
models are given, and for N=16, D=3 supergravities, which are much more rich in
structure than their higher-dimensional cousins, a series of new nontrivial
vacua is identified and analysed.
"
1748,"All order epsilon-expansion of Gauss hypergeometric functions with
  integer and half/integer values of parameters","  It is proved that the Laurent expansion of the following Gauss hypergeometric
functions,
  2F1(I1+a*epsilon, I2+b*ep; I3+c*epsilon;z),
  2F1(I1+a*epsilon, I2+b*epsilon;I3+1/2+c*epsilon;z),
  2F1(I1+1/2+a*epsilon, I2+b*epsilon; I3+c*epsilon;z),
  2F1(I1+1/2+a*epsilon, I2+b*epsilon; I3+1/2+c*epsilon;z),
  2F1(I1+1/2+a*epsilon,I2+1/2+b*epsilon; I3+1/2+c*epsilon;z), where I1,I2,I3
are an arbitrary integer nonnegative numbers, a,b,c are an arbitrary numbers
and epsilon is an arbitrary small parameters, are expressible in terms of the
harmonic polylogarithms of Remiddi and Vermaseren with polynomial coefficients.
An efficient algorithm for the calculation of the higher-order coefficients of
Laurent expansion is constructed. Some particular cases of Gauss hypergeometric
functions are also discussed.
"
1749,Symbolic Expansion of Transcendental Functions,"  Higher transcendental function occur frequently in the calculation of Feynman
integrals in quantum field theory. Their expansion in a small parameter is a
non-trivial task. We report on a computer program which allows the systematic
expansion of certain classes of functions. The algorithms are based on the Hopf
algebra of nested sums. The program is written in C++ and uses the GiNaC
library.
"
1750,On Compatibility of Discrete Relations,"  An approach to compatibility analysis of systems of discrete relations is
proposed. Unlike the Groebner basis technique, the proposed scheme is not based
on the polynomial ring structure. It uses more primitive set-theoretic and
topological concepts and constructions. We illustrate the approach by
application to some two-state cellular automata. In the two-state case the
Groebner basis method is also applicable, and we compare both approaches.
"
1751,"Finding Liouvillian first integrals of rational ODEs of any order in
  finite terms","  It is known, due to Mordukhai-Boltovski, Ritt, Prelle, Singer, Christopher
and others, that if a given rational ODE has a Liouvillian first integral then
the corresponding integrating factor of the ODE must be of a very special form
of a product of powers and exponents of irreducible polynomials. These results
lead to a partial algorithm for finding Liouvillian first integrals. However,
there are two main complications on the way to obtaining polynomials in the
integrating factor form. First of all, one has to find an upper bound for the
degrees of the polynomials in the product above, an unsolved problem, and then
the set of coefficients for each of the polynomials by the
computationally-intensive method of undetermined parameters. As a result, this
approach was implemented in CAS only for first and relatively simple second
order ODEs. We propose an algebraic method for finding polynomials of the
integrating factors for rational ODEs of any order, based on examination of the
resultants of the polynomials in the numerator and the denominator of the
right-hand side of such equation. If both the numerator and the denominator of
the right-hand side of such ODE are not constants, the method can determine in
finite terms an explicit expression of an integrating factor if the ODE permits
integrating factors of the above mentioned form and then the Liouvillian first
integral. The tests of this procedure based on the proposed method, implemented
in Maple in the case of rational integrating factors, confirm the consistence
and efficiency of the method.
"
1752,"The Hilbert Zonotope and a Polynomial Time Algorithm for Universal
  Grobner Bases","  We provide a polynomial time algorithm for computing the universal Gr\""obner
basis of any polynomial ideal having a finite set of common zeros in fixed
number of variables. One ingredient of our algorithm is an effective
construction of the state polyhedron of any member of the Hilbert scheme
Hilb^d_n of n-long d-variate ideals, enabled by introducing the Hilbert
zonotope H^d_n and showing that it simultaneously refines all state polyhedra
of ideals on Hilb^d_n.
"
1753,Ideal decompositions and computation of tensor normal forms,"  Symmetry properties of r-times covariant tensors T can be described by
certain linear subspaces W of the group ring K[S_r] of a symmetric group S_r.
If for a class of tensors T such a W is known, the elements of the orthogonal
subspace W^{\bot} of W within the dual space of K[S_r] yield linear identities
needed for a treatment of the term combination problem for the coordinates of
the T. We give the structure of these W for every situation which appears in
symbolic tensor calculations by computer. Characterizing idempotents of such W
can be determined by means of an ideal decomposition algorithm which works in
every semisimple ring up to an isomorphism. Furthermore, we use tools such as
the Littlewood-Richardson rule, plethysms and discrete Fourier transforms for
S_r to increase the efficience of calculations. All described methods were
implemented in a Mathematica package called PERMS.
"
1754,"Determination of the structure of algebraic curvature tensors by means
  of Young symmetrizers","  For a positive definite fundamental tensor all known examples of Osserman
algebraic curvature tensors have a typical structure. They can be produced from
a metric tensor and a finite set of skew-symmetric matrices which fulfil
Clifford commutation relations. We show by means of Young symmetrizers and a
theorem of S. A. Fulling, R. C. King, B. G. Wybourne and C. J. Cummins that
every algebraic curvature tensor has a structure which is very similar to that
of the above Osserman curvature tensors. We verify our results by means of the
Littlewood-Richardson rule and plethysms. For certain symbolic calculations we
used the Mathematica packages MathTensor, Ricci and PERMS.
"
1755,"On the symmetry classes of the first covariant derivatives of tensor
  fields","  We show that the symmetry classes of torsion-free covariant derivatives
$\nabla T$ of r-times covariant tensor fields T can be characterized by
Littlewood-Richardson products $\sigma [1]$ where $\sigma$ is a representation
of the symmetric group $S_r$ which is connected with the symmetry class of T.
If $\sigma = [\lambda]$ is irreducible then $\sigma [1]$ has a multiplicity
free reduction $[\lambda][1] = \sum [\mu]$ and all primitive idempotents
belonging to that sum can be calculated from a generating idempotent e of the
symmetry class of T by means of the irreducible characters or of a discrete
Fourier transform of $S_{r+1}$. We apply these facts to derivatives $\nabla S$,
$\nabla A$ of symmetric or alternating tensor fields. The symmetry classes of
the differences $\nabla S - sym(\nabla S)$ and $\nabla A - alt(\nabla A)$ are
characterized by Young frames (r, 1) and (2, 1^{r-1}), respectively. However,
while the symmetry class of $\nabla A - alt(\nabla A)$ can be generated by
Young symmetrizers of (2, 1^{r-1}), no Young symmetrizer of (r, 1) generates
the symmetry class of $\nabla S - sym(\nabla S)$. Furthermore we show in the
case r = 2 that $\nabla S - sym(\nabla S)$ and $\nabla A - alt(\nabla A)$ can
be applied in generator formulas of algebraic covariant derivative curvature
tensors. For certain symbolic calculations we used the Mathematica packages
Ricci and PERMS.
"
1756,"On reconstructing n-point configurations from the distribution of
  distances or areas","  One way to characterize configurations of points up to congruence is by
considering the distribution of all mutual distances between points. This paper
deals with the question if point configurations are uniquely determined by this
distribution. After giving some counterexamples, we prove that this is the case
for the vast majority of configurations. In the second part of the paper, the
distribution of areas of sub-triangles is used for characterizing point
configurations. Again it turns out that most configurations are reconstructible
from the distribution of areas, though there are counterexamples.
"
1757,Whitehead method and Genetic Algorithms,"  In this paper we discuss a genetic version (GWA) of the Whitehead's
algorithm, which is one of the basic algorithms in combinatorial group theory.
It turns out that GWA is surprisingly fast and outperforms the standard
Whitehead's algorithm in free groups of rank >= 5. Experimenting with GWA we
collected an interesting numerical data that clarifies the time-complexity of
the Whitehead's Problem in general. These experiments led us to several
mathematical conjectures. If confirmed they will shed light on hidden
mechanisms of Whitehead Method and geometry of automorphic orbits in free
groups.
"
1758,"Balanced presentations of the trivial group on two generators and the
  Andrews-Curtis conjecture","  The Andrews-Curtis conjecture states that every balanced presentation of the
trivial group can be reduced to the standard one by a sequence of the
elementary Nielsen transformations and conjugations. In this paper we describe
all balanced presentations of the trivial group on two generators and with the
total length of relators <= 12. We show that all these presentations satisfy
the Andrews-Curtis conjecture.
"
1759,Genetic algorithms and the Andrews-Curtis conjecture,"  The Andrews-Curtis conjecture claims that every balanced presentation of the
trivial group can be transformed into the trivial presentation by a finite
sequence of ""elementary transformations"" which are Nielsen transformations
together with an arbitrary conjugation of a relator. It is believed that the
Andrews-Curtis conjecture is false; however, not so many possible
counterexamples are known. It is not a trivial matter to verify whether the
conjecture holds for a given balanced presentation or not. The purpose of this
paper is to describe some non-deterministic methods, called Genetic Algorithms,
designed to test the validity of the Andrews-Curtis conjecture. Using such
algorithm we have been able to prove that all known (to us) balanced
presentations of the trivial group where the total length of the relators is at
most 12 satisfy the conjecture. In particular, the Andrews-Curtis conjecture
holds for the presentation <x,y|x y x = y x y, x^2 = y^3> which was one of the
well known potential counterexamples.
"
1760,"Generators of algebraic covariant derivative curvature tensors and Young
  symmetrizers","  We show that the space of algebraic covariant derivative curvature tensors R'
is generated by Young symmetrized tensor products W*U or U*W, where W and U are
covariant tensors of order 2 and 3 whose symmetry classes are irreducible and
characterized by the following pairs of partitions: {(2),(3)}, {(2),(2 1)} or
{(1 1),(2 1)}. Each of the partitions (2), (3) and (1 1) describes exactly one
symmetry class, whereas the partition (2 1) characterizes an infinite set S of
irreducible symmetry classes. This set S contains exactly one symmetry class
S_0 whose elements U can not play the role of generators of tensors R'. The
tensors U of all other symmetry classes from S\{S_0} can be used as generators
for tensors R'. Foundation of our investigations is a theorem of S. A. Fulling,
R. C. King, B. G. Wybourne and C. J. Cummins about a Young symmetrizer that
generates the symmetry class of algebraic covariant derivative curvature
tensors. Furthermore we apply ideals and idempotents in group rings C[Sr], the
Littlewood-Richardson rule and discrete Fourier transforms for symmetric groups
Sr. For certain symbolic calculations we used the Mathematica packages Ricci
and PERMS.
"
1761,"Short formulas for algebraic covariant derivative curvature tensors via
  Algebraic Combinatorics","  We consider generators of algebraic covariant derivative curvature tensors R'
which can be constructed by a Young symmetrization of product tensors W*U or
U*W, where W and U are covariant tensors of order 2 and 3. W is a symmetric or
alternating tensor whereas U belongs to a class of the infinite set S of
irreducible symmetry classes characterized by the partition (2,1). Using
Computer Algebra we search for such generators whose coordinate representations
are polynomials with a minimal number of summands. For a generic choice of the
symmetry class of U we obtain lengths of 16 or 20 summands if W is symmetric or
skew-symmetric, respectively. In special cases these numbers can be reduced to
the minima 12 or 10. If these minima occur then U admits an index commutation
symmetry. Furthermore minimal lengths are possible if U is formed from
torsion-free covariant derivatives of symmetric or alternating 2-tensor fields.
Foundation of our investigations is a theorem of S. A. Fulling, R. C. King, B.
G. Wybourne and C. J. Cummins about a Young symmetrizer that generates the
symmetry class of algebraic covariant derivative curvature tensors. Furthermore
we apply ideals and idempotents in group rings C[S_r] and discrete Fourier
transforms for symmetric groups S_r. For symbolic calculations we used the
Mathematica packages Ricci and PERMS.
"
1762,"Generators of algebraic curvature tensors based on a (2,1)-symmetry","  We consider generators of algebraic curvature tensors R which can be
constructed by a Young symmetrization of product tensors U*w or w*U, where U
and w are covariant tensors of order 3 and 1. We assume that U belongs to a
class of the infinite set S of irreducible symmetry classes characterized by
the partition (2,1). We show that the set S contains exactly one symmetry class
S_0 whose elements U can not play the role of generators of tensors R. The
tensors U of all other symmetry classes from S\{S_0} can be used as generators
for tensors R. Using Computer Algebra we search for such generators whose
coordinate representations are polynomials with a minimal number of summands.
For a generic choice of the symmetry class of U we obtain lengths of 8
summands. In special cases these numbers can be reduced to the minimum 4. If
this minimum occurs then U admits an index commutation symmetry. Furthermore
minimal lengths are possible if U is formed from torsion-free covariant
derivatives of alternating 2-tensor fields. We apply ideals and idempotents of
group rings C[S_r] of symmetric groups S_r, Young symmetrizers, discrete
Fourier transforms and Littlewood-Richardson products. For symbolic
calculations we used the Mathematica packages Ricci and PERMS.
"
1763,An hybrid system approach to nonlinear optimal control problems,"  We consider a nonlinear ordinary differential equation and want to control
its behavior so that it reaches a target by minimizing a cost function. Our
approach is to use hybrid systems to solve this problem: the complex dynamic is
replaced by piecewise affine approximations which allow an analytical
resolution. The sequence of affine models then forms a sequence of states of a
hybrid automaton. Given a sequence of states, we introduce an hybrid
approximation of the nonlinear controllable domain and propose a new algorithm
computing a controllable, piecewise convex approximation. The same way the
nonlinear optimal control problem is replaced by an hybrid piecewise affine
one. Stating a hybrid maximum principle suitable to our hybrid model, we deduce
the global structure of the hybrid optimal control steering the system to the
target.
"
1764,"Methods for the construction of generators of algebraic curvature
  tensors","  We demonstrate the use of several tools from Algebraic Combinatorics such as
Young tableaux, symmetry operators, the Littlewood-Richardson rule and discrete
Fourier transforms of symmetric groups in investigations of algebraic curvature
tensors.
"
1765,Stationary or static space-times and Young tableaux,"  Algebraic curvature tensors possess generators which can be formed from
symmetric or alternating tensors S, A or tensors \theta with an irreducible
(2,1)-symmetry. In differential geometry examples of curvature formulas are
known which contain generators on the basis of S or A realized by
differentiable tensor fields in a natural way. We show that certain curvature
formulas for stationary or static space-times contain such differentiable
realizations of generators based on \theta. The tensor \theta is connected with
the timelike Killing vector field of the space-time. \theta lies in a special
symmetry class from the infinite family of irreducible (2,1)-symmetry classes.
We determine characteristics of this class. In particular, this class allows a
maximal reduction of the length of the curvature formulas. We use a projection
formalism by Vladimirov, Young symmetrizers and Littlewood-Richardson products.
Computer calculations were carried out by means of the packages Ricci and
PERMS.
"
1766,"Computing the First Betti Numberand Describing the Connected Components
  of Semi-algebraic Sets","  In this paper we describe a singly exponential algorithm for computing the
first Betti number of a given semi-algebraic set. Singly exponential algorithms
for computing the zero-th Betti number, and the Euler-Poincar\'e
characteristic, were known before. No singly exponential algorithm was known
for computing any of the individual Betti numbers other than the zero-th one.
We also give algorithms for obtaining semi-algebraic descriptions of the
semi-algebraically connected components of any given real algebraic or
semi-algebraic set in single-exponential time improving on previous results.
"
1767,"Computing the First Few Betti Numbers of Semi-algebraic Sets in Single
  Exponential Time","  In this paper we describe an algorithm that takes as input a description of a
semi-algebraic set $S \subset \R^k$, defined by a Boolean formula with atoms of
the form $P > 0, P < 0, P=0$ for $P \in {\mathcal P} \subset \R[X_1,...,X_k],$
and outputs the first $\ell+1$ Betti numbers of $S$, $b_0(S),...,b_\ell(S).$
The complexity of the algorithm is $(sd)^{k^{O(\ell)}},$ where where $s =
#({\mathcal P})$ and $d = \max_{P\in {\mathcal P}}{\rm deg}(P),$ which is
singly exponential in $k$ for $\ell$ any fixed constant. Previously, singly
exponential time algorithms were known only for computing the Euler-Poincar\'e
characteristic, the zero-th and the first Betti numbers.
"
1768,"Gr\""obner Bases and Generation of Difference Schemes for Partial
  Differential Equations","  In this paper we present an algorithmic approach to the generation of fully
conservative difference schemes for linear partial differential equations. The
approach is based on enlargement of the equations in their integral
conservation law form by extra integral relations between unknown functions and
their derivatives, and on discretization of the obtained system. The structure
of the discrete system depends on numerical approximation methods for the
integrals occurring in the enlarged system. As a result of the discretization,
a system of linear polynomial difference equations is derived for the unknown
functions and their partial derivatives. A difference scheme is constructed by
elimination of all the partial derivatives. The elimination can be achieved by
selecting a proper elimination ranking and by computing a Gr\""obner basis of
the linear difference ideal generated by the polynomials in the discrete
system. For these purposes we use the difference form of Janet-like Gr\""obner
bases and their implementation in Maple. As illustration of the described
methods and algorithms, we construct a number of difference schemes for Burgers
and Falkowich-Karman equations and discuss their numerical properties.
"
1769,The Newton Polytope of the Implicit Equation,"  We apply tropical geometry to study the image of a map defined by Laurent
polynomials with generic coefficients. If this image is a hypersurface then our
approach gives a construction of its Newton polytope.
"
1770,Extending the scalars of minimizations,"  In the classical theory of formal languages, finite state automata allow to
recognize the words of a rational subset of $\Sigma^*$ where $\Sigma$ is a set
of symbols (or the alphabet). Now, given a semiring $(\K,+,.)$, one can
construct $\K$-subsets of $\Sigma^*$ in the sense of Eilenberg, that are
alternatively called noncommutative formal power series for which a framework
very similar to language theory has been constructed Particular noncommutative
formal power series, which are called rational series, are the behaviour of a
family of weighted automata (or $\K$-automata). In order to get an efficient
encoding, it may be interesting to point out one of them with the smallest
number of states. Minimization processes of $\K$-automata already exist for
$\K$ being: {\bf a)} a field, {\bf b)} a noncommutative field, {\bf c)} a PID .
When $\K$ is the bolean semiring, such a minimization process (with
isomorphisms of minimal objects) is known within the category of deterministic
automata. Minimal automata have been proved to be isomorphic in cases {\bf (a)}
and {\bf (b)}. But the proof given for (b) is not constructive. In fact, it
lays on the existence of a basis for a submodule of $\K^n$. Here we give an
independent algorithm which reproves this fact and an example of a pair of
nonisomorphic minimal automata. Moreover, we examine the possibility of
extending {\bf (c)}. To this end, we provide an {\em Effective Minimization
Process} (or {\em EMP}) which can be used for more general sets of
coefficients.
"
1771,Direct and dual laws for automata with multiplicities,"  We present here theoretical results coming from the implementation of the
package called AMULT (automata with multiplicities in several noncommutative
variables). We show that classical formulas are ``almost every time'' optimal,
characterize the dual laws preserving rationality and also relators that are
compatible with these laws.
"
1772,"Transitive factorizations of free partially commutative monoids and Lie
  algebras","  Let $\M(A,\theta)$ be a free partially commutative monoid. We give here a
necessary and sufficient condition on a subalphabet $B\subset A$ such that the
right factor of a bisection $\M(A,\theta)=\M(B,\theta\_B).T$ be also partially
commutative free. This extends strictly the (classical) elimination theory on
partial commutations and allows to construct new factorizations of
$\M(A,\theta)$ and associated bases of $L\_K(A,\theta)$.
"
1773,Minimal Polynomials for the Coordinates of the Harborth Graph,"  The Harborth graph is the smallest known example of a 4-regular planar
unit-distance graph. In this paper we give an analytical description of the
coordinates of its vertices for a particular embedding in the Euclidean plane.
More precisely, we show, how to calculate the minimal polynomials of the
coordinates of its vertices (with the help of a computer algebra system), and
list those. Furthermore some algebraic properties of these polynomials, and
consequences to the structure of the Harborth graph are determined.
"
1774,"Fast Jacobian group operations for C_{3,4} curves over a large finite
  field","  Let C be an arbitrary smooth algebraic curve of genus g over a large finite
field K. We revisit fast addition algorithms in the Jacobian of C due to
Khuri-Makdisi (math.NT/0409209, to appear in Math. Comp.). The algorithms,
which reduce to linear algebra in vector spaces of dimension O(g) once |K| >>
g, and which asymptotically require O(g^{2.376}) field operations using fast
linear algebra, are shown to perform efficiently even for certain low genus
curves. Specifically, we provide explicit formulae for performing the group law
on Jacobians of C_{3,4} curves of genus 3. We show that, typically, the
addition of two distinct elements in the Jacobian of a C_{3,4} curve requires
117 multiplications and 2 inversions in K, and an element can be doubled using
129 multiplications and 2 inversions in K. This represents an improvement of
approximately 20% over previous methods.
"
1775,Uncomputably Large Integral Points on Algebraic Plane Curves?,"  We show that the decidability of an amplification of Hilbert's Tenth Problem
in three variables implies the existence of uncomputably large integral points
on certain algebraic curves. We obtain this as a corollary of a new positive
complexity result: the Diophantine prefixes EAE and EEAE are generically
decidable. This means, taking the former prefix as an example, that we give a
precise geometric classification of those polynomials f in Z[v,x,y] for which
the question...
  ``Does there exists a v in N such that for all x in N, there exists a y in N
with f(v,x,y)=0?''
  ...may be undecidable, and we show that this set of polynomials is quite
small in a rigourous sense. (The decidability of EAE was previously an open
question.) The analogous result for the prefix EEAE is even stronger. We thus
obtain a connection between the decidability of certain Diophantine problems,
height bounds for points on curves, and the geometry of certain complex
surfaces and 3-folds.
"
1776,Sufficient set of integrability conditions of an orthonomic system,"  Every orthonomic system of partial differential equations is known to possess
a finite number of integrability conditions sufficient to ensure the validity
of all. Herewith we offer an efficient algorithm to construct a sufficient set
of integrability conditions free of redundancies.
"
